(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 29 Apr 2025 12:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Amazon to display tariff costs for consumers (175 pts)]]></title>
            <link>https://punchbowl.news/article/tech/amazon-display-tariff-costs/</link>
            <guid>43831027</guid>
            <pubDate>Tue, 29 Apr 2025 11:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://punchbowl.news/article/tech/amazon-display-tariff-costs/">https://punchbowl.news/article/tech/amazon-display-tariff-costs/</a>, See on <a href="https://news.ycombinator.com/item?id=43831027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                        <p><img width="1024" height="683" src="https://punchbowl.news/wp-content/uploads/GettyImages-2209820827.jpg" alt="Amazon will soon display how much of an item’s cost is derived from tariffs — right next to the product’s total listed price." decoding="async" srcset="https://punchbowl.news/wp-content/uploads/GettyImages-2209820827.jpg 1024w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-300x200.jpg 300w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-768x512.jpg 768w, https://punchbowl.news/wp-content/uploads/GettyImages-2209820827-50x33.jpg 50w" sizes="(max-width: 1024px) 100vw, 1024px">                        </p>

                        
                        
                        <div>
                                                            <p><strong>Amazon doesn’t want</strong>&nbsp;to shoulder the blame for the cost of President&nbsp;<strong>Donald Trump</strong>’<strong>s</strong>&nbsp;trade war.</p>
<p><strong>So the e-commerce giant</strong>&nbsp;will<strong>&nbsp;</strong>soon show how much Trump’s tariffs are adding to the price of each product, according to a person familiar with the plan.</p>
<p><strong>The shopping site&nbsp;</strong>will display how much of an item’s cost is derived from tariffs – right next to the product’s total listed price.</p>
                                
                                                                                        <div>
        <p><img src="https://punchbowl.news/wp-content/themes/punchbowl-news/assets/images/tech-icon.svg" alt="Subscripion logo">
        </p>
        <h4>You're seeing a preview of our <span>Premium Policy: Tech</span> coverage. Read the full story by <a href="https://punchbowl.news/pricing">subscribing here.</a></h4>
    </div>                            
                            
                            
                        </div>
                    </div><p>Editorial photos provided by Getty Images. Political ads courtesy of AdImpact.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dear "Security Researchers" (136 pts)]]></title>
            <link>https://ftp.bit.nl/pub/debian/</link>
            <guid>43829080</guid>
            <pubDate>Tue, 29 Apr 2025 05:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ftp.bit.nl/pub/debian/">https://ftp.bit.nl/pub/debian/</a>, See on <a href="https://news.ycombinator.com/item?id=43829080">Hacker News</a></p>
<div id="readability-page-1" class="page">
<span>Dear "Security Researchers",<p>

Welcome to our *PUBLIC* OPEN SOURCE SOFTWARE MIRROR SERVER.<br>
Please DO NOT report this under our responsible disclosure policy.<br>
This is a PUBLIC service, with OPEN SOURCE SOFTWARE, and NOT a security threat to our company.<br>
There is NO SENSITIVE INFORMATION on this server.</p><p>

Thanks.</p></span>
<hr>
  <table>
   <tbody><tr><th><img src="https://ftp.bit.nl/icons/blank.gif" alt="[ICO]"></th><th><a href="https://ftp.bit.nl/pub/debian/?C=N;O=D">Name</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=M;O=A">Last modified</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=S;O=A">Size</a></th><th><a href="https://ftp.bit.nl/pub/debian/?C=D;O=A">Description</a></th></tr>
   <tr><th colspan="5"><hr></th></tr>
<tr><td><img src="https://ftp.bit.nl/icons/back.gif" alt="[PARENTDIR]"></td><td><a href="https://ftp.bit.nl/pub/">Parent Directory</a></td><td>&nbsp;</td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/hand.right.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/README">README</a></td><td>2025-03-15 09:29  </td><td>1.2K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/unknown.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.CD-manufacture">README.CD-manufacture</a></td><td>2010-06-26 11:52  </td><td>1.3K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.html">README.html</a></td><td>2025-03-15 09:29  </td><td>2.9K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.mirrors.html">README.mirrors.html</a></td><td>2017-03-04 21:08  </td><td>291 </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/text.gif" alt="[TXT]"></td><td><a href="https://ftp.bit.nl/pub/debian/README.mirrors.txt">README.mirrors.txt</a></td><td>2017-03-04 21:08  </td><td> 86 </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/dists/">dists/</a></td><td>2025-03-15 09:29  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/doc/">doc/</a></td><td>2025-04-29 03:52  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/unknown.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/extrafiles">extrafiles</a></td><td>2025-04-29 04:27  </td><td>201K</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/indices/">indices/</a></td><td>2025-04-29 04:26  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/compressed.gif" alt="[   ]"></td><td><a href="https://ftp.bit.nl/pub/debian/ls-lR.gz">ls-lR.gz</a></td><td>2025-04-29 04:18  </td><td> 15M</td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/pool/">pool/</a></td><td>2022-10-05 19:09  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/project/">project/</a></td><td>2008-11-18 00:05  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/tools/">tools/</a></td><td>2012-10-10 18:29  </td><td>  - </td><td>&nbsp;</td></tr>
<tr><td><img src="https://ftp.bit.nl/icons/folder.gif" alt="[DIR]"></td><td><a href="https://ftp.bit.nl/pub/debian/zzz-dists/">zzz-dists/</a></td><td>2023-10-07 13:07  </td><td>  - </td><td>&nbsp;</td></tr>
   <tr><th colspan="5"><hr></th></tr>
</tbody></table>



  <title>Debian Archive</title>
  <meta name="Modified" content="2025-03-15">



<h2>Debian Archive</h2>

<p>See <a href="https://www.debian.org/">https://www.debian.org/</a>
for information about Debian GNU/Linux.</p>

<h2>Current Releases</h2>

<p>Four Debian releases are available on the main site:</p>

<blockquote>
<dl>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/buster/">Debian 10.13, or buster</a></dt>
<dd>Debian 10.13 was released Saturday, 10th September 2022.
<a href="https://www.debian.org/releases/buster/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/buster/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/bullseye/">Debian 11.11, or bullseye</a></dt>
<dd>Debian 11.11 was released Saturday, 31st August 2024.
<a href="https://www.debian.org/releases/bullseye/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/bullseye/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/bookworm/">Debian 12.10, or bookworm</a></dt>
<dd>Debian 12.10 was released Saturday, 15th March 2025.
<a href="https://www.debian.org/releases/bookworm/amd64/">Installation
and upgrading instructions</a>,
<a href="https://www.debian.org/releases/bookworm/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/testing/">Testing, or trixie</a></dt>
<dd>The current tested development snapshot is named trixie.<br>
Packages which have been tested in unstable and passed automated
tests propagate to this release.<br>
<a href="https://www.debian.org/releases/testing/">More information</a>
</dd>

<dt><a href="https://ftp.bit.nl/pub/debian/dists/unstable/">Unstable, or sid</a></dt>
<dd>The current development snapshot is named sid.<br>
Untested candidate packages for future releases.<br>
<a href="https://www.debian.org/releases/unstable/">More information</a>
</dd>
</dl>
</blockquote>

<h2>Old Releases</h2>

<p>Older releases of Debian are at
<a href="http://archive.debian.org/debian-archive/">http://archive.debian.org/debian-archive</a>
<br>
<a href="https://www.debian.org/distrib/archive">More information</a>
</p>

<h2>CDs</h2>

<p>For more information about Debian CDs, please see
<a href="https://ftp.bit.nl/pub/debian/README.CD-manufacture">README.CD-manufacture</a>.
<br>
<a href="https://www.debian.org/CD/">Further information</a>
</p>

<h2>Mirrors</h2>

<p>For more information about Debian mirrors, please see
<a href="https://ftp.bit.nl/pub/debian/README.mirrors.html">README.mirrors.html</a>.
<br>
<a href="https://www.debian.org/mirror/">Further information</a>
</p>

<h2>Other directories</h2>

<table summary="Other directories">
<tbody><tr><td><a href="https://ftp.bit.nl/pub/debian/doc/">doc</a></td>          <td>Debian documentation.</td></tr>
<tr><td><a href="https://ftp.bit.nl/pub/debian/indices/">indices</a></td>  <td>Various indices of the site.</td></tr>
<tr><td><a href="https://ftp.bit.nl/pub/debian/project/">project</a></td>  <td>Experimental packages and other miscellaneous files.</td></tr>
</tbody></table>




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[LibreLingo – FOSS Alternative to Duolingo (333 pts)]]></title>
            <link>https://librelingo.app</link>
            <guid>43829035</guid>
            <pubDate>Tue, 29 Apr 2025 05:45:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://librelingo.app">https://librelingo.app</a>, See on <a href="https://news.ycombinator.com/item?id=43829035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="LibreLingo Mascot" src="https://librelingo.app/images/mascot-jetpack-noshadow.svg" data-test="mascot-jetpack"> </p> <p><h2><span data-tkey="index.subtitle">an experiment to create a community-driven language-learning platform</span></h2> </p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oracle engineers caused five days software outage at U.S. hospitals (134 pts)]]></title>
            <link>https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html</link>
            <guid>43828915</guid>
            <pubDate>Tue, 29 Apr 2025 05:25:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html">https://www.cnbc.com/2025/04/28/oracle-engineers-caused-days-long-software-outage-at-us-hospitals.html</a>, See on <a href="https://news.ycombinator.com/item?id=43828915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108079907" data-test="InlineImage"><p>Larry Ellison, co-founder and executive chairman of Oracle Corp., speaks during the Oracle OpenWorld 2018 conference in San Francisco, California, U.S., on Monday, Oct. 22, 2018.</p><p>David Paul Morris | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/ORCL/">Oracle</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> engineers mistakenly triggered a five-day software outage at a number of Community Health Systems hospitals, causing the facilities to temporarily return to paper-based patient records.</p><p>CHS told CNBC that the outage involving Oracle Health, the company's electronic health record (EHR) system, affected "several" hospitals, leading them to activate "downtime procedures." Trade publication Becker's Hospital Review reported that 45 hospitals were hit.</p><p>The outage began on April 23, after engineers conducting maintenance work mistakenly deleted critical storage connected to a key database, a CHS spokesperson said in a statement. The outage was resolved on Monday, and was not related to a cyberattack or other security incident.</p><p>CHS is based in Tennessee and includes 72 hospitals in 14 states, according to the medical system's website.</p><p>"Despite this being a major outage, our hospitals were able to maintain services with no&nbsp;material impact," the spokesperson said. "We are proud of our clinical and support teams who worked through the multi-day outage with professionalism and a commitment to delivering high-quality, safe care for&nbsp;patients."&nbsp;</p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Oracle stock this year</p></div><div><p>Oracle didn't immediately respond to CNBC's request for comment.</p><p>An EHR is a digital version of a patient's medical history that's updated by doctors and nurses. It's crucial software within the U.S. health-care system, and outages can cause serious disruptions to patient care. Oracle acquired EHR vendor Cerner in 2022 for $28.3 billion, becoming the second-biggest player in the market, behind Epic Systems.</p><p>Now that Oracle's systems are back online, CHS said that the impacted hospitals&nbsp;are working to "re-establish full functionality and return to normal operations and procedures."</p><p>Oracle's CHS error comes weeks after the company's federal electronic health record experienced a <a href="https://www.cnbc.com/2025/03/06/oracles-federal-electronic-health-record-suffered-nation-wide-outage-.html">nationwide outage</a>. Oracle has struggled with a thorny, years-long EHR rollout with the Department of Veterans Affairs, marred by patient safety concerns. The agency launched a&nbsp;<a href="https://news.va.gov/press-room/va-announces-strategic-review-of-electronic-health-record-modernization-program/" target="_blank">strategic review</a>&nbsp;of Cerner in 2021, before Oracle's acquisition, and it temporarily&nbsp;<a href="https://digital.va.gov/ehr-modernization/ehr-deployment-schedule/" target="_blank">paused deployment</a>&nbsp;of the software in 2023.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/02/20/oracle-ceo-safra-catz-being-number-one-is-very-important.html">Interview with Oracle CEO Safra Catz</a></p></div><div id="Placeholder-ArticleBody-Video-108105025" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000367188" aria-labelledby="Placeholder-ArticleBody-Video-108105025"><p><img src="https://image.cnbcfm.com/api/v1/image/108105026-17400675801740067576-38548750502-1080pnbcnews.jpg?v=1740067578&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Oracle CEO Safra Catz: Being number one is very important"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Knowledge-based society, my ass (274 pts)]]></title>
            <link>https://mihaiolteanu.me/knowledge-based-society-my-ass</link>
            <guid>43828713</guid>
            <pubDate>Tue, 29 Apr 2025 04:33:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mihaiolteanu.me/knowledge-based-society-my-ass">https://mihaiolteanu.me/knowledge-based-society-my-ass</a>, See on <a href="https://news.ycombinator.com/item?id=43828713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contents">

            

            <p>
            Right after I get admitted, I inform Professor that I also have a full-time
            job. He insists that we must start working right away. I quit as a result and
            instantly breathe a refined air. I am now a scientist! A week later I approach
            Professor and let him know I'm ready for work.
            </p>

            <p>
            "Ready for what?" he greets me as though our previous conversation didn't
            happen. I remind him that he's my PhD supervisor and that, at his proposal, we
            are studying the effects of electromagnetic fields on patients with carotid
            stent implants. "There is nothing for you to do at the University, you can
            stay home for now," he tells me. Is he really serious? Does he want me to do
            research from bed? I insist on reading materials related to our field of
            research. I want to start right away. But he can't recommend any.
            </p>

            <p>
            My first day of research is over. It's autumn 2009. I spend the following days
            of my academic life oversleeping and strolling the city parks. I actually
            enjoy this newfound freedom from the alarm clock. I think, not without a
            certain longing, of my former factory colleagues. How we used to laugh at the
            stupidest of things, how it all felt like a big family. But I have a new life
            now. An intellectual one.
            </p>

            <p>
            A few weeks go by. No word from Professor. It's exhausting to conduct research
            like this. I need some color. I approach Professor again and ask for basic
            research equipment, "I need an office, Professor," I begin, and, after a short
            while, I raise my stakes, "And a computer, too!"  I've gone too
            far. "Everybody is happy around here, except you!!!" he snaps at me. I get a
            feeling that I'm going nowhere with Professor.
            </p>

            <p>
            I approach the Head of Department. The Head listens carefully for my
            complaints and kindly informs me he doesn't mingle in Professor's business. It
            is up to my advisor to decide where the resources are allocated within his
            team. A nice way of deflecting responsibility.
            </p>

            <p>
            The Faculty's Dean doesn't give a damn either but I think he wants to avoid
            even more troubles seeing that I'm so stubborn. I soon receive an email from
            Professor as a result. He decides to offer me an office and a computer. Two
            months wasted. Even so, I celebrate my first academic victory.
            </p>

            <p>
            "Grab a computer and follow me," Professor instructs me a few days later. I
            can barely hide my enthusiasm. We take the stairs to the first floor. Then
            ground floor. Then basement. "Almost there," I hear Professor in the
            darkness. After two more turns he opens a big door and hands me over the
            keys. My office is a rather spacious but austere room in the University's
            basement. My initial enthusiasm is fading. There's a simple desk with a basic
            chair at one end and a small, too high to reach window at the other. The walls
            are immaculately white. A hospital-style metal locker where Professor keeps his
            valuables under key completes the picture. 
            </p>

            <p>
            "Doctoral Studies in Engineering Sciences for Developing the Knowledge-Based
            Society" is the name of our project. It pays me, and approximately one hundred
            other colleagues of mine, all PhD candidates, our €500/month scholarships, or
            about the average wage. It's one project from among the four thousand projects
            sponsored by the European Union's "Operational Program for Human Resources
            Development." This grandiose program, with an available budget of €5 billion,
            aims to "develop the human capital and increase competitiveness by bringing
            education and life-long learning in sync with a modern, flexible and inclusive
            labor market and increase future opportunities for 1.650.000 people." Yes,
            those are millions, 15.000 of which are to become PhD students! The Government
            says so, I it as fact.
            </p>

            <p>
            For my part, I have to publish at least three scientific papers, present my
            research at one international conference and successfully defend my thesis in
            public. I have exactly three years at my disposal. If I fail, I have to return
            my scholarship in full. It's also up to me to rejoin the labor market and take
            care of my future, possibly as a teaching assistant here, at the
            University. So I take things seriously and go to work each morning. I learn
            and labor as hard as I can.
            </p>

            <p>
            I begin with the documentation phase and I read, among others, a very detailed
            series of experiments: human subjects placed in anechoic chambers with all
            kinds of electromagnetic fields directed at them. They measure changes in
            sweat rate, breathing rate, exhaled humidity, body temperatures, blood
            pressure and everything one can imagine. They try to figure out how the body
            responds and adapts to such an external stimulus.
            </p>

            <p>
            I, for my side, have to see what happens inside the human neck artery when an
            implanted stent heats up under the influence of electromagnetic fields. How
            does the body react and compensate for such a temperature increase, if there
            is one? I have things to discover. But I also have zero lab equipment. Not
            even a digital thermometer, let alone medical equipment of the kind I would
            need. The whole medical engineering's lab, the one I took my computer from, is
            a room twice the size of my basement with ten desktop computers in it, a
            blackboard, a small window blocked by another building and an extra door for a
            special room: Professor's own office.
            </p>

            <p>
            Critically, I do not even get to see or touch a real stent. We don't have
            any. There are no interactions with patients, no collaboration with doctors
            and no conversations with other engineers from our University. I'm alone in my
            office. I'm not sure what people do around here. When the whole Department
            gathers around, I hear professors complaining about "kids these days" but zero
            technical discussions and hardly any interest in scientific topics. We are
            one-man teams, each working in their only little basements, so to speak.
            </p>

            <p>
            Professor reassures me that computer simulations are enough for our study. So
            I try to find software licenses plus realistic computer models for my stents
            and human heads. They all cost money and are hard to find. To develop them
            from scratch is outside my specialty. Ideally, I should understand a bit of
            human biology, too, but that's again outside my specialty. I wonder at this
            point if I actually have a specialty. What makes me qualified to approach
            these issues? Why would my "discoveries", born out of such meager
            possibilities, have any relevance for science?
            </p>

            <p>
            I don't lack motivation, though. I try to get a license for the €20k per year
            software we're using. It proves to be another catacombic adventure. The
            company offers two free licenses per public institution. I ask Professor for a
            license, but "There aren't any left," he informs me. "Don't we have two?" I
            insist. "Well, yes, but one license is on my laptop, which I always carry with
            me, and the other is on my office computer," he replies. I ask permission to
            his office to run some simulations from time to time, but "No, my office is
            closed and only I have the key." I conclude Professor has a terrible fondness
            for locks and keys. I drop it. I'm not sure what he does with two
            licenses. Maybe he sells them on the black market? Maybe Professor is a
            gangster? Who knows.
            </p>

            <p>
            One of my colleagues who is pursuing his PhD in the same Department under a
            different professor and a similar area of research, with whom I only cross
            paths when our blood pressure runs too high, happens to also work for a public
            institution. He applies for the two free licenses and is generous enough to
            offer me one.
            </p>

            <p>
            Another victory. But I'm fed up with these victories. It's exhausting to fight
            all these absurd battles. My time is running out. I have to write some papers
            soon. I accept my fate. I accept I'm not gonna be a scientist the way I've
            imagined more than a year ago. I don't see any future for me here at the
            University. As a result, I simplify things tremendously. I draw a big sphere
            and pretend it's a human head, I place a long metallic cylinder inside it and
            pretend it's a real stent and I place a simple antenna close by. Anything more
            complicated than this crashes my toy computer. I soon realize that I play
            scientist like kids play cop with water pistols.
            </p>

            <p>
            I get to publish my first paper in this way. I'm actually quite proud of it,
            given the circumstances. I actually start to enjoy writing. I put down my
            colleague as a co-author as a thank you for lending me the license. We've
            learned this trick from the professors who do it all the time with their
            books, papers and conferences. They are required, just as we are, to publish
            and look active in the community per their contract with the University.
            </p>
            
            <p>
            Out of curiosity, I start reading our school's newspaper, as my colleague
            calls our University's scientific journal. I soon spot inconsistencies. The
            wording is in plain, boring language with long introductions repeating the
            same generalities and facts known to all. But the style changes unexpectedly
            sometimes. I search these peculiar phrases online and my intuition is
            confirmed. Unacknowledged commandeering of intellectual labor via
            indiscriminate copy and paste practices. Plagiarism, in short. I find dozens
            of such instances. I see the name of our Head in there, too. I try to raise
            awareness for a month or two. Nobody gives a damn.
            </p><p>

            I stop reading the school's newspaper and concentrate on publishing my other
            papers instead. They are nothing more than variations on the first paper with
            different titles and different pictures. I let my computer run overnight and
            invent slightly different simulation scenarios and I underline different
            aspects of my results in each paper. After this, I take a more relaxed
            approach regarding my scientific pursuits, enjoy the show around me
            instead and stop giving a damn about Professor from now on.
            </p>
            
            <p>
            I notice the Head is emphasizing the "academic dress code" all the time. He
            even publishes an official Department guideline on this topic pressing us all
            to read it. I notice professors are always addressing each other formally even
            in informal settings, though they've been acquainted for years. This title
            caries great importance here. I myself make a blunder in this respect when I
            visit Professor's office one day for some official papers. I ask if he's
            around but I refer to him by his family name only. I get admonished for
            skipping the "professor" part. I apologies, add the missing title and address
            the question again. "No, Professor is not here!" comes the reply abruptly.
            </p>

            <p>
            Our Head both informs and threatens us, "Per the Department guidelines, every
            PhD candidate is required to teach for one semester. Find yourselves a seminar
            or a lab or I'll pick one for you." It so happens that I get friendly with an
            electronics department's professor. He asks me to be his teaching assistant. I
            inform the Head with great pleasure about this development and he, in turn,
            informs me with great satisfaction that "I do not give this position to PhD
            candidates." I insist, but in vain. I get used to insisting in vain. I get
            used to failing to figure out how this whole clusterfuck works. One colleague
            is appointed to teach C++ by the Head. "You know C++?" I ask enthusiastically,
            as I am looking to become a software engineer myself at this point. "I don't,"
            she informs me, "but there's enough time until Monday to learn it." It's
            Friday, the last day of my teaching career.
            </p>

            <p>
            Professor becomes my hero for a short time during a Department meeting. He
            insists that the design of high-voltage power lines is not actually a subject
            for his medical students. He wants more biology and medical related courses,
            instead. I truly believe in his vision. My mouth is wide open. But the Head
            again masterfully defends his position insisting on the necessity of assigning
            the minimum required number of hours per semester to each member of our
            Department, per the University guidelines. Nobody backs up my hero, not even
            he himself. The next topic on the agenda is the training of all our staff in
            the arts of digital blackboards "to help improve the teaching experience."
            </p>

            <p>
            I'm sinfully enjoying myself. What else do they do around here? Mrs. S. is our
            Department's team assistant. She's near retirement age and lives up in the
            attic. We visit her monthly to physically sign our presence in the attendance
            register. Sometimes she scolds us for signing in the wrong place, "That was a
            public holiday! You didn't work then, did you?!" There is no "Sir" nor
            "professor" with her. We are inhabiting a prestigious institution of higher
            learning, otherwise she would certainly call us morons. I'm wondering at the
            inefficiency on relying on handwritten notebooks for timekeeping. This
            Technical University has a Computer Science department, after all. But things
            are as they should be around here. There are many advantages to the analog
            methods. For instance, we avoid software bugs so this method is more precise,
            it fosters social interactions so it is more humane, we avoid proprietary
            software so users have complete control to modify the source code. We turn up
            once a month, sign and then we're free to do whatever. The Professor has given
            me the correct advice on that first day.
            </p>

            <p>
            The only constant human presence in the whole building during the warm summer
            days is the cleaning lady. I befriend her and we talk each morning. She
            provides me with paper towels and liquid soap for "When you might need it."
            Summer is vacation for both students and teachers, after all. From time to
            time I meet a stray professor in the hallways and they tell me I do a good
            job, always working, always studying, always present. Then, they excuses
            themselves with "I have to change my car's windshield" and other such
            important matters and then disappear for days or weeks on end.
            </p>

            <p>
            There's a big park with a lake nearby and a small river passes just behind the
            building. I often take small brakes from my academic life and stroll
            aimlessly. I sometimes watch the little fishes from the nearby bridge
            gathering in the shade of the willow trees. A kid approaches me one day, "Did
            you see the big one?" We chat a little. That's how I spend my
            days.
            </p>

            <p>
            With three months left, I send Professor my thesis. Days later, he warmly
            congratulates me, "You are an embarrassment to our city!" I stand
            alarmed. "Yes, you are ruining the prestige of our University!" I move closer.
            He points out a paragraph in my thesis where "almost impossible" is heavily
            underlined in red. "Something is either possible or impossible," he mocks me
            with a noticeable grin on his face. I update the offending sentence. I also
            fix a few typos in the following week and rephrase some paragraphs which were
            not to his liking. He eventually approves it. I present it in front of the
            whole Department, the last step before facing the official commission. It gets
            approved.
            </p>

            <p>
            My celebration is cut short a few days later. For some reason, it is of the
            utmost importance to have an actual, real-life experiment to confirm our
            theoretical results. "We can't present a theoretical thesis, we're a Technical
            University," Professor accuses me. I actually agree with him, though I have no
            soul left in this endeavor. How did he come up with this idea? I don't
            know. He probably got admonished by some higher-up. It was fine without it,
            the Department approved it, the Head approved it, it was ready for
            defending. Now it isn't. I shrug and accept it as another fact I can't
            understand nor influence.
            </p>

            <p>
            Professor finds a public institution to lend us their watermelon-sized
            anechoic chamber for two hours. We visit the supermarket one morning to buy
            pork chops for the human head. I want to bring to Professor's attention that
            we're studying a dynamic system and not dead meat. But it's autumn 2012
            already and the parks are in full color. It's way too late for any dialogue. I
            pull out a small plastic bag with a few miniature temperature sensors I bought
            the other day. Professor glues them to a metallic cylinder and inserts it in
            "the head." I see Professor was inspired by my way of handling the lack of
            real stents. I think it is a nail or wire of some sort but I'm not
            sure. Professor handles all the "sensitive equipment" himself. I take pictures
            and write down the results in a notebook. For the next two hours we gather
            temperature readings. I publish a paper with our findings shortly after,
            attach his name to it, update my thesis and everything is good again. I start
            to develop a faint feeling that I sleep better at night when I play along and
            nod approvingly to things I don't actually agree with instead of being
            pigheaded.
            </p>

            <p>
            The final day is approaching. Mrs. B., from the Department of Doctoral
            Studies, informs me that I personally have to prepare and bring in food,
            drinks and coffee for the commission when I defend my thesis. I refuse. She
            insists. I point out that the University charges €1000 per student for the
            final show, that each member of the commission is actually getting paid for
            their trouble and that all these expenses plus transport and accommodation are
            already sponsored by our project. She shows signs of slowly winning back her
            memory. Mrs. B. also informs me that I won't be able to hold back my tears
            upon successfully defending my thesis in front of family, friends and
            colleagues. I successfully defend my thesis a few days later and I refuse her
            that pleasure, too.
            </p>

            <p>
            We celebrate at a local restaurant with the whole Department and the
            commission of five professors that evening. I join out from politeness. There
            is not much science to celebrate. After dinner I shake hands with Professor
            and the Head. "He did make a lot of noise around here but he did a great job
            and has very nice results," the Professor praises me in front of the Head. I
            smile without saying anything. I leave the place and begin to think about the
            years in front of me. But Professor catches up with me. He is a changed man,
            "Let's keep working together!" He is brimming with enthusiasm. I refuse him
            politely but he keeps talking as though my previous answer carries no weight
            with him, as he always does. "Yes, let's keep cooperating on new projects
            together," he goes on and on. I don't know what's gotten into him. Maybe he
            likes his name on new papers too much? He begins to get on my nerves. I answer
            respectfully with simple no's to all of his questions and proposals. I
            eventually say my goodbyes to him and turn my back. I leave Professor in the
            dark alley and my basement behind for good.
            </p>

            <p>
                © Mihai Olteanu, 2025
            </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Congress passes Take It Down act despite major flaws (177 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/04/congress-passes-take-it-down-act-despite-major-flaws</link>
            <guid>43828568</guid>
            <pubDate>Tue, 29 Apr 2025 03:57:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/04/congress-passes-take-it-down-act-despite-major-flaws">https://www.eff.org/deeplinks/2025/04/congress-passes-take-it-down-act-despite-major-flaws</a>, See on <a href="https://news.ycombinator.com/item?id=43828568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <div>
        <div>
            <article role="article">
  
  
  <div><p>Today the U.S. House of Representatives passed the <a href="https://www.eff.org/deeplinks/2025/02/take-it-down-act-flawed-attempt-protect-victims-will-lead-censorship">TAKE IT DOWN</a> Act<span>, giving the powerful a dangerous new route to manipulate platforms&nbsp;into removing&nbsp;lawful&nbsp;speech that&nbsp;they&nbsp;simply don't like.&nbsp;President Trump himself has said that <a href="https://www.eff.org/deeplinks/2025/03/trump-calls-congress-pass-overbroad-take-it-down-act-so-he-can-use-it-censor">he would use</a> the law to censor his critics. The bill passed the Senate&nbsp;<a href="https://www.eff.org/deeplinks/2025/02/senate-passed-take-it-down-act-threatening-free-expression-and-due-process">in February</a>,&nbsp;and it now heads to the president's&nbsp;desk.&nbsp;</span></p>
<p><span>The takedown provision in TAKE IT DOWN applies to a much broader category of content—potentially any images involving intimate or sexual content—than the narrower NCII definitions found elsewhere in the bill. The takedown provision also lacks critical safeguards against frivolous or bad-faith takedown requests. Services will rely on automated filters, which are infamously blunt tools.&nbsp;They frequently flag legal content, from fair-use commentary to news reporting.&nbsp;The law’s tight time frame requires that apps and websites remove speech within 48 hours, rarely enough time to verify whether the speech is actually illegal. As a result, online service providers, particularly smaller ones, will likely choose to avoid the onerous legal risk by simply depublishing the speech rather than even attempting to verify it.<br></span></p>
<p>Congress is&nbsp;using the wrong approach to helping people whose intimate images are shared without their consent.&nbsp;TAKE IT DOWN pressures platforms to actively monitor speech, including speech that is presently encrypted. The law thus presents a huge threat to security and privacy online. While the bill is meant to address a serious problem, good intentions alone are not enough to make good policy.<span>&nbsp;</span>Lawmakers should be strengthening and enforcing existing legal protections for victims, rather than inventing new takedown regimes that are ripe for abuse.&nbsp;</p>

</div>

          </article>
    </div>
<div>
          <h2>Related Issues</h2>
            </div>

<div>
          <h2>Join EFF Lists</h2>
        
    </div>
<div>
          <h2>Related Updates</h2>
        <div>
        
  <div>
    <article role="article">
      <header>
                    <h3><a href="https://www.eff.org/deeplinks/2025/04/texass-war-abortion-now-war-free-speech" rel="bookmark">Texas’s War on Abortion Is Now a War on Free Speech</a></h3>
            
    </header>
  
  
  <div><p><strong>Once again, the Texas legislature is coming after the most common method of safe and effective abortion today—medication abortion.</strong><a href="https://capitol.texas.gov/tlodocs/89R/billtext/pdf/SB02880I.pdf#navpanes=0">Senate Bill (S.B.) 2880</a>* seeks to prevent the sale and distribution of abortion pills—but it doesn’t stop there. By restricting access to certain information online, the bill tries to keep people...</p></div>

          </article>
  </div>
  
  <div>
    <article role="article">
      <header>
                    <h3><a href="https://www.eff.org/deeplinks/2025/04/digital-identities-and-future-age-verification-europe" rel="bookmark">Digital Identities and the Future of Age Verification in Europe</a></h3>
            
    </header>
  
  
  <div><p><i>This is the first part of a three-part series about age verification in the European Union. In this blog post, we give an overview of the political debate around age verification and explore the age verification proposal introduced by the European Commission, based on digital identities. Part two takes a</i>...</p></div>

          </article>
  </div>
  
  
  
  <div>
    <article role="article">
      <header>
                    <h3><a href="https://www.eff.org/deeplinks/2025/03/eff-joins-7amleh-campaign-reconnectgaza" rel="bookmark">EFF Joins 7amleh Campaign to #ReconnectGaza</a></h3>
            
    </header>
  
  
  <div><p>In times of conflict, the internet becomes more than just a tool—it is a lifeline, connecting those caught in chaos with the outside world. It carries voices that might otherwise be silenced, bearing witness to suffering and survival. Without internet access, communities become isolated, and the flow of critical information...</p></div>

          </article>
  </div>
  
  <div>
    <article role="article">
      <header>
                    <h3><a href="https://www.eff.org/deeplinks/2025/03/eff-stands-perkins-coie-and-rule-law" rel="bookmark">EFF Stands with Perkins Coie and the Rule of Law </a></h3>
            
    </header>
  
  
  <div><p>As a legal organization that has fought in court to defend the rights of technology users for almost 35 years, including numerous legal challenges to federal government overreach, Electronic Frontier Foundation unequivocally supports Perkins Coie’s challenge to the Trump administration’s shocking, vindictive, and unconstitutional <a href="https://www.whitehouse.gov/presidential-actions/2025/03/addressing-risks-from-perkins-coie-llp/" target="_blank" rel="noopener noreferrer">Executive Order</a>....</p></div>

          </article>
  </div>
    </div>    </div>
      </div>

      <div><h2>Related Issues</h2></div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why did Windows 7 log on slower for months if you had a solid color background? (321 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20250428-00/?p=111121</link>
            <guid>43827214</guid>
            <pubDate>Mon, 28 Apr 2025 23:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20250428-00/?p=111121">https://devblogs.microsoft.com/oldnewthing/20250428-00/?p=111121</a>, See on <a href="https://news.ycombinator.com/item?id=43827214">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-111121">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <p>Personally, I use a solid color background. It was the default in Windows 95,¹ and I’ve stuck with that bluish-green background color ever since. It’s sort of like my comfort food.</p>
<p>Imagine my surprise when someone pointed me to a support article titled “<a title="The Welcome screen may be displayed for 30 seconds during the logon process after you set a solid color as the desktop background in Windows 7 or in Windows Server 2008 R2" href="https://support.microsoft.com/en-us/topic/the-welcome-screen-may-be-displayed-for-30-seconds-during-the-logon-process-after-you-set-a-solid-color-as-the-desktop-background-in-windows-7-or-in-windows-server-2008-r2-b4565ced-703a-cc85-bf9c-6b3d586d6421">The Welcome screen may be displayed for 30 seconds during the logon process after you set a solid color as the desktop background in Windows 7 or in Windows Server 2008 R2</a>.” Why is logon slower with a solid background?</p>
<p>After your logon has been authenticated, Windows sets up your desktop. There are a lot of things going on. The taskbar gets created. The components that are responsible for various system services are loaded and initialized. The desktop window is created and filled with icons. And the desktop background window loads up the desktop wallpaper and paints it to the screen.</p>
<p>The logon system waits for all of these pieces to report that they are ready, and when the all-clear signal is received from everybody, or when 30 seconds have elapsed, the logon system switches away from the Welcome screen.</p>
<p>Given that design, you can imagine the reason for the 30-second delay: It means that one of the pieces failed to report. Perhaps it was written like this:</p>
<pre>InitializeWallpaper()
{
    if (wallpaper bitmap defined)
    {
        LoadWallpaperBitmap();
    }
}

LoadWallpaperBitmap()
{
    locate the bitmap on disk
    load it into memory
    paint it on screen
    Report(WallpaperReady);
}
</pre>
<p>The code to report that the wallpaper is ready was inside the wallpaper bitmap code, which means that if you don’t have a wallpaper bitmap, the report is never made, and the logon system waits in vain for a report that will never arrive.</p>
<p>Later in the article, it notes a related article that calls out that if you have the “Hide desktop icons” group policy enabled, then you might also suffer from the 30-second delay.</p>
<p>Group policies are susceptible to this problem because they tend to be bolted on after the main code is written. When you have to add a group policy, you find the code that does the thing, and you put a giant “if policy allows” around it.</p>
<pre>// Original code
InitializeDesktopIcons()
{
    bind to the desktop folder
    enumerate the icons
    add them to the screen
    Report(DesktopIconsReady);
}

// Updated with group policy support

InitializeDesktopIcons()
{
    <span>if (desktop icons allowed by policy)</span>
    <span>{                                   </span>
        bind to the desktop folder
        enumerate the icons
        add them to the screen
        Report(DesktopIconsReady);
    <span>}                                   </span>
}
</pre>
<p>Oops, the scope of the “if” block extended past the report call, so if the policy is enabled, the icons are never reported as ready, and the logon system stays on the Welcome screen for the full 30 seconds.</p>
<p>Note that in both of these cases, it’s not that the logon is extended by 30 seconds. Rather, the Welcome screen stays on for the full 30 seconds rather than the actual time it took for all systems to report ready (which could be 5 seconds, or it could be 25 seconds, depending on your system’s performance).</p>
<p>If you look at the timestamps on the articles, you can see that the problem was fixed in November 2009, just a few months after Windows 7 was released in July 2009.</p>
<p>¹ Originally, I avoided bitmap backgrounds because they took up a lot of memory, and when you had only 4 or 8 megabytes of memory, eating three quarters of a megabyte of memory just for wallpaper was not a good return on investment.</p>
<p>Also, I tend to stick with default configurations because it makes bug filing easier. If the repro instructions are “install a system from scratch, then perform these steps”, you’re more likely to get traction than if you say “install a system from scratch, change these 50 settings from their defaults, and then perform these additional steps.” It’s much easier to justify a bug fix that affects the default configuration than a bug fix that requires that the user have changed settings from the default, particularly if those settings are obscure.</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><div><p><img src="https://devblogs.microsoft.com/oldnewthing/wp-content/uploads/sites/38/2019/02/RaymondChen_5in-150x150.jpg" alt="Raymond Chen"></p></div><p>Raymond has been involved in the evolution of Windows for more than 30 years. In 2003, he began a Web site known as The Old New Thing which has grown in popularity far beyond his wildest imagination, a development which still gives him the heebie-jeebies. The Web site spawned a book, coincidentally also titled The Old New Thing (Addison Wesley 2007). He occasionally appears on the Windows Dev Docs Twitter account to tell stories which convey no useful information.</p></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 12-bit rainbow palette (270 pts)]]></title>
            <link>https://iamkate.com/data/12-bit-rainbow/</link>
            <guid>43827108</guid>
            <pubDate>Mon, 28 Apr 2025 23:12:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iamkate.com/data/12-bit-rainbow/">https://iamkate.com/data/12-bit-rainbow/</a>, See on <a href="https://news.ycombinator.com/item?id=43827108">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>
        I designed the 12-bit rainbow palette for use on <a href="https://grid.iamkate.com/">National Grid: Live</a>. It consists of twelve colours chosen with consideration for how we perceive luminance, chroma, and hue:
      </p>
      <figure>
        <svg viewBox="0 0 480 40" width="480" height="40">
          <rect fill="#817" x="0" y="0" width="40" height="40"></rect>
          <rect fill="#a36" x="40" y="0" width="40" height="40"></rect>
          <rect fill="#c66" x="80" y="0" width="40" height="40"></rect>
          <rect fill="#e94" x="120" y="0" width="40" height="40"></rect>
          <rect fill="#ed0" x="160" y="0" width="40" height="40"></rect>
          <rect fill="#9d5" x="200" y="0" width="40" height="40"></rect>
          <rect fill="#4d8" x="240" y="0" width="40" height="40"></rect>
          <rect fill="#2cb" x="280" y="0" width="40" height="40"></rect>
          <rect fill="#0bc" x="320" y="0" width="40" height="40"></rect>
          <rect fill="#09c" x="360" y="0" width="40" height="40"></rect>
          <rect fill="#36b" x="400" y="0" width="40" height="40"></rect>
          <rect fill="#639" x="440" y="0" width="40" height="40"></rect>
        </svg>
      </figure>
      <p>
        The palette uses a 12-bit colour depth, so each colour requires only four characters when specified as a hexadecimal colour code in a <abbr>CSS</abbr> or <abbr>SVG</abbr> file:
      </p>
      <div id="palette">
        <p><span>#817</span></p>
        <p><span>#a35</span></p>
        <p><span>#c66</span></p>
        <p><span>#e94</span></p>
        <p><span>#ed0</span></p>
        <p><span>#9d5</span></p>
        <p><span>#4d8</span></p>
        <p><span>#2cb</span></p>
        <p><span>#0bc</span></p>
        <p><span>#09c</span></p>
        <p><span>#36b</span></p>
        <p><span>#639</span></p>
      </div>
      <h2>
        Designing the palette
      </h2>
      <p>
        Computers define colours in terms of red, green, and blue components, which are treated equally. However, we perceive these components as having differing luminance: compared to a pure red, a pure green looks much brighter and a pure blue looks much darker. As a result, a simple <abbr>RGB</abbr> rainbow palette has large changes in luminance between neighbouring colours. This can be seen by converting colours to greys of equal perceived luminance:
      </p>
      <figure>
        <svg viewBox="0 0 480 80" width="480" height="80">
          <rect fill="#ff00ff" x="0" y="0" width="40" height="40"></rect>
          <rect fill="#ff0080" x="40" y="0" width="40" height="40"></rect>
          <rect fill="#ff0000" x="80" y="0" width="40" height="40"></rect>
          <rect fill="#ff8000" x="120" y="0" width="40" height="40"></rect>
          <rect fill="#ffff00" x="160" y="0" width="40" height="40"></rect>
          <rect fill="#80ff00" x="200" y="0" width="40" height="40"></rect>
          <rect fill="#00ff00" x="240" y="0" width="40" height="40"></rect>
          <rect fill="#00ff80" x="280" y="0" width="40" height="40"></rect>
          <rect fill="#00ffff" x="320" y="0" width="40" height="40"></rect>
          <rect fill="#0080ff" x="360" y="0" width="40" height="40"></rect>
          <rect fill="#0000ff" x="400" y="0" width="40" height="40"></rect>
          <rect fill="#8000ff" x="440" y="0" width="40" height="40"></rect>
          <rect fill="#696969" x="0" y="40" width="40" height="40"></rect>
          <rect fill="#5b5b5b" x="40" y="40" width="40" height="40"></rect>
          <rect fill="#4c4c4c" x="80" y="40" width="40" height="40"></rect>
          <rect fill="#979797" x="120" y="40" width="40" height="40"></rect>
          <rect fill="#e2e2e2" x="160" y="40" width="40" height="40"></rect>
          <rect fill="#bcbcbc" x="200" y="40" width="40" height="40"></rect>
          <rect fill="#969696" x="240" y="40" width="40" height="40"></rect>
          <rect fill="#a4a4a4" x="280" y="40" width="40" height="40"></rect>
          <rect fill="#b3b3b3" x="320" y="40" width="40" height="40"></rect>
          <rect fill="#686868" x="360" y="40" width="40" height="40"></rect>
          <rect fill="#1d1d1d" x="400" y="40" width="40" height="40"></rect>
          <rect fill="#434343" x="440" y="40" width="40" height="40"></rect>
        </svg>
      </figure>
      <p>
        The <abbr>LCH</abbr> colour space is an alternative to the <abbr>RGB</abbr> colour space that defines colours in terms of luminance, chroma, and hue components. These components are perceptually uniform, which means that a change by a particular numerical amount will be perceived similarly for any colour.
      </p>
      <p>
        An <abbr>LCH</abbr> rainbow colour palette can be created by choosing fixed chroma and luminance values and varying the hue. However, the resulting palette looks unpleasant because yellow is darkened to brown, red is lightened to pink, and blue becomes very pale.
      </p>
      <p>
        A better approach is to allow the luminance to vary, but in a controlled way. Yellow is given the highest luminance, as it only looks yellow when bright. After choosing two other colours — a red and a blue in this case — the luminance can then be calculated for the other hues.
      </p>
      <p>
        Using a 12-bit colour depth limits the available colours, so slight changes to luminance, chroma, and hue must be made, but these are small enough not to be noticeable. The resulting palette has evenly-spaced hues, only small variations in chroma, and smoothly increasing and decreasing luminance:
      </p>
      <figure>
        <svg viewBox="0 0 480 80" width="480" height="80">
          <rect fill="#817" x="0" y="0" width="40" height="40"></rect>
          <rect fill="#a36" x="40" y="0" width="40" height="40"></rect>
          <rect fill="#c66" x="80" y="0" width="40" height="40"></rect>
          <rect fill="#e94" x="120" y="0" width="40" height="40"></rect>
          <rect fill="#ed0" x="160" y="0" width="40" height="40"></rect>
          <rect fill="#9d5" x="200" y="0" width="40" height="40"></rect>
          <rect fill="#4d8" x="240" y="0" width="40" height="40"></rect>
          <rect fill="#2cb" x="280" y="0" width="40" height="40"></rect>
          <rect fill="#0bc" x="320" y="0" width="40" height="40"></rect>
          <rect fill="#09c" x="360" y="0" width="40" height="40"></rect>
          <rect fill="#36b" x="400" y="0" width="40" height="40"></rect>
          <rect fill="#639" x="440" y="0" width="40" height="40"></rect>
          <rect fill="#404040" x="0" y="40" width="40" height="40"></rect>
          <rect fill="#5c5c5c" x="40" y="40" width="40" height="40"></rect>
          <rect fill="#848484" x="80" y="40" width="40" height="40"></rect>
          <rect fill="#a9a9a9" x="120" y="40" width="40" height="40"></rect>
          <rect fill="#c9c9c9" x="160" y="40" width="40" height="40"></rect>
          <rect fill="#b9b9b9" x="200" y="40" width="40" height="40"></rect>
          <rect fill="#a6a6a6" x="240" y="40" width="40" height="40"></rect>
          <rect fill="#979797" x="280" y="40" width="40" height="40"></rect>
          <rect fill="#858585" x="320" y="40" width="40" height="40"></rect>
          <rect fill="#717171" x="360" y="40" width="40" height="40"></rect>
          <rect fill="#606060" x="400" y="40" width="40" height="40"></rect>
          <rect fill="#4e4e4e" x="440" y="40" width="40" height="40"></rect>
        </svg>
      </figure>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The One-Person Framework in Practice (252 pts)]]></title>
            <link>https://link.mail.beehiiv.com/ss/c/u001.5SRwDQ9qxPQW8vmD5Do73b3R4eTCi2vXqPyztEk6wMFC9_fqEAcDVx6xEJ96T4BSMXrPS7z5exEBSTF4pF48z8SqJkJnkAwMUW9LtYdd8lWmvkDinT92nsk5HmXOHdWgLsysm9FMGrqmu7dnG57cXpga8ZOe8X0IV8pyeC3AswdRMaitfT307y7naP-_6W5CiolKhXCKrEndMGCW2PftFUu9ieYOxpVJ_fhu82gAh-4/4g1/wA_MG-I5SVCyR3KY66oEaQ/h30/h001.kLDFZMgisudi21zmTPbd_O8U7X98d4UxYqZjQTb_D7o</link>
            <guid>43826584</guid>
            <pubDate>Mon, 28 Apr 2025 21:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://link.mail.beehiiv.com/ss/c/u001.5SRwDQ9qxPQW8vmD5Do73b3R4eTCi2vXqPyztEk6wMFC9_fqEAcDVx6xEJ96T4BSMXrPS7z5exEBSTF4pF48z8SqJkJnkAwMUW9LtYdd8lWmvkDinT92nsk5HmXOHdWgLsysm9FMGrqmu7dnG57cXpga8ZOe8X0IV8pyeC3AswdRMaitfT307y7naP-_6W5CiolKhXCKrEndMGCW2PftFUu9ieYOxpVJ_fhu82gAh-4/4g1/wA_MG-I5SVCyR3KY66oEaQ/h30/h001.kLDFZMgisudi21zmTPbd_O8U7X98d4UxYqZjQTb_D7o">https://link.mail.beehiiv.com/ss/c/u001.5SRwDQ9qxPQW8vmD5Do73b3R4eTCi2vXqPyztEk6wMFC9_fqEAcDVx6xEJ96T4BSMXrPS7z5exEBSTF4pF48z8SqJkJnkAwMUW9LtYdd8lWmvkDinT92nsk5HmXOHdWgLsysm9FMGrqmu7dnG57cXpga8ZOe8X0IV8pyeC3AswdRMaitfT307y7naP-_6W5CiolKhXCKrEndMGCW2PftFUu9ieYOxpVJ_fhu82gAh-4/4g1/wA_MG-I5SVCyR3KY66oEaQ/h30/h001.kLDFZMgisudi21zmTPbd_O8U7X98d4UxYqZjQTb_D7o</a>, See on <a href="https://news.ycombinator.com/item?id=43826584">Hacker News</a></p>
Couldn't get https://link.mail.beehiiv.com/ss/c/u001.5SRwDQ9qxPQW8vmD5Do73b3R4eTCi2vXqPyztEk6wMFC9_fqEAcDVx6xEJ96T4BSMXrPS7z5exEBSTF4pF48z8SqJkJnkAwMUW9LtYdd8lWmvkDinT92nsk5HmXOHdWgLsysm9FMGrqmu7dnG57cXpga8ZOe8X0IV8pyeC3AswdRMaitfT307y7naP-_6W5CiolKhXCKrEndMGCW2PftFUu9ieYOxpVJ_fhu82gAh-4/4g1/wA_MG-I5SVCyR3KY66oEaQ/h30/h001.kLDFZMgisudi21zmTPbd_O8U7X98d4UxYqZjQTb_D7o: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3: Think deeper, act faster (691 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwen3/</link>
            <guid>43825900</guid>
            <pubDate>Mon, 28 Apr 2025 20:44:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwen3/">https://qwenlm.github.io/blog/qwen3/</a>, See on <a href="https://news.ycombinator.com/item?id=43825900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-banner.png" alt="Qwen3 Main Image" width="100%"></figure><p><a href="https://chat.qwen.ai/" target="_blank">QWEN CHAT</a>
<a href="https://github.com/QwenLM/Qwen3" target="_blank">GitHub</a>
<a href="https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f" target="_blank">Hugging Face</a>
<a href="https://modelscope.cn/collections/Qwen3-9743180bdc6b48" target="_blank">ModelScope</a>
<a href="https://www.kaggle.com/models/qwen-lm/qwen-3" target="_blank">Kaggle</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen3-Demo" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><h2 id="introduction">Introduction</h2><p>Today, we are excited to announce the release of <strong>Qwen3</strong>, the latest addition to the Qwen family of large language models. Our flagship model, <strong>Qwen3-235B-A22B</strong>, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, <strong>Qwen3-30B-A3B</strong>, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3/qwen3-235a22.jpg" width="100%"></figure><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3/qwen3-30a3.jpg" width="100%"></figure><p>We are open-weighting two MoE models: <strong>Qwen3-235B-A22B</strong>, a large model with 235 billion total parameters and 22 billion activated parameters, and <strong>Qwen3-30B-A3B</strong>, a smaller MoE model with 30 billion total parameters and 3 billion activated parameters. Additionally, six dense models are also open-weighted, including <strong>Qwen3-32B</strong>, <strong>Qwen3-14B</strong>, <strong>Qwen3-8B</strong>, <strong>Qwen3-4B</strong>, <strong>Qwen3-1.7B</strong>, and <strong>Qwen3-0.6B</strong>, under Apache 2.0 license.</p><table><thead><tr><th>Models</th><th>Layers</th><th>Heads (Q / KV)</th><th>Tie Embedding</th><th>Context Length</th></tr></thead><tbody><tr><td>Qwen3-0.6B</td><td>28</td><td>16 / 8</td><td>Yes</td><td>32K</td></tr><tr><td>Qwen3-1.7B</td><td>28</td><td>16 / 8</td><td>Yes</td><td>32K</td></tr><tr><td>Qwen3-4B</td><td>36</td><td>32 / 8</td><td>Yes</td><td>32K</td></tr><tr><td>Qwen3-8B</td><td>36</td><td>32 / 8</td><td>No</td><td>128K</td></tr><tr><td>Qwen3-14B</td><td>40</td><td>40 / 8</td><td>No</td><td>128K</td></tr><tr><td>Qwen3-32B</td><td>64</td><td>64 / 8</td><td>No</td><td>128K</td></tr></tbody></table><table><thead><tr><th>Models</th><th>Layers</th><th>Heads (Q / KV)</th><th># Experts (Total / Activated)</th><th>Context Length</th></tr></thead><tbody><tr><td>Qwen3-30B-A3B</td><td>48</td><td>32 / 4</td><td>128 / 8</td><td>128K</td></tr><tr><td>Qwen3-235B-A22B</td><td>94</td><td>64 / 4</td><td>128 / 8</td><td>128K</td></tr></tbody></table><p>The post-trained models, such as <strong>Qwen3-30B-A3B</strong>, along with their pre-trained counterparts (e.g., <strong>Qwen3-30B-A3B-Base</strong>), are now available on platforms like <strong>Hugging Face</strong>, <strong>ModelScope</strong>, and <strong>Kaggle</strong>. For deployment, we recommend using frameworks like <strong>SGLang</strong> and <strong>vLLM</strong>. For local usage, tools such as <strong>Ollama</strong>, <strong>LMStudio</strong>, <strong>MLX</strong>, <strong>llama.cpp</strong>, and <strong>KTransformers</strong> are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments.</p><p>We believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models.</p><p>Feel free to try Qwen3 out in Qwen Chat Web (<a href="https://chat.qwen.ai/">chat.qwen.ai</a>) and mobile APP!</p><h2 id="key-features">Key Features</h2><ul><li><strong>Hybrid Thinking Modes</strong></li></ul><p>Qwen3 models introduce a hybrid approach to problem-solving. They support two modes:</p><ol><li>Thinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer. This is ideal for complex problems that require deeper thought.</li><li>Non-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions where speed is more important than depth.</li></ol><p>This flexibility allows users to control how much “thinking” the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay. Crucially, the integration of these two modes greatly enhances the model’s ability to implement stable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth performance improvements that are directly correlated with the computational reasoning budget allocated. This design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance between cost efficiency and inference quality.</p><figure><img src="https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen3/thinking_budget.png" width="100%"></figure><ul><li><strong>Multilingual Support</strong></li></ul><p>Qwen3 models are supporting <strong>119 languages and dialects</strong>. This extensive multilingual capability opens up new possibilities for international applications, enabling users worldwide to benefit from the power of these models.</p><table><thead><tr><th>Language Family</th><th>Languages &amp; Dialects</th></tr></thead><tbody><tr><td>Indo-European</td><td>English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian</td></tr><tr><td>Sino-Tibetan</td><td>Chinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese</td></tr><tr><td>Afro-Asiatic</td><td>Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta’izzi-Adeni, Tunisian), Hebrew, Maltese</td></tr><tr><td>Austronesian</td><td>Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines)</td></tr><tr><td>Dravidian</td><td>Tamil, Telugu, Kannada, Malayalam</td></tr><tr><td>Turkic</td><td>Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar</td></tr><tr><td>Tai-Kadai</td><td>Thai, Lao</td></tr><tr><td>Uralic</td><td>Finnish, Estonian, Hungarian</td></tr><tr><td>Austroasiatic</td><td>Vietnamese, Khmer</td></tr><tr><td>Other</td><td>Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili</td></tr></tbody></table><ul><li><strong>Improved Agentic Capabilities</strong></li></ul><p>We have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment.</p><h2 id="pre-training">Pre-training</h2><p>In terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.</p><p>The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-base.jpg" width="100%"></figure><p>Due to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform larger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base models while using only 10% of the active parameters. This results in significant savings in both training and inference costs.</p><h2 id="post-training">Post-training</h2><figure><img src="https://qianwen-res.oss-accelerate.aliyuncs.com/assets/blog/qwen3/post-training.png" width="100%"></figure><p>To develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a four-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based reinforcement learning (RL), (3) thinking mode fusion, and (4) general RL.</p><p>In the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model’s exploration and exploitation capabilities.</p><p>In the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc.</p><h2 id="develop-with-qwen3">Develop with Qwen3</h2><p>Below is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>modelscope</span> <span>import</span> <span>AutoModelForCausalLM</span><span>,</span> <span>AutoTokenizer</span>
</span></span><span><span>
</span></span><span><span><span>model_name</span> <span>=</span> <span>"Qwen/Qwen3-30B-A3B"</span>
</span></span><span><span>
</span></span><span><span><span># load the tokenizer and the model</span>
</span></span><span><span><span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span>
</span></span><span><span><span>model</span> <span>=</span> <span>AutoModelForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span>
</span></span><span><span>    <span>model_name</span><span>,</span>
</span></span><span><span>    <span>torch_dtype</span><span>=</span><span>"auto"</span><span>,</span>
</span></span><span><span>    <span>device_map</span><span>=</span><span>"auto"</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># prepare the model input</span>
</span></span><span><span><span>prompt</span> <span>=</span> <span>"Give me a short introduction to large language model."</span>
</span></span><span><span><span>messages</span> <span>=</span> <span>[</span>
</span></span><span><span>    <span>{</span><span>"role"</span><span>:</span> <span>"user"</span><span>,</span> <span>"content"</span><span>:</span> <span>prompt</span><span>}</span>
</span></span><span><span><span>]</span>
</span></span><span><span><span>text</span> <span>=</span> <span>tokenizer</span><span>.</span><span>apply_chat_template</span><span>(</span>
</span></span><span><span>    <span>messages</span><span>,</span>
</span></span><span><span>    <span>tokenize</span><span>=</span><span>False</span><span>,</span>
</span></span><span><span>    <span>add_generation_prompt</span><span>=</span><span>True</span><span>,</span>
</span></span><span><span>    <span>enable_thinking</span><span>=</span><span>True</span> <span># Switch between thinking and non-thinking modes. Default is True.</span>
</span></span><span><span><span>)</span>
</span></span><span><span><span>model_inputs</span> <span>=</span> <span>tokenizer</span><span>([</span><span>text</span><span>],</span> <span>return_tensors</span><span>=</span><span>"pt"</span><span>)</span><span>.</span><span>to</span><span>(</span><span>model</span><span>.</span><span>device</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># conduct text completion</span>
</span></span><span><span><span>generated_ids</span> <span>=</span> <span>model</span><span>.</span><span>generate</span><span>(</span>
</span></span><span><span>    <span>**</span><span>model_inputs</span><span>,</span>
</span></span><span><span>    <span>max_new_tokens</span><span>=</span><span>32768</span>
</span></span><span><span><span>)</span>
</span></span><span><span><span>output_ids</span> <span>=</span> <span>generated_ids</span><span>[</span><span>0</span><span>][</span><span>len</span><span>(</span><span>model_inputs</span><span>.</span><span>input_ids</span><span>[</span><span>0</span><span>]):]</span><span>.</span><span>tolist</span><span>()</span> 
</span></span><span><span>
</span></span><span><span><span># parsing thinking content</span>
</span></span><span><span><span>try</span><span>:</span>
</span></span><span><span>    <span># rindex finding 151668 (&lt;/think&gt;)</span>
</span></span><span><span>    <span>index</span> <span>=</span> <span>len</span><span>(</span><span>output_ids</span><span>)</span> <span>-</span> <span>output_ids</span><span>[::</span><span>-</span><span>1</span><span>]</span><span>.</span><span>index</span><span>(</span><span>151668</span><span>)</span>
</span></span><span><span><span>except</span> <span>ValueError</span><span>:</span>
</span></span><span><span>    <span>index</span> <span>=</span> <span>0</span>
</span></span><span><span>
</span></span><span><span><span>thinking_content</span> <span>=</span> <span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>output_ids</span><span>[:</span><span>index</span><span>],</span> <span>skip_special_tokens</span><span>=</span><span>True</span><span>)</span><span>.</span><span>strip</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>)</span>
</span></span><span><span><span>content</span> <span>=</span> <span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>output_ids</span><span>[</span><span>index</span><span>:],</span> <span>skip_special_tokens</span><span>=</span><span>True</span><span>)</span><span>.</span><span>strip</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>print</span><span>(</span><span>"thinking content:"</span><span>,</span> <span>thinking_content</span><span>)</span>
</span></span><span><span><span>print</span><span>(</span><span>"content:"</span><span>,</span> <span>content</span><span>)</span>
</span></span></code></pre></div><p>To disable thinking, you just need to make changes to the argument <code>enable_thinking</code> like the following:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>text</span> <span>=</span> <span>tokenizer</span><span>.</span><span>apply_chat_template</span><span>(</span>
</span></span><span><span>    <span>messages</span><span>,</span>
</span></span><span><span>    <span>tokenize</span><span>=</span><span>False</span><span>,</span>
</span></span><span><span>    <span>add_generation_prompt</span><span>=</span><span>True</span><span>,</span>
</span></span><span><span>    <span>enable_thinking</span><span>=</span><span>False</span>  <span># True is the default value for enable_thinking.</span>
</span></span><span><span><span>)</span>
</span></span></code></pre></div><p>For deployment, you can use <code>sglang&gt;=0.4.6.post1</code> or <code>vllm&gt;=0.8.4</code> to create an OpenAI-compatible API endpoint:</p><ul><li><p>SGLang:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3
</span></span></code></pre></div></li><li><p>vLLM:</p><div><pre tabindex="0"><code data-lang="shell"><span><span>vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1
</span></span></code></pre></div></li></ul><p>If you use it for local development, you can use ollama by running a simple command <code>ollama run qwen3:30b-a3b</code> to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally.</p><h3 id="advanced-usages">Advanced Usages</h3><p>We provide a soft switch mechanism that allows users to dynamically control the model’s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model’s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.</p><p>Here is an example of a multi-turn conversation:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span><span>,</span> <span>AutoTokenizer</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>QwenChatbot</span><span>:</span>
</span></span><span><span>    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>model_name</span><span>=</span><span>"Qwen/Qwen3-30B-A3B"</span><span>):</span>
</span></span><span><span>        <span>self</span><span>.</span><span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>model</span> <span>=</span> <span>AutoModelForCausalLM</span><span>.</span><span>from_pretrained</span><span>(</span><span>model_name</span><span>)</span>
</span></span><span><span>        <span>self</span><span>.</span><span>history</span> <span>=</span> <span>[]</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>generate_response</span><span>(</span><span>self</span><span>,</span> <span>user_input</span><span>):</span>
</span></span><span><span>        <span>messages</span> <span>=</span> <span>self</span><span>.</span><span>history</span> <span>+</span> <span>[{</span><span>"role"</span><span>:</span> <span>"user"</span><span>,</span> <span>"content"</span><span>:</span> <span>user_input</span><span>}]</span>
</span></span><span><span>
</span></span><span><span>        <span>text</span> <span>=</span> <span>self</span><span>.</span><span>tokenizer</span><span>.</span><span>apply_chat_template</span><span>(</span>
</span></span><span><span>            <span>messages</span><span>,</span>
</span></span><span><span>            <span>tokenize</span><span>=</span><span>False</span><span>,</span>
</span></span><span><span>            <span>add_generation_prompt</span><span>=</span><span>True</span>
</span></span><span><span>        <span>)</span>
</span></span><span><span>
</span></span><span><span>        <span>inputs</span> <span>=</span> <span>self</span><span>.</span><span>tokenizer</span><span>(</span><span>text</span><span>,</span> <span>return_tensors</span><span>=</span><span>"pt"</span><span>)</span>
</span></span><span><span>        <span>response_ids</span> <span>=</span> <span>self</span><span>.</span><span>model</span><span>.</span><span>generate</span><span>(</span><span>**</span><span>inputs</span><span>,</span> <span>max_new_tokens</span><span>=</span><span>32768</span><span>)[</span><span>0</span><span>][</span><span>len</span><span>(</span><span>inputs</span><span>.</span><span>input_ids</span><span>[</span><span>0</span><span>]):]</span><span>.</span><span>tolist</span><span>()</span>
</span></span><span><span>        <span>response</span> <span>=</span> <span>self</span><span>.</span><span>tokenizer</span><span>.</span><span>decode</span><span>(</span><span>response_ids</span><span>,</span> <span>skip_special_tokens</span><span>=</span><span>True</span><span>)</span>
</span></span><span><span>
</span></span><span><span>        <span># Update history</span>
</span></span><span><span>        <span>self</span><span>.</span><span>history</span><span>.</span><span>append</span><span>({</span><span>"role"</span><span>:</span> <span>"user"</span><span>,</span> <span>"content"</span><span>:</span> <span>user_input</span><span>})</span>
</span></span><span><span>        <span>self</span><span>.</span><span>history</span><span>.</span><span>append</span><span>({</span><span>"role"</span><span>:</span> <span>"assistant"</span><span>,</span> <span>"content"</span><span>:</span> <span>response</span><span>})</span>
</span></span><span><span>
</span></span><span><span>        <span>return</span> <span>response</span>
</span></span><span><span>
</span></span><span><span><span># Example Usage</span>
</span></span><span><span><span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
</span></span><span><span>    <span>chatbot</span> <span>=</span> <span>QwenChatbot</span><span>()</span>
</span></span><span><span>
</span></span><span><span>    <span># First input (without /think or /no_think tags, thinking mode is enabled by default)</span>
</span></span><span><span>    <span>user_input_1</span> <span>=</span> <span>"How many r's in strawberries?"</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"User: </span><span>{</span><span>user_input_1</span><span>}</span><span>"</span><span>)</span>
</span></span><span><span>    <span>response_1</span> <span>=</span> <span>chatbot</span><span>.</span><span>generate_response</span><span>(</span><span>user_input_1</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"Bot: </span><span>{</span><span>response_1</span><span>}</span><span>"</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>"----------------------"</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span># Second input with /no_think</span>
</span></span><span><span>    <span>user_input_2</span> <span>=</span> <span>"Then, how many r's in blueberries? /no_think"</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"User: </span><span>{</span><span>user_input_2</span><span>}</span><span>"</span><span>)</span>
</span></span><span><span>    <span>response_2</span> <span>=</span> <span>chatbot</span><span>.</span><span>generate_response</span><span>(</span><span>user_input_2</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"Bot: </span><span>{</span><span>response_2</span><span>}</span><span>"</span><span>)</span> 
</span></span><span><span>    <span>print</span><span>(</span><span>"----------------------"</span><span>)</span>
</span></span><span><span>
</span></span><span><span>    <span># Third input with /think</span>
</span></span><span><span>    <span>user_input_3</span> <span>=</span> <span>"Really? /think"</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"User: </span><span>{</span><span>user_input_3</span><span>}</span><span>"</span><span>)</span>
</span></span><span><span>    <span>response_3</span> <span>=</span> <span>chatbot</span><span>.</span><span>generate_response</span><span>(</span><span>user_input_3</span><span>)</span>
</span></span><span><span>    <span>print</span><span>(</span><span>f</span><span>"Bot: </span><span>{</span><span>response_3</span><span>}</span><span>"</span><span>)</span>
</span></span></code></pre></div><h3 id="agentic-usages">Agentic Usages</h3><p>Qwen3 excels in tool calling capabilities. We recommend using <a href="https://github.com/QwenLM/Qwen-Agent">Qwen-Agent</a> to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.</p><p>To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>qwen_agent.agents</span> <span>import</span> <span>Assistant</span>
</span></span><span><span>
</span></span><span><span><span># Define LLM</span>
</span></span><span><span><span>llm_cfg</span> <span>=</span> <span>{</span>
</span></span><span><span>    <span>'model'</span><span>:</span> <span>'Qwen3-30B-A3B'</span><span>,</span>
</span></span><span><span>
</span></span><span><span>    <span># Use the endpoint provided by Alibaba Model Studio:</span>
</span></span><span><span>    <span># 'model_type': 'qwen_dashscope',</span>
</span></span><span><span>    <span># 'api_key': os.getenv('DASHSCOPE_API_KEY'),</span>
</span></span><span><span>
</span></span><span><span>    <span># Use a custom endpoint compatible with OpenAI API:</span>
</span></span><span><span>    <span>'model_server'</span><span>:</span> <span>'http://localhost:8000/v1'</span><span>,</span>  <span># api_base</span>
</span></span><span><span>    <span>'api_key'</span><span>:</span> <span>'EMPTY'</span><span>,</span>
</span></span><span><span>
</span></span><span><span>    <span># Other parameters:</span>
</span></span><span><span>    <span># 'generate_cfg': {</span>
</span></span><span><span>    <span>#         # Add: When the response content is `&lt;think&gt;this is the thought&lt;/think&gt;this is the answer;</span>
</span></span><span><span>    <span>#         # Do not add: When the response has been separated by reasoning_content and content.</span>
</span></span><span><span>    <span>#         'thought_in_content': True,</span>
</span></span><span><span>    <span>#     },</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># Define Tools</span>
</span></span><span><span><span>tools</span> <span>=</span> <span>[</span>
</span></span><span><span>    <span>{</span><span>'mcpServers'</span><span>:</span> <span>{</span>  <span># You can specify the MCP configuration file</span>
</span></span><span><span>            <span>'time'</span><span>:</span> <span>{</span>
</span></span><span><span>                <span>'command'</span><span>:</span> <span>'uvx'</span><span>,</span>
</span></span><span><span>                <span>'args'</span><span>:</span> <span>[</span><span>'mcp-server-time'</span><span>,</span> <span>'--local-timezone=Asia/Shanghai'</span><span>]</span>
</span></span><span><span>            <span>},</span>
</span></span><span><span>            <span>"fetch"</span><span>:</span> <span>{</span>
</span></span><span><span>                <span>"command"</span><span>:</span> <span>"uvx"</span><span>,</span>
</span></span><span><span>                <span>"args"</span><span>:</span> <span>[</span><span>"mcp-server-fetch"</span><span>]</span>
</span></span><span><span>            <span>}</span>
</span></span><span><span>        <span>}</span>
</span></span><span><span>    <span>},</span>
</span></span><span><span>  <span>'code_interpreter'</span><span>,</span>  <span># Built-in tools</span>
</span></span><span><span><span>]</span>
</span></span><span><span>
</span></span><span><span><span># Define Agent</span>
</span></span><span><span><span>bot</span> <span>=</span> <span>Assistant</span><span>(</span><span>llm</span><span>=</span><span>llm_cfg</span><span>,</span> <span>function_list</span><span>=</span><span>tools</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span># Streaming generation</span>
</span></span><span><span><span>messages</span> <span>=</span> <span>[{</span><span>'role'</span><span>:</span> <span>'user'</span><span>,</span> <span>'content'</span><span>:</span> <span>'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'</span><span>}]</span>
</span></span><span><span><span>for</span> <span>responses</span> <span>in</span> <span>bot</span><span>.</span><span>run</span><span>(</span><span>messages</span><span>=</span><span>messages</span><span>):</span>
</span></span><span><span>    <span>pass</span>
</span></span><span><span><span>print</span><span>(</span><span>responses</span><span>)</span>
</span></span></code></pre></div><h2 id="friends-of-qwen">Friends of Qwen</h2><p>Thanks to the support of so many friends. Qwen is nothing without its friends! We welcome more people or organizations to join our community and help us become better!</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/qwen3-logo.png" width="100%"></figure><h2 id="future-work">Future Work</h2><p>Qwen3 represents a significant milestone in our journey toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). By scaling up both pretraining and reinforcement learning (RL), we have achieved higher levels of intelligence. We have seamlessly integrated thinking and non-thinking modes, offering users the flexibility to control the thinking budget. Additionally, we have expanded support for a wide range of languages, enhancing global accessibility.</p><p>Looking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone’s work and life.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One Million Chessboards (261 pts)]]></title>
            <link>https://eieio.games/blog/one-million-chessboards/</link>
            <guid>43825336</guid>
            <pubDate>Mon, 28 Apr 2025 19:52:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/blog/one-million-chessboards/">https://eieio.games/blog/one-million-chessboards/</a>, See on <a href="https://news.ycombinator.com/item?id=43825336">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><a href="https://eieio.games/blog/one-million-chessboards/"></a><div><p>a million chessboards that anyone can play on</p><p>Apr 28, 2025</p></div></div>
<p>I made a website. It’s called <a href="https://onemillionchessboards.com/">One Million Chessboards</a>. It has one million chessboards on it.</p>
<p>Moving a piece moves it for everyone, instantly. There are no turns. You can move between boards.</p>
<div><video controls="" playsinline="" poster="https://eieio.games/images/one-million-chessboards/gameplay-firstframe.png" width="2136" height="1708" preload="metadata" alt="Gameplay from One Million Chessboards. The player moves a queen around on a grid of a million boards as pieces move around him."><p>Loading...</p></video><p>moving some pieces</p></div>
<!-- -->
<h2 id="toc:what">What</h2>
<p>Well last year I made this game called <a href="https://eieio.games/blog/one-million-checkboxes">One Million Checkboxes</a>.</p>
<p>It was a pretty fun time! So I thought I’d do something like this again.</p>
<p>I worked really hard on this one. I hope you like it.</p>
<h2 id="toc:how">How</h2>
<p>This was the most technically challenging thing that I’ve worked on in a long time. I’m going to save a full technical writeup until I see how my decisions pan out, since I think there’s a decent chance I’ll need to make a lot of changes.</p>
<p>But I’ll summarize a few things for you.</p>
<ul>
<li>Unlike One Million Checkboxes, I designed this for scale</li>
<li>The game runs on a single server (!)</li>
<li>The board is stored fully in-memory; it’s a 2D array of 64 million uint64s</li>
<li>The backend is written in go. This is my first go project.</li>
<li>I use a single writer thread, tons of reader threads, and coordinate access to the board with a mutex</li>
<li>The frontend optimistically applies all moves you make immediately. It then builds up a dependency graph of the moves you’ve made, and backs them out if it receives a conflicting update before the server acks your move.</li>
<li>The server ships zstd-compressed protobufs to the clients over websockets for state snapshots (approximately a 100x100 square around the client), move and capture updates, and acks/rejections for moves</li>
<li>Clients are grouped into 50x50 “zones” and only receive moves for zones adjacent to their current zone</li>
<li>Clients fetch global data (game stats, the minimap, etc) by polling via GET; data is cached in Cloudflare with a low TTL so this is much cheaper than shipping it over every websocket</li>
</ul>
<p>That last part - optimistic move application with what games people sometimes call “rollback” - is about 1,600 lines of code that took me a ~7 days of fulltime work to write. I don’t remember the last time I wrestled with a problem that hard!</p>
<p>As of 8 PM, 8 hours after launch, players have made about 1.3 million moves and there are about 400 concurrent users most of the time. Load on my server is neglibible!</p>
<h2 id="toc:can-i-play">Can I play</h2>
<p>Yes! <a href="https://onemillionchessboards.com/">Play it here</a>.</p>
<p>I really hope you like this one. More updates to come :)</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Migrating Away from Rust (607 pts)]]></title>
            <link>https://deadmoney.gg/news/articles/migrating-away-from-rust</link>
            <guid>43824640</guid>
            <pubDate>Mon, 28 Apr 2025 18:47:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deadmoney.gg/news/articles/migrating-away-from-rust">https://deadmoney.gg/news/articles/migrating-away-from-rust</a>, See on <a href="https://news.ycombinator.com/item?id=43824640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When I started building <strong>Architect of Ruin </strong>in December 2023 I chose to build it in the <a href="https://bevyengine.org/" target="_blank">Bevy</a>&nbsp;game engine. My choice was motivated by a personal interest in <a href="https://www.rust-lang.org/" target="_blank">Rust</a>&nbsp;-- a language I derive a lot of joy in using. This was furthered by <a href="https://bevyengine.org/learn/quick-start/getting-started/ecs/" target="_blank">Bevy's ECS</a> model which I also find fun to work with and the openness of Bevy's community which I have a genuine appreciation for.</p><center><iframe width="500" height="282" src="https://www.youtube.com/embed/vCEbQXeGc3U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Bevy Rap by Tantan"></iframe></center><p>So, it came as a surprise that in&nbsp;January of 2025 we transitioned the game away from Rust and Bevy. I spent about six weeks rewriting the game entirely in C# and we have been using Unity for the past three months.</p><p>Switching engines is a classic project killer. Productivity can nosedive, regressions inevitably emerge, and every step forward seems to lead to three steps back. Not to mention that domain expertise built up in one language and engine doesn't cleanly transfer to a new language and engine.</p><p>But I bit the bullet and I want to explain why.</p><h3>The Bevy Journey</h3><p>A lot of good work was accomplished in Bevy. The tilemap, most of my approach to composing the scene, and a large amount of character and gameplay logic were implemented in Bevy. I learned about the guts of Spine and skeletal animation by tearing apart Rust <a href="https://github.com/jabuwu/rusty_spine" target="_blank">transpiles</a>&nbsp;of the spine runtime. I learned a lot about custom render pipelines by implementing my own rendering features in Bevy's render world. Bevy's pure ECS was a joy to work with and Rust's compile-time checks meant I could refactor large swathes of code quickly and with confidence.</p><p>The Bevy community was also an active source of inspiration - not just ideas about how to use the engine, but a positive community of builders and contributors. This community is very good at being excited about game development and being energetic about debate.</p><p>I had the opportunity to contribute features and fixes to a number of community crates, although most of those contributions were small and were focused on work that moved my own goals forward.</p><p>Despite these positive experiences and the progress made, practical challenges emerged as development continued.</p><h3>Emergent Problems</h3><p>I want to begin by stating that I anticipated many of these challenges before they manifested. I knew that using a game engine early in its development lifecycle would pose unique risks and costs. I considered those costs to be likely worthwhile and surmountable. My love of Rust and Bevy meant that I would be willing to bear some pain that other game developers might choose to avoid. I didn't walk blindly into these specific problems, but they bit harder than I was expecting.</p><p><strong>Collaboration </strong>- I started this project with my brother. While he's sharp and eager, he's new to coding. Onboarding him directly into game dev while simultaneously navigating Rust's unique aspects proved challenging. We found ourselves with a steeper learning curve that slowed his ability to contribute effectively to gameplay logic.</p><center></center><p><strong>Abstraction</strong> - While my initial motivation was the enjoyment of Rust, the project's bottleneck increasingly became the rapid iteration of higher-level gameplay mechanics. As the codebase grew, we found that translating gameplay ideas into code was less direct than we hoped. Rust's (powerful) low-level focus didn't always lend itself to a flexible high-level scripting style needed for rapid prototyping within our specific gameplay architecture. I found that my motivation to build and ship fun gameplay was stronger than my desire to build with Rust.</p><p>I had anticipated that this was going to be a thing, but I wasn't calibrated to the degree it would start to annoy me and slow the project down.</p><p><img src="https://manakeep.us-east-1.linodeobjects.com/users/647f40267263835eb33ad65d/2025-04-25/bevyskeleverbose.png" alt="Rust can be verbose."></p><center><em>(A relatively simple gameplay function signature.)</em></center><center><em></em></center><center><em></em></center><p><strong>Migration&nbsp;</strong>- Bevy is young and changes quickly. Each update brought with it incredible features, but also a substantial amount of API thrash. As the project grew in size, the burden of update migration also grew. Minor regressions were common in core Bevy systems (such as sprite rendering), and these led to moments of significant friction and unexpected debugging effort.</p><p>This came to a head on one specific day where I was frustrated with a sprite rendering issue that had emerged in a new release. Blake had run into the same problem at the same time and our shared frustration boiled over into a kind of table flip moment. He turned to me and said something along the lines of "this shouldn't happen, this kind of thing should just be solved" and that triggered the conversation that led to a re-evaluation.</p><p>The point isn't that specific sprite problem, but that because all systems in Bevy are open to tinkering and improvement, all systems were potentially subject to regressions.</p><p><strong>Learning </strong>- Over the past year my workflow has changed immensely, and I regularly use AI to learn new technologies, discuss methods and techniques, review code, etc. The maturity and vast amount of stable historical data for C# and the Unity API mean that tools like Gemini consistently provide highly relevant guidance. While Bevy and Rust evolve rapidly - which is exciting and motivating - the pace means AI knowledge lags behind, reducing the efficiency gains I have come to expect from AI assisted development. This could change with the introduction of more modern tool-enabled models, but I found it to be a distraction and an unexpected additional cost.</p><p><strong>Modding </strong>- Modding means a lot to me. <a href="https://web.archive.org/web/20160306093336/http://orangesmoothie.org/" target="_blank">I got my start in the industry as a modder</a>&nbsp;and I want my game to be highly moddable. Over time, as I learned more about how to realize this goal, I came to understand many inherent limitations in Rust and Bevy that would make the task more difficult. Lack of a clear solution to scripting and an unstable ABI (application binary interface) raised concerns. I am not an expert in this area, perhaps these are all easily surmounted. I can only say that I did not find a path (after much searching) that I felt confident trusting.</p><p>These factors combined - the desire for a smoother workflow across experience levels, the need for a high-level abstraction for gameplay, optimizing productivity, and modding - pointed towards a re-evaluation of the project's next phase.</p><h3>The Switch</h3><p>To be honest, I completely disregarded Unity when I started the project.</p><p>Some of this stemmed from unforced errors on the part of Unity. They had just gone through a crisis of pricing that culminated in the resignation of their CEO and they seemed out of touch with indie developers.&nbsp;I also made several assumptions. I felt sick of coding in the outdated form of C++ that pervades older game engines and assumed I'd feel similarly about C#. I figured that since Unreal doesn't offer much for 2D render pipelines that Unity wouldn't either. This led me to fail to give serious thought to using Unity in 2023.</p><p>In the first week of January of 2025, Blake and I decided to do a cost-benefit analysis. We wrote down all the options: Unreal, Unity, Godot, continuing in Bevy, or rolling our own. We wrote extensive pros and cons, emphasizing how each option fared by the criteria above: Collaboration, Abstraction, Migration, Learning, and Modding.</p><p>Having some experience with the other options, I decided I needed to understand Unity better. An afternoon's research led me to conclude that it seemed to score high on the pros over the cons.</p><p>We had a team meeting where I laid out the trade-offs. Ulrick pointed out that a bunch of unknowns, like particles, would just be solved in a packaged engine. Blake pointed out that if things went well, and a new engine meant faster gameplay development, we could end up ahead of schedule.</p><h3>10% for 90%</h3><p>The team decided to invest in an experiment. I would pick three core features and see how difficult they would be to implement in Unity. We would spend no more than 3 weeks on the task.  We would invest 10% of effort to see if we should invest the other 90% in a full port.</p><p><strong>Tilemap</strong> - I figured this one would be straightforward, since the basic logic is simple. It would require implementing custom shaders. We wouldn't be using the built in Unity Tilemap because our needs were specific and well-known to us. This was foundational to the game scene and I had a good mental model for how long it took me to write the first time in Rust.</p><p><strong>Characters </strong>- Our characters use Spine and have unique customization requirements and features. This gave me a lot of trouble in Rust, so I figured it would be a good point of comparison in C#.</p><p><strong>UI</strong> - I wanted UI to be easy to build, fast to iterate, and moddable. This was an area where we learned a lot in Rust and again had a good mental model for comparison. Some research led me to conclude that <a href="https://www.noesisengine.com/" target="_blank">Noesis</a>&nbsp;would be a good fit because of its emphasis on data-driven XAML and the fact that the WPF model is very well documented. Even if I didn't know WPF, I knew I could learn it quickly with AI assistance.</p><p>The first two tasks: Tilemap and Characters, were chosen because they were fundamental, but also because let me check my time-expectations against reality on an easy task and a hard task. This would allow me to project the workload of future tasks and the port more broadly. The UI task was chosen because our game is UI heavy and any significant speed improvement in iterating on UI would have compounding returns on future development.</p><p>We finished all three tasks in 3 days!</p><p>Commit 1 was on Jan 8th and the Tilemap was done the same day.&nbsp;</p><p><img src="https://manakeep.us-east-1.linodeobjects.com/users/647f40267263835eb33ad65d/2025-04-25/tasklog1.png" alt=""></p><p>While I implemented the tilemap, Blake wrote the camera system. This demonstrated a significant boost in his ability to contribute when the technical framework was more scrutable. It was also a huge boost to his confidence and contributed to a new feeling of momentum. I should point out that Blake had never written C# before.</p><p>I implemented the Tilemap shader in Unity Shader Graph, thinking this would be easier for Ulrick to play with. This wasn't the last Shader Graph I wrote, but ultimately, I decided that visual shader creation and iteration was too slow, and refactoring was much slower as well. I now write all shaders in HLSL.</p><p><img src="https://manakeep.us-east-1.linodeobjects.com/users/647f40267263835eb33ad65d/2025-04-25/tilemapshader.png" alt=""></p><p>On Jan 10th we had figured out the basics of building UI in Noesis. Blake wrote a few simple UI widgets and then built the main menu and I built the first part of the game HUD, the toolbar.</p><p>The work went far more smoothly than I expected, and nothing was left on the cutting room floor. The tilemap took a day, a basic panel in Noesis was an afternoon including hooking up the plugin. The rest of the time was on wiring up characters. To be clear, I didn't port the entire UI in an afternoon or implement our equipment system. What we did have were customizable character bodies, a fully ported tilemap, some basic menus and enough knowledge to make projections on how long the rest of the port would take.</p><p><img src="https://manakeep.us-east-1.linodeobjects.com/users/647f40267263835eb33ad65d/2025-04-25/initialport.png" alt="Very representative of the end of that first week."></p><p><em>This image is very representative of where we were at the end of the first week of porting, although this one is actually from a few days later after items had gone in.</em></p><p>At the end of the week, we convened to discuss what we had learned and made the decision to move ahead with the full port.</p><p>The following six weeks were dedicated to rewriting the remaining systems and content from the Bevy version into Unity/C#. The overall process largely validated the findings from our three-day test. Gameplay systems with the same number of features could be implemented with less verbosity.&nbsp;</p><p><img src="https://manakeep.us-east-1.linodeobjects.com/users/647f40267263835eb33ad65d/2025-04-25/longerconvo.png"></p><center><em>This conversation dates March 4, 2025.</em></center><p>Code size shrank substantially, massively improving maintainability. As far as I can tell, most of this savings was just in the elimination of ECS boilerplate.</p><p>Everything felt tighter and more straightforward. Update migration anxiety was gone and while it was replaced with "we gotta get this done" anxiety, that dissipated quickly as progress was constant.</p><h3>Life Since the Switch</h3><p>We've now been developing <strong>Architect of Ruin</strong> exclusively in Unity for the past three months. The shift has measurably improved our day-to-day development. Iteration feels faster, allowing ideas to flow into the game more easily. We've also been able to leverage ecosystem tools like the <a href="https://arongranberg.com/astar/" target="_blank">AStar Pathfinding Project</a>.</p><p>One area that isn't solved and which will likely cost us some pain is localization. In Rust, the <a href="https://github.com/projectfluent/fluent" target="_blank">Fluent</a>&nbsp;project is excellent and exactly what we needed -- I haven't yet found a comparable solution in Unity.</p><p>I plan to discuss specific elements of the game's implementation in Unity and the porting process in future posts. The goal of today's post was only to explain the reasoning that led us to our current position.</p><p>A few conclusions stand out.</p><p><strong>I failed to fairly evaluate my options at the start of the project. </strong>Rust is great and I love it, but I didn't give alternatives a fair shake. In particular, I didn't spend time examining the differences between Unreal and Unity more closely.</p><p><strong>Sometimes you have to burn time to earn time. </strong>I think we are way ahead of where we would have been had we stuck with Bevy. Our agility in implementing rendering features while also pushing gameplay forward is much higher.</p><p>Rust remains a language I deeply enjoy, and Bevy is an exciting engine with a fantastic community -- I have immense respect for both and may well use them again for different projects. For <strong>Architect of Ruin</strong>, however, the needs for accessible collaboration, rapid gameplay iteration, and leveraging a stable ecosystem pointed towards an alternative.</p><p>It was a difficult decision, one that felt counter to my instincts, but ultimately it put us in a much stronger position to realize our vision for the game.</p>
  
<p>
  <strong>
    Stay in the Loop&nbsp;🗡️
  </strong>
  If you’ve read this far and want to watch our sword-and-sorcery colony grow,
  <a href="https://deadmoney.gg/#block_6802c2b072638302caf714e7" target="_blank" rel="noopener">
    join the newsletter
  </a>
  to get dev updates&nbsp;&amp; first-look builds.
</p>
<center><video controls="controls" autoplay="autoplay" muted="muted" loop="loop" playsinline="playsinline" src="https://manakeep.us-east-1.linodeobjects.com/devlog/videos/2025/4/23/y379zowPZHtDvSKp/lutfix.mp4"></video></center></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reports of the death of California High-Speed Rail have been greatly exaggerated (195 pts)]]></title>
            <link>https://asteriskmag.com/issues/10/reports-of-the-death-of-california-high-speed-rail-have-been-greatly-exaggerated</link>
            <guid>43824544</guid>
            <pubDate>Mon, 28 Apr 2025 18:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/10/reports-of-the-death-of-california-high-speed-rail-have-been-greatly-exaggerated">https://asteriskmag.com/issues/10/reports-of-the-death-of-california-high-speed-rail-have-been-greatly-exaggerated</a>, See on <a href="https://news.ycombinator.com/item?id=43824544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If there is one subject liberals and conservatives can agree on, it might be their shared hatred for California High-Speed Rail. Today, the project is <a href="https://apnews.com/article/california-high-speed-rail-trump-investigation-5b4d6494a8cdd9a3fe3b8949bb5b1bba">under investigation</a> by the Trump administration and is facing a possible withdrawal of federal funds. Even California Democrats seem disinclined to put up a fight. In Ezra Klein and Derek Thompson’s <em>Abundance</em>, it is the example par excellence of blue states’ failure to build, a casualty (if it ever lived) of environmental proceduralism, prohibitive regulatory processes, a bloated bureaucracy, and general infrastructural incompetence. &nbsp;</p><p>All of these challenges are real. What critics miss is that many of them have already been overcome. What they ignore is the reason they exist in the first place. The story of CAHSR is not about a state trying and failing to overcome its own bureaucracy and broken political process. It’s about a state that barely tried.</p><p>In 2008, when the state originally put Prop 1A to voters to build a high-speed rail between Los Angeles and San Francisco, the expectation was that the $9.95 billion in state bonds voters approved would be matched or exceeded by federal funding (as is normally the case for highway projects), and perhaps further complemented by private funding (as with the Brightline West project between Las Vegas and Los Angeles). It passed with 52.6% of the vote.&nbsp;</p></div><div><p>It took four years to even begin work, but not solely because of permitting — release of the funds was delayed until 2012. And they were held up further by a series of court cases that prevented the state from selling the bonds allocated by 1A until it identified the source of the rest of the project’s funding. During this time, the project received around $3 billion in federal funds from the Obama-era American Recovery and Reinvestment Act of 2009, which came with requirements to match federal funds and to begin work in disadvantaged areas of the Central Valley.</p><p>The result was that work on the Central Valley segment’s initial 120 miles was delayed for several years. The first major construction contract, Construction Package 1, was <a href="https://www.railwaygazette.com/high-speed/california-high-speed-rail-authority-invites-interest-in-second-construction-package/38771.article">issued in June 2013</a>. In the meantime, then-Governor Jerry Brown and the legislature worked to grant CAHSR what would be its sole ongoing funding source. In June 2014, the project was awarded a 15% allocation of the revenue from sales in California's cap-and-trade carbon auctions, which in recent years have raised between $3 and $4.7 billion a year. In 2017, this allocation was extended until 2030. With this funding secure, contracts were finally issued for Construction Package 2-3 in 2015 and Construction Package 4 in 2016.&nbsp;</p><p>But the delays in issuing these contracts started a vicious cycle. At multiple points in the rail’s history, approvals and construction plans already started for utility reconstructions, interactions with and crossings of existing freight lines, grade separations, and so on were left incomplete. This led to pauses in work and further delays so that plans could catch up to the ground already broken. The lack of continuity between planning and construction — changes in contractors, personnel, and others with expertise — made necessary change orders that only drove the cost up higher.&nbsp;</p><p>Through Trump’s first term, the initial portion of bond funding, combined with funding from the cap-and-trade fund allocation, was enough to keep <em>some</em> progress moving forward in the Central Valley under rail authority CEO Brian Kelly. Concrete viaducts and grade separations began to rise as visible signs of the work in progress. However, there were strict limits on what could be accomplished because of the piecemeal disbursement of cash. The sum of these funds, the only source of persistent cash flow, was generally about a billion dollars a year. They also varied each year due to the fluctuation in the cap-and-trade auction system. As a result, yearly funds couldn’t be reliably planned against, nor could they be borrowed against. Thus, even when work was finally underway, progress was slower than hoped, as change orders resulting from delayed property acquisition and permitting interactions with freight railroads and local municipalities increased costs and complexity.&nbsp;</p><p>To put this into perspective: Brightline West, the high-speed rail project between Los Angeles and Las Vegas, is hoping to spend the $12 billion they have raised in just over four years. It has taken many times as long for California High-Speed Rail to have their first $12 billion <em>in hand</em>. &nbsp;</p><p>Gavin Newsom’s election as governor in 2018 introduced further problems for the project that continue to plague it today. “There simply isn’t a path to get from Sacramento to San Diego, let alone from San Francisco to LA,” he claimed. “I wish there were.” Instead, he directed the project to focus on the 172-mile stretch between Merced and Bakersfield, through which an eventual route from San Francisco to Los Angeles might run. Crucially, Newsom also cut funding that should have gone to the geological surveys needed for designing the tunnels required to punch through the Diablo range south of San Francisco and San Gabriel Mountains north of Los Angeles — thereby fulfilling his own prophecy.</p><p>Initially, Newsom also didn’t put political capital to work to push for more funding for the project. Indeed, for the first few years of his tenure, $4.1 billion in Prop 1A bond funds remained unallocated by the state legislature and thus unavailable for the project to use. It wasn’t until 2021, as debates about Biden-era federal infrastructure funding began, that Newsom finally began to pressure the legislature to release these funds to the project in hopes of attracting new federal matching funds. The legislature released the remaining bond funds in 2022, and that renewed state investment may indeed have played a role in the <a href="https://pelosi.house.gov/news/press-releases/pelosi-announces-landmark-3-billion-federal-investment-california-high-speed">award of $3.07 billion in federal infrastructure funds in the fall of 2023</a>.</p><p>With all this in mind, we can start to look at common criticisms of California High-Speed Rail in a different light. These critiques — maybe you’ve seen them on Twitter — generally glibly identify solutions or alternatives that are insufficient to address the rail’s real issues, and focus attention on the wrong problems.</p><p>&nbsp;One regular snipe is that it’s “easier to build rail in Morocco than in California.” This critique stems from the fact that the French national railroad company, SNCF, which participated early in the planning process before 1A passed, also helped design the Moroccan Al Boraq high-speed rail service. Such critics often note that the Al Boraq service is operational today and claim that the relative failure of the California High-Speed Rail “boondoggle” represents the political dysfunction of either California, the United States, or the West as a whole. This appears to be based entirely on one quote in the <em>New York Times</em>,<em> </em>from an SNCF project manager, that the company left for Morocco, “which was less politically dysfunctional.” In fact, SNCF has employees in 120 countries and has projects in Israel, Taiwan, and South Korea, among others.</p><p>This criticism also misunderstands one of the main challenges that CAHSR has faced. Al Boraq had full funding lined up before the project began. CAHSR did not. This led to delays that reduced support and encouraged critics, which starved it of funding commitments and thus led to further delays. California undermined CAHSR from the start.&nbsp;</p><p>Another common criticism, as laid out in <a href="https://benjaminschneider.substack.com/p/california-high-speed-rails-original">an article by Benjamin Schneider</a>, is that California High-Speed Rail is built in the wrong place, to the wrong standards, and with the wrong goals — and that’s why it failed. The argument goes like this: The currently planned alignment through the Central Valley makes building unnecessarily complicated. Because it goes through major population centers — Bakersfield, Merced, Fresno — it introduces the necessity of grade separations (bridges or tunnels built so that trains and vehicles pass over or under each other) and requires complex property acquisitions. The right place to build the rail alignment is a direct route between Los Angeles and San Francisco paralleling Interstate 5.&nbsp;</p><p>This was, in fact, the route proposed by SNCF prior to Prop 1A’s passing. The I-5 alignment would save mileage, reduce grade separations and utility relocations, and use property already under the control of the state’s department of transportation. These cities could then perhaps be connected with branch lines to the high-speed rail trunk, establishing a line through the Central Valley with a faster build out and lower cost compared with the more expensive “political” route directly through Central Valley cities and towns.</p><p>&nbsp;Although it has intuitive appeal, this proposal suffers from major issues. First, the I-5 route avoids every major population center in the Central Valley, bypassing more than a million people who would be unserved. The only way to connect them to the line would be through stub-end branch lines. Building these lines would add the same kinds of costs as the as-built line directly through the Central Valley cities while offering them worse service. Second, it’s quite possible the I-5 alignment would never have been popular enough to pass as a ballot proposition. The Central Valley population areas only narrowly voted in favor of 1A. Had the original proposal outlined in the ballot initiative offered a different route from which Central Valley residents would not benefit, it’s possible it would not have passed at all.<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
&nbsp;</p><p>Third, much has been made of the fact that construction has begun on the middle section of the alignment rather than in Los Angeles or San Francisco. This is, in fact, standard practice in building high-speed rail around the world. The route alignment as selected means that even if the Central Valley portion of the rail is all that operates within the next decade, it will still see service, as it will replace the existing Amtrak line that already carries a million riders a year between the Central Valley population centers. Only if the Central Valley alignment had been selected would CAHSR actually the “train to nowhere” that critics deride it as.&nbsp;</p><p>&nbsp;Perhaps most importantly, critics appear to forget that the Central Valley alignment, political or not, overambitious or not, <em>is nearly built</em>. The property is acquired and cleared, embankments are under construction, and many viaducts and bridges have been completed. Abandoning it in the current state and switching to I-5 would almost certainly cost more than finishing the current alignment.</p><p>And this alignment isn’t the real problem, anyway. The true cost driver for CAHSR isn’t the difference between I-5 and the Central Valley, but access <em>into</em> Los Angeles and San Francisco. There, the challenge is not permits or politics but geology. While rail services like Metrolink’s Antelope Valley commuter line in Los Angeles and the Altamont Corridor Express in San Francisco do already cross the mountains, these are slow trains — necessarily so, because of the tortuous route demanded by the terrain. According to the 2024 California High-Speed Rail business plan, the total cost of the three major required tunnels is expected to make up approximately half of the total project cost. The Central Valley’s 172 miles under construction and advanced planning are estimated to cost up to $33 billion. The total to get from the Central Valley into the Los Angeles region is expected to be $34 billion, while at the north end of the Central Valley, the link from the Central Valley to the San Francisco Bay region is expected to cost $20 billion, including the tunnel and improvements along existing rail corridors from San Jose to Gilroy. These costs can’t be blamed on Californian political dysfunction: These are tunnels on the scale of the Gotthard Base Tunnel through the Alps in Switzerland, which had a similar cost-per-mile.&nbsp;</p><p>A second line of criticism comes from the abundance movement. <a href="https://archive.ph/pYBxj">Critics like Ezra Klein emphasize</a> the problems created by environmental permitting and cooperating with utilities, and other existing interests like the freight railroads and (mostly Republican) landowners. All of these are real problems, and each has been a factor in delaying CAHSR. Environmental review has taken over a decade. Breaking ground sometimes uncovered new lawsuits. The entire project has been negotiated and renegotiated, inflating prices at each step. But every single one of these problems is also faced by other infrastructure projects — new bridges and highways in particular — that manage to proceed in spite of regulatory difficulties. &nbsp;</p><p>One major reason that highway infrastructure succeeds where CAHSR has failed is that such projects are routine. Highways have assured funding from federal and state sources. They also have significant political support from state and federal officials. More to the point, there is an industry of professionals experienced in both building roads and navigating the red tape necessary to make doing so possible. The equivalent network for American high-speed rail does exist — yet.&nbsp; This is part of the reason that high-speed rail costs many times as much in the United States as it does in European countries. Unless we actually commit to building <em>and finishing</em> high-speed rail, that network will never form.</p><p>Many commentators have suggested the project cut its losses and bow out. But one piece missing from the discourse is an accurate understanding of what the project has cost to date and what it needs to be completed. This problem dates to the earliest days of the project. If the CAHSR has an original sin, it is that the bond issue that went before voters didn’t ask for enough money. Early business plans in 2008 expected a total budget of $33 billion to be sufficient, approximately $50 billion today with inflation. Because the expectation was that federal government or private investment would bear much of this cost, the bond issue requested less than a third of the amount needed even in these early estimates.<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
&nbsp;</p><p>It was only in 2011, once the project was able to finalize more of the route plan in detail, that the projected price rose to $65 billion. Inflation adjusted, that is $94 billion today — close to the base-cost estimate of $106 billion from the 2024 business plan that so many have critiqued. As many critics have rightfully and fairly noted, this is a higher cost-per-mile of track than in many countries that routinely build high-speed rail. But America, of course, does not routinely build high-speed rail, and the project remains only slightly more expensive than it was forecasted to be 13 years ago. Most important, to date, CAHSR has spent only about $15 billion, which has limited the ability to proceed with extensive planning beyond the Central Valley. It has also stopped the award of any new heavy construction contracts since the initial 119 miles of Construction Packages 1 through 4. Work has begun on the next 50 miles of civil construction to bring tracks to Merced, where they will connect with ACE and San Joaquins train services to San Francisco and Sacramento, and to Bakersfield, where connections are available via Amtrak Thruway bus service into Los Angeles. Contracts are also intended to be issued this year for the tracks, overhead electrical system, and trains to run the initial segments.&nbsp;</p><p>This money has also paid for contributions to the electrification of Caltrain in San Francisco, grade separations removing freight crossings from streets in Los Angeles, and the complete environmental clearance and basic geotechnical design of the entire route from Los Angeles to San Francisco. Completing the Central Valley segment will cost between $4 and $7 billion.&nbsp;</p><p>The additional funding required to reach San Francisco and Los Angeles to complete the core route will be approximately $80 billion. This is a lot of money, but as inflation-adjusted estimates show, it is not due purely to incompetence and cost overruns. More to the point, California can afford projects on a similar scale. The recent BART extension to San Jose is expected to cost $12.75 billion, and Caltrans receives a budget of $15 billion a year. California has had substantial budget surpluses in the past decade. In 2022 alone, the surplus totaled nearly $100 billion (although the budget is now roughly balanced). As a matter of practical economics, the project could be paid for. The question now is if the political will is there to fund it either in whole or in some revised form.&nbsp;</p><p>Despite more than a decade of predictions of its failure, the project has persevered — even if its completion is in limbo. The initial operating segment between Merced and Bakersfield is projected to begin operating in 2030. Progress is invisible to those in San Francisco and Los Angeles, and it can’t be seen just driving along I-5. But everything from satellite imagery to drone footage to a drive along State Route 43 reveals the progress being made along the 119 miles of construction underway in the Central Valley. The 22 miles making up Construction Package 4 are effectively complete, from the massive Wasco Viaduct, where the high-speed rail rises to cross over Burlington Northern and Santa Fe freight railroad tracks, to the numerous smaller-grade separation structures rising from the valley floor.</p><p>A ceremony was held in January 2025 to mark the beginning of work on the “construction railhead,” where 10 miles of freight rail yard will be built to support track laying and the erection of overhead wires in coming years. This will also include some of the first permanent high-speed tracks to be laid on the alignment. Of the 81 structures planned between Construction Packages 1-3, all but seven are underway. Seventeen structures are planned to be completed in 2025, including rail viaducts over low-lying swamps and rivers, and underpasses and overpasses that will remove grade crossings with urban streets in Fresno.&nbsp;</p><p>While no final track has yet been laid, this constitutes the vast majority of the work to prepare the route. For comparison, were this project a highway, at this stage it would need only paving and striping. If funding holds from the state, contracts for track laying and the purchase of the first train sets should go out this year. Work is also proceeding on the remainder of the route: Design work and property acquisition are underway for the next 50 miles of right-of-way extending to Merced and Bakersfield, this time phased to allow utility relocations and other local interference to be cleared ahead of construction on the main structures. All of the environmental permits are complete for extending the tunnels through Pacheco Pass to Gilroy and up to San Francisco and from Bakersfield south to Palmdale and Los Angeles. The only thing holding back construction is money.</p><p>Watching the progress has converted some former skeptics. Kings County Supervisor Doug Verboon was once a plaintiff in one of the lawsuits seeking to stop the project in its tracks. As of this year, he’s expressed support for finishing the sections of the project already underway. In an article in the <em>Hanford Sentinel </em>from December 2024, he expressed that, as chair of the San Joaquin Valley Rail Commission, he now wants to see the structures completed to make use of the investment.&nbsp;</p><p>For those who can see every day the tangible progress of the project, opposition has evolved into something quite different: doubt about whether the funding can be provided to finish the job. The dream of a complete line from San Francisco to Los Angeles remains as popular as it did when 1A first passed with 52% of the vote in 2008. <a href="https://ktla.com/news/california/majority-of-californians-still-support-high-speed-rail-project-polling-shows/">An Emerson College poll commissioned by KTLA in February</a> found that 54% of Californians still support the project. When Trump’s secretary of transportation, Sean Duffy, came to Los Angeles to announce the administration was investigating the project, he was met by a large crowd of protestors chanting “build the rail” loud enough to drown him out on press microphones.</p><p>The next four years of the second Trump administration will prove a crucial test of support from the state government. Even before Trump threatened to cut off the awarded federal funding, the Office of the Inspector General raised the possibility that the project could be between $3 and $6.5 billion short of completing the 172 miles from Merced to Bakersfield, depending on the variation in state cap-and-trade auction revenue. The project has sufficient California funds only to last through the Trump administration, complete and electrify the existing 120 miles, purchase train sets, and begin construction of the Merced and Bakersfield extensions — but not fully complete them. &nbsp;</p><p>This is not an easy project. But exaggerating the difficulties of completing it can blind us to both the possibilities and the reasons the project was — and remains — popular. A high-speed rail connection in San Jose would place the Central Valley a mere 30-45 minutes away, opening up options for new housing and commercial opportunities. Effectively, it would make something like the “California Forever” plan possible in a different region of the state, complete with high-speed rail connections directly to Silicon Valley. Long term, connecting Los Angeles to San Francisco by high-speed rail would be time competitive with flying and several times faster than driving.&nbsp;</p><p>If California politicians match action to desires, it is within California’s capability to fund the project itself. Governor Newsom and the California legislature have hinted at such a possibility. Rather than buying into the narratives of predetermined failure pushed by the project’s longtime opposition, Californian citizens and politicians should push for the project’s continuation — and for as much support as the state can give.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is outbound going to die? (115 pts)]]></title>
            <link>https://rnikhil.com/2025/04/25/sales-outbound-ai-dead</link>
            <guid>43823851</guid>
            <pubDate>Mon, 28 Apr 2025 17:28:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rnikhil.com/2025/04/25/sales-outbound-ai-dead">https://rnikhil.com/2025/04/25/sales-outbound-ai-dead</a>, See on <a href="https://news.ycombinator.com/item?id=43823851">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><blockquote><p><a href="https://news.ycombinator.com/item?id=43823851">HN Discussion</a></p></blockquote><p>I see a ton of sales/marketing products all powered by AI, making hyper personalised content to target potential users and customers. These tools now make sophisticated, high-volume paid marketing campaigns accessible to everyone, from large enterprises to individual consumers. With LLMs, the messages and content have gotten to a point where the accuracy and quality of the has improved tremendously at scale. Moreover, the scale of outbound sales has also increased rapidly. You are now able to pump out 1000s of SEO blog posts, generate reels/videos on the fly and even use AI agents to do email/phone outreach at a never before seen scale.</p><p>While I think these AI powered sales products are going to perform very well in the short run, this is also going to cause a certain about of fatigue for the users and customers. <u>(there is a small window here where companies adopting these tools are going to crush it)</u> Eventually, humans are going to get used to the constant spam and start mentally tuning out these hyper personalized initiatives. It will kill the trust, attention and the subsequent conversion rates of these products. The SEO posts won’t be read, emails and calls will go unanswered and people will start tuning out even the personalised videos.</p><p>On top of this, <strong>ALL</strong> the companies will have equal access to these tools to create content and campaigns. Imagine giving out every SaaS company in the world these tools and asking them to sell their products to the same 10000 enterprises which pretty much everybody is targeting. There is going to be so much personalised AI slop in the future.</p><h4 id="so-what-will-happen-how-will-sales-evolve-in-the-future-how-will-new-companies-build-and-acquire-users">So what will happen? How will sales evolve in the future? How will new companies build and acquire users?</h4><p>Your existing distribution will start mattering more. Having private access to the buyers or people will be absolutely important. Making personal relationships to these decision makers and other key people in the network would become compulsory since they essentially become gatekeepers. If outbound doesn’t work and it’s all going to be inbound, referrals and personal relationships will basically be everything.</p><p>We will start seeing companies build all this bottoms up. They will try to create and engineer virality on Twitter(like the <a href="http://icons.com/">icons.com</a> team) and spend a ton of money on branding (for the company and maybe the CEO). Having a good Twitter/social media presence will become a compulsory pre-condition. The company owned channels (like websites, email lists or apps) where you have direct access to customers will become key demand generation pipelines. These will generate organic growth as satisfied customers and partners naturally promote the business.</p><p>The community and network effects will become critical competitive advantages. Companies will invest heavily in building engaged and trusted user communities, starting platforms where users create value for each other, and developing network driven acquisition strategies. These interconnected relationships will generate demand, creating defensible moats against competitors relying solely on paid acquisition.</p><p>If you are working on alternative sales/GTM products working on the above problem, please reach out to me.</p><span><time datetime="2025-04-25T00:00:00+00:00">April 25, 2025</time> · </span></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Side Hustle From Hell (325 pts)]]></title>
            <link>https://blog.jacobstechtavern.com/p/the-side-hustle-from-hell</link>
            <guid>43823620</guid>
            <pubDate>Mon, 28 Apr 2025 17:05:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jacobstechtavern.com/p/the-side-hustle-from-hell">https://blog.jacobstechtavern.com/p/the-side-hustle-from-hell</a>, See on <a href="https://news.ycombinator.com/item?id=43823620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><blockquote><p><em>Being exploited by a startup is almost a rite of passage. I don’t think I can even call this a cautionary tale, because of what I took out of the experience.</em></p></blockquote><blockquote><p><em>Subscribe to Jacob’s Tech Tavern for free to get ludicrously in-depth articles on iOS, Swift, tech, &amp; indie projects in your inbox every week.</em></p><p><em><span>Full subscribers unlock </span><a href="https://blog.jacobstechtavern.com/t/quick-hacks" rel="">Quick Hacks</a><span>, my advanced tips series, and enjoy my long-form articles </span><strong>3 weeks </strong><span>before anyone else.</span></em></p></blockquote><blockquote><p><em><span>To </span><a href="https://news.ycombinator.com/item?id=43823620" rel="">celebrate going viral on Hacker News</a><span>, I’m offering </span><strong>35% off</strong><span> for new subscribers. Grab it today to lock in the price forever!</span></em></p></blockquote><p data-attrs="{&quot;url&quot;:&quot;https://blog.jacobstechtavern.com/subscribe?coupon=8f8f6123&amp;utm_content=155578632&quot;,&quot;text&quot;:&quot;Get 35% off forever&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.jacobstechtavern.com/subscribe?coupon=8f8f6123&amp;utm_content=155578632" rel=""><span>Get 35% off forever</span></a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png" width="1200" height="857.1428571428571" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:1040,&quot;width&quot;:1456,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:2188644,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.jacobstechtavern.com/i/155578632?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18242a29-2f8d-46e3-8f08-8f164012ce7a_1680x1200.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Gather ‘round, friends. This one has been a long time coming. </p><p>Grab a beer, we’ve got time 🍺. </p><p>This is a cautionary tale—one which left me scarred, jaded, and wiser. I hope that by reading this story, I can protect some of you from 11 months of pain.</p><p>The year was 2019.</p><p>My migration from academic overachiever to professional underachiever was an overwhelming success, as a slightly under-levelled junior at a big consultancy shop. But I couldn’t shake the feeling I was destined for great things.</p><p><span>I was engulfed by the daydreams many 24-year-olds experience when they stop being described as “precocious” and start spending their commutes listening to billionaires on podcasts. As my parasocial relationships to Peter Thiel and Reid Hoffman took hold, I was pretty sure I could become the next Jeff Bezos, or </span><em>at least</em><span> the next Drew Houston—I just had to </span><em>execute</em><span> well. </span></p><p>The chance of a lifetime fell into my inbox through a friend of a friend of a friend. I was put in touch with Jimmy, the cofounder &amp; Chief Financial Officer of a startup. His team was on the lookout for someone that knew mobile apps, to advise their startup and put out a few fires. </p><p><span>Involvement with a </span><em>Startup</em><span>.</span></p><p><span>In an </span><em>Advisory Capacity.</em><span> </span></p><p>If my weeks of tech consultancy training, and several years in mobile development, had prepared me for anything: it was this.</p><p>The venture was operating in stealth mode (if you don’t count the website), so Jimmy was cagey with the details until our initial call. I understood—as (probably) the next Zuckerberg, you need to be careful with your ideas around me.</p><p>We got on great. I briefed him on my credentials and showcased my passion for startups, and he told me I’d be a great fit for the team. It was a done deal. I’d taken the first step to fulfilling my destiny as the next great startup (advisor).</p><p>Fixr was the operating system for your car. It was the one-stop shop connecting you to qualified, vetted mechanics in your local area for anything your car needs—annual MOT, on-demand repair, and even roadside recovery. </p><p>Fixr had been operating for close to 3 years under a crack team of 3 part-time visionaries: </p><ul><li><p>Jimmy (CFO) — an innovation manager at a consultancy </p></li><li><p>Kim (CMO) — a legal associate at an accounting firm </p></li><li><p>Mike (COO) — a mechanic </p></li></ul><blockquote><p><em>*I changed some names and obfuscated a few details outside the public domain; please simply imagine the cast of Better Call Saul. While you’re at it, feel free to picture me as Tony Dalton.</em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png" width="1456" height="1040" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1040,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2118504,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F807ac2f6-6335-427d-814c-50cfd87cd00e_1680x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In a few short years, the team had an impressive collection of achievements:</p><ul><li><p>Securing some startup capital to fund development, from a personal bank loan.</p></li><li><p>Winning a pitch contest at a local university.</p></li><li><p>Potential interest from VCs and accelerators, subject to getting traction.</p></li><li><p>Registering with the UK SEIS scheme; giving a tax break on seed investment.</p></li><li><p>Arranging a referral partnership, where we could get paid for convincing mechanics to switch banks. </p></li><li><p>Extensive market research, speaking to hundreds of mechanics, with nearly all of them expressing interest in joining the platform when launched. </p></li><li><p>Contriving a financial model demonstrating how £250k of seed funding will be transformed into £3M in revenue, with operations across Europe, by year 3. </p></li><li><p>A live static landing page, hosted on an AWS EC2 Medium server.</p></li><li><p>4 mostly-built apps for customers &amp; mechanics, on iOS &amp; Android.</p></li></ul><p>That final point was where they needed some expertise. Most of their bank loan runway had been burned on an overseas contractor they fired after 2 years, for incompetence. They switched to a Hyderabad-based agency, who were sanding off the rough edges and getting the 4 apps ready for launch day.</p><p><span>I met up with Jimmy for some casual in-person end-to-end testing of the near-finished product and… </span><em>Oh boy.</em></p><p>One could say the app wasn’t quite production-ready.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png" width="1456" height="881" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/acde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:881,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1171217,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facde6e3d-3cc9-4794-a2ef-197b98b811dc_1902x1151.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Turns out the storyboards were locked to iPhone 4s dimensions—this is how they rendered on my shiny new XR</figcaption></figure></div><p><span>In my advisory capacity, I was in charge of speaking to the agency and communicating the many, many, </span><em>many</em><span> bugs that remained. This is where I got my first taste of gaslighting from a </span><em>client relationship manager</em><span> whose job is to keep suckers on the hook.</span><em> </em></p><blockquote><p><em>“We agreed to the previous list of fixations only and never agreed anywhere as the part of the agreement. Please refer the email snapshot above.”</em></p><p><em>“Look at the specification. It does not specify anywhere we were asked to support screen dimensions larger than iPhone 4s sized.”</em></p><p><em>“The stripe payment integration is not of the phone application, you do not need this from us”</em></p></blockquote><p>Things looked a little bleak with the late 2019 launch date, so I suggested to Jimmy perhaps I could help squash some of these bugs. The cofounders didn’t have access to the code repo, so I had to spend more time battling the relationship manager for access.</p><p>And eventually, I got it.</p><blockquote><p><em>“Far, far below the deepest delving of FAANG and big consultancies, the world is gnawed by nameless devs. Even Bill Gates knows them not. They are older than he. Now I have walked there, but I will bring no report to darken the light of day.”</em></p><p><em>—Gandalf, a famous staff engineer</em><span> </span></p></blockquote><p>I saw only one way to bring Fixr to market. </p><p><span>We would need a </span><em>full rewrite</em><span>. </span></p><p>Despite being £20,000 in the hole across 4 apps, the management team didn’t take  much convincing to let go of a sunk costs. I whipped up a quick live-demo of a modern iOS map app, with custom photo location pin, plus an input form and photo capabilities.</p><p>In one Monster-and-Elvanse-fuelled night of passion, I was light years ahead of a build that had taken 3 years. They were sold.</p><p>In order to justify the extra time investment, I was provisionally brought on as cofounder and CTO. But we had a marketplace to build, and two platforms to serve. We needed an “Android guy”. I knew just the person: Gus. </p><p>Like me, he was keen on the idea, and in the market for a new side project. At a pre-lockdown party, I sold the dream: this product lives or dies via the traction we get in the next couple of months.</p><blockquote><p><em>Dragging my good friend into the blender is a regret, but we kept each other sane through the subsequent 9 months.</em><span> </span></p></blockquote><p>Despite being late to the party, the team valued our potential contributions at 10% apiece—the level required to get into Y Combinator. </p><p>To make things official, and certify our equity stakes, Kim drew up contracts, which were quite obviously rehashed drafts of the contracts they gave the original ill-fated dev agency, plus a very spicy clause pertaining to equity shares.</p><blockquote><p><em>The Management Team may terminate this agreement and reduce or fully remove the equity held by the Developer where the Development Services are not sufficiently met. This clause is to be carried out at the discretion of the Management Team.</em></p></blockquote><p>I was about to sign, when Gus cleverly consulted his dad who, in a brilliant blaze of prescience, suggested we run a mile and block our teammates’ phone numbers. Gus and I knew better, and simply pushed the team to change the wording to “gross negligence”.</p><p>We were officially cofounders. </p><p>It was go-time.</p><p>The prevailing months flew by in a COVID-tinged haze. While everyone else was baking bread and remunerating nursing staff via saucepans, Gus and I spent every free minute turning Fixr into a serious product.</p><p>Our weekly Zoom calls had a regular push-and-pull. Gus and I presented the latest flows. Kim and Jimmy praised our progress, while pushing to expand the scope. We had to add more features to win the market as a one-stop-shop.</p><p>Engineering busywork complete, we turned to our operators.</p><ul><li><p>Kim shared her latest dream for our upcoming “viral marketing campaign” that would land thousands of sign-ups on launch day.</p></li><li><p>Mike spitballed when we might start to acquire mechanics. Perhaps we’d hook into local apprenticeship schemes once we had demand from customers. Achieving liquidity would be straightforward.</p></li><li><p><span>Jimmy went through his latest tinkering with the financial model—the cornerstone of our investment thesis—and talked us through the latest couple of emails from VC analysts who asked us to </span><a href="https://blog.jacobstechtavern.com/p/yes-actually-means-no-the-curious" rel="">“get in touch when we had traction”</a><span>.</span></p></li></ul><p>This early traction would be everything. </p><p>After months of neglecting our families, Gus and I had the MVP ready for prime-time, with a single robust user flow: </p><ul><li><p>Users on the Customer apps can post local repair jobs. They can pay via the app and approve each invoice item during a repair job in real-time.</p></li><li><p>Mechanics on the companion app could bid for jobs, perform vehicle inspections, append work items during a job, and handle invoices automatically.</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png" width="728" height="352.6679245283019" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:1027,&quot;width&quot;:2120,&quot;resizeWidth&quot;:728,&quot;bytes&quot;:1097509,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F676bf9e9-6be8-4ec4-ac9e-1df899844d1e_2120x1027.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Screens from our Android MVP. The iOS build is lost to time.</figcaption></figure></div><blockquote><p><em>Fun fact, this project is how I cut my teeth on SwiftUI 1.0. The screenshots are lost to time (and dodgy source control), but the internal architecture was a hot mess. </em></p><p><em>Early UIKit inter-op was janky and it was difficult to pass state about. This was a problem when two of our major features used the Google Maps SDK (MapView didn’t exist!) and the camera.</em></p><p><em>A gigantic @EnvironmentObject singleton controlled the entire app state, and because NavigationView was so broken in SwiftUI 1.0, every screen transition was a modal. </em></p></blockquote><p>This flow worked phenomenally, across our suite of 2 iOS apps and 2 Android apps. But it wasn’t enough for our cofounders. We had to have on-demand roadside recovery. We needed to include annual MOT checks. Flame decals are a must-have. Anything you need for your car, we had to be.</p><p>Gus and I noticed we actually had vertebrae, and pushed back hard.</p><p><span>Implementing their vision would take a full-time team multiple years. We had the MVP. We had to </span><em>launch</em><span> instead of spinning our wheels for another 3 years. Our cofounders had to stop dreaming and start operating.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png" width="1456" height="571" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:571,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1764067,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff15b1ea8-c14f-49d8-bdb6-8738292646e4_2210x866.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Everything was ready to release in App Store Connect and the Google Play Store. </p><p>4 apps. One dream. Billions in potential.</p><p><span>We pressed the button. </span><strong>Release</strong><span>.</span></p><p>and…</p><p>Crickets.</p><p>Without any waitlisted mechanics, our imaginary viral marketing campaign was dead before it began. We had nothing on either side of our car repair marketplace for anything more than a tech demo.</p><p>Faced with the cold, harsh reality of zero users, zero revenue, and zero funding, Gus and I had a moment of clarity. We asked ourselves one question. </p><blockquote><p><em>WTF had our cofounders been doing this whole time?</em></p></blockquote><p>It’s probably time to tell you that Jimmy and Mike despised each other. Jimmy didn’t respect Mike’s intelligence or commitment. Mike didn’t like Jimmy on a personal level. One of Kim’s primary roles in the business was as mediator between the two.</p><p>Unbeknownst to Gus and myself, our tidy 10% equity packages were mostly carved out of the hog carcass of Mike’s original third share. Jimmy, as chief visionary and de-facto CEO, had a capricious habit of unilaterally reallocating equity based on how much value he thought you brought to the table.</p><p>I was never in the room when these conversations happened, but Gus and I were immune to this chicanery—we’d negotiated our contractual protections up to “gross negligence”—a fairly high legal bar.</p><blockquote><p><em>Gus’ dad is looking preeety smart right now.</em></p></blockquote><p>The incompetence of the original dev agencies concealed a far deeper issue. The apps Gus and I hacked together? Not pretty. Worked brilliantly. </p><p>We were no longer sitting on a product problem, and the abject failure of the startup to achieve anything meaningful in 3 years no longer had anything to hide behind.</p><p>I spent summer hustling to bail out the sinking ship:</p><ul><li><p>I sat with Mike to reach out to mechanics and bootstrap our supply-side, trying to start at the town level.</p></li><li><p>I helped Kim to set up social media ads to bolster the demand-side, running campaigns in target towns to nurture local liquidity. </p></li><li><p>I wanted to go all-in: Jimmy offered to double my equity in exchange for co-signing to the business loan. Uhhh… I said I’d think about it.</p></li><li><p>I picked up some of the load of reaching out to angels, VCs, and business partners on LinkedIn.</p></li></ul><p>This grind actually landed us a potential whale: a meeting with the CTO of RAC, one of the 2 big car recovery companies in the UK.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png" width="954" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:954,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:143926,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab67bb2a-644f-4dfb-b90b-56a73505594b_954x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We pulled out all the stops to demo our product, impressing him with the simplicity and depth of the end-to-end repair flow from the mechanic side. Was this going well?</p><p>Astoundingly, Jimmy kept drawing attention to the £50-per-referral banking partnership we had arranged. Did he not realise they offer this to everyone?!</p><p>We staggered when asked about our sign-ups from mechanics (zero) and daily repair jobs (also zero). None of us had the presence of mind to request a more junior, less busy contact point.</p><p><span>Our collective—really, </span><em>my</em><span>, efforts—yielded nothing substantial. We didn’t have the demand to convince mechanics to sign up. We didn’t have the mechanics to supply any demand that might exist. Marketplaces are hard, it turns out. And we were not the team to build it.</span></p><p>I got a much-needed reality check at a post-lockdown family party. I enthusiastically gave my cousin the rundown on Fixr, talking him through our development process and my summer spent hustling. He simply asked, bewildered:</p><blockquote><p><em>“Why are you doing all this work and not being paid?”</em><span> </span></p></blockquote><p>Come Autumn, things still weren’t going anywhere with Fixr. </p><p>Simultaneously, my underpaid mid-level consultancy role passed me up for promotion again. I wanted out, double-time.</p><p>I performed the reluctant ritual of reaching out to recruiters. On learning about my recent brush with startups, one recruiter suggested they had just the person to introduce me to.</p><p><span>Enter: </span><strong>Carbn</strong><span>. The app for building green habits and offsetting your emissions. </span></p><p>It was everything Fixr was not. </p><ul><li><p>A bootstrapping first-time founder, a commercial strategist who could fund a full-time salary for the right cofounder and was very generous with our equity split. </p></li><li><p>He’d shopped around several ideas, validating a high-potential market niche in the US prepared to spend money on the product.</p></li><li><p>After this validation, he committed: £10k on a solid contractor for a full set of designs; shaping the early roadmap for our early product work and giving us a solid branding foundation. </p></li><li><p>The guy whipped out a rudimentary financial model in a few hours. Turns out it’s not that important—it’s for illustrating your runway and spending plan, not to justify an imaginary revenue number. </p></li><li><p>No clandestine activity with contracts. We hashed it out on SeedLegals over beers.</p></li></ul><p>I tendered my resignation to my cofounders and relinquished my equity. </p><p>Gus quickly followed, immediately landing on his feet as an engineer in the banking sector. With no developers to keep the pretence of activity afloat, Fixr and the team soon dissolved.</p><p>This was cathartic, but bloody hard to write. </p><p>The earliest draft of this article began on August 18, 2023. I wanted to do right by you, and waited until I could tell my story properly.</p><p><span>The temptation to annotate each paragraph with commentary was excruciating. But I wanted my naivety to speak for itself—I hope, by the end, you were also screaming </span><em>“Get out! What are you doing!?”</em><span> to both myself and Gus. </span></p><p>But do you want to know the terrible truth? </p><p><strong>I loved every minute of it.</strong></p><p><span>Like the intrepid doctor, the stoic investment banker, or the wily consultant, suffering is part of the package. I was in my element. I had my finger in every pie. I was </span><em>doing</em><span> a startup. I was </span><em>executing</em><span>, and for the first time in my professional life I wasn’t insulated from the results. I didn’t achieve my destiny of </span><em>great things</em><span>, but I’d </span><em>built</em><span> something.</span></p><p>My story is far from unique—being exploited by a startup is almost a rite of passage. I don’t think I can even call this a cautionary tale, because of what I took out of the experience. My hard-earned learning from Fixr opened another door to Carbn which I was uniquely qualified for.</p><p>If you’re early in your career (and childless), I would even go as far as to recommend a quixotic startup journey. As a 29-year-old developer, I’ve seen first-hand the compounding career benefits of side projects, in many of my colleagues and my friends.</p><blockquote><p><em>Just watch out for some of the red flags which I summarised below!</em></p></blockquote><p>I wanted to end with a quick summary of some red flags you could encounter if you find yourself mired in a satanic side hustle. </p><ol><li><p><span>If a startup has operated for a long time without launching, consider whether the team are serious. Even if you are all very driven, if your cofounders aren’t obviously A-players, your chances of outsized startup success are low (</span><em>but you might still learn a lot).</em></p></li><li><p><span>If your equity share is contingent on being cut in on a business loan, you are being asked to invest your own money. </span><em>Run</em><span>. </span></p></li><li><p>If there is political infighting between cofounders, consider whether you want to raise funds and become legally bound to these people for several years.</p></li><li><p><span>There are 2 jobs at a startup: building and selling. If you aren’t sure what your cofounders are doing, trust your </span><em>git instinct</em><span>.</span></p></li><li><p><span>Building native on multiple platforms is an extremely inefficient allocation of resources when pre-product-market-fit. Building </span><em>two</em><span> apps on each platform is just mad.</span></p></li><li><p>Startup pitch competitions are mostly a waste of time—validation comes from talking to users and iterating, not from impressing a judge. This iterating is the hard part—everyone will tell you they like your app, and most will say they’d pay for it. Unless you have deep domain knowledge, talking to 300 technicians without a working product will not yield useful validation.</p></li><li><p><span>Did I mention that I never met Kim or Mike in person? COVID aside, nothing beats the magic of hacking side-by-side to bring your dream to life. It’s not a great sign if </span><em>all</em><span> your communication is remote.</span></p></li><li><p><span>Marketplace startups are often considered the hardest software startups to build, because you have to create two markets at once. They work best with </span><strong>frequent </strong><span>and </span><strong>inexpensive </strong><span>transactions (from which you can reasonably take a 20% cut).</span></p></li><li><p><span>When VCs tell you “</span><em>talk to us again when you have traction”</em><span>, they mean </span><em><a href="https://blog.jacobstechtavern.com/p/yes-actually-means-no-the-curious" rel="">“if you prove there is a market opportunity and that you can execute as a management team, then we might consider you. But because I don’t believe either of those things will happen, I will not be taking a risk on you”.</a></em></p></li><li><p><span>It’s also a red flag if you aren’t vetted much </span><em>yourself</em><span>. If a company takes you on, consider if they want you based on merit, or whether </span><strong>you are the first engineer who agreed to work for free.</strong></p></li></ol><blockquote><p><em>Thanks for reading Jacob’s Tech Tavern! I hope you enjoyed my story. 🍺 </em></p><p><em><span>If you enjoyed this, please consider paying me. Full subscribers to Jacob’s Tech Tavern unlock </span><strong><a href="https://blog.jacobstechtavern.com/t/quick-hacks" rel="">Quick Hacks</a></strong><span>, my advanced tips series, and enjoy my long-form articles </span><strong>3 weeks </strong><span>before anyone else. </span></em></p><p><em><span>To </span><a href="https://news.ycombinator.com/item?id=43823620" rel="">celebrate going viral on Hacker News</a><span>, I’m offering </span><strong>35% off</strong><span> for new subscribers. Grab it today to lock in the price forever!</span></em></p></blockquote><p data-attrs="{&quot;url&quot;:&quot;https://blog.jacobstechtavern.com/subscribe?coupon=8f8f6123&amp;utm_content=155578632&quot;,&quot;text&quot;:&quot;Get 35% off forever&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://blog.jacobstechtavern.com/subscribe?coupon=8f8f6123&amp;utm_content=155578632" rel=""><span>Get 35% off forever</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reality Check (119 pts)]]></title>
            <link>https://www.wheresyoured.at/reality-check/</link>
            <guid>43823492</guid>
            <pubDate>Mon, 28 Apr 2025 16:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/reality-check/">https://www.wheresyoured.at/reality-check/</a>, See on <a href="https://news.ycombinator.com/item?id=43823492">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p>I'm sick and god-damn tired of this! I have written tens of thousands of words about this and still, to this day, people are babbling about the "AI revolution" as the sky rains blood and crevices open in the Earth, dragging houses and cars and domesticated animals into their maws. Things are astronomically fucked outside, yet the tech media continues to tell me to get my swimming trunks and <em>take a nice long dip in the pool.</em></p><p>I apologize, this is going to be a little less reserved than usual.</p><p>I don't know why I'm the one writing what I'm writing, and I frequently feel weird that I, a part-time blogger and podcaster, am writing the things that I'm writing. Since I put out <a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/"><u>OpenAI Is A Systemic Risk To The Tech Industry</u></a>, I've heard <em>nothing</em> in response, as was the case with<a href="https://www.wheresyoured.at/to-serve-altman/"> <u>How Does OpenAI Survive?</u></a> and<a href="https://www.wheresyoured.at/oai-business/"> <u>OpenAI Is A Bad Business</u></a>.&nbsp;</p><p>There seems to be little concern — or belief — that there is <em>any</em> kind of risk at the heart of OpenAI,<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=As%20a%20note,run%20this%20company."> <u>a company that spent $9 billion in 2024 to <em>lose $5 billion</em></u></a><em>. </em>While I'd love to add a "because..." here, if not because it’s important to be intellectually honest and represent views that directly contrast my own, even if I do so in a somewhat sardonic fashion, nobody seems to actually have a cogent response to how they right this ship other than Hard Forker<a href="https://bsky.app/profile/edzitron.com/post/3lmkahymkec2t?ref=wheresyoured.at"> <u>Casey Newton throwing a full-scale tantrum on a podcast</u></a> and saying I'm wrong because "inference costs are coming down."</p><p>Newton is a nakedly-captured booster that<a href="https://www.platformer.news/people-are-using-ai-to-learn-more/?ref=wheresyoured.at"> <u>ran an infographic from Anthropic a few weeks ago the likes of which I haven't seen since 2013</u></a>, but he's far from the only one with a flimsy attachment to reality.</p><p><a href="https://www.theinformation.com/articles/openai-forecasts-revenue-topping-125-billion-2029-agents-new-products-gain?rc=kz8jh3&amp;ref=wheresyoured.at"><u>The Information ran a piece a couple of weeks ago</u></a> that made me furious, which was a surprise because — for the most part — their coverage of tech, and especially AI, has been some of the best around, and they generally avoid the temptation to be shills for shaky and unsustainable tech companies.&nbsp;</p><p>The story claimed that OpenAI was "forecasting revenue topping $125 billion in 2029" based on "selling agents" and "monetizing free users...as a driver to higher revenue." The piece, reported out based on things "...told [to] some potential and current investors," takes great pains to accept literally everything that OpenAI says as perfectly reasonable, if not gospel, even if said things make absolutely no sense.</p><p>According to The Information's reporting, OpenAI expects "agents" and "new products" to contribute tens of billions of dollars of revenue, both in the near-term (somehow contributing $3 billion in revenue this year, which I'll get to in a little bit) and in the long-term, with an egregious $25 billion in revenue in 2029 projected to come from "new products."&nbsp;</p><p>If you're wondering what those new products might be, I am too, because The Information doesn't seem to know, and instead of saying "OpenAI has no idea what the fuck they're talking about and is just saying stuff," the outlet chooses instead to publish things with the kind of empty optimism that's indistinguishable from GPT-generated LinkedIn posts.</p><p><em>Check out this fucking chart.</em></p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcTvV_KScCMt6maPCLAD5qACLXB0A9UFlxBy7vmR4oO6y99lGkJJOfKhOCB9a3-GeZrY83n6xFepHBUxmf4fUXpDuYAypFPjGBTV-i1B8xYCkhZ1JaIaiAkpUazxVVqwjbiVJF9?key=5iQ8LZzyYfTKSGqXLxb-sOQh" alt="" loading="lazy" width="624" height="624"><figcaption><span>The Information — OpenAI Forecasts Revenue Topping $125 Billion in 2029 as Agents, New Products Gain</span></figcaption></figure><p>I want to be really, really clear: we are nearly in May 2025, and I see no evidence <strong><em>that OpenAI even has a marketable agent product, let alone one that will make it <u>three billion god damn dollars in the next six or seven months.</u></em></strong></p><p><em>For context, that’s triple the revenue OpenAI reportedly made from selling access to its models via its APIs — essentially allowing third-party companies to use GPT in their apps — in the entirety of 2024. And those APIs and models actually exist in a meaningful sense, as opposed to whatever the fuck OpenAI’s half-baked Agents stuff is.&nbsp;</em></p><p>In fact, no, no, I'm not going to be mean, I'm going to explain exactly what The Information is reporting in an objective way, because writing it out really shows how silly it all sounds. I am going to write "they believe" a lot because I must be clear how stupid this is:</p><ul><li>According to The Information's reporting, <strong>they believe that OpenAI will make $3 billion in 2025 from selling access to its agents in 2025</strong>. This appears to come from SoftBank,<a href="https://www.theinformation.com/briefings/softbank-pledges-to-spend-3-billion-annually-on-openai?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>which has said it will buy $3 billion worth of OpenAI products annually</u></a>.</li><li>Earlier this year, we got a bit of extra information about how SoftBank would use those products. It plans to <a href="https://group.softbank/en/news/press/20250203_0?ref=wheresyoured.at"><u>create a system called Cristal Intelligence</u></a> that will be a kind-of general purpose AI agent platform for big enterprises. The exact specifics of what it does is vague (shocker, I know) but SoftBank intends to use the technology internally, across its various portfolio companies, as well as market it to other large enterprise companies in Japan.&nbsp;&nbsp;</li><li>I also want to add that The Information can't keep its story straight on this issue.<a href="https://www.theinformation.com/articles/openai-forecast-shows-shift-from-microsoft-to-softbank?ref=wheresyoured.at&amp;rc=kz8jh3"> <u>Back in February</u></a>, they reported that OpenAI would make $3 billion in revenue <em>only from agents,</em> with a big, beautiful chart that said $3 billion would come from “it," only to add that “it” would be SoftBank "...[using] OpenAI's products across its companies."&nbsp;</li><li>Based on these numbers, it seems like SoftBank will be the <em>only</em> customer for OpenAI’s agents. While this won’t be the case — and isn’t, because it excludes anyone willing to pay a few bucks to test it out — it nonetheless doesn’t signal good things for Agents as a mass market product.&nbsp;&nbsp;<ul><li>Agents do not exist as a product that can be sold at that scale.<a href="https://www.theinformation.com/articles/ai-agents-fall-short-shopping?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information's own reporting from last week</u></a> highlighted how OpenAI’s "Operator" agent "struggle[d] with comparison shopping on financial products," and how Operator and other agents are "...tripped by pop-ups or logins, as well as prompts asking for email addresses and phone numbers for marketing purposes," which I think accurately describes <em>most of the internet.</em></li><li><strong>To summarize, The Information is saying that the above product will make OpenAI <em><u>three billion dollars by the end of the year.</u></em></strong></li></ul></li><li>According to The Information's reporting, <strong>they believe that OpenAI will basically double revenue every single year for the next four years</strong> and make $13 billion in revenue in 2025, more than doubling that to $29 billion in 2026, nearly doubling that to $54 billion in 2027, nearly doubling that to $86 billion in 2028, and eventually hitting $125 billion in 2029.<ul><li>Said revenue estimates, as of 2026, include billions of dollars of "new products" that include "free user monetization."<ul><li>If you are wondering what that means, I have no idea. The Information does not explain. They do, however, say that "OpenAI won’t start generating much revenue from free users and other products until next year. In 2029, however, it projects revenue from free users and other products will reach $25 billion, or one-fifth of all revenue," and said that "shopping is another potential avenue."</li></ul></li></ul></li></ul><p>I cannot express my disgust about how willing publications are to blindly publish projections like these, especially when they're so utterly ridiculous. Check out this quote:</p><blockquote>OpenAI has already begun experimenting with launching software features for shopping. Starting in January, some users can access web-browsing agent Operator as part of their pro ChatGPT subscription tier to order groceries from Instacart and make restaurant reservations on OpenTable.</blockquote><p>So you're saying this <strong><em>experimental software launched to an indeterminate amount of people that barely works is going to make OpenAI $13 billion in 2025, and $29 billion in 2026, and later down the line $125 billion in 2029? How? <u>How?</u></em></strong></p><p>What fucking universe are we all living in? There's no proof that OpenAI can do this other than the fact that it has a lot of users and venture capital!&nbsp;</p><p>In fact, I think we have reason to worry about whether OpenAI even makes its current projections.<a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/#:~:text=Bloomberg%20reported%20recently,to%20%243.43%20billion)."> <u>In my last piece</u></a> I wrote that Bloomberg had estimated that OpenAI would triple revenue to $12.7 billion in 2025, and based on its current subscriber base,<a href="https://www.wheresyoured.at/openai-is-a-systemic-risk-to-the-tech-industry-2/#:~:text=The%20Information%20reported%20back%20in%20January"> <u>OpenAI would have to effectively double its <em>current</em> subscription revenue <em>and massively increase its API revenue </em>to hit these targets</u></a>.</p><p>These projections rely on <strong><em>one entity (SoftBank) spending $3 billion on OpenAI's services, meaning that it’d make enough API calls to generate more revenue than OpenAI made in subscriptions in the entirety of 2024, and something else that I can only describe as “an act of God.”</em></strong></p><blockquote>That, I admit, assumes that Softbank’s spending commitment is based on usage, and not a flat fee (where Softbank pays $3bn and gets a set — or infinite — level of access). Assuming it’s the former, I’d be stunned if SoftBank’s consumption hits $3bn this year, even with the massive cost of the reasoning models that Cristal Intelligence will be based on. Softbank announced its deal with OpenAI in February.&nbsp;<p>Cristal Intelligence, if it works — and that is possibly the most load-bearing “if” of all time — will be a massive, complicated, ambitious product. Details are vague, but from what I understand, SoftBank wants to create an AI that handles the infinitely varied tasks that knowledge workers perform on a daily basis.&nbsp;</p><p>To be clear, OpenAI’s agents cannot consistently do, well… <em>anything</em>.&nbsp;</p></blockquote><p>What I believe is happening is that reporters are taking<a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html?ref=wheresyoured.at"> <u>OpenAI's rapid growth in revenue from 2023 to 2024</u></a> (from tens of millions a month at the start of 2023 to $300 million in August 2024) to mean that the company will always effectively double or triple revenue every single year forever, with their evidence being "OpenAI has projected this will be the case."</p><p>It's bullshit! I'm sorry!<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=ChatGPT%20is%20popular,and%20unsustainable."> <u>As I wrote before</u></a>, OpenAI effectively <em>is</em> the generative AI industry, and nothing about the <em>rest</em> of the generative AI industry suggests that the revenue exists to sustain these ridiculous, obscene and fantastical projections. Believing this — and yes, reporting it objectively is both endorsing and believing these numbers — is engaging in childlike logic, where you take one event (OpenAI's revenue grew 1700% from 2023 to 2024! Wow!) to mean another will take place (OpenAI will continue to double revenue literally every other year! Wow!), consciously ignoring difficult questions such as "how?" and "what's the total addressable market of Large Language Model subscriptions exactly?" and "how does this company even survive when it "expects the costs of inference to triple this year to $6 billion alone"?</p><p>Wait, wait, sorry, I need to be really clear with that last one, this is a direct quote from The Information:</p><blockquote>The company also expects growth in inference costs—the costs of running AI products such as ChatGPT and underlying models—to moderate over the next half-decade. Those costs will triple this year, to about $6 billion and rise to nearly $47 billion in 2030. Still, the annual growth rate will fall to about 30% then.</blockquote><p>Are you <em>fucking kidding me?</em></p><p><em>Six billion fucking dollars for inference alone? </em>Hey Casey, I thought those costs were coming down! Casey, are you there? Casey? <em>Casey?????</em></p><p><strong>Anyway, </strong>that's not great at all! That's really bad! The Information reports that OpenAI will make "about $8 billion" from subscriptions to ChatGPT in 2025, meaning that <strong><em>75% of OpenAI's largest revenue source is eaten up by the price to provide it</em></strong>. This is meant to be the cheaper part! This is the one fucking thing people say is meant to come down in price!</p><p>Are we living in different dimensions? Are there large parts of the tech media that have gas leaks in their offices? What am I missing? Tell me what I'm missing!</p><p><strong><em>Nerr, Ed, you haven't talked to the people building these things, you don't know what you're-</em></strong> shut the fuck up! Shut up! I am sick and tired of people (<a href="https://bsky.app/profile/edzitron.com/post/3lmkahymkec2t?ref=wheresyoured.at"><u>like Casey</u></a>!) suggesting that what's missing from my analysis is to "interview people who work at these companies and understand how this technology works." What would these people say to me, exactly? What response would they have to these numbers?</p><h2 id="forgive-me-im-going-to-be-a-little-rude"><strong>Forgive Me I'm Going To Be A Little Rude</strong></h2><p>In fact, you know what, let me just sit down and go through the critiques one-by-one. Some of you are going to say I'm being rude to these people and it weakens my analysis, to which I respond "kiss my entire ass." I can beat you to death with the truth while making fun of you for believing stupid things.</p><ul><li><strong>The costs of inference are coming down:</strong> Source? Because it sure seems like they're increasing for OpenAI, and they're effectively the entire userbase of the generative AI industry!&nbsp;<ul><li><strong>But DeepSeek…</strong> No, my sweet idiot child. <em>DeepSeek is not OpenAI</em>, and OpenAI’s latest models only get more expensive as time drags on. GPT-4.5 costs $75 per million input tokens, and $150 per million output tokens. And at the risk of repeating myself, OpenAI is effectively the generative AI industry — at least, for the world outside China.&nbsp;</li></ul></li><li><strong>This is the company at its growth stage, it can simply "hit the button" and it'll all be profitable:</strong> You have the mind of a child! If this was the case, why would both Anthropic and OpenAI be losing so much money? Why are none of the hyperscalers making profit on AI? Why does nobody want to talk about the underlying economics?</li><li><strong>These are the early days of AI: </strong>Wrong! We have the entire tech industry and more money than has ever been invested into anything piled into generative AI and the result has been utterly mediocre. Nobody's making money but NVIDIA!</li><li><strong>They're already showing signs that it'll be powerful: </strong>No it's not! If it was there'd be people doing crazy, impressive things with it!<ul><li><strong><em>But Ed, look at o-3</em></strong>: Oh you mean<a href="https://techcrunch.com/2025/04/18/openais-new-reasoning-ai-models-hallucinate-more/?ref=wheresyoured.at"> <u>the new and extremely expensive reasoning model that hallucinates <em>more</em> somehow</u></a>? Is that AGI? Is the AGI in the room with us now? Did it tell you it loved you?<a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html?ref=wheresyoured.at"> <u>Did it tell you to leave your wife</u></a>? I hope you're okay!!</li></ul></li><li><strong>But Ed, really, it's the early days, it was just like this in the early days of the internet: </strong>No it wasn't!<a href="https://www.wheresyoured.at/pop-culture/#:~:text=He%20also%20notes%20that%20the%20costs%20are%20so%20high%20that%20even%20if"> <u>Read Jim Covello of Goldman Sachs' note from last year</u></a>, the early days of the internet were absolutely nothing like this-<ul><li><strong>Smartphones! YES! Got you, Ed! Smartphones! People doubted those too-</strong> I am going to drown you in an icy lake! Covello's note also included an entire thing about how smartphones were fully telegraphed to analysts in advance, with "hundreds of presentations" that accurately fit how smartphones rolled out, no such roadmap exists for AI!</li></ul></li><li><strong>Heh, heh, Ed, you're so boned. Check out this article from Newsweek in 1995</strong><a href="https://www.newsweek.com/clifford-stoll-why-web-wont-be-nirvana-185306?ref=wheresyoured.at"><strong> <u>where a guy says that the internet won't be a big business</u></strong></a><strong>. This somehow proves that AI is going to be big, due to the fact one guy was wrong once: </strong>Motherfucker, have you read that piece? He basically says that the internet, at that time, was pretty limited, and yes, he conflated that with the idea that it wouldn't be big in the future. Clifford Stoll's piece also —<a href="https://www.latimes.com/business/hiltzik/la-fi-mh-actually-that-offbase-20150227-column.html?ref=wheresyoured.at"> <u>as Michael Hiltzik wrote for the LA Times</u></a> — was alarmingly accurate about misinformation and sleazy companies selling computerized replacements for education.<ul><li><strong><em>In any case, one guy saying that the internet won't be big doesn't mean a fucking thing about generative AI and you are a simpleton if you think it does. One guy being wrong in some way is not a response to my work. I will crush you like a bug.</em></strong></li><li>Stoll's analysis also isn't based on hundreds of hours of research and endless reporting. Mine is! I will grab you from the ceiling like the Wallmaster from Zelda and you will never be heard from again.</li></ul></li><li><strong>OpenAI and Anthropic are research entities not businesses, they aren't focused on profit:</strong> Okay so are they just going to burn money forever? No, really, is that the case? Or do you think they hit the "be profitable" button sometime?</li></ul><h3 id="record-scratch-wait-a-second"><strong>[Record Scratch] Wait a second...</strong></h3><ul><li><strong>OpenAI has as many as 800 million weekly active users! That's proof of adoption! </strong>Hey, woah, I get that you're really horny about this number, but something don't make no sense here! On March 31 2025, OpenAI said that it had "<a href="https://openai.com/index/march-funding-updates/?ref=wheresyoured.at"><u>...500 million people who use ChatGPT every week</u></a>." <em>Two weeks later, </em>Sam Altman claimed that "something like 10% of the world "uses our systems a lot,"<a href="https://fortune.com/2025/04/14/sam-altman-openai-user-base-doubled-few-weeks-10-of-world-uses-system/?ref=wheresyoured.at"> <u>which the media took to mean that ChatGPT has 800 million weekly active users</u></a>.</li><li><strong>Here are the three ways to interpret this, and you tell me which one sounds real:</strong><ul><li><strong>OpenAI's userbase increased by 300 million weekly active users in two weeks.</strong></li><li><strong>OpenAI understated its userbase <em><u>in the announcement of their funding announcement on OpenAI dot com</u></em></strong> <strong>by 300 million users.</strong></li><li><strong>Sam Altman <em>fucking lied.</em></strong></li></ul></li></ul><p>I get that some members of the media have a weird attachment to this nasty little man, but have any of you ever considered he’s just <em>fucking says things knowing you will print them with the kindest possible interpretation?</em></p><p><a href="https://www.wheresyoured.at/sam-altman-is-full-of-shit/"><u>Sam Altman is a liar! He lies! He's lied before and he'll lie again!</u></a></p><p><em><strong>But wait, Ed! Google says it has 350 million monthly active users on Gemini! Eat shit, Zitron! </strong></em>No, <em>you</em> eat shit! Yes, <a href="https://techcrunch.com/2025/04/23/google-gemini-has-350m-monthly-users-reveals-court-hearing/?ref=wheresyoured.at"><u>Google Gemini has 350 million monthly active users</u></a><u>.</u></p><p><a href="https://www.pcworld.com/article/2638233/so-long-google-assistant-its-geminis-world-now.html?ref=wheresyoured.at"><em><u>And that’s because it started replacing Google Assistant with Google Gemini in early March</u></em></a><em>! You are being <strong>had! You are being swindled! </strong></em>If Google replaced Google Search with Google Gemini it would have billions of monthly active users!&nbsp;</p><h3 id="anyway-back-to-the-critiques"><strong>Anyway, back to the critiques...</strong></h3><ul><li>OpenAI having hundreds of millions of <em>free users, each losing it money</em>, is proof that the <em>free version of ChatGPT is popular, </em>largely because <em>the entirety of the media has written about AI nonstop for two straight years and mentioned ChatGPT every single fucking time. </em><strong>Yes</strong> there is a degree here of marketing, of partnerships, of word of mouth, of <em>some degree of utility</em>, but remove the non-stop free media campaign and ChatGPT would've peetered out by now along with this stupid fucking bubble.<ul><li><em><strong>But Ed it's proof of something right-</strong></em> yeah! It's proof that something is broken in society. Generative AI has never had the kind of meaningful business returns or utility that actually underpins something meaningful, but it has <em>enough to make people give it a try.</em></li></ul></li></ul><h2 id="you-know-what-lets-talk-about-why-this-bubble-actually-inflated"><strong>You know what? Let's talk about why this bubble actually inflated!</strong></h2><p>So, let's start simple: the term "artificial intelligence" is bastardized to the point it effectively means nothing and everything at the same time. When people hear "AI" they think of an autonomous intelligence that can do things <em>for them</em>, and generative AI can "do things for you" like generate an image or text "from a simple prompt." As a result, it's easy to manipulate people who don't know much about tech into believing that this will naturally progress from "it can create a bunch of text for me that I have to write for my job just by me typing in a prompt" to "it can do my job for me just by typing in a prompt."</p><p>Basically everything you read about "the future of AI" extrapolates generative AI's ability to <em>sort of generate something a human would make</em> and turns it into <em>do whatever a human can do</em>, all because tech has, in the past, been bad at the beginning and linearly improved as time drags on.&nbsp;</p><p>This illogical thinking underpins the entire generative AI boom, because we've found out exactly how many people do not know what the fuck they're talking about and are willing to believe the last semi-intelligent person they talked to. Generative AI is a remarkable con — a just-good-enough simulacrum of human expression to get it past the gatekeepers in finance and the media, knowing that neither will apply a second gear of critical thinking beyond "huh guess we're doing AI now."</p><p>The expectation that generative AI will transform into something much, much more powerful requires you to first ignore the existing limitations, believing it to be more capable than it is, and also ignore the fact that these models have yet to show meaningful improvement over the past few years. They still hallucinate. They’re still ungodly expensive to run. They’re still unreliable. <em>And they still don’t do much</em>.&nbsp;&nbsp;</p><p>Worse still, ChatGPT's growth has galvanized these people into believing that this is a legitimate, meaningful movement, rather than the most successful PR campaign of all time.&nbsp;&nbsp;</p><p>Think of it like this: if almost every single media outlet talked about one thing (generative AI), and that one thing was available from one company (OpenAI), wouldn't it look exactly how things look today? You've got OpenAI with hundreds of millions of monthly active users, and then a bunch of other companies — including big tech firms with multi-trillion dollar market caps —<a href="https://www.wheresyoured.at/wheres-the-money/#:~:text=Is%20Generative%20AI%20A%20Real%20Industry%3F"> <u>with somewhere between 10 and 69 million monthly active users</u></a>.</p><p>What we're seeing is one company taking most of the users and money available and doing so <em>because the media fucking helped them.</em>&nbsp; People aren't amazed by ChatGPT — they're curious! They're curious about why the media won't shut up about it!</p><h2 id="this-bubble-was-also-inflated-by-the-failure-of-google-search"><strong>This Bubble Was Also Inflated By The Failure of Google Search</strong></h2><p>Everybody I talk to that uses ChatGPT regularly uses it as either a way to generate shitty limericks or as a replacement for Google search,<a href="https://www.wheresyoured.at/the-men-who-killed-google/"> <u>a product that Google has deliberately made worse as a means of increasing profits</u></a>.</p><p>ChatGPT is, if I'm honest, better at processing search strings than Google Search, which is not so much a sign that ChatGPT is <em>good</em> at something as it is that <em>Google has stopped innovating in any meaningful way.</em> Over time, Google Search should've become something that was able to interpret your searches into the perfect result, which would require the company to <em>improve how it processes your requests.</em> Instead, Google Search has become <em>dramatically worse</em>, mostly because the company's incentives changed from "help people find something on the web" to "funnel as much traffic and show as many ad impressions as possible on Google.com."</p><p>By this point, Google Search should have been <em>more</em> magical, <em>more</em> capable of taking a dimwitted question and turning it into a great answer, with said answer being a result on the internet. Note that nothing I'm writing here is actually about generating a result — it's about processing a user's query and presenting an answer, the very foundation of computing and the thing that Google, at one point, was the best in the world at doing.<a href="https://podcasts.apple.com/us/podcast/the-man-that-destroyed-google-search/id1730587238?i=1000653621646&amp;ref=wheresyoured.at"> <u>Thanks to Prabhakar Raghavan</u></a>, the former head of ads that led a coup to become head of search, Google was pulled away from being a meaningful source of information.</p><p>And I'd argue that ChatGPT filled that void by doing the thing that people wanted Google Search to do: answer a question, even if the user isn't really sure how to ask it. Google Search has become clunky, obfuscatory, putting the burden of using the service on the user rather than helping fill the gap between query and answer in any meaningful way. Google's AI summaries don't even try to do what ChatGPT does — they generate summaries based on search results and say "okay man, uhh, is this what you want?"&nbsp;</p><blockquote>One note on Google’s AI summaries: They’re designed to answer a question, rather than provide a right answer. That’s a distinction that needs to be made, because it speaks to the underlying utility of this product.&nbsp;<p>One good illustration of this came earlier this week, when someone noticed that you could ask Google to explain the meaning of a completely made-up phrase, and it would dutifully obey. “</p><a href="https://x.com/rose_matt/status/1915355300506325282?ref=wheresyoured.at"><u>Two dry frogs in a situation</u></a>,” Google said, referred to a group of people in an awkward or difficult social situation.&nbsp;<p>“</p><a href="https://x.com/enfyscheese/status/1915356621716307979?ref=wheresyoured.at"><u>Not every insect has a mortgage</u></a>,” Google claimed,” is a humorous way of explaining that not everything is as it seems. My favorite, “<a href="https://x.com/shagbark_hick/status/1915560564065218845?ref=wheresyoured.at"><u>big winky on the skillet bowl</u></a>,” is apparently a slang term that refers to a piece of bread with an egg in the middle.&nbsp;<p>Funny? Sure. But is it useful? No.&nbsp;</p></blockquote><p>With all its data and all its talent, Google has put the laziest version of a Large Language Model on top of a questionably-functional search product as a means of impressing shareholders.</p><p>None of this is to say that ChatGPT is <em>good</em>, just that it is better at understanding a user's request than Google Search.</p><p>Yes, I fundamentally believe that 500 million people a week could be using ChatGPT as some sort of search replacement, and no, I do not believe that's a functional business model, in part because if it was, ChatGPT would've been a functional business.&nbsp;</p><p>That, and it appears that Google's ability to turn search into such a big business was because<a href="https://www.cnbc.com/2025/04/17/google-hit-with-second-antitrust-blow-adding-to-concerns-about-ads.html?ref=wheresyoured.at"> <u>it held a monopoly on search, search advertising and the entire online ads industry</u></a>, and if it was a truly competitive market and it wasn’t allowed to be vertically integrated with the entire digital advertising apparatus of the web, it would likely be making much less revenue per user. And that’s bad if your Google Replacement costs many, many times more than Google to run.&nbsp;</p><blockquote>As an aside: if you're wondering, no, OpenAI cannot "just create a Google Search competitor."<a href="https://www.wheresyoured.at/burst-damage/#:~:text=What%27s%20that%3F%20SearchGPT%3F%20Sam%2C%20you%27re%20crazy!%20There%27s%20no%20way%20you%20can%20build%20a%20Google%20Search%20competitor!%20Trust%20me%2C%20I%20know.%C2%A0"> <u>SearchGPT will be significantly more expensive to run at Google's scale than ChatGPT</u></a> — both infrastructurally and in the cost of revenue, with OpenAI forced to create a massive advertising arm that currently doesn't exist at the company.</blockquote><p>People love the ChatGPT interface — the box where they can type one thing and get another thing out — because it resembles how everybody has always wanted Google Search to work. Does it actually work? Who knows. But people feel like they're getting more out of it.</p><h2 id="lets-talk-about-agi-really-quick"><strong>Let's Talk About AGI Really Quick</strong></h2><p>This newsletter has been a break from the extremely deep and onerous analysis I've been on for the last few months, in part because I needed to have a little fun writing.</p><p>It also comes from a place of frustration. None of this has ever felt substantive or real because the actual things that you can do with generative AI never seem to come close to the things that people like Sam Altman and Dario Amodei seem to be promising, nor do they come close to the bullshit that people like Casey Newton and<a href="https://bsky.app/profile/edzitron.com/post/3ln4p3ms7522j?ref=wheresyoured.at"> <u>Kevin Roose are peddling</u></a>. None of this ever resembled "artificial general intelligence," and if I'm honest, very little of it seems to even suggest it's a functional industry.</p><p>When cynical plants like Roose bumble around asking theoretical questions such as "<a href="https://bsky.app/profile/edzitron.com/post/3ln4p3ms7522j?ref=wheresyoured.at"><u>do you think that there is a 50% chance or greater that AGI, defined as an AI system that outperforms human experts at virtually all cognitive tasks, will be built before 2030</u></a>," we should all be terrified, not of AGI, but that the lead tech columnist at the New York Times appears to have an undiagnosed concussion. Roose's logic (as with Newton's) is based on the idea that he's talked to a bunch of people that say "yeah dude AGI is right around the corner" rather than any kind of proof or tangible evidence, just "the curve is going up."</p><p>Roose’s most egregious example of this company-forward credulousness came last week, when he published <a href="https://www.nytimes.com/2025/04/24/technology/ai-welfare-anthropic-claude.html?ref=wheresyoured.at"><u>a thinly-veiled puff piece</u></a> about what to do if AI models become conscious in the near future. He interviewed two people — both employed by Anthropic, with one holding the genuinely hilarious job description of “AI welfare researcher” — who said batshit things like “there’s only a small chance (maybe 15 percent or so) that Claude or another current A.I. system is conscious” and “It seems to me that if you find yourself in the situation of bringing some new class of being into existence… then it seems quite prudent to at least be asking questions about whether that system might have its own kinds of experiences.”</p><p>What makes this so appalling is that Roose acknowledges that this shit is seen by most level-headed people as nothing less than utter fantasy. He describes the concept of AI consciousness as “a taboo subject” and that many critics will see this as “crazy talk,” but doesn’t bother to speak to any actual critics. He does, however, speculate on the motives of said critics, saying that “they might object to an A.I. company’s studying consciousness in the first place, because it might create incentives to train their systems to act more sentient than they actually are.”</p><p>Yeah Kevin, wouldn’t it be terrible if a company somehow convinced someone that their AI was more powerful than it was? Also, do you bark at the mirror every time you walk past it because you think you see another guy?</p><p>Nothing about anything that Anthropic or OpenAI is building or shipping suggests we are anywhere near any kind of autonomous computing. They've used the concept of "AI safety" — and now, AI welfare — as a marketing term to convince people that their expensive, wasteful software will somehow become conscious because they're having discussions about what to do if it does so, and anyone — literally any reporter — accepting this at face value is doing their readers a disservice and embarrassing themselves in the process.</p><p>If AI safety advocates cared about, say, safety or AI, they'd have cared about the environmental impact, or the fact these models train using stolen material, or the fact that if these models actually delivered on their promises, it would deliver a shock to the labor market that would meaningfully hurt millions — if not billions — of people, and we don’t have anywhere near the social safety net to support them.&nbsp;</p><p>These companies don't care about your safety and they don't have any way to get to AGI. They are full of shit and it's time to start being honest that you don't have any proof they will do <em>anything</em> they say they will.</p><h2 id="oh-by-the-way-the-bubble-might-be-bursting"><strong>Oh, By The Way, The Bubble Might Be Bursting</strong></h2><p>Hey, remember in August of last year when I talked about the pale horses of the AIpocalpyse? One of the major warning signs that the bubble was bursting was<a href="https://www.wheresyoured.at/burst-damage/#:~:text=Any%20suggestion%20that%20Google%20or%20Microsoft%20is%20reducing%20their%20capex%3A%20Venture%20capital%20isn%E2%80%99t%20really%20what%E2%80%99s%20propping%20up%20generative%20AI%20%E2%80%94%20it%E2%80%99s%20Google%20or%20Microsoft.%20If%20either%20of%20them%20decide%20it%E2%80%99s%20time%20to%20slow%20down%20investment%2C%20the%20boom%20is%20done%2C%20as%20referenced%20above."> <u>big tech firms reducing their capital expenditures</u></a>, a call I've made before, with a little more clarity, on April 4 2024:</p><blockquote>While I hope I'm wrong, the calamity I fear is one where the massive over-investment in data centers is met with a lack of meaningful growth or profit, leading to the markets turning on the major cloud players that staked their future on unproven generative AI. If businesses don't adopt AI <em>at scale</em> — not experimentally, but at the core of their operations — the revenue is simply not there to sustain the hype, and once the market turns, it will turn hard, demanding efficiency and cutbacks that will lead to tens of thousands of job cuts.</blockquote><p>We're about to find out if I'm right.</p><p>Last week, Yahoo Finance reported that analyst Josh Beck<a href="https://finance.yahoo.com/news/amazon-stock-falls-as-raymond-james-downgrades-shares-citing-tariff-headwinds-and-limited-ai-monetization-144747355.html?ref=wheresyoured.at"> <u>said that Amazon's generative AI revenue for Amazon Web Services would be $5 billion</u></a>, a remarkably small sum that is A) not profit and B) a drop in the bucket compared to Amazon's projected $105 billion in capital expenditures in 2025, its $78.2 billion in 2024, or its $48.4 billion in 2023.</p><h3 id="is-that-really-it-are-you-kidding-me-amazon-will-only-make-5-billion-from-ai-in-2025-what"><strong>Is That Really It? Are you kidding me? Amazon will only make $5 billion from AI in 2025? What?</strong></h3><p>5 billion dollars? <em>Five billion god damn dollars? Are you fucking kidding me?</em><a href="https://www.youtube.com/watch?v=G07sWzYObnk&amp;ref=wheresyoured.at"> <u>You'd make more money auctioning dogs</u></a>! This is a disgrace! And if you're wondering, yes! All of this is for AI:</p><blockquote>CEO Andy Jassy said in February that the vast majority of this year’s $100 billion in capital investments from the tech giant will go toward building out artificial intelligence capacity for its cloud segment, Amazon Web Services (AWS).</blockquote><p>Well shit, I bet investors are gonna love this! Better save some money, Andy!</p><p>What's that? You already did? How?</p><p>Oh, shit!<a href="https://www.reuters.com/business/retail-consumer/amazon-has-halted-some-data-center-leasing-talks-wells-fargo-analysts-say-2025-04-21/?ref=wheresyoured.at"> <u>A report from Wells Fargo analysts</u></a> (called "Data Centers: AWS Goes on Pause") says that Amazon has "paused a portion of its leasing discussions on the colocation side...[and while] it's not clear the magnitude of the pause...the positioning is similar to what [analysts have] heard recently from Microsoft, [that] they are digesting aggressive recent lease-up deals...pulling back from a pipeline of LOIs or SOQs."</p><blockquote>Some asshole is going to say "LOIs and SOQs aren't a big deal," but they <em>are</em>. I wrote about it<a href="https://www.wheresyoured.at/power-cut/#:~:text=Sidebar%3A%20Let%27s%20explain%20some%20terms!"> <em><u>here</u></em></a><em>.</em></blockquote><p>"Digesting" in this case refers to when hyperscalers sit with their current capacity for a minute, and Wells Fargo adds that these periods typically last 6-12 months, though can be much shorter. It's not obvious <em>how much</em> capacity Amazon is walking away from, but they are walking away from capacity. <em>It's happening.</em></p><p>But what if it wasn't just Amazon? Another report from friend of the newsletter (read: people I email occasionally asking for a PDF) analyst TD Cowen put out a report last week that, while titled in a way that suggested there wasn't a pull back, actually said there was.</p><p>Let's take a look at one damning quote:</p><blockquote>...relative to the hyperscale demand backdrop at PTC, hyperscale demand has moderated a bit (driven by the Microsoft pullback and to a lesser extent Amazon, discussed below), particularly in Europe, 2) there has been a broader moderation in the urgency and speed with which the hyperscalers are looking to take down capacity, and 3) the number of large deals (i.e. +400MW deals) in the market appears to have moderated.</blockquote><p>In plain English, this means "demand has come down, there's less urgency in building this stuff, and the market is slowing down. Cowen also added that it "...observed a moderation in the exuberance around the outlook for hyperscale demand which characterized the market this time last year."&nbsp;</p><p>Brother, isn't this meant to be the next big thing? We need more exuberance! Not less!</p><p>Worse still, Microsoft appears to have pulled back <em>even further</em>, with TD Cowen noting that there has been a "slowdown in demand," and that it saw "very little third-party leasing from Microsoft" this quarter, and, most damningly, and I'll bold this for effect, "<strong>these deals in totality suggest Microsoft's run-rate demand has decelerated materially,</strong>" which for those of you wondering means <strong><em>it’s not getting the fucking demand for generative AI.</em></strong></p><p>Well, at least Meta and Oracle aren't slowing down, right?</p><p><em>Well...</em></p><p>TD Cowen reported that it received "reverse inquiries from industry participants around a potential slowdown in demand from Oracle," leading the analyst to ask around and find that "there had been a NT (near-term) slowdown in decision-making amid organizational changes at Oracle," though it adds this might not mean that this is changing its needs or the speed at which it secures capacity. If you're wondering what else this could mean, you are correct to do so, because "slowing down" traditionally refers to a change in speed.</p><p>TD Cowen also adds that Meta has continued demand "albeit with less volume of MW (Megawatt) signings quarter-over-quarter..." then adding that "Meta's data center activity has historically been characterized by short periods of strong activity followed by digestion." In essence, Meta is signing less megawatts of compute and has, in the past, followed periods of aggressive buildouts with, well, fewer buildouts.</p><h2 id="if-im-wrong-how-am-i-wrong-exactly"><strong>If I'm Wrong, How Am I Wrong Exactly?</strong></h2><p>I dunno man, all of this sure seems like the hyperscalers are reducing their capital expenditures at a time when tariffs and economic uncertainty are making investors more critical of revenues. It sure seems like nobody outside of OpenAI is making any real revenue on generative AI, and they're certainly not making a profit.</p><p>It also, at this point, is pretty obvious that generative AI isn't going to do much more than it does today. If Amazon is only making $5 billion in revenue from <em>the literal only shiny new thing it has, sold on the world's premier cloud platform, at a time when businesses are hungry and desperate to integrate AI,</em> then there's little chance this suddenly turns into a remarkable revenue-driver.</p><p>Amazon made <em>$187.79 billion in its last quarterly earnings</em>, and if $5 billion is all it’s making at the very height of the bubble, it heavily suggests that there may not actually be that much money to make, either because it's too expensive to run these services or because these services don't have the kind of total addressable market as the rest of Amazon's services.</p><p><a href="https://www.geekwire.com/2025/microsoft-earnings-2/?ref=wheresyoured.at"><u>Microsoft reported that it was making a paltry $13 billion a year</u></a> — so the equivalent of $3.25 billion a quarter — selling generative AI services and model access.<a href="https://www.theinformation.com/articles/ai-giving-salesforce-boost?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>The Information reported that Salesforce's "Agentforce" bullshit isn't even going to boost sales growth in 2025</u></a>, in part because it’s pitching it as "digital labor that can essentially replace humans for tasks" and it turns out that it doesn't do that very well at all, costs $2 a conversation, and requires paying Salesforce to use its "data cloud" product.</p><p>What, if anything, suggests that I'm wrong here? That things have worked out in the past with things like the Internet and smartphones, and so it surely must happen for generative AI and, by extension, OpenAI? That companies like Uber lost money and eventually worked out (<a href="https://www.wheresyoured.at/to-serve-altman/#:~:text=As%20another%20aside%2C%20Uber%2C%20a%20company%20famed%20for%20burning%20%2425%20billion%20dollars%20to%20achieve%20profitability%2C%20raised%20a%20total%20of...well%2C%20%2425%20billion%2C%20which%20included%20four%20different%20funding%20rounds%20in%20the%20year%202018%20alone."><u>see my response here</u></a>)? That OpenAI is growing fast, and that somehow discounts the fact it burns billions of dollars and does not appear to have any path to making a profit? That agents will suddenly start working and everything will be fine?</p><p>It's a fucking joke and I'm tired of it!</p><p>Large Language Models and their associated businesses are a $50 billion industry masquerading as a trillion-dollar panacea for a tech industry that’s lost the plot. Silicon Valley is dominated by management consultants that no longer know what innovation looks like, tricked by Sam Altman, a savvy con artist who took advantage of tech’s desperation for growth.&nbsp;</p><p>Generative AI is the perfected nihilistic form of tech bubbles — a way for people to spend a lot of money and power on cloud compute because they don’t have anything better to do. Large Language Models are boring, unprofitable cloud software stretched to their limits — both ethically and technologically — as a means of tech’s collapsing growth era, OpenAI’s non-profit mission fattened up to make foie gras for SaaS companies to upsell their clients and cloud compute companies to sell GPUs at an hourly rate.&nbsp;</p><p><a href="https://www.wheresyoured.at/the-rot-economy/" rel="noreferrer">The Rot Economy</a> has consumed the tech industry. Every American tech firm has become corrupted by the growth-at-all-costs mindset, and thus they no longer know how to make sustainable businesses that solve real problems, largely because the people that run them haven’t experienced them for decades.&nbsp;</p><p>As a result, none of them were ready for when Sam Altman tricked them into believing he was their savior.&nbsp;</p><p>Generative AI isn’t about helping you or me do things — it’s about making new SKUs, new monthly subscription costs for consumers and enterprises, new ways to convince people to pay more for the things that they already used to be slightly different in a way that often ends up being worse.&nbsp;</p><p>Only an industry out of options would choose this bubble, and the punishment for doing so will be grim. I don’t know if you think I’m wrong or not. I don’t know if you think I’m crazy for the way I communicate about this industry. Even if you think I am, think long and hard about why it is you disagree with me, and the consequences of me being wrong.&nbsp;</p><p>There is nothing else after generative AI. There are no other hypergrowth markets left in tech. SaaS companies are out of things to upsell. Google, Microsoft, Amazon and Meta do not have any other ways to continue showing growth, and when the market works that out, there will be hell to pay, hell that will reverberate through the valuations of, at the very least, every public software company, and many of the hardware ones too.</p><p>And I fear it'll go much further, too. The longer this bubble inflates - the longer everybody pretends - the worse the consequences will be.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Books of Earthsea by Ursula K. Le Guin (113 pts)]]></title>
            <link>https://lars.ingebrigtsen.no/2025/04/28/book-club-2025-the-books-of-earthsea-by-ursula-k-le-guin/</link>
            <guid>43823462</guid>
            <pubDate>Mon, 28 Apr 2025 16:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lars.ingebrigtsen.no/2025/04/28/book-club-2025-the-books-of-earthsea-by-ursula-k-le-guin/">https://lars.ingebrigtsen.no/2025/04/28/book-club-2025-the-books-of-earthsea-by-ursula-k-le-guin/</a>, See on <a href="https://news.ycombinator.com/item?id=43823462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28.jpeg"><img decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-scaled.jpeg" alt="" width="840" height="1050" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-scaled.jpeg 2048w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-240x300.jpeg 240w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-819x1024.jpeg 819w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-768x960.jpeg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-1229x1536.jpeg 1229w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-1638x2048.jpeg 1638w" sizes="(max-width: 840px) 100vw, 840px"></a></p><p>I discovered that they’d published a complete, illustrated version of the Earthsea cycle a couple weeks ago.  I’m not overly fond of omnibus editions, but this is illustrated by Charles Vess, and I love his artwork, so I thought that this might be a good time to re-read these books.</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-1.jpeg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-1.jpeg" alt="" width="33" height="15"></a><br> <a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-2.jpeg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/2025-04-28-2.jpeg" alt="" width="191" height="300"></a></p><p>Because of course I’ve read these books before.  I read the first one (in translation) when I was about 10-11 years old, and it had the cover above.  I had that book, so I read it several times, but the other two of the first trilogy I borrowed from the library, so I probably just read them once?</p><p>(Man, that’s a weird cover.  Le Guin complains about horrible covers, but she doesn’t mention this one.)</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01900-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>This book is massive.  It’s just almost 1K pages, but they’re big pages.  Looking at <a href="https://www.goodreads.com/series/40909-earthsea-cycle" data-cached-time="2025-04-28T02:10:17" data-cached-image="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/cache-2025-04-28-web-scaled.webp" onmouseenter="hoverLink(event)">the original editions</a>, it looks like the six books altogether were 1,400 pages, and this one also includes some other short stories and stuff, so there’s some heft to this book.</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01901-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>Vess does illustrations of key scenes, as well as title plates…</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01902-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>… and one colour piece per book.</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01906-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>Charles Vess has done some extremely stylish comics, and those are mostly pen and ink.  This is just pencil, and of course it’s good looking, but I have to admit I’m a bit disappointed.</p><p>I was also going to quibble about how he depicts some things, like the dragons, but the artwork was done over four years, and in collaboration with Le Guin.  I guess if she say’s that that’s correct, I can’t really argue.  Darn!</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01903-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01904-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>Now, as for the text itself…  I mean, just reading it, I don’t think this format is ideal?  I guess they had to limit themselves to under 1K pages or something, so they had to step down the font size slightly.  I mean, it’s not uncomfortable, but it’s just a smidgen too small.  And since the pages are so big, they felt the need to increase the line height, so that your eyes can snap back to the left side of the page without losing place.</p><p>I just thing the look of these pages isn’t ideal.  But it does give you the feeling you’re reading some old huge grimoire, which is probably what they were going for, and that’s fun.</p><p><a href="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805.jpg"><img loading="lazy" decoding="async" src="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-scaled.jpg" alt="" width="840" height="560" srcset="https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-scaled.jpg 2560w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-300x200.jpg 300w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-1024x683.jpg 1024w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-768x512.jpg 768w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-1536x1024.jpg 1536w, https://lars.ingebrigtsen.no/wp-content/uploads/2025/04/DSC01805-2048x1365.jpg 2048w" sizes="auto, (max-width: 840px) 100vw, 840px"></a></p><p>But since it’s uncomfortable to hold, and also slightly uncomfortable to read, I just read the old editions I had.  D’oh!  But I did look at the illustrations, too.</p><p>(And I couldn’t find The Other Wind, so I read that from this new collection, along with the afterwords and the extra included short stories and stuff.)</p><p>So how is it?  Presumably, you’ve all read these books — or at least the first three.</p><p>I had forgotten how oldee tymey the first book is.  It’s written in a style halfway between fairy tales and modern “classic” fantasy.  That is, people are introduced like “and then he met Grumbledork, who would go on to sail to Vinklebump and vanquish Zomplefart, the dragon, but that’s a story for another day”, etc etc.  As someone who hates fairy tales…  I really enjoyed it.  As much as I did when I was 11.</p><p>I think the second book, The Tombs of Atuan is generally considered to be the best?  It’s written in a very different way, and it is indeed very good.  But I think of the first three books, I prefer the third, The Farthest Shore.  It’s like a remake of the first book (a road movie, but at sea), but the encounters are more memorable and vivid — like the people who live at sea all their lives, in the floating cities, and so on.  And it has the most moving end.</p><p>Then 30 years passed, and the fourth book came, Tehanu.  If you were 11 years old in 1991, and read the four books in quick succession, you’d get whiplash when you got to the fourth one: It starts with a five year old girl who was raped and then put into a campfire to burn to death.  (And that’s just the first page.)  But if you grew up with the first three books, and then got to the fourth as an adult (as I did), then it seemed quite natural:  It’s a brutal, angry book, and it’s fantastic.</p><p>Ten more years passed, and we got a short story collection — but times have moved on, and “short” isn’t the same as in the olden days.  The first story in the collection is about two thirds the length of the first novel.  All the stories are solid, and we retrench a lot from the harshness of Tehanu.  Le Guin says she wrote the stories to figure out herself how to finish off the series with the fifth novel, and to explore things about Earthsea.  Now, nothing strikes more fear into a reader’s heart than an author who wants to “explore things” in their universe: You’re likely to get a story filling you in on the “lore” of the second lieutenant mentioned in half of a sentence in book two, and nothing in the short story must affect the “proper” novels whatsoever.  But of course, Le Guin does nothing of the kind: She writes interesting stories that fill in the milieu and enriches your love for the world, and not trite info dumps or backstory you’ve never asked for.</p><p>And finally, the sixth and final book came hot on the heels of the fifth, and it’s a novel that manages to be a wonderful ending to the entire series.  I had totally forgotten that.  It echoes the first and third books storytelling wise, and concludes the storylines started in the fourth book, while starring (sort of) Tenar from the second book.  Some people when they finish a series like this seem to have a checklist of points they go through and make everything neat — Le Guin is way too smart to do something like that, but it’s a really solid ending to the cycle.</p><p>(Except that there are a few more short stories included in this collecion.  The final story was published in The Paris Review originally in 2018, after Le Guin had died, and is a very moving coda.)</p><p>So to sum up: These books are still darn good, and even better than I remembered.</p><p>And now I want to read something that’s not fantasy.</p><p><b>The Books of Earthsea (2018)</b> by Ursula K. Le Guin (<a screenshot="true" href="https://bookshop.org/book/9781481465588">buy new</a>, <a screenshot="true" href="https://www.biblio.com/9781481465588">buy used</a>, <a screenshot="true" href="https://www.goodreads.com/search?q=9781481465588">4.46 on Goodreads</a>)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Sim Studio – Open-Source Agent Workflow GUI (158 pts)]]></title>
            <link>https://github.com/simstudioai/sim</link>
            <guid>43823096</guid>
            <pubDate>Mon, 28 Apr 2025 16:14:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/simstudioai/sim">https://github.com/simstudioai/sim</a>, See on <a href="https://news.ycombinator.com/item?id=43823096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/simstudioai/sim/blob/main/sim/public/static/sim.png"><img src="https://github.com/simstudioai/sim/raw/main/sim/public/static/sim.png" alt="Sim Studio Logo" width="500"></a>
</p>
<p dir="auto">
  <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/859a1a0bc85ce8bbd7a730a274fec5c9e77c4726ffdf6aa762a78685e26033a4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667" alt="License: Apache-2.0" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg"></a>
  <a href="https://discord.gg/Hr4UWYEcTT" rel="nofollow"><img src="https://camo.githubusercontent.com/3fd3029bb1d53b3b6f05655b7763d43b6d1d761479537d4c6bd9062e2646fffa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d4a6f696e2532305365727665722d3732383944413f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-Join%20Server-7289DA?logo=discord&amp;logoColor=white"></a>
  <a href="https://x.com/simstudioai" rel="nofollow"><img src="https://camo.githubusercontent.com/4adaee445a27f63f4d71fcbc27f9e22981c30561f9fe677caefe22b8c8bd883a/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f73696d73747564696f61693f7374796c653d736f6369616c" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/simstudioai?style=social"></a>
  <a href="https://github.com/simstudioai/sim/pulls"><img src="https://camo.githubusercontent.com/d88d8d77fa79e828eea397f75a1ebd114d13488aeec4747477ffbd2274de95ed/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e2e737667" alt="PRs welcome" data-canonical-src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg"></a>
  <a href="https://github.com/simstudioai/sim/issues"><img src="https://camo.githubusercontent.com/3e9fd289aaf4070825ad7ea65b389d4b185c4e1986208abf13110dcc8d1dc61c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737570706f72742d636f6e74616374253230617574686f722d707572706c652e737667" alt="support" data-canonical-src="https://img.shields.io/badge/support-contact%20author-purple.svg"></a>
</p>
<p dir="auto">
  <strong>Sim Studio</strong> is a powerful, user-friendly platform for building, testing, and optimizing agentic workflows.
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run</h2><a id="user-content-run" aria-label="Permalink: Run" href="#run"></a></p>
<ol dir="auto">
<li>Run on our <a href="https://simstudio.ai/" rel="nofollow">cloud-hosted version</a></li>
<li>Self-host</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Self-Host</h2><a id="user-content-how-to-self-host" aria-label="Permalink: How to Self-Host" href="#how-to-self-host"></a></p>
<p dir="auto">There are several ways to self-host Sim Studio:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 1: Docker Environment (Recommended)</h3><a id="user-content-option-1-docker-environment-recommended" aria-label="Permalink: Option 1: Docker Environment (Recommended)" href="#option-1-docker-environment-recommended"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone your forked repository
git clone https://github.com/YOUR_USERNAME/sim.git
cd sim

# Create environment file and update with required environment variables (BETTER_AUTH_SECRET)
cp sim/.env.example sim/.env

# Start Sim Studio using the provided script
docker compose up -d --build

or

./start_simstudio_docker.sh"><pre><span><span>#</span> Clone your forked repository</span>
git clone https://github.com/YOUR_USERNAME/sim.git
<span>cd</span> sim

<span><span>#</span> Create environment file and update with required environment variables (BETTER_AUTH_SECRET)</span>
cp sim/.env.example sim/.env

<span><span>#</span> Start Sim Studio using the provided script</span>
docker compose up -d --build

or

./start_simstudio_docker.sh</pre></div>
<p dir="auto">After running these commands:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Access the Application</strong>:</p>
<ul dir="auto">
<li>Open <a href="http://localhost:3000/w/" rel="nofollow">http://localhost:3000/w/</a> in your browser</li>
<li>The <code>/w/</code> path is where the main workspace interface is located</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Useful Docker Commands</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# View application logs
docker compose logs -f simstudio

# Access PostgreSQL database
docker compose exec db psql -U postgres -d simstudio

# Stop the environment
docker compose down

# Rebuild and restart (after code changes)
docker compose up -d --build"><pre><span><span>#</span> View application logs</span>
docker compose logs -f simstudio

<span><span>#</span> Access PostgreSQL database</span>
docker compose <span>exec</span> db psql -U postgres -d simstudio

<span><span>#</span> Stop the environment</span>
docker compose down

<span><span>#</span> Rebuild and restart (after code changes)</span>
docker compose up -d --build</pre></div>
</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">Working with Local Models</h4><a id="user-content-working-with-local-models" aria-label="Permalink: Working with Local Models" href="#working-with-local-models"></a></p>
<p dir="auto">To use local models with Sim Studio, follow these steps:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Pull Local Models</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run the ollama_docker.sh script to pull the required models
./sim/scripts/ollama_docker.sh pull <model_name>"><pre><span><span>#</span> Run the ollama_docker.sh script to pull the required models</span>
./sim/scripts/ollama_docker.sh pull <span>&lt;</span>model_name<span>&gt;</span></pre></div>
</li>
<li>
<p dir="auto"><strong>Start Sim Studio with Local Models</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="#Start Sim Studio with local model support
./start_simstudio_docker.sh --local

# or

# Start Sim Studio with local model support if you have nvidia GPU
docker compose up --profile local-gpu -d --build

# or

# Start Sim Studio with local model support if you don't have nvidia GPU
docker compose up --profile local-cpu -d --build"><pre><span><span>#</span>Start Sim Studio with local model support</span>
./start_simstudio_docker.sh --local

<span><span>#</span> or</span>

<span><span>#</span> Start Sim Studio with local model support if you have nvidia GPU</span>
docker compose up --profile local-gpu -d --build

<span><span>#</span> or</span>

<span><span>#</span> Start Sim Studio with local model support if you don't have nvidia GPU</span>
docker compose up --profile local-cpu -d --build</pre></div>
</li>
</ol>
<p dir="auto">The application will now be configured to use your local models. You can access it at <a href="http://localhost:3000/w/" rel="nofollow">http://localhost:3000/w/</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 2: Dev Containers</h3><a id="user-content-option-2-dev-containers" aria-label="Permalink: Option 2: Dev Containers" href="#option-2-dev-containers"></a></p>
<ol dir="auto">
<li>Open VS Code or your favorite VS Code fork (Cursor, Windsurf, etc.)</li>
<li>Install the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers" rel="nofollow">Remote - Containers extension</a></li>
<li>Open the project in your editor</li>
<li>Click "Reopen in Container" when prompted</li>
<li>The environment will automatically be set up in the <code>sim</code> directory</li>
<li>Run <code>npm run dev</code> in the terminal or use the <code>sim-start</code> alias</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option 3: Manual Setup</h3><a id="user-content-option-3-manual-setup" aria-label="Permalink: Option 3: Manual Setup" href="#option-3-manual-setup"></a></p>
<ol dir="auto">
<li><strong>Install Dependencies</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/YOUR_USERNAME/sim.git
cd sim/sim

# Install dependencies
npm install"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/YOUR_USERNAME/sim.git
<span>cd</span> sim/sim

<span><span>#</span> Install dependencies</span>
npm install</pre></div>
<ol start="2" dir="auto">
<li><strong>Set Up Environment</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Copy .env.example to .env
cp .env.example .env

# Configure your .env file with the required environment variables:
# - Database connection (PostgreSQL)
# - Authentication settings (Better-Auth Secret)"><pre><span><span>#</span> Copy .env.example to .env</span>
cp .env.example .env

<span><span>#</span> Configure your .env file with the required environment variables:</span>
<span><span>#</span> - Database connection (PostgreSQL)</span>
<span><span>#</span> - Authentication settings (Better-Auth Secret)</span></pre></div>
<p dir="auto"><g-emoji alias="warning">⚠️</g-emoji> <strong>Important Notes:</strong></p>
<ul dir="auto">
<li>If <code>RESEND_API_KEY</code> is not set, verification codes for login/signup will be logged to the console.</li>
<li>You can use these logged codes for testing authentication locally.</li>
<li>For production environments, you should set up a proper email provider.</li>
</ul>
<ol start="3" dir="auto">
<li><strong>Set Up Database</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Push the database schema
npx drizzle-kit push"><pre><span><span>#</span> Push the database schema</span>
npx drizzle-kit push</pre></div>
<ol start="4" dir="auto">
<li><strong>Start Development Server</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# Start the development server
npm run dev"><pre><span><span>#</span> Start the development server</span>
npm run dev</pre></div>
<ol start="5" dir="auto">
<li><strong>Open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a> in your browser</strong></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tech Stack</h2><a id="user-content-tech-stack" aria-label="Permalink: Tech Stack" href="#tech-stack"></a></p>
<ul dir="auto">
<li><strong>Framework</strong>: <a href="https://nextjs.org/" rel="nofollow">Next.js</a> (App Router)</li>
<li><strong>Database</strong>: PostgreSQL with <a href="https://orm.drizzle.team/" rel="nofollow">Drizzle ORM</a></li>
<li><strong>Authentication</strong>: <a href="https://better-auth.com/" rel="nofollow">Better Auth</a></li>
<li><strong>UI</strong>: <a href="https://ui.shadcn.com/" rel="nofollow">Shadcn</a>, <a href="https://tailwindcss.com/" rel="nofollow">Tailwind CSS</a></li>
<li><strong>State Management</strong>: <a href="https://zustand-demo.pmnd.rs/" rel="nofollow">Zustand</a></li>
<li><strong>Flow Editor</strong>: <a href="https://reactflow.dev/" rel="nofollow">ReactFlow</a></li>
<li><strong>Docs</strong>: <a href="https://fumadocs.vercel.app/" rel="nofollow">Fumadocs</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions! Please see our <a href="https://github.com/simstudioai/sim/blob/main/.github/CONTRIBUTING.md">Contributing Guide</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the Apache License 2.0 - see the <a href="https://github.com/simstudioai/sim/blob/main/LICENSE">LICENSE</a> file for details.</p>

<p dir="auto">Made with ❤️ by the Sim Studio Team</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A pure WebGL image editor with filters, crop and perspective correction (204 pts)]]></title>
            <link>https://github.com/xdadda/mini-photo-editor</link>
            <guid>43823044</guid>
            <pubDate>Mon, 28 Apr 2025 16:10:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xdadda/mini-photo-editor">https://github.com/xdadda/mini-photo-editor</a>, See on <a href="https://news.ycombinator.com/item?id=43823044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>GitHub Advanced Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code Review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>Code Search</p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      
      <div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      
      <div>
                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:xdadda/mini-photo-editor" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="q_7IPmEIStooSTXYel3i6A5hNJCIixIT2_hurZxYtw1MqERkvSufxC_yBRY_RMOrwtwevc1fpOETpNNQ8HO3Cw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="xdadda/mini-photo-editor" data-current-org="" data-current-owner="xdadda" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=xdadda%2Fmini-photo-editor" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/xdadda/mini-photo-editor&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="bcc2159001bb38ece52e7d700d10d76cbc8dc11bc69e5b5c5ce167e3cd70b80d" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>

              
          
        </p></div>
      </div></div>]]></description>
        </item>
    </channel>
</rss>