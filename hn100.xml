<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 13 May 2024 05:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: I made an open-source Loom alternative (184 pts)]]></title>
            <link>https://Cap.so</link>
            <guid>40338275</guid>
            <pubDate>Sun, 12 May 2024 23:09:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://Cap.so">https://Cap.so</a>, See on <a href="https://news.ycombinator.com/item?id=40338275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 719 269"><path fill="#0A486B" d="M474.954 156.466c-3.122 16.278-10.217 29.23-21.286 38.856-10.927 9.484-25.118 14.226-42.572 14.226-14.333 0-26.892-3.397-37.677-10.192-10.643-6.936-18.873-16.349-24.692-28.24-5.818-11.89-8.727-25.267-8.727-40.13 0-14.863 2.838-28.31 8.514-40.342 5.819-12.032 14.12-21.445 24.905-28.24C384.204 55.468 396.763 52 411.096 52c16.603 0 30.368 4.53 41.295 13.589 10.927 9.059 18.164 21.304 21.712 36.733l-27.885 1.486c-2.129-8.776-6.244-15.641-12.346-20.596-6.102-5.096-13.765-7.644-22.989-7.644-9.65 0-17.739 2.407-24.266 7.22-6.386 4.812-11.14 11.465-14.262 19.959-3.122 8.351-4.683 17.764-4.683 28.239 0 10.475 1.561 19.818 4.683 28.028 3.122 8.21 7.876 14.792 14.262 19.746 6.527 4.813 14.616 7.219 24.266 7.219 9.65 0 17.526-2.76 23.628-8.28 6.243-5.662 10.359-13.236 12.346-22.72l28.097 1.487zM486.826 127.801c2.271-11.607 7.734-20.596 16.391-26.965 8.656-6.512 19.796-9.768 33.419-9.768 16.177 0 28.452 4.106 36.825 12.316 8.372 8.21 12.559 20.171 12.559 35.883v39.281c0 2.973.567 5.096 1.703 6.37 1.277 1.132 3.051 1.698 5.321 1.698h4.257v19.535l-6.598.212h-.852c-3.831.142-7.734-.142-11.707-.849-3.832-.708-7.237-2.336-10.217-4.884-2.981-2.548-4.683-6.441-5.109-11.678-2.838 5.804-7.45 10.546-13.836 14.226-6.386 3.68-14.191 5.521-23.415 5.521-11.636 0-21.357-2.902-29.162-8.706-7.663-5.945-11.494-13.73-11.494-23.356 0-7.078 1.631-12.811 4.895-17.199 3.406-4.388 8.089-7.856 14.049-10.404 6.102-2.548 14.049-4.742 23.841-6.582l31.716-6.37c-.142-8.635-2.058-15.004-5.747-19.109-3.69-4.247-9.366-6.37-17.029-6.37-5.96 0-10.927 1.628-14.9 4.883-3.832 3.114-6.457 7.715-7.876 13.802l-27.034-1.487zm25.331 47.987c0 4.105 1.703 7.502 5.108 10.191 3.548 2.548 8.586 3.822 15.114 3.822 5.25 0 9.933-1.274 14.048-3.822 4.258-2.547 7.522-6.369 9.792-11.465 2.413-5.238 3.619-11.537 3.619-18.898v-1.061l-21.712 3.822-3.832.637c-5.25.991-9.366 1.981-12.346 2.972-2.838.991-5.179 2.619-7.024 4.884-1.845 2.123-2.767 5.096-2.767 8.918zM611.079 93.616h24.905l.851 23.569-2.128-2.76c2.838-7.503 7.308-13.236 13.41-17.199 6.244-4.105 13.765-6.158 22.563-6.158 10.502 0 19.371 2.62 26.608 7.857 7.237 5.237 12.63 12.315 16.178 21.233 3.689 8.776 5.534 18.684 5.534 29.726 0 11.041-1.845 21.02-5.534 29.938-3.548 8.776-8.941 15.783-16.178 21.02-7.237 5.238-16.106 7.857-26.608 7.857-5.818 0-11.139-.92-15.964-2.761-4.683-1.84-8.728-4.529-12.133-8.068-3.264-3.539-5.818-7.927-7.663-13.165l2.554-2.123V238h-26.395V93.616zm24.266 56.268c0 6.794.994 13.022 2.98 18.684 2.129 5.521 5.322 9.98 9.579 13.377 4.257 3.397 9.437 5.096 15.539 5.096 9.224 0 16.178-3.539 20.861-10.616 4.824-7.078 7.237-15.925 7.237-26.541 0-10.475-2.413-19.322-7.237-26.542-4.683-7.219-11.637-10.828-20.861-10.828-6.102 0-11.282 1.769-15.539 5.308-4.257 3.397-7.45 7.927-9.579 13.589-1.986 5.521-2.98 11.678-2.98 18.473z"></path><path fill="url(#paint0_linear_0_1)" d="M218.623 0H50C22.386 0 0 22.386 0 50v168.623c0 27.614 22.386 50 50 50h168.623c27.614 0 50-22.386 50-50V50c0-27.614-22.386-50-50-50z"></path><path fill="url(#paint1_radial_0_1)" d="M134.646 240.418c58.415 0 105.77-47.355 105.77-105.77s-47.355-105.77-105.77-105.77-105.77 47.355-105.77 105.77 47.355 105.77 105.77 105.77z"></path><path fill="url(#paint2_linear_0_1)" d="M134.983 224.972c49.699 0 89.988-40.289 89.988-89.988 0-49.7-40.289-89.989-89.988-89.989-49.7 0-89.989 40.29-89.989 89.989 0 49.699 40.29 89.988 89.989 89.988z"></path><path fill="url(#paint3_radial_0_1)" d="M134.983 202.81c37.46 0 67.828-30.368 67.828-67.828 0-37.46-30.368-67.827-67.828-67.827-37.46 0-67.827 30.367-67.827 67.827 0 37.46 30.367 67.828 67.827 67.828z"></path><defs><linearGradient id="paint0_linear_0_1" x1="134.311" x2="134.311" y1="0" y2="268.623" gradientUnits="userSpaceOnUse"><stop stop-color="#A8DBF8"></stop><stop offset="1" stop-color="#BCCBFF"></stop></linearGradient><radialGradient id="paint1_radial_0_1" cx="0" cy="0" r="1" gradientTransform="rotate(107.38 31.314 98.475) scale(149.225)" gradientUnits="userSpaceOnUse"><stop offset="0.063" stop-color="#84D3FF"></stop><stop offset="0.411" stop-color="#17ABFE"></stop><stop offset="0.906" stop-color="#0770D1"></stop></radialGradient><linearGradient id="paint2_linear_0_1" x1="134.983" x2="134.983" y1="44.995" y2="224.972" gradientUnits="userSpaceOnUse"><stop stop-color="#BBE8FF"></stop><stop offset="1" stop-color="#018FEF"></stop></linearGradient><radialGradient id="paint3_radial_0_1" cx="0" cy="0" r="1" gradientTransform="rotate(98.584 14.543 129.964) scale(68.5955)" gradientUnits="userSpaceOnUse"><stop stop-color="#7EC3FF"></stop><stop offset="1" stop-color="#EAF5FF"></stop></radialGradient></defs></svg><p><span>Beta v<!-- -->0.2.3</span></p></div><p>Cap is the open source alternative to Loom. Lightweight, powerful, and stunning. Record and share in seconds.</p><p>© Cap Software, Inc. <!-- -->2024<!-- -->.</p><div><p><a href="https://cap.so/terms">Terms of Service</a><a href="https://cap.so/privacy">Privacy Policy</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPUs Go Brrr (391 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2024-05-12-tk</link>
            <guid>40337936</guid>
            <pubDate>Sun, 12 May 2024 22:05:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">https://hazyresearch.stanford.edu/blog/2024-05-12-tk</a>, See on <a href="https://news.ycombinator.com/item?id=40337936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>AI uses an <a href="https://www.semianalysis.com/p/gpt-4-architecture-infrastructure">awful</a> <a href="https://www.instagram.com/p/C2QARHJR1sZ">lot</a> <a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini">of</a> <a href="https://arxiv.org/pdf/2202.05924">compute</a>.</p>
<p>In the last few years we’ve focused a great deal of our work on making AI use less compute (e.g. <a href="https://www.together.ai/blog/based">Based</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-12-11-truly-subquadratic">Monarch Mixer</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-01-20-h3">H3</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-06-08-hyena-safari">Hyena</a>, <a href="https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4">S4</a>, among others) and run more efficiently on the compute that we have (e.g. <a href="https://hazyresearch.stanford.edu/blog/2023-01-12-flashattention-long-sequences">FlashAttention</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-07-17-flash2">FlashAttention-2</a>, <a href="https://hazyresearch.stanford.edu/blog/2023-11-13-flashfftconv">FlashFFTConv</a>). Lately, reflecting on these questions has prompted us to take a step back, and ask two questions:</p>
<ul>
<li>What does the hardware actually want?</li>
<li>And how can we give that to it?</li>
</ul>
<p>This post is a mixture of practice and philosophy. On the practical side, we’re going to talk about what we’ve learned about making GPUs go brr -- and release an embedded DSL, <a href="https://github.com/HazyResearch/ThunderKittens">ThunderKittens</a>, that we’ve built to help us write some particularly speedy kernels (which we are also <a href="https://github.com/HazyResearch/ThunderKittens/tree/main/examples">releasing</a>). On the philosophical side, we’ll briefly talk about how what we’ve learned has changed the way we think about AI compute.</p>
<h2>What's in an H100?</h2>
<p>For this post, we’re going to focus on the NVIDIA H100 for two reasons. First, it represents an awful lot of new compute going online. Second, we think the trends it implies are going to continue in future generations, and probably from other manufacturers, too. But bear in mind (and we will repeat in case you forget) that most of this post applies in some form to other GPUs, too.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/brr.png"><figcaption><p>Figure 1: brr</p></figcaption></figure>
<p>Advance apologies for restating the data sheet, but the details of the hardware are important for the discussion to come. An H100 SXM GPU contains, for our purposes:</p>
<ul>
<li>80 GB of HBM3 with 3 TB/s of bandwidth. (A bit less bandwidth in practice.)</li>
<li>50 MB of L2 cache with 12 TB/s of bandwidth, split across the GPU into two 25MB sections connected by a crossbar. (The crossbar sucks.)</li>
<li>132 streaming multiprocessors (SM’s), where each has:<!-- -->
<ul>
<li>up to 227 KB of shared memory within a 256 KB L1 cache. (Together, these have about 33 TB/s of bandwidth.)</li>
<li>a tensor memory accelerator (TMA) -- a new chunk of hardware in Hopper that can do asynchronous address generation and fetch memory. It also does other things like facilitate the on-chip memory network (distributed shared memory) but we’re not going to focus on this much, today.</li>
<li>4 quadrants, where each quadrant has:<!-- -->
<ul>
<li>A warp scheduler</li>
<li>512 vector registers (each containing 32 4-byte words)</li>
<li>A tensor core for matrix multiplies</li>
<li>A bunch of built-in instructions like sums, multiplies, that operate in parallel on these vector registers.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>There’s a lot of other stuff, too (memory controllers, instruction caches, etc) but we don’t care about any of that right now.</p>
<p>All of the compute happens in the SM’s. <em>Most</em> of it happens in the registers.</p>
<p><em>Great, how do I make it go brr?</em></p>
<p>Keep the tensor core fed. That’s it.</p>
<p><em>Wait, really?</em></p>
<p>Yes. That’s the game.</p>
<p>An H100 GPU has 989 TFLOPs of half-precision matrix multiply compute, and ~60 TFLOPs of “everything else”. So, every cycle the tensor core is in use, you’re getting at least 94% utilization of the hardware. And every cycle the tensor core is not in use, you’re getting no more than 6% utilization of the hardware. Put another way:</p>
<p>% utilization H100 = % tensor cores active cycles +/- 6%.</p>
<p>Now it turns out that keeping the tensor core fed is easier said than done. We’ve discovered a number of quirks to the hardware that are important to keeping the matrix multiplies rolling. Much of this also applies to non-H100 GPUs, but the H100 is particularly tricky to keep fed so we focus on it here. (The RTX 4090, by comparison, is very easy to work with as illustrated in figure 2.)</p>
<ul>
<li>WGMMA instructions are necessary but also really irritating to use.</li>
<li>Shared memory is not actually that fast and also requires great care.</li>
<li>Address generation is expensive.</li>
<li>Occupancy remains helpful, and registers are generally the key resource.</li>
</ul>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/spirits.png"><figcaption><p>Figure 2: NVIDIA GPUs (H100 and 4090) and their spirit animals (canadian goose and golden retriever puppy).</p></figcaption></figure>
<p>Let’s go through each of these in order.</p>
<h3>WGMMA Instructions</h3>
<p>The H100 has a new set of instructions called “warp group matrix multiply accumulate” (<code>wgmma.mma_async</code> in PTX, or <code>HGMMA</code>/<code>IGMMA</code>/<code>QGMMA</code>/<code>BGMMA</code> in SASS). To understand what makes them special, we need to look briefly at how you used to have to use tensor cores. The tensor core instructions available on previous GPUs were <code>wmma.mma.sync</code> and <code>mma.sync</code> instructions. With these instructions a warp of 32 threads on a single quadrant of an SM would synchronously feed their chunk of the data into the tensor core and await the result. Only then could they move on.</p>
<p>Not so with <code>wgmma.mma_async</code> instructions. Here, 128 consecutive threads -- split across all quadrants of the SM -- collaboratively synchronize, and asynchronously launch a matrix multiply directly from shared memory (and optionally also registers.) These warps can then go do other things with their registers while the matrix multiply happens, and await the result whenever they want.</p>
<p>In our microbenchmarks, we found that these instructions are necessary to extract the full compute of the H100. Without them, the GPU seems to top out around 63% of its peak utilization; we suspect this is because the tensor cores want a deep hardware pipeline to keep them fed, even from local resources.</p>
<p>Unfortunately, the memory layouts for these instructions <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-register-fragment-wgmma-64n16">are</a> <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout">quite</a> <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout-swizzling-modes">complicated</a>. The unswizzled shared memory layouts suffer from very poor coalescing, and so they require substantial additional bandwidth from L2. The swizzled memory layouts are flat-out incorrectly documented, which took considerable time for us to figure out. They’re also brittle, in that they appear to only work for specific matrix shapes and do not play well with other parts of the wgmma.mma_async instructions. For example, the hardware can transpose sub-matrices on its way to the tensor cores -- but only if the layout is not swizzled.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/lies.png"><figcaption><p>Figure 3: NVIDIA’s <span><em>lies</em></span>. This is an extraordinarily misleading representation of the actual 128b swizzled wgmma layout. This diagram cost us three weeks of life that we will not get back, hence the public shaming.</p></figcaption></figure>
<p>We’ve also found that unswizzled wgmma layouts have both poor memory coalescing as well as bank conflicts. On kernels such as flash attention, TMA and the L2 cache are both fast enough so as to hide these problems reasonably well. But to make the full use of the hardware, memory request must be coalesced and bank conflicts avoided, and then controlling layouts very carefully becomes critical.</p>
<p>Despite these pains, these instructions really are necessary to make full use of the H100. Without them, you’ve already lost 37% of the potential performance of the GPU!</p>
<h3>Shared memory</h3>
<p>Shared memory appears to have a single-access latency of around <a href="https://chipsandcheese.com/2023/07/02/nvidias-h100-funny-l2-and-tons-of-bandwidth/">30 cycles</a> (this matches our observations, too). That doesn’t sound like much, but in that time the SM’s tensor cores could have done almost two full 32x32 square matrix multiplies.</p>
<p>In previous work (like Flash Attention), we’ve focused more on the HBM-SRAM bottleneck. And indeed: this really used to be the bottleneck! But as HBM has gotten faster and the tensor cores continue to grow out of proportion with the rest of the chip, even relatively small latencies like those from shared memory have also become important to either remove or hide.</p>
<p>Shared memory can be tricky to work with because it is “banked” into 32 separate stores of memory. If one is not careful, this can lead to something called “bank conflicts”, where the same memory bank is being asked to simultaneously provide multiple different pieces of memory. This leads to requests being serialized, and in our experience this can disproportionately slow down a kernel -- and the register layouts required by wgmma and mma instructions would naively suffer from these bank conflicts. The solution is to rearrange shared memory with various “swizzling” patterns so as to avoid these conflicts, but it is an important detail to get right.</p>
<p>More generally, we have found it very valuable to avoid movement between registers and shared memory when possible, and otherwise to use the built-in hardware (wgmma and TMA instructions) to do data movement asynchronously when possible. Synchronous movement using the actual warps is a worst-case fallback with the greatest generality.</p>
<h3>Address Generation</h3>
<p>One interesting quirk of the H100 is that the tensor cores and memory are both fast enough that merely producing the memory addresses to fetch takes a substantial fraction of the resources of the chip. (This is even more the case when complicated interleaved or swizzling patterns are added in.)</p>
<p>NVIDIA appears to understand this, as they have bestowed on us the Tensor Memory Accelerator (or TMA, as it likes to be called). TMA allows you to specify a multi-dimensional tensor layout in global and shared memory, tell it to asynchronously fetch a subtile of that tensor, and trip a barrier when it’s done. This saves all of the address generation costs, and additionally makes it much easier to construct pipelines.</p>
<p>We have found TMA to be, like wgmma.mma_async, completely indispensable in achieving the full potential of the H100. (Probably moreso than wgmma, in our experience.) It saves register resources and instruction dispatches, and also has useful features such as the ability to perform reductions onto global memory asynchronously, too -- this is particularly useful in complex backwards kernels. As with wgmma, the main quirk of it is that its swizzling modes are a bit difficult to decipher without some reverse engineering, but we had substantially less pain on this point.</p>
<h3>Occupancy</h3>
<p>For those newer to CUDA, occupancy refers to the number of co-scheduled threads on the exact same execution hardware. Each cycle, the warp scheduler on that quadrant of the SM will try to issue an instruction to a warp of threads that are ready for an instruction. NVIDIA uses this model because it can enable the hardware to be more easily kept full. For example, while one warp of threads is waiting for a matrix multiply, another can receive an instruction to use the fast exponential hardware.</p>
<p>In some ways, the H100 is less reliant on occupancy than previous generations of the hardware. The asynchronous features of the chip mean that even a single instruction stream can keep many parts of the hardware busy -- fetching memory, running matrix multiplies, doing shared memory reductions, and still simultaneously running math on the registers.</p>
<p>But occupancy is very good at hiding both sins and sync’s. A perfectly designed pipeline might run reasonably fast even without any additional occupancy, but our observations suggest that NVIDIA really has designed their GPUs with occupancy in mind. And there are enough synchronizations -- and enough ways to make mistakes -- that finding ways to increase occupancy has, in our experience, usually yielded good returns at increasing the realized utilization of the hardware.</p>
<p>Finally, while occupancy is merely useful on the H100, we have found it to be increasingly important on the A100 and RTX 4090, respectively, likely because they rely increasingly on synchronous instruction dispatches, relative to the H100.</p>
<h2>ThunderKittens</h2>
<p>Based on the above, we asked ourselves how we might make it easier to write the kinds of kernels we care about while still extracting the full capabilities of the hardware. Motivated by a continuing proliferation of new architectures within the lab (and the fact that Flash Attention is like 1200 lines of code), we ended up designing a DSL embedded within CUDA -- at first for our own internal use.</p>
<p>But then we decided it was useful enough that, with love in our hearts, we cleaned it up and have released it for you. ThunderKittens is that embedded DSL. It is named ThunderKittens because we think kittens are cute, and also we think it is funny to make you type <code>kittens::</code> in your code.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/tk.png"><figcaption><p>Figure 4: A ThunderKitten. Look at her big eyes! Are you not be entranced!?!?</p></figcaption></figure>
<p>It is meant to be as simple as possible, and contains four templated types:</p>
<ul>
<li>Register tiles -- 2D tensors on the register file.</li>
<li>Register vectors -- 1D tensors on the register file.</li>
<li>Shared tiles -- 2D tensors in shared memory.</li>
<li>Shared vectors -- 1D tensors in shared memory.</li>
</ul>
<p>Tiles are parameterized by a height, width, and layout. Register vectors are parameterized by a length and a layout, and shared vectors just by a length. (They don’t generally suffer from bank conflicts.)</p>
<p>We also give operations to manipulate them, either at the warp level or at the level of a collaborative group of warps. Examples include:</p>
<ul>
<li>Initializers -- zero out a shared vector, for example.</li>
<li>Unary ops, like exp</li>
<li>Binary ops, like mul</li>
<li>Row / column ops, like a row_sum</li>
</ul>
<p>Since ThunderKittens is embedded within CUDA (contrasting libraries like Triton which we also love very much and rely on heavily), the abstractions fail gracefully. If it’s missing something, just extend it to do what you want!</p>
<p>To show an example of these primitives in action, consider Tri’s lovely flash attention -- a beautiful algorithm, but <a href="https://github.com/Dao-AILab/flash-attention/blob/main/csrc/flash_attn/src/flash_fwd_kernel.h">complicated</a> to implement in practice, even on top of NVIDIA’s wonderful Cutlass library.</p>
<p>Here's a simple forward flash attention kernel for an RTX 4090, written in ThunderKittens.</p>

<p>Altogether, this is about 60 lines of CUDA sitting at 75% hardware utilization -- and while it is fairly dense, most of the complexity is in the algorithm, rather than in swizzling patterns or register layouts. And what of all of the complexity of TMA, WGMMA, swizzling modes, and descriptors? Here’s a FlashAttention-2 forward pass for the H100, written with ThunderKittens.</p>

<p>So how does it do?</p>
<p>This kernel is just 100 lines, and it actually outperforms FlashAttention-2 on the H100 by about 30%. ThunderKittens takes care of wrapping up the layouts and instructions, and gives you a mini-pytorch to play with on the GPU.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/attn.png"><figcaption><p>Figure 5: FA2 (via Pytorch) versus TK for a wide range of configs on the H100 SXM.</p></figcaption></figure>
<p>We also release kernels for Based linear attention and other forthcoming architectures, too. Our Based linear attention kernel runs at 215 TFLOPs (or more than 300 TFLOPs when the recompute inherent in the algorithm is considered). And while linear attention is of course theoretically more efficient, historically, they have been dramatically less efficient on real hardware. So we feel this could open up a broad range of high-throughput applications -- more to come on this point later.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/based.png"><figcaption><p>Figure 6: Linear attention can be quite quick with TK!</p></figcaption></figure>
<p>If this seems up your alley, feel free to play with it!</p>
<h2>Tiles Seem Like a Good Idea</h2>
<p>In our view, what has made ThunderKittens work well for us is that it does not try to do everything. CUDA is indeed far more expressive than ThunderKittens. ThunderKittens is small and dumb and simple.</p>
<figure><img src="https://hazyresearch.stanford.edu/static/posts/2024-05-12-tk/theydontknow.png"><figcaption><p>Figure 7: the main message of this blog post.</p></figcaption></figure>
<p>But ThunderKittens has good abstractions -- small tiles -- that match where both AI and hardware are going. ThunderKittens doesn’t support any dimension less than 16. But in our view, this doesn’t really matter, since the hardware doesn’t particularly want to, either. And we ask: if your matrix multiply is smaller than 16x16, are you sure what you’re doing is AI?</p>
<p>From a philosophical point of view, we think a frame shift is in order. A “register” certainly shouldn’t be a 32-bit word like on the CPUs of old. And a 1024-bit wide vector register, as CUDA uses, is certainly a step in the right direction. But to us a “register” is a 16x16 tile of data. We think AI wants this -- after all this time, it’s still just matrix multiplies, reductions, and reshapes. And we think the hardware wants this, too -- small matrix multiplies are just begging for hardware support beyond just the systolic mma.</p>
<p>In fact, more broadly we believe we should really reorient our ideas of AI around what maps well onto the hardware. How big should a recurrent state be? As big can fit onto an SM. How dense should the compute be? No less so than what the hardware demands. An important future direction of this work for us is to use our learnings about the hardware to help us design the AI to match.</p>
<h2>Tiles Seem Pretty General</h2>
<p>Coming soon -- ThunderKittens on AMD hardware!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Alternative Implementation Problem (140 pts)]]></title>
            <link>https://pointersgonewild.com/2024/04/20/the-alternative-implementation-problem/</link>
            <guid>40337036</guid>
            <pubDate>Sun, 12 May 2024 19:49:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pointersgonewild.com/2024/04/20/the-alternative-implementation-problem/">https://pointersgonewild.com/2024/04/20/the-alternative-implementation-problem/</a>, See on <a href="https://news.ycombinator.com/item?id=40337036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p><img width="250" height="250" src="https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=250" alt="" decoding="async" srcset="https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=250 250w, https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=500 500w, https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=150 150w, https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=300 300w" sizes="(max-width: 250px) 100vw, 250px" data-attachment-id="5687" data-permalink="https://pointersgonewild.com/2024/04/20/the-alternative-implementation-problem/screenshot-2024-04-20-at-11-54-09-am/" data-orig-file="https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png" data-orig-size="1164,1164" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-04-20 at 11.54.09 AM" data-image-description="" data-image-caption="" data-medium-file="https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=300" data-large-file="https://pointersgonewild.files.wordpress.com/2024/04/screenshot-2024-04-20-at-11.54.09-am.png?w=497"></p><p>In this post, I want to talk about a dynamic that I’ve seen play itself over and over again in the software world. In fact, I would venture a guess that this kind of situation probably happens in the hardware world as well, but I’ll speak about software systems since this is where my experience lies. This discussion is going to touch a bit on human psychology, and outline a common trap so that you can hopefully avoid getting stuck in it.</p>



<p>Most of my career, both in academia and industry, has been spent trying to optimize dynamically-typed programming languages. During my master’s I worked on a simple optimizing <a href="https://www.sable.mcgill.ca/publications/papers/2010-3/mcvmcc2010.pdf">JIT for MATLAB</a>. For my PhD I worked on a <a href="https://drops.dagstuhl.de/storage/00lipics/lipics-vol056-ecoop2016/LIPIcs.ECOOP.2016.7/LIPIcs.ECOOP.2016.7.pdf">JIT for JavaScript</a>. Today I’m working on YJIT, an optimizing <a href="https://shopify.engineering/ruby-yjit-is-production-ready">JIT for Ruby</a> which has now been upstreamed into CRuby. </p>



<p>During my PhD, while working on my own JavaScript JIT, I read many papers and blog posts about JIT compilers for other dynamic languages. I read about the design of HotSpot, Self, LuaJIT, PyPy, TruffleJS, V8, SpiderMonkey, and JavaScriptCore among others. I also had the chance to interact with and meet face to face with many of the really smart people behind these projects.</p>



<p>One of the things that struck me is that the PyPy project was kind of stuck in a weird place. They had developed an advanced JIT compiler for Python which could produce <a href="https://speed.pypy.org/">great speedups</a> over CPython. By all accounts many people could benefit from these performance gains, but PyPy was seeing very little use in the “real world”. One of the challenges that they faced is that Python is a moving target. New versions of CPython come out regularly, always adding many new features, and PyPy struggles to keep up, is always several Python versions behind. If you want you Python software to be PyPy-compatible, you’re much more limited in terms of which Python features you use, and most Python programmers don’t want to have to think about that.</p>



<p>Reading about LuaJIT, I found that it was and still is highly regarded. Many people regard its creator, Mike Pall, as an incredible programmer. LuaJIT offers <a href="https://staff.fnwi.uva.nl/h.vandermeer/docs/lua/luajit/luajit_performance.html">great performance</a> gains over the default, interpreted Lua implementation, and has seen some decent adoption in the wild. However, I again saw that there are a number of Lua users who do not want to use LuaJIT because the Lua language keeps adding new features and LuaJIT is several versions behind. This is a bit strange considering that Lua is a language that is known for its minimalism. It seems like they could have made an effort to slow down the addition of new features and/or coordinate with Mike Pall, but this wasn’t done.</p>



<p>Almost 4 years ago, I joined Shopify to work on Ruby. For some reason, the space of Ruby JITs has been particularly competitive, and there had been a number of projects to build Ruby JITs. The TruffleRuby JIT boasted the most impressive performance numbers, but again, had seen limited deployments. There were some practical reasons for this, the warm up time of TruffleRuby is much longer than that of CRuby, but I also saw a similar dynamic to that of PyPy and LuaJIT, where CRuby kept adding features, and TruffleRuby contributors had to work hard to try and keep up. It didn’t really matter if TruffleRuby could be quite a bit faster, because Ruby users would always view CRuby as the canonical implementation, and anything that wasn’t fully compatible wasn’t seen as worthy of consideration.</p>



<p>Hopefully, at this point, you see where I’m going with this. What I’ve concluded, based on experience, is that positioning your project as an alternative implementation of something is a losing proposition. It doesn’t matter how smart you are. It doesn’t matter how hard you work. The problem is, when you build an alternative implementation, you’ve made yourself subject to the whims of the canonical implementation. They have control over the direction of the project, and all you can do is try to keep up. In the case of JITted implementations of traditionally interpreted languages, there’s a bit of a weird dynamic, because it’s much faster to implement new features in an interpreter. The implementers of the canonical implementation may see you as competition they are trying to outrun. You may be stuck trying to ice skate uphill.</p>



<p>Almost 4 years ago, with support from Shopify, two dedicated colleagues and I started a project to build YJIT, yet another Ruby JIT. The difference is that we made the key choice to build YJIT not as an alternative implementation, but directly inside CRuby itself. This came with a number of design tradeoffs, but critically, YJIT could be 100% compatible with every CRuby feature from the start. YJIT is now the “official” Ruby JIT, and is deployed at Shopify, Discourse and GitHub among others. If you’ve visited github.com today, or any Shopify store, you’ve interacted with YJIT. We’ve had more success than any other Ruby JIT compiler so far, and compatibility has been key in achieving this.</p>



<p>You may read this and think that the key lesson of this post follows the old adage that “if you can’t beat them, join them”. In some ways, I suppose it does. What I want to say is that if you start a project to try and position yourself as an alternative but better implementation of something, you are likely to find yourself stuck in a spot where you’re always playing catch up and living in the shadow of the canonical implementation. The canonical project keeps evolving, and you have no choice but to follow along with limited decisional power over where your own project is headed. That’s no fun. You may have better luck trying to join up with the canonical implementation instead. However, that’s only part of the answer.</p>



<p>In the Ruby space, there is also Crystal, a Ruby-like language that is statically compiled with type inference. This language is intentionally not Ruby-compatible, it has chosen to diverge from Ruby, but has still seen limited success. I think this is interesting because it gives us a broader perspective. Rubyists don’t like Crystal because it’s almost-Ruby-but-not-quite. It looks like Ruby, syntactically, but has many subtle differences and is very much incompatible in practice. This just confuses people, it breaks their expectations. Crystal probably would have had better luck if it had never marketed itself as being similar to Ruby in the first place.</p>



<p>Peter Thiel has a saying that <a href="https://www.youtube.com/watch?v=Z3D8Vo9hRyw">“competition is for losers”</a>. His main point is that you shouldn’t put yourself in a position where you’re forced to compete if you don’t have to. My advice to younger programmers would be, if you’re thinking of creating your own programming language, for example, then don’t go trying to create a subset of Python, or something superficially very close to an existing language. Do your own thing. That way, you can evolve your system at your own pace and in your own direction, without being chained by expectations that your language should have to match the performance, feature set, or library ecosystem of another implementation.</p>



<p>I’ll finish with some caveats. What I said above applies when you have a situation where there is a canonical implementation of a language or system. It doesn’t apply in a space where you have open standards. For example, if you want to implement your own JSON parser, there is a clearly defined specification that is relatively small and doesn’t evolve very fast. This is very much something you can achieve. You also have a situation where there are multiple browser-based implementations of JavaScript. This is possible in part because there is an external standard body that governs the JS specification, and the people working on the JS standard understand that JIT-compiled implementations are critical for performance and guide the evolution of the language accordingly. They are not in the game of adding many new features as fast as possible.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Professional Corner-Cutting (2016) (146 pts)]]></title>
            <link>https://blog.ometer.com/2016/05/04/professional-corner-cutting/</link>
            <guid>40336609</guid>
            <pubDate>Sun, 12 May 2024 18:55:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ometer.com/2016/05/04/professional-corner-cutting/">https://blog.ometer.com/2016/05/04/professional-corner-cutting/</a>, See on <a href="https://news.ycombinator.com/item?id=40336609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p>Steve Jobs famously <a href="http://thenextweb.com/apple/2011/10/24/steve-jobs-obsession-with-the-quality-of-the-things-unseen/">cared about the unseen backs of cabinets</a>. Antique furniture built with hand tools isn’t like that at all. Cabinetmakers made each part to the tolerance that mattered.&nbsp;The invisible parts were left rough, with plane and saw marks, to save time. The visible parts, however, were cleaned up and polished. Some surfaces were made precisely straight and square, for structural reasons; while nonstructural surfaces were only straight enough to look good to the eye.</p>
<p>Think about an apprentice in an old cabinet shop. An apprentice painstakingly smoothing an invisible surface would be yelled at for wasting time. An apprentice failing to smooth a visible surface would be yelled at for producing crappy work. To become a professional, the apprentice learned to work <em>efficiently</em> but still <em>do a good job</em>. Crucially, a “good job” was defined in terms of customer concerns. <sup><a href="#fn1">[1]</a></sup></p>
<p>Cabinetmakers were focused on <em>what their customers cared about</em>. Customers wanted the furniture to look good, and they wanted it to be structurally sound. They didn’t care about invisible tool marks, and didn’t want to pay extra to have those removed.</p>
<p>Software remains a craft rather than a science, relying on the experience of the craftsperson. Like cabinetmakers, we proceed one step at a time, making judgments about what’s important and what isn’t at each step.</p>
<p><strong>A professional developer does thorough work when it matters, <em>and</em> cuts irrelevant corners that aren’t worth wasting time on.</strong> Extremely productive developers don’t have supernatural coding skills; their secret is to write only the code that matters.<strong><br>
</strong></p>
<p>How can we do a better job cutting corners? I think we can learn a lot from people building tables and dressers.</p>
<h2>1. Own the implementation decisions</h2>
<p>It is irresponsible to ask the customer (or manager, or other not-doing-the-work stakeholder) to tell you how to make technical tradeoffs. Cabinetmakers didn’t ask their customers how flat a tenon had to be and this is not the customer’s problem. The customer wants us to do it <em>properly</em> but not <em>wastefully</em>. It is our decision how to go about this, and if we get it wrong it’s our fault.</p>
<p>On software teams, there’s often a developer trying to push these decisions up to management or onto the customer, because they don’t want to “get in trouble” later. Perhaps they complain to management about “technical debt” and being “given time to work on it.” This is a sign that we aren’t owning our decisions. If the technical debt is a problem, 1) we shouldn’t have put it in there, and 2) we should include it in our estimates and address it. A cabinetmaker would not ask the customer to put “make tenons straight” on the sprint. Nobody cares. Technical debt is our problem; that’s the job.</p>
<p><strong>If you don’t own your technical decisions, you can never get them right, because nobody else knows how to make them.</strong> Insist on making them. And yes, that means getting them wrong is your fault. It may mean giving people bad news about how long things will take. It may mean you get yelled at sometimes.</p>
<h2>2. Understand the customer’s needs and preferences</h2>
<p>Because we must make tradeoffs and not push choices onto the customer, we have to understand what matters and what doesn’t. It’s easier to establish this in the world of furniture (“doesn’t break when you sit on it,” “looks nice”). In software, we have to know <a href="http://blog.ometer.com/2016/01/26/the-dangerous-ui-team/">what job our software will do</a> for the customer.</p>
<p>This is where we <em>should</em> be getting customer input (though watching what they do may be more valuable than asking them what they think), and reaching a consensus with our management team or client.</p>
<p>We should not ask customers for more precision than they can give us (a symptom of this is to badger customers or managers for detailed “requirements,” then complain endlessly about “changing requirements”). Our job involves converting vague needs into concrete software — if we’re lucky, we have the help of a skilled product designer, or guidance from a management team that’s able to be somewhat precise, but if not we have to do it ourselves. Accept the job and learn to do it.</p>
<p>It’s unprofessional to be the kind of developer who doesn’t care about user experience, doesn’t care about business context, or “just wants to be told the requirements.” <strong>It’s impossible to work efficiently or to do a good job without understanding the context.</strong></p>
<p>A professional developer can take a desired UX and work out the technical steps to get there as efficiently as possible. And they do get there; they don’t build something odd that doesn’t meet the need, or something slapdash that <a href="http://blog.ometer.com/2011/10/24/it-has-to-work/">doesn’t work</a>.</p>
<h2>3. Don’t be lazy</h2>
<p>Corner-cutting should be a deliberate decision; “this truly doesn’t matter.” It should not be because it’s 5pm and we’re going home. When we find ourselves asking “do I really have to redo this…” <a href="http://blog.lostartpress.com/2016/02/24/the-art-of-self-interrogation/">then we need to redo it</a>.</p>
<p><strong>Cutting corners should feel like you have a clear focus and you’re skipping tasks that don’t matter for that focus. Cutting corners should not feel like you’re doing poor-quality work.</strong></p>
<p>To push back on an unrealistic schedule, work to narrow the scope or weaken the requirements.</p>
<p>Let’s say you’re making some kitchen cabinets. You could make them all with hand tools, no metal connectors and no plywood. They would be gorgeous and cost about $150,000. When the customer says that’s too much time and too expensive, you could make them the usual modern way with machines, screws and plywood; which is a sound approach, though a little uglier. This is like offering to build a web app that’s not quite as slick and beautiful — something a little more off-the-shelf.</p>
<p>That’s all fine. What’s not fine: delivering either of those choices unfinished and broken. “Oh, I forgot the cabinet doors.” “Sorry these things aren’t painted!”</p>
<p>To cut scope, we should do something defined (such as leave out a feature or refinement), rather than something undefined (like skipping testing).</p>
<h2>Professionals are doing it for others</h2>
<p>All of this sounds hard, and it is. As in <a href="https://unicornfree.com/2012/why-blacksmiths-are-better-at-startups-than-you">Amy Hoy’s description of these students learning a craft</a>, at first we may fight it and focus on our own needs and emotions.</p>
<p>Professional software developers are <em>performing a service for others</em>. That’s the difference between a professional and a hobbyist or an artist. To perform a service for others, we have to know what others need, and apply our expertise to meet those needs.</p>

<p><a name="fn1">[1]</a> <em>Furniture made by machine doesn’t have the same ability to flex tolerances to save time. For the most part, with woodworking machines you get what you get; the machine doesn’t know whether your surface will be visible, or how flat it has to be. It makes one kind of surface and that’s it. Some parts of machine-made furniture aren’t as good as a handmade joint or surface could be, while other parts are far more precise than necessary. Check out this <a href="http://www.popularwoodworking.com/woodworking-blogs/editors-blog/dado-joints-by-hand">discussion of how to cut a dado joint by hand</a>, which mentions several ways to save time on the back, non-show side of the piece.</em></p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Becoming an Amateur Polyglot (125 pts)]]></title>
            <link>https://www.lesswrong.com/posts/BPpeBH8brSCRvZajs/how-to-be-an-amateur-polyglot</link>
            <guid>40336607</guid>
            <pubDate>Sun, 12 May 2024 18:55:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lesswrong.com/posts/BPpeBH8brSCRvZajs/how-to-be-an-amateur-polyglot">https://www.lesswrong.com/posts/BPpeBH8brSCRvZajs/how-to-be-an-amateur-polyglot</a>, See on <a href="https://news.ycombinator.com/item?id=40336607">Hacker News</a></p>
Couldn't get https://www.lesswrong.com/posts/BPpeBH8brSCRvZajs/how-to-be-an-amateur-polyglot: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Homoiconic Python (148 pts)]]></title>
            <link>https://aljamal.substack.com/p/homoiconic-python</link>
            <guid>40335608</guid>
            <pubDate>Sun, 12 May 2024 16:24:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aljamal.substack.com/p/homoiconic-python">https://aljamal.substack.com/p/homoiconic-python</a>, See on <a href="https://news.ycombinator.com/item?id=40335608">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png" width="326" height="315.8914728682171" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1000,&quot;width&quot;:1032,&quot;resizeWidth&quot;:326,&quot;bytes&quot;:436178,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbee1c0c5-488b-4fe9-a6ca-ced469e022dc_1032x1000.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>The idea of a programming language being able to implement itself is fascinating. It stirs an intense feeling of curiosity: "What would that even look like?" Since its inception in the early '60s, </span><strong>Lisp</strong><span> has managed to do exactly that.</span></p><p><a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)" rel="nofollow ugc noopener">John McCarthy</a><span>, In the early '60s, unlocked a collection of remarkable ideas that work really well together and continue to be relevant decades later. First through the </span><a href="http://www-formal.stanford.edu/jmc/recursive.pdf" rel="nofollow ugc noopener">Lisp paper</a><span>, and shortly thereafter with the </span><a href="https://www.softwarepreservation.org/projects/LISP/book/LISP%201.5%20Programmers%20Manual.pdf" rel="nofollow ugc noopener">Lisp 1.5 manual</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp" width="296" height="276.33948339483396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:506,&quot;width&quot;:542,&quot;resizeWidth&quot;:296,&quot;bytes&quot;:36280,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac5ae3ca-7054-43fd-9cdf-2c396be4f2c0_542x506.webp 1456w" sizes="100vw"></picture></div></a><figcaption>John McCarthy</figcaption></figure></div><p><span>One such idea is </span><strong>homoiconicity</strong><span>, a trait where code and data are interchangeable. Typically, we think of code as a sequence of operations acting upon data. This understanding shapes our view of most programming languages today. However, Lisp breaks this mold by treating both code and data as the same—what's famously known as its </span><a href="https://en.wikipedia.org/wiki/Homoiconicity#:~:text=In%20computer%20programming%2C%20homoiconicity%20(from,as%20data%20using%20the%20language." rel="nofollow ugc noopener">homoiconic</a><span> nature. This distinct characteristic effectively blurs the lines between the operator (code) and the operated (data).</span></p><p>This unification of code and data in Lisp is profound, it allows a level of expression where the language can be naturally expressed in itself.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png" width="1456" height="759" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:759,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:235648,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b0bdd68-04bf-4246-b3df-b0980d664c17_2222x1158.png 1456w" sizes="100vw"></picture></div></a><figcaption>Lisp in Lisp</figcaption></figure></div><p><span>This tiny card is the entire programming language of Lisp! A programming language written in itself. Here is famous quote from </span><a href="https://en.wikipedia.org/wiki/Alan_Kay" rel="nofollow ugc noopener">Alan Kay</a><span> about this piece of coding history:</span></p><p>that was the big revelation to me … when I finally understood that the half page of code on the bottom of page 13 of the Lisp 1.5 manual was Lisp in itself. These were "Maxwell's Equations of Software!" This is the whole world of programming in a few lines that I can put my hand over.</p><p>Okay, but what sort of magic did Mr. Kay see in this artifact that led him to call it the "Maxwell's Equations of Software"? How was the entire world of programming encapsulated in just a few lines of code?</p><p><span>One fun way to answer that, of course, is by applying the principle that: "</span><strong>In order to understand something, you need to code it</strong><span>".</span></p><p><span>And to keep this spiritual implementation of the original Lisp fun and fresh, I decided to pick Python as the tool of choice. Most programmers are either not familiar or not comfortable with Lisp's syntax (</span><a href="https://xkcd.com/297/" rel="nofollow ugc noopener">too many parentheses</a><span>), but probably quite familiar with Python's syntax. My idea is to rewrite the "Lisp in Lisp" code in Python and try to maintain as much of the spirit of the old code as possible.</span></p><p><span>Lisp originally, and quite brilliantly, came with two </span><strong>syntactical</strong><span> flavors. A code flavor named </span><strong>M-expression</strong><span> (short for meta) and a data flavor named </span><strong>S-expression</strong><span> (short for symbolic). They're </span><strong>semantically</strong><span> equivalent.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png" width="626" height="179.2870879120879" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:417,&quot;width&quot;:1456,&quot;resizeWidth&quot;:626,&quot;bytes&quot;:176726,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F405c58a5-58d4-40d2-97e8-4d6b297dd561_2080x596.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>M-expressions side by side to their S-expressions equivalent</figcaption></figure></div><p>The "Lisp in Lisp" code presented earlier is written as an M-expression (code flavor) and implements an S-expression Lisp (data flavor).</p><p>One little trick to get us going is by translating Lisp M-expressions into Python code constructs, such as function calls and conditional statements. Additionally, we can represent Lisp S-expressions using Python lists. Lisp is short for "List Processing" because it only uses one data structure: the list. Therefore, it makes perfect sense to use Python lists to emulate Lisp S-expressions. Below, I present our mini dictionary that would act as our rosetta stone: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png" width="496" height="101.85714285714286" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:299,&quot;width&quot;:1456,&quot;resizeWidth&quot;:496,&quot;bytes&quot;:70594,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F968c725c-dec5-449e-af08-c5ec69fe8f9f_1900x390.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>These are four ways of expressing one thing. They're all semantically</span><strong> </strong><span>equivalent.</span></p><p>An additional advantage of this mapping approach is that it eliminates the need to implement a parser for our language. This simplifies our code base and allows us to stay true to the original spirit of Lisp without getting distracted by string manipulations.</p><p>With this context and motivation in place, we can move on to the actual implementation. Lisp requires a set of basic functions to be implemented outside of its scope, they're mostly about the basic building blocks of lists.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png" width="1456" height="443" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:443,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:141182,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e85e14c-0eb9-45ce-809a-3df68ae4c11a_2682x816.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Python list primitives to act like Lisp</figcaption></figure></div><ul><li><p>Equivalence</p><ul><li><p><strong>atom(x)</strong><span>: is x a list?</span></p></li><li><p><strong>eq(x,y)</strong><span>: is x equal y?</span></p></li></ul></li><li><p>Cutting</p><ul><li><p><strong>car(x)</strong><span>: first element of the list</span></p></li><li><p><strong>cdr(x)</strong><span>: the rest of the list</span></p></li></ul></li><li><p>Stitching</p><ul><li><p><strong>cons(x,y)</strong><span>: append an atom to a list</span></p></li><li><p><strong>append(x,y)</strong><span>: append two lists together</span></p></li></ul></li></ul><p><span>Ignoring a few recursive primitives, and with a little help from </span><strong><a href="https://llama.meta.com/llama3/" rel="nofollow ugc noopener">Llama3-70b</a><span> </span></strong><span>(</span><a href="https://groq.com/" rel="nofollow ugc noopener">on Groq</a><span>), we can quickly get a working interpreter for a subset of the "Lisp in Lisp" code:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png" width="1456" height="1169" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1169,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:312032,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ae0243d-7e73-421a-960a-f50b6e9ac123_1958x1572.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A working version of the Lisp in Lisp </span><strong>M-expression</strong><span> translated to Python</span></figcaption></figure></div><p>Here are a few examples you can play with:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png" width="1456" height="1496" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1496,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:220649,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dbce346-7368-48d8-85dc-37b42429ca20_1472x1512.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Python lists acting as </span><strong>S-expressions</strong></figcaption></figure></div><p><span>Full code available on </span><a href="https://gist.github.com/aburjg/1dffba5516095050089f6ea293cb782b#first" rel="nofollow ugc noopener">github gists</a></p><p><span>Our first iteration is nearly complete, except for one crucial feature: </span><a href="https://en.wikipedia.org/wiki/Anonymous_function" rel="nofollow ugc noopener">lambdas</a><span>. Lambdas are anonymous functions that serve as the primary method for defining and calling functions in Lisp. Without lambdas in Lisp, we cannot implement recursion, and without recursion, our Lisp would not be </span><a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis#:~:text=The%20thesis%20can%20be%20stated,represented%20as%20a%20Turing%20Machine%22." rel="nofollow ugc noopener">Turing complete</a><span> (the minimum threshold to compute all which can be computed). </span></p><p><span>To incorporate lambdas, we need to add a few functions we previously overlooked, specifically two primitives: </span><strong>assoc(x,y)</strong><span> and </span><strong>pairlis(x,y)</strong><span>. </span><strong>assoc(x,y)</strong><span> is a key/value dictionary style lookup but implemented with lists instead (associative lists). parlis is just the </span><strong>zip(x,y)</strong><span> in Python (ziping two lists together).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png" width="594" height="361.0508241758242" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:885,&quot;width&quot;:1456,&quot;resizeWidth&quot;:594,&quot;bytes&quot;:484840,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab54c10f-a5d7-4605-8665-f4f65cb6927f_1694x1030.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><strong>pairlis</strong><span> and </span><strong>assoc</strong><span> as they first appeared in the Lisp 1.5 manual</span></figcaption></figure></div><p>A literal translation in Python would be:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png" width="1456" height="671" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:671,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:234791,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F507f7502-872f-4cc8-a950-e95d714c8caf_2576x1188.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A literal (</span><strong>recursive</strong><span>) Lisp to Python translation of </span><strong>assoc</strong><span> and </span><strong>pairlis</strong></figcaption></figure></div><p><span>The original Lisp had to resort to </span><strong>recursion</strong><span> (function calling itself) even for simple, linear scans, since it had no </span><a href="https://en.wikipedia.org/wiki/For_loop" rel="nofollow ugc noopener">loops</a><span>. However, </span><strong>assoc</strong><span> and </span><strong>pairlis</strong><span> can be elegantly translated to Python using </span><strong>list comprehensions</strong><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png" width="1456" height="176" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:176,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:64335,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1e8befb-3b17-47c9-940d-2c7310ad7016_2184x264.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Python supports both </span><strong>loops</strong><span> and </span><strong>recursion</strong></figcaption></figure></div><p><span>and if you hadn't noticed already I actually cheated a bit in the COND case as this in the original lisp was a </span><strong>evcon</strong><span> which was also translated into a loop. The same trick will be done again with </span><strong>evlis</strong><span> for the LAMBDA case.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png" width="642" height="201.06593406593407" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:642,&quot;bytes&quot;:102839,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ac6b137-0f3b-417b-abb3-2a8909702103_1532x480.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><strong>evcon</strong><span>, and </span><strong>evlis</strong><span> are other examples of loops in "Lisp in Lisp" code</span></figcaption></figure></div><p><span>We're almost there! There is one last thing, and we're done. The </span><strong>eval</strong><span> function actually takes two arguments in the original Lisp. The first is the expression (s-exp) of course. The second is the environment, which is yet another list (of key/values). Environment maintains variable binding for the LAMBDA case by mapping variable names to their corresponding values. &nbsp;</span></p><p><span>For example, when you define a function with x variable then substitute that function with data, the data gets binded (with </span><strong>pairlis</strong><span>) to the x symbol and is then stored/appended to the environment list. When x is needed, it gets looked up (with </span><strong>assoc</strong><span>) and subtitued back to expression. The specific technique for this binding is known as </span><a href="https://en.wikipedia.org/wiki/Scope_(computer_science)" rel="nofollow ugc noopener">dynamic scoping</a><span>. </span></p><p>Here is the full 'Lisp in Lisp' code in Python:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png" width="1354" height="1550" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1550,&quot;width&quot;:1354,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:352315,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7161bbe-d80e-42b9-8b93-1a70bbab2a57_1354x1550.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Original 'Lisp in Lisp' in Python</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png" width="1456" height="214" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:214,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:83226,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F909b94d3-7004-4029-b03c-de794601f3f7_2174x320.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Lambdas at last!</figcaption></figure></div><p><span>Full code available on </span><a href="https://gist.github.com/aburjg/1dffba5516095050089f6ea293cb782b#second" rel="nofollow ugc noopener">github gists</a></p><p><strong>Helpful resources:</strong></p><ul><li><p>https://inst.eecs.berkeley.edu/~cs61a/fa14/assets/interpreter/scheme.html</p></li><li><p>http://languagelog.ldc.upenn.edu/myl/llog/jmc.pdf</p></li><li><p>https://justine.lol/sectorlisp2</p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yi 1.5 (139 pts)]]></title>
            <link>https://github.com/01-ai/Yi-1.5</link>
            <guid>40335599</guid>
            <pubDate>Sun, 12 May 2024 16:23:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/01-ai/Yi-1.5">https://github.com/01-ai/Yi-1.5</a>, See on <a href="https://news.ycombinator.com/item?id=40335599">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture> 
  <img src="https://raw.githubusercontent.com/01-ai/Yi/main/assets/img/Yi_logo_icon_light.svg" width="150px">
</picture></themed-picture>
</div>

<p dir="auto">
  <a href="https://huggingface.co/01-ai" rel="nofollow">🤗 HuggingFace</a> •
  <a href="https://www.modelscope.cn/organization/01ai/" rel="nofollow">🤖 ModelScope</a> •
  <a href="https://wisemodel.cn/organization/01.AI" rel="nofollow">✡️ WiseModel</a> 
  <br>
  <a href="https://discord.gg/hYUwWddeAu" rel="nofollow">👾 Discord</a> •
  <a href="https://twitter.com/01ai_yi" rel="nofollow">🐤 Twitter</a> •
  <a href="https://github.com/01-ai/Yi-1.5/issues/2" data-hovercard-type="issue" data-hovercard-url="/01-ai/Yi-1.5/issues/2/hovercard">💬 WeChat</a> 
  <br>
  <a href="https://arxiv.org/abs/2403.04652" rel="nofollow">📝 Paper</a> •
  <a href="https://github.com/01-ai/Yi/tree/main?tab=readme-ov-file#faq">🙌 FAQ</a> •
  <a href="https://github.com/01-ai/Yi/tree/main?tab=readme-ov-file#learning-hub">📗 Learning Hub</a>
</p>
<hr>
<ul dir="auto">
<li><a href="#intro">Intro</a></li>
<li><a href="#news">News</a></li>
<li><a href="#quick-start">Quick Start</a></li>
<li><a href="#web-demo">Web Demo</a></li>
<li><a href="#deployment">Deployment</a></li>
<li><a href="#fine-tuning">Fine-tuning</a></li>
<li><a href="#api">API</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro</h2><a id="user-content-intro" aria-label="Permalink: Intro" href="#intro"></a></p>
<p dir="auto">Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.</p>
<p dir="auto">Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.</p>
<p dir="auto">Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see <a href="https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8" rel="nofollow">Model Card</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">News</h2><a id="user-content-news" aria-label="Permalink: News" href="#news"></a></p>
<ul dir="auto">
<li>2024-05-13: The Yi-1.5 series models are open-sourced, further improving coding, math, reasoning, and instruction-following abilities.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Make sure Python 3.10 or a later version is installed.</p>
</li>
<li>
<p dir="auto">Set up the environment and install the required packages.</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto">Download the Yi-1.5 model from <a href="https://huggingface.co/01-ai" rel="nofollow">Hugging Face</a>, <a href="https://www.modelscope.cn/organization/01ai/" rel="nofollow">ModelScope</a>, or <a href="https://wisemodel.cn/organization/01.AI" rel="nofollow">WiseModel</a>.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto">This tutorial runs Yi-1.5-34B-Chat locally on an A800 (80G).</p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = '<your-model-path>'

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

# Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map=&quot;auto&quot;,
    torch_dtype='auto'
).eval()

# Prompt content: &quot;hi&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;hi&quot;}
]

input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'))
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

# Model response: &quot;Hello! How can I assist you today?&quot;
print(response)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>model_path</span> <span>=</span> <span>'&lt;your-model-path&gt;'</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>, <span>use_fast</span><span>=</span><span>False</span>)

<span># Since transformers 4.35.0, the GPT-Q/AWQ model can be loaded using AutoModelForCausalLM.</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(
    <span>model_path</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>torch_dtype</span><span>=</span><span>'auto'</span>
).<span>eval</span>()

<span># Prompt content: "hi"</span>
<span>messages</span> <span>=</span> [
    {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"hi"</span>}
]

<span>input_ids</span> <span>=</span> <span>tokenizer</span>.<span>apply_chat_template</span>(<span>conversation</span><span>=</span><span>messages</span>, <span>tokenize</span><span>=</span><span>True</span>, <span>add_generation_prompt</span><span>=</span><span>True</span>, <span>return_tensors</span><span>=</span><span>'pt'</span>)
<span>output_ids</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>input_ids</span>.<span>to</span>(<span>'cuda'</span>))
<span>response</span> <span>=</span> <span>tokenizer</span>.<span>decode</span>(<span>output_ids</span>[<span>0</span>][<span>input_ids</span>.<span>shape</span>[<span>1</span>]:], <span>skip_special_tokens</span><span>=</span><span>True</span>)

<span># Model response: "Hello! How can I assist you today?"</span>
<span>print</span>(<span>response</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment</h2><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto">Prerequisites: Before deploying Yi-1.5 models, make sure you meet the <a href="https://github.com/01-ai/Yi/tree/main?tab=readme-ov-file#software-requirements">software and hardware requirements</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">vLLM</h3><a id="user-content-vllm" aria-label="Permalink: vLLM" href="#vllm"></a></p>
<p dir="auto">Prerequisites: Download the lastest version of <a href="https://docs.vllm.ai/en/latest/getting_started/installation.html" rel="nofollow">vLLM</a>.</p>
<ol dir="auto">
<li>
<p dir="auto">Start the server with a chat model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m vllm.entrypoints.openai.api_server  --model 01-ai/Yi-1.5-9B-Chat  --served-model-name Yi-1.5-9B-Chat"><pre>python -m vllm.entrypoints.openai.api_server  --model 01-ai/Yi-1.5-9B-Chat  --served-model-name Yi-1.5-9B-Chat</pre></div>
</li>
<li>
<p dir="auto">Use the chat API.</p>
</li>
</ol>
<ul dir="auto">
<li>
<p dir="auto">HTTP</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl http://localhost:8000/v1/chat/completions \
    -H &quot;Content-Type: application/json&quot; \
    -d '{
        &quot;model&quot;: &quot;Yi-1.5-9B-Chat&quot;,
        &quot;messages&quot;: [
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;}
        ]
    }'"><pre>curl http://localhost:8000/v1/chat/completions \
    -H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
    -d <span><span>'</span>{</span>
<span>        "model": "Yi-1.5-9B-Chat",</span>
<span>        "messages": [</span>
<span>            {"role": "system", "content": "You are a helpful assistant."},</span>
<span>            {"role": "user", "content": "Who won the world series in 2020?"}</span>
<span>        ]</span>
<span>    }<span>'</span></span></pre></div>
</li>
<li>
<p dir="auto">Python client</p>
<div dir="auto" data-snippet-clipboard-copy-content="from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = &quot;EMPTY&quot;
openai_api_base = &quot;http://localhost:8000/v1&quot;

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

chat_response = client.chat.completions.create(
    model=&quot;Yi-1.5-9B-Chat&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;},
    ]
)
print(&quot;Chat response:&quot;, chat_response)"><pre><span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>
<span># Set OpenAI's API key and API base to use vLLM's API server.</span>
<span>openai_api_key</span> <span>=</span> <span>"EMPTY"</span>
<span>openai_api_base</span> <span>=</span> <span>"http://localhost:8000/v1"</span>

<span>client</span> <span>=</span> <span>OpenAI</span>(
    <span>api_key</span><span>=</span><span>openai_api_key</span>,
    <span>base_url</span><span>=</span><span>openai_api_base</span>,
)

<span>chat_response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
    <span>model</span><span>=</span><span>"Yi-1.5-9B-Chat"</span>,
    <span>messages</span><span>=</span>[
        {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
        {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>"Tell me a joke."</span>},
    ]
)
<span>print</span>(<span>"Chat response:"</span>, <span>chat_response</span>)</pre></div>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Web Demo</h2><a id="user-content-web-demo" aria-label="Permalink: Web Demo" href="#web-demo"></a></p>
<div data-snippet-clipboard-copy-content="python demo/web_demo.py -c <your-model-path>"><pre><code>python demo/web_demo.py -c &lt;your-model-path&gt;
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Fine-tuning</h2><a id="user-content-fine-tuning" aria-label="Permalink: Fine-tuning" href="#fine-tuning"></a></p>
<p dir="auto">You can use <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>, <a href="https://github.com/modelscope/swift">Swift</a>, <a href="https://github.com/InternLM/xtuner">XTuner</a>, and <a href="https://github.com/yangjianxin1/Firefly">Firefly</a> for fine-tuning. These frameworks all support fine-tuning the Yi series models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<p dir="auto">Yi APIs are OpenAI-compatible and provided at <a href="https://platform.lingyiwanwu.com/" rel="nofollow">Yi Platform</a>. Sign up to get free tokens, and you can also pay-as-you-go at a competitive price. Additionally, Yi APIs are also deployed on <a href="https://replicate.com/search?query=01+ai" rel="nofollow">Replicate</a> and <a href="https://openrouter.ai/models?q=01%20ai" rel="nofollow">OpenRouter</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code and weights of the Yi-1.5 series models are distributed under the <a href="https://github.com/01-ai/Yi/blob/main/LICENSE">Apache 2.0 license</a>.</p>
<p dir="auto"> [
  <a href="#top">Back to top ⬆️ </a>  ] 
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain Uses Quantum Effects, New Study Finds (136 pts)]]></title>
            <link>http://backreaction.blogspot.com/2024/05/brain-really-uses-quantum-effects-new.html</link>
            <guid>40335209</guid>
            <pubDate>Sun, 12 May 2024 15:27:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://backreaction.blogspot.com/2024/05/brain-really-uses-quantum-effects-new.html">http://backreaction.blogspot.com/2024/05/brain-really-uses-quantum-effects-new.html</a>, See on <a href="https://news.ycombinator.com/item?id=40335209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-7793863980395232228" itemprop="description articleBody"><p>
When Roger Penrose originally came out with the idea that the human brain uses quantum effects in microtubules and that was the origin of consciousness, many thought the idea was a little crazy. According to a new study, it turns out that Penrose was actually right… about the microtubules anyways. Let’s have a look.</p><center><iframe width="560" height="315" src="https://www.youtube.com/embed/R6G1D2UQ3gg?si=E96lZI49ezXUIa2G" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  
</center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Whole-body magnetic resonance imaging at 0.05 Tesla (137 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/science.adm7168</link>
            <guid>40335170</guid>
            <pubDate>Sun, 12 May 2024 15:20:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/science.adm7168">https://www.science.org/doi/10.1126/science.adm7168</a>, See on <a href="https://news.ycombinator.com/item?id=40335170">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/science.adm7168: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Solar Storm Knocks Out Farmers' Tractor GPS Systems During Peak Planting Season (115 pts)]]></title>
            <link>https://www.404media.co/solar-storm-knocks-out-tractor-gps-systems-during-peak-planting-season/</link>
            <guid>40334391</guid>
            <pubDate>Sun, 12 May 2024 13:42:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/solar-storm-knocks-out-tractor-gps-systems-during-peak-planting-season/">https://www.404media.co/solar-storm-knocks-out-tractor-gps-systems-during-peak-planting-season/</a>, See on <a href="https://news.ycombinator.com/item?id=40334391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->
  <div>
    <h5>Subscribe</h5>
    <div>
      <p>Join the newsletter to get the latest updates.</p>
      <form data-members-form="subscribe">
        
        
        <div>
          
          <p>
            Great! Check your inbox and click the link.
          </p>
        </div>
        <div>
          
          <p>
            Please enter a valid email address.
          </p>
        </div>
      </form>
    </div>
  </div>

<!--kg-card-end: html-->
<div><p>🖥️</p><p><i><em>404 Media is a journalist-owned website that is written by real people for real people. Sign up to support independent journalism.</em></i></p></div><p>The solar storm that brought the aurora borealis to large parts of the United States this weekend also broke critical GPS and precision farming functionality in tractors and agricultural equipment during a critical point of the planting season, 404 Media has learned. These outages caused many farmers to fully stop their planting operations for the moment.</p><p>One chain of John Deere dealerships warned farmers that the accuracy of some of the systems used by tractors are “extremely compromised,” and that farmers who planted crops during periods of inaccuracy are going to face problems when they go to harvest, according to text messages obtained by 404 Media and an <a href="https://landmarkimp.com/news/news/blog/geomagnetic-storm-affecting-gps-signals--may-2024/?ref=404media.co"><u>update posted by the dealership</u></a>. The outages highlight how vulnerable modern tractors are to satellite disruptions, which experts have been warning about for years. </p><p>“All the tractors are sitting at the ends of the field right now shut down because of the solar storm,” Kevin Kenney, a farmer in Nebraska, told me. “No GPS. We’re right in the middle of corn planting. I’ll bet the commodity markets spike Monday.”&nbsp;</p><p>Specifically, some GPS systems were temporarily knocked offline. This caused intermittent connections and accuracy problems with “Real-Time Kinematic” (RTK) systems, which connect to John Deere “<a href="https://www.deere.com/en/technology-products/precision-ag-technology/guidance/starfire-7500-receiver/?ref=404media.co"><u>StarFire” receivers</u></a> that are in modern tractors and agricultural equipment. RTK systems use GPS plus a stream of <a href="https://www.fullinefarm.com/fckimages/pdf/differential.pdf?ref=404media.co"><u>constantly-updating “correction” data</u></a> from a fixed point on the ground to achieve centimeter-level positional accuracy for planting crops, tilling fields, spraying fertilizer and herbicide, etc.&nbsp;</p><p>According to updates from Landmark Implement, which owns John Deere dealerships in Kansas and Nebraska, the solar storm ruined the accuracy of RTK systems for many farmers using John Deere tractors. Similar systems in other brands of tractors have also been compromised, the dealer and farmers I spoke to said.</p><p>“Due to the way the RTK network works, the base stations were sending out corrections that have been affected by the geomagnetic storm and were causing drastic shifts in the field and even some heading changes that were drastic,” the dealership told farmers Saturday morning. “When you head back into these fields to side dress, spray, cultivate, harvest, etc. over the next several months, we expect that the rows won't be where the AutoPath lines think they are. This will only affect the fields that are planted during times of reduced accuracy. It is most likely going to be difficult—if not impossible—to make AutoPath work in these fields as the inaccuracy is most likely inconsistent.”</p><div><p>🚜</p><p><b><strong>Has your farm been affected by the solar storm? I would love to hear from you. Using a non-work device, you can message me securely on Signal at +1 202 505 1702. Otherwise, send me an email at jason@404media.co.</strong></b></p></div><p>These automated systems have become critical to modern farming (often called “<a href="https://www.scientificamerican.com/article/precision-farming/?ref=404media.co"><u>precision agriculture</u></a>”), with farmers using increasingly automated tractors to plant crops in perfectly straight lines with uniform spacing. Precision agriculture has greatly increased the yield of farms, and a <a href="https://www.ers.usda.gov/webdocs/publications/105894/eib-248.pdf?v=4429.1&amp;ref=404media.co"><u>2023 report by the US Department of Agriculture</u></a> noted that more than 50 percent of corn, cotton, rice, sorghum, soybeans, and winter wheat are planted and harvested with “automated guidance.” Many modern tractors essentially steer themselves, with the oversight of a farmer in the cab. If the planting or harvesting is even slightly off, the tractors or harvesters could damage crops or plant crooked or inconsistently, which can cause problems during the growing season and ultimately reduce yield.</p><p>Landmark Implement first warned farmers about the problems Friday in a text message blast obtained by 404 Media: “Please be advised that there is a significant solar flare and space weather activity currently affecting GPS and RTK networks. This severe geomagnetic storm is the worst since 2005 and is forecasted to continue throughout the weekend.” That message advised farmers to shut down their reliance on the networks entirely.</p><p>The National Oceanic and Atmospheric Administration (NOAA) said that what’s happening is the “most extreme geomagnetic storm since 2003,” and that “there have been preliminary reports of power grid irregularities, degradation to high-frequency communications, GPS, and possibly satellite navigation.” NOAA said the storm <a href="https://www.swpc.noaa.gov/news/historic-geomagnetic-storm-continues?ref=404media.co" rel="noreferrer">will continue into Monday</a>. Geomagnetic storms happen when there is a coronal mass ejection from the sun, which is an eruption of electromagnetic radiation that can impact GPS, the electric grid, and other communications. &nbsp;</p><p>Farmers and experts I spoke to told me that GPS outages in farming are a very big deal. Landmark Implement has not given any further updates to farmers, and John Deere did not respond to a request for comment. Farmers I spoke to seem to think that the situation is getting better, but solar storms are expected to continue hitting Earth over the next day or so.&nbsp;</p><p>“In the corn belt, May 15 is a critical date to get corn planted,” Willie Cade of Repair.org, who has been working to pass legislation that would make tractors more repairable, told me. “But you don’t want to go out there with your equipment right now. Oh my God, the corn belt can’t get its corn in the ground by May 15? It’s huge. I’m thinking it’s going to go away, but if it doesn’t, shit.”</p><p>Tom Schwarz, who owns an organic farm in Nebraska, told me he had to stop planting on Friday and Saturday because of the issue, and said that the outage, combined with a bad weather forecast in the next few days, is threatening the small time window that he has to plant corn.</p><p>“When you have your window, you have to go,” Schwarz said. “We’ve just had two beautiful days from a weather perspective of being able to plant and you know, we just have to sit here. We’re not getting anything done, and it’s driving everyone nuts.”&nbsp;</p><p>Schwarz says organic farms like his rely heavily on precision farming features, because rows of crops are planted very tightly together to prevent weeds from growing in the spaces between plants. “We used to have markers that would scratch a line in the ground that you could then use to kind of drive by eye,” he said. “Now, we plant so tight in terms of how much the tractor can go side to side and how much the equipment can go side to side that if we aren't absolutely perfect, it just doesn't work. You just physically can’t drive that straight [without guidance]. If you're sitting up there in a tractor seat, you can't steer fast enough or well enough to not kill the crop because we're so tight on clearances.” &nbsp;</p><p>The fact that this happened also highlights how vulnerable modern food supplies are to GPS outages. Modern society as a whole is obviously very reliant on GPS, but experts have warned that tractors that rely on internet connections and on satellites are a particularly concerning attack vector.&nbsp;</p><p>Last year, an outage with Inmarsat satellites caused tractors in Australia and New Zealand to “<a href="https://www.smh.com.au/national/farmers-crippled-by-satellite-failure-as-gps-guided-tractors-grind-to-a-halt-20230418-p5d1de.html?ref=404media.co"><u>grind to a halt</u></a>,” and the remote bricking of <a href="https://www.csoonline.com/article/572811/remote-bricking-of-ukrainian-tractors-raises-agriculture-security-concerns.html?ref=404media.co"><u>connected tractors in Ukraine</u></a> in 2022 showed a type of vulnerability many hadn’t considered before. Experts have warned about the <a href="https://www.bbc.com/news/science-environment-61336659?ref=404media.co"><u>possibility of a cyber attack</u></a> targeting tractors or the <a href="https://www.nature.com/articles/s42256-022-00440-4?ref=404media.co"><u>satellites they rely on</u></a> as a major threat to our food supply.&nbsp;</p><p>Farmers in online communities all over the internet have been discussing the GPS outages, with some saying that they’ve been running into “very weird issues.” One <a href="https://thefarmingforum.co.uk/index.php?threads%2Fsolar-storm.409445%2F=&amp;ref=404media.co#post-9299330"><u>farmer in Manitoba posted</u></a> “Yesterday was a shit show. Going to be some stripes in the fields. Today it’s back to normal.” I also messaged with two people who work on separate farms and posted in a <a href="https://www.reddit.com/r/farming/comments/1cp2m78/anyone_else_lose_gps_right_now/?ref=404media.co"><u>Reddit thread called</u></a> “Anyone else lose GPS right now?” I agreed not to use their names because they do not own the farms and were not authorized to speak to the press.</p><p>One of them, in southern Ontario, said that they use a positioning system called TopCon, which normally offers “sub-inch accuracy,” they said. “Yesterday I saw issues I’ve never seen before. The really puzzling part was when it was actually seeing enough satellites to function from time to time it was wandering off position by 3 or even 6 feet at times! And I know that the system thought it was on target because the coverage map on the monitor in the tractor was showing that I was exactly on the row, but physically I was off by a huge amount.”&nbsp;</p><p>The other farmer from Reddit told me that they were in the middle of planting wheat when problems arose. “We didn't shut down, we just carried on and however it went in is how it went in lol,” they said. “I probably take the most pride in my work of those on this farm so the boss said I had to lower my standards of quality for the sake of getting the crop in. It wasn't the common problem of the lines just not matching up, the GPS shading thought it was spot on to where it should be but a lot of the time it either left a gap or overlapped by as much as a few feet. It wasn't even uniform enough that I could recenter the tractor.” </p>
                    <div>
    <div>
      <p>About the author</p>
      <p>Jason is a cofounder of 404 Media. He was previously the editor-in-chief of Motherboard. He loves the Freedom of Information Act and surfing.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-jason-01-copy.jpeg" alt="Jason Koebler" src="https://www.404media.co/content/images/2023/08/404-jason-01-copy.jpeg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23% of bachelor's degrees and 43% of master's degrees have a negative ROI (177 pts)]]></title>
            <link>https://reason.com/2024/05/10/nearly-half-of-all-masters-degrees-arent-worth-getting/</link>
            <guid>40333471</guid>
            <pubDate>Sun, 12 May 2024 11:08:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reason.com/2024/05/10/nearly-half-of-all-masters-degrees-arent-worth-getting/">https://reason.com/2024/05/10/nearly-half-of-all-masters-degrees-arent-worth-getting/</a>, See on <a href="https://news.ycombinator.com/item?id=40333471">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
																		<article>
	<header>
							<p>
				<a href="https://reason.com/category/education/college/">College</a>
			</p>
				
		
					<h2>According to new research, 23 percent of bachelor's degree programs and 43 percent of master's degree programs have a negative ROI.</h2>
				<p>
								
				
						<span>|</span>
			<time datetime="2024-05-10T15:23:31-04:00">5.10.2024 3:23 PM</time>
			
</p>
					
										<div>
						<div>
			<picture>
									<source type="image/webp" data-lazy-srcset="https://d2eehagpk5cl65.cloudfront.net/img/c2400x1350-w2400-q80/uploads/2024/05/dreamstime_xxl_32533865-2400x1350.jpg.webp 2400w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2024/05/dreamstime_xxl_32533865-1200x675.jpg.webp 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2024/05/dreamstime_xxl_32533865-800x450.jpg.webp 800w,https://d2eehagpk5cl65.cloudfront.net/img/c600x338-w600-q80/uploads/2024/05/dreamstime_xxl_32533865-600x338.jpg.webp 600w,https://d2eehagpk5cl65.cloudfront.net/img/c331x186-w331-q80/uploads/2024/05/dreamstime_xxl_32533865-331x186.jpg.webp 331w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2024/05/dreamstime_xxl_32533865-1200x675.jpg.webp 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c1920x1080-w1920-q80/uploads/2024/05/dreamstime_xxl_32533865-1920x1080.jpg.webp 1920w" sizes="(min-width: 753px) 70vw, (min-width: 1190px) 768px, 100vw">
											<source type="image/jpeg" data-lazy-srcset="https://d2eehagpk5cl65.cloudfront.net/img/c2400x1350-w2400-q80/uploads/2024/05/dreamstime_xxl_32533865-2400x1350.jpg 2400w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2024/05/dreamstime_xxl_32533865-1200x675.jpg 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2024/05/dreamstime_xxl_32533865-800x450.jpg 800w,https://d2eehagpk5cl65.cloudfront.net/img/c600x338-w600-q80/uploads/2024/05/dreamstime_xxl_32533865-600x338.jpg 600w,https://d2eehagpk5cl65.cloudfront.net/img/c331x186-w331-q80/uploads/2024/05/dreamstime_xxl_32533865-331x186.jpg 331w,https://d2eehagpk5cl65.cloudfront.net/img/c1200x675-w1200-q80/uploads/2024/05/dreamstime_xxl_32533865-1200x675.jpg 1200w,https://d2eehagpk5cl65.cloudfront.net/img/c1920x1080-w1920-q80/uploads/2024/05/dreamstime_xxl_32533865-1920x1080.jpg 1920w" sizes="(min-width: 753px) 70vw, (min-width: 1190px) 768px, 100vw">
													<img src="https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2024/05/dreamstime_xxl_32533865-800x450.jpg" width="1200" height="675" title="Graduation caps are held in the air with the sky in the background" alt="Graduation caps are held in the air with the sky in the background | Photo 32533865 © Hxdbzxy | Dreamstime.com" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201200%20675'%3E%3C/svg%3E" data-lazy-src="https://d2eehagpk5cl65.cloudfront.net/img/c800x450-w800-q80/uploads/2024/05/dreamstime_xxl_32533865-800x450.jpg">
			</picture>
		</div>
							<p><span>
					 (Photo 32533865 © Hxdbzxy | Dreamstime.com)				</span>
					</p>
								</div>
							
	</header>
	<div>
							<p><span>Is college worth it? Well, it depends on what degree you're getting and where you're getting it, according to a new <a href="https://freopp.org/does-college-pay-off-a-comprehensive-return-on-investment-analysis-563b9cb6ddc5">paper</a> from the Foundation for Research on Equal Opportunity (FREOPP), an economic opportunity think tank.</span></p> <p><span>While more than three-quarters of all bachelor's degrees have a positive return on investment (ROI), according to the paper, master's and associate degrees are much riskier bets—with many costing students in the long run.</span></p> <p><span>The paper, by Senior Fellow Preston Cooper, examined data from over 50,000 degree and certificate programs at thousands of American colleges and universities. Cooper's analysis looked at how much students were earning immediately after graduation, as well as how much they were making 10 years later. The paper also took into account a student's chance of dropping out when calculating a degree program's ROI.</span></p> <p><span>In all, Cooper found that 31 percent of students are enrolled in a program with a negative ROI—meaning that "the earnings benefits of the degree are unlikely to fully compensate students for the cost and risk of pursuing post-secondary education."</span></p> <p><span>However, different kinds of degrees were more likely to have a negative ROI than others. For example, 77 percent of bachelor's degrees and doctoral and professional degrees have a positive ROI. In contrast, just 57 percent of master's and associate degree programs have a positive ROI.&nbsp;</span></p> <p><span>For bachelor's degrees, fine arts, education, and biology programs had the lowest median ROI, while engineering, computer science, and nursing degrees gave students the highest long-term rewards. </span></p> <p><span>However, <a href="https://freopp.wpengine.com/roi-undergraduate/">where college students were enrolled</a> also mattered when it came to ROI. </span><span>For example, an English degree from the University of Virginia has a $581,925 positive return on investment—climbing to over $600,000 when only including students who graduated on time. In contrast, students at Virginia Commonwealth University—another public university—who majored in English have a negative $30,000 ROI, with just a $3,624 benefit for those who end up graduating on time.</span></p> <p><span>"When choosing a college and program of study, students should evaluate several key variables that contribute to ROI. The most important is earnings after graduation," Cooper writes. "Besides starting salary, another critical factor is the institution's completion rate. While students' individual ability and motivation affects their likelihood of completion, research shows that college quality also has an impact on completion rates."</span></p> <p><span>Cooper also pointed out just how much federal dollars go toward funding low-value degree programs. He found that 29 percent of the federal funding that went to the programs he studied went to programs with a negative ROI.</span></p> <p><span>"That figure includes $37 billion in Pell Grants, $47 billion in loans to undergraduates, and $39 billion in loans to graduate students," Cooper writes. "Because ROI is negative for these programs, it's unlikely that most of those loan dollars will be repaid."&nbsp;</span></p> <p><span>This latest paper paints a detailed picture of the kinds of concerns prospective students and their families should take into account when deciding whether to enroll in college. While bachelor's degrees are still a good bet overall, students need to consider what they'll really get out of <em>both</em> the major they want to study and the school they've been accepted into.</span></p>						</div>
		
</article>
<nav>
	
		<p>
        <a href="https://reason.com/2024/05/10/marco-rubio-used-to-know-how-tariffs-work-what-happened/" data-ga-click="true" data-ga-action="Next Article Click" data-ga-label="Marco Rubio Used To Know How Tariffs Work. What Happened?"><span>NEXT:</span> Marco Rubio Used To Know How Tariffs Work. What Happened?</a>
    </p>
	
	<span><a rel="tag" href="https://reason.com/category/education/college/">College</a><a rel="tag" href="https://reason.com/category/education/college/college-debt/">College Debt</a><a rel="tag" href="https://reason.com/category/higher-education/">Higher Education</a><a rel="tag" href="https://reason.com/category/education/">Education</a><a rel="tag" href="https://reason.com/tag/government-spending/">Government Spending</a></span>			
					</nav>
				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garbage Collectors Are Scary (101 pts)]]></title>
            <link>https://www.enyo.de/fw/notes/garbage-collectors-are-scary.html</link>
            <guid>40333375</guid>
            <pubDate>Sun, 12 May 2024 10:32:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.enyo.de/fw/notes/garbage-collectors-are-scary.html">https://www.enyo.de/fw/notes/garbage-collectors-are-scary.html</a>, See on <a href="https://news.ycombinator.com/item?id=40333375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="mailto:fw@deneb.enyo.de">Florian Weimer</a></p><p><b> Garbage collectors are expected to be invisible: if programmers (or even users) notice them, something is wrong. Usually, such problems are are performance problems, but there could be correctness issues as well. This makes implementing them a bit scary. </b></p><p> As a hobby, I am working on a virtual instruction set architecture, mainly to better to understand the nature of computing, and eventually, trade-offs in programming language design. The project stalled for several months because I was reluctant to start working on a garbage collector, which I consider necessary to maintain memory safety. I was a bit puzzled by this, but now I think it is because garbage collectors are scary to work with. Not only are they intended to be largely invisible, but they need to compute a highly non-local property, too: Whether an object is live and cannot yet be deallocated depends on the state of the pointers in all the other object on the heap. Production-grade garbage collectors are very complex, too. So it all seemed very daunting. </p><p> This article attempts to document what I did to get unstuck. Most of the ideas below focus on making the garbage collector operation more obvious. </p><h2><a name="1">What can be done?</a></h2><p> The choice of collector matters. A copying semispace collector, such as Cheney's algorithm looks like a great starting point. (See C. J. Cheney, <a href="https://dl.acm.org/doi/10.1145/362790.362798">A nonrecursive list compacting algorithm</a><img src="https://www.enyo.de/fw/external.png" alt=" <https://dl.acm.org/doi/10.1145/362790.362798>"> (1970), or its Wikipedia page, <a href="https://en.wikipedia.org/wiki/Cheney%27s_algorithm">Cheney's algorithm</a><img src="https://www.enyo.de/fw/external.png" alt=" <https://en.wikipedia.org/wiki/Cheney%27s_algorithm>">, retrieved 2024-03-07.) Such a collector can be very small and simple: Not counting the code for identifying root object pointers and for scanning objects and stacks, a functional implementation of tge collector is just a few dozen lines of code (even fewer if there is already code for duplicating objects). The collector copies all objects from a fromspace to a tospace part of the heap (hence the name “semispace collector”). This has the important side effect that all object addresses are guaranteed to change during a garbage collection cycle. Consequently, any use of an object pointer that was missed and not processed during collection results in very obvious issues (typically a crash, but bogus data could be the result as well). With a less memory-hungry collector that tries to work in place, perhaps one that never moves any live objects at all, unprocessed pointers may go unnoticed until a more complex test case comes along. With a copying collector, it is pretty much a necessity to get the root pointer, stack, and object scanning code right from the start. </p><p> With such a semispace collector, the address range of the previous heap (what used to be the fromspace) can be kept reserved at the kernel level and mapped with the <code>PROT_NONE</code> protection flag, so that accidental deferencing through an outdated pointer that has not been updated in the previous garbage collection cycle is very likely to fault. This works even if pointers are stored externally (probably by accident), in places that are not expected to be scanned by the garbage collector. </p><p> When interfaces with separately developed C code, the untracked pointer problem can be mitigated by careful interface design. The C interface contained in the reference implementation of the Lua programming language uses a virtual stack on which object pointers are stored (<a href="https://lua.org/manual/5.4/manual.html#4.1">Section 4.1, The Stack,</a><img src="https://www.enyo.de/fw/external.png" alt=" <https://lua.org/manual/5.4/manual.html#4.1>"> in Roberto Ierusalimschy et al.: Lua 5.4 Reference Manual (2023)). The object pointers are never exposed. C programmers use stack slot indices to manipulate them instead. Contrast this with the CPython approach, where the actual object pointers are exposed to C extension modules. With a copying collector, restricting access to object pointers to a small part of the virtual machine only seems reasonable, too. </p><p> My virtual ISA has unboxed (not heap-allocated) fixed-size integers and floating point values, both as local variables in procedures, and as fields in heap-allocated objects. The assembler performs type checking, so it already knows whether a register contains an object pointer at the start of each instruction. (An instruction may only use a register as an input if all execution paths to it have a sufficiently compatible type, but the register itself does not have a single fixed type during the execution of the procedure.) At run time, the virtual machine interpreter does not need to perform any type checking because apart from register-to-register moves, separate instructions are used for scalar and pointer values. I already had a working stack unwinder (for exception handling), but it was not clear whether the encoding of the stack frame information at relevant instructions (that may trigger garbage collection) in the form of stack maps was working. Although there is no debugger interface yet, I added a very basic stack dumping implementation, which shows the contents of the stack and, for object pointers identified with the help of stack maps, the object header from the heap. The test suite can then use simple regular expressions to verify that the dump has pointers at the expected places for a set of representative test cases. </p><p> For the pointer maps for heap-allocated objects, it is possible to do better: compare the pointer values from explicit object access with the results of the object scanning procedure that uses the same way for identifying pointers as the garbage collector. (I'm explicitly not aiming for general run-time reflection support, so this is very much an internal testing interface only.) This approach based on redundant data (comparing the garbage collector results against the pointers known to be there) makes it straightforward to do end-to-end testing for the object scanning code. Similar introspective testing is possible for stack maps if there is a way to cast object pointers to integers, losing the pointer property. (Against this is just for internal testing purposes only because exposing the address of objects in this way encourages direct manipulation of objects, perhaps from separately written C code.) The garbage collector will not update these integers anymore, but the address object of the object changes. The test case knows which registers contain pointers and make sure that the collector updated those registers (assuming that we have a copying collector). </p><p> Continuing the theme of using redundant data for consistency checking, it is possible to keep the heap parseable at all times, or at least at safepoints: Starting at the low address of the heap or a segment, it is possible to find all previously allocated objects (whether live or not). For each object, the address of the next object can be determined based on header information found in the current object. This means that object headers must exist, but they are useful in other contexts as well (such as for implementing dynamic dispatch, or checked downcasts from base class pointers to derived class pointers). Heap traversal can be used to verify at certain points (for example, right after garbage collection) that the heap is consistent, without relying on a fresh traversal from the roots. A first phase can build a bitmap containing set bits for all the start addresses of objects in the heap, and a second phase of the verification procedure can then check that every identified pointer is either null, or points to the start of a previously identified heap object. Currently, this verification procedure is very similar to the Cheney-style copying collector (minus the object copying parts), but it can be used unmodified with any other collector that maintains a parseable heap, even if only artificially (by inserting dummy objects to bridge heap gaps). </p><p> One more thing allowed me to get better coverage with the limited tests I have so far: triggering a garbage collection at every allocation, or at least much more frequently than usual. (I think I saw this in Hotspot first, which has various <code>GCALot</code> flags in debugging builds.) With such an execution mode, the tests exercise the garbage collection code much more heavily. </p><h2><a name="2">Was it worthwhile?</a></h2><p> So far, the collector seems to be working well, which should not be surprising given that it is so simple. The collector-specific issues I encountered were associated with the heuristics for heap sizing, and determining the heap limit at which the next garbage collection starts. The address calculations for that (and which memory regions to unmap, given that they will not be needed) turned out to be quite tricky. There were some early integration issues with stack maps because the assembler and virtual machine implementation did not quite agree upon their encoding, but that was already visible in the stack debugging dumps I added for testing. </p><h2>Revisions</h2><ul><li><p>2024-03-07: published</p></li></ul><hr><p><a href="mailto:fw@deneb.enyo.de">Florian Weimer</a><br><a href="https://www.enyo.de/fw/">Home</a> <a href="https://www.enyo.de/fw/blog/">Blog (DE)</a> <a href="https://www.enyo.de/fw/blog/en.html">Blog (EN)</a> <a href="https://www.enyo.de/fw/impressum.html">Impressum</a> <a href="https://www.enyo.de/fw/blog/feeds.html">RSS Feeds</a> </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[16 years of CVE-2008-0166 – Debian OpenSSL Bug (142 pts)]]></title>
            <link>https://16years.secvuln.info/</link>
            <guid>40333169</guid>
            <pubDate>Sun, 12 May 2024 09:19:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://16years.secvuln.info/">https://16years.secvuln.info/</a>, See on <a href="https://news.ycombinator.com/item?id=40333169">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<h3>DKIM keys vulnerable to Debian OpenSSL bug</h3>
<p>DKIM is a mechanism that allows sending mail servers to sign emails with a cryptographic
key published via a DNS TXT record.</p>
<p>By scanning DKIM keys with my tool <a href="https://badkeys.info/">badkeys</a>,
I discovered a surprisingly large number of hosts vulnerable to the 2008 Debian
OpenSSL bug. This trivially allowed sending emails with forged DKIM signatures for those
hosts and thereby also passing DMARC checks.</p>
<p>The hosts included notable names like <em>@cisco.com</em>, <em>@oracle.com</em>,
<em>@skype.net</em>, <em>@github.partners</em>, <em>@partner.crowdstrike.com</em>,
and <em>@seznam.cz</em> (unfixed).
</p>
<h3>How many keys were vulnerable?</h3>
<p>By scanning the Tranco Top 1 Million list, I collected 355,055 TXT records
with a valid RSA key. Ed25519 keys
were only found in negligible numbers (to be precise, two). DKIM does not support
other key types.</p>
<p>Of the records with RSA keys, 855 were vulnerable to CVE-2008-0166 - around 0.24 %.
777 records contained identical keys. Overall, there were 22 unique vulnerable
keys.</p>
<p>I found 21 additional records with vulnerable keys through other methods.
(Details about the <a href="https://16years.secvuln.info/scandkim.html">scanning methodology can be found here</a>.)
</p>
<h3>Most keys came from one company</h3>
<p>Most records in the large set of 777 identical keys
were configured as a CNAME to a host belonging to
the company Cakemail.</p>
<p>I tried disclosing this issue to Cakemail.
The affected host has
a <a href="https://md02.com/.well-known/security.txt" rel="nofollow">security.txt</a>
file with a security contact address (<em>security@cakemail.com</em>) and a link to a PGP
key that does not work (404 error).</p>
<p>After trying to contact
<em>security@cakemail.com</em>, I got this error message:
<em>"We're writing to let you know that the group you tried to contact (security) may not exist, or you may not have permission to post messages to the group."</em>
</p>
<p>Therefore, I did not directly communicate with Cakemail, but they appear to have replaced
their DKIM key. I have disclosed this
issue to many affected organizations. Some have probably contacted Cakemail.</p>
<h3>Why are people using keys with a vulnerability from 2008 in 2024?</h3>
<p>I can only speculate, but it may have to do with timing.
The Debian OpenSSL bug was introduced in 2006. It was discovered and fixed in 2008.
The DKIM specification was published as <a href="https://datatracker.ietf.org/doc/html/rfc4871">RFC 4871</a> in 2007.</p>
<p>DKIM has no requirement to rotate keys. It appears that a sizeable number of companies
configured DKIM TXT records in 2007 and never changed them.</p>
<h3>Why is seznam.cz listed as unfixed?</h3>
<p>The security team at Seznam - a Czech search engine and email provider -
did not believe me when I reported this issue.
They assume that as they are not actively using that key
(<em>beta._domainkey.seznam.cz</em>), that means that it cannot be used to forge emails.
This is, of course, not true.</p>

<h3>Any other disclosure stories?</h3>
<p>While disclosing this issue, I had plenty of bad experiences
with bug bounty programs. In multiple instances, triagers showed
incompetent or highly problematic behavior. I am still in the process of trying
to clarify some issues, I may share some stories at a later point in time.</p>

<h3>Did you find any other vulnerabilities?</h3>
<p>I found some RSA keys with very short key sizes, some with 512 bits, some even
smaller. Those can be broken on consumer hardware these days. I have not looked closer into
this.</p>
<p>I also found some unparseable or defective keys. I have not found any other
interesting vulnerabilities.</p>
<p>Generally, large parts of the DKIM ecosystem use substandard cryptographic security.
The majority of DKIM keys use RSA with 1024 bits. The standard recommendation for RSA keys is
to use a minimum key size of 2048 bit. While it has been assumed that powerful
attackers may be able to break 1024-bit RSA for more than 20 years,
such attacks have not been publicly documented.</p>

<h3>How can I check if my DKIM keys are affected by this vulnerability?</h3>
<p>The latest version of badkeys allows you to scan DKIM keys directly.
badkeys is a Python tool that can be <a href="https://pypi.org/project/badkeys/">installed via pip</a>.</p>
<p>With the parameter <em>--dkim</em>, it checks DKIM records in text files. It supports common
formats like DNS zone files or the output from tools like dig or host:</p>
<pre><span>$ </span><span>badkeys -aw --dkim example.org.zone</span>
rsa[2048] key ok, example.org.zone[12]</pre>
<p>The <em>-a</em> parameter shows all keys found, not
just vulnerable ones.
The <em>-w</em> parameter enables warnings about insecure or discouraged key
sizes and exponent values.</p>
<p>badkeys can also directly scan DKIM TXT records with the parameter <em>--dkim-dns</em>:</p>
<pre><span>$ </span><span>badkeys -aw --dkim-dns 20230601._domainkey.gmail.com dkim._domainkey.entrust.com</span>
rsa[2048] key ok, 20230601._domainkey.gmail.com
blocklist/debianssl vulnerability, rsa[1024], dkim._domainkey.entrust.com
rsawarnings/too_small vulnerability, rsa[1024], dkim._domainkey.entrust.com</pre>
<p>Obvious caveat: a tool like badkeys can
never find all possible vulnerabilities or compromised keys.</p>

<h3>What is BIMI?</h3>
<p>It is a moneymaking scheme invented by certificate authorities.</p>

<h3>What is BIMI, really?</h3>
<p>BIMI allows publishing a logo in SVG format that some email services will display
alongside your emails. While not strictly required by the specification,
usually, that will only happen if you also buy an expensive certificate (so-called
Verified Mark Certificate) that asserts that this is your logo.</p>
<p>Those certificates are very expensive (above $1,000 per year).</p>
<p>Here's a Gmail screenshot showing some logos:</p>
<img src="https://16years.secvuln.info/gmailscreenshot.avif" alt="Gmail screenshot" width="500" height="350">
<h3>What does BIMI have to do with this vulnerability?</h3>
<p>While testing to send DKIM-signed emails, I noticed
that some would show a company logo in Gmail.</p>
<p>BIMI is designed in a way that when you publish a default BIMI
record, all DKIM-signed emails will automatically get BIMI treatment and show a logo.
In some cases, I only controlled the DKIM key of a subdomain, but even then,
the BIMI logo configured for the main domain is shown automatically.</p>

<h3>Wait, haven't you just said that I need an expensive certificate?
Doesn't that mean that there needs to be some signature involved from that certificate?</h3>
<p>No, and I was very surprised by this.</p>
<p>The <a href="https://www.ietf.org/archive/id/draft-brand-indicators-for-message-identification-05.html">BIMI spec</a>
is written in a way that there is no cryptographic connection
between the certificate and the DKIM key. As soon as one is able to sign a mail with DKIM,
it passes BIMI.</p>
<p>While BIMI certificates are standard X.509 certificates that contain a cryptographic
key, that key is not used at all.</p>
<p>Certificate authorities in other areas, e.g., in the WebPKI,
are required to guarantee some basic security checking of cryptographic keys, disallow
short RSA keys, and revoke known-compromised keys. No such mechanisms exist for BIMI
and DKIM.
</p>
<h3>Can you explain this with an example?</h3>
<p>Sure. Let's take the domain <em>entrust.com</em>. It has a DKIM TXT record configured
at <em>dkim._domainkey.entrust.com</em>:</p>
<pre>v=DKIM1; k=rsa; p=MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCyGF0xzO7Eig1H8QdIErjEKOGnIVvoLU5VjcMRBRWZK65NinL+gVnjuMD2mYdjC3f+7sQCWxGDSKIFn/bB+iXxO2x1/ktkwXHQfQ/9FcFuy+LE0Snsm0SwXN/2l1m5f9e1xdswC+dzHt6DIpDSDENsRal019YKQTqwVyB++7QORwIDAQAB</pre>
<p>This is a 1024-bit RSA key generated with a version of Debian vulnerable to
CVE-2008-0166. We can use badkeys to figure out that
the <a href="https://github.com/badkeys/debianopenssl/blob/main/rsa1024/ssl/le32/25731-rnd.key">private key is this one</a>:</p>
<pre>-----BEGIN RSA PRIVATE KEY-----
MIICWwIBAAKBgQCyGF0xzO7Eig1H8QdIErjEKOGnIVvoLU5VjcMRBRWZK65NinL+
gVnjuMD2mYdjC3f+7sQCWxGDSKIFn/bB+iXxO2x1/ktkwXHQfQ/9FcFuy+LE0Sns
m0SwXN/2l1m5f9e1xdswC+dzHt6DIpDSDENsRal019YKQTqwVyB++7QORwIDAQAB
AoGAH7zFxt0tY6ryaPKkCI0Fjjd21xDTzxFb11U3AO52BeDJ5BmbGo20lidTg96i
SN0/Whf0qDLQcSPdc8Eo+TJ51jIbLPdy16Rn7cPPJzUDIzJPas4ewkI7lsU37hlC
cr7T521l8g6w/mS2TtTZykpsHvb0tztjQuElS7TxWUuVuskCQQDmrvjJU3u9CMkb
3avhUAXlsK4nboo8GTPt4Ss9O2/ipTx3K+qtAfHM7HiZie3iYwxCKQQ3MdMlmh/N
fPR/rTS9AkEAxaPt3EN8wB4w95FAQd9J338n1hXF0lE7UH1aBemuIbWXgY1UTlL4
X1Kx7rLeuQv8mzFkuQyXSFbq7eBXSBOZUwJACKxLbkZVQKYz6XhMHgyELD6YTaM6
T0gjS65LkeHKMxtDSre7+wU3shyx7BPjfb97loE0R174MVG6IF+yUZqRgQJALhtt
LTqNSuCAOfEn1XY67KnkaDxSFxueQ8vKiaCXYAPWIYIQDemrScmn+vC9ptvWBXqD
bewzCsxEKFRy6DyyQwJAGVyC2vMIL7aeMhB+V9YVDZJlV3MEGCYmbF84ZtLhSWTX
QfUe81qGdL3IBMRqz2GnSS0y7qPN/8CP5W9cKy8aDg==
-----END RSA PRIVATE KEY-----</pre>
<p>We can use a tool like <a href="https://manpages.ubuntu.com/manpages/trusty/man1/dkimsign.1.html">dkimsign</a> (part of <a href="https://pypi.org/project/dkimpy/">dkimpy</a>)
to sign a mail with a sender using
an <em>@entrust.com</em> mail address:</p>
<pre><span>$ </span><span>dkimsign dkim entrust.com 25731-rnd.key &lt; unsigned.txt &gt; signed.txt</span></pre>
<p>You can find a <a href="https://16years.secvuln.info/testmail.msg">signed test mail here</a>.</p>
<p><em>entrust.com</em> has a BIMI default record configured at <em>default._bimi.entrust.com</em>.
Therefore, all DKIM-signed emails automatically get BIMI treatment.
The record looks like this:</p>
<pre>v=BIMI1;l=https://bimi.entrust.net/entrust.com/logo.svg;a=https://bimi.entrust.net/entrust.com/certchain.pem</pre>
<p>The URLs point to a logo in SVG format (<em>l=</em>)
and a corresponding certificate (<em>a=</em>).</p>
<p>A BIMI-supporting email service receiving an email with a valid DKIM signature for <em>entrust.com</em> - which
we can generate, as we have the private key - will show a BIMI logo.</p>
<p>By the way, have I mentioned that Entrust is one of only two companies that sell
BIMI certificates?</p>

<h3>Any other security problems with BIMI?</h3>
<p>Yes, after learning about this, I started reading the specification. It is extremely
weird and has
<a href="https://mailarchive.ietf.org/arch/msg/bimi/PS8Xf1hQ41oCAwtsUvVsbRSs34Q/">obvious security design flaws</a>.
It does "trustworthy" things like explaining
that certain things (that are crucial for a secure implementation) are explained
<em>"elsewhere"</em> or <em>"in other documents"</em> - without revealing
where <em>"elsewhere"</em> is or where to
find these <em>"other documents"</em>.</p>

<h3>I am an email provider. Should I implement BIMI?</h3>
<p>No.</p>

<h3>I develop an email client. Should I implement BIMI?</h3>
<p>It is impossible to implement BIMI in mail user agents in a secure way based on its specification.
You need additional security measures that are explained
<em>"elsewhere"</em> and <em>"in other documents"</em>.</p>

<h3>I am developing a mail server. What should I do about BIMI?</h3>
<p>You could implement BIMI troll mode by inserting a logo "say no to BIMI"
into emails with BIMI records configured.</p>

<h3>Wait, that works? What about the certificates and all that?</h3>
<p>It would work in any MUA that implements the current version of
the BIMI specification. However, I am not aware of any that do.</p>
<p>According to the specification, the mail server checks the certificate and sets
those headers. The MUA
should trust the MTA based on <em>"locally defined checks</em>". It is not explained what that
means. (Possibly, that explanation is <em>"elsewhere"</em>
or <em>"in other documents"</em>.)</p>


<h3>Why have you created a logo for this vulnerability?</h3>
<p>I haven't. It is
<a href="https://www.svgrepo.com/svg/41964/confetti">a free icon (CC0) from SVG Repo</a>.</p>

<h3>Anything else?</h3>

<p>If you need help with email security, DKIM, or cryptography,
<a href="https://itsec.hboeck.de/">you can find more information and reach me here</a>.</p>

<p>These days, I spend a lot less time working on IT security. Most of my time, I try
to understand the technologies we need to fix the climate crisis and create hopefully
insightful and well-researched content. If that sounds interesting to you, you can
<a href="https://industrydecarbonization.com/">you can read some articles here</a> and
<a href="https://industrydecarbonization.com/subscribe.html">subscribe to my newsletter</a>.</p>

<p><a href="https://hboeck.de/">Hanno Böck</a><br>
First published: <time datetime="2024-05-12">2024-05-12</time><br>
Last update: <time datetime="2024-05-12">2024-05-12</time>
</p>


</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Automatically Detecting Under-Trained Tokens in Large Language Models (169 pts)]]></title>
            <link>https://arxiv.org/abs/2405.05417</link>
            <guid>40332651</guid>
            <pubDate>Sun, 12 May 2024 06:46:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.05417">https://arxiv.org/abs/2405.05417</a>, See on <a href="https://news.ycombinator.com/item?id=40332651">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.05417">View PDF</a></p><blockquote>
            <span>Abstract:</span>The disconnect between tokenizer creation and model training in language models has been known to allow for certain inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted behaviour. Although such `glitch tokens' that are present in the tokenizer vocabulary, but are nearly or fully absent in training, have been observed across a variety of different models, a consistent way of identifying them has been missing. We present a comprehensive analysis of Large Language Model (LLM) tokenizers, specifically targeting this issue of detecting untrained and under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across various models and provide insights into improving the efficiency and safety of language models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Sander Land [<a href="https://arxiv.org/show-email/216051e6/2405.05417">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 8 May 2024 20:37:56 UTC (5,003 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>