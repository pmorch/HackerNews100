<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 26 Feb 2025 00:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Framework's first desktop is a strange–but unique–mini ITX gaming PC (337 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/</link>
            <guid>43176314</guid>
            <pubDate>Tue, 25 Feb 2025 19:39:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/">https://arstechnica.com/gadgets/2025/02/framework-known-for-upgradable-laptops-intros-not-particularly-upgradable-desktop/</a>, See on <a href="https://news.ycombinator.com/item?id=43176314">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>In Framework's first-party case, the PC starts at $1,099, which gets you a Ryzen AI Max 385 (that's an 8-core CPU and 32 GPU cores) and 32GB of RAM. A fully loaded 128GB with a Ryzen AI Max+ 395 configuration (16 CPU cores, 40 GPU cores) will run you $1,999. There's also an in-between build with the Ryzen AI Max+ 395 chip and 64GB of RAM for $1,599.&nbsp;If you just want the mini ITX board to put in a case of your choosing, that starts at $799.</p>
<p>None of these are impulse buys, exactly, but they're priced a bit better than a gaming-focused mini PC like the Asus ROG NUC, which <a href="https://www.bhphotovideo.com/c/product/1860500-REG/asus_rnuc14srku7169aui_republic_of_gamers_nuc.html?ap=y&amp;smp=y&amp;msclkid=665d398fca4517ecc9d75e33332b247f">starts at nearly $1,300 as of this writing</a> and comes with half as much RAM. It's also priced well compared to what you can get out of a DIY mini ITX PC based on integrated graphics—the Ryzen 7 8700G, an AM5 ITX motherboard, and 32GB of DDR5 can all be had for around $500 collectively before you add a case, power supply, or SSD, but for considerably slower performance.</p>
<p>The volume of the Framework Desktop's first-party case is just 4.5 liters—for reference, <a href="https://ssupd.co/products/cases-meshroom-s-v2">the SSUPD Meshroom S</a> is 14.9 liters, a fairly middle-of-the-road volume for an ITX case that can fit a full-size GPU. An Xbox Series X is about 6.9 liters, and the Xbox Series S is 4.4 liters. Apple's Mac Studio is about 3.7 liters. The Framework Desktop isn't breaking records, but it's definitely tiny. </p>

<figure>
    <p><img width="2560" height="1708" src="https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso.jpg" alt="" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso.jpg 2560w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-640x427.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1024x683.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-768x512.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1536x1025.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-2048x1366.jpg 2048w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-980x654.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/02/FWDesktop-Teardown-Iso-1440x961.jpg 1440w" sizes="auto, (max-width: 2560px) 100vw, 2560px">
                  </p>
          <figcaption>
        <div>
    
    <p>
      Despite the non-upgradeability of the main components, Framework has tried to stick to existing standards where it can by using a flex ATX power supply, ATX headers on the motherboard, regular 120 mm fans that can be changed out, and of course the mini ITX form factor itself.

              <span>
          Credit:

          
          Framework

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>So the pitch for the system is easy: You get a reasonably powerful 1440p-capable gaming and workstation PC inside a case the size of a small game console. "If the Series S could run Windows, I'd buy it in a second" is a thought that has occurred to me, so I can see the appeal, even though it costs at least three times as much.</p>
<p>But it does feel like a strange fit for Framework, given that it's so much&nbsp;<em>less</em> upgradeable than most PCs. The CPU and GPU are one piece of silicon, and they're soldered to the motherboard. The RAM is also soldered down and not upgradeable once you've bought it, setting it apart from nearly every other board Framework sells.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Went to SQL Injection Court (557 pts)]]></title>
            <link>https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</link>
            <guid>43175628</guid>
            <pubDate>Tue, 25 Feb 2025 18:39:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/">https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/</a>, See on <a href="https://news.ycombinator.com/item?id=43175628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          

<p>Should public bodies in Illinois, like cities and school districts
and sheriff’s departments, be allowed to hide information from Freedom
of Information requests by keeping them in databases? That question is
before the 104th Illinois General Assembly, thanks to a bill sponsored
by Donald P. DeWitte, elected state senator by the wise citizens of
Batavia and Elgin (motto: “The City In The Suburbs”; indeed), and
prompted in part by my friend Matt Chapman.</p>
<p>I play a very small part in this story, so I get to tell it.</p>
<h2 id="background">Background</h2>
<p>Illinois has an <a href="https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=85">excellent,
toothy FOIA statute</a>.</p>
<p>With <a href="https://casetext.com/statute/illinois-compiled-statutes/government/chapter-5-general-provisions/subchapter-freedom-of-information/act-140-freedom-of-information-act/section-5-ilcs-1407-exemptions">very
few exceptions</a>, any information collected by an Illinois public body
is public property. Anybody is entitled to ask for it. You can’t
generally be charged for asking. Public bodies can’t really limit the
number of requests you make. They get just 5 days to respond, with 5
additional extension days if requested in writing. Improper denials can
get you legal fee recovery if you sue over them, so there are lawyers
that will take these cases on contingency. It’s pretty neat!</p>
<p>I think people are too shy about making FOIA requests. It’s easier
than it looks! You just need to send an email to the public body you
want information from. Put “FOIA” in the subject line. By law, there’s
no more ceremony to it than that. And you’ll find that the people
responding to those emails are generally kind and happy to help.</p>
<p>The one big limitation of Illinois FOIA (with FOIA laws everywhere, really)
is that you can’t use them to compel public bodies to create new
records. Often, what you’ll be looking for is some kind of report about
some issue of public policy. If that exact report exists, you’re golden.
But if it doesn’t, you have to find and request the raw data for that
report, and you have to assemble it yourself. This limitation is about
to matter a lot.</p>
<p>To understand what’s happening in this story, I’m going to have to
explain a technical concept: the idea of a “database schema”. More and
more of the information tracked by public bodies now lives in databases,
rather than filing cabinets or shared drives. Databases are organized
according to schemas.</p>
<p>Think of a modern database as a huge Excel spreadsheet file, with
many dozens of tabs. Each tab has a name; under each of those tabs is a
separate spreadsheet. Each spreadsheet has a header row, labeling the
columns, like “price” and “quantity” and “name”. A database schema is
simply the names of all the tabs, and each of those header rows.</p>
<p>Congratulations! You now understand databases. 

</p><h2>Matt Chapman vs. City of Chicago</h2>
<p>My friend Matt is a self-styled “<a href="https://mchap.io/">civic
hacker</a>” and a national expert at performing data journalism with
large-scale FOIA requests. Matt’s love language is pushing FOIA statutes
to their limits, sniffing out buried data and bulk-extracting it with
clever requests.</p>
<p>A good example of the kind of stuff Matt does is this <a href="https://www.propublica.org/article/chicago-ticket-data-chi-hack-night">ProPublica
collaboration</a> about how Chicago issues parking tickets. After Matt
was towed over a facially bogus ticket and successfully took the city to
court over it, he got curious about the patterns of towing for things
like compliance violations. As it turns out, parking tickets have pushed
thousands of Illinoisans into bankruptcy, and, once you <a href="https://mchap.io/parking-ticket-visualization-in-chicago.html">get
your hands on the ticket data</a>, it turns out there’s a very clear
pattern of majority-Black neighborhoods being systematically targeted
for higher enforcement.</p>
<p>In the course of this reporting work, Matt learned about a system
Chicago operates called CANVAS. CANVAS is the central repository for all
parking ticket data in the city. It’s a giant database, and Matt would
very much like to know what’s in it. So he filed a FOIA request for the
CANVAS database schema.</p>
<p>The city flatly refused. To do so, they relied on a specific
exemption in the statute:</p>
<blockquote>
<p>“(o) Administrative or technical information associated with
automated data processing operations, including but not limited to
software, operating protocols, computer program abstracts, file layouts,
source listings, object modules, load modules, user guides,
documentation pertaining to all logical and physical design of
computerized systems, employee manuals, and any other information that,
if disclosed, would jeopardize the security of the system or its data or
the security of materials exempt under this Section.”</p>
</blockquote>
<p>In plain English, this exemption says that public bodies aren’t
required to reveal information that might jeopardize the security of
their systems. You obviously can’t FOIA logins and passwords. You also
generally can’t FOIA the source code of programs they run. Chicago
claimed that Matt was a “hacker”, and that the CANVAS schema could in
the wrong hands put the city at risk.</p>
<p>With the help of Merrick Wayne and Matt Topic of Loevy and Loevy,
Matt sued the city. Here’s where I come in.</p>
<h2 id="they-put-me-on-the-stand">They Put Me On The Stand</h2>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/dresscode.png"></p>
<p>Is the CANVAS schema too scary to give Matt Chapman? To decide that,
we have to answer a bunch of questions:</p>
<ol type="1">
<li>Does disclosure of a database schema really jeopardize the security
of the system?</li>
<li>How plausible or likely does that jeopardy need to be?</li>
<li>Does a database schema constitute “source code”?</li>
<li>Is a SQL schema a “file format”?</li>
<li>And, finally, does the “would jeopardize” language apply to
everything in the exemption, or just to the nearest noun “any other
information”?</li>
</ol>
<p>I’ve spent the last 25 years of my life doing software vulnerability
research, which is a stuffy way of saying that I’m a software developer
who looks for bugs in software that would let people do scary things.
Matt retained me as his expert witness for his trial, which took place
in Cook County Chancery Court. Lined up against me was Bruce Coffing, the
Chief Information Security Officer of the City of Chicago.</p><p> The trial
would revolve mostly around questions 1-3.</p>
<p>At this point, I need to read you in to another technical concept:
“SQL Injection”. “SQL” is the language most programs use to talk to
databases. “SQL Injection” is a security vulnerability that programs
that use SQL can have. It’s the primary way databases get attacked, and it’s
straightforward to explain.</p>
<p>Applications that use databases include in their code “SQL queries”,
which are form-letter templates of questions they might need to ask the
database; for instance:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘[INSERT
NAME]’</span></p>
</blockquote>
<p>Now, let's say it comes time to pull tickets for “Dave Arnold”. Simple: stick
his name in the template:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Dave
Arnold’</span></p>
</blockquote>
<p>But now imagine we need to look up “Bob O’Connor”:</p>
<blockquote>
<p>Retrieve the dates of every parking ticket issued to <span>‘Bob
O’</span><span>Connor’</span></p>
</blockquote>
<p>We’ve confused the database: the name in our query is surrounded by
quotes, but our name includes a quote. Normally, when your program has
this bug, it just generates an error message. But attackers look for
this bug, and do things like:</p>
<blockquote>
<p>Retrieve the data of every parking ticket issued to <span>‘Bob O’</span><span> and also
all the rest of the information in the database including everyone’s
passwords.</span></p>
</blockquote>
<p>This works because the quote the attacker supplied cuts off the text
placeholder in the template; all the rest of the attacker’s input gets
interpreted as code, which the database executes.</p>
<p>Most of the people who will read this post are annoyed with me for
taking the time to explain SQL injection. But that is the experience of
getting on the stand in Chancery Court and making an argument that the
CISO of Chicago was wrong about database vulnerabilities: trying to
ensure that a judge shares your understanding of how software
vulnerabilities work.</p>
<p>On the other hand, if you’re one of my non-nerd readers,
congratulations, you now know how to hack the Internets. If anybody
asks, I didn’t tell you any of this.</p>
<p>The bench trial for Matt’s case came down to the question of whether
releasing the CANVAS schema would enable this attack. Specifically,
Bruce Coffing argued: </p>
<ol>
<li>The schema makes it possible to spot
vulnerabilities.</li>
<li>Further, it makes it easier for attackers to be
sneaky about probing for vulnerabilities.</li>
<li>Finally, it helps attackers
pick which applications are most profitable to attack.</li></ol>
<p>Coffing seems like a perfectly lovely and well-qualified person. <strong>But
no, no to all of this.</strong></p>
<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/slowdown.png"></p>
<p>To Coffing’s first point: you don’t find SQL injection
vulnerabilities by reading database schemas. You find them instead in
the application’s source code, where those database template queries
live. Matt isn’t asking for source code. He just wants the header rows
from the tables.</p>
<p>Here I want to point out that <a href="https://www.documentcloud.org/documents/6746618-Matt-Chapman-v-Department-of-Finance-XX-1092020/?q=file+layout&amp;mode=document#document/p131">I
fucked up in multiple ways</a> expert-witnessing for Matt. For example,
in <a href="https://youtu.be/pPC9Bntrsew?feature=shared&amp;t=28">my affidavit</a>, I wrote that SQL schemas would provide “only marginal
value” to an attacker. Big mistake. Chicago jumped on those words and
said “see, you yourself agree that a schema is of some value to an
attacker.” Of course, I don’t really believe that; “only marginal value”
is just self-important message-board hedging. I also claimed on the
stand that “only an incompetently built application” could be attacked
with nothing but it’s schema. Even I don’t know what I meant by
that.</p>
<p>I recovered my footing when I came up with this argument: “Attackers
like me use SQL injection attacks to recover SQL schemas. The schema is
the product of an attack, not one of its predicates”. This, too, is
self-important puffery. But I’ll tell you who loves “products” and
“predicates”, especially used in relation to each other in a single
sentence: a Chicago Chancery Court judge.</p>
<p>To Coffing’s second argument, about the schema helping attackers stay
off his radar when they try attacks, the problem is that every computer
system connected to the Internet is being attacked every minute of every
day. The noise is deafening.</p>
<p>Thousands of people have built scanner bot programs that probe every
computer system they can find and fire batteries of well-known attacks
(almost none of them ever work, but bots don’t get bored and give up,
and eventually the teenager in Malaysia who launched the bot gets
lucky). Chicago has no operational response to people turning the
doorknobs of their various applications. They can’t; if they did, they’d
spend all their time responding to kids in Kuala Lumpur goofing
around.</p>
<p>Finally, Coffing argued that having the schema might help an attacker
decide whether or not an attack would be profitable. A schema might tell
you, for instance, that an application deals in credit card data. The
thing is, CANVAS already tells you it’s dealing in sensitive
information: it’s the backend for processing parking tickets. You don’t
need a schema to know that CANVAS is interesting to attackers.</p>
<p>The judge bought my arguments. I think my attire gave me
salt-of-the-earth credibility; Coffing wore a suit.</p>
<p>Providing testimony was a lot of fun. I’d like to do it again
sometime. Litigation is super fascinating to watch! For example: we
wanted me to testify after Bruce Coffing, so we’d have some idea of what
arguments we needed to rebut. But we brought the FOIA case, so the
burden was ostensibly on us, and our witnesses went first. But, a-ha!
Invoking an exemption in Illinois FOIA is an affirmative defense, and
the burden of those arguments shifts to the defendant. But wait: to get
fee recovery under the law, we want to assert a willful violation of
FOIA; to make that claim, Chicago argues, the burden shifts back to us.
Ultimately, Matt Topic and Chicago compromised; Topic dropped
“wilfullness” and we got to go second.</p>
<p>I’m not saying this is the most interesting thing ever to have
happened, but only that if someone works out a way to use AI to make a
home version of Chancery Court trials that you can play on a
Playstation, I will rack up 10,000 hours playing that game easily.</p>

<p><img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/iwin.png"></p>
<p>We won. But Chicago immediately appealed. Matt Chapman didn’t get the
CANVAS schema. Two years later, <a href="https://ilcourtsaudio.blob.core.windows.net/antilles-resources/resources/5ff80f52-17df-4b2a-ba91-4b977f150c6f/Chapman%20v.%20Chicago%20Department%20of%20Finance,%202022%20IL%20App%20%281st%29%20200547.pdf">the
case came before the First District Appellate Court</a>.</p>
<p>The basic idea of the appeals court is that the original trial court
is the primary “trier of fact”. You appeal legal conclusions, but the
facts determined in the original case generally stand. Our bench trial
took care of questions 1 and 3. That left 2, 4, and 5. Here’s what the
appeals court found:</p>
<p><em>In considering the danger of disclosing information under FOIA,
how likely does an attack need to be?</em></p><p> Answer: it has to be very
likely.</p><p>The statute says “information that, if disclosed, would
jeopardize”.</p><p> Believe it or not, there’s case law on “would” versus
“could” with respect to safety. “Could” means you could imagine
something happening. But the legal standard for “would” is “clear
evidence of harm leaving no reasonable doubt to the judge”. The statute
set the bar for me very low and I managed to clear it.</p><p> Doesn’t this just
make you want to immediately drop everything and become a litigator? I
want to litigate!</p>
<p><em>Is a SQL schema a “file layout”?</em></p><p>
If a schema isn’t source code and it isn’t a file layout, the exemption
doesn’t appear to apply at all. The verdict: “shrug emoji”. The appeals
court didn’t reach this question, because:</p>
<p><em>Does the “would jeopardize” language in the statute apply to
everything in the exemption, or just to the nearest noun “any other
information”?</em></p><p> Ladies and gentlemen it is time for some legal
mumbo-jumbo.</p>
<p>Here’s the FOIA exemption Chicago relies on: <img src="https://sockpuppet.org/blog/2025/02/09/fixing-illinois-foia/statute.png"> To what does
the qualifying language at point (4) in this text refer? Is it “any
other information” (3)? Os is it “Administrative or technical
information”, meaning everything in the exemption?</p>
<p>If it’s the former, “any other information”, Matt has a problem. That
interpretation means things like file layouts (and
employee manuals and “load modules”, whatever those are) are <em>per
se</em> exempt; that the Illinois legislature meant them as examples of
things that would jeopardize security.</p><p> If it’s the latter, Matt has
already won: whether or not a SQL schema is a “software” or a “file
layout” or a “load module”, we’ve already proven that it won’t
jeopardize security.</p>
<p>The court decides it’s the latter. Also, that I am very charming. We
win on appeal. Chicago immediately appeals again. Whatever’s in CANVAS,
they really don’t want you and I to know about it.</p>
<p>A year and change later, <a href="https://caselaw.findlaw.com/court/il-supreme-court/2201134.html">the
case is decided before the Illinois Supreme Court</a>. And, on the
question of how to read the FOIA statute, the Supreme Court disagrees
with the appeals court. The qualifying language in the statute applies
only to “any other information”. Everything else is “per se” exempt.</p>
<p>We started this legal process, of challenging Chicago’s attempt to
exempt CANVAS from FOIA, with 5 questions. What happens now is that the
4th question, of whether a schema is a “file layout”, finally becomes
very important. The Illinois Supremes have just decided that “file
layouts” are <em>per se</em> exempt under Illinois FOIA.</p>
<p>Is a SQL schema a file layout? Of course not. The same SQL schema can
be used by multiple database engines, and each will use a different
underlying file layout to manage the resulting data.</p>
<p>The McGraw-Hill Dictionary of Scientific &amp; Technical Terms, 6E —
which the Illinois Supreme Court cites — describes a “file layout” as “A
description of the arrangement of the data in a file.” <strong>A SQL schema is
almost the exact opposite thing: it’s an abstraction of the data in a
file, invented specifically so you don’t have to think about how the
data is actually arranged.</strong> Checkmate!</p>
<p>Unfortunately, the Illinois Supreme Court had at their disposal a
second dictionary. In the Merriam-Webster Online Dictionary, a “schema”
is defined as “a structured framework or plan: outline”. “This is a
difference in name only”, said the court. Argh. Schemas are now file
layouts. We lose.</p>
<h3 id="where-this-leaves-us">Where This Leaves Us</h3>
<p>Obviously, <a href="https://www.bettergov.org/2023/05/19/better-government-association-statement-supreme-court-foia-ruling-limits-transparency-significantly-broadens-exemptions-to-foia/">we
should have won on appeal to the Illinois Supremes</a>. If you sit on
that court, call me, we can straighten this out.</p>
<p>That said: today, Illinois public bodies can refuse to divulge
database schemas.</p>
<p>This is problematic, because more and more data is finding its way
out of file cabinets and shared drives and Word documents and into
specialized applications, where the only way to get at the underlying
data is to FOIA a database query.</p>
<p>Databases shouldn’t be a safe harbor for municipalities to conceal
information from the public.</p>
<p>But, thanks to the good people of Elgin, and also Crystal Lake
(motto: “No, Not The One From Friday the 13th”), the Illinois
legislature has an opportunity to fix this. <a href="https://www.ilga.gov/legislation/fulltext.asp?DocName=&amp;SessionId=114&amp;GA=104&amp;DocTypeId=SB&amp;DocNum=226&amp;GAID=18&amp;LegID=157537&amp;SpecSess=&amp;Session=">SB0226</a>
would add the following language to the statute:</p>
<blockquote>
<p>[Public bodies] shall provide a sufficient description of the
structures of all databases under the control of the public body to
allow a requester to request the public body to perform specific
database queries.</p>
</blockquote>
<p>⚡️<strong>Hell yes</strong>.⚡️</p>
<p>My understanding is that this bill was proposed in no small part
because Matt Chapman has steadfastly refused to shut up about this
issue, and so I’ll conclude this long piece by saying (1) obviously the
bill should pass, and (2) it should be called “The Chapman Act”.</p>
<p>Call your reps!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The XB-70 (2019) (117 pts)]]></title>
            <link>http://codex99.com/photography/the-xb70.html</link>
            <guid>43175315</guid>
            <pubDate>Tue, 25 Feb 2025 18:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://codex99.com/photography/the-xb70.html">http://codex99.com/photography/the-xb70.html</a>, See on <a href="https://news.ycombinator.com/item?id=43175315">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">

		
		<h3>My Dad and the Cold War</h3>


<p>On the occasion of the public unveiling of the XB-70 Valkyrie, brigadier general Fred Ascani stood at his podium and began addressing the crowd at North American Aviation’s plant no. 42 in Palmdale, California.
General Ascani was the Air Force’s program director for the project and knew as much as anyone about the plane, but as the massive delta-winged XB-70 was towed out of the hangar he stopped his prepared speech, turned to look,
 and, like everyone else in attendance that afternoon, 
simply stared in disbelief.</p>

<p>The XB-70, conceived during the height of the Cold War as a replacement for the B-52, was perhaps the most audacious plane ever commissioned by the Air Force.
It was designed to fly higher and faster than any enemy fighter, drop thermo-nuclear bombs over targets in the Soviet Union, outfly the blast radius, 
and then return home without ever refueling. It was intended to be the ultimate display of American technological prowess; the ultimate weapon of mass destruction. The plane wasn’t simply an evolution of any current design, 
but a leap in technology so advanced that it seemed to come from a science-fiction movie.
“She is so unlike other aircraft that comparisons are almost meaningless,” wrote Mel Hunter.
</p>

<p>But by the time that first XB-70 rolled out onto the tarmac in the California desert it had already been replaced by a new weapon of mass destruction—the ballistic missile. 
The plane became a political pawn and was eventually reduced to a just a test program.</p>

<p>By this time my dad had spent six years working on the XB-70 project. So this is the story of my dad’s brush with the Cold War and, as best as I can piece together, the story of my dad before he was my dad.</p>


<p>I.</p>

<p>My dad was the oldest son of a family of tenant farmers in Nothern Indiana. He grew up during the Depression and was so poor, at least according to the family story, that after the farm was 
electrified by Roosevelt in the mid-1930s, my grandfather had to choose between buying him a baseball glove or buying lightbulbs for the farmhouse. 
Obviously, my dad couldn’t make a living on a farm that could barely support my grandfather, so after high school he went to work; first in a hatchery (a job he hated), then for a company that built 
<a href="https://amusementparkives.com/2017/01/31/miniature-train-company/" title="Amusement Parkives">miniature trains</a> for amusement parks (a job he liked).</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/hatchery_lg.jpg" title=", click for larger image"><img src="http://codex99.com/photography/images/xb70/hatchery_sm.jpg" alt="My dad, about to work at Hadley Hatcheries, click for larger image"></a>
	<figcaption>Newspaper article, likely from the <em>Rensselaer Republican, ca.</em>1948</figcaption> 
</figure>

<p>In November, 1952—when my dad was 22—it became clear that he was going to be drafted into the Army and sent to fight in Korea. Instead he enlisted in the Navy,
trained as an aviation structural mechanic, and, among other things, spent time aboard the escort carrier <i>USS Sitkoh Bay.</i>
“It was the best thing that ever happened to him,” my mom said, “If it weren’t for the Navy he would have ended up back on a farm in Indiana.”</p>

<figure>
	<img src="http://codex99.com/photography/images/xb70/navy_sm.jpg" alt="My dad in the Navy, 1953">
	<figcaption>My dad in the Navy, <em>ca.</em>1953</figcaption> 
</figure>

<p>After he was discharged in 1956, my dad didn’t return to the farm, but, with the help of a friend, got a job with the Flight Propulsion Division of General Electric in Evendale (just outside of Cincinnati), 
initially working night shift in the Controls and Accessories department. This is when he met my mom, who was a secretary in a different department.
“He would bring me lunch after working all night,” she told me. They were married within a year.</p>

<p>II.</p>

<p>While my dad was fixing airplane hydraulic systems at the tail end of the war, Eisenhower, the newly-elected president, was working on something a little bigger: a national security policy to counter the growing Soviet military threat in Europe.
This “<a href="http://www.claremont.org/crb/basicpage/a-new-look-at-the-new-look/" title="A New Look at New Look - CRB">New Look</a>” policy, as it was called, relied on a nuclear deterrent with the Strategic Air Command as its’ centerpiece.
The idea was that America didn’t need to match the Soviets’ perceived superiority in conventional forces, but instead, could develop the 
capability to deliver an overwhelming retaliatory nuclear strike anywhere in the world on just a few hours notice.</p>

<p>At the time, the only way to reliably deliver that nuclear strike was by aircraft; first the B-47 Stratojet, then the massive B-52 Stratofortress.
But in 1955, the same year the B-52 was put into service, the Soviets introduced the MiG-19, a supersonic fighter capable of intercepting any American bomber.
Realizing the problem now at hand, the Air Force issued General Operational Requirement No. 38, calling for a next-generation strategic bomber.
After several different design proposals, the contract for this new plane, designated the B-70 and nicknamed the “Valkyrie,” was finally awarded to North American Aviation.
The engine contract was given to General Electric.</p> 

<p>GE was accustomed to adjusting their workforce based on their military and civilian contracts and the XB-70 contract was huge—potentially hundreds of engines. They soon put together a team, including my dad, to design, 
build and test this new engine-the 30,000-lb class YJ93-3. </p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engine_lg.jpg" title="YJ93 schematic, click for larger image"><img src="http://codex99.com/photography/images/xb70/engine_sm.jpg" alt="YJ93 schmatic, click for larger image"></a>
</figure>

<p>Although North American’s chief engineer Harrison Storms was credited with the overall design of the plane—a box fuselage with forward canards and large, foldable wing tips that allowed the plane to literally surf atop its own 
shockwave—the XB-70 was the result of thousands of nameless engineers and technicians.
Among other things, it required new stainless steel-aluminum honeycomb wing and fuselage panels (and new particle-beam welding techniques), more than 200 high-pressure hydraulic devices (including 17 different filters),
a novel anti-lock brake system, and even a new type of heat-ablative paint.
In all, North American reported that 14 <em>million</em> engineering hours were spent on the project.</p>

<p>Like the airframe, the engine required the efforts of hundreds of engineers to design everything from a new turbofan and compressor, to new fire-suppression systems, to a special high-temperature fuel. Exactly what part my dad worked on is unclear; 
I always thought it was an oil pan, but my older brother was sure it was an oil pump. Whatever it was, it was a small contribution to something much bigger and something he was always proud of.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/bob_lg.jpg" title="My dad in a cubicle at GE, early 1960s, click for larger image"><img src="http://codex99.com/photography/images/xb70/bob_sm.jpg" alt="My dad in a cubicle at GE, early 1960s, click for larger image"></a>
	<figcaption>My dad at GE, <em>ca.</em>early 1960s</figcaption> 
</figure>

<p>III.</p>

<p>After the Soviets tested their first thermonuclear bomb in 1953, Eisenhower made research into missile technology his highest defense priority.
Land-based Atlas ICBMs were put into service in 1959 and submarine-based Polaris ICBMs the next year. Missiles were now a much cheaper and more effective way to deliver nuclear weapons
than any plane, and the very idea of a supersonic bomber now seemed obsolete. 
Then, in October 1960, Gary Power’s U2 spy plane was shot down over the Soviet Union and it became clear that no plane,
regardless of how fast or high it could fly, was safe from surface-to-air missiles.</p>

<p>During secret meetings in November, 1959, the chairman of the Joint Chiefs of Staff recommended downsizing the XB-70 to a bare minimum R&amp;D project, but 
during the 1960 presidential campaign both Kennedy and Nixon supported the XB-70 to court California voters. After Kennedy was elected, however, he again
cancelled the project, calling it “unnecessary and economically unjustifiable.” Finally, in March, 1961 a production order for three XB-70s was placed.
All of the weapon systems were cancelled and the program was reduced to a study of high-speed aeronautics.</p>

<p>GE delivered the first engines, which included my dad’s oil pan or pump or whatever, in early 1962, but it took North American another two years to complete the first plane: Air Vehicle 1 was delivered on May 7th, 1964 and Air Vehicle 2 five months later.
The third plane was cancelled before it was completed.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engine_color_lg.jpg" title="Tail half of the YJ93, likely in Evendale, click for larger image"><img src="http://codex99.com/photography/images/xb70/engine_color_sm.jpg" alt="Tail half of the YJ93, likely in Evendale, click for larger image"></a>
	<figcaption><em>“C-2096”</em></figcaption> 
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/xb70_lg.jpg" title="The YJ93 and the XB-70 at Edwards AFB, click for larger image"><img src="http://codex99.com/photography/images/xb70/xb70_sm.jpg" alt="The YJ93 and the XB-70 at Edwards AFB, click for larger image"></a>
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/installation_lg.jpg" title="Technicians installing the YJ93 into AV-1 at Palmdale, click for larger image"><img src="http://codex99.com/photography/images/xb70/installation_sm.jpg" alt="Technicians installing the YJ93 into AV-1 at Palmdale, click for larger image"></a>
	<figcaption>Installation of a J93 in Air Vehicle 1, June 1962<br>
	<em>“14–50. Return to RT Hughes”</em></figcaption> 
</figure>



<figure>
	<a href="http://codex99.com/photography/images/xb70/J93_1_lg.jpg" title="The YJ93 on a dolly at Palmdale, click for larger image"><img src="http://codex99.com/photography/images/xb70/J93_1_sm.jpg" alt="The YJ93 on a dolly at Palmdale, click for larger image"></a>
</figure>
<figure>
	<a href="http://codex99.com/photography/images/xb70/J93_2_lg.jpg" title="The YJ93 on a dolly at Palmdale, verso, click for larger image"><img src="http://codex99.com/photography/images/xb70/J93_2_sm.jpg" alt="The YJ93 on a dolly at Palmdale, verso, click for larger image"></a>
	<figcaption>A completed J93 at the GE hanger at Edwards AFB<br>
	Until September 1965 this image was classified by the Dept. of Defense</figcaption> 
</figure>


<figure>
	<a href="http://codex99.com/photography/images/xb70/booklet_lg.jpg" title="The Mach 3 Commeorative booklet, click for larger image"><img src="http://codex99.com/photography/images/xb70/booklet_sm.jpg" alt="The Mach 3 Commeorative booklet, click for larger image"></a>
	<figcaption>GE commemorative booklet, <em>ca.</em>1967...</figcaption> 
</figure>
<figure>
	<a href="http://codex99.com/photography/images/xb70/booklet_photo_lg.jpg" title="Annotated photo for the commorative booklet, click for larger image"><img src="http://codex99.com/photography/images/xb70/booklet_photo_sm.jpg" alt="Annotated photo for the commorative booklet, click for larger image"></a>
	<figcaption>...and the original photo used for the cover</figcaption> 
</figure>

<p>While congress was debating the future of the B-70 project my parents started a family (older brother, 1960), lived through the Cuban Missile Crisis, bought the only 
<a href="http://www.codex99.com/design/greenhills.html" title="Greenhills">house</a> they ever lived in, experienced the collective shock of the Kennedy Assassination,
then watched the Beatles on Ed Sullivan (although they never liked the Beatles; they liked the <a href="https://www.youtube.com/watch?v=_EPsuOEH1fY" title="Youtube - The Irish Rovers">Irish Rovers</a> and <a href="https://www.youtube.com/watch?v=0Vur0033a-U" title="Youtube - Danny Boy">Vivian Della Chiesa</a>). 
During all of this my dad continued to work on the J93 project during the day and began studying engineering at night. I was born just a week after the first XB-70 was rolled out.
</p>

<p>IV.</p>

<p>On September 21st, 1964, AV-1 took its maiden flight from Palmdale to Edwards AFB. One engine failed during the flight and the rear wheels locked on landing, causing a fire.
Later problems included honeycomb panels that separated, landing gear that wouldn’t retract, loss of hydraulic pressure and even peeling paint.
Eventually the mechanical problems were fixed and the program began achieving its operational goals: the plane reached Mach 1 on October 12th and Mach 2 on March 24th, 1965. Then, on 14 October 14th, 1965—exactly 17 years to the day after Chuck Yeager first broke the 
sound barrier—it surpassed mach 3. AV-2, an updated design that solved many of the problems with the first airframe, first flew in July, 1965, and eight months later reached Mach 3.08.
</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/first_flight_lg.jpg" title="Maiden flight photo, click for larger image"><img src="http://codex99.com/photography/images/xb70/first_flight_sm.jpg" alt="Maiden flight photo, click for larger image"></a>
	<figcaption><em>“XB-70A. First Flight. 21 September, 1964. Air Force plant 42 to Edwards AFB.”</em></figcaption> 
</figure>

<figure>
	<a href="http://codex99.com/photography/images/xb70/early_flight_lg.jpg" title="Early test flight photo, click for larger image"><img src="http://codex99.com/photography/images/xb70/early_flight_sm.jpg" alt="Early test flight photo, click for larger image"></a>
	<figcaption>Early test fight. Note the paint loss</figcaption> 
</figure>


<p>In 1966 the AV-2 was outfitted with sensors for a joint USAF/NASA program to study sonic boom signtures. Then on June 8th, 1966, GE requested a publicity photoshoot of the AV-2 in formation with four other aircraft, all using GE engines. After the photos, an F-104 Starfighter, piloted by NASA’s Joe Walker, collided with the XB-70’s right wing, 
flipped over and damaged the wing and rear stabilizers on the AV-2 before it went down in a fireball, killing Walker. The AV-2 went into an uncontrollable spin. Pilot Al White managed to eject and survived, albeit with serious injuries, but
co-pilot Carl Cross was killed. The AV-2 was destroyed, creating a debris field near Barstow, California.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/postmortem_lg.jpg" title="Engine after the June 8, 1966 accident, click for larger image"><img src="http://codex99.com/photography/images/xb70/postmortem_sm.jpg" alt="Engine after the June 8, 1966 accident, click for larger image"></a>
	<figcaption>Engine postmortem, July 22nd, 1966</figcaption> 
</figure>
<p>Despite the loss of the more capable AV-2, the joint test program continued with the AV-1, which was now speed-limited to Mach 2.5. In 1967 the XB-70 was chosen for its last research program—a NASA study of supersonic aircraft control.
Finally, on February 4th, 1969, the AV-1 took its last flight from Edwards AFB to Wright-Patterson AFB in Dayton, where it was destined to become a museum piece.</p>

<p>In all, the AV-1 flew 83 test flights and the AV-2 flew 46 flights, for a combined total of 235 hours (including 108 minutes above Mach 3).
The Air Force learned that pushing the technological envelope resulted in plane that was difficult to build, difficult to maintain, difficult to fly, and perhaps even more importantly, was incredibly 
expensive; the program cost nearly 1.5 billion dollars, or around 11 million dollars per flight.</p>

<p>The data from the program, which took years to analyze, was used in the design of the Boeing SST project, the French/British Concorde and the B-1 bomber. The data was even used by the Soviets, through old-fashioned espionage, 
in the design of the Tu-144.
</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/engineers_lg.jpg" title="GE engineeres hard at work, click for larger image"><img src="http://codex99.com/photography/images/xb70/engineers_sm.jpg" alt="GE engineeres hard at work, click for larger image"></a>
	<figcaption>Project engineers in their white Arrow shirts and dark ties<br>
	<em>PN 100265 <del> 10 </del> 9</em></figcaption> 
</figure>

<p>GE built only 38 engines for the XB-70, and as the project winded down most involved with the engine were reassigned to other projects or were laid off. My dad, however, remained on the J93 project to the very end. 
He even went to California to watch a test flight. Then, after five years of night school, he finally completed an undergraduate degree in engineering and became the first person on either side of the family to ever graduate from college.</p>

<p>V.</p>

<p>After the J93 project my dad, now a licensed mechanical engineer, worked on a technical manual (a job he hated), then the gas turbine LM2500 engine (a job he liked).
Eventually, however, he tired of the constant reassignments and worried about his job security, so in 1972 he left GE to work at Nixon’s newly-created National Institute of Occupational Safety and Health.
At the age of 41 he reinvented himself as an industrial hygenist,
and became, of all things, an international expert in <a href="https://www.tandfonline.com/doi/abs/10.1080/1047322X.1990.10389612" title="An Overview of Push—Pull Ventilation Characteristics">push-pull</a> ventilation.
There’s now even an <a href="https://www.acgih.org/membership/other/awards/robert-t-hughes-memorial-award" title="ACGIH Robert T. Hughes Memorial Award">award</a> given out in his name.</p>

<figure>
	<a href="http://codex99.com/photography/images/xb70/niosh_lg.jpg" title="My dad with his NIOSH training class, click for larger image"><img src="http://codex99.com/photography/images/xb70/niosh_sm.jpg" alt="My dad with his NIOSH training class, click for larger image"></a>
	<figcaption>NIOSH training program, Norman, Oklahoma, summer 1972<br>
	My dad, with his snazzy tie (my daughters’ description, not mine), is in the 2nd row, 2nd from the right</figcaption> 
</figure>

<p>In the early 1990s, when my dad was nearing retirement, we would take a yearly trip to Dayton and the 
<a href="https://www.nationalmuseum.af.mil/" title="National Museum of the USAF">Air Force Museum</a> (usually on the day after Christmas—it became something of a family tradition). In the back hanger was the only remaining XB-70, 
a monster dwarfing the aircraft displayed around it.
 My dad would point to the engine on a yellow dolly under the wing and said to his grandsons, “You know I worked on that, right?”</p>
 
 <p>Although my dad didn’t live long enough to see it, he now has great-grandchildren. In a few years—when their old enough to understand—I’ll take them to the Air Force Museum (hopefully on the day after Christmas), 
 show them the engine under the XB-70
 and tell them the story of their great-grandpa.</p>




<p><span>—February 5th, 2019.</span> <em>Photography</em></p>



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Hey Number 17 ' (300 pts)]]></title>
            <link>https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</link>
            <guid>43175023</guid>
            <pubDate>Tue, 25 Feb 2025 17:48:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/">https://www.404media.co/email/b7eb2339-2ea1-4a37-96cc-a360494c214c/</a>, See on <a href="https://news.ycombinator.com/item?id=43175023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <article>
          <div>
              
<!--kg-card-begin: html-->

<!--kg-card-end: html-->
<p>A venture capital-backed “AI performance monitoring system for factory workers” is proposing what appears to be dehumanizing surveillance of factories, where machine vision tracks workers’ hand movements and output so a boss can look at graphs and yell at them about efficiency.</p><p>In a launch video demoing the product, founders Vivaan Baid and Kushal Mohta put on a skit showing how Optifye.ai would be used by factory bosses.&nbsp;</p><figure><blockquote><p lang="en" dir="ltr">The YC deleted video for sweatshop startup Optifye <a href="https://t.co/vCJvm2HTce">pic.twitter.com/vCJvm2HTce</a></p>— Adam Lerman (@AdamLerman5) <a href="https://twitter.com/AdamLerman5/status/1894215433366245457?ref_src=twsrc%5Etfw">February 25, 2025</a></blockquote>
</figure><p>“Ugh, it’s workspace 17. Workspace 17 is the bottleneck. The worst performing workspace here,” one of the bosses says, while watching a video of a man making clothing in a factory. “Hey number 17, what’s going on man? You are in red,” he says. “I have been working all day,” the person playing the worker says. “Working all day?” the line boss replies. “You haven’t hit your hourly output even once today. And you have 11.4% efficiency, this is really bad!”&nbsp;</p><p>“It’s just been a rough day,” the “worker” replies. “Rough day?” the boss says, looking at a calendar full of red days. “More like a rough month.”&nbsp;</p><p>Optifye.ai, launched by Duke University computer science students Baid and Mohta, is backed by Y Combinator, <a href="https://archive.is/GtuTi" rel="noreferrer">according to the company’s site</a>. On their <a href="https://archive.is/Qu09c"><u>Y Combinator company profile</u></a>, they write that both of their families run manufacturing plants, where they’ve been exposed to factory working conditions since they were children. “I've been around assembly lines for as long as I can remember,” Baid wrote.&nbsp;</p><p>Mohta wrote, “My family also runs several manufacturing plants in various industries, which has given me unrestricted access to assembly lines since I was 15.”&nbsp;</p><p>They hope to sell cameras to factory owners to use on assembly lines, their website says, and “use computer vision to tell supervisors who's working and who's not in real-time.”</p><p>Y Combinator deleted its recent Linkedin and X posts congratulating the company on launching.</p><figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXex26pUs8TaRaVO2vdq6klwTPae_pOM-aPnusQ-7taTTHHXcV3knsBxpl1boHfEFumrOfXZigYa0kti6H4K0JPvkpEpC3IOlHXj-SnYvPhf1RQ2lSmW0zOxFIo-rv8snT7FOLJF?key=Yrtm75ZzgcMcjAeui5ZPpzJb" alt="" loading="lazy" width="444" height="553"></figure><p>On their Y Combinator profile, Baid and Mohta outline who gets what out of installing micromanaging AI surveillance on assembly lines. Owners gets “accurate real-time factory, line, and worker productivity metrics,” production heads get “line-wise and worker-wise metrics,” shopfloor supervisors get to “identify who/what is causing inefficiency in the line and fix the problem on the go.” For the workers? They get the tantalizing benefit of being “held accountable for good or bad performance.”&nbsp;</p><p>Worker surveillance is already happening across industries. After the rise of remote work, companies started tracking workers’ productivity based on mouse movements, so <a href="https://www.vice.com/en/article/mouse-mover-jiggler-app-keep-screen-on-active/"><u>workers started using “mouse jigglers”</u></a> so they could walk away from their computers and use the bathroom in peace. In Amazon warehouses, workers <a href="https://www.vice.com/en/article/internal-documents-show-amazons-dystopian-system-for-tracking-workers-every-minute-of-their-shifts/"><u>are tracked and punished</u></a> for not meeting grueling expectations and <a href="https://www.bbc.com/news/business-64384287"><u>bathroom breaks are timed</u></a>, resulting in <a href="https://www.oxfamamerica.org/press/press-releases/amazon-and-walmarts-excessive-warehouse-surveillance-erodes-workers-rights-seriously-harms-worker-health-and-safety/"><u>more injuries and less safe working conditions</u></a>. Optifye.ai’s approach and pitch, however, stands out because of the way its founders seem to embrace cruelty to workers in the name of productivity.</p><p>Optifye.ai and Y Combinator did not immediately respond to requests for comment.</p>
                    <div>
    <div>
      <p>About the author</p>
      <p>Sam Cole is writing from the far reaches of the internet, about sexuality, the adult industry, online culture, and AI. She's the author of How Sex Changed the Internet and the Internet Changed Sex.</p>
      
    </div>
      <p><img data-src="/content/images/2023/08/404-sam-10--1-.jpg" alt="Samantha Cole" src="https://www.404media.co/content/images/2023/08/404-sam-10--1-.jpg">  
      </p>
  </div>
          </div>
        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hard problems that reduce to document ranking (163 pts)]]></title>
            <link>https://noperator.dev/posts/document-ranking-for-complex-problems/</link>
            <guid>43174910</guid>
            <pubDate>Tue, 25 Feb 2025 17:37:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noperator.dev/posts/document-ranking-for-complex-problems/">https://noperator.dev/posts/document-ranking-for-complex-problems/</a>, See on <a href="https://news.ycombinator.com/item?id=43174910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
  
  <div><p>There are two claims I’d like to make:</p>
<ol>
<li>LLMs can be used effectively<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> for listwise <a href="https://en.wikipedia.org/wiki/Learning_to_rank#Approaches">document ranking</a>.</li>
<li>Some complex problems can (surprisingly) be solved by <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">transforming</a> them into document ranking problems.</li>
</ol>
<p>I’ve primarily explored both of these claims in the context of using patch diffing to locate N-day vulnerabilities—a sufficiently domain-specific problem that can be solved using general purpose language models as comparators in document ranking algorithms. I demonstrated at <a href="https://youtu.be/IBuL1zY69tY?si=l27sUOaECO-o9QFW&amp;t=1846">RVAsec ‘24</a> that listwise document ranking can be used to locate the specific function in a patch diff that actually fixes a vulnerability described by a security advisory, and later wrote on the <a href="https://bishopfox.com/blog/raink-llms-document-ranking">Bishop Fox blog</a> in greater defense of listwise ranking by publishing a <a href="https://github.com/noperator/raink">command-line tool implementation (<code>raink</code>)</a> to prove the idea.</p>
<p>The key insight is that instead of treating patch diffing as a complex problem requiring specialized security engineering knowledge, you can reframe it as ranking diffs (documents) by their relevance to a security advisory (query), applying proven document ranking techniques from information retrieval.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<center>
<img src="https://noperator.dev/posts/reduction.png">
</center>

<p>

Using this technique, I proved at <a href="https://www.youtube.com/live/aQyBRlu-cA4?si=3V79VdVmPO9D5WVW&amp;t=260">DistrictCon ‘25</a> that GPT-4o mini could locate a fixed vulnerability in a haystack of over 1600 changed (and stripped!) functions in a patch—costing only 5 minutes and 30 cents to do so<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>Document ranking can be applied to other offensive security problems, like identifying candidate functions for fuzzing targets (in addition to using them for <a href="https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/">auto-generating harnesses</a>), or prioritizing potential injection points in a web application for deeper testing. A few potentially powerful improvements to this technique:</p>
<ul>
<li>Analyze the top N ranked results, and then apply the same ranking algorithm to the analyses.</li>
<li>Make the ranked results verifiable; e.g., for N-day vulnerabilities, use an LLM to generate an automatically testable proof-of-concept exploit<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</li>
</ul>
<p>Following Thomas Dullien’s FUZZING ‘24 keynote <a href="https://www.youtube.com/watch?v=Jd1hItbf52k&amp;t=95s">“Reasons for the Unreasonable Success of Fuzzing”</a>, I’m inclined to give a similar talk—“Reasons for the Unreasonable Success of LLMs.”</p>



  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hyperspace (535 pts)]]></title>
            <link>https://hypercritical.co/2025/02/25/hyperspace</link>
            <guid>43173462</guid>
            <pubDate>Tue, 25 Feb 2025 15:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hypercritical.co/2025/02/25/hyperspace">https://hypercritical.co/2025/02/25/hyperspace</a>, See on <a href="https://news.ycombinator.com/item?id=43173462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<div id="nav">
<ul>
<li><a href="https://hypercritical.co/apps/">Apps</a></li>
<li><a href="https://hypercritical.co/about/">About</a></li>
<li><a href="https://hypercritical.co/archive/">Archive</a></li>
<li><a href="https://hypercritical.co/contact/">Contact</a></li>
<li><a href="https://hypercritical.co/feeds/main">RSS</a></li>
</ul>
</div>

<h2><a href="https://hypercritical.co/">Hypercritical<span><img src="https://hypercritical.co/images/tiny-mac.gif" width="16" height="16" alt=""></span></a></h2> 

<hr>



<div><p><time datetime="2025-02-25T10:00:10-05:00">February 25, 2025 at 10:00 AM</time>
</p></div>





<!--
<div class="app-icon-right" style="position:relative;z-index:3;"><a href="/hyperspace/" style="display:block;"><img src="/2025/02/25/images/hyperspace-icon-256.png" width="128" height="128" alt="Hyperspace app icon"></a></div>
-->

<p>My interest in file systems started when I discovered how <a href="https://folklore.org/The_Grand_Unified_Model_The_Finder.html">type and creator codes</a><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> and <a href="https://folklore.org/The_Grand_Unified_Model.html?sort=date">resource forks</a> contributed to the fantastic user interface on my original Macintosh in 1984. In the late 1990s, when it <a href="https://hypercritical.co/2025/02/25/images/macworld-february-1997.jpg">looked like</a> Apple might buy <a href="https://en.wikipedia.org/wiki/Be_Inc.">Be Inc.</a> to solve its operating system problems, the <a href="https://en.wikipedia.org/wiki/Be_File_System">Be File System</a> was the part I was <a href="http://nobius.org/~dbg/practical-file-system-design.pdf">most excited</a> about. When Apple bought NeXT instead and (<a href="https://en.wikipedia.org/wiki/Rhapsody_(operating_system)">eventually</a>) created Mac OS X, I was <a href="https://hypercritical.co/fatbits/2005/11/21/whos-minding-the-store">extremely</a> <a href="https://hypercritical.co/fatbits/2005/12/09/zfs-data-integrity-explained">enthusiastic</a> about the <a href="https://www.theregister.com/2007/06/07/apple_using_zfs_in_leopard/">possibility</a> of ZFS becoming the new file system for the Mac. But that <a href="https://arstechnica.com/gadgets/2016/06/zfs-the-other-new-apple-file-system-that-almost-was-until-it-wasnt/">didn’t happen</a> either.</p>

<p>Finally, at WWDC 2017, Apple announced <a href="https://en.wikipedia.org/wiki/Apple_File_System">Apple File System</a> (APFS) for macOS (after <a href="https://www.youtube.com/watch?v=IcyaadNy9Jk&amp;t=1670s">secretly test-converting everyone’s iPhones to APFS and then reverting them back to HFS+</a> as part of an earlier iOS 10.x update in one of the most audacious technological gambits in history).</p>

<p>APFS wasn’t ZFS, but it was still a huge leap over <a href="https://en.wikipedia.org/wiki/HFS_Plus">HFS+</a>. Two of its most important features are <a href="https://en.wikipedia.org/wiki/Snapshot_(computer_storage)">point-in-time snapshots</a> and <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage">copy-on-write</a> <a href="https://en.wikipedia.org/wiki/Apple_File_System#Clones">clones</a>. Snapshots allow for more reliable and efficient <a href="https://en.wikipedia.org/wiki/Time_Machine_(macOS)">Time Machine</a> backups. Copy-on-write clones are based on the same underlying architectural features that enable snapshots: a flexible arrangement between directory entries and their corresponding file contents.</p>

<p>Today, most Mac users don’t even notice that using the “Duplicate” command in the Finder to make a copy of a file doesn’t actually copy the file’s contents. Instead, it makes a “clone” file that shares its data with the original file. That’s why duplicating a file in the Finder is nearly instant, no matter how large the file is.</p>

<p>Despite knowing about clone files since the APFS introduction nearly eight years ago, I didn’t give them much thought beyond the tiny thrill of knowing that I wasn’t eating any more disk space when I duplicated a large file in the Finder. But late last year, as my Mac’s disk slowly filled, I started to muse about how I might be able to get some disk space back.</p>

<p>If I could find files that had the same content but were <i>not</i> clones of each other, I could convert them into clones that all shared a single instance of the data on disk. I took an afternoon to whip up a Perl script (that called out to a command-line tool written in C and another written in Swift) to run against my disk to see how much space I might be able to save by doing this. It turned out to be a lot: dozens of gigabytes.</p>

<p>At this point, there was no turning back. I had to make this into an app. There are plenty of Mac apps that will save disk space by finding duplicate files and then <i>deleting</i> the duplicates. Using APFS clones, my app could reclaim disk space <i><a href="https://hypercritical.co/hyperspace/#without-removing">without removing any files</a>!</i> As a digital pack rat, this appealed to me immensely.</p>

<p>By the end of that week, I’d written a barebones Mac app to do the same thing my Perl script was doing. In the months that followed, I polished and tested the app, and christened it <a href="https://hypercritical.co/hyperspace/">Hyperspace</a>. I’m happy to announce that Hyperspace is now available in the Mac App Store.</p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/2025/02/25/images/hyperspace-icon-1024-lossy.png" width="512" height="512" alt="The Hyperspace app icon, created by Iconfactory" title="The Hyperspace app icon, created by Iconfactory"></a></p>

<p><a href="https://apps.apple.com/us/app/hyperspace-reclaim-disk-space/id6739505345?mt=12"><img src="https://hypercritical.co/images/download-on-mac-app-store.svg" height="40" alt="Download Hyperspace from the Mac App Store"></a></p>

<p>Hyperspace is a free download, and it’s free to scan to see how much space you might save. To actually reclaim any of that space, you will have to <a href="https://hypercritical.co/hyperspace/#purchase">pay for the app</a>.</p>

<p>Like <a href="https://hypercritical.co/apps/">all my apps</a>, Hyperspace is a bit difficult to explain. I’ve attempted to do so, at length, in the <a href="https://hypercritical.co/hyperspace/">Hyperspace documentation</a>. I hope it makes enough sense to enough people that it will be a useful addition to the Mac ecosystem.</p>

<p>For my fellow developers who might be curious, this is my second Mac app that uses SwiftUI and my first that uses the <a href="https://developer.apple.com/documentation/swiftui/migrating-to-the-swiftui-life-cycle">SwiftUI life cycle</a>. It’s also my second app to use Swift 6 and my first to do so since very early in its development. I found it <i>much</i> easier to use Swift 6 from (nearly) the start than to convert an existing, released app to Swift 6. Even so, there are still many rough edges to Swift 6, and I look forward to things being <a href="https://github.com/swiftlang/swift-evolution/blob/main/visions/approachable-concurrency.md">smoothed</a> <a href="https://hachyderm.io/@holly/114039272702099974">out</a> a bit in the coming years.</p>

<p>In <a href="https://atp.fm/617">a recent episode of ATP</a>, I described the then-unnamed Hyperspace as “<a href="https://atp.fm/617">An Incredibly Dangerous App</a>.” Like the process of converting from HFS+ to APFS, Hyperspace modifies files that it did not create and does not own. It is, by far, the riskiest app I’ve created. (Reclaiming disk space ain’t like dusting crops…) But I also think it might be the most useful to the largest number of people. I hope you like it.</p>




<hr>



<p>© 2010-2025 John Siracusa</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Launch HN: Browser Use (YC W25) – open-source web agents (105 pts)]]></title>
            <link>https://github.com/browser-use/browser-use</link>
            <guid>43173378</guid>
            <pubDate>Tue, 25 Feb 2025 15:45:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/browser-use/browser-use">https://github.com/browser-use/browser-use</a>, See on <a href="https://news.ycombinator.com/item?id=43173378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/browser-use/browser-use/raw/main/static/browser-use-dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/browser-use/browser-use/raw/main/static/browser-use.png">
  <img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/browser-use/browser-use/raw/main/static/browser-use.png" width="full">
</picture></themed-picture>
<p dir="auto"><h2 tabindex="-1" dir="auto">Enable AI to control your browser 🤖</h2><a id="user-content-enable-ai-to-control-your-browser-" aria-label="Permalink: Enable AI to control your browser 🤖" href="#enable-ai-to-control-your-browser-"></a></p>
<p dir="auto"><a href="https://github.com/gregpr07/browser-use/stargazers"><img src="https://camo.githubusercontent.com/0f19403bd69bc9dd8ab794fa4cce28d24300bfd37881d117c0b68e001bc86418/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f67726567707230372f62726f777365722d7573653f7374796c653d736f6369616c" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/gregpr07/browser-use?style=social"></a>
<a href="https://link.browser-use.com/discord" rel="nofollow"><img src="https://camo.githubusercontent.com/3d2c750e6207adb7f3fe4b1ed35677bc0b7a12e67bbd3d5e6678a1de6df4c217/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313330333734393232303834323334303431323f636f6c6f723d373238394441266c6162656c3d446973636f7264266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="Discord" data-canonical-src="https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white"></a>
<a href="https://docs.browser-use.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/7f6d88507e659f89eb75356e49b3e34237a3118de06ec289f5b8614eef0db6ce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63756d656e746174696f6e2d2546302539462539332539352d626c7565" alt="Documentation" data-canonical-src="https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue"></a>
<a href="https://cloud.browser-use.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/da9350dbaad0c571888c174efb66b7798a6a02b219334f1ed1a4669339897ca8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c6f75642d2545322539382538312545462542382538462d626c7565" alt="Cloud" data-canonical-src="https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue"></a>
<a href="https://x.com/gregpr07" rel="nofollow"><img src="https://camo.githubusercontent.com/43ecab07dd115c164a7cddcc48ac1f2cf29fac3feb0fea9622075c0345a80083/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f477265676f723f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/Gregor?style=social"></a>
<a href="https://x.com/mamagnus00" rel="nofollow"><img src="https://camo.githubusercontent.com/cc71749cd2ee32cde0bbd96d99c062c14ccfaed867d2d9c5766d4c955352e130/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f4d61676e75733f7374796c653d736f6369616c" alt="Twitter Follow" data-canonical-src="https://img.shields.io/twitter/follow/Magnus?style=social"></a>
<a href="https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615" rel="nofollow"><img src="https://camo.githubusercontent.com/721ea8438276831e89ead91abf596b8d69f58d4683912856313acdd3435ab2dc/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e743f75726c3d68747470732533412532462532466170702e776f726b77656176652e61692532466170692532467265706f7369746f727925324662616467652532466f72675f543550766e3355427377544849734e3164575333766f5067253246383831343538363135266c6162656c436f6c6f723d23454336333431" alt="Weave Badge" data-canonical-src="https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;labelColor=#EC6341"></a></p>
<p dir="auto">🌐 Browser-use is the easiest way to connect your AI agents with the browser.</p>
<p dir="auto">💡 See what others are building and share your projects in our <a href="https://link.browser-use.com/discord" rel="nofollow">Discord</a> - we'd love to see what you create!</p>
<p dir="auto">🌩️ Skip the setup - try our hosted version for instant browser automation! <a href="https://cloud.browser-use.com/" rel="nofollow">Try it now</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto">With pip (Python&gt;=3.11):</p>

<p dir="auto">install playwright:</p>

<p dir="auto">Spin up your agent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from langchain_openai import ChatOpenAI
from browser_use import Agent
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    agent = Agent(
        task=&quot;Go to Reddit, search for 'browser-use', click on the first post and return the first comment.&quot;,
        llm=ChatOpenAI(model=&quot;gpt-4o&quot;),
    )
    result = await agent.run()
    print(result)

asyncio.run(main())"><pre><span>from</span> <span>langchain_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>browser_use</span> <span>import</span> <span>Agent</span>
<span>import</span> <span>asyncio</span>
<span>from</span> <span>dotenv</span> <span>import</span> <span>load_dotenv</span>
<span>load_dotenv</span>()

<span>async</span> <span>def</span> <span>main</span>():
    <span>agent</span> <span>=</span> <span>Agent</span>(
        <span>task</span><span>=</span><span>"Go to Reddit, search for 'browser-use', click on the first post and return the first comment."</span>,
        <span>llm</span><span>=</span><span>ChatOpenAI</span>(<span>model</span><span>=</span><span>"gpt-4o"</span>),
    )
    <span>result</span> <span>=</span> <span>await</span> <span>agent</span>.<span>run</span>()
    <span>print</span>(<span>result</span>)

<span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto">Add your API keys for the provider you want to use to your <code>.env</code> file.</p>

<p dir="auto">For other settings, models, and more, check out the <a href="https://docs.browser-use.com/" rel="nofollow">documentation 📕</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test with UI</h3><a id="user-content-test-with-ui" aria-label="Permalink: Test with UI" href="#test-with-ui"></a></p>
<p dir="auto">You can test <a href="https://github.com/browser-use/web-ui">browser-use with a UI repository</a></p>
<p dir="auto">Or simply run the gradio example:</p>

<div dir="auto" data-snippet-clipboard-copy-content="python examples/ui/gradio_demo.py"><pre>python examples/ui/gradio_demo.py</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demos</h2><a id="user-content-demos" aria-label="Permalink: Demos" href="#demos"></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/use-cases/shopping.py">Task</a>: Add grocery items to cart, and checkout.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=L2Ya9PYNns8" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/67061560/410923398-d9359085-bde6-41d4-aa4e-6520d0221872.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5MjMzOTgtZDkzNTkwODUtYmRlNi00MWQ0LWFhNGUtNjUyMGQwMjIxODcyLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNhNjZjMTcxOWU1YzNjYTU5ZDRkMWQ3MGFlOWQxMGVhNjk0YmM1YmU1OWEyOThkZjY4ODJmMjQ0NTJlNDg2N2ImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iuip4JqlEDSXBL7vS-X7b5Cn3rkDj5_FF2cbFJMWqdI" alt="AI Did My Groceries" secured-asset-link="" data-animated-image=""></a></p>

<p dir="auto">Prompt: Add my latest LinkedIn follower to my leads in Salesforce.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67061560/410929479-1440affc-a552-442e-b702-d0d3b277b0ae.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5Mjk0NzktMTQ0MGFmZmMtYTU1Mi00NDJlLWI3MDItZDBkM2IyNzdiMGFlLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiNzEyNmUxZmQ1ZGNkMGQ2ZTU5OWQ2ZDAzZTViNWI3ZjE4MmM4NmUxMTFiYjFjOTMyNDBhNzcxNTQ1YTk2ZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wcKqqm0bMc_ZWFGnpSOznP1xdGFykWnAcS1FYIMX5C0"><img src="https://private-user-images.githubusercontent.com/67061560/410929479-1440affc-a552-442e-b702-d0d3b277b0ae.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC80MTA5Mjk0NzktMTQ0MGFmZmMtYTU1Mi00NDJlLWI3MDItZDBkM2IyNzdiMGFlLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPThiNzEyNmUxZmQ1ZGNkMGQ2ZTU5OWQ2ZDAzZTViNWI3ZjE4MmM4NmUxMTFiYjFjOTMyNDBhNzcxNTQ1YTk2ZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wcKqqm0bMc_ZWFGnpSOznP1xdGFykWnAcS1FYIMX5C0" alt="LinkedIn to Salesforce" data-animated-image=""></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/use-cases/find_and_apply_to_jobs.py">Prompt</a>: Read my CV &amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'</p>
<details open="">
  <summary>
    
    <span aria-label="Video description apply.to.jobs.8x.mp4">apply.to.jobs.8x.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/67061560/389458132-171fb4d6-0355-46f2-863e-edb04a828d04.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODk0NTgxMzItMTcxZmI0ZDYtMDM1NS00NmYyLTg2M2UtZWRiMDRhODI4ZDA0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjODAxYjgwNmQ2NWYxZGMyMTc3MDZlYWQwMDY4NDk5NDQzZGI1ZTY2MzZiNDhhMjc3MGFiZDcwZDJhNDE2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.OuikpmmfgFRymB1zgU52Yj9sKQ2JVxy7iKPXBIBrZSE" data-canonical-src="https://private-user-images.githubusercontent.com/67061560/389458132-171fb4d6-0355-46f2-863e-edb04a828d04.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODk0NTgxMzItMTcxZmI0ZDYtMDM1NS00NmYyLTg2M2UtZWRiMDRhODI4ZDA0Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjODAxYjgwNmQ2NWYxZGMyMTc3MDZlYWQwMDY4NDk5NDQzZGI1ZTY2MzZiNDhhMjc3MGFiZDcwZDJhNDE2YWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.OuikpmmfgFRymB1zgU52Yj9sKQ2JVxy7iKPXBIBrZSE" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py">Prompt</a>: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/67061560/398959783-242ade3e-15bc-41c2-988f-cbc5415a66aa.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zOTg5NTk3ODMtMjQyYWRlM2UtMTViYy00MWMyLTk4OGYtY2JjNTQxNWE2NmFhLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYxODdkM2EzNjM2MzBkNjlkNzE1MzFjYmVlMzA2NTNlYmIwNzhjODkxODYzMzJkNjZmMzQ2ZDFkNDhkZDQzZjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xDwEGfpOUBptaRvyNJvK_bfpFZyZ1Kni4zv6I28vZE"><img src="https://private-user-images.githubusercontent.com/67061560/398959783-242ade3e-15bc-41c2-988f-cbc5415a66aa.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zOTg5NTk3ODMtMjQyYWRlM2UtMTViYy00MWMyLTk4OGYtY2JjNTQxNWE2NmFhLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYxODdkM2EzNjM2MzBkNjlkNzE1MzFjYmVlMzA2NTNlYmIwNzhjODkxODYzMzJkNjZmMzQ2ZDFkNDhkZDQzZjYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xDwEGfpOUBptaRvyNJvK_bfpFZyZ1Kni4zv6I28vZE" alt="Letter to Papa" data-animated-image=""></a></p>

<p dir="auto"><a href="https://github.com/browser-use/browser-use/blob/main/examples/custom-functions/save_to_file_hugging_face.py">Prompt</a>: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description hugging_face_high_quality.mp4">hugging_face_high_quality.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/67061560/388001354-de73ee39-432c-4b97-b4e8-939fd7f323b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODgwMDEzNTQtZGU3M2VlMzktNDMyYy00Yjk3LWI0ZTgtOTM5ZmQ3ZjMyM2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY5YmVmNWZkYjdlY2NjNTM5MGMyMjY3ZGQwYjZkNGQyNDllNmQ4YTQxZWUxMGJmZjlmZjA0MjllMjcyMjNjZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xhPfKYKFxiCHcrwRW3WWVGGJukaZ6-msGd3BEu3EvM" data-canonical-src="https://private-user-images.githubusercontent.com/67061560/388001354-de73ee39-432c-4b97-b4e8-939fd7f323b3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA1MzAxMDEsIm5iZiI6MTc0MDUyOTgwMSwicGF0aCI6Ii82NzA2MTU2MC8zODgwMDEzNTQtZGU3M2VlMzktNDMyYy00Yjk3LWI0ZTgtOTM5ZmQ3ZjMyM2IzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI2VDAwMzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY5YmVmNWZkYjdlY2NjNTM5MGMyMjY3ZGQwYjZkNGQyNDllNmQ4YTQxZWUxMGJmZjlmZjA0MjllMjcyMjNjZDgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.5xhPfKYKFxiCHcrwRW3WWVGGJukaZ6-msGd3BEu3EvM" controls="controls" muted="muted">

  </video>
</details>


<p dir="auto"><h2 tabindex="-1" dir="auto">More examples</h2><a id="user-content-more-examples" aria-label="Permalink: More examples" href="#more-examples"></a></p>
<p dir="auto">For more examples see the <a href="https://github.com/browser-use/browser-use/blob/main/examples">examples</a> folder or join the <a href="https://link.browser-use.com/discord" rel="nofollow">Discord</a> and show off your project.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Vision</h2><a id="user-content-vision" aria-label="Permalink: Vision" href="#vision"></a></p>
<p dir="auto">Tell your computer what to do, and it gets it done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Agent</h3><a id="user-content-agent" aria-label="Permalink: Agent" href="#agent"></a></p>
<ul>
<li> Improve agent memory (summarize, compress, RAG, etc.)</li>
<li> Enhance planning capabilities (load website specific context)</li>
<li> Reduce token consumption (system prompt, DOM state)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">DOM Extraction</h3><a id="user-content-dom-extraction" aria-label="Permalink: DOM Extraction" href="#dom-extraction"></a></p>
<ul>
<li> Improve extraction for datepickers, dropdowns, special elements</li>
<li> Improve state representation for UI elements</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rerunning tasks</h3><a id="user-content-rerunning-tasks" aria-label="Permalink: Rerunning tasks" href="#rerunning-tasks"></a></p>
<ul>
<li> LLM as fallback</li>
<li> Make it easy to define workfows templates where LLM fills in the details</li>
<li> Return playwright script from the agent</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Datasets</h3><a id="user-content-datasets" aria-label="Permalink: Datasets" href="#datasets"></a></p>
<ul>
<li> Create datasets for complex tasks</li>
<li> Benchmark various models against each other</li>
<li> Fine-tuning models for specific tasks</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">User Experience</h3><a id="user-content-user-experience" aria-label="Permalink: User Experience" href="#user-experience"></a></p>
<ul>
<li> Human-in-the-loop execution</li>
<li> Improve the generated GIF quality</li>
<li> Create various demos for tutorial execution, job application, QA testing, social media, etc.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the <code>/docs</code> folder.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local Setup</h2><a id="user-content-local-setup" aria-label="Permalink: Local Setup" href="#local-setup"></a></p>
<p dir="auto">To learn more about the library, check out the <a href="https://docs.browser-use.com/development/local-setup" rel="nofollow">local setup 📕</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cooperations</h2><a id="user-content-cooperations" aria-label="Permalink: Cooperations" href="#cooperations"></a></p>
<p dir="auto">We are forming a commission to define best practices for UI/UX design for browser agents.
Together, we're exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.</p>
<p dir="auto">Email <a href="mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A">Toby</a> to apply for a seat on the committee.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use Browser Use in your research or project, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}"><pre><span>@software</span>{<span>browser_use2024</span>,
  <span>author</span> = <span><span>{</span>Müller, Magnus and Žunič, Gregor<span>}</span></span>,
  <span>title</span> = <span><span>{</span>Browser Use: Enable AI to control your browser<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2024<span>}</span></span>,
  <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
  <span>url</span> = <span><span>{</span>https://github.com/browser-use/browser-use<span>}</span></span>
}</pre></div>
  
<p>
Made with ❤️ in Zurich and San Francisco
 </p> 
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSearcher: A Local open-source Deep Research (160 pts)]]></title>
            <link>https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</link>
            <guid>43172338</guid>
            <pubDate>Tue, 25 Feb 2025 14:33:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md">https://milvus.io/blog/introduce-deepsearcher-a-local-open-source-deep-research.md</a>, See on <a href="https://news.ycombinator.com/item?id=43172338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deep_researcher_a0170dadd0.gif" alt="deep researcher.gif" id="deep-researcher.gif">
    <span>deep researcher.gif</span>
  </span>
</p>
<p>In the previous post, <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><em>“I Built a Deep Research with Open Source—and So Can You!”</em></a>, we explained some of the principles underlying research agents and constructed a simple prototype that generates detailed reports on a given topic or question. The article and corresponding notebook demonstrated the fundamental concepts of <em>tool use</em>, <em>query decomposition</em>, <em>reasoning</em>, and <em>reflection</em>. The example in our previous post, in contrast to OpenAI’s Deep Research, ran locally, using only open-source models and tools like <a href="https://milvus.io/docs">Milvus</a> and LangChain. (I encourage you to read the <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md">above article</a> before continuing.)</p>
<p>In the following weeks, there was an explosion of interest in understanding and reproducing OpenAI’s Deep Research. See, for example, <a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">Perplexity Deep Research</a> and <a href="https://huggingface.co/blog/open-deep-research">Hugging Face’s Open DeepResearch</a>. These tools differ in architecture and methodology although sharing an objective: iteratively research a topic or question by surfing the web or internal documents and output a detailed, informed, and well-structured report. Importantly, the underlying agent automates reasoning about what action to take at each intermediate step.</p>
<p>In this post, we build upon our previous post and present Zilliz’s <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> open-source project. Our agent demonstrates additional concepts: <em>query routing, conditional execution flow</em>, and <em>web crawling as a tool</em>. It is presented as a Python library and command-line tool rather than a Jupyter notebook and is more fully-featured than our previous post. For example, it can input multiple source documents and can set the embedding model and vector database used via a configuration file. While still relatively simple, DeepSearcher is a great showcase of agentic RAG and is a further step towards a state-of-the-art AI applications.</p>
<p>Additionally, we explore the need for faster and more efficient inference services. Reasoning models make use of “inference scaling”, that is, extra computation, to improve their output, and that combined with the fact that a single report may require hundreds or thousands of LLM calls results in inference bandwidth being the primary bottleneck. We use the <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">DeepSeek-R1 reasoning model on SambaNova’s custom-built hardware</a>, which is twice as fast in output tokens-per-second as the nearest competitor (see figure below).</p>
<p>SambaNova Cloud also provides inference-as-a-service for other open-source models including Llama 3.x, Qwen2.5, and QwQ. The inference service runs on SambaNova’s custom chip called the reconfigurable dataflow unit (RDU), which is specially designed for efficient inference on Generative AI models, lowering cost and increasing inference speed. <a href="https://sambanova.ai/technology/sn40l-rdu-ai-chip">Find out more on their website.</a></p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/Output_speed_deepseek_r1_d820329f0a.png" alt="Output speed- deepseek r1.png" id="output-speed--deepseek-r1.png">
    <span>Output speed- deepseek r1.png</span>
  </span>
</p>
<p>The architecture of <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a> follows our previous post by breaking the problem up into four steps - <em>define/refine the question</em>, <em>research</em>, <em>analyze</em>, <em>synthesize</em> - although this time with some overlap. We go through each step, highlighting <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>’s improvements.</p>
<p>
  <span>
    <img translate="no" src="https://assets.zilliz.com/deepsearcher_architecture_088c7066d1.png" alt="deepsearcher architecture.png" id="deepsearcher-architecture.png">
    <span>deepsearcher architecture.png</span>
  </span>
</p>
<h3 id="Define-and-Refine-the-Question">Define and Refine the Question</h3><pre><code translate="no">Break down the original query into new sub queries: [
  'How has the cultural impact and societal relevance of The Simpsons evolved from its debut to the present?',
  'What changes in character development, humor, and storytelling styles have occurred across different seasons of The Simpsons?', 
  'How has the animation style and production technology of The Simpsons changed over time?',
  'How have audience demographics, reception, and ratings of The Simpsons shifted throughout its run?']
</code></pre>
<p>In the design of DeepSearcher, the boundaries between researching and refining the question are blurred. The initial user query is decomposed into sub-queries, much like the previous post. See above for initial subqueries produced from the query “How has The Simpsons changed over time?”. However, the following research step will continue to refine the question as needed.</p>
<h3 id="Research-and-Analyze">Research and Analyze</h3><p>Having broken down the query into sub-queries, the research portion of the agent begins. It has, roughly speaking, four steps: <em>routing</em>, <em>search</em>, <em>reflection, and conditional repeat</em>.</p>
<p>Our database contains multiple tables or collections from different sources. It would be more efficient if we could restrict our semantic search to only those sources that are relevant to the query at hand. A query router prompts an LLM to decide from which collections information should be retrieved.</p>
<p>Here is the method to form the query routing prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_vector_db_search_prompt</span>(<span>
    question: <span>str</span>,
    collection_names: <span>List</span>[<span>str</span>],
    collection_descriptions: <span>List</span>[<span>str</span>],
    context: <span>List</span>[<span>str</span>] = <span>None</span>,
</span>):
    sections = []
    <span># common prompt</span>
    common_prompt = <span>f"""You are an advanced AI problem analyst. Use your reasoning ability and historical conversation information, based on all the existing data sets, to get absolutely accurate answers to the following questions, and generate a suitable question for each data set according to the data set description that may be related to the question.

Question: <span>{question}</span>
"""</span>
    sections.append(common_prompt)
    
    <span># data set prompt</span>
    data_set = []
    <span>for</span> i, collection_name <span>in</span> <span>enumerate</span>(collection_names):
        data_set.append(<span>f"<span>{collection_name}</span>: <span>{collection_descriptions[i]}</span>"</span>)
    data_set_prompt = <span>f"""The following is all the data set information. The format of data set information is data set name: data set description.

Data Sets And Descriptions:
"""</span>
    sections.append(data_set_prompt + <span>"\n"</span>.join(data_set))
    
    <span># context prompt</span>
    <span>if</span> context:
        context_prompt = <span>f"""The following is a condensed version of the historical conversation. This information needs to be combined in this analysis to generate questions that are closer to the answer. You must not generate the same or similar questions for the same data set, nor can you regenerate questions for data sets that have been determined to be unrelated.

Historical Conversation:
"""</span>
        sections.append(context_prompt + <span>"\n"</span>.join(context))
    
    <span># response prompt</span>
    response_prompt = <span>f"""Based on the above, you can only select a few datasets from the following dataset list to generate appropriate related questions for the selected datasets in order to solve the above problems. The output format is json, where the key is the name of the dataset and the value is the corresponding generated question.

Data Sets:
"""</span>
    sections.append(response_prompt + <span>"\n"</span>.join(collection_names))
    
    footer = <span>"""Respond exclusively in valid JSON format matching exact JSON schema.

Critical Requirements:
- Include ONLY ONE action type
- Never add unsupported keys
- Exclude all non-JSON text, markdown, or explanations
- Maintain strict JSON syntax"""</span>
    sections.append(footer)
    <span>return</span> <span>"\n\n"</span>.join(sections)
</code></pre>
<p>We make the LLM return structured output as JSON in order to easily convert its output to a decision on what to do next.</p>
<p>Having selected various database collections via the previous step, the search step performs a similarity search with <a href="https://milvus.io/docs">Milvus</a>. Much like the previous post, the source data has been specified in advance, chunked, embedded, and stored in the vector database. For DeepSearcher, the data sources, both local and online, must be manually specified. We leave online search for future work.</p>
<p>Unlike the previous post, DeepSearcher illustrates a true form of agentic reflection, inputting the prior outputs as context into a prompt that “reflects” on whether the questions asked so far and the relevant retrieved chunks contain any informational gaps. This can be seen as an analysis step.</p>
<p>Here is the method to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_reflect_prompt</span>(<span>
   question: <span>str</span>,
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    reflect_prompt = <span>f"""Determine whether additional search queries are needed based on the original query, previous sub queries, and all retrieved document chunks. If further research is required, provide a Python list of up to 3 search queries. If no further research is required, return an empty list.

If the original query is to write a report, then you prefer to generate some further queries, instead return an empty list.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
   
    
    footer = <span>"""Respond exclusively in valid List of str format without any other text."""</span>
    <span>return</span> reflect_prompt + footer
</code></pre>
<p>Once more, we make the LLM return structured output, this time as Python-interpretable data.</p>
<p>Here is an example of new sub-queries “discovered” by reflection after answering the initial sub-queries above:</p>
<pre><code translate="no">New search queries <span>for</span> <span>next</span> iteration: [
  <span>"How have changes in The Simpsons' voice cast and production team influenced the show's evolution over different seasons?"</span>,
  <span>"What role has The Simpsons' satire and social commentary played in its adaptation to contemporary issues across decades?"</span>,
  <span>'How has The Simpsons addressed and incorporated shifts in media consumption, such as streaming services, into its distribution and content strategies?'</span>]
</code></pre>
<p>Unlike our previous post, DeepSearcher illustrates conditional execution flow. After reflecting on whether the questions and answers so far are complete, if there are additional questions to be asked the agent repeats the above steps. Importantly, the execution flow (a while loop) is a function of the LLM output rather than being hard-coded. In this case there is only a binary choice: <em>repeat research</em> or <em>generate a report</em>. In more complex agents there may be several such as: <em>follow hyperlink</em>, <em>retrieve chunks, store in memory, reflect</em> etc. In this way, the question continues to be refined as the agent sees fit until it decides to exit the loop and generate the report. In our Simpsons example, DeepSearcher performs two more rounds of filling the gaps with extra sub-queries.</p>
<p>Finally, the fully decomposed question and retrieved chunks are synthesized into a report with a single prompt. Here is the code to create the prompt:</p>
<pre><code translate="no"><span>def</span> <span>get_final_answer_prompt</span>(<span>
   question: <span>str</span>, 
   mini_questions: <span>List</span>[<span>str</span>],
   mini_chuncks: <span>List</span>[<span>str</span>],
</span>):
    mini_chunk_str = <span>""</span>
    <span>for</span> i, chunk <span>in</span> <span>enumerate</span>(mini_chuncks):
        mini_chunk_str += <span>f"""&lt;chunk_<span>{i}</span>&gt;\n<span>{chunk}</span>\n&lt;/chunk_<span>{i}</span>&gt;\n"""</span>
    summary_prompt = <span>f"""You are an AI content analysis expert, good at summarizing content. Please summarize a specific and detailed answer or report based on the previous queries and the retrieved document chunks.

    Original Query: <span>{question}</span>
    Previous Sub Queries: <span>{mini_questions}</span>
    Related Chunks: 
    <span>{mini_chunk_str}</span>
    """</span>
    <span>return</span> summary_prompt
</code></pre>
<p>This approach has the advantage over our prototype, which analyzed each question separately and simply concatenated the output, of producing a report where all sections are consistent with each other, i.e., containing no repeated or contradictory information. A more complex system could combine aspects of both, using a conditional execution flow to structure the report, summarize, rewrite, reflect and pivot, and so on, which we leave for future work.</p>
<p>Here is a sample from the report generated by the query “How has The Simpsons changed over time?” with DeepSeek-R1 passing the Wikipedia page on The Simpsons as source material:</p>
<pre><code translate="no">Report: The Evolution of The Simpsons (1989–Present)
1. Cultural Impact and Societal Relevance
The Simpsons debuted as a subversive critique of American middle-class life, gaining notoriety for its bold satire in the 1990s. Initially a countercultural phenomenon, it challenged norms with episodes tackling religion, politics, and consumerism. Over time, its cultural dominance waned as competitors like South Park and Family Guy pushed boundaries further. By the 2010s, the show transitioned from trendsetter to nostalgic institution, balancing legacy appeal with attempts to address modern issues like climate change and LGBTQ+ rights, albeit with less societal resonance.
…
Conclusion
The Simpsons evolved from a radical satire to a television institution, navigating shifts in technology, politics, and audience expectations. While its golden-age brilliance remains unmatched, its adaptability—through streaming, updated humor, and global outreach—secures its place as a cultural touchstone. The show’s longevity reflects both nostalgia and a pragmatic embrace of change, even as it grapples with the challenges of relevance in a fragmented media landscape.
</code></pre>
<p>Find <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">the full report here</a>, and <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">a report produced by DeepSearcher with GPT-4o mini</a> for comparison.</p>
<p>We presented <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, an agent for performing research and writing reports. Our system is built upon the idea in our previous article, adding features like conditional execution flow, query routing, and an improved interface. We switched from local inference with a small 4-bit quantized reasoning model to an online inference service for the massive DeepSeek-R1 model, qualitatively improving our output report. DeepSearcher works with most inference services like OpenAI, Gemini, DeepSeek and Grok 3 (coming soon!).</p>
<p>Reasoning models, especially as used in research agents, are inference-heavy, and we were fortunate to be able to use the fastest offering of DeepSeek-R1 from SambaNova running on their custom hardware. For our demonstration query, we made sixty-five calls to SambaNova’s DeepSeek-R1 inference service, inputting around 25k tokens, outputting 22k tokens, and costing $0.30. We were impressed with the speed of inference given that the model contains 671-billion parameters and is 3/4 of a terabyte large. <a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency">Find out more details here!</a></p>
<p>We will continue to iterate on this work in future posts, examining additional agentic concepts and the design space of research agents. In the meanwhile, we invite everyone to try out <a href="https://github.com/zilliztech/deep-searcher">DeepSearcher</a>, <a href="https://github.com/zilliztech/deep-searcher">star us on GitHub</a>, and share your feedback!</p>
<ul>
<li><p><a href="https://github.com/zilliztech/deep-searcher"><strong>Zilliz’s DeepSearcher</strong></a></p></li>
<li><p>Background reading: <a href="https://milvus.io/blog/i-built-a-deep-research-with-open-source-so-can-you.md"><strong><em>“I Built a Deep Research with Open Source—and So Can You!”</em></strong></a></p></li>
<li><p><em>“</em><a href="https://sambanova.ai/press/fastest-deepseek-r1-671b-with-highest-efficiency"><strong>SambaNova Launches the Fastest DeepSeek-R1 671B with the Highest Efficiency</strong></a><em>”</em></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1GE3rvxFFTKqro67ctTkknryUf-ojhduN/view?usp=sharing">DeepSeek-R1 report on The Simpsons</a></p></li>
<li><p>DeepSearcher: <a href="https://drive.google.com/file/d/1EGd16sJDNFnssk9yTd5o9jzbizrY_NS_/view?usp=sharing">GPT-4o mini report on The Simpsons</a></p></li>
<li><p><a href="https://milvus.io/docs">Milvus Open-Source Vector Database</a></p></li>
</ul>
</div><section><p>Like the article? Spread the word</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Saved My Life (no, seriously, I'm writing this from the ER) (127 pts)]]></title>
            <link>https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</link>
            <guid>43171639</guid>
            <pubDate>Tue, 25 Feb 2025 13:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er">https://hardmodefirst.xyz/chatgpt-saved-my-life-no,-seriously,-im-writing-this-from-the-er</a>, See on <a href="https://news.ycombinator.com/item?id=43171639">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Unknown illness kills over 50 in Congo with hours between symptoms and death (105 pts)]]></title>
            <link>https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</link>
            <guid>43171371</guid>
            <pubDate>Tue, 25 Feb 2025 13:11:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db">https://apnews.com/article/congo-mystery-unknown-illness-cd8b1fdcb3b2ed032968b2c6044dc6db</a>, See on <a href="https://news.ycombinator.com/item?id=43171371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>KINSHASA, Congo (AP) — An unknown illness first discovered in three children who ate a bat has rapidly killed more than 50 people in northwestern Congo over the past five weeks, health experts say.</p><p>The interval between the onset of symptoms – which include fever, vomiting and internal bleeding – and death has been 48 hours in most cases and “that’s what’s really worrying,” said Serge Ngalebato, medical director of Bikoro Hospital, a regional monitoring center.</p><p>These “hemorrhagic fever” symptoms are commonly linked to known deadly viruses, such as Ebola, dengue, Marburg and yellow fever, but researchers have ruled these out based on tests of more than a dozen samples collected so far.</p><p>The latest disease outbreak in the Democratic Republic of Congo began on Jan. 21, with 419 cases recorded and 53 deaths.</p><p>The outbreak began in the village of Boloko after three children ate a bat and died within 48 hours, the Africa office of the World Health Organization said Monday.</p>
    

<p>There have long been concerns about <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/covid-health-united-nations-animals-world-organization-7d104d2f4a87dd29ddb5d263986f731c">diseases jumping from animals to humans</a></span> in places where wild animals are popularly eaten. The number of such outbreaks in Africa has surged by more than 60% in the last decade, the WHO said in 2022.</p>



<p>After the second outbreak of the mystery disease began in the village of Bomate on Feb. 9, samples from 13 cases were sent to the National Institute for Biomedical Research in Congo’s capital, Kinshasa, for testing, the WHO said. All samples were negative for common hemorrhagic fever diseases, although some tested positive for malaria.</p><p>Last year, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/mystery-disease-congo-malaria-who-c772403273a89b5dcdc86778ad7eeed3">another mystery flu-like illness</a></span> that killed dozens of people in another part of Congo was determined likely to be malaria.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE will use AI to assess the responses of federal workers (114 pts)]]></title>
            <link>https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</link>
            <guid>43171265</guid>
            <pubDate>Tue, 25 Feb 2025 12:56:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439">https://www.nbcnews.com/politics/doge/federal-workers-agencies-push-back-elon-musks-email-ultimatum-rcna193439</a>, See on <a href="https://news.ycombinator.com/item?id=43171265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — Responses to the Elon Musk-directed <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">email to government employees </a>about what work they had accomplished in  the last week are expected to be fed into an artificial intelligence system to determine whether those jobs are necessary, according to three sources with knowledge of the system.</p><p>The information will go into an LLM (Large Language Model), an advanced AI system that looks at huge amounts of text data to understand, generate and process human language, the sources said. The AI system will determine whether someone’s work is mission-critical or not.</p><p>The U.S. Office of Personnel Management emails were sent to federal workers on Saturday, shortly after <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">Musk wrote in a post on X </a>that “all federal employees will shortly receive an email requesting to understand what they got done last week. Failure to respond will be taken as a resignation.”</p><p>The OPM email did not mention the resignation threat, but said: “Please reply to this email with approx. 5 bullets of what you accomplished last week and cc your manager. Please do not send any classified information, links, or attachments. Deadline is this Monday at 11:59pm EST.”</p><p>The reason the email requested no links or attachments was because of the plan to send the information to the AI system, the sources said.</p><p>A request for comment from OPM as to whether humans will be involved in reviewing the responses was not answered immediately. The White House declined to comment.</p><p>But in response to a tweet about the usage of LLMs, Musk wrote on X that they were not “needed here,” and “this was basically a check to see if the employee had a pulse and was capable of replying to an email.” </p><p><a href="https://www.nbcnews.com/politics/politics-news/live-blog/trump-elon-musk-doge-live-updates-rcna193571" target="_blank"><strong><em>Follow live politics coverage here</em></strong></a></p><p>After the deadline for employees to reply to the email had passed late Monday, OPM did not immediately respond to a request for comment regarding how many workers replied and how many were required to do so.</p><p>In an email to its workforce earlier Monday, the Justice Department said that during a meeting with the interagency Chief Human Capital Officers Council, OPM informed agencies that employee responses to the email are voluntary. OPM also clarified that despite what Musk had posted, not responding to the email does not equate to a resignation, the email said.</p><p>Musk complained about backlash to the directive on his<a href="https://x.com/elonmusk/status/1894173297786720718" target="_blank"> social media platform</a> late Monday.</p><p>"The email request was utterly trivial, as the standard for passing the test was to type some words and press send! Yet so many failed even that inane test, urged on in some cases by their managers. Have you ever witnessed such INCOMPETENCE and CONTEMPT for how YOUR TAXES are being spent?" he wrote.</p><p>In a subsequent tweet, he seemed to indicate that a second email could be sent to government workers who don't respond to the first one.</p><p>"Subject to the discretion of the President, they will be given another chance. Failure to respond a second time will result in termination," he <a href="https://x.com/elonmusk/status/1894177129887404484" target="_blank">wrote</a>.</p><p>The White House did not immediately respond to a request for comment on the post.</p><p>The initial directive has faced pushback from unions, workers and even some agencies since it was sent, but the effort was praised by President Donald Trump earlier Monday.</p><p>“I thought it was great,” <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73634?canonicalCard=true" target="_blank">Trump told reporters </a>in the Oval Office, where he was meeting with French President Emmanuel Macron.</p><p>"We have people that don’t show up to work and nobody even knows if they work for the government, so by asking the question ‘tell us what you did this week,’ what he's doing is saying are you actually working. And then, if you don’t answer, like, you’re sort of semi-fired or you're fired," he said, claiming without providing evidence that "a lot of people are not answering because they don't even exist."</p><p>"There was a lot of genius in sending it," Trump said. "If people don’t respond, it’s very possible that there is no such person or they’re not working.”</p><p>A coalition of unions and groups that have been fighting the Trump administration's mass layoffs of probationary workers charge the effort was unlawful. They amended their lawsuit against the U.S. Office of Personnel Management over the weekend to add a claim involving the OPM email directing workers to justify their workweek.</p><p>The<a href="https://storage.courtlistener.com/recap/gov.uscourts.cand.444883/gov.uscourts.cand.444883.17.0.pdf" target="_blank"> lawsuit</a> charges that the administration didn't follow proper procedure for such an order and should be voided by a judge.</p><p>"The mass firings ordered by OPM are illegal and betray the trust of countless federal employees. The patronizing demand that federal workers still on the job have to justify themselves by enumerating five accomplishments just adds insult to injury. That too is against the law," lawyer Norm Eisen said in a statement on behalf of the plaintiffs.</p><p>Musk has been tasked by Trump with reducing the size of the government, and the email is seen as part of his push to reduce the federal workforce by as much as 10%.</p><p>Some agencies, including ones led by close Trump allies, had told their employees to ignore the directive.</p><p>Justice Department employees were informed earlier Monday that they did not need to respond to the message, according to emails seen by NBC News. “Due to the confidential and sensitive nature of the Department’s work, DOJ employees do not need to respond to the email from OPM. If you have already responded to this email, no further action is needed,” read one email sent by Assistant Attorney General for Administration Jolene Ann Lauria.</p><p>FBI Director&nbsp;<a href="https://www.nbcnews.com/politics/justice-department/fbi-director-kash-patel-named-atf-chief-rcna193332" target="_blank">Kash Patel</a>&nbsp;instructed employees over the weekend to “pause any responses” to the email, and said his agency would do its own review. Employees of the<strong>&nbsp;</strong>State Department, the National Institutes of Health, the Defense Department, the National Security Agency and the Office of the Director of National Intelligence were also told not to respond to the email.</p><p>The Department of Agriculture also sent out an unsigned email to employees informing them that any response to the email "is voluntary and not required."</p><p>The email was also sent to an <a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73631?canonicalCard=true" target="_blank">unknown amount</a> of judicial branch employees, including judges and court staffers. Spokespeople for federal courts in Manhattan and the Northern District of Illinois confirmed to NBC News that “some” people had gotten the message.</p><p>Julie Hodek, a spokesperson for the Northern District of Illinois, also confirmed the email and said the court’s chief judge and clerk “communicated with the staff that as we are judiciary employees, our policies and procedures are governed by the Judicial Conference of the United States and our local court HR handbooks.”</p><p>A spokesperson for the Southern District of New York said personnel there had been directed not to respond to the email.</p><p>Managers within the Environmental Protection Agency on Monday sent employees model responses to the email to make it easier for them.</p><p>“As empathy for their staff, they sent examples,” said one agency employee who shared two of the managers’ own responses with NBC News. The employee asked NBC not to publish the managers’ responses in full out of fear of reprisal.</p><p>Officials at the Health and Human Services Department and the Centers for Medicare and Medicaid Services directed employees to respond by the deadline. HHS informed employees of OPM's changed guidance later on Monday, and<a href="https://www.nbcnews.com/politics/doge/hhs-warns-responses-elon-musks-email-may-read-malign-foreign-actors-rcna193553" target="_blank"> warned</a> that whatever information they choose to share may&nbsp;“be read by malign foreign actors.”</p><p>An email sent to Department of Transportation employees and obtained by NBC News instructed them to respond to OPM’s weekend email asking for five bullet points of their work. The message also asked employees to exclude any classified info from their responses. Transportation Secretary Sean Duffy <a href="https://x.com/SecDuffy/status/1894031690256818515" target="_blank">embraced the challenge </a>himself in a post on social media.</p><p>Musk appears to be following the same playbook he used when he bought Twitter, which he renamed X.</p><p>Musk began his tenure there with massive layoffs, asking employees to commit to "Extremely hardcore" work in an email titled "A Fork in the Road" or be fired. The email subject line is the same as the email sent out to federal employees by the Office of Personnel Management offering buyouts in January. About<a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank"> 75,000 </a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">f</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">deral e</a><a href="https://www.nbcnews.com/politics/white-house/white-house-says-75000-accepted-federal-buyout-trump-rcna191971" target="_blank">mployees</a> took the deal.</p><p>Twitter employees who stayed were then asked to print out pages of code they'd written from the last month and prepare to present the work to Musk personally. The code reviews reportedly were abandoned, and instead, managers were asked to rank their employees, according to <a href="https://www.theverge.com/23551060/elon-musk-twitter-takeover-layoffs-workplace-salute-emoji" target="_blank">The Verge</a>.</p><p>Musk and DOGE's access to government data and information has become a central point of friction between the group and its critics. In at least <a href="https://www.nbcnews.com/tech/security/doge-lawsuits-11-cases-musk-group-focus-data-privacy-rcna191695" target="_blank">11 lawsuits</a>, plaintiffs have argued that DOGE has flouted laws and rules around data and privacy. Some of the lawsuits have referenced allegations that DOGE is using artificial intelligence to analyze and process government data. The <a href="https://www.washingtonpost.com/nation/2025/02/06/elon-musk-doge-ai-department-education/" target="_blank">Washington Post reported</a> in February that DOGE was using artificial intelligence to analyze spending at the Education Department, citing two people familiar with the project.</p><p>House Speaker Mike Johnson, R-La.,<a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank"> </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">on Monday </a><a href="https://www.nbcnews.com/politics/donald-trump/live-blog/trump-france-emmanuel-macron-white-house-ukraine-tariffs-live-updates-rcna193386/rcrd73645?canonicalCard=true" target="_blank">touted Musk's work</a> in remarks that seemed to confirm the AI reports.</p><p>"Elon has cracked the code. He is now inside the agencies. He’s created these algorithms that are constantly crawling through the data. And as he told me in his office, the data doesn’t lie. We’re going to be able to get the information. We’re going to be able to transform the way the federal government works at the end of this, and that is a very exciting prospect. It is truly a revolutionary moment for the nation,” Johnson said at an event in Washington.</p><p>Sen. Lisa Murkowski, R-Alaska, had some harsh words for the way DOGE was operating, calling the email sent to federal workers “intimidation,”&nbsp;and saying that she’s hearing from federal workers who are being “treated with a level of disregard to their service and to their tenure.”</p><p>“Just a little bit of humanity and dignity to the process, I think, is what many of the Alaskan federal employees are asking for, and I don’t think that that’s asking for too much,”&nbsp;Murkowski said.</p><p>DOGE's work has led to criticism and instances of workers having to be rehired after they were removed from essential jobs. On Monday, two people familiar with the matter said the administration was <a href="https://www.nbcnews.com/health/health-news/fda-rehires-staff-medical-devices-division-mass-layoffs-rcna193501" target="_blank">reinstating some employees</a> in the Food and Drug Administration’s medical devices division after dozens were laid off as part of DOGE's <a href="https://www.nbcnews.com/politics/doge/elon-musk-says-federal-workers-must-justify-work-resign-rcna193340" target="_blank">cost-cutting initiative</a>.</p><p>The medical devices division is responsible for approving and monitoring the safety of a range of products, from X-ray machines to surgical implants. The layoffs took place earlier this month, and included physicians and cybersecurity experts.</p><p>Some of those employees received phone calls or emails over the weekend informing them that their termination had been rescinded. It’s unclear how many employees were offered their jobs back, or how many would ultimately return.</p><p>The administration had a similar issue this month with<a href="https://www.nbcnews.com/politics/national-security/trump-administration-wants-un-fire-nuclear-safety-workers-cant-figure-rcna192345" target="_blank"> nuclear safety personnel</a> who had been let go.</p><p>In a court ruling Friday, a federal judge in New York issued a preliminary&nbsp;injunction&nbsp;<a href="https://www.nbcnews.com/politics/politics-news/judge-temporarily-blocks-doge-sensitive-treasury-department-systems-rcna191316" target="_blank">barring DOGE’s access&nbsp;</a>to sensitive Treasury Department systems after a coalition of states <a href="https://www.nbcnews.com/politics/trump-administration/trump-scored-big-legal-wins-week-efforts-reshape-government-still-face-rcna193000" target="_blank">presented evidence </a>that its employees weren't following proper safety protocols.</p><p>In a&nbsp;<a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.636609/gov.uscourts.nysd.636609.76.0_2.pdf" target="_blank">scathing ruling</a>, U.S. District Judge Jeanette Vargas blasted DOGE's "chaotic and haphazard approach" and found the coalition had “established that there is a realistic danger that confidential financial information will be disclosed absent the grant of injunctive relief.”</p></div><div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2020_02/3181226/courtney-kube-circle-byline-template.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/courtney-kube-ncpn3621">Courtney Kube</a></span><span><a href="https://x.com/ckubeNBC" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Courtney Kube is a correspondent covering national security and the military for the NBC News Investigative Unit.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_46/3669091/241113-julie-tsirkin.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/julie-tsirkin-ncpn947511">Julie Tsirkin</a></span><span><a href="https://x.com/JulieNBCNews" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Julie&nbsp;Tsirkin is a&nbsp;correspondent covering Capitol Hill.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2024_36/3661282/240904-yamiche-alcindor.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/yamiche-alcindor-ncpn1294685">Yamiche Alcindor</a></span><span><a href="mailto:Yamiche.Alcindor@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Yamiche Alcindor is an NBC News Washington correspondent. </p></div><div data-activity-map="expanded-byline-article-bottom"><p><span data-testid="byline-thumbnail"></span><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/laura-strickler-ncpn894696">Laura Strickler</a></span><span><a href="https://x.com/strickdc" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Laura.Strickler@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Laura Strickler is a senior investigative producer and reporter for NBC News. She is based in Washington.</p></div><div data-activity-map="expanded-byline-article-bottom"><div data-testid="byline-thumbnail"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686" tabindex="-1"><picture data-testid="picture"><source media="(min-width: 320px)" srcset="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_avif,q_auto:eco,dpr_2/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 2x, https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg 1x"><img loading="lazy" src="https://media-cldnry.s-nbcnews.com/image/upload/t_focal-60x60,f_auto,q_auto:best/newscms/2019_27/2923711/190612-dareh_gregorian-byline-30871.jpg" alt="" height="48" width="48"></picture></a></div><p><span data-testid="byline-name"><a href="https://www.nbcnews.com/author/dareh-gregorian-ncpn925686">Dareh Gregorian</a></span><span><a href="https://x.com/darehgregorian" target="_blank" rel="noopener noreferrer"><span></span></a><a href="mailto:Dareh.Gregorian@nbcuni.com" target="_blank" rel="noopener noreferrer"><span></span></a></span></p><p>Dareh Gregorian is a politics reporter for NBC News.</p></div><div><p>Ben Goggin</p><!-- --><p>, </p><!-- --><p>Sarah Dean</p><!-- --><p>, </p><!-- --><p>Ken Dilanian</p><!-- --><p>, </p><!-- --><p>Allan Smith</p><!-- --><p>, </p><!-- --><p>Jonathan Allen</p><!-- --><p>, </p><!-- --><p>Julia Jester</p><!-- --><p>, </p><!-- --><p>Ryan J. Reilly</p><!-- --><p>, </p><!-- --><p>Megan Lebowitz</p><!-- --><p>, </p><!-- --><p>Ryan Nobles</p><!-- --><p>, </p><!-- --><p>Frank Thorp V</p><!-- --><p>, </p><!-- --><p>Zoë Richards</p><!-- --><p> and </p><!-- --><p>Phil Helsel</p><!-- --> <!-- --><p>contributed</p><!-- --><p>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Embedding Python in Elixir, It's Fine (241 pts)]]></title>
            <link>https://dashbit.co/blog/running-python-in-elixir-its-fine</link>
            <guid>43171239</guid>
            <pubDate>Tue, 25 Feb 2025 12:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dashbit.co/blog/running-python-in-elixir-its-fine">https://dashbit.co/blog/running-python-in-elixir-its-fine</a>, See on <a href="https://news.ycombinator.com/item?id=43171239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    
<ul>
  <li>
    <i></i> Jonatan Kłosko
  </li>
  <li>
    <i></i> February 21st, 2025
  </li>
  <li>
    <i></i><a href="https://dashbit.co/blog/tags/python">python</a>, <a href="https://dashbit.co/blog/tags/livebook">livebook</a>, <a href="https://dashbit.co/blog/tags/nifs">nifs</a>
  </li>
</ul>
<p>
In the recent years, Elixir has been expanding its capabilities in Machine Learning and Data through the <a href="https://github.com/elixir-nx">Nx (Numerical Elixir)</a> effort. A number of projects emerged (Nx, Explorer, Axon, Bumblebee, Scholar, and more), drawing learnings from decades of work in ecosystems such as Python and R, often standing on the shoulders of C++ and Rust codebases.</p>
<p>
When we started, we made the explicit choice to not depend on Python libraries directly. We wanted to design and develop our ecosystem with full control of making the best decisions for Elixir, which would not necessarily match the decisions made for Python. We also wished to avoid bringing to our ecosystem the complexities in getting a Python environment up and running. While young, the Nx ecosystem already enabled <a href="https://youtu.be/VcOvNTxUaIo">running pre-trained ML models</a>, <a href="https://youtu.be/5FlZHkc4Mq4">simplifying production systems with a unified AI stack</a>, <a href="https://youtu.be/4qoHPh0obv0">managing GPU cluster workflows from a notebook</a>, to point a few.</p>
<p>
A key component driving the adoption of Elixir in these areas is <a href="https://livebook.dev/">Livebook</a>, a computational notebook platform that builds on the strengths of the Elixir and Erlang, bringing reproducibility, distributed execution, and app development to the forefront. With Livebook, we have seen a growing interest from teams and companies in dipping their toes into the Elixir ecosystem for the first time.</p>
<p>
All of this builds a good case to go all in with Elixir, but some hurdles remain. As one would expect, most companies interested in bringing Elixir and Livebook into their infrastructure, have existing workflows, packages, and repositories that they already rely on. The choices we have made so far imply that they either have to find an equivalent package in Elixir or write one from scratch, increasing the risk and costs of adding Elixir to their data stack.</p>
<p>
To address these concerns, today we announce Pythonx, which embeds the Python interpreter within the Erlang VM, bringing automatic data conversion between Elixir and Python, code evaluation, and automatic virtual environment management. We compare Pythonx with other options for interoperability and outline future work.</p>
<h2>
Enter Pythonx</h2>
<p>
Imagine we have an image and want to read the text on that image. We need to do what is known as Optical Character Recognition (OCR). Sure enough, there are a few Python packages doing just that, one of them being <code>pytesseract</code>. For the sake of this example, we will download the image using Req:</p>
<pre><code><span>Mix</span><span>.</span><span>install</span><span data-group-id="8115916589-1">(</span><span data-group-id="8115916589-2">[</span><span>
  </span><span data-group-id="8115916589-3">{</span><span>:pythonx</span><span>,</span><span> </span><span>"~&gt; 0.4.0"</span><span data-group-id="8115916589-3">}</span><span>,</span><span>
  </span><span data-group-id="8115916589-4">{</span><span>:req</span><span>,</span><span> </span><span>"~&gt; 0.5.8"</span><span data-group-id="8115916589-4">}</span><span>
</span><span data-group-id="8115916589-2">]</span><span data-group-id="8115916589-1">)</span><span>

</span><span>url</span><span> </span><span>=</span><span> </span><span>"https://unsplash.com/photos/95t94hZTESw/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzQwMDYwMjg4fA&amp;force=true&amp;w=640"</span><span>
</span><span>binary</span><span> </span><span>=</span><span> </span><span>Req</span><span>.</span><span>get!</span><span data-group-id="8115916589-5">(</span><span>url</span><span data-group-id="8115916589-5">)</span><span>.</span><span>body</span></code></pre>
<p>
Now, let’s bring in Python.</p>
<pre><code><span>Pythonx</span><span>.</span><span>uv_init</span><span data-group-id="6229236571-1">(</span><span>"""
[project]
name = "project"
version = "0.0.0"
requires-python = "==3.13.*"
dependencies = [
  "pytesseract==0.3.13",
  "pillow==11.1.0"
]
"""</span><span data-group-id="6229236571-1">)</span></code></pre>
<p>
Calling <code>Pythonx.uv_init/1</code> downloads Python and the listed dependencies using the excellent <a href="https://docs.astral.sh/uv">uv</a> package manager. It also immediately initializes the Python interpreter for evaluation. Note the dependencies section where we list <code>pytesseract</code> for OCR and <code>pillow</code> for image handling.</p>
<p>
Next, let’s write some Python.</p>
<pre><code><span data-group-id="8721759126-1">{</span><span>result</span><span>,</span><span> </span><span>_globals</span><span data-group-id="8721759126-1">}</span><span> </span><span>=</span><span>
  </span><span>Pythonx</span><span>.</span><span>eval</span><span data-group-id="8721759126-2">(</span><span>
    </span><span>"""
    import pytesseract
    import io
    import PIL

    image = PIL.Image.open(io.BytesIO(binary))
    pytesseract.image_to_string(image)
    """</span><span>,</span><span>
    </span><span data-group-id="8721759126-3">%{</span><span>"binary"</span><span> </span><span>=&gt;</span><span> </span><span>binary</span><span data-group-id="8721759126-3">}</span><span>
  </span><span data-group-id="8721759126-2">)</span><span>

</span><span>Pythonx</span><span>.</span><span>decode</span><span data-group-id="8721759126-4">(</span><span>result</span><span data-group-id="8721759126-4">)</span><span>
</span><span>#=&gt; "The Journey\nof a thousand\nmiles begins\nwith a single\n\nstep.\n\n-Lao Tzu\n\n"</span></code></pre>
<p>
Above we call <code>Pythonx.eval/2</code>, which accepts Python code and a map with variables for the evaluation. Note how we pass the Elixir binary and it is automatically converted to a <code>bytes</code> object on the Python side. The evaluation returns <code>result</code>, which is a <code>%Pythonx.Object{}</code>, and also an updated map with variables. In this case we only care about the result and we use <code>Pythonx.decode/1</code> to convert it to an Elixir string right away.</p>
<p>
There we go! To learn more about Pythonx, see <a href="https://hexdocs.pm/pythonx">the documentation</a>. And if you are struggling to write Python for your task, consult with your AI specialist, it went to school for that.</p>
<h2>
Under the hood</h2>
<p>
If you are raising your eyebrow, thinking that this just calls <code>python</code>, bear with me!</p>
<p>
So Python, or more specifically its <a href="https://github.com/python/cpython">CPython</a> reference implementation, has the interesting capability of being embedded into other applications. What this means is that the core functionality of the Python interpreter is available as a C library, so a C/C++ application can link that library and use its APIs to run code and interact with objects. In fact, you can think of the <code>python</code> executable as one such application.</p>
<p>
Elixir provides C/C++ interoperability via Erlang NIFs and that’s exactly what Pythonx uses to embed Python, which means that the Python interpreter operates in the same OS process as Elixir itself. By living in the same memory space, passing data between Elixir and Python is cheap. Pythonx ties Python and Erlang garbage collection, so that the objects can be safely kept between evaluations. Also, it conveniently handles conversion between Elixir and Python data structures, bubbles Python exceptions and captures standard output.</p>
<h2>
Livebook goes multilingual</h2>
<p>
To enable even more powerful workflows, we <a href="https://github.com/livebook-dev/livebook/pull/2936">started working</a> on Python support in Livebook, building on Pythonx. The idea, though, is not to support Python separately, but rather to allow Elixir and Python interacting in the same notebook! To give you a better picture, below you can see the same example using Python cells in Livebook nightly.</p>
<p><img src="https://dashbit.co/images/posts/2025/pythonx_ocr.png" width="100%"></p>
<p>
Livebook automatically installs Python and its dependencies, as it manages Elixir’, ensuring a reproducible environment. It also tracks which Elixir variables are used by Python, and vice-versa, and automatically converts them between cells. While there is still work ahead of us, including code completion, documentation, and a few surprises, we are open to feedback. <a href="https://github.com/livebook-dev/livebook#desktop-app">You can download Livebook nightly</a> to give it a try. Once we add all bells and whistles, we will do an official announcement over <a href="https://news.livebook.dev/">news.livebook.dev</a>.</p>
<p>
At this point I want to thank <a href="https://github.com/cocoa-xu">Cocoa Xu</a> for starting off the work on Embedded Python, and <a href="https://github.com/cigrainger">Christopher Grainger</a> for the initial push to run Python in Livebook.</p>
<h2>
Usage considerations and alternatives</h2>
<p>
The primary goal of Pythonx is to better integrate Python workflows within Livebook and scripts. Pythonx usage in actual projects must be done with care due to Python’s global interpreter lock (GIL). The GIL prevents multiple threads from executing Python code at the same time, so calling <code>Pythonx</code> from multiple Elixir processes does not provide the concurrency you might expect and thus it can be a source of bottlenecks. However, this limitation concerns regular Python code. Packages with CPU-intense functionality, such as <code>numpy</code>, have native implementation of many functions and invoking those releases the GIL. The GIL is also released when waiting on I/O operations. In other words, if you are using this library to integrate with Python, make sure it happens in a single Elixir process or that its underlying libraries can deal with concurrent invocation.</p>
<p>
If the above is a dealbreaker, remember that interoperability already exists at a few levels. For example, you could write a Python script and then invoke it with <a href="https://hexdocs.pm/elixir/System.html#cmd/3"><code>System.cmd/3</code></a> or open a <a href="https://hexdocs.pm/elixir/Port.html">Port</a>. In those cases, you could start several or even a pool of Python processes that you would manage.</p>
<p>
Furthermore, depending on your needs, you may also be able to interoperate through higher-level abstractions. For example, for AI workflows, you can run pre-trained models directly, some via <a href="https://github.com/elixir-nx/bumblebee">Bumblebee</a>, others via <a href="https://github.com/elixir-nx/ortex">Ortex</a>. When using an LLM, you often end up talking to a third-party provider, or perhaps you run a drop-in llama.cpp Docker container on-premise, optimised for inference. In such cases the interface is HTTP and Elixir has high-level tools for interacting with LLMs too, namely <a href="https://github.com/thmsmlr/instructor_ex">Instructor</a> and <a href="https://github.com/brainlid/langchain">LangChain</a>.</p>
<p>
That said, if you do decide that Pythonx fits into your application, you can configure it to download all Python dependencies at compile time and include them as part of the Elixir release. For more details, refer to <a href="https://hexdocs.pm/pythonx/Pythonx.html#module-usage-application">this section</a> in the doc.</p>
<p>
You could also use Pythonx to give you immediate access to more tools to unblock you. Once your idea pays off, you can invest more time to arrive at a Elixir-centric solution, if you so desire.</p>
<h2>
It’s Fine</h2>
<p>
Speaking of interoperability, I mentioned that Pythonx uses NIFs. NIFs are Elixir functions with the implementation living in C. We reach for NIFs either when we want to write native code with mutability for something performance-critical or when we integrate with third-party libraries via C API (often both).</p>
<p>
To give an example, below is a NIF implementation that adds two numbers.</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>erl_nif.h</span><span>&gt;</span><span>
</span><span>
</span><span>ERL_NIF_TERM </span><span>add</span><span>(</span><span>ErlNifEnv</span><span>*</span><span> </span><span>env</span><span>,</span><span> </span><span>int</span><span> </span><span>argc</span><span>,</span><span> </span><span>const</span><span> ERL_NIF_TERM </span><span>argv</span><span>[</span><span>]</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>int</span><span> x</span><span>,</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>if</span><span> </span><span>(</span><span>argc </span><span>!=</span><span> </span><span>2</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>0</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>x</span><span>)</span><span> </span><span>||</span><span> </span><span>!</span><span>enif_get_int</span><span>(</span><span>env</span><span>,</span><span> argv</span><span>[</span><span>1</span><span>]</span><span>,</span><span> </span><span>&amp;</span><span>y</span><span>)</span><span>)</span><span> </span><span>{</span><span>
</span><span>    </span><span>return</span><span> </span><span>enif_make_badarg</span><span>(</span><span>env</span><span>)</span><span>;</span><span>
</span><span>  </span><span>}</span><span>
</span><span>
</span><span>  </span><span>int</span><span> result </span><span>=</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>
</span><span>  </span><span>return</span><span> </span><span>enif_make_int</span><span>(</span><span>env</span><span>,</span><span> result</span><span>)</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>ErlNifFunc nif_funcs</span><span>[</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>
</span><span>  </span><span>{</span><span>"</span><span>add</span><span>"</span><span>,</span><span> </span><span>2</span><span>,</span><span> add</span><span>}</span><span>
</span><span>}</span><span>;</span><span>
</span><span>
</span><span>ERL_NIF_INIT</span><span>(</span><span>Elixir</span><span>.</span><span>MyLib</span><span>.</span><span>NIF</span><span>,</span><span> nif_funcs</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>,</span><span> </span><span>NULL</span><span>)</span></code></pre>
<p>
Looking at the signature, you can see that the function receives a C-array of Erlang terms and returns an Erlang term. We are responsible for converting between terms and C data structures using the <code>enif_*</code> APIs. The example may look pretty straightforward, though it is a fair amount of boilerplate code to end up adding two numbers. From there the ceremony escalates quickly once we need to deal with nested data structures and return errors more specific than <code>:badarg</code>. A natural progression is to extract some of the logic to helper functions, but this doesn’t fully alleviate the boilerplate and it results in reinventing the wheel a lot.</p>
<p>
Additionally, Pythonx (and other NIF-extensive projects) actually use C++, while the <code>enif_*</code> APIs are (rightfully so) C. Since C++ brings more powerful constructs, theoretically there is a possibility of a more expressive API, however it is also easy to get into weeds with C++ metaprogramming. The main question I asked myself is how far can we go inferring the conversion from types. With <a href="https://github.com/rusterlium/rustler">Rustler</a> and <a href="https://github.com/E-xyza/zigler">Zigler</a>, NIFs are written as regular functions and the data structures conversion is handled automatically based on the signature types.</p>
<p>
This brings us to <a href="https://github.com/elixir-nx/fine">Fine</a>, C++ library enabling more ergonomic NIFs, tailored to Elixir. Let’s see an update example:</p>
<pre><code><span>#include</span><span> </span><span>&lt;</span><span>fine.hpp</span><span>&gt;</span><span>
</span><span>
</span><span>int64_t</span><span> </span><span>add</span><span>(</span><span>ErlNifEnv </span><span>*</span><span>env</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>x</span><span>,</span><span> </span><span>int64_t</span><span> </span><span>y</span><span>)</span><span> </span><span>{</span><span>
</span><span>  </span><span>return</span><span> x </span><span>+</span><span> y</span><span>;</span><span>
</span><span>}</span><span>
</span><span>
</span><span>FINE_NIF</span><span>(</span><span>add</span><span>,</span><span> </span><span>0</span><span>)</span><span>;</span><span>
</span><span>
</span><span>FINE_INIT</span><span>(</span><span>"</span><span>Elixir.MyLib.NIF</span><span>"</span><span>)</span><span>;</span></code></pre>
<p>
Other than extendable encoding/decoding, Fine provides smart pointers to safely manage resource objects and support for raising exceptions anywhere in the NIF. I’ve refactored EXLA NIFs to use Fine and <a href="https://github.com/elixir-nx/nx/pull/1581">it removed over 1k LOC</a>, so it may be worth considering next time you have to write some NIFs.</p>
<h2>
Summing up</h2>
<p>
When we started Numerical Elixir, our goal was for Elixir to develop and have its own identity within the data and machine learning ecosystem. Now we are ready to make interoperability a key focus of our efforts too.</p>
<p>
Pythonx embeds Python into Elixir, bringing a new class of interoperability with a third-party language not seen before within the Erlang VM. It is more than just integrating the Python interpreter, it is about transparently translating idioms from one language to the other.</p>
<p>
The Fine project also consolidates and streamlines our collective experiences in integrating C++ and Elixir, tracing back to Sean Moriarity’s work on Nx four years ago.</p>
<p>
There is more to come.</p>
<p>
Stay interoperable!</p>

  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Signal to leave Sweden if backdoor law passes (444 pts)]]></title>
            <link>https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</link>
            <guid>43171205</guid>
            <pubDate>Tue, 25 Feb 2025 12:50:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden">https://swedenherald.com/article/signals-ceo-then-were-leaving-sweden</a>, See on <a href="https://news.ycombinator.com/item?id=43171205">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="single-entry-content"><p>According to Whittaker, the bill requires the encrypted messaging app Signal to install so-called backdoors in the software.</p>
<blockquote>
<p>If you create a vulnerability based on Swedish wishes, it would create a way to undermine our entire network. Therefore, we would never introduce these backdoors, she says.</p>
</blockquote>
<p>The purpose of the bill – which may be passed next year – is for the police and Security Service to be able to request message history in retrospect for individuals suspected of crimes.</p>
<p>The Armed Forces, on the other hand, are negative and write in a letter to the government that the proposal cannot be realized "without introducing vulnerabilities and backdoors that can be exploited by third parties", reports SVT.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Defense of Weird Research (107 pts)]]></title>
            <link>https://asteriskmag.com/issues/09/a-defense-of-weird-research</link>
            <guid>43171002</guid>
            <pubDate>Tue, 25 Feb 2025 12:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/09/a-defense-of-weird-research">https://asteriskmag.com/issues/09/a-defense-of-weird-research</a>, See on <a href="https://news.ycombinator.com/item?id=43171002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
				
		 			<h2>
				   
					<span>Deena Mousa</span>
				   
					<span>Lauren Gilbert</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>Government-funded scientific research may appear strange or impractical, but it has repeatedly yielded scientific breakthroughs — and continues to pay for itself many times over.</p>
				<div>
											<div><p>President Donald Trump announced the formation of the Department of Government Efficiency, or DOGE, days after his re-election. Tasked with eliminating wasteful spending and streamlining federal operations, DOGE’s ostensible mission is to ensure every taxpayer dollar produces measurable results.</p><p>In the weeks following the announcement, the MAGA memeplex <a href="https://x.com/DefiyantlyFree/status/1856942575673827582">took specific aim</a> at seemingly obscure government-funded research. And since the Elon Musk-led U.S. DOGE Service assumed its legally ambiguous power in January, <a href="https://www.insidehighered.com/news/government/politics-elections/2025/01/29/grant-reviews-nsf-and-nih-still-paused">both</a> the National Science Foundation and National Institutes of Health have found their <a href="https://www.insidehighered.com/news/government/politics-elections/2025/01/29/grant-reviews-nsf-and-nih-still-paused">funding</a> frozen, then unfrozen, and as of this writing, still in limbo. While the ultimate impacts of DOGE and Trump’s “efficiency” campaign remain unclear, early indications suggest that federally funded research will face <a href="https://www.rdworldonline.com/nsf-layoffs-in-2025-deep-budget-cuts-headed-for-u-s-research-sector/">steep cuts</a>.</p><p>This would be a mistake. Projects that seem odd or impractical — on animal behavior, obscure molecules, or fundamental physics — may be <a href="https://www.aaas.org/news/basic-research-often-mocked-targeted-budget-cuts-due-lack-public-understanding">easy targets for ridicule,</a> but such studies have repeatedly yielded transformative breakthroughs. While it makes sense to pursue efficiency in government spending, the returns on federal science funding are, on average, extremely high.</p></div>
											<p><h2>The returns to research</h2>
</p>
											<div><p>In 1979, a Duke University team led by neuroscientist Saul Schanberg, began studying the effects of separating rat pups from their mothers. Their work — funded by the NIH — found that isolated pups experienced impaired development. Yet, strikingly, stroking the pups with a soft brush to simulate a mother rat’s grooming prevented these developmental issues from occurring.&nbsp;</p><p>Several years later, developmental psychologist Tiffany Field saw an opportunity to apply these findings to human preterm infants. <a href="https://www.cdc.gov/maternal-infant-health/preterm-birth/index.html#:~:text=The%20preterm%20birth%20rate%20declined,in%20preterm%20birth%20rates%20remain">Approximately 10%</a> of births in the US are preterm, and those infants face higher rates of both developmental complications and mortality due to poor growth, weakened immune systems, and extended hospital stays. Field — also partially supported by the NIH — conducted clinical trials in neonatal intensive care units to test whether a similar form of tactile stimulation could improve the infants’ trajectories. Babies who received massages for 15 minutes, three times a day <a href="https://pubmed.ncbi.nlm.nih.gov/3754633/">gained 47% more weight</a> than those who did not. Massaged infants had shorter hospital stays, reducing costs for families and health care systems.</p><p>By the mid-1990s, <a href="https://www.apa.org/topics/parenting/massage-therapy#:~:text=As%20a%20direct%20result%20of,give%20massage%20therapy%20to%20preemies.">massage therapy</a> for preterm infants had become widespread in U.S. NICUs. One study <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC2844909/">estimates</a> that the practice may reduce hospital stays by an average of 3 to 6 days and save approximately $10,000 in medical costs per infant.<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 If applied across the roughly 400,000 preterm infants born each year in the U.S., this could amount to $4 billion in savings.<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
</p><p>This case exemplifies a <a href="https://www.goldengooseaward.org/">broader pattern</a>: Government spending on scientific research tends to result in returns that far exceed the initial investment.<strong> </strong>According to <a href="https://www.dallasfed.org/~/media/documents/research/papers/2023/wp2305r1.pdf">the Dallas Fed</a>, science spending writ-large <a href="https://www.dallasfed.org/~/media/documents/research/papers/2023/wp2305r1.pdf">generates</a> enough social benefit to pay for itself 1.5 times over and is responsible for roughly 25% of productivity growth since World War II — and that estimate is conservative. Other papers suggest that every dollar spent on science generates anywhere between <a href="https://www.nber.org/system/files/working_papers/w27863/w27863.pdf">$14</a> to <a href="https://arxiv.org/pdf/2312.14289">$70</a> in social benefits. Some economists <a href="https://www.newthingsunderthesun.com/pub/ijugr2h6/release/11?readingCollection=9f57d356">believe</a> that <em>all</em> growth in living standards comes from scientific research and development.&nbsp;</p></div>
											<div><figure>
      <figcaption>
    A live frog levitates inside the vertical bore of a Bitter solenoid in a magnetic field of about 16 Tesla at the Nijmegen High Field Magnet Laboratory.  </figcaption>
    <p><img src="https://asteriskmag.com/media/pages/issues/09/a-defense-of-weird-research/e90a7040e5-1739990483/frog_diamagnetic_levitation.jpg" alt="">
  </p>
    <p><em>Lijnis Nelemans</em>  </p>
  </figure>
</div>
											<p><h2>Mad science</h2>
</p>
											<div><p>It's easy to defend research like the preterm infant studies. The practical application is clear, and the benefits appeared early and conclusively. Few people believe that the federal government should have no role in research funding. But when agencies fund research to, for example, levitate frogs,<sup>
    <!-- <a id="fnref-3" href="#fn-3"> -->
    <span id="fnref-3">
        3    </span>
    <!-- </a> -->
</sup>
 skeptics reasonably ask: Shouldn't we focus instead on creating useful technologies or medicines?</p><p>But this perspective misunderstands how scientific progress happens. Scientific research broadly falls into two categories. Basic research seeks to understand fundamental principles about how our world works. Applied research develops specific technologies or treatments using that existing knowledge. Basic research often appears strange to outside observers precisely because it isn't tied to immediate applications.</p><p>And so there are scientists who <a href="https://www.goldengooseaward.org/01awardees/frog-skin-cholera">study frog skin</a> or become experts in the <a href="https://www.goldengooseaward.org/01awardees/screwworms">sex lives of flies</a>. But that frog skin led to a new theory of rehydration, and ultimately the <a href="https://asteriskmag.com/issues/02/salt-sugar-water-zinc-how-scientists-learned-to-treat-the-20th-century-s-biggest-killer-of-children">invention of oral rehydration therapy</a>, which has saved over 70 million lives — most of them children. The sex lives of flies? Well, understanding how flies reproduce led to the development of a sterilized screwworm fly and the elimination of a common livestock pest, saving some <a href="https://www.aphis.usda.gov/media/111568">$200 million</a> a year.</p><p>The applications of basic science are often unexpected because new technologies can be broadly applied. NASA was certainly not trying to develop better vacuums when they invested in research on batteries — yet the result was the <a href="https://www.nasa.gov/image-article/dustbuster/">Dustbuster</a>. NASA-funded research is also responsible for the fundamental developments behind LASIK eye surgery (laser research), TempurPedic mattresses (materials research), and even Astroglide (initially developed as a substance to improve heat transfer). These important (or, at the very least, useful)&nbsp;technologies all relied on doing basic science first.</p><p>Of course, none of this happens overnight. It often takes decades for something to go from “science” to “a new technology.” But basic research is necessary to get to a stage where funding the development of a piece of useful technology, or the clinical trial of a new drug, is even possible.</p><p>Consider CRISPR, the gene-editing technology with the potential to cure diseases caused by genetic mutations. In 1987, <a href="https://pubmed.ncbi.nlm.nih.gov/3316184/">scientists</a> in Japan investigating E. coli DNA discovered unusual repetitive sequences. It took another 18 years before <a href="https://pubmed.ncbi.nlm.nih.gov/15791728/">a team</a> of Spanish researchers recognized these sequences as part of bacterial immune systems, and another seven years to figure out how to use them to target and edit at a specific point in the genome. Though it took more than two decades, these fundamental insights, which might have seemed esoteric at the time, eventually enabled the development of CRISPR, one of the most important biotechnology breakthroughs of the century.&nbsp;</p><p>This is not a one-off. Basic research is, on average, estimated to give <a href="https://libres.uncg.edu/ir/uncg/f/A_Link_Basic_1981.pdf">even higher returns on investment</a> than the average research endeavor. If all science returns 150% of investment, basic science returns even more. A <a href="https://www.jstor.org/stable/1804132">study</a> of hundreds of major manufacturing firms in the 1970s found that increasing basic research spending had three to five times more impact on productivity growth than other research and development investments.</p><p>However, these impacts are often impossible to articulate <em>before</em> the research has been done. The lack of a clear path to application means this research may seem pointless from the outside view. This makes it a prime target for efficiency-based cuts. We didn’t know that studying <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3219279/">Gila monster venom</a> would lead to the invention of GLP-1 agonists like Ozempic, or that <a href="https://www.npr.org/2023/06/10/1180761446/coastal-biomedical-labs-are-bleeding-more-horseshoe-crabs-with-little-accountabi">horseshoe crab blood</a> would prove crucial to vaccine development, or that <a href="https://www.goldengooseaward.org/01awardees/thermus-aquaticus">studying bacteria in geysers</a> would lead to the <a href="https://www.goldengooseaward.org/01awardees/thermus-aquaticus">development of PCR</a>, the technology which allows scientists to detect DNA in small samples, and on which much of modern molecular biology — from genetic testing and COVID diagnostics — rests. Basic research is often high risk — projects sometimes go nowhere or fail to provide meaningful results.&nbsp; But some discoveries revolutionize entire fields.</p></div>
											<p><h2>What about the private sector?</h2>
</p>
											<div><p>If basic scientific research produces such high returns, why isn't the private sector eager to fund it? Several structural barriers make it particularly challenging for private companies to support such endeavors, even when the aggregate returns are compelling.</p><p>The first is time. Basic research typically takes about <a href="https://www.newthingsunderthesun.com/pub/6nunnxqx/release/11#:~:text=It%20seems%20clear%20that%20a,a%20good%20rule%20of%20thumb.">20 years</a> to go from "science" to "technology," and often longer. Few companies can justify waiting this long for a return, regardless of its magnitude. Shareholders and investors expect profits on much shorter timelines, typically quarterly or annual returns. Even the most patient venture capital rarely extends beyond a 10 year horizon. Governments, with their ability to think in terms of decades rather than quarters — and their focus on societal rather than financial returns — can more readily make these long-term investments.</p><p>Even if a company could wait out these timelines, they face a more fundamental challenge: Basic research is a public good, meaning private actors cannot capture the full value their investment creates. The knowledge generated through basic research is non-rivalrous (everyone can use it without diminishing others' access) and largely non-excludable (no one can be prevented from benefiting). This means many companies will benefit from the research, not just the one that funded it.</p><p>Take CRISPR. While companies like Editas and Intellia have begun to commercialize gene-editing technologies, they built on decades of foundational research funded primarily by government grants. The researchers who discovered these unusual DNA sequences in bacteria — and the institutions that funded them — won't capture the financial benefits from CRISPR applications, even though their work was essential to creating that value. Nor is CRISPR unusual: <a href="https://www.pnas.org/doi/10.1073/pnas.1715368115">One study</a> which looked at new drugs introduced between 2010 and 2016 found that every single one relied on federally funded basic research.</p><p>This value-capture problem is compounded by spillover effects, unexpected applications that emerge far beyond the original field of research. These spillovers often form the <a href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9466">majority</a> of the value from basic research, but are impossible to predict or monetize. When the Department of Defense funded <a href="https://clintonwhitehouse4.archives.gov/WH/EOP/OVP/24hours/internet.html#:~:text=The%20ARPANET%2C%20the%20precursor%20of,function%20after%20a%20nuclear%20attack.">ARPANET</a> in the 1960s to help researchers communicate, few predicted it would evolve into the internet and transform almost every aspect of modern life. The original funders had no way to capture the massive value created across different sectors.</p><p>Finally, basic research requires a portfolio approach that private companies struggle to sustain. While the aggregate returns are high, individual projects are inherently high-risk and speculative. Many will fail to produce valuable results, while a few will generate extraordinary breakthroughs. This uncertainty makes it difficult for any single company to justify investments. Success requires scale — the ability to take many shots on goal and withstand individual failures while still coming out ahead. Governments can better absorb these risks while benefiting long-term from the hits.</p><p>The government’s unique structure and position allows it to play a distinct role from the private sector. Treating government funding like corporate investment is a false equivalence. Focusing on near-term returns will not make public investments more efficient in the long run. It merely eliminates the government’s unique advantages to advance scientific research for the public benefit.</p></div>
											<p><h2>Picking winners</h2>
</p>
											<div><p>The government may be in a better position to fund this work, but critics often question whether they can identify the right work to fund. A common objection to government-funded research is that — even if basic scientific research broadly offers high returns — the government is poorly placed to recognize good investments because they lack market signals to guide their decision-making. Or, more pointedly, the government simply has skewed political incentives.</p><p>While profit motives provide a natural guiding star for the private sector, federal agencies have developed their own methods for finding high-return opportunities. Agencies like the NIH and the NSF rely on <a href="https://grants.nih.gov/policy-and-compliance/policy-topics/peer-review/simplifying-review/background">peer review</a> to evaluate grant applications, which use the expertise of scientists and researchers to identify the most promising projects. These processes are not perfect, but they represent a form of internal competition that mirrors market forces. Applicants must compete not only against others within their field but also against the broader pool of submissions, ensuring that only the strongest proposals receive funding. Typically, less than 30% of <a href="https://nexus.od.nih.gov/all/2023/03/01/fy-2022-by-the-numbers-extramural-grant-investments-in-research/">NIH</a> or <a href="https://www.nsf.gov/nsb/news/news_summ.jsp?cntn_id=307818">NSF</a> grant applicants are funded. Every seemingly silly-sounding study has to outcompete many others.</p><p>Another potential argument against continued federal research funding is that research productivity has been <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20180338">steadily declining</a> across multiple sectors, including agriculture, semiconductors, and pharmaceuticals. It is much harder to find new drugs today than it in the past. Pharmaceutical companies now spend 15 times more to bring a single drug to market than they did in the 1970s.</p><p><a href="https://www.nber.org/papers/w23782">One theory</a> behind this trend is that early discoveries were “low-hanging fruit." Now that these are exhausted, new ideas are harder to uncover and require significantly more effort. Another explanation, the <a href="https://www.nber.org/papers/w11360">“burden of knowledge”</a> argument, proposes that as the body of scientific knowledge grows, researchers must spend more time mastering existing work before they can contribute something new. </p><p>If this is true, that might suggest that funding research is becoming less valuable over time. Perhaps science is just more expensive now, and each new discovery will be smaller. But it’s also possible that the decline in research productivity points to institutional hurdles rather than inherent challenges in discovery. Academic science has faced <a href="https://blog.jacobtrefethen.com/nih-and-nsf/?ref=jacob-trefethens-blog-newsletter">growing bureaucracy</a> and siloed disciplines. The emphasis on near-term research outputs has stifled interdisciplinary collaboration and discouraged <a href="https://www.freaktakes.com/p/math-and-physics-divorce-poetry-and">bold, exploratory research</a>. These systemic barriers may artificially amplify the perception that ideas are harder to find.</p><p>Innovation getting more difficult and more expensive over time does not make research funding a bad investment. But, if these theories of what is driving this are true, this would suggest there is room to improve its returns.</p></div>
											<p><h2>Fixing the pipeline</h2>
</p>
											<div><p>Of course, it’s possible that both things are true — that new ideas are both hard to find, and that&nbsp;we have made them harder to find. The current peer review process, though rigorous and better than many alternatives, can present exactly this kind of issue. Scientists spend significant time on grant proposals at the cost of actually doing research. These proposals typically require preliminary data, which can further discourage high-risk, high-reward projects. Most grants also last only a few years, pressuring researchers to prioritize short-term results over long-term breakthroughs.</p><p>To better support curiosity-driven science, funding mechanisms need <a href="https://blog.jacobtrefethen.com/nih-and-nsf/">reform</a>. Simplifying grant applications and reducing administrative overhead would allow scientists to focus on discovery. Extending grant cycles for exploratory research would provide stability for transformative work without the constant need to reapply for funding.</p><p>Shifting from a consensus-driven peer review process to one that allows for more reviewer discretion could also improve outcomes. For example, the <a href="https://www.hhmi.org/">Howard Hughes Medical Institute</a> adopts a risk-tolerant approach, granting researchers greater freedom to pursue unconventional ideas rather than holding them precisely to their proposals. This model produces high-impact research papers at <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-2171.2011.00140.x">nearly double</a> the rate of traditional NIH-funded projects. Incorporating elements of reviewer discretion, like discretionary funding overrides, into existing systems could unlock similar benefits.</p></div>
											<p><h2>Worth the risk</h2>
</p>
											<div><p>Even with our current system, seemingly odd experiments in pursuit of basic science frequently yield transformative discoveries that improve lives and drive economic growth. This is true not only for headline-grabbing breakthroughs but also for projects that hover just below traditional funding thresholds.</p><p>Consider a 2019 <a href="https://www.sciencedirect.com/science/article/abs/pii/S0047272719301276">paper</a> that analyzed how universities allocate the windfalls generated by their football teams better-than-expected seasons.<sup>
    <!-- <a id="fnref-4" href="#fn-4"> -->
    <span id="fnref-4">
        4    </span>
    <!-- </a> -->
</sup>
 Departments often use these surprise funds to support previously unfunded, “on-the-bubble” research proposals. These marginal projects produced valuable publications and patents, showing that even more research funding would continue to deliver returns.</p><p>Unfortunately, it appears likely that the Trump administration will fund less science, not more. It remains to be seen whether the administration's scientific cuts will make it through courts or Congress. But we can be confident that, on average, we will be worse off if they do.</p><p>It's not always possible to predict which projects will succeed, or how their benefits will manifest. But history shows that even seemingly inefficient investments in basic science often yield transformative returns over time. By funding the speculative, the strange, and the bold, we’re not just taking risks — we’re making some of the best bets for our collective future.</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Deena Mousa</strong> is the Chief of Staff for the Global Health and Wellbeing portfolio at Open Philanthropy, and serves as a grantmaker and researcher. She is also a freelance journalist covering global health and development.</p>		 		 		 <p><strong>Lauren Gilbert</strong> is the Horizon Scanning Study Group Programme Manager at Renaissance Philanthropy. She is also a fellow at the Energy for Growth Hub and Roots of Progress.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published February 2025		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">science</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Y Combinator backing AI company to abuse factory workers (401 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43170850</link>
            <guid>43170850</guid>
            <pubDate>Tue, 25 Feb 2025 12:04:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43170850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43171286"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171286" href="https://news.ycombinator.com/vote?id=43171286&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Tbh this isn’t that crazy. If you hire someone to do their job outputting 10 items per hour and that number is reasonable because a bunch of other workers you hired for the same job are doing it and 1 guy hits 1 per hour then that guys shouldn’t be doing that job.</p><p>The outrage should be focused on the absolute meme of their ad video cuz they were like “lets literally have a convo with an individual but refer to them as a workspace and have them say human painful responses but then just shit on them anyway impersonally”</p><p>The product is not crazy. The video is wild.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171389"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171389" href="https://news.ycombinator.com/vote?id=43171389&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Because this will definitely be used only to innocently tell off people doing 1/10 the work of everyone else, and not micromanage and hound people to increasingly unrealistic standards in already desperate conditions.</p><p>Safe to say you aren't in any position where every move you make will be watched by AI and analysed for faults so that your boss can scream at you more efficiently whenever you don't meet standards for their pitiful wages.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171537"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171537" href="https://news.ycombinator.com/vote?id=43171537&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Your example sounds reasonable but it's not realistic: The actual use of this type of tools is to intimidate those workers who have outputted 9.8 items instead of the average 9.9 over the past week.</p><p>This is who our society ended up making Amazon delivery workers urinating in fucking bottles inside their trucks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171411"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171411" href="https://news.ycombinator.com/vote?id=43171411&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Shouldn't the manager of the 'bunch of workers' notice the guy is underperforming and understand why ? Maybe that manager is the one that shouldn't be doing that job</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171485"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171485" href="https://news.ycombinator.com/vote?id=43171485&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>And they do that by either looking over your shoulder (1 person at a time) or collecting metrics on the entire team and the output. Both of these have different downsides.</p><p>The biggest issue is leadership or managers always wanting the number to go up from the individual. "We need 12 widgets per hour instead of 10 for just this one quarter bro" but then that becomes the new norm and eventually "We need 14/16/18/20 widgets per hour..."</p><p>It's boiling frog management that makes people distrust managers doing any kind of performance monitoring</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171528"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171528" href="https://news.ycombinator.com/vote?id=43171528&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>yah this is exactly what labor law says in some countries:  a manager standing behind your desk?  ok.  a machine surveilling you?  not ok.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171304"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171304" href="https://news.ycombinator.com/vote?id=43171304&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Continued rant :</p><p>It’s kinda like a ruler. If you measure workers so that one’s doing 10x less/worse output than the average that’s good.</p><p>If you compare workers down to the .01% difference in output that’s stupid and inhumane.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171550"><td></td></tr>
                  <tr id="43171524"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171524" href="https://news.ycombinator.com/vote?id=43171524&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>It's clear you never worked in a factory and you have just as much empathy as these CS grads.
This kind of thinking why I hate capitalism so much.</p><p>I worked in a factory multiple times and I can tell from experience nobody needs a stupid performance measurement like this. Your manager will make sure you work you ass off. Or you work with a big dangerous machine so you have to pay very much attention all day.
Of course not every factory is the same, but putting even more pressure to factory workers like this is just inhumane and the most capitalist move I can imagine. Next step is to put robotic whips next to the lines and when their productivity goes below a specific value hit them automatically... Literal slavery.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171536"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171536" href="https://news.ycombinator.com/vote?id=43171536&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Looking for 10x discrepancies is not how this will be used, and you know it. Adoption of this sort of tech is going to lead to Amazon "peeing in bottles" situations. It's wild how much faith people have in the ethics of business owners, especially the ultra-wealthy.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171313"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171313" href="https://news.ycombinator.com/vote?id=43171313&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; <i>Tbh this isn’t that crazy.</i></p><p>Yep, seems like a bog standard accountability / performance management.</p><p>&gt; <i>The product is not crazy. The video is wild.</i></p><p>This is how it all starts. Sane solutions wielded by madmen.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171360"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171360" href="https://news.ycombinator.com/vote?id=43171360&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>This sort of performance management is unfortunately necessary. The problem is that we need tools for it to be built by people who can empathise with those subjected to them, and who want to do the right thing, and not these sorts of folks who are too immature and inexperienced to get it right.</p><p>My previous company ran a warehouse and there was a clear bell curve of productivity. Most people were fine, some were excellent, but some were below the level that was realistically achievable. We did careful and considerate analysis and it helped improve productivity.</p><p>When done badly however you end up with management using productivity tracking as a lever to increase productivity across the curve. Amazon driver delivery quotas are a great example – people urinating in bottles is clearly a symptom of the quota being too high. Unfortunately software built naively to help bring up the bottom 10% can too easily be used to force up the productivity of the other 90%.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171508"><td></td></tr>
                  <tr id="43171378"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171378" href="https://news.ycombinator.com/vote?id=43171378&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>I used to run a small factory (two assembly lines 10 people) and something like this would have been useful, not to force people to work harder but the optimise movements and points of friction. I would actively encourage and reward people for making suggestion and we had a process in place to test if changes made thing better (and not just faster - we included easier, simpler, more enjoyable etc in the test)</p><p>Sadly it’s not about the tool in this case, it’s how it’s being promoted and positioned. The line “know who’s working and who’s not” on their website says it all sadly.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171394"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171394" href="https://news.ycombinator.com/vote?id=43171394&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>As someone who grew up in a 3rd world country and whose mother owned a clothing factory, this product seems...fine? The response is an indication of how little people know about how their t-shirts and shoes are made.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171478"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171478" href="https://news.ycombinator.com/vote?id=43171478&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>It's nuanced. If it allows you to find outliers (low performers to manage and high performers to praise), that's fine. If you try to push everyone further and further to their breaking point and make them trade the same amount of money for more of their time and more importantly health, <i>it's certainly not fine</i>.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171419"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171419" href="https://news.ycombinator.com/vote?id=43171419&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Is it dystopian, or is it just real-time performance monitoring poorly marketed by inexperienced founders?</p><p>There are tools like this for tracking git commits and velocity (that I’ve been on the receiving end of). It probably makes less sense in that context, but if your job is a repetitive task, I don’t think it’s necessarily abuse or dystopian to track it.</p><p>Monitoring bottlenecks isn’t a bad thing. They probably could have chosen an example where the solution to the bottleneck didn’t involve berating a low performer (e.g. adjusting the line to add another station or similar)</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171484"><td></td></tr>
                  <tr id="43171321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171321" href="https://news.ycombinator.com/vote?id=43171321&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Brought to you by the VC famous for InstallMonetizer? Make no mistake, it’ll basically back anything that makes money, there’s no moral high ground. And like it or not, this kind of AI (or should I say A-eye) is here to stay.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171431"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171431" href="https://news.ycombinator.com/vote?id=43171431&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Boost your assembly line efficiency by up to 30%</p><p>Ethics of this aside the above claim must be dubious I would think the majority of manufacturing inefficiencies are due to down time as a result of raw material shipping delays or machine break down… of course I’m in no position to offer an informed opinion but just based on the product website I have a hard time taking this stuff seriously.</p><p>Monitoring of factory workers isn’t hard to do with current surveillance and 1 or 2 humans in the loop</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171209"><td></td></tr>
            <tr id="43171369"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171369" href="https://news.ycombinator.com/vote?id=43171369&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Software can not abuse workers. Managers can, with our without software.</p><p>We've had automated KPI measuring tools since punch clocks. Nowadays it's OK in some companies to install remote access software to monitor employees' screens. It's nothing new. It's just collecting data. Question is, what will bosses do with this data, will they abuse or develop.</p><p>I have no hate towards those guys. No love also. It's just business.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171368"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171368" href="https://news.ycombinator.com/vote?id=43171368&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>The secondary school the founder went to is a dead giveaway. Ofcourse yc would fund them. Anyone would fund them infact.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171056"><td></td></tr>
                <tr id="43171434"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171434" href="https://news.ycombinator.com/vote?id=43171434&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Why not deploy it across every moment of everyone's life, with algorithmic prediction of economically unproductive deviance and BadThink?</p><p>Maybe I'll pitch that to someone with money.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43170866"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43170866" href="https://news.ycombinator.com/vote?id=43170866&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Thank you, I’ve been seeing the reaction to the announcement but hadn’t yet found the announcement itself.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171452"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171452" href="https://news.ycombinator.com/vote?id=43171452&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>That post was pulled partly because of my comment. I commented this:</p><p>"While I see the economical usefullness, this sounds like the worst possible application of AI.</p><p>Using AI to surveil is building hell on earth. AI should be used to help people work less/easier, not whip them into working more."</p><p>Which ended up on the top of the thread. Was surprised to wake up this morning and see it gone.</p><p>LinkedIn post I made about this:</p><p><a href="https://www.linkedin.com/posts/crufter_today-y-combinator-deleted-this-announcement-activity-7300050840852086786-x6WH?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAABcNA2MB_d5j-ofPyG8XDq4OUyAsV3UKRKw" rel="nofollow">https://www.linkedin.com/posts/crufter_today-y-combinator-de...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171276"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171276" href="https://news.ycombinator.com/vote?id=43171276&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Imagine the features you could add to this. Like a robot that walks around behind the workers and gives well-timed corrective communications with a whip.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171339"><td></td></tr>
                  <tr id="43171022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171022" href="https://news.ycombinator.com/vote?id=43171022&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>This kind of product is really shameful, and peak capitalism... looking at people as mere robots to serve your, disgusting</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171316"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171316" href="https://news.ycombinator.com/vote?id=43171316&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Nobody looks at people as robots. Robots are cheaper, do not require food or sleep, and do not have to be murdered when they attempt to unionize.</p><p>Robots are far superior.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171370"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171370" href="https://news.ycombinator.com/vote?id=43171370&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Not yet for many tasks. Humans are more flexible, easier to replace (nothing to install etc) and one fte in this type of work is 10 years of robot. Hope it will change soon, but it's not there yet.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171348"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171348" href="https://news.ycombinator.com/vote?id=43171348&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>I think there is a balance.</p><p>Otherwise its going to be 1984 in more than one way (the spying part is already there) (it would also do of that the countries are ready to produce things as much but they won't and limit it to create that constant mood of war to make people not question them / make them weak.)</p><p>I think capitalism has fallen. Capitalism is a good system but to its degrees. If you push the accelerator too hard , you get fuedalism.</p><p>and we are at feudalism. I am not sure if we can undo this. Let this sink in, the american dream , all our thinkng that capitalism being good and communism being bad fundamentally doesn't matter because we have entered a system where the lines of division are so blurry that they are practically nonexistent.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171501"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43171501" href="https://news.ycombinator.com/vote?id=43171501&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; But then if everyone is out of their job and unemployed ? who will buy the stuff if noone has the money.</p><p>Last time I tried to say something like that I got plenty comments calling me for reading too many sci-fi books... I guess some people just lack imagination and experience with exponentials.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171393"><td></td></tr>
                        <tr id="43171243"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171243" href="https://news.ycombinator.com/vote?id=43171243&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Peak capitalism would have the employees holding their own value and not being crushed in trash power dynamics to allow this kind of stuff. Capitalism is about nonviolent voluntary exchanges between two parties, when one party has a power dynamic skewed in such a way they can use tools like this that employees hate, then that’s not capitalism anymore.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171503" href="https://news.ycombinator.com/vote?id=43171503&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>Funny that what you describe will never exist without heavy (extreme?) regulation and gradual taxation, which are anathema to most advocates of capitalism. Have you considered that maybe your definition of capitalism doesn't agree with the definition society has agreed upon?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171322"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43171322" href="https://news.ycombinator.com/vote?id=43171322&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>so this is more like peak feudalism huh?</p><p>back to 1800's I suppose.</p><p>There is no seperation b/w private entities , the state and the church , all trying to exploit the middle class / lower class was one of the gists that I think when I recall feudalism
sounds familiar ?
Guess what ?
We are living at one right now.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171301"><td></td></tr>
                <tr id="43171379"><td></td></tr>
                        <tr id="43171053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171053" href="https://news.ycombinator.com/vote?id=43171053&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Apparently on their website they're asking for comments/feedback, so... you can tell them yourself what you think of their disgusting tech</p><p>&gt; Let us know at founders@optifye.ai, and we’ll help them drop their cortisol levels :)</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43171315"><td></td></tr>
                <tr id="43171375"><td></td></tr>
                  <tr id="43171257"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43171257" href="https://news.ycombinator.com/vote?id=43171257&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>Yeah it has terrible optics, yet it's clearly going to be normalized and come.  The question is who does it and what is the organization of it.  If this company doesn't do it, the next will.</p><p>In certain roles, AI micromanagement clearly will create higher performance.  Add the marketplace of capitalism and it'll all compete away.</p><p>There are certain roles, like artists, where this is the wrong solution wholly: monitoring whether an artist is at her desk will create badly performing artists, and this will show.  In these roles, these tools won't apply.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43171324"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171324" href="https://news.ycombinator.com/vote?id=43171324&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p><i>In these roles, these tools won't apply.</i></p><p>There will be companies that will apply them regardless, even in roles where they'll make things worse. The incentive for managers to show 'a bias for action' often results in managers doing any action that they can think of rather than the right action backed by data.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171335"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171335" href="https://news.ycombinator.com/vote?id=43171335&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div>
                  <p>In the US? Sure. In more developed parts of the world? Doubtful. European labor laws are already much, much stronger than their US counterparts, and most countries outright ban using cameras to monitor employees.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43171303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43171303" href="https://news.ycombinator.com/vote?id=43171303&amp;how=up&amp;goto=item%3Fid%3D43170850"></a></center>    </td><td><br><div><p>&gt; Yeah it has terrible optics, yet it's clearly going to be normalized and come. The question is who does it and what is the organization of it. If this company doesn't do it, the next will.</p><p>Where have we seen this before..</p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chicory: A JVM native WebAssembly runtime (139 pts)]]></title>
            <link>https://chicory.dev/</link>
            <guid>43170545</guid>
            <pubDate>Tue, 25 Feb 2025 11:22:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chicory.dev/">https://chicory.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=43170545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><p><a href="#__docusaurus_skipToContent_fallback">Skip to main content</a></p></div><nav aria-label="Main"><div><div><a href="https://chicory.dev/"><p><img src="https://chicory.dev/img/chicory1.png" alt="Chicory Logo"><img src="https://chicory.dev/img/chicory1.png" alt="Chicory Logo"></p><b>Chicory</b></a><p><a href="https://chicory.dev/docs/">Docs</a><a href="https://chicory.dev/blog">Blog</a></p></div><div><a href="https://github.com/dylibso/chicory" target="_blank" rel="noopener noreferrer">GitHub</a></div></div></nav><div id="__docusaurus_skipToContent_fallback"><header><div><p>Chicory, a JVM native WebAssembly runtime</p><p><img src="https://chicory.dev/img/chicory1.png"></p><div><p><a href="https://chicory.dev/docs">Get Started</a></p></div></div></header><main><div><div><h3>Pure Java</h3><p>Using Chicory you don't need to rely on any system resource. Everything runs in 100% pure Java on top of the standard library.</p></div><div><h3>Easy integration</h3><p>Integrating Chicory in your project is smooth and only requires a few steps. Give your application a twist with a plugin system.</p></div><div><h3>Secure by design</h3><p>Web Assembly modules are running in a sandboxed environment. You have full control over the used resources.</p></div></div></main></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla sales in Europe down 45% in January (253 pts)]]></title>
            <link>https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</link>
            <guid>43170090</guid>
            <pubDate>Tue, 25 Feb 2025 10:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523">https://www.ft.com/content/cdd0b5c8-2703-4fd4-9ebf-26087cac8523</a>, See on <a href="https://news.ycombinator.com/item?id=43170090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offer-6dcd8564-bf41-453c-93b5-7b00c6676b60" data-component="heroOffer" data-component-unique-name="Hero offer"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Limited time offer</span></h2><h2><strong><span>Save 40% on Standard Digital</span></strong></h2></p><p><span>was </span><span>CHF660</span><span> </span><span>now </span><span>CHF395</span><span> for your first year
Make up your own mind. Build robust opinions on the FT's trusted journalism.
Offer available until 27 February 2025.</span></p></div></div><div id="recommendedOffers-Recommended offers" data-component="recommendedOffers" data-component-unique-name="Recommended offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_trial.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF1</span><span> for 4 weeks</span></p><p><span>Then </span><span>CHF85</span><span> per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_weekend_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>was </span><span>CHF949</span><span> </span><span>now </span><span>CHF815</span><span> per year</span></p><p><span>Get Premium &amp; FT Weekend Print edition for the price of Premium. Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_print.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF345</span><span> for your first year</span></p><p><span>FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription options"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=cdd0b5c8-2703-4fd4-9ebf-26087cac8523">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Awesome DeepSeek Integrations (115 pts)]]></title>
            <link>https://github.com/deepseek-ai/awesome-deepseek-integration</link>
            <guid>43169827</guid>
            <pubDate>Tue, 25 Feb 2025 09:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/awesome-deepseek-integration">https://github.com/deepseek-ai/awesome-deepseek-integration</a>, See on <a href="https://news.ycombinator.com/item?id=43169827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><table>
    <tbody><tr>    
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/build/icon.png?raw=true"><img src="https://github.com/ThinkInAIXYZ/deepchat/raw/main/build/icon.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/ThinkInAIXYZ/deepchat/blob/main/README.md">DeepChat</a></td>
        <td>DeepChat is a fully free desktop smart assistant, with a powerful DeepSeek large model, supporting multi-round conversations, internet search, file uploads, knowledge bases, and more.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4"><img src="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/quantalogic/quantalogic">Quantalogic</a> </td>
        <td> QuantaLogic is a ReAct (Reasoning &amp; Action) framework for building advanced AI agents. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI"><img src="https://private-user-images.githubusercontent.com/13600976/295807353-224d547a-6fbc-47c8-859f-aa14813e2b0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xMzYwMDk3Ni8yOTU4MDczNTMtMjI0ZDU0N2EtNmZiYy00N2M4LTg1OWYtYWExNDgxM2UyYjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTZkM2Q2NTI3ZmRjNWY4MDdhMDYxYzMyMWFiZDZhZTA3NGI5NWM0NjNkODIxN2Q4NzA0YWUzODg3NDg4MDJjZjcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.s8qVrORMqA5dEZLpjkrDFrv8Rxifqq3fZxYSP5VLboI" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatbox/README.md">Chatbox</a> </td>
        <td> Chatbox is a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU"><img src="https://private-user-images.githubusercontent.com/59196087/295846436-bb65404c-f867-42d8-ae2b-281fe953ab54.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDY0MzYtYmI2NTQwNGMtZjg2Ny00MmQ4LWFlMmItMjgxZmU5NTNhYjU0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTExNmIxYWRiNzQyMDU5NDBjYWUwYjdkZjEzZWFjMTUyYTdhYzE2ZGQ0ZDM0Zjc5NjZkMTBiN2FmNGY5M2QxNjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.YcFUqt7RuqpUhSaeADZlZ27zCUVVY-RAppj8UVlsCCU" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/chatgpt_next_web/README.md"> ChatGPT-Next-Web </a> </td>
        <td> ChatGPT Next Web is a cross-platform ChatGPT web UI, with GPT3, GPT4 &amp; Gemini Pro support. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/assets/favicon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Coco%20AI/assets/favicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Coco%20AI/README.md">Coco AI</a></td>
        <td> <a href="https://coco.rs/" rel="nofollow">Coco AI</a> is a fully open-source, cross-platform unified search and productivity tool that connects and searches across various data sources, including applications, files, Google Drive, Notion, Yuque, Hugo, and more, both local and cloud-based. By integrating with large models like DeepSeek, Coco AI enables intelligent personal knowledge management, emphasizing privacy and supporting private deployment, helping users quickly and intelligently access their information. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/assets/liubai-logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/liubai/assets/liubai-logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/liubai/README.md">Liubai</a> </td>
        <td> Liubai allows DeepSeek to have arms and legs to manipulate your notes, tasks, calendars, and to-do lists just on WeChat! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc"><img src="https://private-user-images.githubusercontent.com/59196087/295849194-1ac9791b-87f7-41d9-9282-a70698344e1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii81OTE5NjA4Ny8yOTU4NDkxOTQtMWFjOTc5MWItODdmNy00MWQ5LTkyODItYTcwNjk4MzQ0ZTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQzMjRiMmUxMjRkNWI4MGEzNTI0M2IwNDUwZTMxY2ZhYzM5ZjhiMTlhNTNiOWFkNWY4MWFkMWExMTQzNmZjYzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wmTb4DZoPsZgMv1my1lUq5PKOo8Aj4aArlZW21eLQEc" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/pal/README.md"> Pal - AI Chat Client<br>(iOS, ipadOS) </a> </td>
        <td> Pal is a customized chat playground on iOS. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667"><img src="https://camo.githubusercontent.com/099fcbeaa5d07a52e261dffe575cae7fe0fcfffbaaeff5b1eed55f9956a0b2b9/68747470733a2f2f7777772e6c69627265636861742e61692f6c69627265636861742e737667" alt="LibreChat" width="64" height="auto" data-canonical-src="https://www.librechat.ai/librechat.svg"></a> </td>
        <td> <a href="https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints/deepseek" rel="nofollow">LibreChat</a> </td>
        <td> LibreChat is a customizable open-source app that seamlessly integrates DeepSeek for enhanced AI interactions. </td>
    </tr>
     <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png"><img src="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/longevity-genie/just-chat">Just-Chat</a> </td>
        <td> Make your LLM agent and chat with it simple and fast!</td>
     </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/3856d7880b3489a97086a10235267f303677b64de726b694510e2c2b946c8e4b/68747470733a2f2f7777772e7061706572736770742e636f6d2f696d616765732f6c6f676f2f66617669636f6e2e69636f" alt="PapersGPT" width="64" height="auto" data-canonical-src="https://www.papersgpt.com/images/logo/favicon.ico"></a> </td>
        <td> <a href="https://github.com/papersgpt/papersgpt-for-zotero">PapersGPT</a> </td>
        <td> PapersGPT is a Zotero plugin that seamlessly with DeepSeek and other multiple AI models for quickly reading papers in Zotero. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico"><img src="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/rss_translator/README.md"> RSS Translator </a> </td>
        <td> Translate RSS feeds into your language! </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png"><img src="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/enconvo/README.md"> Enconvo </a> </td>
        <td> Enconvo is the Launcher of the AI era, the entry point for all AI functions, and a thoughtful intelligent assistant.</td>
    </tr>
    <tr>
        <td><a target="_blank" rel="noopener noreferrer" href="https://github.com/kangfenmao/cherry-studio/blob/main/src/renderer/src/assets/images/logo.png?raw=true"><img src="https://github.com/kangfenmao/cherry-studio/raw/main/src/renderer/src/assets/images/logo.png?raw=true" alt="Icon" width="64" height="auto"></a></td>
        <td><a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/cherrystudio/README.md">Cherry Studio</a></td>
        <td>A powerful desktop AI assistant for producer</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/bf93b3f6e5e8507f37f363ade24096d52f638646f083e0b46828ec9a0c32c6f7/68747470733a2f2f746f6d656d6f2e746f702f696d616765732f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://tomemo.top/images/logo.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/tomemo/README.md"> ToMemo (iOS, ipadOS) </a> </td>
        <td> A phrasebook + clipboard history + keyboard iOS app with integrated AI macromodeling for quick output use in the keyboard.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png"><img src="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/buxuku/video-subtitle-master">Video Subtitle Master</a></td>
        <td> Batch generate subtitles for videos, with the ability to translate subtitles into other languages. This is a client-side tool that supports both Mac and Windows platforms and integrates with multiple translation services such as Baidu, Volcengine, DeepLx, OpenAI, DeepSeek, and Ollama.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/dist/assets/chatworm-72x72.png"><img src="https://github.com/UnknownEnergy/chatgpt-api/raw/master/dist/assets/chatworm-72x72.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/README.md">Chatworm</a> </td>
        <td> Chatworm is a webapp for multiple cutting-edge LLM models, open-source and also available on Android. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png"><img src="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/tisfeng/Easydict">Easydict</a></td>
        <td> Easydict is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text. Supports calling large language model APIs for translation.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67"><img src="https://camo.githubusercontent.com/6f1817b6a7cec2fcf91243e7b695f87de7da88269d9ba27287301dd4085ea803/68747470733a2f2f7777772e726179636173742e636f6d2f66617669636f6e2d70726f64756374696f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://www.raycast.com/favicon-production.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/raycast/README.md">Raycast</a></td>
        <td> <a href="https://raycast.com/?via=ViGeng" rel="nofollow">Raycast</a> is a productivity tool for macOS that lets you control your tools with a few keystrokes. It supports various extensions including DeepSeek AI.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/7b296d5a5eb2fe1fdef84d531050039229fa75eb1939ae069cd99b8461684bdb/68747470733a2f2f6e69636570726f6d70742e6170702f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://niceprompt.app/favicon.ico"></a> </td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a></td> <td> <a href="https://niceprompt.app/" rel="nofollow">Nice Prompt</a> Organize, share and use your prompts in your code editor, with Cursor and VSCode。</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4" alt="PHP Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-php-client/blob/master/README.md">PHP Client</a> </td>
        <td> Deepseek PHP Client is a robust and community-driven PHP client library for seamless integration with the Deepseek API. </td>
    </tr>
        <tr>
  <td>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/logo.webp"><img src="https://github.com/tornikegomareli/DeepSwiftSeek/raw/main/logo.webp" alt="DeepSwiftSeek Logo" width="64" height="auto"></a>
  </td>
  <td>
    <a href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/README.md">DeepSwiftSeek</a>
  </td>
  <td>
    DeepSwiftSeek is a lightweight yet powerful Swift client library, pretty good integration with the DeepSeek API. 
    It provides easy-to-use Swift concurrency for chat, streaming, FIM (Fill-in-the-Middle) completions, and more.
  </td>
</tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4" alt="Laravel Integration" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-php/deepseek-laravel/blob/master/README.md">Laravel Integration</a> </td>
        <td> Laravel wrapper for Deepseek PHP client, to seamless deepseek API integration with laravel applications.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/assets/zotero-icon.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/zotero/assets/zotero-icon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/zotero/README_cn.md">Zotero</a></td>
        <td> <a href="https://www.zotero.org/" rel="nofollow">Zotero</a> is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67"><img src="https://camo.githubusercontent.com/f6cdd8c1aadf66cbf471860750d26bfaab8f49b153e2ca3bd5afb12d3628f2e4/68747470733a2f2f62336c6f672e6f72672f696d616765732f6272616e642f73697975616e2d3132382e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://b3log.org/images/brand/siyuan-128.png"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/SiYuan/README.md">SiYuan</a> </td>
        <td> SiYuan is a privacy-first personal knowledge management system that supports complete offline usage, as well as end-to-end encrypted data sync.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png"><img src="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/ArvinLovegood/go-stock/blob/master/README.md">go-stock</a> </td>
        <td>go-stock is a Chinese stock data viewer built by Wails with NativeUI and powered by LLM.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4"><img src="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4" alt="Wordware" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/wordware/README.md">Wordware</a> </td>
        <td><a href="https://www.wordware.ai/" rel="nofollow">Wordware</a> is a toolkit that enables anyone to build, iterate, and deploy their AI stack with just natural language.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67"><img src="https://camo.githubusercontent.com/8b5c668210552bd34e3310b85b1c0e38fdf3c843973b741e355f8e269c370128/68747470733a2f2f6672616d657275736572636f6e74656e742e636f6d2f696d616765732f78524a36764e6f396d555965564e7874304b49545843584575536b2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://framerusercontent.com/images/xRJ6vNo9mUYeVNxt0KITXCXEuSk.png"></a> </td>
        <td> <a href="https://github.com/langgenius/dify/">Dify</a> </td>
        <td> <a href="https://dify.ai/" rel="nofollow">Dify</a> is an LLM application development platform that supports DeepSeek models for creating assistants, workflows, text generators, and more. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico"><img src="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico" alt="Big-AGI" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/enricoros/big-AGI/blob/v2-dev/README.md">Big-AGI</a> </td>
        <td><a href="https://big-agi.com/" rel="nofollow">Big-AGI</a> is a groundbreaking AI suite designed to democratize access to advanced artificial intelligence for everyone.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/LiberSonora/LiberSonora/blob/main/assets/avatar.jpeg?raw=true"><img src="https://github.com/LiberSonora/LiberSonora/raw/main/assets/avatar.jpeg?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/LiberSonora/LiberSonora/blob/main/README_en.md">LiberSonora</a> </td>
        <td> LiberSonora, meaning "Voice of Freedom", is an AI-powered, robust, open-source audiobook toolkit that includes features like intelligent subtitle extraction, AI title generation, multilingual translation, with support for GPU acceleration and batch offline processing.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png"><img src="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a></td>
        <td> <a href="https://bobtranslate.com/" rel="nofollow">Bob</a> is a macOS translation &amp; OCR tool ready to use in any app — right out of the box!</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/902ed4848e827dcbea0f8d3300782f922664846019be600557f4d8b77b857035/68747470733a2f2f6167656e746963666c6f772e61692f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://agenticflow.ai/favicon.ico"></a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> </td>
        <td> <a href="https://agenticflow.ai/" rel="nofollow">AgenticFlow</a> is a no-code platform where marketers build agentic AI workflows for go-to-market automation, powered by hundreds of everyday apps as tools for your AI agents.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg"><img src="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a></td>
        <td> <a href="https://stranslate.zggsong.com/en/" rel="nofollow">STranslate</a>（Windows） is a ready-to-go translation ocr tool developed by WPF </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA"><img src="https://private-user-images.githubusercontent.com/32067919/408034405-5e16beb0-993e-47bf-807e-7c8804b313a2.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8zMjA2NzkxOS80MDgwMzQ0MDUtNWUxNmJlYjAtOTkzZS00N2JmLTgwN2UtN2M4ODA0YjMxM2EyLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTMyOTE4ZWUyZjlhMGZhNTY3MzE2OGQxYjcxZmZjYTNmNzUwZTcyMjdlYmJkM2VmNDVkZGIwMzQ0N2U3NzI0YTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.Icv1_i2hpH31uR1lFrsozznE55XOb2Sun-5ubFNbtBA" alt="Asp Client" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">ASP Client</a> </td>
        <td><a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">Deepseek.ASPClient</a>  is a lightweight ASP.NET wrapper for the Deepseek AI API, designed to simplify AI-driven text processing in .NET applications.. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/64c8ae493f81ae0abfb66e0e21ec886a6dafc733ea6a53afd31cfec1893eddc6/68747470733a2f2f7777772e6770746169666c6f772e746563682f6c6f676f2e706e67" alt="gpt-ai-flow-logo" width="64" height="auto" data-canonical-src="https://www.gptaiflow.tech/logo.png"></a> </td>
        <td> <a href="https://www.gptaiflow.tech/docs/product/api-keys-setup#setup-deepseek-api-keys" rel="nofollow">GPT AI Flow</a></td>
        <td>
            The ultimate productivity weapon built by engineers for efficiency enthusiasts (themselves): <a href="https://www.gptaiflow.tech/" rel="nofollow">GPT AI Flow</a>
            <ul dir="auto">
                <li>`Shift+Alt+Space` Wake up desktop intelligent hub</li>
                <li>Local encrypted storage</li>
                <li>Custom instruction engine</li>
                <li>On-demand calling without subscription bundling</li>
            </ul>
        </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg"><img src="https://private-user-images.githubusercontent.com/14835226/409142715-b09f17a8-936d-4dac-8b24-1682d52c9a3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDA0OTQxMDEsIm5iZiI6MTc0MDQ5MzgwMSwicGF0aCI6Ii8xNDgzNTIyNi80MDkxNDI3MTUtYjA5ZjE3YTgtOTM2ZC00ZGFjLThiMjQtMTY4MmQ1MmM5YTNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAyMjUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMjI1VDE0MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUzNzBlNTliNTY4M2M2Y2IwMWU0NTRjN2QxOWIzMWYxY2YxOWIyNzBhZmVlODY1NmNjMTk5N2EwNGI4MDFhNTUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.26iZD75OPiqfIie5FHmrdlrNawBOWwLcdDt0oRXetAg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/alecm20/story-flicks">Story-Flicks</a></td>
        <td>With just one sentence, you can quickly generate high-definition story short videos, supporting models such as DeepSeek.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/901a787a65901d2f6d8daa5988635d8f914336711faf00634d3e957c6669e364/68747470733a2f2f70726f6d70742e3136782e656e67696e6565722f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://prompt.16x.engineer/favicon.ico"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/16x_prompt/README.md">16x Prompt</a> </td>
        <td> <a href="https://prompt.16x.engineer/" rel="nofollow">16x Prompt</a> is an AI coding tool with context management. It helps developers manage source code context and craft prompts for complex coding tasks on existing codebases.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/Alpha%E6%B4%BE/assets/favicon1.png?raw=true" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/Alpha%E6%B4%BE/README.md"> Alpha Pai </a> </td>
        <td> AI Research Assistant / The Next-Generation Financial Information Portal Driven by AI.<br>Proxy for investors to attend meetings and take notes, as well as providing search and Q&amp;A services for financial information and quantitative analysis for investment research.</td>
    </tr>
        <tr><td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67"><img src="https://camo.githubusercontent.com/ddb4b0af3d5d1e4276301599439fd72d0cf19471dd70ac0a70046103d30d451a/68747470733a2f2f646f63732e7861726b2d6172676f2e636f6d2f696d672f6c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://docs.xark-argo.com/img/logo.png"></a> </td> 
        <td> <a href="https://www.xark-argo.com/" rel="nofollow">argo</a> </td>
        <td>Locally download and run Ollama and Huggingface models with RAG on Mac/Windows/Linux. Support LLM API too.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f"><img src="https://camo.githubusercontent.com/f33d267a05cee432e7e3e5bb88f47d0d8b9a0c93fa985daa535a48c351c597fc/68747470733a2f2f7777772e70657465726361742e61692f696d616765732f66617669636f6e2e69636f" alt="Icon" width="64" height="auto" data-canonical-src="https://www.petercat.ai/images/favicon.ico"></a> </td>
        <td> <a href="https://www.petercat.ai/" rel="nofollow">PeterCat</a> </td>
        <td> A conversational Q&amp;A agent configuration system, self-hosted deployment solutions, and a convenient all-in-one application SDK, allowing you to create intelligent Q&amp;A bots for your GitHub repositories.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg"><img src="https://raw.githubusercontent.com/labring/FastGPT/refs/heads/main/.github/imgs/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://fastgpt.cn/en" rel="nofollow">FastGPT</a> </td>
        <td> 
            FastGPT is an open-source AI knowledge base platform built on large language models (LLMs), supporting various models including DeepSeek and OpenAI. We provide out-of-the-box capabilities for data processing, model invocation, RAG retrieval, and visual AI workflow orchestration, enabling you to effortlessly build sophisticated AI applications.
        </td>
   </tr>
   <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/assets/play_store_512.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/ruzhiai_note/assets/play_store_512.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/ruzhiai_note/README.md">RuZhi AI Notes</a> </td>
        <td>RuZhi AI Notes is an intelligent knowledge management tool powered by AI, providing one-stop knowledge management and application services including AI search &amp; exploration, AI results to notes conversion, note management &amp; organization, knowledge presentation &amp; sharing. Integrated with DeepSeek model to provide more stable and higher quality outputs.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67"><img src="https://camo.githubusercontent.com/7d03f3a5a1f9316d2fc9dd60b466b3e63fdd8d0026ec8f2ab322152199ac5b42/68747470733a2f2f63646e2e6c696e6b2d61692e746563682f646f632f436f572532306c6f676f2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://cdn.link-ai.tech/doc/CoW%20logo.png"></a> </td>
        <td> <a href="https://github.com/zhayujie/chatgpt-on-wechat">Chatgpt-on-Wechat</a> </td>
        <td> Chatgpt-on-Wechat(CoW) is a flexible chatbot framework that supports seamless integration of multiple LLMs, including DeepSeek, OpenAI, Claude, Qwen, and others, into commonly used platforms or office software such as WeChat Official Accounts, WeCom, Feishu, DingTalk, and websites. It also supports a wide range of custom plugins. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667"><img src="https://camo.githubusercontent.com/0feb9a6a193c29d48bc5a74ffc68a9af3d0aee605f5c707e324f27c3df09d97a/68747470733a2f2f617468656e616c61622e61692f6173736574732f66617669636f6e2f66617669636f6e2e737667" alt="Icon" width="64" height="auto" data-canonical-src="https://athenalab.ai/assets/favicon/favicon.svg"></a> </td> 
        <td> <a href="https://athenalab.ai/" rel="nofollow">Athena</a> </td>
        <td>The world's first autonomous general AI with advanced cognitive architecture and human-like reasoning capabilities, designed to tackle complex real-world challenges.</td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67"><img src="https://camo.githubusercontent.com/d4a5745a9a6978748eb1c432efe9c5e8bd8dbc9af981a746f68316713c2411ed/68747470733a2f2f6d61786b622e636e2f696d616765732f66617669636f6e2e706e67" alt="Icon" width="64" height="auto" data-canonical-src="https://maxkb.cn/images/favicon.png"></a> </td>
        <td> <a href="https://github.com/1Panel-dev/MaxKB">MaxKB</a> </td>
        <td> <a href="https://maxkb.cn/" rel="nofollow">MaxKB</a> is a ready-to-use, flexible RAG Chatbot. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/TigerGPT/assets/logo.png"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/TigerGPT/assets/logo.png" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://ttm.financial/gpt" rel="nofollow">TigerGPT</a> </td>
        <td>TigerGPT is the first financial AI investment assistant of its kind based on OpenAI, developed by Tiger Group. TigerGPT aims to provide intelligent investment decision-making support for investors. On February 18, 2025, TigerGPT officially integrated the DeepSeek-R1 model to provide users with online Q&amp;A services that support deep reasoning. </td>
    </tr>
    <tr>
        <td> <a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/HIX.AI/assets/logo.svg"><img src="https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/HIX.AI/assets/logo.svg" alt="Icon" width="64" height="auto"></a> </td>
        <td> <a href="https://hix.ai/" rel="nofollow">HIX.AI</a> </td>
        <td>Try DeepSeek for free and enjoy unlimited AI chat on HIX.AI. Use DeepSeek R1 for AI chat, writing, coding &amp; more. Experience next-gen AI chat now!</td>
    </tr>
</tbody></table></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Core Git Developers Configure Git (495 pts)]]></title>
            <link>https://blog.gitbutler.com/how-git-core-devs-configure-git/</link>
            <guid>43169435</guid>
            <pubDate>Tue, 25 Feb 2025 08:17:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.gitbutler.com/how-git-core-devs-configure-git/">https://blog.gitbutler.com/how-git-core-devs-configure-git/</a>, See on <a href="https://news.ycombinator.com/item?id=43169435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
	
	<div>
		<p>A few weeks ago I <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/" rel="noreferrer">wrote about</a> Git’s <code>help.autocorrect</code> setting and the strange tale of the origin of it’s deciseconds value.</p><p>It got me to thinking about other <code>git config</code> settings that most people likely don’t know about and which should probably be defaulted differently. </p><p>In this post, I’ll go through some of the perhaps obscure Git config settings that I have personally globally enabled and go into them to explain what they do and why they should <em>probably</em> be the default settings. </p><p>Also, it turns out that I learned most of these from the people who actually work on the core Git codebase every day.</p><h2 id="tldr">TLDR</h2><p>First, though, some of you may not particularly care about the wonderful and sordid history of the <code>rerere</code> values or whatever. You may just be thinking “just give me the settings so I can blindly throw them into my <code>~/.gitconfig</code> file." </p><p>Well, fair enough. Here is the fun stuff:</p><pre><code># clearly makes git better

[column]
        ui = auto
[branch]
        sort = -committerdate
[tag]
        sort = version:refname
[init]
        defaultBranch = main
[diff]
        algorithm = histogram
        colorMoved = plain
        mnemonicPrefix = true
        renames = true
[push]
        default = simple
        autoSetupRemote = true
        followTags = true
[fetch]
        prune = true
        pruneTags = true
        all = true

# why the hell not?

[help]
        autocorrect = prompt
[commit]
        verbose = true
[rerere]
        enabled = true
        autoupdate = true
[core]
        excludesfile = ~/.gitignore
[rebase]
        autoSquash = true
        autoStash = true
        updateRefs = true

# a matter of taste (uncomment if you dare)

[core]
        # fsmonitor = true
        # untrackedCache = true
[merge]
        # (just 'diff3' if git version &lt; 2.3)
        # conflictstyle = zdiff3 
[pull]
        # rebase = true</code></pre><p>Copypasta, my friends.</p><h2 id="how-do-git-core-devs-configure-their-gits">How do Git core devs configure their Gits?</h2><p>Before I dig into these one by one, there is an interesting question about if even the core Git developers think that some of these default values should be changed.</p><p>This came up not too long ago on the Git mailing list, and honestly, a few of these settings I personally learned from <a href="https://lore.kernel.org/git/60b5d281552d6_e359f20828@natae.notmuch/?ref=blog.gitbutler.com">this thread</a> called "Spring Cleaning" where Felipe Contreras challenged the Git core team to remove all their built up config options and aliases and see what it’s like to use Git stock, out of the box.</p><p>He challenged the list to pay attention to what settings they really wanted to change and share the top settings changes that seemed the most important with the list.</p><p>The <a href="https://lore.kernel.org/git/60df97ed24687_34a92088a@natae.notmuch/?ref=blog.gitbutler.com">results</a> were very interesting, a rather concise list of 9 config settings and 3 aliases that the experiment participants more or less agreed should arguably be new defaults. Let's just take a look at the proposed config setting changes.</p><pre><code>merge.conflictstyle = zdiff3
rebase.autosquash = true
rebase.autostash = true 
commit.verbose = true
diff.colorMoved = true
diff.algorithm = histogram
grep.patternType = perl
feature.experimental = true
branch.sort = committerdate</code></pre><p>Now, <em>none</em> of these have become the new defaults in the 3 or 4 years since this experiment, but it’s interesting that a lot of the Git developers themselves have a hard time using Git without several of these turned on.</p><p>Even more interesting is that <em>most of you </em>probably don’t know what <em>any</em> of these do.</p><p>So, let’s dig into them. What do these do and why should you almost certainly blindly trust me and go ahead and enable them?</p><p>I'm going to group these settings into three categories:</p><ul><li><a href="#clearly-makes-git-better" rel="noreferrer">Clearly Makes Git Better</a></li><li><a href="#why-the-hell-not" rel="noreferrer">Why the Hell Not?</a></li><li><a href="#a-matter-of-taste" rel="noreferrer">A Matter of Taste</a></li></ul><p>Let's get started.</p><h2 id="clearly-makes-git-better">Clearly Makes Git Better</h2><p>This first group of settings <em>clearly</em> makes Git better by default. There are generally zero downsides to enabling any of them.</p><h2 id="listing-branches">Listing branches</h2><p>I noted this in a previous blog post here about Git Tips under “<a href="https://blog.gitbutler.com/git-tips-2-new-stuff-in-git/#some-git-branch-stuff">Branch Stuff</a>” but as this was also in the Spring Cleaning list, I think everyone agrees that listing out Git branches should probably not be alpha-ordered by default.</p><p>The two settings which help improve this are <code>branch.sort</code> and <code>column.ui</code>. The first of which sorts the list by the most recent commit date (so probably more interesting at the top) rather than by alpha order. The second will put the branch names in a column format so you can see more per screen.</p><pre><code>git config --global column.ui auto
git config --global branch.sort -committerdate
</code></pre><p>The <code>column.ui</code> setting also affects the output of other listing commands (clean, status, tag), but generally I think it’s better than the default.</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png" alt="" loading="lazy" width="1598" height="984" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-08.04.03@2x.png 1598w" sizes="(min-width: 720px) 720px"></figure><p>You can also sort by other things than committer date, but I think it’s pretty clearly the most useful one.</p><p>Speaking of listing things, it’s also pretty ridiculous that this isn’t the default for listing tags, since it’s what nearly everyone actually wants.</p><p>Normally, if you list tags by alpha order, you’ll get something like this:</p><pre><code>$ git tag
nightly/0.5.100
nightly/0.5.1000
nightly/0.5.1001
nightly/0.5.101
nightly/0.5.1010
</code></pre><p>Nobody wants <code>0.5.101</code> to come after <code>0.5.1000</code>, but that’s alpha order. You can fix this by setting this:</p><pre><code>git config --global tag.sort version:refname
</code></pre><p>Which will generally do what you expect, treating dotted version numbers as a series of integer values for sorting purposes. Trust me, just enable this.</p><h2 id="default-branch">Default branch</h2><p>This one may be a little more controversial, since it can be argued to be somewhat political, but there should be a default branch name in Git where it doesn’t complain every time you <code>init</code> a new repo.</p><pre><code>git config --global init.defaultBranch main
</code></pre><p>Personally, I don’t have a problem with <code>master</code> and most of my repositories use that since that used to be the default, but I’m also fine with <code>main</code>, so whatever it is you want to use, just go ahead and set it.</p><p>Mostly what I find stupid is that now Git is annoying about this rather than just updating the default value. I wish Git had some taste here, but they don't, so you should just set it to something you find reasonable. But whatever.</p><h2 id="better-diff">Better diff</h2><p>There is actually a whole blog post that could be written about <code>git diff</code> algorithms, but the short story is that by default Git will use an old, fast, pretty reliable diff algorithm called "myers diff".</p><p>To give you a sense of what ‘old’ means, it was first published in a paper in 1986, so it’s almost 40 years old now. If you’re as old as I am, perhaps I can give you some childhood perspective as to what that means. The movies ‘The Three Amigos’, ‘An American Tail’ and the first ‘Highlander’ came out in theaters that year.</p><p>In any case, some advances have been made since then (with some tradeoffs too) and it may surprise you to know that Git actually ships with 4 built in diff algorithms it can use: <a><code>myers</code></a>, <code>minimal</code>, <a href="https://blog.jcoglan.com/2017/09/19/the-patience-diff-algorithm/?utm_source=chatgpt.com"><code>patience</code></a> and <code>histogram</code>.</p><p>Almost certainly what you want to be using is the <code>histogram</code> algorithm (an incremental improvement on ‘patience’), rather than the default of 'myers'. You can globally change it like this:</p><pre><code>git config --global diff.algorithm histogram
</code></pre><p>Here is an example of simple code movement diffed in <code>myers</code> vs <code>histogram</code>, to give a short taste of how it can be a bit smarter:</p><p>Let's say we move a css class below a similar one, change it a little, and then run <code>git diff</code> with the default <code>myers</code> algorithm. We may get something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png" alt="" loading="lazy" width="1626" height="1156" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.07.58@2x.png 1626w" sizes="(min-width: 720px) 720px"></figure><p>Ok, a little confusing. Here is what <code>histogram</code> would give us in the same scenario:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png" alt="" loading="lazy" width="1628" height="1266" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1000w, https://blog.gitbutler.com/content/images/size/w1600/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1600w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.08.28@2x.png 1628w" sizes="(min-width: 720px) 720px"></figure><p>It's a bit more clear here what's actually happened.</p><p>As recently as last year, Elijah (of our <a href="https://www.youtube.com/watch?v=KXPmiKfNlZE&amp;ref=blog.gitbutler.com">Git Merge fame</a>) suggested that<br><a href="https://lore.kernel.org/git/CABPp-BEmgOAj17DozyXNaf-9CawDic4uTpMbckef3+zHf7URqQ@mail.gmail.com/?ref=blog.gitbutler.com">histogram or patience</a> might make better defaults, in addition to Felipe's Spring Cleaning suggestion of the same thing, but in reality it’s unlikely to get through the gauntlet anytime soon.</p><p>That’s a big one, but there are also a few more smaller tweaks you can make to <code>git diff</code>:</p><pre><code>git config --global diff.colorMoved plain
git config --global diff.mnemonicPrefix true
git config --global diff.renames true
</code></pre><p>The <code>colorMoved</code> was also in the Spring Cleaning suggestion list, so it also should probably be a default change.</p><p>Here is an example of the previous code movement with the <code>colorMoved</code> turned on:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png" alt="" loading="lazy" width="1254" height="778" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-16.14.15@2x.png 1254w" sizes="(min-width: 720px) 720px"></figure><p>You can see actually the difference between the moved code and the added line. With <code>colorMoved</code> it will show code movement in different colors then added and removed lines.</p><p>The <code>diff.renames</code> option will detect if a file has been renamed, which is generally good (if slightly more expensive) and <code>diff.mnemonicPrefix</code> will replace the <code>a/</code> and <code>b/</code> in your diff header output with where the diff is coming from, so <code>i/</code> (index), <code>w/</code> (working directory) or <code>c/</code> commit. </p><p>So if I diff a change in my index to my working directory I get this as my diff header instead:</p><pre><code>❯ git diff
diff --git i/apps/web/page.js w/apps/web/page.js
index 7568be2ef..b9e9a00d7 100644
--- i/apps/web/page.js
+++ w/apps/web/page.js
</code></pre><p>A little difficult to see in this example perhaps, but you can tell which side is from the index and which is from the working directory by the leading path names. It’s really subtle, but I like it.</p><h2 id="better-pushing">Better pushing</h2><p>One of the things that has continued to confuse and frustrate me since the very early days of Git is setting up tracking branches properly. When I push, where does it push, or does it push at all?</p><p>There are three updated push settings that I think make for a much nicer default experience. The first (<code>push.default simple</code>) has been the new default since Git 2.0, but the others still need to be set explicitly.</p><pre><code>git config --global push.default simple # (default since 2.0)
git config --global push.autoSetupRemote true
git config --global push.followTags true
</code></pre><p>This has always been a bit of a pain in Git. The new <code>simple</code> default is built more or less for centralized workflows and by default pushes the current branch to the same name on the remote. I think this is a pretty sensible default.</p><p>However, if that branch does not exist and there is no tracking branch setup, you’ll still get this error:</p><pre><code>$ git push
fatal: The current branch my-branch-name has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin my-branch-name
</code></pre><p>I have to imagine that you have all seen this roughly <em>one million</em> times.</p><p>If you set <code>push.autoSetupRemote</code> to true, then you won’t get this error anymore. If the upstream is not set, it will automatically set it. I cannot tell you how much I love this setting.</p><p>Finally, the <code>push.followTags</code> setting will push all tags that you have locally that aren’t on the server, every time you push anything. I’ve been bitten by this a few times - if you ever create tags locally, set this up so you don’t have to worry about other people not seeing them.</p><h2 id="better-fetching">Better fetching</h2><p>It can be argued that it’s nice to keep some historical local copies of branches and tags that used to be on the server but are not any longer, but I don’t really buy that.</p><p>Personally, I think the default behavior of Git should be to make your remote references as close to what is on the remote as possible. Prune stuff that’s gone, etc. </p><p>So, I think these fetch settings should be the default:</p><pre><code>git config --global fetch.prune true
git config --global fetch.pruneTags true
git config --global fetch.all true
</code></pre><p>Really all that this does is make sure we delete <code>origin/blah</code> if <code>blah</code> is deleted on the server, and also do it automatically for all the remotes that we have configured. Seems pretty reasonable to me.</p><h2 id="why-the-hell-not">Why the Hell Not?</h2><p>This next batch of settings are generally harmless and occasionally helpful.</p><p>I’m not sure I would necessarily change the defaults, but I also don’t think it would hurt anyone and in many cases would be more helpful, so I’m including them in my list.</p><h2 id="autocorrect-prompting">Autocorrect prompting</h2><p>As I explained in great length in my <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">previous post</a>, there is a rather nice feature in Git where if your fingers trip up while typing a command, it will guess what you meant and try to run it.</p><p>The default is to not do this at all. What I rather prefer is to guess and prompt you.</p><pre><code>git config --global help.autocorrect prompt
</code></pre><p>If you want to read about this setting, it’s reasoning and it’s history ad nauseam, I have <a href="https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/">just the post for you</a>.</p><h2 id="commit-with-diffs">Commit with diffs</h2><p>This was also one of the suggestions in the Spring Cleaning list, I think mostly because it just adds more information to the context you can reference when you write your commit message in your editor.</p><p>By default, a <code>git commit</code> will give you a message that looks something like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png" alt="" loading="lazy" width="1494" height="462" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.03.26@2x.png 1494w" sizes="(min-width: 720px) 720px"></figure><p>Where there is just a list of files that were changed. If you set <code>commit.verbose</code> to be true, it will put the whole <code>diff</code> output in there for you to reference as you write your message.</p><pre><code>git config --global commit.verbose true
</code></pre><p>Here’s what it looks like now when you go to commit:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png" alt="" loading="lazy" width="1488" height="1276" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-20-at-17.01.54@2x.png 1488w" sizes="(min-width: 720px) 720px"></figure><p>All of this will be removed from the commit message (everything under the hilarious <code>-- &gt;8 --</code> "scissors" line), but it can give you much more context in writing your message.</p><h2 id="reuse-recorded-resolutions">Reuse recorded resolutions</h2><p>This setting is only useful if you’re doing rebases with conflicts over and over again. It’s not the most common situation, but there is not really an issue if it’s turned on and never used.</p><pre><code>git config --global rerere.enabled true
git config --global rerere.autoupdate true</code></pre><p>The <code>enabled</code> option will make sure it records the before and after states of rebase conflicts and the <code>autoupdate</code> will automatically re-apply the resolutions if it sees them again. I wrote about this at some length <a href="https://blog.gitbutler.com/git-tips-1-theres-a-git-config-for-that/#reuse-recorded-resolution">over here</a>, so I won’t bore you with the recap any further.</p><h2 id="global-ignore-file">Global ignore file</h2><p>This is pretty dumb, but as there is a <code>~/.gitconfig</code> file with global values, it would be cool if there were a <code>~/.gitignore</code> file with global values. This setting accomplishes that:</p><pre><code>git config --global core.excludesfile ~/.gitignore
</code></pre><p>In reality, this is sort of unnecessary, since Git will already look for global ignore values in the following two places: <code>~/git/ignore</code> and <code>~/.config/git/ignore</code> but since those are a little obscure, I feel like it’s nice to have this more guessable path.</p><h2 id="slightly-nicer-rebase">Slightly nicer rebase</h2><p>This section mostly has to do with the use case where you're fixing up and squashing your commits. If you don't know what that is, please check out our previous blog post on <a href="https://blog.gitbutler.com/git-autosquash/" rel="noreferrer">autosquashing</a>.</p><p>However, if you are squashing and rebasing a lot (or even occasionally), these settings could help and certainly won't hurt things.</p><pre><code>git config --global rebase.autoSquash true
git config --global rebase.autoStash true
git config --global rebase.updateRefs true</code></pre><p>The <code>updateRefs</code> setting should almost certainly be a default, honestly. It just takes stacked refs in a branch and makes sure they're also moved when a branch is rebased.</p><p>If you want to learn a tiny bit more about how to use fixup, autosquash and updateRefs, it's probably easiest to watch a few minutes of a talk where I go over it here:</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/Md44rcw13k4?start=810&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="So You Think You Know Git Part 2 - DevWorld 2024"></iframe></figure><h2 id="a-matter-of-taste">A Matter of Taste</h2><p>The next group is based on your personal taste, but most people don’t know they exist and a lot of people may find them useful. They are commented out in my TLDR settings.</p><h2 id="better-merge-conflicts">Better merge conflicts</h2><p>So, while this is brought up in the Spring Cleaning thread as something that might want to be the new default, I'm not sure that all of you would agree.</p><p>When you have a merge conflict in Git, instead of inserting the conflict markers from left and right, you can ask it to insert what the base of it looked like too. Sometimes this can be really useful, but some people can find it pretty annoying.</p><pre><code>git config --global merge.conflictstyle zdiff3</code></pre><p>There have been discussions on the Git mailing list to make this the default and actually GitButler uses the <code>diff3</code> strategy when dealing with merge conflict markers and to be totally honest, not all of us love it.</p><p>Here is an example of a simple merge conflict marker you might get in a file when doing a merge or rebase:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png" alt="" loading="lazy" width="1326" height="480" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.47.34@2x.png 1326w" sizes="(min-width: 720px) 720px"></figure><p>With the <code>merge.conflictStyle zdiff3</code> setting, it would look like this:</p><figure><img src="https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png" alt="" loading="lazy" width="1320" height="558" srcset="https://blog.gitbutler.com/content/images/size/w600/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 600w, https://blog.gitbutler.com/content/images/size/w1000/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1000w, https://blog.gitbutler.com/content/images/2025/02/CleanShot-2025-02-21-at-11.48.36@2x.png 1320w" sizes="(min-width: 720px) 720px"></figure><p>Essentially, in addition to the <code>&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;</code> sections that show you how you changed the block and how the other person changed it, it adds a <code>|||||||</code> block that shows you what the block looked like before either of you changed it.</p><p>That extra context (what that section looked like before either side modified it) can sometimes be super useful, but often it's just more data and somewhat confusing.</p><p>Really, it's up to you if you prefer more data there.</p><div><p>⚠️</p><p>Git has nearly always had <code spellcheck="false">diff3</code> as a strategy. I'm recommending <code spellcheck="false">zdiff3</code> here, which stands for "<i><em>zealous diff3</em></i>" and is slightly better, but only available since Git 2.35 (Jan 2022). If you have an older Git version, just remove the "z".</p></div><h2 id="better-pulling">Better pulling</h2><p>The merge versus rebase debate is of course one that may never be agreed upon, but most of us have a preference. However, you may not know that you can set the <code>git pull</code> default so that it will only do one or the other. No need for <code>git pull --rebase</code>, you can make it the default:</p><pre><code>git config --global pull.rebase true
</code></pre><p>This is a personal decision, but as I’ve migrated to the rebase only camp recently, it is in fact in my config.</p><h2 id="run-the-fsmonitor-processes">Run the fsmonitor processes</h2><p>Again, this is really only a thing for larger repositories, and maybe you don’t want filesystem monitors running all over the place, but it can make things like <code>git status</code> much faster if you have big working directories.</p><p>Maybe it shouldn’t be a default, but it’s not very bad and can make a big difference. Maybe <code>git clone</code> should ask you if you want to set it or not. Whatever, it’s an option for you.</p><pre><code>git config --global core.fsmonitor true
git config --global core.untrackedCache true</code></pre><p>This will run a filesystem monitor (per repository) that notices file changes and updates a cache so that <code>git status</code> doesn’t have to crawl every file and see if anything changed via a thousand <code>mtime</code> stat calls, it can just look at a simple log of file changes.</p><div><p>⚠️</p><p>Be aware that this will run a single process <i><em>per repository</em></i> that you are active in, which can be a lot. They mostly don't do much as they're event based, so it shouldn't affect memory or CPU noticeably, even with hundreds of them, but it's something to keep in mind. You can also leave out the <code spellcheck="false">--global</code> and just enable it for your larger repos.</p></div><h2 id="final-thoughts">Final thoughts</h2><p>Hopefully this has been a useful reference and maybe you learned some new Git config things, some of which should almost certainly already be the defaults, which isn’t even a controversial option in the Git mailing list community.</p><p>There are lots of other ways to pimp your Git ride (aliases, cool external <a href="https://github.com/dandavison/delta?ref=blog.gitbutler.com">pager</a> and <a href="https://github.com/so-fancy/diff-so-fancy?ref=blog.gitbutler.com">diff</a> tools, things like that) but I thought it would be best to just stick to globally useful and relatively simple vanilla Git settings.</p><p>Hope you enjoyed this and see you next time!</p>
			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What would happen if we didn't use TCP or UDP? (186 pts)]]></title>
            <link>https://github.com/Hawzen/hdp</link>
            <guid>43169103</guid>
            <pubDate>Tue, 25 Feb 2025 07:13:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Hawzen/hdp">https://github.com/Hawzen/hdp</a>, See on <a href="https://news.ycombinator.com/item?id=43169103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">What would happen if we didn't use TCP or UDP?</h2><a id="user-content-what-would-happen-if-we-didnt-use-tcp-or-udp" aria-label="Permalink: What would happen if we didn't use TCP or UDP?" href="#what-would-happen-if-we-didnt-use-tcp-or-udp"></a></p>
<p dir="auto">Switches, bridges, routers, load balancers, firewalls—these network boxes keep the internet running. Routing, blocking, mirroring, duplicating and deduplicating traffic in ways most people never think about. Without them, this document wouldn’t have reached you</p>
<p dir="auto">But the network is just one layer. The OS has its own way of handling packets—classifying, queuing, enforcing firewall rules, translating addresses, deciding what gets through and what gets dropped without a trace. Every part plays by its own rules, shaping what’s “allowed” and what's not</p>
<p dir="auto">At some point, I wondered—<em>what if I sent a packet using a transport protocol that didn’t exist?</em> Not TCP, not UDP, not even ICMP—something completely made up. Would the OS let it through? Would it get stopped before it even left my machine? Would routers ignore it, or would some middlebox kill it on sight? Could it actually move faster by slipping past common firewall rules?</p>
<p dir="auto">No idea.</p>
<p dir="auto">So I had to try.</p>
<p dir="auto">First, I sent the packets to myself, just to see how my own machine handled the poison I made up. Then, I sent them across continents to a remote Linux machine to see if they’d actually make it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Some background first</h2><a id="user-content-some-background-first" aria-label="Permalink: Some background first" href="#some-background-first"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Feel free to skip this section if you already know how the internet works. Otherwise, continue reading on</p>
</div>
<p dir="auto">But wait—what exactly is a transport layer protocol?</p>
<p dir="auto">The internet isn’t magic. It just looks that way. Underneath, it’s a stack of protocols, each one shoving data to the next until it reaches its destination. At the application level, you send a request—loading a website, streaming a video, or whatever you do. That request gets wrapped by the OS in multiple layers of metadata, addresses, and headers, until it’s nothing but raw bits flying through the network</p>
<p dir="auto">It kinda works like this:</p>
<p dir="auto">  <a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/internet_protocols.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/internet_protocols.png" alt="a visual guide to how the internet works, it kinda sucks but that why i like it."></a> </p>
<p dir="auto"><sub>The diagram is 100% correct and should be included in all networking textbooks.</sub></p>
<p dir="auto">At the top, apps—browsers, games, whatever—generate requests (Load this page, Send this message, Connect to this game server). Then the requests start their descent through the network stack, getting wrapped, encoded, and addressed at each layer, until all that’s left is a stream of bits flying into the void</p>
<p dir="auto">Each layer plays a role. IP assigns addresses and makes sure packets know where they’re going. The link layer handles the actual transmission—Wi-Fi, Ethernet, fiber optics, whatever. There’s more to it, but we’re not going down that rabbit hole right now. What matters is the layer that makes network communication actually usable</p>
<p dir="auto">The <strong>transport layer</strong> is where networking personally starts to get interesting. It’s the first truly complex protocol layer. It doesn’t just move packets—it manages connections, makes sure multiple applications can share the same machine, and decides how data should flow.</p>
<p dir="auto">This is where <strong>TCP</strong>, <strong>UDP</strong>, and their weird cousins live. The <strong>IP Protocol</strong> defines a field called <code>Protocol</code>. Setting this field to 6 means the encapsulated packet is TCP, 17 is UDP, and <a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">there are others defined</a> but some numbers are deliberately left out for future use</p>
<p dir="auto">But what if we used those <em>unused</em> numbers?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #1: Sending traffic.. to me!</h2><a id="user-content-experiment-1-sending-traffic-to-me" aria-label="Permalink: Experiment #1: Sending traffic.. to me!" href="#experiment-1-sending-traffic-to-me"></a></p>
<p dir="auto">There are simply too many variables to this experiment. My OS, my router, the receiver's OS, and god knows how many middle boxes are littered on the open internet. It's hard to extrapolate conclusions from experimentation with all these moving parts—so I thought of the following: To begin, I'll send the packets to <em>my own machine</em>, this guarantees that any results are solely due to my OS's behaviour</p>
<p dir="auto">First, I designed a <a href="https://github.com/Hawzen/hdp/blob/master/hdp_specification.md">simple protocol</a>: <strong>HDP</strong>. The specifics don’t matter—what matters is that it doesn’t resemble any known protocol. It’s an outsider, something the OS and network stack weren’t expecting</p>
<p dir="auto">Next, I built a <a href="https://github.com/Hawzen/hdp/blob/master/src/server/main.rs">server, or a listener</a>, whatever you call it. The machine running this code will be patiently waiting for any packets. Then I wrote a <a href="https://github.com/Hawzen/hdp/blob/master/src/client/main.rs">client</a>, the machine running this code will send HDP packets to the server</p>
<p dir="auto">Finally, here are the steps I'll attempt</p>
<ol dir="auto">
<li>Startup an HDP server
<ul dir="auto">
<li>Which will ask the OS to forward any packets with the protocol 255 to a socket it controls</li>
</ul>
</li>
<li>Run the HDP client, sending packets to my local machine
<ul dir="auto">
<li>The client will ask the OS to nicely deliver the packets to 127.0.01
<ul dir="auto">
<li>The OS is configured to hand packets with that target address to the loopback <a href="https://en.wikipedia.org/wiki/Network_interface_controller" rel="nofollow">network interface</a>
<ul dir="auto">
<li>The loopback interface should realize: "uhhh.. this packet should go right back in?", and send it back to my own machine</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>The OS delivers them to the HDP server unmodified..?? 🤞</li>
</ol>
<p dir="auto">Let's do it</p>
<p dir="auto">I opened two shells—one was the server:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span></pre></div>
<p dir="auto">And in another shell I opened the client</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ fortune | cowsay | sudo cargo run --bin client 127.0.0.1"><pre><span><span>$</span></span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1</span></pre></div>
<p dir="auto">Alright, let's send the packet via the client. 3, 2, 1, and..</p>
<p dir="auto">The server got the message!</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ sudo cargo run --bin server
~~~ IP Header ~~~
Version: 4
IHL: 5
DSCP: 0
ECN: 0
Total Length: 58625
Identification: 36455
Flags: 0
Fragment Offset: 0
TTL: 64
Protocol: 255
Header Checksum: 0
Source IP: [127, 0, 0, 1]
Destination IP: [127, 0, 0, 1]


~~~ HDP Header &amp; Data ~~~
Source Port: 420
Destination Port: 420
Timestamp: 1739640243546134000
Data:  _________________________________________
/ Marriage is not merely sharing the      \
| fettucine, but sharing the burden of    |
| finding the fettucine restaurant in the |
| first place.                            |
|                                         |
\ -- Calvin Trillin                       /
 -----------------------------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||"><pre><span><span>$</span></span> sudo cargo run <span><span>--</span>bin server</span>
<span>~~~</span> <span>IP</span> <span>Header</span> <span>~~~</span>
<span>Version</span><span>:</span> <span>4</span>
<span>IHL</span><span>:</span> <span>5</span>
<span>DSCP</span><span>:</span> <span>0</span>
<span>ECN</span><span>:</span> <span>0</span>
<span>Total</span> <span>Length</span><span>:</span> <span>58625</span>
<span>Identification</span><span>:</span> <span>36455</span>
<span>Flags</span><span>:</span> <span>0</span>
<span>Fragment</span> <span>Offset</span><span>:</span> <span>0</span>
<span>TTL</span><span>:</span> <span>64</span>
<span>Protocol</span><span>:</span> <span>255</span>
<span>Header</span> <span>Checksum</span><span>:</span> <span>0</span>
<span>Source</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]
<span>Destination</span> <span>IP</span><span>:</span> [<span>127</span>, <span>0</span>, <span>0</span>, <span>1</span>]


<span>~~~</span> <span>HDP</span> <span>Header</span> <span>&amp;</span> <span>Data</span> <span>~~~</span>
<span>Source</span> <span>Port</span><span>:</span> <span>420</span>
<span>Destination</span> <span>Port</span><span>:</span> <span>420</span>
<span>Timestamp</span><span>:</span> <span>1739640243546134000</span>
<span>Data</span><span>:</span>  _________________________________________
<span><span>/</span></span> <span>Marriage</span> is <span>not</span> merely sharing the      \
<span>|</span> fettucine, but sharing the burden <span>of</span>    <span>|</span>
<span>|</span> finding the fettucine restaurant <span>in</span> the <span>|</span>
<span>|</span> first place<span>.</span>                            <span>|</span>
<span>|</span>                                         <span>|</span>
<span>\</span> <span><span>--</span> Calvin Trillin                       /</span>
 <span><span>-----------------------------------------</span></span>
        <span>\</span>   <span>^</span>__<span>^</span>
         <span>\</span>  (oo)<span>\</span>_______
            (__)<span>\</span>       )<span>\/\</span>
                <span>||----</span>w <span>|</span>
                <span>||</span>     <span>||</span></pre></div>
<p dir="auto">Success! The OS accepted my protocol, looped it back, and delivered it to the server with no shenanigans happening, unexpected!. But before calling it a day, I had another question:</p>
<p dir="auto">What would happen if we repeated this experiment, whilst changing the protocol number defined in the IP packet?</p>
<p dir="auto">My initial choice of <strong>255</strong> was arbitrary—it was an unused protocol number. But what if I tried something more… unconventional? I decided to test different protocol numbers, including:</p>
<ul dir="auto">
<li>6, the number assigned to <strong>TCP</strong> packets</li>
<li>Or 2, which is the protocol number used for <strong>ICMP</strong> (i.e., the thing powering <code>ping</code>)</li>
<li>Or even 256, an index beyond the defined boundaries of the IP Protocol
Would they make it? Would the OS freak out?</li>
</ul>
<p dir="auto">Let's see:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fortune | cowsay | sudo cargo run --bin client 127.0.0.1 # This time looping over protocol numbers"><pre>fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 127.0.0.1 # This time looping over protocol numbers</span></pre></div>
<details>
<summary><p dir="auto"><h2 tabindex="-1" dir="auto">Results</h2><a id="user-content-results" aria-label="Permalink: Results" href="#results"></a></p></summary>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Protocol Number</th>
<th>Source IP (Server)</th>
<th>Byte Sum (Server)</th>
<th>Received (Server)</th>
<th>Succeeded (Client)</th>
<th>Byte sum (Client)</th>
<th>Failure reason (Client)</th>
<th>Time difference (μs)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>1</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>2</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>3</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>4</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>5</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>6</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>7</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>8</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>9</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>10</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>11</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>12</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>13</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>63</td>
</tr>
<tr>
<td>14</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>15</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>16</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>17</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>18</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>42</td>
</tr>
<tr>
<td>19</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>82</td>
</tr>
<tr>
<td>20</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>21</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>22</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>23</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>24</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>25</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>26</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>27</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>28</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>29</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>30</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>31</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>32</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>33</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>34</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>35</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>36</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>37</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>38</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>39</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>40</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>41</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>56</td>
</tr>
<tr>
<td>42</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>43</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>44</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>58</td>
</tr>
<tr>
<td>45</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>46</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>47</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>48</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>49</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>50</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>51</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Operation not supported on socket (os error 102)</td>
<td>nan</td>
</tr>
<tr>
<td>52</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>53</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>54</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>55</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>56</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>57</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>58</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>69</td>
</tr>
<tr>
<td>59</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>60</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>61</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>62</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>63</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>64</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>65</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>66</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>124</td>
</tr>
<tr>
<td>67</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>68</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>69</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>87</td>
</tr>
<tr>
<td>70</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>71</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>72</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>73</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>74</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>75</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>115</td>
</tr>
<tr>
<td>76</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>77</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>78</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>79</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>80</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>150</td>
</tr>
<tr>
<td>81</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>82</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>83</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>84</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>85</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>86</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>87</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>70</td>
</tr>
<tr>
<td>88</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>89</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>90</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>91</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>92</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>61</td>
</tr>
<tr>
<td>93</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>94</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>95</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>96</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>97</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>98</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>66</td>
</tr>
<tr>
<td>99</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>100</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>101</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>102</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>148</td>
</tr>
<tr>
<td>103</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>104</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>119</td>
</tr>
<tr>
<td>105</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>106</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>107</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>108</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>52</td>
</tr>
<tr>
<td>109</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>44</td>
</tr>
<tr>
<td>110</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>59</td>
</tr>
<tr>
<td>111</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>51</td>
</tr>
<tr>
<td>112</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>45</td>
</tr>
<tr>
<td>113</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>114</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>115</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>116</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>117</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>118</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>24</td>
</tr>
<tr>
<td>119</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>120</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>121</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>48</td>
</tr>
<tr>
<td>122</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>123</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>124</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>125</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>126</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>54</td>
</tr>
<tr>
<td>127</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>46</td>
</tr>
<tr>
<td>128</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>129</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>130</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>131</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>49</td>
</tr>
<tr>
<td>132</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>62</td>
</tr>
<tr>
<td>133</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>43</td>
</tr>
<tr>
<td>134</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>47</td>
</tr>
<tr>
<td>135</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>136</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>112</td>
</tr>
<tr>
<td>137</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>138</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>53</td>
</tr>
<tr>
<td>139</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>140</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>141</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>64</td>
</tr>
<tr>
<td>142</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>143</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>144</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>145</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>146</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>147</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>148</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>149</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>72</td>
</tr>
<tr>
<td>150</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>151</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>77</td>
</tr>
<tr>
<td>152</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>153</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>154</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>155</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>156</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>157</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>110</td>
</tr>
<tr>
<td>158</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>159</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>83</td>
</tr>
<tr>
<td>160</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>161</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>162</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>111</td>
</tr>
<tr>
<td>163</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>103</td>
</tr>
<tr>
<td>164</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>165</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>166</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>167</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>168</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>57</td>
</tr>
<tr>
<td>169</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>50</td>
</tr>
<tr>
<td>170</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>65</td>
</tr>
<tr>
<td>171</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>172</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>80</td>
</tr>
<tr>
<td>173</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>174</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>67</td>
</tr>
<tr>
<td>175</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>55</td>
</tr>
<tr>
<td>176</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>60</td>
</tr>
<tr>
<td>177</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>85</td>
</tr>
<tr>
<td>178</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>179</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>73</td>
</tr>
<tr>
<td>180</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>181</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>182</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>183</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>184</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>185</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>186</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>74</td>
</tr>
<tr>
<td>187</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>188</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>189</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>75</td>
</tr>
<tr>
<td>190</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>81</td>
</tr>
<tr>
<td>191</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>192</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>193</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>194</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>195</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>196</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>197</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>198</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>199</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>200</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>201</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>202</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>203</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>204</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>205</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>206</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>207</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>208</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>209</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>210</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>211</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>212</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>78</td>
</tr>
<tr>
<td>213</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>214</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>215</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>216</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>217</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>105</td>
</tr>
<tr>
<td>218</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>219</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>220</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>221</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>222</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>108</td>
</tr>
<tr>
<td>223</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>224</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>104</td>
</tr>
<tr>
<td>225</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>109</td>
</tr>
<tr>
<td>226</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>227</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>228</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>229</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>230</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>84</td>
</tr>
<tr>
<td>231</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>79</td>
</tr>
<tr>
<td>232</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>102</td>
</tr>
<tr>
<td>233</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>101</td>
</tr>
<tr>
<td>234</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>113</td>
</tr>
<tr>
<td>235</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>95</td>
</tr>
<tr>
<td>236</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>100</td>
</tr>
<tr>
<td>237</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>238</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>106</td>
</tr>
<tr>
<td>239</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>92</td>
</tr>
<tr>
<td>240</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>97</td>
</tr>
<tr>
<td>241</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>89</td>
</tr>
<tr>
<td>242</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>99</td>
</tr>
<tr>
<td>243</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>244</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>98</td>
</tr>
<tr>
<td>245</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>93</td>
</tr>
<tr>
<td>246</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>247</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>91</td>
</tr>
<tr>
<td>248</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>249</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>250</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>90</td>
</tr>
<tr>
<td>251</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>88</td>
</tr>
<tr>
<td>252</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>94</td>
</tr>
<tr>
<td>253</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>96</td>
</tr>
<tr>
<td>254</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>76</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>nan</td>
</tr>
<tr>
<td>255</td>
<td>127.0.0.1</td>
<td>373</td>
<td>🫡</td>
<td>🫡</td>
<td>373</td>
<td>-</td>
<td>71</td>
</tr>
<tr>
<td>256</td>
<td>nan</td>
<td>nan</td>
<td>🤯</td>
<td>🤯</td>
<td>-</td>
<td>Invalid argument (os error 22)</td>
<td>nan</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">What’s up with these failures?</h2><a id="user-content-whats-up-with-these-failures" aria-label="Permalink: What’s up with these failures?" href="#whats-up-with-these-failures"></a></p>
<p dir="auto">Most protocol numbers worked fine—the OS saw the packet, looped it back, and my server received it without an issue. But a few of them outright&nbsp;<em>failed</em>&nbsp;at different points in the stack</p>
<ul dir="auto">
<li><strong>Protocols 1, 2, and 6 failed at the server side</strong>.&nbsp;Meaning: the client successfully sent them, but the server never saw them</li>
<li><strong>Protocols 50 and 51 failed at the client side</strong>.&nbsp;The OS refused to even send them</li>
<li><strong>Protocol 256 didn't even make it past the&nbsp;<code>socket()</code>&nbsp;call</strong></li>
</ul>
<p dir="auto">But&nbsp;<em>why?</em>&nbsp;What’s making the OS treat these packets differently?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Syscalls: What actually matters</h2><a id="user-content-syscalls-what-actually-matters" aria-label="Permalink: Syscalls: What actually matters" href="#syscalls-what-actually-matters"></a></p>
<p dir="auto">One of the most useful debugging techniques I learnt debugging this stuff is, when dealing with low-level code, trace the <em>system calls</em> a process is making</p>
<p dir="auto">A <a href="https://en.wikipedia.org/wiki/System_call" rel="nofollow">system call</a> for the uninitiated is just a function that allows applications to request privileged resources from the OS—whether that’s opening a file, allocating memory, or, in our case,&nbsp;sending a packet over the network</p>
<p dir="auto">In my Rust code I use a library called <a href="https://docs.rs/socket2/latest/socket2/index.html" rel="nofollow"><code>socket2</code></a> which implements a pretty wrapper over the system calls provided by my OS. And to send a packet, I request a socket—which you can think of as just a special file my code can write in to communicate over the network</p>
<p dir="auto">Here's what the client would do:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols
    SOCK_RAW,   // Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.
    255         // Protocol: We looped over this field.
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Domain: ARPA Internet protocols. This tells the OS that we're interested in the IP protocols</span>
    <span>SOCK_RAW</span>,   <span>// Type: Raw socket. The OS normally handles the transport layer, but this gives us full control.</span>
    <span>255</span>         <span>// Protocol: We looped over this field.</span>
);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Revisiting the failures</h2><a id="user-content-revisiting-the-failures" aria-label="Permalink: Revisiting the failures" href="#revisiting-the-failures"></a></p>
<p dir="auto"><strong>1, 2, and 6: The Server Never Sees Them</strong><br>
These packets were successfully transmitted from the client, but they were intercepted before my server had a chance to look at them. That suggests something inside the OS intercepted them</p>
<p dir="auto">Originally, I assumed my server would capture any raw IP packet it received. The socket looked like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="int sockfd = socket(
    AF_INET,    // Internet domain
    SOCK_RAW,   // Raw socket: should give us full control
    0           // Let the OS decide the protocol
);"><pre><span>int</span> <span>sockfd</span> <span>=</span> <span>socket</span>(
    <span>AF_INET</span>,    <span>// Internet domain</span>
    <span>SOCK_RAW</span>,   <span>// Raw socket: should give us full control</span>
    <span>0</span>           <span>// Let the OS decide the protocol</span>
);</pre></div>
<p dir="auto">I expected 0 to mean:
<em>"Give me everything—TCP, UDP, whatever it is, forward it"</em></p>
<p dir="auto">For context, I ran these experiments on my Mac, which runs Darwin. Looking at the <a href="https://developer.apple.com/library/archive/documentation/System/Conceptual/ManPages_iPhoneOS/man2/socket.2.html" rel="nofollow">documentation</a>, there is really nothing mentioning the Protocol Number = 0 trick</p>
<p dir="auto">Under the hood, Darwin is just like BSD but with a ton of makeup, meaning it inherits BSD’s socket behaviour and network stack quirks. And on a whim I checked the <strong><a href="https://man.openbsd.org/socket.2" rel="nofollow">BSD socket documentation</a></strong>, and I found this frustratingly vague line:</p>
<blockquote>
<p dir="auto">"A value of 0 for <code>protocol</code> will let the system select an appropriate protocol for the requested socket type."</p>
</blockquote>
<p dir="auto">So instead of delivering <strong>all</strong> raw packets, my OS was silently (and haphazardly) filtering them. My server never even saw the ICMP (1), IGMP (2), or TCP (6) packets—because Darwin likely deemed my socket not appropriate to receive those protocols.. or something?</p>
<p dir="auto"><strong>50 and 51: The Client Can’t Even Send Them</strong><br>
Here, the OS flat-out refused to send the packets. These aren’t just arbitrary numbers—they’re part of <strong>IPSec (ESP and AH)</strong>, which is used for encrypted VPN traffic. I'm not sure <em>why</em> the OS blocked them, but I imagine it's a security feature of sorts in Darwin</p>
<p dir="auto"><strong>256: The <code>socket()</code> Call Fails Immediately</strong><br>
This one is simple:</p>
<ul dir="auto">
<li>The IPv4 protocol field is 8 bits meaning valid values range from 0 to 255</li>
<li>256 is simply too large—the OS rejects it outright as an invalid argument</li>
</ul>
<p dir="auto">No surprises here. But what <em>was</em> surprising is what happened when I tried the same experiment on Linux..</p>
<p dir="auto">After seeing these inconsistencies, I was curious as to how Linux would behave. So I spun up a Linux VM and re-ran the experiment. Right away, the behaviour was very different</p>
<p dir="auto">Running the server I quickly noticed that Linux does not allow binding a raw socket to protocol <code>0</code>—Some invalid protocol numbers like 256 <em>worked</em>. For reference, I logged the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/results_no_server_linux_client_loopback.md"><code>results_no_server_linux_client_loopback</code></a>. I was satisfied that at least <em>some</em> of the protocol numbers were working as expected</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned" aria-label="Permalink: Lessons learned" href="#lessons-learned"></a></p>
<p dir="auto">Custom transport-layer protocols are doable, buuuuut the OS isn’t exactly welcoming. The networking stack has so many assumptions baked in, and raw sockets aren’t as raw as you’d expect</p>
<p dir="auto">I imagine this is why most new protocols live at the application layer instead. Instead of fighting the OS, engineers just build on top of existing transport protocols. QUIC, for example, runs over UDP and avoids these issues entirely</p>
<p dir="auto">And if you're ever working with raw sockets, <em>please</em> test across multiple OSes. If Darwin lets you do something, Linux might shut it down. If Linux is fine with it, Windows might pretend it doesn’t exist. There’s really no universal behaviour, even if they claim to <em>implement the POSIX standard</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Next step: What happens outside loopback?</h2><a id="user-content-next-step-what-happens-outside-loopback" aria-label="Permalink: Next step: What happens outside loopback?" href="#next-step-what-happens-outside-loopback"></a></p>
<p dir="auto">So far, these packets never left my machine. Now, I want to send HDP over the public internet:</p>
<ul dir="auto">
<li>Will routers forward it, or will they drop it?</li>
<li>Will firewalls let it through, or flag it as an attack?</li>
<li>Will it have different latency compared to TCP?</li>
<li>Will I accidentally brick DigitalOcean’s network? :D
Time to find out</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Experiment #2:</h2><a id="user-content-experiment-2" aria-label="Permalink: Experiment #2:" href="#experiment-2"></a></p>
<p dir="auto">At first I expected this experiment to be straight-forward (spoilers: it was NOT). How could it not..?</p>
<p dir="auto">I planned to deploy my server on a machine using a cheap cloud provider like Digital Ocean—then I'd send all sorts of packets to it, TCP, UDP, my own protocol, you name it. Gathering statistics about packet drop, latency, whatever, then I'd make conclusions about the feasibility of not using TCP/UDP</p>
<p dir="auto">Simple!</p>
<p dir="auto">But oh it was not, not at all. It wasn't that the experiment was difficult to setup—but what weirded me out was the results.. they weren't anything I expected or was prepared to deal with. Keep reading to see why</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setting up the server</h2><a id="user-content-setting-up-the-server" aria-label="Permalink: Setting up the server" href="#setting-up-the-server"></a></p>
<p dir="auto">I rented the the cheapest VPS on Digital Ocean I could find, then set up my server and all the tooling I needed. Nice!</p>
<p dir="auto">Let's see where the server is..</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl myip.wtf
161.35.222.56
root@debian-s-1vcpu-512mb-10gb-fra1-01:~# curl ipinfo.io/161.35.222.56
{
  &quot;ip&quot;: &quot;161.35.222.56&quot;,
  &quot;city&quot;: &quot;Frankfurt am Main&quot;,
  &quot;region&quot;: &quot;Hesse&quot;,
  &quot;country&quot;: &quot;DE&quot;,
  &quot;loc&quot;: &quot;50.1155,8.6842&quot;,
  &quot;org&quot;: &quot;AS14061 DigitalOcean, LLC&quot;,
  &quot;postal&quot;: &quot;60306&quot;,
  &quot;timezone&quot;: &quot;Europe/Berlin&quot;,
  &quot;readme&quot;: &quot;https://ipinfo.io/missingauth&quot;
}"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl myip<span>.</span>wtf
<span>161.35</span><span>.</span><span>222.56</span>
root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~#</span> curl ipinfo<span>.</span>io<span>/</span><span>161.35</span><span>.</span><span>222.56</span>
{
  <span><span>"</span>ip<span>"</span></span><span>:</span> <span><span>"</span>161.35.222.56<span>"</span></span>,
  <span><span>"</span>city<span>"</span></span><span>:</span> <span><span>"</span>Frankfurt am Main<span>"</span></span>,
  <span><span>"</span>region<span>"</span></span><span>:</span> <span><span>"</span>Hesse<span>"</span></span>,
  <span><span>"</span>country<span>"</span></span><span>:</span> <span><span>"</span>DE<span>"</span></span>,
  <span><span>"</span>loc<span>"</span></span><span>:</span> <span><span>"</span>50.1155,8.6842<span>"</span></span>,
  <span><span>"</span>org<span>"</span></span><span>:</span> <span><span>"</span>AS14061 DigitalOcean, LLC<span>"</span></span>,
  <span><span>"</span>postal<span>"</span></span><span>:</span> <span><span>"</span>60306<span>"</span></span>,
  <span><span>"</span>timezone<span>"</span></span><span>:</span> <span><span>"</span>Europe/Berlin<span>"</span></span>,
  <span><span>"</span>readme<span>"</span></span><span>:</span> <span><span>"</span>https://ipinfo.io/missingauth<span>"</span></span>
}</pre></div>
<p dir="auto">Alright, looks like the experiment will span continents given that I'm running my client on Saudi Arabia, and the server is hosted in Frankfurt</p>
<p dir="auto">Before running any deep analysis, I wanted to check that there is a network path between my Mac and the server, so I <code>ping</code>'ed the server from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ ping 161.35.222.56
PING 161.35.222.56 (161.35.222.56): 56 data bytes
64 bytes from 161.35.222.56: icmp_seq=0 ttl=47 time=125.364 ms
64 bytes from 161.35.222.56: icmp_seq=1 ttl=47 time=128.061 ms
64 bytes from 161.35.222.56: icmp_seq=2 ttl=47 time=177.931 ms
64 bytes from 161.35.222.56: icmp_seq=3 ttl=47 time=225.798 ms
64 bytes from 161.35.222.56: icmp_seq=4 ttl=47 time=130.101 ms
64 bytes from 161.35.222.56: icmp_seq=5 ttl=47 time=194.563 ms
64 bytes from 161.35.222.56: icmp_seq=6 ttl=47 time=159.518 ms
64 bytes from 161.35.222.56: icmp_seq=7 ttl=47 time=134.343 ms
64 bytes from 161.35.222.56: icmp_seq=8 ttl=47 time=501.139 ms
64 bytes from 161.35.222.56: icmp_seq=9 ttl=47 time=153.672 ms
64 bytes from 161.35.222.56: icmp_seq=10 ttl=47 time=137.927 ms
64 bytes from 161.35.222.56: icmp_seq=11 ttl=47 time=355.672 ms
64 bytes from 161.35.222.56: icmp_seq=12 ttl=47 time=138.777 ms
64 bytes from 161.35.222.56: icmp_seq=13 ttl=47 time=166.116 ms
64 bytes from 161.35.222.56: icmp_seq=14 ttl=47 time=288.758 ms
64 bytes from 161.35.222.56: icmp_seq=15 ttl=47 time=151.458 ms
64 bytes from 161.35.222.56: icmp_seq=16 ttl=47 time=164.025 ms
64 bytes from 161.35.222.56: icmp_seq=17 ttl=47 time=170.132 ms
64 bytes from 161.35.222.56: icmp_seq=18 ttl=47 time=279.034 ms
^C
--- 161.35.222.56 ping statistics ---
19 packets transmitted, 19 packets received, 0.0% packet loss"><pre><span>❯</span> ping <span>161.35</span><span>.</span><span>222.56</span>
<span>PING</span> <span>161.35</span><span>.</span><span>222.56</span> (<span>161.35</span><span>.</span><span>222.56</span>)<span>:</span> <span>56</span> <span>data</span> bytes
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>0</span> ttl<span>=</span><span>47</span> time<span>=</span><span>125.364</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>1</span> ttl<span>=</span><span>47</span> time<span>=</span><span>128.061</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>2</span> ttl<span>=</span><span>47</span> time<span>=</span><span>177.931</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>3</span> ttl<span>=</span><span>47</span> time<span>=</span><span>225.798</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>4</span> ttl<span>=</span><span>47</span> time<span>=</span><span>130.101</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>5</span> ttl<span>=</span><span>47</span> time<span>=</span><span>194.563</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>6</span> ttl<span>=</span><span>47</span> time<span>=</span><span>159.518</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>7</span> ttl<span>=</span><span>47</span> time<span>=</span><span>134.343</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>8</span> ttl<span>=</span><span>47</span> time<span>=</span><span>501.139</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>9</span> ttl<span>=</span><span>47</span> time<span>=</span><span>153.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>10</span> ttl<span>=</span><span>47</span> time<span>=</span><span>137.927</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>11</span> ttl<span>=</span><span>47</span> time<span>=</span><span>355.672</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>12</span> ttl<span>=</span><span>47</span> time<span>=</span><span>138.777</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>13</span> ttl<span>=</span><span>47</span> time<span>=</span><span>166.116</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>14</span> ttl<span>=</span><span>47</span> time<span>=</span><span>288.758</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>15</span> ttl<span>=</span><span>47</span> time<span>=</span><span>151.458</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>16</span> ttl<span>=</span><span>47</span> time<span>=</span><span>164.025</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>17</span> ttl<span>=</span><span>47</span> time<span>=</span><span>170.132</span> ms
<span>64</span> bytes from <span>161.35</span><span>.</span><span>222.56</span><span>:</span> icmp_seq<span>=</span><span>18</span> ttl<span>=</span><span>47</span> time<span>=</span><span>279.034</span> ms
<span><span>^</span></span><span>C</span>
<span><span>---</span> 161.35.222.56 ping statistics ---</span>
<span>19</span> packets transmitted, <span>19</span> packets received, <span>0.0</span><span>%</span> packet loss</pre></div>
<p dir="auto">It seems it's quite far, but looks fine to me, let's send some packets using our new protocol!</p>
<p dir="auto">First let's start the server in our Digital Ocean machine</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span></pre></div>
<p dir="auto">And now we can send a packet from my Mac</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ fortune | cowsay | sudo cargo run --bin client 161.35.222.56
| Protocol Number | Succeeded (Client) | Time (μs) (Client) | Byte sum (Client) | Failure reason (Client) |
| 255 | 🫡 | timestamp | 563 | - |"><pre><span>❯</span> fortune <span>|</span> cowsay <span>|</span> sudo cargo run <span><span>--</span>bin client 161.35.222.56</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Succeeded</span> (<span>Client</span>) <span>|</span> <span>Time</span> (μs) (<span>Client</span>) <span>|</span> <span>Byte</span> <span>sum</span> (<span>Client</span>) <span>|</span> <span>Failure</span> reason (<span>Client</span>) <span>|</span>
<span>|</span> <span>255</span> <span>|</span> 🫡 <span>|</span> timestamp <span>|</span> <span>563</span> <span>|</span> <span>-</span> <span>|</span></pre></div>
<p dir="auto">Packet sent. Let's check the server again</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp/hdp# sudo cargo run --bin server
Listening on protocol 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>/</span>hdp<span>#</span> sudo cargo run <span><span>--</span>bin server</span>
<span>Listening</span> on protocol <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">Excellent. It seems that all went well, or so I thought. In-fact, all went downhill starting here. I took a quick break then came back. Let's try sending the packet again..</p>
<div dir="auto" data-snippet-clipboard-copy-content="| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | my_ip | 563 |"><pre><span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> my_ip <span>|</span> <span>563</span> <span>|</span></pre></div>
<p dir="auto">It's stuck? I can't see the second packet</p>
<p dir="auto">I <code>Ctrl+C</code> and attempt doing it again. No results..? That can't be right, could it be a client side bug? Let's use <code>tcpdump</code> to see all outgoing packets from my device</p>
<div dir="auto" data-snippet-clipboard-copy-content="❯ sudo tcpdump -i any 'ip[9] == 255'
tcpdump: data link type PKTAP
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type PKTAP (Apple DLT_PKTAP), snapshot length 524288 bytes
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427
IP mac > 161.35.222.56:  reserved 427"><pre><span>❯</span> sudo tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>==</span> <span>255</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>PKTAP</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>PKTAP</span> (<span>Apple</span> <span>DLT_PKTAP</span>), snapshot <span>length</span> <span>524288</span> bytes
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span>
<span>IP</span> mac <span>&gt;</span> <span>161.35</span><span>.</span><span>222.56</span><span>:</span>  reserved <span>427</span></pre></div>
<p dir="auto">They're definitely leaving my Mac. What about doing the same thing on the receiving end?</p>
<div dir="auto" data-snippet-clipboard-copy-content="root@debian-s-1vcpu-512mb-10gb-fra1-01:~/hdp# tcpdump -i any 'ip[9] > 17'
tcpdump: data link type LINUX_SLL2
tcpdump: verbose output suppressed, use -v[v]... for full protocol decode
listening on any, link-type LINUX_SLL2 (Linux cooked v2), snapshot length 262144 bytes
"><pre>root<span>@</span>debian<span>-</span>s<span>-</span><span>1</span>vcpu<span>-</span><span>512</span>mb<span>-</span><span>10</span>gb<span>-</span>fra1<span>-</span><span>01</span><span>:~/</span>hdp<span>#</span> tcpdump <span>-</span>i <span>any</span> 'ip[<span>9</span>] <span>&gt;</span> <span>17</span>'
tcpdump<span>:</span> <span>data</span> link <span>type</span> <span>LINUX_SLL2</span>
tcpdump<span>:</span> verbose output suppressed, use <span>-</span>v[v]<span>...</span> for full protocol decode
listening on <span>any</span>, link<span>-</span><span>type</span> <span>LINUX_SLL2</span> (<span>Linux</span> cooked v2), snapshot <span>length</span> <span>262144</span> bytes
</pre></div>
<p dir="auto">Nothing appeared</p>
<p dir="auto">I began doubting my earlier results, there they are in my shell. The timestamps and byte sums match. Was I imagining them? Is Linus Torvalds himself gaslighting me??</p>
<p dir="auto">Wait..? How did my ISP's <a href="https://simple.wikipedia.org/wiki/Network_address_translation" rel="nofollow">NATing box</a> forward the packet? NAT'ing relies on ports—but my protocol is just black magic to them</p>
<p dir="auto">I'm confused</p>
<p dir="auto">Very confused</p>
<p dir="auto">After digging a bit in, I found that Digital Ocean doesn't support non-standard IP Protocols</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/ihatedigitalocean.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/ihatedigitalocean.png" alt="digital_ocean_sucks"></a></p>
<p dir="auto">This still doesn't explain it. How did one packet survive? There really is no way to know, and I was banging my head against the wall trying to figure it out</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">One. Last. Try</h3><a id="user-content-one-last-try" aria-label="Permalink: One. Last. Try" href="#one-last-try"></a></p>
<p dir="auto">if any cloud provider would support non-standard IP Protocols, it'd be AWS</p>
<p dir="auto">I provisioned two machines. Set them up. Server. Client. It works.. !</p>
<div dir="auto" data-snippet-clipboard-copy-content="admin@ip-172-31-13-218:~/hdp$ sudo cargo run --bin server 255
Server is listening on SockAddr { ss_family: 2, len: 16 }, protocol: 255
| Protocol Number | Time (μs) (Server) | Source IP (Server) | Byte Sum (Server) |
| --- | --- | --- |
| 255 | timestamp | 54.153.13.186 | 33 |
| 255 | timestamp | 54.153.13.186 | 34 |
| 255 | timestamp | 54.153.13.186 | 35 |
| 255 | timestamp | 54.153.13.186 | 36 |
"><pre>admin<span>@</span>ip<span>-</span><span>172</span><span>-</span><span>31</span><span>-</span><span>13</span><span>-</span><span>218</span><span>:~/</span>hdp<span>$</span> sudo cargo run <span><span>--</span>bin server 255</span>
<span>Server</span> is listening on <span>SockAddr</span> { ss_family<span>:</span> <span>2</span>, len<span>:</span> <span>16</span> }, protocol<span>:</span> <span>255</span>
<span>|</span> <span>Protocol</span> <span>Number</span> <span>|</span> <span>Time</span> (μs) (<span>Server</span>) <span>|</span> <span>Source</span> <span>IP</span> (<span>Server</span>) <span>|</span> <span>Byte</span> <span>Sum</span> (<span>Server</span>) <span>|</span>
<span>|</span> <span>---</span> <span>|</span><span> --- | --- |</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>33</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>34</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>35</span> <span>|</span>
<span>|</span> <span>255</span> <span>|</span> timestamp <span>|</span> <span>54.153</span><span>.</span><span>13.186</span> <span>|</span> <span>36</span> <span>|</span>
</pre></div>
<p dir="auto">Granted, the server was just two hops away from the client, and it didn't have to pass through the scary sea of the internet</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Hawzen/hdp/blob/master/readme_assets/latency_difference_between_hdp_and_udp.png"><img src="https://github.com/Hawzen/hdp/raw/master/readme_assets/latency_difference_between_hdp_and_udp.png" alt="Description"></a></p>
<p dir="auto"><sub>The latency is in the microseconds due to both machines being in the same datacenter.</sub></p>
<p dir="auto">The latency difference between the HDP &amp; UDP was a consistent, but negligible 20μs across various benchmarks</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">But what about the internet?</h4><a id="user-content-but-what-about-the-internet" aria-label="Permalink: But what about the internet?" href="#but-what-about-the-internet"></a></p>
<p dir="auto">I tried sending packets from my Mac to the AWS server, and I reproduced the same one packet behaviour above. I left a sample of the results in <a href="https://github.com/Hawzen/hdp/blob/master/samples/tcpdump_tokyo_sever_mac_client.md"><code>tcpdump_tokyo_server_mac_client.md</code></a>. I sent 1 packet for all protocols, and all of them stopped working after the first packet except TCP/UDP/ICMP</p>
<p dir="auto">And as expected, sending or recieving packets from the Digital Ocean machine to the AWS machine didn't work</p>
<p dir="auto">There's no way to know for sure.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Lessons learned</h2><a id="user-content-lessons-learned-1" aria-label="Permalink: Lessons learned" href="#lessons-learned-1"></a></p>
<p dir="auto">Technically <em>yes</em>, you could use your own IP protocol. But unless you're a masochist, I do not suggest it</p>
<ul dir="auto">
<li>Your code won't be portable, and you'll need to support various operating systems</li>
<li>Your protocol will be randomly dropped at NAT gateways &amp; firewalls. It might work on your own network, but I gaurentee it won't work on the internet</li>
<li>From my testing, there's no latency improvements from using a non-standard IP protocol</li>
</ul>
<p dir="auto">TL;DR: <em><strong>Use TCP or UDP</strong></em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li>The <a href="https://datatracker.ietf.org/doc/html/rfc768" rel="nofollow">UDP protocol specification</a> is so minimal it is almost funny</li>
<li><a href="https://datatracker.ietf.org/doc/html/rfc3692#section-2.1" rel="nofollow">IP Protocol numbers that are assigned for testing</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers" rel="nofollow">The list of protocols</a> supported under the IP protocol is pretty interesting</li>
<li><a href="https://hackaday.com/2024/09/21/when-raw-network-sockets-arent-raw-raw-sockets-in-macos-and-linux/" rel="nofollow">This</a> article speaks about some differences between raw sockets in Linux &amp; FreeBSD</li>
<li>How would you implement NAT on something other than TCP or UDP? <a href="https://superuser.com/a/1108226" rel="nofollow">This</a> answer is pretty insightful</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xcode Constantly Phones Home (135 pts)]]></title>
            <link>https://lapcatsoftware.com/articles/2025/2/5.html</link>
            <guid>43168589</guid>
            <pubDate>Tue, 25 Feb 2025 05:43:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lapcatsoftware.com/articles/2025/2/5.html">https://lapcatsoftware.com/articles/2025/2/5.html</a>, See on <a href="https://news.ycombinator.com/item?id=43168589">Hacker News</a></p>
<div id="readability-page-1" class="page">
<nav>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
<br><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a></nav>
<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>

<h3>February 24 2025</h3>

<p>Building <a href="https://underpassapp.com/StopTheMadness/">StopTheMadness Pro</a> in Xcode is usually very fast, because my project doesn't use any Swift. It's a combination of Objective-C, which compiles much more quickly than Swift, and JavaScript, which doesn't need to be compiled. However, sometimes the builds were very slow for some strange reason. Checking the Xcode build transcripts, I found that the delay was in the "Gather provisioning inputs" build phase.</p>
<p><img src="https://lapcatsoftware.com/articles/2025/2/5.png" width="425" height="145" alt="Xcode build messages"></p>
<p>This one phase took 50.6 seconds when the entire build was 56.8 seconds!</p>
<p>I tested with my internet disabled, and the slow builds did not occur. Obviously, though, it's impractical to disable my internet every time I want to build and run. After all, my project is a Safari extension! I do use <a href="https://www.obdev.at/products/littlesnitch/">Little Snitch</a>, but I had previously allowed all connections from Xcode to <code>apple.com</code>, because that's required to upload builds to App Store Connect. When I scrutinized the individual Xcode connections with Little Snitch, I saw that <code>developerservices2.apple.com</code> was responsible for the slow "Gathering provisioning inputs" build phase. When I denied those connections with Little Snitch, my builds were always fast. And successful. The build phase is <em>mostly</em> unnecessary.</p>
<p>I found a <a href="https://developer.apple.com/forums/thread/756120">thread in the Apple Developer Forums</a> that discusses the problem, mentioning the <code>-allowProvisioningUpdates</code> option of the command-line <code>xcodebuild</code> tool. From the <code>man</code> page:</p>
<blockquote>Allow xcodebuild to communicate with the Apple Developer website.
           For automatically signed targets, xcodebuild will create and update
           profiles, app IDs, and certificates. For manually signed targets,
           xcodebuild will download missing or updated provisioning profiles.
           Requires a developer account to have been added in Xcode's Accounts
           preference pane.</blockquote>
<p>Connecting to <code>developerservices2.apple.com</code>, and to some other domains, is required in order to upload a build to App Store Connect. For most local builds, on the other hand, the "Gathering provisioning inputs" build phase is unnecessary and can slow down the build considerably. Thus, I've now denied Xcode connections to <code>developerservices2.apple.com</code> by default in Little Snitch and disable the rule only when uploading to App Store Connect.</p>
<p>During my investigation of slow builds, I noticed some other frequent Xcode connections. For example, Xcode connects to <code>devimages-cdn.apple.com</code> every time it launches. According to Apple's support document <a href="https://support.apple.com/101555">Use Apple products on enterprise networks</a>, that domain is used for "Xcode downloadable components". I assume this refers to platform support in the Components pane of Xcode Settings. (Note that the document doesn't mention <code>developerservices2.apple.com</code>.) Again, though, it's unnecessary to check for updates on every launch. I'd rather not tell Apple whenever I launch Xcode, or whenever I make a local build of my app. It certainly doesn't align with Apple's claim that they believe privacy is a fundamental human right. Or perhaps Apple believes that developers are subhuman…</p>
<p>I've saved the worst for last. For some reason, Xcode phones home to <code>appstoreconnect.apple.com</code> every time I open an Xcode project. This also appears to be unnecessary, and I experience no problems after denying the connections in Little Snitch, so I do! I assume that the connections send identifying information about the Xcode project to Apple, otherwise why even make the connections when opening a project? And all of these connections from Xcode, to every domain, require login to your Apple Developer account, so Apple is definitely receiving identifying information about you in any case.</p>
<p>In effect, Xcode is a developer analytics collection mechanism, whether you like it or not, which I don't.</p>

<header><a href="https://lapcatsoftware.com/">Jeff Johnson</a> (<a href="https://underpassapp.com/">My apps</a>, <a href="https://www.paypal.me/JeffJohnsonWI">PayPal.Me</a>, <a href="https://mastodon.social/@lapcatsoftware" title="@lapcatsoftware@mastodon.social">Mastodon</a>)</header>
<nav><a href="https://lapcatsoftware.com/articles/index.html" title="The Desolation of Blog">Articles index</a><br>
Previous: <a href="https://lapcatsoftware.com/articles/2025/2/4.html">Inaccessible .bnnsir files on macOS Sequoia</a>
</nav>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dogs may have domesticated themselves because they liked snacks, model suggests (105 pts)]]></title>
            <link>https://www.livescience.com/animals/dogs/dogs-may-have-domesticated-themselves-because-they-really-liked-snacks-model-suggests</link>
            <guid>43168534</guid>
            <pubDate>Tue, 25 Feb 2025 05:33:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.livescience.com/animals/dogs/dogs-may-have-domesticated-themselves-because-they-really-liked-snacks-model-suggests">https://www.livescience.com/animals/dogs/dogs-may-have-domesticated-themselves-because-they-really-liked-snacks-model-suggests</a>, See on <a href="https://news.ycombinator.com/item?id=43168534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1200-80.jpg" alt="A wolf in a snowy landscape licks its lips" srcset="https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/L26TfjDeqvSafuSB4EnwzD.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>Did dogs domesticate themselves?</span>
<span>(Image credit: martinagebarovska via Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>Scientists don't know exactly how wolves were domesticated into early <a data-analytics-id="inline-link" href="https://www.livescience.com/animals/land-mammals/dogs" data-before-rewrite-localise="https://www.livescience.com/animals/land-mammals/dogs"><u>dogs</u></a>, but it's possible that they domesticated themselves by choosing to coexist with humans so that, a new study finds, they could be fed consistently. Then, these snackish canines likely selected mates that did the same.</p><p>While <a data-analytics-id="inline-link" href="https://www.taylorfrancis.com/chapters/edit/10.4324/9781315179988-3/self-domestication-human-control-upper-palaeolithic-domestication-wolf-mietje-germonpr%C3%A9-martina-l%C3%A1zni%C4%8Dkov%C3%A1-galetov%C3%A1-mikhail-sablin-herv%C3%A9-bocherens" target="_blank" data-url="https://www.taylorfrancis.com/chapters/edit/10.4324/9781315179988-3/self-domestication-human-control-upper-palaeolithic-domestication-wolf-mietje-germonpr%C3%A9-martina-l%C3%A1zni%C4%8Dkov%C3%A1-galetov%C3%A1-mikhail-sablin-herv%C3%A9-bocherens" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>the theory is nothing new</u></a>, the new research shows it's statistically possible for the wolves to have driven their own domestication into dogs through <a data-analytics-id="inline-link" href="https://www.livescience.com/474-controversy-evolution-works.html" data-before-rewrite-localise="https://www.livescience.com/474-controversy-evolution-works.html"><u>natural selection</u></a>.</p><p>Archaeological and genetic evidence suggests that dogs (<em>Canis familiaris</em>) are descended from gray wolves (<em>Canis lupus</em>) and became domesticated over two historical periods: between about 30,000 and 15,000 years ago <a data-analytics-id="inline-link" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7109016/" target="_blank" data-url="https://pmc.ncbi.nlm.nih.gov/articles/PMC7109016/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>when wild wolves were domesticated into early dogs</u></a>, then from around 15,000 years ago to the modern era when these dogs became differentiated into breeds. These periods of domestication were driven by different evolutionary forces, according to a new study published Feb. 12 in the journal <a data-analytics-id="inline-link" href="https://royalsocietypublishing.org/doi/10.1098/rspb.2024.2646" target="_blank" data-url="https://royalsocietypublishing.org/doi/10.1098/rspb.2024.2646" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>Proceedings of the Royal Society B</u></a>.</p><p>The second domestication period is thought to have been mostly driven by artificial selection: humans chose more tame wolves to accompany them for hunting and companionship, and purposely bred the least-wild animals together, which ultimately produced the tame species we know as the domestic dog.</p><p>But the selective forces that drove the first domestication period are less clear. According to one hypothesis, wolves underwent "self-domestication": after prehistoric animals started coming to human settlements to scavenge for food scraps, they grew more accustomed to living near people. These more-tolerant wolves preferred a steady food supply over inconsistent sources in the wild, and their descendants gave rise to the first domestic dogs.</p><p><strong>Related: </strong><a data-analytics-id="inline-link" href="https://www.livescience.com/animals/dogs/smarter-dogs-have-smaller-brains-surprising-study-reveals" data-before-rewrite-localise="https://www.livescience.com/animals/dogs/smarter-dogs-have-smaller-brains-surprising-study-reveals"><u><strong>Smarter dogs have smaller brains, surprising study reveals</strong></u></a></p><p>A similar process is believed to have occurred with <a data-analytics-id="inline-link" href="https://www.livescience.com/59538-cat-domestication-dispersal-in-ancient-world.html" data-before-rewrite-localise="https://www.livescience.com/59538-cat-domestication-dispersal-in-ancient-world.html"><u>cat domestication</u></a>, as <a data-analytics-id="inline-link" href="https://go.redirectingat.com/?id=92X1590019&amp;xcust=livescience_us_3220871986965120385&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41559-017-0139&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fdogs%2Fdogs-may-have-domesticated-themselves-because-they-really-liked-snacks-model-suggests" target="_blank" data-url="https://www.nature.com/articles/s41559-017-0139" referrerpolicy="no-referrer-when-downgrade" rel="sponsored noopener" data-hl-processed="skimlinks" data-placeholder-url="https://go.redirectingat.com/?id=92X1590019&amp;xcust=hawk-custom-tracking&amp;xs=1&amp;url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41559-017-0139&amp;sref=https%3A%2F%2Fwww.livescience.com%2Fanimals%2Fdogs%2Fdogs-may-have-domesticated-themselves-because-they-really-liked-snacks-model-suggests" data-google-interstitial="false" data-merchant-name="SkimLinks - nature.com" data-merchant-network="SkimLinks"><u>research found</u></a> that the ancestors of house cats spent time near farming communities around 10,000 years ago, before settling into a mutually-beneficial relationship in which they hunted and ate rodents <a data-analytics-id="inline-link" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7414175/" target="_blank" data-url="https://pmc.ncbi.nlm.nih.gov/articles/PMC7414175/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>in exchange for food</u></a>.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-Qhg84EAgRXm6mHHLQvGd6g"><section><p>Get the world’s most fascinating discoveries delivered straight to your inbox.</p></section></div><h2 id="dog-domestication-debate-3">Dog domestication debate</h2><p>Scientists have long debated the dog self-domestication hypothesis. One criticism is that natural selection may not be able to work quickly enough to turn wild wolves into domestic dogs over thousands of years — without selective breeding by humans to speed the process up.</p><p>In the new study, researchers addressed this question of time constraint by using a statistical model that considered potential timelines of dog domestication and gave its computerized canines some agency over choosing their lifestyle and mates.</p><p>The model showed that over 15,000 years, natural selection could potentially drive dog self-domestication. But for this to happen, two conditions had to be met: Wolves had to choose to stay near humans to eat food scraps, and they had to select mates with a similar temperament.</p><p>"When females were selecting mates, they also had to select males that had a similar tameness to themselves," study co-author <a data-analytics-id="inline-link" href="https://www.jmu.edu/mathstat/people/faculty-full-time/capaldi-alex.shtml" target="_blank" data-url="https://www.jmu.edu/mathstat/people/faculty-full-time/capaldi-alex.shtml" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>Alex Capaldi</u></a>, a mathematician and statistician at James Madison University in Virginia, told Live Science. "So if both of those processes are in play, then it is possible for the self-domestication hypothesis to beat the time constraint critique."</p><p>The scientists pointed out that while the model doesn't show exactly how wolves became domesticated, the model's results do mean self-domestication is a possibility.</p><p>Dogs are the earliest animal to be domesticated, so studying their evolution can help scientists better understand how domestication may work in other species. And since the process of dog domestication was so closely linked to the development of early human societies — dogs <a data-analytics-id="inline-link" href="https://academic.oup.com/af/article/4/3/23/4638686" target="_blank" data-url="https://academic.oup.com/af/article/4/3/23/4638686" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>herded cattle</u></a> in early human settlements, and <a data-analytics-id="inline-link" href="https://pubmed.ncbi.nlm.nih.gov/33495362/" target="_blank" data-url="https://pubmed.ncbi.nlm.nih.gov/33495362/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>migrated with early humans</u></a> to the Americas — understanding its timeline can illuminate our history, and how <a data-analytics-id="inline-link" href="https://www.science.org/doi/10.1126/science.1261022" target="_blank" data-url="https://www.science.org/doi/10.1126/science.1261022" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>we ourselves evolved to communicate and bond</u></a> with dogs.</p>
</div>
<div id="slice-container-authorBio-Qhg84EAgRXm6mHHLQvGd6g"><p>Olivia Ferrari is a New York City-based freelance journalist with a background in research and science communication. Olivia has lived and worked in the U.K., Costa Rica, Panama and Colombia. Her writing focuses on wildlife, environmental justice, climate change, and social science.  </p></div>

</section>






<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to change your settings to make yourself less valuable to Meta (271 pts)]]></title>
            <link>https://johnoliverwantsyourraterotica.com/</link>
            <guid>43167936</guid>
            <pubDate>Tue, 25 Feb 2025 03:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://johnoliverwantsyourraterotica.com/">https://johnoliverwantsyourraterotica.com/</a>, See on <a href="https://news.ycombinator.com/item?id=43167936">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">


			
				<article id="post-7" class="page">

				
					<div>
		<div id="home">
				
				
				
				
				
				
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Gray-logo.png" alt="" title="Gray logo"></span>
			</p>
			</div><div>
				
				
				
				
				<p><strong>How to change your settings</strong></p>
			</div><div>
<p><span>to make yourself less valuable to Meta</span></p></div><div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Both-logos.png" alt="" title="Both logos"></span>
			</p>
			</div><div>
				<div>
				
				
				
				
				<p><a href="#fb"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span></a>
			</p>
			</div><div>
				
				
				
				
				<p><a href="#ig"><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span></a>
			</p>
			</div>
				
				
				
				
			</div>
				
				
			</div><div id="fb">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Facebook_Logo_Primary.png" alt="" title="Facebook_Logo_Primary"></span>
			</p>
			</div><div><p><strong>TO STOP META FROM FEEDING YOU ADS BASED ON DATA COLLECTED ABOUT YOU FROM OTHER APPS AND WEBSITES</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Activity information from ad partners.”</span></li>
<li><span>Click “Review setting.”</span></li>
<li><span>Select “No, don’t make my ads more relevant by using this information.”</span></li>
<li><span>Click “Confirm.”</span></li>
</ul>

<p><strong>TO STOP META FROM USING YOUR DATA TO HELP ADVERTISERS TARGET YOU ON OTHER APPS</strong></p>
<ul>
<li><span>Click “Ad preferences.”</span></li>
<li><span>Click “Manage info.”</span></li>
<li><span>Click “Ads from ad partners.”</span></li>
<li><span>Select “Don’t show me ads from ad partners.”</span></li>
<li><span>Click the “X” button to close out.</span></li>
</ul>

<p><strong>TO UNLINK YOUR ACCOUNT FROM THE DATA ABOUT YOU THAT OTHER COMPANIES GIVE TO META</strong></p>
<ul>
<li><span>Click “Your information and permissions.”</span></li>
<li><span>Click “Your activity off Meta technologies.”</span></li>
<li><span>Click “Manage future activity.”</span></li>
<li><span>Select </span><b>“</b><span>Disconnect future activity.</span><b>”</b></li>
<li><span>Click “Continue.”</span></li>
<li><span>Click “Disconnect future activity.”</span></li>
</ul>
</div>
				
				
				
				
			</div><div id="ig">
				<div>
				
				
				
				
				<p><span><img decoding="async" src="https://johnoliverwantsyourraterotica.com/wp-content/uploads/2025/02/Instagram_Glyph_Gradient-1.png" alt="" title="Instagram_Glyph_resize"></span>
			</p>
			</div><div><p><strong>If your Facebook and Instagram accounts are linked, you’re good to go! </strong></p>

<p><strong>If your Facebook and Instagram accounts are NOT linked, go to<a href="http://accountscenter.instagram.com/" target="_blank" rel="noopener"> accountscenter.instagram.com</a> and repeat the steps above.</strong></p></div>
				
				
				
				
			</div><div><p><strong>OTHER STEPS YOU CAN TAKE:</strong></p>
<ul>
<li><span>Use a privacy-focused web browser like </span><a href="https://www.mozilla.org/en-US/firefox/" target="_blank" rel="noopener"><span>Firefox</span></a><span>.</span></li>
<li><span>Add a browser extension like </span><a href="https://privacybadger.org/" target="_blank" rel="noopener"><span>Privacy Badger</span></a><span> to block advertisers and other third parties from tracking you.&nbsp;</span></li>
<li><span>Disable your phone’s advertising identifier (see instructions for </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>iOS</span></a><span> and </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#disable-ad-tracking" target="_blank" rel="noopener"><span>Android</span></a><span> devices).&nbsp;</span></li>
</ul>

<p><span>SPECIAL THANKS TO THE </span><a href="https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data" target="_blank" rel="noopener"><span>ELECTRONIC FRONTIER FOUNDATION</span></a></p></div>		</div>

				
				</article>

			

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[History of CAD – David Weisberg (122 pts)]]></title>
            <link>https://www.shapr3d.com/blog/history-of-cad</link>
            <guid>43167865</guid>
            <pubDate>Tue, 25 Feb 2025 03:36:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.shapr3d.com/blog/history-of-cad">https://www.shapr3d.com/blog/history-of-cad</a>, See on <a href="https://news.ycombinator.com/item?id=43167865">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div fs-toc-offsettop="96px" fs-toc-element="contents"><p>Writing a book about the history of CAD is an almost insurmountable task. If we, the editors, engineers, and CAD experts at Shapr3D were to accomplish this task, we’d most definitely fall short of the mark while spending countless hours with it.With over 80 years (and counting), the storied past of CAD slowly became the stuff of legends. </p><p>Today’s industry veterans might know a thing or two about the pioneers of the ’50s and ’60s, but the experts of tomorrow might even look at AutoCAD as a thing of the past.</p><p>David E. Weisberg chose to tell the story of CAD at the right moment: by still being able to interview the first key figures of a nascent technology, Weisberg gave a recollection of events, landmarks, and actors with accuracy that would be unattainable today.This digital edition of the original tome of around 650 pages is broken down into separate blog posts as per the author’s original &nbsp;chapters. By hosting this all-encompassing timeline of the history of CAD, we hope to offer our readers the in-depth look at the very origins of the industry, all the way to the early ’00s and modern-era CAD.</p><p>The late David E. Weisberg’s <em>The Engineering Design Revolution</em> is published on the Shapr3D Blog with the approval of the author’s relatives.</p><blockquote><strong><em>The People, Companies and Computer Systems That Changed Forever the Practice of Engineering<br>By David E. Weisberg</em></strong></blockquote><p>This book has been over five years in the making and is now freely available on this web site for your personal use. It can be read online or downloaded and printed for your later perusal. If a company wishes to produce multiple copies of specific material, please contact me at the address below.</p><p>In return for free access to over 650 pages of material discussing the people, companies and products that made the CAD industry what it is today, I am asking readers contribute whatever they wish to a foundation that means very much to me. In the mid-1990s I lost both of my sisters to cancer. I am sure many of you have also had relatives and acquaintances struck by this disease. I became very active in a non-profit organization in Denver that raises funds for cancer research and patient support. The endowment portion of this organization is called Cancer League of Colorado Foundation. You can either make a check out to the Foundation and send it to me at the address below (U.S. funds only) or contribute through PayPal.</p><p>Sit back and enjoy a trip through the nearly 60 years that totally revolutionized the practice of engineering design. One final word before you start if you find any errors in this material or take issue with what I have written, please let me hear from you. Also, if you have photographs or illustrations you would like to see incorporated into this document, please forward them to me. Having this book online results in a dynamic document that I will update as needed in the future.</p><ol start="0" role="list"><li><a href="https://www.shapr3d.com/history-of-cad/foreword">Foreword</a></li><li><a href="https://www.shapr3d.com/history-of-cad/introduction">Introduction</a></li><li><a href="https://www.shapr3d.com/history-of-cad/brief-overview-of-the-history-of-cad">Brief Overview</a></li><li><a href="https://www.shapr3d.com/history-of-cad/computer-aided-designs-strong-roots-at-mit">Computer-Aided Design Strong Roots at MIT</a></li><li><a href="https://www.shapr3d.com/history-of-cad/research-in-the-mid-to-late-1960s">Research in the Second Half of the 1960s</a></li><li><a href="https://www.shapr3d.com/history-of-cad/civil-engineering-software-development-at-mit">Civil Engineering Software Development at MIT</a></li><li><a href="https://www.shapr3d.com/history-of-cad/the-first-commercial-cad-system">The First Commercial CAD System</a></li><li><a href="https://www.shapr3d.com/history-of-cad/applicon">Applicon</a></li><li><a href="https://www.shapr3d.com/history-of-cad/autodesk-and-autocad">Autodesk and AutoCAD</a></li><li><a href="https://www.shapr3d.com/history-of-cad/auto-trol-technology">Auto-trol Technology</a></li><li><a href="https://www.shapr3d.com/history-of-cad/bentley-systems-incorporated">Bentley Systems</a></li><li><a href="https://www.shapr3d.com/history-of-cad/calma">Calma</a></li><li><a href="https://www.shapr3d.com/history-of-cad/computervision">Computervision</a></li><li><a href="https://www.shapr3d.com/history-of-cad/ibm-lockheed-and-dassault-systemes">IBM/Lockheed/Dassault Systèmes</a></li><li><a href="https://www.shapr3d.com/history-of-cad/intergraph">Intergraph</a></li><li><a href="https://www.shapr3d.com/history-of-cad/patrick-hanratty-and-manufacturing-consulting-services">Patrick Hanratty and Manufacturing &amp; Consulting Services</a></li><li><a href="https://www.shapr3d.com/history-of-cad/parametric-technology-corporation">Parametric Technology Corporation</a></li><li><a href="https://www.shapr3d.com/history-of-cad/structural-dynamics-research-corporation">Structural Dynamics Research Corporation</a></li><li><a href="https://www.shapr3d.com/history-of-cad/solidworks/">SolidWorks</a></li><li><a href="https://www.shapr3d.com/history-of-cad/siemens-plm-software-unigraphics">Siemens PLM Software (UGS)</a></li><li><a href="https://www.shapr3d.com/history-of-cad/tom-lazear-and-versacad">Tom Lazear and VersaCAD</a></li><li><a href="https://www.shapr3d.com/history-of-cad/miscellaneous-companies">Miscellaneous Companies</a></li><li><a href="https://www.shapr3d.com/history-of-cad/analysis-companies">Analysis Companies</a></li></ol></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disclosure of personal information to DOGE “is irreparable harm,” judge rules (245 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</link>
            <guid>43167579</guid>
            <pubDate>Tue, 25 Feb 2025 02:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/">https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</a>, See on <a href="https://news.ycombinator.com/item?id=43167579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>"The plaintiffs have made a clear showing that they are likely to suffer irreparable harm without injunctive relief," the order said. "DOGE affiliates have been granted access to systems of record that contain some of the plaintiffs' most sensitive data—Social Security numbers, dates of birth, home addresses, income and assets, citizenship status, and disability status—and their access to this trove of personal information is ongoing. There is no reason to believe their access to this information will end anytime soon because the government believes their access is appropriate."</p>
<p>The American Federation of Teachers, which represents 1.8 million teachers and nurses, was joined in the lawsuit by the International Association of Machinists and Aerospace Workers, International Federation of Professional and Technical Engineers, National Active and Retired Federal Employees Association, and National Federation of Federal Employees.</p>

<h2>No need to know</h2>
<p>The government insisted that the DOGE affiliates are employees of Education and OPM, and the judge assumed that is true for purposes of evaluating the motion for a restraining order. Even with that allowance, Boardman decided the data access is not permissible under the "need-to-know" exception to the law prohibiting unnecessary disclosure.</p>
<p>The Trump administration did not explain why "the DOGE affiliates at Education <em>need</em> such comprehensive, sweeping access to the plaintiffs' records to audit student loan programs for waste, fraud, and abuse or to conduct cost-estimate analyses," Boardman wrote, adding that "there appears to be no precedent with similar facts."</p>
<p>There are six DOGE affiliates working at Education. They include Adam Ramada, a United States DOGE Service employee, and five "DOGE-affiliated individuals" who have not been identified by name.</p>
<p>"It may be that, with additional time, the government can explain why granting such broad access to the plaintiffs' personal information is necessary for DOGE affiliates at Education to do their jobs, but for now, the record before the Court indicates they do not have a <em>need</em> for these records in the performance of their duties," Boardman wrote.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek open source DeepEP – library for MoE training and Inference (478 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepEP</link>
            <guid>43167373</guid>
            <pubDate>Tue, 25 Feb 2025 02:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a>, See on <a href="https://news.ycombinator.com/item?id=43167373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepEP</h2><a id="user-content-deepep" aria-label="Permalink: DeepEP" href="#deepep"></a></p>
<p dir="auto">DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p dir="auto">To align with the group-limited gating algorithm proposed in the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p>
<p dir="auto">For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p>
<p dir="auto">Notice: the implementation in this library may have some slight differences from the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Normal kernels with NVLink and RDMA forwarding</h3><a id="user-content-normal-kernels-with-nvlink-and-rdma-forwarding" aria-label="Permalink: Normal kernels with NVLink and RDMA forwarding" href="#normal-kernels-with-nvlink-and-rdma-forwarding"></a></p>
<p dir="auto">We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Type</th>
<th>Dispatch #EP</th>
<th>Bottleneck bandwidth</th>
<th>Combine #EP</th>
<th>Bottleneck bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intranode</td>
<td>8</td>
<td>153 GB/s (NVLink)</td>
<td>8</td>
<td>158 GB/s (NVLink)</td>
</tr>
<tr>
<td>Internode</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>32</td>
<td>44 GB/s (RDMA)</td>
<td>32</td>
<td>47 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>64</td>
<td>46 GB/s (RDMA)</td>
<td>64</td>
<td>45 GB/s (RDMA)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-latency kernels with pure RDMA</h3><a id="user-content-low-latency-kernels-with-pure-rdma" aria-label="Permalink: Low-latency kernels with pure RDMA" href="#low-latency-kernels-with-pure-rdma"></a></p>
<p dir="auto">We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dispatch #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
<th>Combine #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>163 us</td>
<td>46 GB/s</td>
<td>8</td>
<td>318 us</td>
<td>46 GB/s</td>
</tr>
<tr>
<td>16</td>
<td>173 us</td>
<td>43 GB/s</td>
<td>16</td>
<td>329 us</td>
<td>44 GB/s</td>
</tr>
<tr>
<td>32</td>
<td>182 us</td>
<td>41 GB/s</td>
<td>32</td>
<td>350 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>64</td>
<td>186 us</td>
<td>40 GB/s</td>
<td>64</td>
<td>353 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>128</td>
<td>192 us</td>
<td>39 GB/s</td>
<td>128</td>
<td>369 us</td>
<td>39 GB/s</td>
</tr>
<tr>
<td>256</td>
<td>194 us</td>
<td>39 GB/s</td>
<td>256</td>
<td>360 us</td>
<td>40 GB/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Hopper GPUs (may support more architectures or devices later)</li>
<li>Python 3.8 and above</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.1 and above</li>
<li>NVLink for intranode communication</li>
<li>RDMA network for internode communication</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and install NVSHMEM dependency</h3><a id="user-content-download-and-install-nvshmem-dependency" aria-label="Permalink: Download and install NVSHMEM dependency" href="#download-and-install-nvshmem-dependency"></a></p>
<p dir="auto">DeepEP also depends on our modified NVSHMEM. Please refer to our <a href="https://github.com/deepseek-ai/DeepEP/blob/main/third-party/README.md">NVSHMEM Installation Guide</a> for instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build and make symbolic links for SO files
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
# You may modify the specific SO names according to your own platform
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

# Run test cases
# NOTES: you may modify the `init_dist` function in `tests/utils.py`
# according to your own cluster settings, and launch into multiple nodes 
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py"><pre><span><span>#</span> Build and make symbolic links for SO files</span>
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
<span><span>#</span> You may modify the specific SO names according to your own platform</span>
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

<span><span>#</span> Run test cases</span>
<span><span>#</span> NOTES: you may modify the `init_dist` function in `tests/utils.py`</span>
<span><span>#</span> according to your own cluster settings, and launch into multiple nodes </span>
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install"><pre>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install</pre></div>
<p dir="auto">Then, import <code>deep_ep</code> in your Python project, and enjoy!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Network configurations</h2><a id="user-content-network-configurations" aria-label="Permalink: Network configurations" href="#network-configurations"></a></p>
<p dir="auto">DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Traffic isolation</h3><a id="user-content-traffic-isolation" aria-label="Permalink: Traffic isolation" href="#traffic-isolation"></a></p>
<p dir="auto">Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p>
<p dir="auto">To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p>
<ul dir="auto">
<li>workloads using normal kernels</li>
<li>workloads using low-latency kernels</li>
<li>other workloads</li>
</ul>
<p dir="auto">For DeepEP, you can control the virtual lane assignment by setting the <code>NVSHMEM_IB_SL</code> environment variable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adaptive routing</h3><a id="user-content-adaptive-routing" aria-label="Permalink: Adaptive routing" href="#adaptive-routing"></a></p>
<p dir="auto">Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Currently, low-latency kernels support adaptive routing, while normal kernels do not (support may be added soon). <strong>Enabling adaptive routing for normal internode kernels may lead to deadlocks or data corruption issues</strong>.</p>
<p dir="auto">For low-latency kernels, enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p>
<ul dir="auto">
<li>enable adaptive routing in environments with heavy network loads</li>
<li>use static routing in environments with light network loads</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Congestion control</h3><a id="user-content-congestion-control" aria-label="Permalink: Congestion control" href="#congestion-control"></a></p>
<p dir="auto">Congestion control is disabled as we have not observed significant congestion in our production environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interfaces and examples</h2><a id="user-content-interfaces-and-examples" aria-label="Permalink: Interfaces and examples" href="#interfaces-and-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in model training or inference prefilling</h3><a id="user-content-example-use-in-model-training-or-inference-prefilling" aria-label="Permalink: Example use in model training or inference prefilling" href="#example-use-in-model-training-or-inference-prefilling"></a></p>
<p dir="auto">The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)
_buffer: Optional[Buffer] = None

# Set the number of SMs to use
# NOTES: this is a static variable
Buffer.set_num_sms(24)


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -> Buffer:
    global _buffer
    
    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if not existed or not enough buffer size
    # NOTES: the adaptive routing configuration of the network **must be off**
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes < num_nvl_bytes or _buffer.num_rdma_bytes < num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer


def get_hidden_bytes(x: torch.Tensor) -> int:
    t = x[0] if isinstance(x, tuple) else x
    return t.size(1) * max(t.element_size(), 2)


def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,
                     num_experts: int, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency 
    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please
    # refer to the docs of `Buffer.dispatch`
    global _buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    # NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph
    # For more advanced usages, please refer to the docs of the `dispatch` function
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    # For event management, please refer to the docs of the `EventOverlap` class
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event


def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -> \
        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
    global _buffer

    # The backward process of MoE dispatch is actually a combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_grad_x, combined_grad_recv_topk_weights, event


def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[torch.Tensor, EventOverlap]:
    global _buffer

    # Do MoE combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_x, event


def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
    global _buffer

    # The backward process of MoE combine is actually a dispatch
    # For more advanced usages, please refer to the docs of the `combine` function
    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return grad_x, event"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>List</span>, <span>Tuple</span>, <span>Optional</span>, <span>Union</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>, <span>EventOverlap</span>

<span># Communication buffer (will allocate at runtime)</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>

<span># Set the number of SMs to use</span>
<span># NOTES: this is a static variable</span>
<span>Buffer</span>.<span>set_num_sms</span>(<span>24</span>)


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>hidden_bytes</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span>global</span> <span>_buffer</span>
    
    <span># NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests</span>
    <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span> <span>=</span> <span>0</span>, <span>0</span>
    <span>for</span> <span>config</span> <span>in</span> (<span>Buffer</span>.<span>get_dispatch_config</span>(<span>group</span>.<span>size</span>()), <span>Buffer</span>.<span>get_combine_config</span>(<span>group</span>.<span>size</span>())):
        <span>num_nvl_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_nvl_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_nvl_bytes</span>)
        <span>num_rdma_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_rdma_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_rdma_bytes</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span># NOTES: the adaptive routing configuration of the network **must be off**</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>_buffer</span>.<span>num_nvl_bytes</span> <span>&lt;</span> <span>num_nvl_bytes</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span>)
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>get_hidden_bytes</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>) <span>-&gt;</span> <span>int</span>:
    <span>t</span> <span>=</span> <span>x</span>[<span>0</span>] <span>if</span> <span>isinstance</span>(<span>x</span>, <span>tuple</span>) <span>else</span> <span>x</span>
    <span>return</span> <span>t</span>.<span>size</span>(<span>1</span>) <span>*</span> <span>max</span>(<span>t</span>.<span>element_size</span>(), <span>2</span>)


<span>def</span> <span>dispatch_forward</span>(<span>x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>,
                     <span>num_experts</span>: <span>int</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>List</span>, <span>Tuple</span>, <span>EventOverlap</span>]:
    <span># NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency </span>
    <span># of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please</span>
    <span># refer to the docs of `Buffer.dispatch`</span>
    <span>global</span> <span>_buffer</span>

    <span># Calculate layout before actual dispatch</span>
    <span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span>, <span>num_tokens_per_expert</span>, <span>is_token_in_rank</span>, <span>previous_event</span> <span>=</span> \
        <span>_buffer</span>.<span>get_dispatch_layout</span>(<span>topk_idx</span>, <span>num_experts</span>,
                                    <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                    <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)
    <span># Do MoE dispatch</span>
    <span># NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph</span>
    <span># For more advanced usages, please refer to the docs of the `dispatch` function</span>
    <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>dispatch</span>(<span>x</span>, <span>topk_idx</span><span>=</span><span>topk_idx</span>, <span>topk_weights</span><span>=</span><span>topk_weights</span>,
                         <span>num_tokens_per_rank</span><span>=</span><span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span><span>=</span><span>num_tokens_per_rdma_rank</span>,
                         <span>is_token_in_rank</span><span>=</span><span>is_token_in_rank</span>, <span>num_tokens_per_expert</span><span>=</span><span>num_tokens_per_expert</span>,
                         <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                         <span>allocate_on_comm_stream</span><span>=</span><span>True</span>)
    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span>


<span>def</span> <span>dispatch_backward</span>(<span>grad_recv_x</span>: <span>torch</span>.<span>Tensor</span>, <span>grad_recv_topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE dispatch is actually a combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>combine</span>(<span>grad_recv_x</span>, <span>handle</span>, <span>topk_weights</span><span>=</span><span>grad_recv_topk_weights</span>, <span>async_finish</span><span>=</span><span>True</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span>


<span>def</span> <span>combine_forward</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_x</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>combine</span>(<span>x</span>, <span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>, <span>previous_event</span><span>=</span><span>previous_event</span>,
                                           <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_x</span>, <span>event</span>


<span>def</span> <span>combine_backward</span>(<span>grad_combined_x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE combine is actually a dispatch</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>grad_x</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>dispatch</span>(<span>grad_combined_x</span>, <span>handle</span><span>=</span><span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                                 <span>previous_event</span><span>=</span><span>previous_event</span>,
                                                 <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>grad_x</span>, <span>event</span></pre></div>
<p dir="auto">Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/normal.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/normal.png" alt="normal"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in inference decoding</h3><a id="user-content-example-use-in-inference-decoding" aria-label="Permalink: Example use in inference decoding" href="#example-use-in-inference-decoding"></a></p>
<p dir="auto">The low latency kernels can be used in the inference decoding phase as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import Tuple, Optional

from deep_ep import Buffer

# Communication buffer (will allocate at runtime)
# NOTES: there is no SM control API for the low-latency kernels
_buffer: Optional[Buffer] = None


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -> Buffer:
    # NOTES: the low-latency mode will consume much more space than the normal mode
    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256
    global _buffer
    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)

    # Allocate a buffer if not existed or not enough buffer size
    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes < num_rdma_bytes:
        # NOTES: for best performance, the QP number **must** be equal to the number of the local experts
        assert num_experts % group.size() == 0
        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())
    return _buffer


def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):
    global _buffer

    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)
    recv_hidden_states, recv_expert_count, handle, event, hook = \
        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,
                                     async_finish=False, return_recv_hook=True)

    # NOTES: the actual tensor will not be received only if you call `hook()`,
    # it is useful for double-batch overlapping, but **without any SM occupation**
    # If you don't want to overlap, please set `return_recv_hook=False`
    # Later, you can use our GEMM library to do the computation with this specific format
    return recv_hidden_states, recv_expert_count, handle, event, hook


def low_latency_combine(hidden_states: torch.Tensor,
                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):
    global _buffer

    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)
    combined_hidden_states, event_overlap, hook = \
        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,
                                    async_finish=False, return_recv_hook=True)

    # NOTES: the same behavior as described in the dispatch kernel
    return combined_hidden_states, event_overlap, hook"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span>, <span>Optional</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>

<span># Communication buffer (will allocate at runtime)</span>
<span># NOTES: there is no SM control API for the low-latency kernels</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>hidden</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span># NOTES: the low-latency mode will consume much more space than the normal mode</span>
    <span># So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256</span>
    <span>global</span> <span>_buffer</span>
    <span>num_rdma_bytes</span> <span>=</span> <span>Buffer</span>.<span>get_low_latency_rdma_size_hint</span>(<span>num_max_dispatch_tokens_per_rank</span>, <span>hidden</span>, <span>group</span>.<span>size</span>(), <span>num_experts</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>not</span> <span>_buffer</span>.<span>low_latency_mode</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span># NOTES: for best performance, the QP number **must** be equal to the number of the local experts</span>
        <span>assert</span> <span>num_experts</span> <span>%</span> <span>group</span>.<span>size</span>() <span>==</span> <span>0</span>
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>0</span>, <span>num_rdma_bytes</span>, <span>low_latency_mode</span><span>=</span><span>True</span>, <span>num_qps_per_rank</span><span>=</span><span>num_experts</span> <span>//</span> <span>group</span>.<span>size</span>())
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>low_latency_dispatch</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_dispatch</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>num_max_dispatch_tokens_per_rank</span>, <span>num_experts</span>,
                                     <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the actual tensor will not be received only if you call `hook()`,</span>
    <span># it is useful for double-batch overlapping, but **without any SM occupation**</span>
    <span># If you don't want to overlap, please set `return_recv_hook=False`</span>
    <span># Later, you can use our GEMM library to do the computation with this specific format</span>
    <span>return</span> <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span>


<span>def</span> <span>low_latency_combine</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>,
                        <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_combine</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>topk_weights</span>, <span>handle</span>,
                                    <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the same behavior as described in the dispatch kernel</span>
    <span>return</span> <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span></pre></div>
<p dir="auto">For two micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffics are happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e. the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/low-latency.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/low-latency.png" alt="low-latency"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notices</h2><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<ul dir="auto">
<li>For extreme performance, we discover and use a behavior-out-of-doc PTX instruction: <code>ld.global.nc.L1::no_allocate.L2::256B</code>. This instruction will lead to an undefined behavior: accessing volatile GPU memory with non-coherent read-only PTX modifiers <code>.nc</code>. But the correctness is tested to be guaranteed with <code>.L1::no_allocate</code> on Hopper architectures, and performance will be much better. If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to <code>setup.py</code> and disable this, or file an issue.</li>
<li>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek's internal cluster.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepEP/blob/main/LICENSE">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href="https://docs.nvidia.com/nvshmem/api/sla.html" rel="nofollow">NVSHMEM SLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepep2025,
      title={DeepEP: an efficient expert-parallel communication library},
      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},
}"><pre><span>@misc</span>{<span>deepep2025</span>,
      <span>title</span>=<span><span>{</span>DeepEP: an efficient expert-parallel communication library<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepEP}<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DigiCert: Threat of legal action to stifle Bugzilla discourse (554 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</link>
            <guid>43167087</guid>
            <pubDate>Tue, 25 Feb 2025 01:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</a>, See on <a href="https://news.ycombinator.com/item?id=43167087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">Bug 1950144</a>
      </span>
      <span>
        <span>Opened <span title="2025-02-24 08:36 PST" data-time="1740414973">13 hours ago</span></span>
          <span>Updated <span title="2025-02-24 20:23 PST" data-time="1740457405">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>


<div id="module-categories">
        <p><span id="field-value-component">
      <div>
        <p><span id="component-name" tabindex="0" role="button" aria-haspopup="menu" aria-controls="component-info">CA Certificate Root Program
          
        </span></p>
      </div>
        </span>
    </p></div>






































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;136.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;128.7.0esr&quot;,&quot;FIREFOX_ESR115&quot;:&quot;115.20.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;137.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2025-02-03&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2025-02-04&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2025-01-30&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2025-01-31&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;135.0.1&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2025-03-03&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2025-03-04&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2025-02-27&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2025-02-28&quot;}">



<div id="c0" data-comment-id="17364853"><p>In <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c74" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322 comment 74</a> DigiCert wrote,</p>
<blockquote>
<p>“We have not used a legal team as a shield against accountability.”</p>
</blockquote>
<p>Contrary to this statement, I received a letter from DigiCert’s lawyers, Wilson Sonsini, regarding posts made by Sectigo’s Chief Compliance Officer in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322</a>.  The upshot of the letter was that DigiCert expected Sectigo to “ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization.”</p>
<p>I’m Brian Holland, General Counsel for Sectigo, and this is my first time posting on Bugzilla.  I’m posting because at Sectigo we believe that the WebPKI is best served by open, transparent, and honest debate about issues that impact our community.  Attempts to shut down these conversations, through lawyers or otherwise, are harmful to our collective core mission.</p>
<p>In its opening passages, this letter reads (emphasis mine),</p>
<blockquote>
<p>We ask for your prompt cooperation and assistance in taking corrective action and <strong>forcing Mr. Callan to cease his disparaging public statements. We hope your assistance in this matter will render unnecessary legal action by DigiCert against Sectigo.</strong></p>
</blockquote>
<p>After three pages of detail about specific Bugzilla posts and references to the Lanham Act, deceptive trade practices, corporate disparagement, and tortious interference, the letter (the full letter is included as an attachment to this bug) goes on to say (emphasis mine):</p>
<blockquote>
<p>At this point, we are bringing this situation to your attention on behalf of DigiCert because we are hopeful that Mr. Callan’s actions were the actions of one individual and were not part of an organized plan or institutional practice. We also hope that, upon receiving this information, Sectigo will recognize the impropriety of Mr. Callan’s statements and the substantial public, industry, and browser scrutiny and legal risk such statements would prompt if they were to continue. To that end, we expect that Sectigo will investigate this incident promptly and take the appropriate corrective actions, confirm that this situation was not part of an institutional practice, and <strong>ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization. We hope we can resolve this situation as soon as possible before DigiCert is compelled to seek legal action.</strong></p>
</blockquote>
<p>On December 10, 2024 I sent this response in email to my contact at Wilson Sonsini:</p>
<blockquote>
<p>I have reviewed your letter and the Bugzilla thread referenced therein.  In that letter, you suggest that DigiCert has various legal claims against Sectigo and/or its COO [sic], Tim Callan, for what you call “false and misleading statements about DigiCert” made on the Bugzilla forum.  We strongly disagree.  The statements you point to are questions and/or statements of opinion that are not actionable statements of fact.  Moreover, those comments were made with the intent of facilitating discussion and debate about important questions of first impression for our industry.  They were made by Tim Callan in good faith, are fully protected by the First Amendment, and cannot, as a matter of law, form the basis for any of the causes of action mentioned in your letter.</p>
</blockquote>
<blockquote>
<p>As you are aware, the PKI community is a self-regulating group that, as set out in the bylaws of the Certificate Authority Browser Forum, works “closely together in defining the guidelines and means of implementation for best practices as a way of providing a heightened security for Internet transactions and creating a more intuitive method of displaying secure sites to Internet users.”  For the community to self-regulate, there needs to be open, uninhibited, and robust discussion and debate about best practices in the industry.  Any litigation threats that chill or stifle such debate undermine the self-regulatory system that has worked so well for the industry.</p>
</blockquote>
<blockquote>
<p>Certificate Authorities post incident reports on Bugzilla to “provide lessons learned and transparency about the steps the CA Owner takes to address the immediate issue and prevent future issues.” As the Common CA Database goes on to state “incident reports help the Web PKI ecosystem as a whole because they promote continuous improvement, information sharing, and highlight opportunities to define and adopt improved practices, policies, and controls” of all parties.</p>
</blockquote>
<blockquote>
<p>The TRO involved in this incident report, as one Bugzilla commenter noted, is “an unprecedented event in the WebPKI, and . . . if allowed to proliferate, it would potentially be used by subscribers en masse to do an end-run around important technical security controls.”</p>
</blockquote>
<blockquote>
<p>The PKI Community has never considered how it should respond to TROs and now needs to do so. Understanding the situation faced by your client and why it made certain decisions is important to improving the WebPKI ecosystem. This is why Mr. Callan, and many others, have been asking questions – some of which have been critical questions designed to achieve a consensus as to how best handle situations like this in the future.  In any such discussion, there will be differences of opinion, but open, uninhibited, robust, and transparent discussion is essential for the industry to learn how to best move forward.</p>
</blockquote>
<blockquote>
<p>I hope that your client will, on deeper reflection, realize that as a leader in the PKI Community, it should be driving, rather than stifling, discussion of this topic.  Your client’s threat of litigation is, in our view, both misguided and without merit.  We will strive to be respectful in our tone, but neither Mr. Callan nor Sectigo will be silenced or prevented from asking critical questions and/or engaging in critical discussion about issues of substantial concern to the public and the industry.</p>
</blockquote>
<p>We find the threat of legal action to stifle scrutiny and discussion of public CA practices to be deeply troubling and entirely at odds with the transparent, blameless post-mortem culture that the CCADB incident report guidelines expect CAs to embrace.  Even for a company like Sectigo, the threat of a lawsuit from a well-resourced organization like DigiCert is worrisome, regardless of our confidence that Mr. Callan’s speech was proper, legally protected, and in the best interest of the WebPKI.  Another party challenging DigiCert’s behavior, faced with this same threat, might choose simply to stop asking uncomfortable questions.</p>
<p>No CA should be allowed to intimidate its critics into silence. This would irreparably damage the integrity and quality of the WebPKI.</p>
<p>I am sharing this incident to bring attention to DigiCert’s actions and allow the community to evaluate this approach. What began as a discussion of the threat posed by certificate subscribers using the legal system to circumvent WebPKI security controls needs, in my opinion, to be broadened.</p>
</div><div id="a3699_1689"><p>Component: CA Certificate Compliance → CA Certificate Root Program</p></div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="1">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone at NSF overseeing the Platforms for Wireless Experimentation is gone (389 pts)]]></title>
            <link>https://discuss.systems/@ricci/114059690609284323</link>
            <guid>43166830</guid>
            <pubDate>Tue, 25 Feb 2025 00:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.systems/@ricci/114059690609284323">https://discuss.systems/@ricci/114059690609284323</a>, See on <a href="https://news.ycombinator.com/item?id=43166830">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's still worth blogging in the age of AI (292 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</link>
            <guid>43166761</guid>
            <pubDate>Tue, 25 Feb 2025 00:46:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=43166761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>My post about blogging as
<a href="https://www.gilesthomas.com/2025/02/20250223-til-deep-dive-posts">writing the tutorial that you wished you'd found</a>
really took off
<a href="https://news.ycombinator.com/item?id=43154666">on Hacker News</a>.  There were
a lot of excellent comments, but one thing kept coming up: what's the point
in blogging if people are using ChatGPT, Claude and DeepSeek to spoon-feed them
answers?  Who, apart from the AIs, will read what you write?</p>

<p>I was asking myself the same question when I started blogging semi-regularly again
last year, and this post is an attempt to summarise why I decided that it was worthwhile.
The TL;DR: <strong>blogging isn't just about being read -- it's about learning and thinking, and
having a durable proof that you can do both.</strong></p>


    
        <p>Let's start off by summarising the two big reasons to blog about what you've learned,
as you learn:</p>

<ul>
<li>It helps you make your newly-gained knowledge concrete.</li>
<li>It will help other people in the future -- they might be looking for the information
you blogged about, and find it on your blog.</li>
</ul>

<p>When we're thinking about AI, it's only the second one that matters; you'll learn better
by writing whether or not other people or LLMs read it.  But in terms of
helping other people, these days
you might publish your hard-earned learnings on <a href="https://www.gilesthomas.com/2021/03/fun-with-network-namespaces">Linux Network Namespaces</a>,
but when, the next day, someone wants to find out how to use them, they ask
ChatGPT, it does a search, finds your page, ingests it, and presents the results
as its own, perhaps mashed up with some scraps from elsewhere.  Sure, your site is probably
linked in the "references" section in the response, but frankly, no-one ever looks at that.
What's worse, within the next six months your site is likely to be sucked into the
AIs' next training run, and after that you won't even get a reference.</p>

<p>Now if the "solving other people's problems" aspect of blogging was purely altruistic,
that wouldn't matter a jot.  But of course it's not, there are a bunch of other reasons.
Three that come to mind:</p>

<ol>
<li>Making a name for yourself.</li>
<li>The sheer dopamine hit of knowing that other people like what you've done --
a higher-effort version of getting an upvote or like on social media.</li>
<li>Building a portfolio of writing you can point to.</li>
</ol>

<p>Let's take those in turn.</p>

<p>If you want to blog to make a name for yourself, then you're going to have a hard
time.  Here's an example: if you're not a regular reader of this blog, where do
I (as in, the author of this post) live?  What is my day job?  No cheating and
clicking on the "About" link above, please.</p>

<p>If you knew the answer, you're one of a rare few.  Yesterday there
were about 35,000 visits to this site thanks to that HN link, and fewer than 300 hits
on the "About" page.  This is normal!  If you write a blog post, then even if people
find it interesting, they'll come, read it, hopefully think that it was worth their
time, and then move on.  That is how it should be, there's no need for someone to
become fascinated with your life just because you said something useful once -- and
that's a good thing, no-one wants a stalker.</p>

<p>Even if you churn out banger posts again and again, as a pure blogger you're not going
to build up a "personal brand" that's worth much.</p>

<p>Think about the well-known bloggers you read: they’re famous because
they did something else
important.  They started a major open-source project, or a company, or invented something.
They give regular talks at conferences.  They write successful science fiction.
Or something else.</p>

<p>So, I don't think you can make a name for yourself by blogging alone, and if you
are blogging with that as your goal, I fear you're going to be disappointed.</p>

<p>The dopamine hit is definitely more of a thing.  When people comment on my
posts, I get a nice warm glow.  And when last night, just before I went to bed, I saw that
my previous post was #1 on the front page of HN, I took a screenshot and posted it
to my "Fellow Geeks" WhatsApp group with the caption "w00t!".</p>

<p>But those moments
are rare, and I don't really think AI will make them rarer.
Blogging can sometimes feel like you're
shouting into the void -- most posts get no engagement, and that has been true since
I started back in 2006.  You might have 500 loyal
readers, or none -- there's no way to tell.</p>

<p>I think that all I can say regarding that is to echo what serviceberry
<a href="https://news.ycombinator.com/item?id=43155587">said on HN</a> (bold mine):</p>

<blockquote>
  <p>The corollary is that if you find that post, <strong>say something</strong>. Drop the author a
  note, leave a comment. No one else does. For every YT celebrity, there are
  thousands of people posting good content on the internet and not knowing if
  it's being seen or appreciated by anyone.</p>
</blockquote>

<p>...and maybe suggest that we all occasionally check the references in our helpful AI-generated
responses and drop a line to the authors to say "thanks"!</p>

<p>But let's finish with the last one, which is more positive.  I said that you will
be vanishingly unlikely to make a name for yourself with blogging on its own.  But that
doesn't mean it's pointless from a career perspective.  You're building up a portfolio
of writing about topics that interest you.  Imagine you're in a job interview and
are asked about X.  You reply with the details you know, and add
"but I blogged about that in detail a while back, shall I send you a link later?"
Or if you're aiming to close a contract with a potential consulting client in a
particular area -- wouldn't it be useful to send them a list of links showing your
thoughts on aspects of exactly that topic?</p>

<p>Your GitHub profile shows your contributions to open source and lets people know
how well you can code.  But your blog shows your contributions to knowledge, and
shows how well you can think.  That's valuable!</p>

<p>It's time to wrap this up.  Blogging is valuable because it helps you learn, because it
helps others solve problems, because you get a rare buzz when you realise that yes,
people are reading this stuff, and because you're building a portfolio of writing to show
your skills.  The only one of those that I believe AI might harm is the buzz of
engagement, and that's so rare for most blogs that I don't think it's worth worrying about.</p>

<p>And after all -- if the AI doom scenario does come true, at least as someone whose
thoughts have been regularly published on the Internet, you'll be part of the
paperclip maximisers' training set, so they'll remember you in a sense.  So
there's that.</p>

    

    
        
    

    



            
        </div></div>]]></description>
        </item>
    </channel>
</rss>