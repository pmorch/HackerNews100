<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 28 Oct 2023 17:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The UK's Controversial Online Safety Act Is Now Law (118 pts)]]></title>
            <link>https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</link>
            <guid>38048811</guid>
            <pubDate>Sat, 28 Oct 2023 10:56:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/">https://www.wired.com/story/the-uks-controversial-online-safety-act-is-now-law/</a>, See on <a href="https://news.ycombinator.com/item?id=38048811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Jeremy Wright was the first of five UK ministers charged with pushing through the British government’s landmark legislation on regulating the internet, the Online Safety Bill. The current UK government likes to brand its initiatives as “<a data-offer-url="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/&quot;}" href="https://bylinetimes.com/2023/04/28/the-uks-world-beating-rhetoric-a-distraction-from-reality/" rel="nofollow noopener" target="_blank">world-beating</a>,” but for a brief period in 2019 that might have been right. Back then, three prime ministers ago, the bill—or at least the white paper that would form its basis—outlined an approach that recognized that social media platforms were already de facto arbiters of what was acceptable speech on large parts of the internet, but that this was a responsibility they didn’t necessarily want and weren’t always capable of discharging. Tech companies were pilloried for things that they missed, but also, by free speech advocates, for those they took down. “There was a sort of emerging realization that self-regulation wasn’t going to be viable for very much longer,” Wright says. “And therefore, governments needed to be involved.”</p><p>The bill set out to define a way to handle “legal but harmful” content—material that wasn’t explicitly against the law but which, individually or in aggregate, posed a risk, such as health care disinformation, posts encouraging suicide or eating disorders, or political disinformation with the potential to undermine democracy or create panic. The bill had its critics—notably, those who worried it gave Big Tech too much power. But it was widely praised as a thoughtful attempt to deal with a problem that was growing and evolving faster than politics and society were able to adapt. Of his 17 years in parliament, Wright says, “I’m not sure I’ve seen anything by way of potential legislation that’s had as broadly based a political consensus behind it.”</p><p>Having passed, eventually, through the UK’s two houses of Parliament, the bill received royal assent today. It is no longer world-beating—the European Union’s competing <a href="https://www.theverge.com/23845672/eu-digital-services-act-explained" target="_blank">Digital Services Act</a> came into force in August. And the Online Safety Act enters into law as a broader, more controversial piece of legislation than the one that Wright championed. The act’s more than 200 clauses cover a wide spectrum of illegal content that platforms will be required to address and give platforms a “duty of care” over what their users—particularly children—see online. Some of the more nuanced principles around the harms caused by legal but harmful content have been watered down, and added in is a highly divisive requirement for messaging platforms to scan users’ messages for illegal material, such as child sexual abuse material, which tech companies and privacy campaigners say is an unwarranted attack on encryption.</p><p>Companies, from Big Tech down to smaller platforms and messaging apps, will need to comply with a long list of new requirements, starting with age verification for their users. (Wikipedia, the eighth-most-visited website in the UK, has said it <a href="https://www.bbc.co.uk/news/technology-65388255">won’t be able to comply</a> with the rule because it violates the Wikimedia Foundation’s principles on collecting data about its users.) Platforms will have to prevent younger users from seeing age-inappropriate content, such as pornography, cyberbullying, and harassment; release risk assessments on potential dangers to children on their services; and give parents easy pathways to report concerns. Sending threats of violence, including rape, online will now be illegal, as will assisting or encouraging self-harm online or transmitting deepfake pornography, and companies will need to quickly act to remove them from their platforms, along with scam adverts.</p><p>In a statement, UK Technology Secretary Michelle Donelan said: “The Bill protects free speech, empowers adults and will ensure that platforms remove illegal content. At the heart of this Bill, however, is the protection of children. I would like to thank the campaigners, parliamentarians, survivors of abuse and charities that have worked tirelessly, not only to get this Act over the finishing line, but to ensure that it will make the UK the safest place to be online in the world.”</p><p>Enforcement of the act will be left to the UK’s telecommunications regulator, Ofcom, which <a data-offer-url="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update&quot;}" href="https://www.ofcom.org.uk/online-safety/information-for-industry/roadmap-to-regulation/0623-update" rel="nofollow noopener" target="_blank">said in June</a> that it would begin consultations with industry after royal assent was granted. It’s unlikely that enforcement will begin immediately, but the law will apply to any platform with a significant number of users in the UK. Companies that fail to comply with the new rules face fines of up to £18 million ($21.9 million) or 10 percent of their annual revenue, whichever is larger.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the controversy around the act is less about what is in it and more about what isn’t. The long passage of the legislation means that its development straddled the Covid-19 pandemic, giving legislators a live view of the social impact of mis- and disinformation. The spread of anti-vaccination and anti-lockdown messages became an impediment to public health initiatives. After the worst of the pandemic was over, those same falsehoods fed into <a href="https://www.wired.com/story/15-minute-cities-conspiracy-climate-denier/">other conspiracy theories</a> that continue to disrupt society. The original white paper that was the bill’s foundation included proposals for compelling platforms to tackle this kind of content—which individually might not be illegal but which en masse creates dangers. That’s not in the final legislation, although the act does create a new offense of “false communications,” criminalizing deliberately causing harm by communicating something the sender knows to be untrue.</p><p>“One of the most important things was tackling harms that happen at scale. And because it’s focused so much on individual pieces of content, it’s missed that,” says Ellen Judson, head of the digital research hub at the think tank Demos. The act includes strict rules forcing platforms to move swiftly to remove any illegal post—such as terrorist content or child sexual abuse material—but not on disinformation campaigns comprised of a drip-drip of misleading content, failing to understand that “when that turns into things going viral and spreading, then the harm can occur cumulatively.”</p><p>Wright says that the exclusion of disinformation and misinformation from the bill was partly due to confusion between the remits of different departments. The Department of Culture, Media and Sport “was told that the Cabinet Office would be taking care of all of this. ‘Don’t you worry your pretty little heads about it, it’ll be done elsewhere in something called the <a data-offer-url="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme&quot;}" href="https://hansard.parliament.uk/commons/2019-07-22/debates/19072238000019/DefendingDemocracyProgramme" rel="nofollow noopener" target="_blank">Defending Democracy agenda</a>,’” he says. “And then I think, subsequently, it wasn’t really. So I think … there still is a gap there.”</p><p>Under the Act, bigger platforms will be expected to police potentially harmful, but not illegal, content by applying their own standards more consistently than they currently do—something that free-speech campaigners have decried as giving private companies control over what’s acceptable discourse online, but which some experts on dis- and misinformation say is a cop-out that means Big Tech will be less accountable for spreading falsehoods. Legal experts, however, say compliance with the law will require platforms to be more transparent and proactive. “They have to put all of those processes in place as to how their decisions will be made, or they risk actually being seen as a platform that is controlling all kinds of free speech,” says Emma Wright, technology lead at the law firm Harbottle &amp; Lewis. That’s likely to become quite a significant burden. “It’s the new <a href="https://www.wired.com/story/gdpr-2022/">GDPR</a>,” she says.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>By far the most divisive clause out of the more than 300 pages of the Online Safety Act is Section 122, which has been widely interpreted as compelling companies to scan users’ messages to make sure that they aren’t transmitting illegal material. That would be incredibly difficult—perhaps even impossible—to do without breaking the end-to-end encryption on platforms such as WhatsApp and Signal. End-to-end encryption means that the sender and recipient of a message can see its content but the owner of the platform that it’s sent on cannot. The only way to comply with the law, experts say, would be to put so-called client-side scanning software on users’ devices to examine messages before they’re sent, which would make the encryption largely useless. The government said during the bill’s development that companies could find a technical solution to scan messages without undermining encryption; companies and experts countered that that technology doesn’t, and may never, exist.</p><p>“That gives Ofcom, as a regulator, the ability to obligate people like us to go and put third-party content monitoring [on our products] that unilaterally scans everything going through the apps,” Matthew Hodgson, CEO of encrypted messaging company Element, told WIRED before the bill passed. “That’s undermining the encryption and providing a mechanism where bad actors of any kind could compromise the scanning system in order to steal the data flying around the place.”</p><p>Companies whose products depend on end-to-end encryption threatened to leave the country, including Signal. Meta said it may pull WhatsApp from the UK if the bill were to pass. That cliff edge has come and gone, and both services are still available—albeit after an 11th-hour restatement by the government that it wouldn’t force platforms to adopt nonexistent technology to scan users’ messages—which was seen by some as a climbdown.</p><p>However, the clause remains in the act, which worries privacy and free-speech activists, who see it as part of a spectrum of threats against encryption. If the Online Safety Act means companies have to remove encryption or circumvent it using client-side scanning, “it then potentially opens [data] up to being scooped up into the broader surveillance apparatus,” according to Nik Williams, policy and campaigns officer at the campaign group Index on Censorship.</p><p>The Online Safety Act has concerning overlaps with another piece of legislation, the Investigatory Powers Act, which allows the government to compel platforms to remove encryption. Williams says the overlap between the two pieces of legislation creates “a surveillance gateway between the OSB and the IPA in that this can give the security services, such as MI5, MI6, and GCHQ, access to data they previously could not access … I would say it’s probably an unprecedented expansion of surveillance powers.”</p><p>The morning after the Online Safety Bill passed through the House of Lords, the UK Home Office <a href="https://www.bbc.co.uk/news/technology-66854622">launched a new campaign</a> against encrypted messaging, specifically targeting Facebook Messenger.</p><p>Former minister Jeremy Wright says that the question over encryption “is frankly not resolved. I think the government has sort of dodged around giving a concluded view on what it means for encryption.” However, he says, the answer is unlikely to be as absolute as the act’s opponents are making out. Encryption won’t be banned, he says, but platforms will have to explain how their policies around it balance safety with their users’ right to privacy. “If you can meet those [safety] duties by using encryption or with encryption as part of the service, you’re fine,” he says. If not, “you have a problem … it can’t be true, surely, that a platform is entitled to say, ‘Well, I operate encryption, so that’s a get-out-of-jail-free card for me on the safety duties.’”</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google agrees to invest up to $2B in OpenAI rival Anthropic (281 pts)]]></title>
            <link>https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</link>
            <guid>38048155</guid>
            <pubDate>Sat, 28 Oct 2023 08:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/">https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</a>, See on <a href="https://news.ycombinator.com/item?id=38048155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg" srcset="https://www.reuters.com/resizer/R4Z2uz8RcluFRA-5t_A4vBPD5ZQ=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 480w,https://www.reuters.com/resizer/0JGEgq7x2OviOX_SI0PEgvGT6O4=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 960w,https://www.reuters.com/resizer/lzEztbC-1tYEiIpLH_S6m41FrYQ=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1080w,https://www.reuters.com/resizer/Ht-xjY7xrUmaianFeFUDU9ssJ_U=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="5342" height="3027" alt="An illuminated Google logo is seen inside an office building in Zurich"></p></div><p data-testid="Body"><span>An illuminated Google logo is seen inside an office building in Zurich, Switzerland December 5, 2018. REUTERS/Arnd Wiegmann/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Oct 27 (Reuters) - Alphabet's <a data-testid="Link" href="https://www.reuters.com/markets/companies/GOOGL.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(GOOGL.O)</a> Google has agreed to invest up to $2 billion in the artificial intelligence company Anthropic, a spokesperson for the startup said on Friday.</p><p data-testid="paragraph-1">The company has invested $500 million upfront into the OpenAI rival and agreed to add $1.5 billion more over time, the spokesperson said.</p><p data-testid="paragraph-2">Google is already an investor in Anthropic, and the fresh investment would underscore a ramp-up in its efforts to better compete with Microsoft <a data-testid="Link" href="https://www.reuters.com/markets/companies/MSFT.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(MSFT.O)</a>, a major backer of ChatGPT creator OpenAI, as Big Tech companies race to infuse AI into their applications.</p><p data-testid="paragraph-3">Amazon.com <a data-testid="Link" href="https://www.reuters.com/markets/companies/AMZN.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(AMZN.O)</a> also said <a data-testid="Link" href="https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/" referrerpolicy="no-referrer-when-downgrade">last month</a> it would invest up to $4 billion in Anthropic to compete with growing cloud rivals on AI.</p><p data-testid="paragraph-4">In Amazon's quarterly report to the U.S. Securities and Exchange Commission this week, the online retailer detailed it had invested in a $1.25 billion note from Anthropic that can convert to equity, while its ability to invest up to $2.75 billion in a second note expires in the first quarter of 2024.</p><p data-testid="paragraph-5">Google declined to comment, and Amazon did not immediately respond to a Reuters request for comment.</p><p data-testid="paragraph-6">The Wall Street Journal earlier reported the news of Google's latest agreement with Anthropic.</p><p data-testid="paragraph-7">The rising number of investments shows ongoing maneuvering by cloud companies to secure ties with the AI startups that are reshaping their industry.</p><p data-testid="paragraph-8">Anthropic, which was co-founded by former OpenAI executives and siblings Dario and Daniela Amodei, has shown efforts to secure the resources and deep-pocketed backers needed to compete with OpenAI and be leaders in the technology sector.</p><p data-testid="Body">Reporting by Krystal Hu in New York and Chavi Mehta in Bengaluru; Additional reporting by Jeffrey Dastin; Editing by Anil D'Silva, Devika Syamnath and Chris Reese</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Krystal reports on venture capital and startups for Reuters. She covers Silicon Valley and beyond through the lens of money and characters, with a focus on growth-stage startups, tech investments and AI. She has previously covered M&amp;A for Reuters, breaking stories on Trump's SPAC and Elon Musk's Twitter financing. Previously, she reported on Amazon for Yahoo Finance, and her investigation of the company's retail practice was cited by lawmakers in Congress. Krystal started a career in journalism by writing about tech and politics in China. She has a master's degree from New York University, and enjoys a scoop of Matcha ice cream as much as getting a scoop at work. </p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WinterJS (117 pts)]]></title>
            <link>https://wasmer.io/posts/announcing-winterjs-service-workers</link>
            <guid>38047872</guid>
            <pubDate>Sat, 28 Oct 2023 07:19:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wasmer.io/posts/announcing-winterjs-service-workers">https://wasmer.io/posts/announcing-winterjs-service-workers</a>, See on <a href="https://news.ycombinator.com/item?id=38047872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we are incredibly excited to announce <a href="https://github.com/wasmerio/winterjs">WinterJS</a> (<a href="https://wasmer.io/wasmer/winterjs"><code>wasmer/winterjs</code> package</a>).</p>
<p>WinterJS is a JavaScript Service Workers server written in Rust, that uses the SpiderMonkey runtime to execute JavaScript (the same runtime that Firefox uses). We chose to follow the <a href="https://wintercg.org/">WinterCG</a> specification to aim for maximum compatibility with other services such as Cloudflare Workers, Deno Deploy and Vercel (hence the name <em>WinterJS</em>).</p>
<p>WinterJS is not only <em>blazing fast™️</em> but can also be compiled to WebAssembly <a href="https://wasix.org/">thanks to WASIX</a> and thus also run fully with Wasmer.</p>
<p>Let's see how it works. We'll start by creating a simple <code>serviceworker.js</code> file that just returns a simple "hello world";</p>
<pre tabindex="0"><code><span><span>addEventListener</span><span>(</span><span>'fetch'</span><span>, (</span><span>req</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>  req.</span><span>respondWith</span><span>(</span><span>`hello world from ${</span><span>req</span><span>.</span><span>request</span><span>.</span><span>url</span><span>.</span><span>href</span><span>}`</span><span>);</span></span>
<span><span>});</span></span></code></pre>
<p>Running it with WinterJS is as simple as this:</p>
<pre tabindex="0"><code><span><span>$</span><span> wasmer run wasmer</span><span>/</span><span>winterjs </span><span>--</span><span>net </span><span>--</span><span>mapdir </span><span>/</span><span>app</span><span>:.</span><span> </span><span>/</span><span>app</span><span>/</span><span>serviceworker</span><span>.</span><span>js</span></span></code></pre>
<blockquote>
<p>WinterJS can also be run natively with Rust (<code>cargo install --git https://github.com/wasmerio/winterjs &amp;&amp; winterjs serviceworker.js</code>).
You can find the source code of WinterJS in the GitHub repo: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p>
</blockquote>
<p>Thanks to the WASIX capabilities of WinterJS, the JavaScript service worker can also be deployed to <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.
Check the working demo here: <a href="https://js-service-worker-demo.wasmer.app/">https://js-service-worker-demo.wasmer.app/</a></p>
<hr>
<p>And now that you have seen a sneak peak on how to use WinterJS, lets do a deep dive on our journey building it.</p>
<h2>Choosing the JS engine</h2>
<p>Before starting on the quest of creating a JavaScript Service Workers server, we analyzed the Javacript runtimes that we could use.</p>
<p>Here are the main requirements we have for such JavaScript runtime:</p>
<ul>
<li><strong>Speed</strong>: It should be fast to run</li>
<li><strong>Wasm-compatible</strong>: It should be able to run without restrictions in a Wasm environment (such as Wasmer)</li>
<li><strong>Development time</strong>: We should be able to iterate fast on it</li>
</ul>
<p>And here are the JS runtimes that we analyzed:</p>
<ul>
<li><strong>QuickJS.</strong> Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API entirely in C</li>
</ul>
</li>
<li><strong>Static Hermes.</strong> Challenges:
<ul>
<li>Node.js polyfills not available in static mode</li>
<li>Not a lot of functions (such as http calls) are available in the polyfill</li>
<li>Had to make it compile to Wasm with WASIX</li>
</ul>
</li>
<li><strong>Bun</strong> (JavascriptCore). Challenges:
<ul>
<li>Zig not fully supporting WASIX</li>
<li>Compiling JavascriptCore to WASIX is possible (was done before), but not trivial</li>
</ul>
</li>
<li><strong>MozJS</strong> (SpiderMonkey). Challenges:
<ul>
<li>We will need to implement all the JS apis diff (<code>peformance.now()</code>, <code>addEventListener</code>, …)</li>
<li>We need to implement the serviceWorker API (in Rust)</li>
<li>We will need to plug the service worker with a WASIX http server</li>
</ul>
</li>
<li><strong>Node.js</strong> (v8). Challenges:
<ul>
<li>Compile v8 in jitless mode to Wasm is an unknown-unknown</li>
</ul>
</li>
</ul>
<h2>Using SpiderMonkey with mozjs</h2>
<p>After a few runtime trials we set on SpiderMonkey as the most reasonable approach that fitted our tight timeline.</p>
<p>So we begin porting. We started with a fork of mozjs that supported a <strong><a href="https://cfallin.org/blog/2023/10/11/spidermonkey-pbl/">new compilation tier called PBI</a></strong> (Portable Baseline Interpreter).</p>
<p>After some work on the mozjs build system to target WASIX, we were able to bypass most of the issues, except one: the bindings generation.</p>
<p>The bindings that allow using the SpiderMonkey C++ API from Rust were automatically generated using c-bindgen. Plugging those bindings onto WASIX was a challenge so we simply decided to target a 32 bit system and modify them by hand (a 32,000 file!) to target <a href="https://wasix.org/">WASIX</a>.</p>
<p>And voilá… everything worked!</p>
<p>However, after adding a few missing resources to JS, we realized that perhaps mozjs didn’t have the easiest API to use:</p>
<pre tabindex="0"><code><span><span>unsafe</span><span> </span><span>extern</span><span> </span><span>"C"</span><span> </span><span>fn</span><span> </span><span>base64_encode</span><span>(cx</span><span>:</span><span> </span><span>*mut</span><span> </span><span>JSContext</span><span>, argc</span><span>:</span><span> </span><span>u32</span><span>, vp</span><span>:</span><span> </span><span>*mut</span><span> </span><span>Value</span><span>) </span><span>-&gt;</span><span> </span><span>bool</span><span> {</span></span>
<span><span>    </span><span>let</span><span> args </span><span>=</span><span> </span><span>CallArgs</span><span>::</span><span>from_vp</span><span>(vp, argc);</span></span>
<span></span>
<span><span>    </span><span>if</span><span> args</span><span>.</span><span>argc_ </span><span>&lt;</span><span> </span><span>1</span><span> {</span></span>
<span><span>        </span><span>return</span><span> </span><span>false</span><span>;</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    </span><span>let</span><span> source </span><span>=</span><span> </span><span>js_try!</span><span>(cx, </span><span>raw_handle_to_string</span><span>(cx, args</span><span>.</span><span>get</span><span>(</span><span>0</span><span>)));</span></span>
<span><span>    </span><span>let</span><span> result </span><span>=</span><span> </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes);</span></span>
<span></span>
<span><span>    </span><span>rooted!</span><span>(</span><span>in</span><span>(cx) </span><span>let</span><span> </span><span>mut</span><span> rval </span><span>=</span><span> </span><span>UndefinedValue</span><span>());</span></span>
<span><span>    result</span><span>.</span><span>to_jsval</span><span>(cx, rval</span><span>.</span><span>handle_mut</span><span>());</span></span>
<span></span>
<span><span>    args</span><span>.</span><span>rval</span><span>()</span><span>.</span><span>set</span><span>(rval</span><span>.</span><span>get</span><span>());</span></span>
<span></span>
<span><span>    </span><span>true</span></span>
<span><span>}</span></span></code></pre>
<h2>Using SpiderMokey with spiderfire</h2>
<p>Thankfully, the <a href="https://github.com/Redfire75369/spiderfire/">spiderfire</a> project had been working on improving the API surface for using SpiderMonkey from Rust.</p>
<p>So the example laid out before now looks way simpler and easier to read/maintain:</p>
<pre tabindex="0"><code><span><span>#[js_fn]</span></span>
<span><span>fn</span><span> </span><span>btoa</span><span>&lt;'</span><span>cx</span><span>&gt;(val</span><span>:</span><span> </span><span>String</span><span>) </span><span>-&gt;</span><span> </span><span>String</span><span> {</span></span>
<span><span>    </span><span>let</span><span> bytes </span><span>=</span><span> val</span><span>.</span><span>as_bytes</span><span>();</span></span>
<span><span>    </span><span>::</span><span>base64</span><span>::</span><span>engine</span><span>::</span><span>general_purpose</span><span>::</span><span>STANDARD</span><span>.</span><span>encode</span><span>(bytes)</span></span>
<span><span>}</span></span></code></pre>
<h2>Deploying to Wasmer Edge</h2>
<p>Compiling WinterJS to WASIX was challenging, but completely worth it. Thanks to its WASIX capabilities we can now run any Javascript Service Workers workloads in <a href="https://wasmer.io/products/edge">Wasmer Edge</a>.</p>
<p>We have put together an <strong>in depth tutorial on how to use Javascript Service Workers in Wasmer Edge</strong>... please check it out!</p>
<p><a href="https://docs.wasmer.io/edge/quickstart/js-wintercg">https://docs.wasmer.io/edge/quickstart/js-wintercg</a></p>
<hr>
<p>We believe WinterJS will enable many new use cases. For example, running Service Workers natively in your IoT device (where Node is too heavy to run), or even in your browser.</p>
<p>At Wasmer we are incredibly excited to see how you will use WinterJS.</p>
<p>WinterJS on GitHub: <a href="https://github.com/wasmerio/winterjs">https://github.com/wasmerio/winterjs</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The fun factor of the video game Uplink (263 pts)]]></title>
            <link>https://vertette.github.io/post/funfactoruplink</link>
            <guid>38047861</guid>
            <pubDate>Sat, 28 Oct 2023 07:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vertette.github.io/post/funfactoruplink">https://vertette.github.io/post/funfactoruplink</a>, See on <a href="https://news.ycombinator.com/item?id=38047861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Remember family computers? Before we had tablets, middle class families would buy overpriced computers from dodgy computer stores that the whole family got to share. As they were mostly used by children and adults who didn't have the slightest understanding of technology, it didn't take long for the family computer to down to a crawl, plagued by dodgy Kazaa downloads and suspicious Internet Explorer toolbars. Between a lack of money to buy a better computer, no easy way to buy digital games in Europe for the longest time and no nearby stores that sold computer games, it took until my brother moved out of the home before I really started getting into PC gaming.</p>
<p>My mom bought a gaming PC for my 14th birthday, and while it was incredibly overpriced for what it was, that didn't matter to me; it was mine and I could do whatever I wanted with it. As the only computer games I was familiar with were <a href="https://www.newgrounds.com/games">free Flash games on Newgrounds</a>, I asked around on various communities what kind of games I could play on a slightly outdated hunk of junk like mine. I forgot who exactly was responsible, but someone recommended I try this old hacking game called <a href="https://store.steampowered.com/app/1510/Uplink/"><em>Uplink</em></a>, and so I found a torrent on ThePirateBay and tried it. I make the following statement with zero hyperbole - that single throwaway recommendation changed my life. Various hours later, I fell in love with it so hard that I made a Steam account just so I could buy the game and support the developers properly.</p>
<p><img src="https://vertette.github.io/img/uplink_hardware.png" alt="A screenshot of the Uplink hardware upgrade screen">
<em>Uplink's interface might be outdated and slightly janky, but dammit, it still looks very cool.</em></p>
<p>If you're unfamiliar with it, here's how it works: <em>Uplink</em> is a hacking simulator reminiscent of old hacking movies like <a href="https://www.imdb.com/title/tt0105435/"><em>Sneakers</em></a> and <a href="https://www.imdb.com/title/tt0113243/"><em>Hackers</em></a>, where the portrayal is less about realism and more about flashiness. You play as a hacker who does various odd jobs like changing people's identities or destroying valuable data. At the beginning of the game, hacking is as easy as using the password breaker on a password screen and finishing up in less than five minutes to avoid getting caught, but the game quickly starts bombarding you with new concepts - deleting logs to avoid being tracked down, shutting down security systems that get in your way and travelling through local area networks. The game is never outright unfair, as most information you need to get through the game can be found in the in-game help section, but certain concepts require a bit of trial and error before you truly get how they work and that can result in you getting caught by the authorities, which results in an instant game over. Thankfully, starting over and getting back to where you were before isn't as daunting as it seems due to <em>Uplink</em>'s fairly open structure. While the game is a bit on the short side, there's enough depth to its mechanics to feel satisfying to master, and the realization that a game that gave you so much trouble at first has turned into a total cakewalk can't be matched.</p>
<p>Before <em>Uplink</em>, I only really played games like <em>Mario</em>, <em>Grand Theft Auto</em> and <em>Alien Hominid</em>; games that might or might not feature mature content, but were decidedly arcadey in nature. They didn't care that much about immersion or emotional engagement. <em>Uplink</em> was a different beast: it pulled me right in with its <a href="https://www.youtube.com/watch?v=QliQ0livbeQ">beautiful ambient soundtrack</a>, retrofuturistic visuals and gameplay that was unlike anything I had ever experienced. Sure, it might not have any fancy 3D models and complex shaders, but I still felt absorbed in a way no other game had done before. Its gameplay was highly addictive and its presentation deceptively brilliant, with a story that would've been deemed too ambitious for an AAA game even at the time. I became an obsessed man, looking up everything that I could find about the game and its developers, buying their newer games <a href="https://store.steampowered.com/app/1500/Darwinia/"><em>Darwinia</em></a> and <a href="https://store.steampowered.com/app/1520/DEFCON/"><em>DEFCON</em></a> and reading the <em>Uplink</em> design documents on the Bonus Disk religiously. Before <em>Uplink</em>, I never gave game design much consideration. I never thought about all the possibilities games have to tell unique stories or how certain game mechanics can make you feel certain emotions. So what is it about the gameplay that makes it so engaging, so immersive and so much fun? Well, the answer might not be what you'd expect: even though there's plenty to praise about <em>Uplink</em>'s design, it manages to be so engaging and immersive because it isn't actually that much fun.</p>
<p><img src="https://vertette.github.io/img/uplink_hack.png" alt="A screenshot of an Uplink hack in progress">
<em>Fun fact: the Trace Tracker's beeps were a last minute addition. There is a world map upgrade that shows you exactly how far the administrator's trace is, but nobody buys it because it doesn't beep.</em></p>
<p>That might make it sound like yet another pretentious indie game that sacrificies good gameplay in service of a Very Important Message™, but that's not actually the case. The anticipation of planning your next attack, the tension as the trace tracker's beeps become quicker and quicker as the system administrator starts closing in on you, the euphoria of a successful job that gets quickly swallowed up by the creeping paranoia of whether you properly correctly cleaned up after yourself or not - <em>Uplink</em> is a hurricane of emotions, but a lot of the emotions it invokes aren't exactly what you'd call positive ones. In that sense, while the game <em>can</em> be fun, it can also feel very tense, obtuse and frustrating, and that's important. Without that, the experience would not nearly be as effective at making you feel like a real hacker as it is, even if the moment-to-moment gameplay pretty much boils down to a script kiddie simulator. The way it goes about it elevates it to something much grander, something truly innovative and memorable, and in that sense "fun" is simply too limiting a term to describe <em>Uplink</em>'s design.</p>
<p>That might sound silly to a lot of players, because "if the game's not fun, why bother", right? But there is an actual precedence for this claim, for example horror games. Most people play horror games not to feel amused but to feel spooked, and those two emotions are almost directly on the opposite end of the emotion wheel. If a horror game is fun to you, then it's doing a very bad job. Another good example is <em>Pathologic</em>, a game that deliberately goes out of its way to be an unpleasant experience to sell the setting of a plague-ridden town in a very effective and memorable way. Even though I find it hard to recommend it, I also find it hard to dismiss it as not being worth your time. And I don't want to ruin people's laughs from back when <a href="https://www.gamesradar.com/we-dont-use-the-word-fun-says-the-last-of-us-2-director-neil-druckmann/&amp;utm_campaign=buffer_grtw/">Neill Druckmann infamously claimed they didn't use the word "fun" during development of <em>The Last of Us 2</em></a>, but that <em>is</em> a valid way to design your game. The way he phrased it made it come across as more pretentious than he meant it to, but the gameplay of <em>TLOU2</em> invokes a lot of the same emotions that <em>Uplink</em> does: tension, paranoia, and euphoria. If games really are an art form, then limiting the design to what's fun is ignoring so many other emotional reactions your game can inspire in others.</p>
<p>That doesn't mean that designing a fun game isn't valuable, but it <em>does</em> mean that it's worth exploring emotions through your game that aren't directly adjacent to fun. With all the opportunities the medium of video games has over others, it would be a waste not to.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cortex X2: ARM aims high (145 pts)]]></title>
            <link>https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</link>
            <guid>38047743</guid>
            <pubDate>Sat, 28 Oct 2023 06:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/">https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/</a>, See on <a href="https://news.ycombinator.com/item?id=38047743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Arm has traditionally targeted the low end of the power and performance curve, but just as Intel has been looking to expand into the low power market, ARM is looking to expand into higher power and performance segments. The Cortex X series is at the forefront of this effort.</p>
<blockquote>
<p>Delivers ultimate peak performance within an expanded envelope for power and area.</p>
<cite><a href="https://www.arm.com/products/cortex-x">Cortex-X Custom CPU Program</a>, Arm</cite></blockquote>
<p>Here, we’ll be looking at the Cortex X2 as implemented in the Snapdragon 8+ Gen 1. This SoC features a single X2 core, alongside four Cortex A510 and three Cortex A710 cores. The Cortex X2 in this SoC typically runs at 2.8 GHz, although lscpu indicates its clock speed can range from 787.2 MHz to 3.187 GHz. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?ssl=1"><img data-attachment-id="20270" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_boost/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-orig-size="971,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_boost" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=971%2C476&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?fit=688%2C337&amp;ssl=1" decoding="async" width="688" height="337" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?w=971&amp;ssl=1 971w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_boost.png?resize=768%2C376&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Tested on an Asus Zenfone 9</figcaption></figure></div>
<p>When placed under load, the Cortex X2 quickly boosts to an intermediate clock speed of 2.56 GHz. After 55 ms, it reaches 2.8 GHz. No higher clock speeds were observed when testing over a longer duration.</p>
<h2>Core Overview</h2>
<p>Cortex X2 is similar to its 7-series cousin, the Cortex A710, but is substantially larger. X2 has more reordering capacity, a wider pipeline, and more execution units. Despite these changes, X2 has a 10-stage pipeline just like A710.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?ssl=1"><img data-attachment-id="23077" data-permalink="https://chipsandcheese.com/cortex_x2-drawio-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-orig-size="1392,1188" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=1392%2C1188&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?fit=688%2C587&amp;ssl=1" decoding="async" width="688" height="587" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=688%2C587&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?w=1392&amp;ssl=1 1392w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=768%2C655&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1200%2C1024&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2.drawio.jpg?resize=1320%2C1127&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<h2>Branch Prediction</h2>
<p>Branch prediction is important for any CPU because wasted work from mispredicts will hurt both performance and power efficiency. Cortex X2 gets more area and power budget than other ARM cores, and therefore gets a more capable branch predictor. It can recognize somewhat longer patterns than Cortex A710 and its server cousin, Neoverse N2. Alongside that, it does better when there are a ton of branches in play. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="20169" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/yitian710_branchhist.png?ssl=1" alt="" width="1154" height="690"></p></figure>
<p>However, ARM’s statement that X2 has an “expanded envelope for power and area” has to be taken in context. X2 still goes into mobile chips even if high end SoCs only feature a single X-series core. Passive smartphone cooling means X2 is still working within a much tighter power budget than desktop CPUs. AMD’s Zen 4 in comparison pulls all the stops to maximize branch prediction accuracy. </p>
<figure><p><img decoding="async" id="20275" src="https://i1.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_branchhist.png?ssl=1" alt="" width="1162" height="691"><img decoding="async" loading="lazy" id="11077" src="https://i2.wp.com/chipsandcheese.com/wp-content/uploads/2022/10/zen4_bpu_pattern.png?ssl=1" alt="" width="1159" height="679"></p></figure>
<p>The branch predictor’s job is to make sure the frontend is well-fed with fetch addresses. Accurately predicting branch direction is one component of this. Another component is delivering those fetch addresses quickly. To do so, the branch predictor keeps a cache of branch destinations, called a branch target buffer (BTB). </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?ssl=1"><img data-attachment-id="20280" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_btb/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-orig-size="1215,557" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_btb" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=1215%2C557&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?fit=688%2C315&amp;ssl=1" decoding="async" loading="lazy" width="688" height="315" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=688%2C315&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?w=1215&amp;ssl=1 1215w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=768%2C352&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_btb.png?resize=1200%2C550&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Cortex X2’s BTB is mostly unchanged from A710’s. A micro-BTB can handle two taken branches per cycle, and can track up to 64 branches. Then we see about 10K branches tracked with 1-2 penalty cycles. Returns are handled with a 14 entry return stack as well.</p>
<h2>Frontend: Fetch and Decode</h2>
<p>Cortex X2 has an enlarged version of A710’s frontend, and enjoys both increased caching capacity and higher throughput. The micro-op cache grows to 3072 entries, making it larger than Sunny Cove’s. Also, X2 mandates a 64 KB instruction cache, while A710 implementers could pick between 32 KB or 64 KB instruction cache. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?ssl=1"><img data-attachment-id="20286" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_frontend-drawio-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-orig-size="1613,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_frontend.drawio-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=1613%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?fit=688%2C175&amp;ssl=1" decoding="async" loading="lazy" width="688" height="175" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=688%2C175&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1613&amp;ssl=1 1613w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=768%2C196&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1536%2C391&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1200%2C306&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1600%2C408&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?resize=1320%2C336&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/cortex_x2_frontend.drawio-1.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Compared to AMD’s Zen 4, X2’s micro-op cache is smaller, but its larger instruction cache is a notable advantage for larger code footprints. If code footprints exceed 32 KB (but not 64 KB), and have a lot of unpredictable branches, Zen 4 will suffer from L2 latency and see a lot of frontend bubbles.</p>
<p>In terms of throughput, X2’s micro-op cache can provide 8 operations per cycle, which is more than enough to feed the 6-wide renamer downstream. The 5-wide decoder can provide generous instruction throughput for larger code footprints and compares favorably to the 4-wide decoders found on Zen 4 and A710. X2 can sustain more than four instructions per cycle even when running code from L2.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?ssl=1"><img data-attachment-id="20289" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_ifetch/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-orig-size="1086,465" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_ifetch" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=1086%2C465&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?fit=688%2C295&amp;ssl=1" decoding="async" loading="lazy" width="688" height="295" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=688%2C295&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?w=1086&amp;ssl=1 1086w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_ifetch.png?resize=768%2C329&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>However once you get past L2, Zen 4 pulls ahead again. Thanks to AMD’s very high performance L3 and an aggressive branch predictor, Zen 4 can sustain over 3 IPC when running code from L3. Cortex X2 doesn’t do badly and can still average 1.66 IPC in that case.</p>
<h2>Out of Order Execution</h2>
<p>After micro-ops from the frontend have been renamed, out-of-order execution tracks and executes them as their data dependencies become available. X2 has a much larger OoO engine than A710 while enjoying similar instruction fusion optimizations. ROB size increased to 288 entries with other structure sizes scaled up to match.</p>
<figure><table><tbody><tr><td>Structure</td><td>Entry required if instruction…</td><td>Cortex X2 Capacity</td><td>A710 Capacity</td><td>Zen 4 Capacity</td></tr><tr><td>Reorder Buffer</td><td>exists</td><td>288</td><td>160</td><td>320</td></tr><tr><td>Integer Register File</td><td>writes to an integer register</td><td>~213</td><td>~147</td><td>224</td></tr><tr><td>FP/Vector Register File</td><td>writes to a FP/vector register</td><td>~156x 128-bit</td><td>~124x 128-bit</td><td>192x 512-bit</td></tr><tr><td>Flags Register File</td><td>sets condition flags</td><td>70</td><td>46</td><td>108 documented<br>238 measured</td></tr><tr><td>Load Queue</td><td>reads from memory</td><td>174</td><td>64</td><td>88 documented<br>136 measured</td></tr><tr><td>Store Queue</td><td>writes to memory</td><td>72</td><td>36</td><td>64</td></tr><tr><td>Branch Order Buffer</td><td>potentially affects control flow (NT branches tested here)</td><td>68</td><td>44</td><td>118</td></tr></tbody></table></figure>
<p>X2 ends up getting close to Zen 4 in most areas, and even exceeds it in a few. ARM’s core can keep a staggering number of loads in flight. Instruction fusion allows it to track 249 FP operations pending retirement, while Zen 4 can only track 154. However, Zen 4 does better if 512-bit vectors are used because its large AVX-512 register file lets it keep a lot more explicitly parallel work in flight.</p>
<p>A710 had an overbuilt scheduler considering its ROB capacity and other structure sizes. Cortex X2 brings things back into balance. Integer scheduler capacity is surprisingly similar to Zen 4’s, with four 24 entry queues. Zen 4 shares those scheduler queues with the AGUs, while Cortex X2 has separate AGU schedulers.</p>
<h3>FP/Vector Execution</h3>
<p>Arm’s Cortex 7 series cores had weak vector execution thanks to tight area and power constraints. Cortex X2 uses its larger power and area budget to implement a quad-pipe FP and vector setup. All four pipes can handle common math operations and enjoy the same low floating point execution latency that Cortex A710 does. Cortex X2 is therefore a very strong contender for scalar or 128-bit vector operations. </p>
<p>I wasn’t able to fully utilize all four pipes even with instructions that should have been able to do so (according to the optimization guide), but even so, throughput is very good.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex A710</td><td>Zen 4</td></tr><tr><td>FP32 Add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr><tr><td>FP fused multiply-add</td><td>2.53 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td><td>2 per cycle<br>4 cycle latency</td></tr><tr><td>128-bit vector INT32 add</td><td>2.53 per cycle<br>2 cycle latency</td><td>2 per cycle<br>2 cycle latency</td><td>4 per cycle<br>1 cycle latency</td></tr><tr><td>128-bit vector INT32 multiply</td><td>1.26 per cycle<br>4 cycle latency</td><td>1 per cycle<br>4 cycle latency</td><td>2 per cycle<br>3 cycle latency</td></tr></tbody></table><figcaption>Latency and throughput is identical for vector versions of those FP operations</figcaption></figure>
<p>Zen 4 still has an advantage with longer vector lengths and lower latency for vector integer operations. But even if Zen 4 uses 256-bit vectors, Cortex X2 can put up a decent fight because it has identical theoretical throughput (per cycle) for common operations. For example, Zen 4 can do two 256-bit FMAs per cycle. Cortex X2 can match that by doing four 128-bit FMAs. AMD’s core also enjoys better scheduling capacity. X2 seems to have a pair of 23 entry schedulers. I couldn’t find any operations that only go to one of the ADDV pipes, so I can’t tell if it’s a single 23 entry queue, or a 11+12 entry setup. I think a pair of dual port schedulers is more likely. AMD’s Zen 4 uses a pair of 32 entry schedulers, giving it 64 FP scheduling entries compared to Cortex X2’s 46.</p>
<figure><img data-attachment-id="20349" data-permalink="https://chipsandcheese.com/x2_fp_exec/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-orig-size="888,275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_fp_exec" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=888%2C275&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?fit=688%2C213&amp;ssl=1" decoding="async" loading="lazy" width="688" height="213" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=688%2C213&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?w=888&amp;ssl=1 888w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_fp_exec.jpg?resize=768%2C238&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure>
<p>Like Zen 4, X2 has a non-scheduling queue (NSQ) in front of the FP schedulers, which lets the core track more incomplete operations without using a larger scheduler. An NSQ can contain a lot more entries than a scheduling queue, because it doesn’t have to check each entry each cycle to see if it’s ready for execution. With its 29 entry NSQ, Cortex X2 can keep a total of 75 incomplete FP operations in flight. X2 is an improvement over A710, but AMD prioritizes FP execution more. Zen 4 uses a larger 64 entry non-scheduling queue and can keep a total of 128 incomplete FP operations in flight.</p>
<h3>Memory Execution</h3>
<p>Cortex X2 handles memory accesses with three address generation units (AGUs), with some similarities to A710 and Zen 4. The memory subsystem can handle three memory accesses per cycle, of which three can be loads and two can be stores. Its scheduling setup appears similar to the one on <a href="https://chipsandcheese.com/2023/09/11/hot-chips-2023-arms-neoverse-v2/">Neoverse V2</a>, but with slightly smaller scheduling queues and tiny non-scheduling queues in front of them.</p>
<p>After addresses are calculated, the load/store unit has to ensure they appear to execute in program order. Loads might have to get their data from prior in-flight stores. Ideally, data from the store gets sent to a dependent load with minimal delay. But detecting dependencies can be complicated because loads and stores can overlap without matching addresses.</p>
<p>Cortex X2 acts a lot like prior ARM cores starting from Neoverse N1. The load/store unit can forward either half of a 64-bit load to a dependent 32-bit store, but can’t handle any other cases. Fast-path store forwarding has a latency of five cycles, while the slow path incurs a 10-11 cycle penalty.</p>

<p>Zen 4 has a far more robust mechanism for resolving memory dependencies. Any load contained within a prior store can have its data forwarded, and exact address matches can be handled with zero latency. ARM is falling a bit behind here with essentially pre-2010s forwarding capability on a core design going for ultimate performance. However, the slow fallback path on Zen 4 is more expensive at 19-20 cycles, likely indicating Zen 4 has more pipeline stages between between address calculation and store retirement.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?ssl=1"><img data-attachment-id="20370" data-permalink="https://chipsandcheese.com/zen4_stlf-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2718%2C1189&amp;ssl=1" data-orig-size="2718,1189" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=2560%2C1120&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?fit=688%2C301&amp;ssl=1" decoding="async" loading="lazy" width="688" height="301" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=688%2C301&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=768%2C336&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1536%2C672&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=2048%2C896&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1200%2C525&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1600%2C700&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?resize=1320%2C577&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Same test on Zen 4</figcaption></figure></div>
<p>Cortex X2 does better with avoiding misalignment penalties. Zen 4’s data cache has 32B store alignment, so stores that cross a 32B aligned boundary have a throughput of one per two cycles. X2 doesn’t see any penalty unless accesses cross a 64B cacheline boundary.</p>
<p>Henry Wong <a href="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/" data-type="link" data-id="https://blog.stuffedcow.net/2014/01/x86-memory-disambiguation/">experimented</a> with smaller load and store sizes and didn’t see a significant difference. However, vector loads do behave differently on on some CPUs. Cortex X2 can again forward either 64-bit half of a 128-bit store, but curiously can also forward the low 32 bits and merge that with another 32 bits from the data cache to quickly complete a partially overlapping 64-bit load.</p>
<figure><img data-attachment-id="20355" data-permalink="https://chipsandcheese.com/x2_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2738%2C1321&amp;ssl=1" data-orig-size="2738,1321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=2560%2C1235&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?fit=688%2C332&amp;ssl=1" decoding="async" loading="lazy" width="688" height="332" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=688%2C332&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=2738&amp;ssl=1 2738w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=768%2C371&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1536%2C741&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=2048%2C988&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1200%2C579&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1600%2C772&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?resize=1320%2C637&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>Using str q,[x] and ldr d, [x]</figcaption></figure>
<p>Zen 4’s vector side acts a lot like the scalar integer side, but with a couple cycles of additional latency. AMD can impressively handle misaligned loads with no cost, but again is more prone to hitting misaligned store penalties than Cortex X2. </p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?ssl=1"><img data-attachment-id="20371" data-permalink="https://chipsandcheese.com/zen4_128_stlf/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2718%2C1171&amp;ssl=1" data-orig-size="2718,1171" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen4_128_stlf" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=2560%2C1103&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?fit=688%2C296&amp;ssl=1" decoding="async" loading="lazy" width="688" height="296" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=2718&amp;ssl=1 2718w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=768%2C331&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1536%2C662&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=2048%2C882&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1200%2C517&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1600%2C689&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?resize=1320%2C569&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/zen4_128_stlf.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Using movups store and movsd load</figcaption></figure></div>
<h3>Address Translation</h3>
<p>User programs don’t directly address locations in DRAM. Instead, they use virtual addresses, and the operating system sets up a map of virtual address to physical addresses for each process. This allows cool things like swapping to disk when physical memory runs low. However, hardware has to translate addresses on the fly while maintaining high performance. Translation lookaside buffers (TLBs) cache virtual to physical address mappings. TLB hits let the CPU avoid traversing the operating system’s paging structures, which would turn one memory access into several dependent ones.</p>
<p>Cortex X2 has a two-level TLB setup. The first TLB level has 48 entries and is fully associative. It’s a welcome size increase over the 32 entries in A710, but is still smaller than Zen 4’s 72 entry DTLB.</p>
<div>
<figure><a href="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?ssl=1"><img data-attachment-id="20377" data-permalink="https://chipsandcheese.com/x2_tlb-drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-orig-size="1355,402" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_tlb.drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=1355%2C402&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?fit=688%2C204&amp;ssl=1" decoding="async" loading="lazy" width="688" height="204" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=688%2C204&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?w=1355&amp;ssl=1 1355w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=768%2C228&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1200%2C356&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/08/x2_tlb.drawio.png?resize=1320%2C392&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L1 DTLB misses can be caught by Cortex X2’s 2048 entry L2 TLB, with a cost of 5 extra cycles. This is a welcome improvement over the Cortex A710’s 1024 entry TLB, and Neoverse N2’s 1280 entries. Cortex X2’s improved TLB sizes let it incur less address translation latency for programs with larger data footprints. It’s still a step behind Zen 4’s 3072 entry L2 TLB, but it matches Zen 2.</p>
<h2>Cache and Memory</h2>
<p>Caching is an important component of a CPU’s performance. In the Snapdragon 8+ Gen 1, Cortex X2 gets a triple level cache hierarchy. The large 64 KB L1D has 4 cycle latency. It’s not the best for a CPU clocked below 3 GHz, considering the old AMD Athlon and Phenom CPUs achieved 3 cycle L1D latency years ago. As a consolation, indexed addressing doesn’t cost an extra cycle like on recent AMD and Intel CPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23091"><img data-attachment-id="23091" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-orig-size="1239,604" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=1239%2C604&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?fit=688%2C335&amp;ssl=1" decoding="async" loading="lazy" width="688" height="335" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=688%2C335&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?w=1239&amp;ssl=1 1239w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=768%2C374&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_latency.png?resize=1200%2C585&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Arm mandates a 64 KB L1D on Cortex X2, but lets implementers configure the L2 with 512 KB or 1 MB of capacity. The L2 is inclusive of the L1D, so Arm is making a good decision in not offering smaller L2 options. Both L2 configurations have 8-way associativity, so Arm is changing capacity by increasing the number of sets. Qualcomm picked the 1 MB option on the Snapdragon 8+ Gen 1. L2 hits have 11 cycle latency, which comes out to just under 4 nanoseconds. Cortex X2 can’t clock as high as Zen 4, but the short L2 pipeline helps close some of the gap. Just like the L1D, the L2 is always ECC protected. I’m glad Arm isn’t making ECC protection optional.</p>
<p>The L2 has a 256-bit bus to the DSU-110, which connects cores to the rest of the system. Arm lets implementers configure the DSU-110 with up to 16 MB of L3 cache. The L3 is 16-way set associative with power of two capacities, or 12-way set associative if capacity is divisible by 3. Qualcomm in their infinite wisdom has chosen 6 MB of L3 cache, so the Snapdragon 8+ Gen 1’s L3 is 12-way set associative.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23114"><img data-attachment-id="23114" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_layout/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-orig-size="632,334" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_layout" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?fit=632%2C334&amp;ssl=1" decoding="async" loading="lazy" width="632" height="334" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_layout.png?resize=632%2C334&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Figure from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>The L3 is arranged into slices, and is filled by victims from core private caches. Cortex X2 suffers higher L3 latency than Zen 4. At the 4 MB test size, its 18.18 ns result is similar to the 17.41 ns seen by the Intel Core i9-12900K’s E-cores. A small 6 MB cache should make up for its lack of capacity by at least being fast, but I suppose that would be asking too much from a mobile SoC. At least it’s reasonable from the ~51 core cycle latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23124"><img data-attachment-id="23124" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/dsu-110_l3_configurable_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-orig-size="623,229" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dsu-110_l3_configurable_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?fit=623%2C229&amp;ssl=1" decoding="async" loading="lazy" width="623" height="229" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/dsu-110_l3_configurable_cycles.png?resize=623%2C229&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>Part of the L3 pipeline is configurable. Image from Arm’s DSU-110 Technical Reference Manual</figcaption></figure></div>
<p>Arm’s Technical Reference manual suggests five to seven cycles are spent accessing L3 data storage, so the remaining cycles are spent checking tags, traversing the interconnect, and at upper level caches. Program-visible L3 latency includes time spent accessing the L2 TLB, since the L1 TLB is not large enough to cover the L3 cache.</p>
<p>At the 1 GB test size, we see 202 ns of DRAM latency. L2 TLB misses and page walks add potentially heavy address translation latency on top, but separating that from DRAM latency is difficult because there’s no way to use huge pages on Android. It’s not too bad for a cell phone SoC, but is a world apart from desktop or laptop CPUs. It’s also worse than Apple’s M1, which should worry Qualcomm because Apple shares designs across phones and tablets.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_m1_latency/"><img data-attachment-id="23150" data-permalink="https://chipsandcheese.com/x2_m1_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-orig-size="1374,714" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_m1_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=1374%2C714&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?fit=688%2C358&amp;ssl=1" decoding="async" loading="lazy" width="688" height="358" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=688%2C358&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?w=1374&amp;ssl=1 1374w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=768%2C399&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1200%2C624&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_m1_latency.png?resize=1320%2C686&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Apple’s 12 MB shared L2 serves the same role as the Snapdragon 8+ Gen 1’s 6 MB L3, but has both higher capacity and lower latency. I wonder how Cortex X2 would do if it were better fed.</p>
<h3> Bandwidth</h3>
<p>Cortex X2’s three AGUs and triple port data cache allow it to service three 128-bit accesses per cycle. The core therefore can get the same per-cycle L1D bandwidth as A710 and Apple’s M1, and beats older Arm cores like the Neoverse N1 by a large margin. Apple’s M1 still gets an absolute bandwidth lead thanks to higher clocks. Compared to recent x86 cores, X2’s L1D bandwidth is still low due to lower clocks and lack of wider vector support.</p>
<div>
<figure><a href="https://chipsandcheese.com/cortex_x2_read_vs_arm/"><img data-attachment-id="23128" data-permalink="https://chipsandcheese.com/cortex_x2_read_vs_arm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-orig-size="1243,581" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_read_vs_arm" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=1243%2C581&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?fit=688%2C322&amp;ssl=1" decoding="async" loading="lazy" width="688" height="322" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=688%2C322&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?w=1243&amp;ssl=1 1243w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=768%2C359&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_read_vs_arm.png?resize=1200%2C561&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 bandwidth is decent at 28 bytes per cycle. It’s close to Apple M1’s L2 bandwidth. Zen 4 and Skylake again enjoy a large L2 bandwidth lead over Cortex X2 thanks to higher clock speeds.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23092"><img data-attachment-id="23092" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/cortex_x2_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-orig-size="1248,578" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cortex_x2_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=1248%2C578&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?fit=688%2C319&amp;ssl=1" decoding="async" loading="lazy" width="688" height="319" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=688%2C319&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?w=1248&amp;ssl=1 1248w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=768%2C356&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/cortex_x2_bw.png?resize=1200%2C556&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L2 misses go into a transaction queue with size configurable from 72 to 96 entries. The large transaction queue helps the core cope with high L3 latency, so X2’s L3 bandwidth is on par with Skylake. DRAM bandwidth from the single Cortex X2 core is decent at 32.5 GB/s, hinting at the L3’s ability to track a lot of pending misses. The DSU-110’s CHI (Coherent Hub Interface) can track up to 128 reads per master port. If Qualcomm is using that to connect memory controllers, it would explain the decent memory bandwidth in the face of high latency.</p>
<h3>Write Bandwidth</h3>
<p>We can examine bandwidth without latency restrictions by testing writes instead of reads. Normally, writes have much lower bandwidth because a write access involves a read-for-ownership first to fill the line into cache. However, Cortex X2 detects when entire cachelines are being overwritten without any of the data getting read. If that happens to enough consecutive lines, the core’s bus interface switches into write streaming mode. In write streaming mode, cache misses don’t cause fills and simply write out the data. Thus, writes won’t be held back by read latency and RFO bandwidth won’t compete with writebacks.</p>
<div>
<figure><a href="https://chipsandcheese.com/x2_write/"><img data-attachment-id="23133" data-permalink="https://chipsandcheese.com/x2_write/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-orig-size="1244,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="X2_write" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=1244%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?fit=688%2C320&amp;ssl=1" decoding="async" loading="lazy" width="688" height="320" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?w=1244&amp;ssl=1 1244w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=768%2C357&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/X2_write.png?resize=1200%2C559&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Bandwidth from L1D is lower because only two AGUs can handle writes. But every lower level in the cache hierarchy benefits. L2 bandwidth goes up to 30 bytes per cycle, while L3 bandwidth reaches 67 GB/s. Finally, DRAM bandwidth sits around 41.2 GB/s. I suspect that’s a better reflection of what the memory controller can deliver using its 64bit LPDDR5-6400 interface.</p>
<h2>Final Words</h2>
<p>Arm’s Cortex X line reaches for higher performance with an increased power and area budget. Cortex X2 is the second member of that line. It apparently has an <a href="https://www.eetasia.com/express/mediatek-delivers-efficient-cortex-x2/">area of about 2.1 mm<sup>2</sup></a>, making it just slightly smaller than Zen 4c. While Arm tries to move up the performance ladder, Intel and AMD are trying to move down to hit lower power and area targets. Arm’s efforts to move up the performance ladder mean it’s starting to overlap with AMD and Intel as those x86 companies try to move down into lower power and area targets.</p>
<p>AMD, Arm, and Intel share another commonality. They all have to maintain multiple cores to broaden their coverage of performance targets. AMD has the most modest and cost efficient approach. Zen 4c uses a different physical implementation of the Zen 4 architecture to reduce core area at the cost of clock speed. Intel goes all the way for maximum flexibility. Gracemont is a completely different core than Golden Cove, so Intel is splitting engineering effort between two core lines. Arm lands in the middle. Cortex X2 is a scaled up A710. The two cores have similar scheduler layouts and instruction fusion optimizations, so they’re really siblings rather than completely different designs. Some of Arm’s engineering effort can be shared across both cores, but additional time has to be spent tuning and validating A710 and X2.</p>
<p>To build Cortex X2, Arm took everything in A710 and moved the sliders up. Out-of-order structures enjoy increased capacity. L1, L2, and micro-op cache sizes get larger. X2 gets a quad pipe FPU, giving it a welcome upgrade over A710’s dual pipe one. Floating point units are area hungry because FP operations involve several basic operations under the hood, so X2’s larger area budget is getting put to good use. The L2 TLB is another good use of extra area. A710’s 1024 entry L2 TLB was small by modern standards, so X2’s 2048 entry one is great to see.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=23142"><img data-attachment-id="23142" data-permalink="https://chipsandcheese.com/2023/10/27/cortex-x2-arm-aims-high/x2_arm_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-orig-size="2264,1272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="x2_arm_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=2264%2C1272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?fit=688%2C387&amp;ssl=1" decoding="async" loading="lazy" width="688" height="387" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=2264&amp;ssl=1 2264w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=768%2C431&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1536%2C863&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=2048%2C1151&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1600%2C899&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/x2_arm_slide.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm’s slide. Labels for some core components added in red</figcaption></figure></div>
<p>Cortex X2 is therefore a cool showing of what Arm’s out-of-order architecture can do when allowed to stretch its legs. Arm’s engineers have made good use of their increased area and power budget to patch up A710’s weakest areas. Newer Cortex X cores carry forward X2’s strengths while using increased transistor budgets to continue patching weaknesses.</p>
<figure><table><tbody><tr><td></td><td>Cortex X2</td><td>Cortex X4</td></tr><tr><td>L1 TLBs</td><td>48 entry iTLB<br>48 entry dTLB</td><td>128 entry iTLB<br>96 entry dTLB</td></tr><tr><td>L2 Cache</td><td>512 KB or 1 MB</td><td>512 KB, 1 MB, or 2 MB</td></tr></tbody></table><figcaption>From looking through the Cortex X4 TRM</figcaption></figure>
<p>I like where Cortex X is going and can see Arm putting pressure on AMD and Intel to keep up the pace. But when your core is stuffed into a SoC with a slow L3 and horrible DRAM latency, it’s going to suffer even when core width and structure sizes look competitive. I hope future implementations will better showcase Cortex X’s potential. </p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Study Says Maybe Helicopter Parenting Is Making Kids Depressed (131 pts)]]></title>
            <link>https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</link>
            <guid>38046910</guid>
            <pubDate>Sat, 28 Oct 2023 03:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/">https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</a>, See on <a href="https://news.ycombinator.com/item?id=38046910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-423910">


<h3>from the <i>correlation-and-causation</i> dept</h3>

<p>We’ve been covering, at great length, the moral panic around the claims that social media is what’s making kids depressed. The problem with this narrative is that there’s basically no real evidence to support it. As the American Psychological Association found when it reviewed all the literature, despite many, many dozens of studies done on the impact of social media on kids, <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2023/05/12/apa-report-says-that-media-politicians-are-simply-wrong-about-kids-social-media-media-then-lies-about-report/">no one was able to establish a causal relationship</a>.</p>
<p>As that report noted, the research seemed to show no inherent benefit or harm for most kids. For some, it showed a real benefit (often around kids being able to find like-minded people online to communicate with). For a very small percentage, it appeared to potentially exacerbate existing issues. And those are really the cases that we should be focused on.</p>
<p>But, instead, the narrative that continues to make the rounds is that social media is inherently bad for kids. That leads to various bills around age verification and age gating to keep kids off of social media.</p>
<p>Supporters of these bills will point to charts like this one, regarding teen suicide rates, noting the uptick correlates with the rise of social media.</p>

<p>Of course, they seem to cherry pick the start date of that chart, because if you go back further, you realize that while the uptick is a concern, it’s still way below what it had been in the 1990s (pre-social media).</p>

<p>In case that embed isn’t working, here’s an image of it:</p>
<figure><img decoding="async" src="https://i0.wp.com/lex-img-p.s3.us-west-2.amazonaws.com/img/bd036c6a-0422-4e33-9c37-427f9f9ddc81-RackMultipart20231025-142-jzngu7.png?ssl=1" alt="Image" data-recalc-dims="1"></figure>
<p>Obviously, the increase in suicides is a concern. But, considering that every single study that tries to link it to social media ends up failing to do so, that suggests that there might be some other factor at play here.</p>
<p>A recent study in the Journal of Pediatrics suggests a compelling alternative. It’s not social media, but the rise of helicopter parenting, in which kids no longer have spaces to just hang out with each other and be kids. It’s titled: <a target="_blank" rel="noreferrer noopener" href="https://doi.org/10.1016/j.jpeds.2023.02.004">Decline in Independent Activity as a Cause of Decline in Children’s Mental Well-being: Summary of the Evidence</a>. If you can’t see the full version, there’s <a target="_blank" rel="noreferrer noopener" href="https://cdn2.psychologytoday.com/assets/2023-02/Children's%20Independence%20IN%20PRESS%20.pdf">a preprint version</a> here.</p>
<p>The research summarizes the decline in “independent mobility” for kids over the last few decades:</p>
<blockquote>
<p><em>Considerable research, mostly in Europe, has focused on children’s independent mobility (CIM), defined as children’s freedom to travel in their neighborhood or city without adult accompaniment. That research has revealed significant declines in CIM, especially between 1970 and 1990, but also some large national differences. For example, surveys regarding the “licenses” (permissions) parents grant to their elementary school children revealed that in England, license to walk home alone from school dropped from 86% in 1971 to 35% in 1990 and 25% in 2010; and license to use public buses alone dropped from 48% in 1971 to 15% in 1990 to 12% in 2010.11 In another study, comparing CIM in 16 different countries (US not included), conducted from 2010 to 2012, Finland stood out as allowing children the greatest freedom of movement. The authors wrote: “At age 7, a majority of Finnish children can already travel to places within walking distance or cycle to places alone; by age 8 a majority can cross main roads, travel home from school and go out after dark alone, by age 9 a majority can cycle on main roads alone, and by age 10 a majority can travel on local buses alone.” Although we have found no similar studies of parental permissions for US children, other data indicate that the US is more like the UK concerning children’s independent mobility than like Finland. For example, National Personal Transportation Surveys revealed that only 12.7% walked or biked to school in 2009 compared with 47.7% in 1969.</em></p>
</blockquote>
<p>And then it notes the general decline in mental health as well, which they highlight started long before social media existed:</p>
<blockquote>
<p><em>Perhaps the most compelling and disturbing evidence comes from studies of suicide and suicidal thoughts. Data compiled by the CDC indicate that the rate of suicide among children under age 15 rose 3.5-fold between 1950 and 2005 and by another 2.4-fold between 2005 and 2020. No other age group showed increases nearly this large. By 2019, suicide was the second leading cause of death for children from age 10 through 15, behind only unintentional injury. Moreover, the 2019 YRBS survey revealed that during the previous year 18.8% of US high school students seriously considered attempting suicide, 15.7% made a suicide plan, 8.9% attempted suicide one or more times, and 2.5% made a suicide attempt requiring medical treatment. We are clearly experiencing an epidemic of psychopathology among young people.</em></p>
</blockquote>
<p>But, unlike those who assume correlation is causation with regards to social media, the researchers here admit there needs to be more. And they bring the goods, pointing to multiple studies that suggest a pretty clear causal relationship, rather than just correlation.</p>
<blockquote>
<p><em>Several studies have examined relationships between the amount of time young children have for self-directed activities at home and psychological characteristics predictive of future wellbeing. These have revealed significant positive correlations between amount of self-structured time (largely involving free play) and (a) scores on two different measures of executive functioning; (b) indices of emotional control and social ability; and (c) scores, two years later, on a measure of self-regulation. There is also evidence that risky play, where children deliberately put themselves in moderately frightening situations (such as climbing high into a tree) helps protect against the development of phobias and reduces future anxiety by increasing the person’s confidence that they can deal effectively with emergencies.</em></p>
<p><em>Studies with adults involving retrospections about their childhood experiences provide another avenue of support for the idea that early independent activity promotes later wellbeing. In one such study, those who reported much free and adventurous play in their elementary school years were assessed as having more social success, higher self-esteem, and better overall psychological and physical health in adulthood than those who reported less such play. In another very similar study, amount of reported free play in childhood correlated positively with measures of social success and goal flexibility (ability to adapt successfully to changes in life conditions) in adulthood. Also relevant here are studies in which adults (usually college students) rated the degree to which their parents were overprotective and overcontrolling (a style that would reduce opportunity for independent activity) and were also assessed for their current levels of anxiety and depression. A systematic review of such studies revealed, overall, positive correlations between the controlling, overprotective parenting style and the measures of anxiety and depression.</em></p>
</blockquote>
<p>They also note that they are not claiming (of course) that this is the sole reason for the declines in mental health. Just that there is strong evidence that it is a key component. They explore a few other options that may contribute, including increased pressure at schools and societal changes. They also consider the impact of social media and digital technologies and note (as we have many times) that there just is no real evidence to support the claims:</p>
<blockquote>
<p><em>Much recent discussion of young people’s mental health has focused on the role of increased use of digital technologies, especially involvement with social media. However, systematic reviews of research into this have provided little support for the contention that either total screen time or time involved with social media is a major cause of, or even correlate of, declining mental health. One systematic review concluded that research on links between digital technology use and teens’ mental health “has generated a mix of often conflicting small positive, negative and null associations” (Odgers &amp; Jensen, 2020). Another, a “review of reviews” concluded that “the association between digital technology use, or social media use in particular, and psychological well-being is, on average, negative but very small” and noted some evidence, from longitudinal research, that negative correlations may result from declining mental health leading to more social media use rather than the reverse (Orben, 2020)</em></p>
</blockquote>
<p>Indeed, if this theory is true, that the lack of spaces for kids to explore and play and experiment without adult supervision <em>is</em> a leading cause of mental health decline, you could easily see how those who are depressed are more likely to <em>seek out</em> those private spaces, and turn to social media, given the lack of any such spaces they can go to physically.</p>
<p>And, if that’s the case, then all of these efforts to ban social media for kids, or to make social media <a rel="noreferrer noopener" href="https://www.techdirt.com/2022/09/20/the-internet-is-not-disneyland-people-should-stop-demanding-it-become-disneyland/" target="_blank">more like Disneyland</a>, could likely end up doing <strong>a lot more harm than good</strong> by cutting off one of the last remaining places where kids can communicate with their peers without adults watching over their every move. Indeed, the various proposals to give parents more access to what their kids are doing online could worsen the problem as well, taking away yet another independent space for kids.</p>
<p>Over the last few years, there’s been a push to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html">bring back more “dangerous” play for kids</a>, as people have begun to realize that things may have gone too far in the other direction. Perhaps it’s time we realize that social media fits into that category as well.</p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/age-appropriate-design/" rel="tag">age appropriate design</a>, <a href="https://www.techdirt.com/tag/age-verification/" rel="tag">age verification</a>, <a href="https://www.techdirt.com/tag/depression/" rel="tag">depression</a>, <a href="https://www.techdirt.com/tag/independent-spaces/" rel="tag">independent spaces</a>, <a href="https://www.techdirt.com/tag/mental-health/" rel="tag">mental health</a>, <a href="https://www.techdirt.com/tag/social-media/" rel="tag">social media</a>, <a href="https://www.techdirt.com/tag/studies/" rel="tag">studies</a>, <a href="https://www.techdirt.com/tag/suicide/" rel="tag">suicide</a>, <a href="https://www.techdirt.com/tag/teens/" rel="tag">teens</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A small warning about UDP based protocols (158 pts)]]></title>
            <link>https://boston.conman.org/2023/10/25.1</link>
            <guid>38046448</guid>
            <pubDate>Sat, 28 Oct 2023 02:13:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boston.conman.org/2023/10/25.1">https://boston.conman.org/2023/10/25.1</a>, See on <a href="https://news.ycombinator.com/item?id=38046448">Hacker News</a></p>
Couldn't get https://boston.conman.org/2023/10/25.1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Staring at a Wall: Embracing Deliberate Boredom (120 pts)]]></title>
            <link>https://www.ch3ngl0rd.com/staring-at-a-wall/</link>
            <guid>38046396</guid>
            <pubDate>Sat, 28 Oct 2023 02:05:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ch3ngl0rd.com/staring-at-a-wall/">https://www.ch3ngl0rd.com/staring-at-a-wall/</a>, See on <a href="https://news.ycombinator.com/item?id=38046396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    







<p>
    <i>
        <time datetime="2023-10-28">
            28 Oct, 2023
        </time>
    </i>
</p>

<p>You should spend more time being bored.</p>
<p>I spent twenty minutes staring at a wall. Was it worth my time? Yes. Did I look a little bit crazy doing it? Maybe a little.</p>
<p>My friend <a href="https://www.joshshipton.com/">Josh Shipton</a> recently talked about the <a href="https://www.joshshipton.com/boredom.html">Power of Embracing Boredom</a>, and how boredom is needed for your mind to process your thoughts. One exercise he recommends is to sit down, and stare at a wall. I had my doubts, but after one session of staring at a wall, I found it extremely rewarding.</p>
<p>The exercise is quite simple to do:</p>
<ol>
<li>Set up a timer for ten to thirty minutes</li>
<li>Stare at a wall</li>
</ol>
<p>I found the exercise to be most effective with twenty minutes and a white wall.</p>
<h2 id="unexpected-insights">Unexpected Insights</h2>
<p><img alt="thoughts by ch3ngl0rd ᕦʕ •ᴥ•ʔᕤ" src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/ch3ngl0rd-1698456991-0.jpg"></p>
<p>During my walling session, a scene from The Lego Movie unexpectedly came to mind: the moment when the old wizard and the emo girl discover Emmet's profoundly empty mind. While they initially mock him for this and crush the idea of a double-decker couch, their laughter is cut short upon witnessing the vision of 'The Man Upstairs'.</p>
<blockquote>
<p>Master Builders spend years training themselves to clear their minds enough to have even a fleeting glimpse of The Man Upstairs.</p>
<p>- The Old Wizard Guy</p>
</blockquote>
<p>This exercise taps into the same power that many meditation practices aim for - an uncluttered mind. When our minds are clear, they become fertile grounds for introspection and fresh ideas. In just twenty minutes, I generated more insights and processed more unfinished thoughts than if I had simply tried to write them down at a desk.</p>
<p>One aspect I appreciate is how simple this exercise is. In the past, I've tried other meditation techniques, only to feel lost as if I was somehow meditating incorrectly. But sitting in front of a wall? There's some unexpected beauty to it. The sheer emptiness seems to prompt the mind better than just closing my eyes and thinking. And the slight discomfort of staring at a wall for twenty minutes provides just enough sensation to anchor you to the present. It's the perfect nudge to keep you grounded in the moment.</p>
<h2 id="don-t-get-lost-in-the-sauce">Don't get Lost in the Sauce</h2>
<p>During my "walling" session, I reflected about the importance of <strong>Taking a Step Back</strong> - we should schedule times in the future to take a step back to make sure that we're not losing sight of the bigger picture or becoming too obsessed with minor details - essentially, not getting "lost in the sauce".</p>
<p>Lately, I've become interested in entrepreneurship and how startups work - reading books like <em>The Lean Startup, Zero to One and The Mom Test</em>. While they've offered invaluable insights, I find myself at a crossroads. Should I continue grokking knowledge through reading, or is it time to take the leap and start building a startup?</p>
<p>Take "The Lean Startup" for instance. It emphasises that startups operate under <strong>conditions of extreme uncertainty</strong>. The key to succeeding is to write down the riskiest assumptions and validate or invalidate them with MVPs (Minimal Viable Products). This insight alone has probably saved me a huge mistake in the future. Yet I'm left wondering: Would diving into more books be as beneficial as actually building and bringing my startup idea to life? Are there insights in the next chapter that could supercharge my performance? It's hard to say.</p>
<p>While staring at the wall, I realised that just as it's easy to get lost in thought, it's also easy to get lost in endless reading and preparation. Sometimes, we need to step back, reflect on what we've learned, and take action. Reading about startups is helpful, but at some point, we need to start building one.</p>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>I think you should stare at a wall at least once. If that doesn't suit you, go on a walk and deliberately plan to be bored. Leave your phone at home. Don't bring your headphones. Simply immerse yourself in your surroundings, free from distractions. Lose track of time and appreciate the world around you.</p>
<p><em>You should spend more time being bored.</em></p>
<center><b>ch3ngl0rd out.</b></center>
<p><a href="https://news.ycombinator.com/item?id=38046396">Discuss</a> or <a href="https://www.linkedin.com/in/zachary-cheng-19a395212/">Get in Touch</a>.</p>



    

    
    



    



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When gradient descent is a kernel method (186 pts)]]></title>
            <link>https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</link>
            <guid>38045665</guid>
            <pubDate>Sat, 28 Oct 2023 00:23:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html">https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</a>, See on <a href="https://news.ycombinator.com/item?id=38045665">Hacker News</a></p>
<div id="readability-page-1" class="page">
    
    
    <p>Suppose that we sample a large number <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> of independent random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi mathvariant="double-struck">R</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f_i \colon \R \to \R</annotation></semantics></math></span></span> from a certain distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and propose to solve a regression problem by choosing a linear combination
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f} = \sum_i \lambda_i f_i.</annotation></semantics></math></span></span></span>
For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">N,</annotation></semantics></math></span></span> adjusting the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> to fit some fixed constraints of the form <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bar{f}(t_i) = y_i</annotation></semantics></math></span></span> amounts to solving a highly underdetermined linear system, meaning that a high-dimensional space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> of vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_1, \dots, \lambda_N)</annotation></semantics></math></span></span> fit our constraints perfectly. So, choosing one element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> requires some additional decision-making. To use the picturesque idea of a
"loss landscape" over parameter space, our problem will have a <em>ridge</em> of equally performing parameters rather than just a single optimal <em>peak</em>.</p>
<p>Now, we make a very strange proposal. What if we simply initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><msqrt><mi>n</mi></msqrt></mrow><annotation encoding="application/x-tex">\lambda_i = 1/\sqrt{n}</annotation></semantics></math></span></span> for all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> and proceed by minimizing some loss function using gradient descent? If all goes well, we should end up with an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Lambda.</annotation></semantics></math></span></span> Of course, many elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> give very bad models. To see this, it's enough to remember that we can expect a linear combination of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> random functions to fit <em>any</em> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> data points, so if we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> data points, there exist models in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> that perfectly interpolate any adversarial selection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">N - m</annotation></semantics></math></span></span> additional data points! Does gradient descent tend to make a "good" choice?</p>
<p>Let's test this empirically. In the widget below, I've chosen functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> by sampling <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trajectories of a Wiener process, also known as Brownian noise. Click anywhere to introduce data points and click the play button in the top right to run gradient descent for the squared loss <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\sum_i (\bar{f}(t_i) - y_i)^2.</annotation></semantics></math></span></span></p>

<p>Interestingly, the functions we obtain are not that bad. They seem to concentrate around piecewise linear interpolations of our data. In fact, in the limit <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N \to \infty</annotation></semantics></math></span></span> of many random functions, it turns out that running gradient descent to convergence has a meaningful statistical interpretation. Specifically, if we view the Wiener process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> as a prior, then <strong>running gradient descent to convergence samples from the posterior for our data points.</strong> Since many optimal solutions to our minimization problem are meaningless, it is not possible to explain this fact if we see gradient descent as "just some optimization method." What explains its relative success?</p>
<p>As we will show in this post, our intriguing Bayesian interpretation can be explained by the relationship between the behavior of gradient descent steps, the statistical properties of our random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span>, and our initialization. In particular, it does <em>not</em> depend on the loss function—so long as it leads gradient descent to converge to an exact interpolation—but <em>does</em> depend significantly on our choice of parameters at initialization. Our analysis will rely on a "tangent kernel" of the sort introduced in the <em>Neural Tangent Kernel</em> paper by Jacot et al.. Specifically, viewing gradient descent as a process occurring in the function space of our regression problem, we will find that its dynamics can be described in terms of a certain kernel function, which in this case is just the kernel function of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span></p>
<p>Of course, there are much easier ways to sample posteriors for low-dimensional Gaussian processes. Nevertheless, it's interesting to notice a relationship between Bayesian inference and gradient descent methods at large, since the latter tend to apply to situations where direct estimation of a posterior distribution is not practical. Furthermore, the results of Jacot suggest that kernel-based interpretations may also hold for large, non-trivial neural networks. To the extent that this is true, we can use the kernel interpretation to reason about many sorts of neural network phenomena, including the benefits of early stopping, the existence of "implicit regularization", and the fact that overparameterization often <em>increases</em> performance despite the apparent risk of overfitting.</p>
<p>In this post, we will focus exclusively on the toy problem introduced above. Our discussion (which admittedly takes a bit of a scenic route) is divided into three sections.</p>
<ul>
<li>First, we will discover how the covariance kernel of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> is related to the dynamics of gradient descent.</li>
<li>Next, we recall the theory of reproducing kernel Hilbert spaces and show how the kernel-based behavior of gradient descent is related to regularization.</li>
<li>Finally, we recall some special properties of Gaussian processes and explain why regularization is related to the Bayesian interpretation of our trained model.</li>
</ul>
<h2>Kernel Functions</h2>
<p>Let's begin by considering the effect that a single step of gradient descent has on our function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In general, the differential of a loss can be written as a sum of differentials <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> is the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> at an input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t,</annotation></semantics></math></span></span> so by linearity it is enough for us to understand how <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> "responds" to differentials of this form.</p>
<p>In response to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">d \pi_t,</annotation></semantics></math></span></span> the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> are assigned differentials
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial \pi_t}{\partial \lambda_i} = f_i(t).</annotation></semantics></math></span></span></span>
So, gradient descent will increase <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> proportional to the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i(t).</annotation></semantics></math></span></span> In terms of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bar{f},</annotation></semantics></math></span></span> we find that the value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\bar{f}(s)</annotation></semantics></math></span></span> at another input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> will increase proportional to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\Delta_t(s) = \sum_{i = 1}^N f_i(s) f_i(t). \tag{1}</annotation></semantics></math></span></span></span>
Note that this expression is independent of the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\lambda_i.</annotation></semantics></math></span></span> This means that the gradient descent step we apply to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> depends only on our learning rate and differential of the loss at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In other words, we can view gradient descent as a process happening in the function space of our regression problem.</p>
<p>For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> we get the approximation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>E</mi><mrow><mi>f</mi><mo>∼</mo><mi mathvariant="script">F</mi></mrow></msub><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\frac{1}{N} \Delta_t(s) \approx E_{f \sim \Fc}[f(s) f(t)]. \tag{2}</annotation></semantics></math></span></span></span>
This last expression is familiar in the study of Gaussian processes; it is called the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> For the Wiener process, the covariance kernel takes the form
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t).</annotation></semantics></math></span></span></span>
In the following, we'll assume <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> is large enough for us to make the approximation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2)</annotation></semantics></math></span></span> confidently. Then, we conclude that a request of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> will cause gradient descent to push <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> in the direction of the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>You can get a visual sense of this behavior in the widget below. As above, I've generated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trials of the Wiener process to use as my functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i.</annotation></semantics></math></span></span> You can choose the request <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> by clicking on the graph, and your browser will compute the corresponding response <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mi mathvariant="normal">/</mi><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Delta_t/N.</annotation></semantics></math></span></span> For comparison I've also drawn the prediction <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>

<p>This is already a significant conclusion. In particular, it means that every step of gradient descent modifies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> by a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K(-, t_i),</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span></span> ranges over the inputs in our training set. Since the linear span of these functions is a certain space of piecewise affine functions, if we initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_i = 0</annotation></semantics></math></span></span> and run gradient descent to convergence with <em>any</em> reasonable loss function, we should approximately converge to a piecewise affine interpolation of our data points. We've run this experiment in the widget below.</p>

<p>This new model has less variance than before. In fact, in the large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> limit, its behavior will be exactly deterministic! In comparison, our previous model will <em>always</em> exhibit variance due to initialization of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> even for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">N.</annotation></semantics></math></span></span> In other words, when given a finite training set, gradient descent cannot entirely "forget" its initialization even when run to convergence.</p>
<p>Another important conclusion is that, when we optimize least squares with gradient descent, the evolution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> is linear in the sense of approximately obeying a linear ODE. Indeed, for data points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t_i, y_i)</annotation></semantics></math></span></span> our loss differential will be
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>d</mi><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>d</mi><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{1}{2} d \sum_i (\bar{f}(t_i) - y_i)^2 = \sum_i d \pi_{t_i} (\bar{f}(t_i) - y_i).</annotation></semantics></math></span></span></span>
So, if we view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> as evolving under the flow of gradient descent with respect to a continuous parameter <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\tau,</annotation></semantics></math></span></span> we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\frac{d \bar{f}}{d \tau} = \sum_i K(-, t_i) (y_i - \bar{f}(t_i)),</annotation></semantics></math></span></span></span>
where the right-hand side is a linear function of the empirical error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h = (y_i - \bar{f}(t_i)).</annotation></semantics></math></span></span> Restricting this equation to the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> over the input points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t_i,</annotation></semantics></math></span></span> we find that the error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> solves the ODE
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mi>K</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\frac{d h}{d \tau} = -K h</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(K(t_i, t_j)).</annotation></semantics></math></span></span> Since this matrix is positive-definite—it is a covariance matrix—we conclude that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> will converge to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> over the training process if our learning rate is sufficiently small. Furthermore, knowing the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> lets us understand the nature of our convergence; gradient descent will "correct" the error <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> along the components of the eigenbasis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> with largest eigenvalues first, and take longer to correct components with smaller eigenvalues.</p>
<p>Now, we could have chosen a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> of random functions with a different covariance kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K.</annotation></semantics></math></span></span> Here the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> were easy to interpret, but in general, what does it mean to fit a data set using a linear combination of functions like these? One interesting perspective comes from the idea of <em>regularization</em>, which we discuss next.</p>
<h2>Regularization</h2>
<p>Consider a Hilbert space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> equipped with bounded projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\pi_t \colon H \to \R</annotation></semantics></math></span></span> for each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t \in \R,</annotation></semantics></math></span></span> and suppose that we want to find an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">v \in H</annotation></semantics></math></span></span> that minimizes some loss function depending on the values <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{t}(v)</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span> belonging to some collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\{t_i\}.</annotation></semantics></math></span></span> (Note that elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> can be viewed as functions from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> by viewing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> as the evaluation at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span>) If this problem is underdetermined—which necessarily will happen if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> is infinite-dimensional and our collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_i\}</annotation></semantics></math></span></span> is finite—then we may ask for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> that both minimizes our loss and has minimal norm in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span> In machine learning, this is called regularization.</p>
<p>Let's write <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> for the product of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{t_i}</annotation></semantics></math></span></span> Because <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is chosen with minimal norm, it cannot be made smaller by adjusting it by an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\ker \Pi,</annotation></semantics></math></span></span> so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is orthogonal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\ker \Pi.</annotation></semantics></math></span></span> But since the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{t}</annotation></semantics></math></span></span> are continuous, they can be represented by vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span> in the sense that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mo>−</mo><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_{t}(-) = \langle K_{t}, - \rangle.</annotation></semantics></math></span></span></span>
(This is the Riesz representation theorem.) Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> can be described as the orthogonal complement to the set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\{K_{t_i}\},</annotation></semantics></math></span></span> the orthogonal complement to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> is exactly the closure of the span of the vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{t_i}.</annotation></semantics></math></span></span> We conclude that any regularized solution to our loss function is a (limit of) linear combinations of these vectors.</p>
<p>What are the projections of the "representative elements" <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span>? By our own definition, we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>s</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\pi_s(K_t) = \langle K_s, K_t \rangle</annotation></semantics></math></span></span></span>
for any other <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">s \in \R.</annotation></semantics></math></span></span> This last expression is a positive semidefinite kernel, which we will denote <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> In other words, the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> and the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> work together to produce a kernel function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> whose partial evaluations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> help us solve optimization problems regularized by the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span></p>
<p>In the literature, a Hilbert space equipped with bounded projections indexed over a set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is called a reproducing kernel Hilbert space (or RKHS). In fact, we can also go in the other direction: every positive definite kernel on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is "reproduced" by some RKHS, which also turns out to be unique in a certain sense. This is known as the Moore-Aronszajn theorem.</p>
<p>What RKHS corresponds to our kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t)</annotation></semantics></math></span></span>? In general, determining the RKHS of a kernel is not entirely straightforward. In fact, notice that for positive definite kernels over a finite set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">I,</annotation></semantics></math></span></span> the inner product for the RKHS expressed in the dual basis for our projections turns out to be the <em>inverse</em> of the matrix encoded by our kernel. Indeed, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">K_i</annotation></semantics></math></span></span> are representatives of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\pi_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">e_i</annotation></semantics></math></span></span> is a dual basis verifying <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\pi_i(e_j) = \delta_{i, j},</annotation></semantics></math></span></span> we find that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><mi>K</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">⟩</mo><mo>=</mo><msub><mi>π</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle e_i, e_j \rangle K(j, k) = \langle e_i, e_j \rangle \pi_j(K_k) = \langle e_i, K_k \rangle = \pi_k(e_j) = \delta_{i, k}.</annotation></semantics></math></span></span></span>
So, interpreting a RKHS norm in terms of projections of elements requires solving some sort of inverse problem.</p>
<p>The RKHS for a centered Gaussian process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X_t)</annotation></semantics></math></span></span> can be viewed as an isometric embedding of the observables <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> with respect to the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span></span> norm for the process measure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span> Specifically, if we define <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f(X_t) = K_t,</annotation></semantics></math></span></span> then clearly
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>k</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><msup><mi>L</mi><mn>2</mn></msup><mi mathvariant="script">F</mi></mrow></msub><mo>=</mo><mo stretchy="false">⟨</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mo stretchy="false">⟩</mo><mtext>RKHS</mtext></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, X_k \rangle_{L^2 \Fc} = \langle f(X_t), f(X_k) \rangle_\text{RKHS}.</annotation></semantics></math></span></span></span>
Indeed, the space of observables of a Gaussian process is already a RKHS for its covariance kernel, if we take the projections to be the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, - \rangle.</annotation></semantics></math></span></span> However, we would like to view the RKHS more directly as a space of functions.</p>
<p>We may begin by observing, then, that observables of the Wiener process can be isometrically mapped into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L^1 [0, \infty)</annotation></semantics></math></span></span> by sending <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>≤</mo><mi>s</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo>:</mo><mtext>otherwise.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">K_s(t) = \begin{cases}
1 : t \le s \\
0 : \text{otherwise.}
\end{cases}</annotation></semantics></math></span></span></span>
Under this perspective, our projections become
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">⟩</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>t</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>s</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_t(f) = \langle K_t, f \rangle = \int_0^t f(s) \, ds.</annotation></semantics></math></span></span></span>
Ultimately, we are led to view the RKHS of the Wiener process as the Sobolev space of absolutely continuous functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f \colon [0, \infty) \to \R</annotation></semantics></math></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0) = 0</annotation></semantics></math></span></span> and such that the norm
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">∥</mo><mi>f</mi><mo stretchy="false">∥</mo><mo>=</mo><mrow><mo fence="true">(</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert f \rVert = \left( \int_0^\infty (f'(t))^2 \, dt \right)</annotation></semantics></math></span></span></span>
is finite. In fact, solving the regularized interpolation problem
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mtext>  </mtext><mtext>for&nbsp;all&nbsp;</mtext><mi>i</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \int_0^\infty (f'(s))^2 \, dt \\
\textbf{subject to} &amp; \quad f(0) = 0, f(t_i) = y_i \; \text{for all }i
\end{align*}</annotation></semantics></math></span></span></span>
results in the piecewise affine interpolations we observed in the widget above.</p>
<p>So far, we have shown that its relationship with kernel functions gives gradient descent a distinct flavor of <em>implicit regularization</em>. We did not have a penalty function in mind when we set up our problem, but our distribution of random functions ended up making our gradient updates interpretable in terms of a RKHS for an associated kernel function. In the last section of this post, we address how this fact is related to the statistical idea of a conditional distribution for a Gaussian process.</p>
<h2>Bayesian Interpretation</h2>
<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> are jointly Gaussian distributed, we know that the remainder
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span></span>
of the conditional expectation is independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> (Remember that for other distributions, this remainder will have zero covariance with all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable events but will not necessarily be independent!) So, we can decompose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> into two components
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y = (Y - E(Y|X)) + E(Y|X),</annotation></semantics></math></span></span></span>
the first being a Gaussian variable independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and the second being <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable. This clarifies the nature of the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span>: it will have constant variance equal to the variance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> and mean equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> a linear function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> In particular, if we want to sample the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> given <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X = x,</annotation></semantics></math></span></span> we could take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>−</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y + E(Y|X = x) - E(Y|X) = Y + E(Y|X = x - X),</annotation></semantics></math></span></span></span>
where the apparently nonsensical conditional expectation on the RHS should be interpreted as the evaluation of the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> viewed as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">x - X.</annotation></semantics></math></span></span> Keep in mind that this is a very special property of Gaussian distributions; in general, the distribution of the remainder <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> conditional on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> will depend on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> and so we won't be able to sample the conditional distribution under another "counterfactual" value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span> simply by translating a sample of the remainder.</p>
<p>Now, consider a random function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> drawn from a Gaussian distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> give the values of our trajectory on a finite set of inputs. If we want to produce a sample from the distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> conditional on some data <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi = \pi,</annotation></semantics></math></span></span> we can take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo>−</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f - E(f | \Pi = \pi - \Pi).</annotation></semantics></math></span></span></span>
But, as it turns out, the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f|\Pi = \pi^*)</annotation></semantics></math></span></span> will be exactly the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> in the RKHS of our process that solves the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> regularized by the RKHS norm! This explains the Bayesian interpretation of our model.</p>
<p>One way to understand this is just to write out the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> at a given value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span> We know that this will be the linear function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\lambda \Pi</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> uniquely determined by the equation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>−</mo><mi>λ</mi><mi mathvariant="normal">Π</mi><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\Cov(f(t) - \lambda \Pi, \Pi) = 0.</annotation></semantics></math></span></span></span>
Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">K = [K(t_i, t_j)]</annotation></semantics></math></span></span> is the covariance matrix of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">v = [\Cov(f(t), f(t_i))]</annotation></semantics></math></span></span> gives the covariance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> with the components <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t_i)</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi,</annotation></semantics></math></span></span> this equation can be written as
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>v</mi><mo>−</mo><mi>λ</mi><mi>K</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v - \lambda K = 0,</annotation></semantics></math></span></span></span>
and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">E(f(t) | \Pi) = v K^{-1} \Pi.</annotation></semantics></math></span></span> We conclude that the function
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mo>↦</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t \mapsto E(f(t)|\Pi)</annotation></semantics></math></span></span></span>
is a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(t, t_i)</annotation></semantics></math></span></span>—the coordinates of the vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>—with constant coefficients, determined by the constraints that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> should agree with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> at the points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><msub><mi>t</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t = t_i.</annotation></semantics></math></span></span> But, as we saw above, this is the same as the solution to the problem of interpolating some constraints <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(t_i) = y_i</annotation></semantics></math></span></span> regularized by the RKHS norm of our process.</p>
<p>To see this connection more directly, remember that the mean of a Gaussian distribution coincides with the mode—the point of highest probability density under linear coordinates. So, for example, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi = \pi^*)</annotation></semantics></math></span></span> is exactly the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> that minimizes
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mo>+</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext>  </mtext><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>  </mtext><mo>…</mo><mtext>  </mtext><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(p(f(t), \pi^*)) = C + -\frac{1}{2} 
[f(t)  \; \pi^*(1)  \; \dots \; \pi(m)] K^{-1}
\begin{bmatrix}
f(t)   \\
\pi^*(1)   \\
\vdots  \\
\pi(m)]
\end{bmatrix}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the inverse of the covariance matrix for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(f(t), \Pi).</annotation></semantics></math></span></span> But from the previous section we know that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K^{-1}</annotation></semantics></math></span></span> expresses the inner product of the RKHS derived from the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> in the dual basis for the projections. So in fact we are asking for the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> that satisfies the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> and is regularized by the RKHS norm corresponding to a restriction of the covariance kernel of our process, which by the representer theorem will be a linear combination of restrictions of functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>Abstractly, we are relying on the fact that whenever we have a positive definite kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>I</mi><mo>×</mo><mi>I</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">K \colon I \times I \to \R</annotation></semantics></math></span></span> and a finite subset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>⊆</mo><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">J \subseteq I,</annotation></semantics></math></span></span> we get a natural projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">P \colon H \to H_J</annotation></semantics></math></span></span> onto the RKHS for the kernel restricted to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>×</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">J \times J</annotation></semantics></math></span></span> given by
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mi>K</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(v) = \sum_j \pi_j(v) K_j.</annotation></semantics></math></span></span></span>
Given a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v \in H,</annotation></semantics></math></span></span> what is the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span> in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span>? Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span> is an isometry over the span of the elements <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K_j,</annotation></semantics></math></span></span> we can view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mo stretchy="false">∥</mo><msub><mi>H</mi><mi>J</mi></msub></msub></mrow><annotation encoding="application/x-tex">\lVert P(v) \rVert_{H_J}</annotation></semantics></math></span></span> as the minimum possible norm for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">w \in H</annotation></semantics></math></span></span> solving the equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w) = P(v).</annotation></semantics></math></span></span> In particular, solving a regularized problem over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> that depends on projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\pi_j</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>∈</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">j \in J</annotation></semantics></math></span></span> and then restricting the solution to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> is the same as restricting to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> and solving the regularized problem with respect to the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H_J.</annotation></semantics></math></span></span></p>
<p>As a final remark, note that we can informally imagine the RKHS of a Gaussian process as specifying the "energy" of the process in a statistical mechanics sense; although the norm of the RKHS is not defined over the same function space that the process takes values, we get the energy for the joint distribution of any finite projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t_1), \dots, f(t_m))</annotation></semantics></math></span></span> as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1, \dots, y_m)</annotation></semantics></math></span></span> by solving
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo stretchy="false">∥</mo><mi>f</mi><msub><mo stretchy="false">∥</mo><mi>H</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \lVert f \rVert_H \\
\textbf{subject to} &amp; \quad f(t_i) = y_i.
\end{align*}</annotation></semantics></math></span></span></span>
This is the most satisfactory way that I've found to connect the interpretation of kernel functions in terms of regularization with their interpretation in terms of conditional expectations of a Gaussian process.</p>

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The destruction of Gaza's internet is complete (123 pts)]]></title>
            <link>https://www.wired.com/story/gaza-internet-blackout-israel/</link>
            <guid>38045514</guid>
            <pubDate>Sat, 28 Oct 2023 00:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gaza-internet-blackout-israel/">https://www.wired.com/story/gaza-internet-blackout-israel/</a>, See on <a href="https://news.ycombinator.com/item?id=38045514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>For more than three weeks, Gaza has faced an almost total internet blackout. The cables, cell towers, and infrastructure needed to keep people online have been damaged or destroyed as Israel launched thousands of missiles in response to <a href="https://www.wired.com/story/israel-hamas-war-surveillance/">Hamas attacking Israel</a> and taking <a href="https://www.wired.com/story/livestream-hostages-israel-hamas-war/">hundreds of hostages</a> on October 7. Then, this evening, amid reports of heavy bombing in Gaza, some of the last remaining connectivity disappeared.</p><p>In the days after October 7, people living in Gaza have been unable to communicate with family or friends, leaving them unsure whether loved ones are alive. Finding reliable news about events has become harder. Rescue workers have not been able to connect to mobile networks, hampering recovery efforts. And information flowing out of Gaza, showing the conditions on the ground, has been stymied.</p><p>As the Israel Defense Forces said it was expanding its ground operations in Gaza this evening, internet connectivity fell further. Paltel, the main Palestinian communications company, has been able to keep some of its services online during Israel’s military response to Hamas’ attack. However, at around 7:30 pm local time today, internet monitoring firm NetBlocks <a data-offer-url="https://twitter.com/netblocks/status/1717942556703551590" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/netblocks/status/1717942556703551590&quot;}" href="https://twitter.com/netblocks/status/1717942556703551590" rel="nofollow noopener" target="_blank">confirmed</a> a “collapse” in connectivity in the Gaza Strip, mostly impacting remaining Paltel services.</p><p>“We regret to announce a complete interruption of all communications and internet services within the Gaza Strip,” Paltel <a data-offer-url="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l&quot;}" href="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" rel="nofollow noopener" target="_blank">posted</a> in a post on its Facebook page. The company claimed that bombing had “caused the destruction of all remaining international routes.” An identical post was made on the Facebook page of Jawwal, the region’s biggest mobile provider, which is owned by Paltel. Separately, Palestinian Red Crescent, a humanitarian organization, <a data-offer-url="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet&quot;}" href="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" rel="nofollow noopener" target="_blank">said on X</a> (formerly Twitter) that it had lost contact with its operation room in Gaza and is “deeply concerned” about its ability to keep caring for people, with landline, cell, and internet connections being inaccessible.</p><p>“This is a terrifying development,” Marwa Fatafta, a policy manager focusing on the Middle East and North Africa at the <a data-offer-url="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.accessnow.org/press-release/communications-blackout-gaza-strip/&quot;}" href="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" rel="nofollow noopener" target="_blank">digital rights group Access Now</a>, tells WIRED. “Taking Gaza completely off the grid while launching an unprecedented bombardment campaign only means something atrocious is about to happen.”</p><p>A WIRED review of internet analysis data, social media posts, and Palestinian internet and telecom company statements shows how connectivity in the Gaza Strip drastically plummeted after October 7 and how some buildings linked to internet firms have been damaged in attacks. Photos and videos show sites that house various internet and telecom firms have been damaged, while reports from official organizations, including the United Nations, describe the impact of people being offline.</p><p>Damaged Lines</p><p>Around the world, the internet and telecoms networks that typically give web users access to international video calls, online banking, and endless social media are a complicated, sprawling mix of hardware and software. Networks of networks, combining data centers, servers, switches, and reams of cables, communicate with each other and send data globally. Local internet access is provided by a mix of companies with no clear public documentation of their infrastructure, making it difficult to monitor the overall status of the system as a whole. In Gaza, experts say, internet connectivity is heavily reliant on Israeli infrastructure to connect to the outside world.</p><p>Amid Israel’s intense bombing of Gaza, physical systems powering the internet have been destroyed. On October 10, the United Nations’ Office for the Coordination of Humanitarian Affairs (OCHA), which oversees emergency responses, <a data-offer-url="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4&quot;}" href="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" rel="nofollow noopener" target="_blank">said</a> air strikes “targeted several telecommunication installations” and had destroyed two of the three main lines of communications going into Gaza.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google can turn ANC earbuds into a heart rate monitor with no extra hardware (174 pts)]]></title>
            <link>https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</link>
            <guid>38045342</guid>
            <pubDate>Fri, 27 Oct 2023 23:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/">https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</a>, See on <a href="https://news.ycombinator.com/item?id=38045342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="" fetchpriority="high">
	
	</figure>

<p>Google today <a href="https://research.google/pubs/pub52460/">detailed its research</a> into audioplethysmography (APG) that adds heart rate sensing capabilities to active noise canceling (ANC) headphones and earbuds “with a simple software upgrade.”</p>



<p><a href="https://blog.research.google/2023/10/audioplethysmography-for-cardiac.html?m=1">Google says</a> the “ear canal [is] an ideal location for health sensing” given that the deep ear artery “forms an intricate network of smaller vessels that extensively permeate the auditory canal.”</p>



<p>This audioplethysmography approach works by “sending a low intensity ultrasound probing signal through an ANC headphone’s speakers.”</p>



<blockquote>
<p>This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes.</p>
</blockquote>



<p>A model that Google created works to process that feedback into a heart rate reading, as well as heart rate variability (HRV) measurement. This technique works even with music playing and “bad earbuds seals.” However, it was impacted by body motion, and Google countered with a multi-tone approach that serves as a calibration tool to “find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform.”</p>



<p>Google performed two sets of studies with 153 people that found APG “achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and heart rate variability (2.70% median error in inter-beat interval) measurements.”</p>



<p>Compared to existing HR sensors, it’s not impacted by skin tones. Ear canal size and “sub-optimal seal conditions” also do not impact accuracy. Google believes this is a better approach than putting traditional&nbsp;photoplethysmograms (PPG) and electrocardiograms (ECG) sensors, as well as a microcontroller, in headphones/earbuds:</p>



<blockquote>
<p>…this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption.</p>
</blockquote>



<p>Google closes on:&nbsp;&nbsp;</p>



<blockquote>
<p>APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing.</p>
</blockquote>



<p>“APG is the result of collaboration across Google Health, product, UX and legal teams,” so this coming to <a href="https://9to5google.com/guides/google-pixel-buds-pro/">Pixel Buds</a> is far from guaranteed at this point.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists simulate backward time travel using quantum entanglement (107 pts)]]></title>
            <link>https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/</link>
            <guid>38045112</guid>
            <pubDate>Fri, 27 Oct 2023 23:03:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/">https://thedebrief.org/scientists-successfully-simulate-backward-time-travel-with-a-25-chance-of-actually-changing-the-past/</a>, See on <a href="https://news.ycombinator.com/item?id=38045112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
																										<p>Scientists trying to take advantage of the unusual properties of the quantum realm say they have successfully simulated a method of backward time travel that allowed them to change an event after the fact one out of four times. The Cambridge University team is quick to caution that they have not built a time machine, per se, but also note how their process doesn’t violate physics while changing past events after they have happened.</p>
<p>“Imagine that you want to send a gift to someone: you need to send it on day one to make sure it arrives on day three,” <a href="https://www.cam.ac.uk/research/news/simulations-of-backwards-time-travel-can-improve-scientific-experiments">explained</a> lead author David Arvidsson-Shukur from the Cambridge Hitachi Laboratory. “However, you only receive that person’s wish list on day two.”</p>
<p>To respect the gift recipient’s timeline, you would need to send it on day one. But, as Arvidsson-Shukur notes, you won’t know what gift to send until later, meaning your gift will either be late or be wrong.</p>
<p>“Now imagine you can change what you send on day one with the information from the wish list received on day two,” he adds. It is exactly this phenomenon that the researchers say can happen in the right scenario.</p>
<p>“Our simulation uses quantum entanglement manipulation to show how you could retroactively change your previous actions to ensure the final outcome is the one you want.”</p>
<h2><strong>Using Quantum Entanglement to Change Your Gift to the Right One After it was Sent</strong></h2>
<p>Quantum entanglement is a process where certain fundamental aspects of quantum particles are shared by two or more particles, and changing those properties in any single particle causes the same change in the others. In their simulations, which were <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.131.150202">published</a> in the journal <em>Physical Review Letters</em>, the Cambridge research team simulated the entanglement of two particles. One of the particles is then sent off to be used in an experiment.</p>
<p>After the experiment’s completion, they gained new information, which would have caused them to act differently. In this situation, “the experimentalist manipulates the second particle to effectively alter the first particle’s past state, changing the outcome of the experiment.” explained the study’s co-author Nicole Yunger Halpern, a researcher at the National Institute of Standards and Technology (NIST) and the University of Maryland.</p>
<p>In effect, by changing the remaining particle, the researchers changed the past. At least in their simulations. It’s an effect the researchers described as “remarkable.” However, they say there is a catch. The experiment only changes the past with the new information about 25% of the time.</p>
<p>“In other words, the simulation has a 75% chance of failure,” says Arvidsson-Shukur. “If we stay with our gift analogy, one out of four times, the gift will be the desired one (for example, a pair of trousers), another time it will be a pair of trousers but in the wrong size, or the wrong colour, or it will be a jacket.”</p>
<p>Fortunately, in their simulations, they at least know when they have failed, which can still allow the researcher to rework the system to effectively achieve backward time travel and get the result that they wanted. To achieve this seemingly impossible goal, they propose using a filter that will allow their theoretical experimenter to send a number of solutions and then simply filter out the 75% that they didn’t want.</p>
<p>“Let’s say sending gifts is inexpensive, and we can send numerous parcels on day one,” said co-author Aidan McConnell, Ph.D., who carried out this research during his master’s degree at the Cavendish Laboratory in Cambridge. “On day two, we know which gift we should have sent. By the time the parcels arrive on day three, one out of every four gifts will be correct, and we select these by telling the recipient which deliveries to throw away.”</p>
<h2><strong>Not a Time Machine, But a Backward Time Travel System?</strong></h2>
<p>The researchers made sure to point out that these are just simulations, albeit successful ones programmed in the known behaviors of entangled particles. So even though it effectively proved a way to change the results of an experiment in the past, 25% of the time at least, its ability to achieve a form of backward time travel still shouldn’t necessarily be compared to a certain Flux Capacitor-equipped DeLorean built by a certain Doc Emmet Brown.</p><div id="block-wrap-46874" data-id="46874">		<article>
					<p><a href="https://thedebrief.org/this-futuristic-bandage-can-heal-wounds-by-harvesting-electricity-out-of-thin-air/">
				<img width="120" height="120" src="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg" alt="bandage harvesting electricity" decoding="async" srcset="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg 120w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-150x150.jpg 150w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-70x70.jpg 70w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-240x240.jpg 240w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-360x360.jpg 360w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-540x540.jpg 540w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-720x720.jpg 720w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-125x125.jpg 125w" sizes="(max-width: 120px) 100vw, 120px" data-srcset="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg 120w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-150x150.jpg 150w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-70x70.jpg 70w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-240x240.jpg 240w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-360x360.jpg 360w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-540x540.jpg 540w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-720x720.jpg 720w, https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-125x125.jpg 125w" data-src="https://thedebrief.b-cdn.net/wp-content/uploads/2023/03/bandaid-120x120.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">			</a>
		</p>
					
		</article>
		</div>
<p>“We are not proposing a time travel machine, but rather a deep dive into the fundamentals of quantum mechanics,” said Arvidsson-Shukur. “These simulations do not allow you to go back and alter your past, but they do allow you to create a better tomorrow by fixing yesterday’s problems today.”</p>
<p>The researcher also notes that the failure rate of their backward time travel system is somewhat reassuring, especially for physicists who depend on Einstein’s theories of relativity and all of the science built upon those theories.</p>
<p>“That we need to use a filter to make our experiment work is actually pretty reassuring,” he said. “The world would be very strange if our time-travel simulation worked every time. Relativity and all the theories that we are building our understanding of our universe on would be out of the window.”</p>
<p><strong><em>Christopher Plain is a Science Fiction and Fantasy novelist and Head Science Writer at The Debrief. Follow and connect with him on <a href="https://twitter.com/plain_fiction" target="_blank" rel="noopener">X</a>, learn about his books at <u><a href="https://plainfiction.com/">plainfiction.com</a></u>, or email him directly at <u><a href="mailto:christopher@thedebrief.org">christopher@thedebrief.org</a></u>.</em></strong></p>
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algebra (159 pts)]]></title>
            <link>https://www.feynmanlectures.caltech.edu/I_22.html</link>
            <guid>38044997</guid>
            <pubDate>Fri, 27 Oct 2023 22:48:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.feynmanlectures.caltech.edu/I_22.html">https://www.feynmanlectures.caltech.edu/I_22.html</a>, See on <a href="https://news.ycombinator.com/item?id=38044997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Ch22">
<h2>
<span>22<a href="https://www.feynmanlectures.caltech.edu/flpphotos.html#23" target="_blank" title="View original lecture photos"><img src="https://www.feynmanlectures.caltech.edu/img/camera.svg"></a></span>Algebra
<p><a href="http://www.feynman.com/the-animated-feynman-lectures/" target="_blank">[Special message</a> from Ralph Leighton, with audio from the lecture.]
</p>
</h2>

<p><img src="https://www.feynmanlectures.caltech.edu/img/FLP_I/f22-00/f22-00.jpg">
</p>



<div id="Ch22-S1">
<h3>
<span>22–1</span>Addition and multiplication</h3>
<p>In our study of oscillating systems we shall have occasion
to use one of the most remarkable, almost astounding, formulas in all of
mathematics. From the physicist’s point of view we could bring forth
this formula in two minutes or so, and be done with it. But science is
as much for intellectual enjoyment as for practical utility, so instead
of just spending a few minutes on this amazing jewel, we shall surround
the jewel by its proper setting in the grand design of that branch of
mathematics which is called elementary algebra.</p>
<p>Now you may ask, “What is mathematics doing in a physics lecture?”
We have several possible excuses: first, of course, mathematics is an
important tool, but that would only excuse us for giving the formula
in two minutes. On the other hand, in theoretical physics we discover
that all our laws can be written in mathematical form; and that this
has a certain simplicity and beauty about it. So, ultimately, in order
to understand nature it may be necessary to have a deeper
understanding of mathematical relationships. But the real reason is
that the subject is enjoyable, and although we humans cut nature up in
different ways, and we have different courses in different
departments, such compartmentalization is really artificial, and we
should take our intellectual pleasures where we find them.</p>
<p>Another reason for looking more carefully at algebra now, even though
most of us studied algebra in high school, is that that was the first
time we studied it; all the equations were unfamiliar, and it was hard
work, just as physics is now. Every so often it is a great pleasure to
look back to see what territory has been covered, and what the great
map or plan of the whole thing is. Perhaps some day somebody in the
Mathematics Department will present a lecture on mechanics in such a
way as to show what it was we were trying to learn in the physics
course!</p>
<p>The subject of algebra will not be developed from the point of view of
a mathematician, exactly, because the mathematicians are mainly
interested in how various mathematical facts are demonstrated, and how
many assumptions are absolutely required, and what is not
required. They are not so interested in the result of what they
prove. For example, we may find the Pythagorean theorem quite
interesting, that the sum of the squares of the sides of a right
triangle is equal to the square of the hypotenuse; that is an
interesting fact, a curiously simple thing, which may be appreciated
without discussing the question of how to prove it, or what axioms are
required. So, in the same spirit, we shall describe qualitatively, if
we may put it that way, the system of elementary algebra. We say
<em>elementary</em> algebra because there is a branch of mathematics
called <em>modern</em> algebra in which some of the rules such as $ab =
ba$, are abandoned, and it is still called algebra, but we shall not
discuss that.</p>
<p>To discuss this subject we start in the middle. We suppose that we
already know what integers are, what zero is, and what it means to
increase a number by one unit. You may say, “That is not in the
middle!” But it is the middle from a mathematical standpoint, because
we could go even further back and describe the theory of sets in order
to <em>derive</em> some of these properties of integers. But we are not
going in that direction, the direction of mathematical philosophy and
mathematical logic, but rather in the other direction, from the
assumption that we know what integers are and we know how to count.</p>
<p>If we start with a certain number&nbsp;$a$, an integer, and we count
successively one unit $b$&nbsp;times, the number we arrive at we call
$a+b$, and that defines <em>addition</em> of integers.</p>
<p>Once we have defined addition, then we can consider this: if we start
with nothing and add $a$ to it, $b$&nbsp;times in succession, we call the
result <em>multiplication</em> of integers; we call it $b$ <em>times</em>&nbsp;$a$.</p>
<p>Now we can also have a <em>succession of multiplications:</em> if we
start with $1$ and multiply by&nbsp;$a$, $b$&nbsp;times in succession, we call
that <em>raising to a power:</em>&nbsp;$a^b$.</p>
<p>Now as a consequence of these definitions it can be easily shown that
all of the following relationships are true:
<span>
\begin{equation}
\begin{alignedat}{4}
&amp;(\text{a})&amp;\quad
&amp;a+b=b+a&amp;\quad\quad
&amp;(\text{b})&amp;\quad
&amp;a+(b+c)=(a+b)+c\\
&amp;(\text{c})&amp;\quad
&amp;ab=ba&amp;\quad\quad
&amp;(\text{d})&amp;\quad
&amp;a(b+c)=ab+ac\\
&amp;(\text{e})&amp;\quad
&amp;(ab)c=a(bc)&amp;\quad\quad
&amp;(\text{f})&amp;\quad
&amp;(ab)^c=a^cb^c\\
&amp;(\text{g})&amp;\quad
&amp;a^ba^c=a^{(b+c)}&amp;\quad\quad
&amp;(\text{h})&amp;\quad
&amp;(a^b)^c=a^{(bc)}\\
&amp;(\text{i})&amp;\quad
&amp;a+0=a&amp;\quad\quad
&amp;(\text{j})&amp;\quad
&amp;a\cdot 1=a\\
&amp;(\text{k})&amp;\quad
&amp;a^1=a
\end{alignedat}
\label{Eq:I:22:1}
\end{equation}
</span>
<span>
\begin{equation}
\begin{alignedat}{4}
&amp;(\text{a})&amp;\quad&amp;a+b=b+a\\
&amp;(\text{b})&amp;\quad&amp;a+(b+c)=(a+b)+c\\
&amp;(\text{c})&amp;\quad&amp;ab=ba\\
&amp;(\text{d})&amp;\quad&amp;a(b+c)=ab+ac\\
&amp;(\text{e})&amp;\quad&amp;(ab)c=a(bc)\\
&amp;(\text{f})&amp;\quad&amp;(ab)^c=a^cb^c\\
&amp;(\text{g})&amp;\quad&amp;a^ba^c=a^{(b+c)}\\
&amp;(\text{h})&amp;\quad&amp;(a^b)^c=a^{(bc)}\\
&amp;(\text{i})&amp;\quad&amp;a+0=a\\
&amp;(\text{j})&amp;\quad&amp;a\cdot 1=a\\
&amp;(\text{k})&amp;\quad&amp;a^1=a
\end{alignedat}
\label{Eq:I:22:1}
\end{equation}
</span>
These results are well known and we shall not belabor the point, we
merely list them. Of course, $1$ and&nbsp;$0$ have special properties; for
example, $a + 0$ is&nbsp;$a$, $a$ times $1= a$, and $a$ to the first power
is&nbsp;$a$.</p>
<p>In this discussion we must also assume a few other properties like
continuity and ordering, which are very hard to define; we will let
the rigorous theory do it. Furthermore, it is definitely true that we
have written down too many “rules”; some of them may be deducible
from the others, but we shall not worry about such matters.</p>
</div>
<div id="Ch22-S2">
<h3>
<span>22–2</span>The inverse operations</h3>
<p>In addition to the direct operations of addition, multiplication, and
raising to a power, we have also the <em>inverse</em> operations, which
are defined as follows. Let us assume that $a$ and&nbsp;$c$ are given, and
that we wish to find what values of&nbsp;$b$ satisfy such equations as $a +
b = c$, $ab = c$, $b^a = c$. If $a + b= c$, $b$ is defined as $c - a$,
which is called <em>subtraction</em>. The operation called division is
also clear: if $ab = c$, then $b = c/a$ defines division—a solution
of the equation $ab = c$ “backwards.” Now if we have a power $b^a =
c$ and we ask ourselves, “What is&nbsp;$b$?,” it is called the
$a$th&nbsp;<em>root</em> of&nbsp;$c$: $b = \sqrt[a]{c}$. For instance, if we ask
ourselves the following question, “What integer, raised to the third
power, equals&nbsp;$8$?,” then the answer is called the <em>cube root</em>
of&nbsp;$8$; it is&nbsp;$2$. Because $b^a$ and&nbsp;$a^b$ are not equal, there are
<em>two</em> inverse problems associated with powers, and the other
inverse problem would be, “To what power must we raise&nbsp;$2$ to
get&nbsp;$8$?”  This is called taking the <em>logarithm</em>. If
$a^b = c$, we
write $b = \log_ac$. The fact that it has a cumbersome notation
relative to the others does not mean that it is any less elementary,
at least applied to integers, than the other processes. Although
logarithms come late in an algebra class, in practice they
are, of
course, just as simple as roots; they are just a different kind of
solution of an algebraic equation. The direct and inverse operations
are summarized as follows:
\begin{equation}
\begin{alignedat}{5}
&amp;(\text{a})&amp;&amp;\quad
\text{addition}&amp;&amp;\quad
&amp;&amp;(\text{a}')&amp;&amp;\quad
\text{subtraction}\\
&amp; &amp;&amp;\quad a+b=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=c-a\\
&amp;(\text{b})&amp;&amp;\quad
\text{multiplication}&amp;&amp;\quad
&amp;&amp;(\text{b}')&amp;&amp;\quad
\text{division}\\
&amp; &amp;&amp;\quad ab=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=c/a\\
&amp;(\text{c})&amp;&amp;\quad
\text{power}&amp;&amp;\quad
&amp;&amp;(\text{c}')&amp;&amp;\quad
\text{root}\\
&amp; &amp;&amp;\quad b^a=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=\sqrt[a]{c}\\
&amp;(\text{d})&amp;&amp;\quad
\text{power}&amp;&amp;\quad
&amp;&amp;(\text{d}')&amp;&amp;\quad
\text{logarithm}\\
&amp; &amp;&amp;\quad a^b=c&amp;&amp;\quad
&amp;&amp; &amp;&amp;\quad b=\log_ac\\
\end{alignedat}
\label{Eq:I:22:2}
\end{equation}
</p>
<p>Now here is the idea. These relationships, or rules, are correct for
integers, since they follow from the definitions of addition,
multiplication, and raising to a power. <em>We are going to discuss
whether or not we can broaden the class of objects which $a$,&nbsp;$b$,
and&nbsp;$c$ represent so that they will obey these same rules</em>, although the
processes for $a + b$, and so on, will not be definable in terms of
the direct action of adding&nbsp;$1$, for instance, or successive
multiplications by integers.</p>
</div>
<div id="Ch22-S3">
<h3>
<span>22–3</span>Abstraction and generalization</h3>
<p>When we try to solve simple algebraic equations using all these
definitions, we soon discover some insoluble problems, such as the
following. Suppose that we try to solve the equation $b = 3 - 5$. That
means, according to our definition of subtraction, that we must find a
number which, when added to&nbsp;$5$, gives&nbsp;$3$. And of course there
<em>is</em> no such number, because we consider only positive integers;
this is an insoluble problem. However, the plan, the great idea, is
this: <em>abstraction and generalization</em>. From the whole structure
of algebra, rules plus integers, we abstract the original definitions
of addition and multiplication, but we leave the rules (<a href="#mjx-eqn-EqI221">22.1</a>)
and&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>), and assume these to be true <em>in general</em> on
a wider class of numbers, even though they are originally derived on a
smaller class. Thus, rather than using integers symbolically to define
the rules, we use the rules as the definition of the symbols, which then
represent a more general kind of number. As an example, by working with
the rules alone we can show that $3 - 5 = 0 - 2$. In fact we can show
that one can make <em>all</em> subtractions, provided we define a whole
set of new numbers: $0 - 1$, $0 - 2$, $0 - 3$, $0 - 4$, and so on,
called the <em>negative integers</em>. Then we may use all the other
rules, like $a(b + c) = ab + ac$ and so forth, to find what the rules
are for multiplying negative numbers, and we will discover, in fact,
that all of the rules can be maintained with negative as well as
positive integers.</p>
<p>So we have increased the range of objects over which the rules work,
but the meaning of the symbols is different.</p>
<p>One cannot say, for instance, that $-2$ times&nbsp;$5$ really means to
add&nbsp;$5$ together successively $-2$&nbsp;times. That means nothing. But
nevertheless everything will work out all right according to the
rules.</p>
<p>An interesting problem comes up in taking powers. Suppose that we wish
to discover what $a^{(3-5)}$ means. We know only that $3 - 5$ is a
solution of the problem, $(3 - 5) + 5 = 3$. Knowing that, we know that
$a^{(3-5)}a^5 = a^3$. Therefore $a^{(3-5)} = a^3/a^5$, by the
definition of division. With a little more work, this can be reduced
to&nbsp;$1/a^2$. So we find that the negative powers are the reciprocals of
the positive powers, but $1/a^2$ is a meaningless symbol, because if
$a$ is a positive or negative integer, the square of it can be greater
than&nbsp;$1$, and we do not yet know what we mean by&nbsp;$1$ divided by a
number greater than&nbsp;$1$!</p>
<p>Onward! The great plan is to continue the process of generalization;
whenever we find another problem that we cannot solve we extend our
realm of numbers. Consider division: we cannot find a number which is
an integer, even a negative integer, which is equal to the result of
dividing $3$ by&nbsp;$5$. But if we suppose that all fractional numbers
also satisfy the rules, then we can talk about multiplying and adding
fractions, and everything works as well as it did before.</p>
<p>Take another example of powers: what is&nbsp;$a^{3/5}$? We know only that
$(3/5)5 = 3$, since that was the definition of&nbsp;$3/5$. So we know also
that $(a^{(3/5)})^5 =$ $a^{(3/5)(5)}=$ $a^3$, because this is one of the
rules. Then by the definition of roots we find that $a^{(3/5)} =
\sqrt[5]{a^3}$.</p>
<p>In this way, then, we can define what we mean by putting fractions in
the various symbols, by using the rules themselves to help us
determine the definition—it is not arbitrary. It is a remarkable
fact that all the rules still work for positive and negative integers,
as well as for fractions!</p>
<p>We go on in the process of generalization. Are there any other
equations we cannot solve? Yes, there are. For example, it is
impossible to solve this equation: $b =$ $2^{1/2} =$ $\sqrt{2}$. It is
impossible to find a number which is rational (a fraction) whose
square is equal to&nbsp;$2$. It is very easy for us in modern days to
answer this question. We know the decimal system, and so we have no
difficulty in appreciating the meaning of an unending decimal as a
type of approximation to the square root of&nbsp;$2$. Historically, this
idea presented great difficulty to the Greeks. To really define
<em>precisely</em> what is meant here requires that we add some
substance of continuity and ordering, and it is, in fact, quite the
most difficult step in the processes of generalization just at this
point. It was made, formally and rigorously, by
Dedekind. However, without
worrying about the mathematical rigor of the thing, it is quite easy to
understand that what we mean is that we are going to find a whole
sequence of approximate fractions, perfect fractions (because any
decimal, when stopped somewhere, is of course rational), which just
keeps on going, getting closer and closer to the desired result. That is
good enough for what we wish to discuss, and it permits us to involve
ourselves in irrational numbers, and to calculate things like the square
root of&nbsp;$2$ to any accuracy that we desire, with enough work.</p>
</div>
<div id="Ch22-S4">
<h3>
<span>22–4</span>Approximating irrational numbers</h3>
<p>The next problem comes with what happens with the irrational
powers. Suppose that we want to define, for instance,
$10^{\sqrt{2}}$. In principle, the answer is simple enough. If we
approximate the square root of&nbsp;$2$ to a certain number of decimal
places, then the power is rational, and we can take the approximate
root, using the above method, and get an <em>approximation</em>
to&nbsp;$10^{\sqrt{2}}$. Then we may run it up a few more decimal places (it
is again rational), take the appropriate root, this time a much higher
root because there is a much bigger denominator in the fraction, and
get a better approximation. Of course we are going to get some
enormously high roots involved here, and the work is quite
difficult. How can we cope with this problem?</p>
<p>In the computations of square roots, cube roots, and other small
roots, there is an arithmetical process available by which we can get
one decimal place after another. But the amount of labor needed to
calculate irrational powers and the logarithms that go with them (the
inverse problem) is so great that there is no simple arithmetical
process we can use. Therefore tables have been built up which permit
us to calculate these powers, and these are called the tables of
logarithms, or the tables of powers, depending on which way the table
is set up. It is merely a question of saving time; if we must raise
some number to an irrational power, we can look it up rather than
having to compute it. Of course, such a computation is just a
technical problem, but it is an interesting one, and of great
historical value. In the first place, not only do we have the problem
of solving $x=10^{\sqrt{2}}$, but we also have the problem of solving
$10^x = 2$, or $x = \log_{10} 2$. This is not a problem where we have
to define a new kind of number for the result, it is merely a
computational problem. The answer is simply an irrational number, an
unending decimal, not a new kind of a number.</p>
<p>Let us now discuss the problem of calculating solutions of such
equations. The general idea is really very simple. If we could
calculate $10^1$, and $10^{4/10}$, and $10^{1/100}$, and $10^{4/1000}$
and so on, and multiply them all together, we would get
$10^{1.414\dots}$ or&nbsp;$10^{\sqrt{2}}$, and that is the general idea on
which things work. But instead of calculating $10^{1/10}$ and so on,
we shall calculate $10^{1/2}$, $10^{1/4}$, and so on. Before we start,
we should explain why we make so much work with $10$, instead of some
other number. Of course, we realize that logarithm tables are of great
practical utility, quite aside from the mathematical problem of taking
roots, since with any base at all,
\begin{equation}
\label{Eq:I:22:3}
\log_b(ac)=\log_ba+\log_bc.
\end{equation}
We are all familiar with the fact that one can use this fact in a
practical way to multiply numbers if we have a table of
logarithms. The only question is, with what base&nbsp;$b$ shall we compute?
It makes no difference what base is used; we can use the same
principle all the time, and if we are using logarithms to any
particular base, we can find logarithms to any other base merely by a
change in scale, a multiplying factor. If we multiply
Eq.&nbsp;(<a href="#mjx-eqn-EqI223">22.3</a>) by&nbsp;$61$, it is just as true, and if we had a table
of logs with a base&nbsp;$b$, and somebody else multiplied all of our table
by&nbsp;$61$, there would be no essential difference. Suppose that we know
the logarithms of all the numbers to the base&nbsp;$b$. In other words, we
can solve the equation $b^a = c$ for any $c$ because we have a table.
The problem is to find the logarithm of the same number&nbsp;$c$ to some
other base, let us say the base&nbsp;$x$. We would like to solve $x^{a'} =
c$. It is easy to do, because we can always write $x = b^t$, which
defines $t$, knowing $x$ and&nbsp;$b$. As a matter of fact, $t = \log_b x$.
Then if we put that in and solve for $a'$, we see that $(b^t)^{a'} =
b^{a't} = c$. In other words, $ta'$ is the logarithm of&nbsp;$c$ in base&nbsp;$b$.
Thus $a' = a/t$. Thus logs to base&nbsp;$x$ are just $1/t$, which is a
constant, times the logs to the base, $b$. Therefore any log table is
equivalent to any other log table if we multiply by a constant, and the
constant is $1/\log_b x$. This permits us to choose a particular base,
and for convenience we take the base&nbsp;$10$. (The question may arise as to
whether there is any natural base, any base in which things are somehow
simpler, and we shall try to find an answer to that later. At the moment
we shall just use the base&nbsp;$10$.)</p>



<div id="Ch22-T1">
<p><span>Table 22–1</span>Successive Square Roots of Ten
</p>
<table>
<thead>
<tr>
    <td>Power $s$</td>
    <td>$1024\,s$</td>
    <td>$10^s$</td>
    <td>$(10^s-1)/s$</td></tr></thead>
<tbody>
<tr>
    <td>$1\phantom{/1024}$</td>
    <td>$1024$</td>
    <td>$10.00000\hphantom{00}$</td>
    <td>$9.00\hphantom{00^{000}}$</td></tr>
<tr>
    <td>$1/2\phantom{000}$</td>
    <td>$\phantom{1}512$</td>
    <td>$\phantom{1}3.16228\hphantom{00}$</td>
    <td>$4.32\hphantom{00^{000}}$</td></tr>
<tr>
    <td>$1/4\phantom{000}$</td>
    <td>$\phantom{1}256$</td>
    <td>$\phantom{1}1.77828\hphantom{00}$</td>
    <td>$3.113\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/8\phantom{000}$</td>
    <td>$\phantom{1}128$</td>
    <td>$\phantom{1}1.33352\hphantom{00}$</td>
    <td>$2.668\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/16\phantom{00}$</td>
    <td>$\phantom{10}64$</td>
    <td>$\phantom{1}1.15478\hphantom{00}$</td>
    <td>$2.476\hphantom{0^{000}}$</td></tr>
<tr>
    <td>$1/32\phantom{00}$</td>
    <td>$\phantom{10}32$</td>
    <td>$\phantom{1}1.074607\hphantom{0}$</td>
    <td>$2.3874\hphantom{^{000}}$</td></tr>
<tr>
    <td>$1/64\phantom{00}$</td>
    <td>$\phantom{10}16$</td>
    <td>$\phantom{1}1.036633\hphantom{0}$</td>
    <td>$2.3445\hphantom{^{000}}$</td></tr>
<tr>
    <td>$1/128\phantom{0}$</td>
    <td>$\phantom{100}8$</td>
    <td>$\phantom{1}1.018152\hphantom{0}$</td>
    <td>$2.3234^{211}$</td></tr>
<tr>
    <td>$1/256\phantom{0}$</td>
    <td>$\phantom{100}4$</td>
    <td>$\phantom{1}1.0090350$</td>
    <td>$2.3130^{104}$</td></tr>
<tr>
    <td>$1/512\phantom{0}$</td>
    <td>$\phantom{100}2$</td>
    <td>$\phantom{1}1.0045073$</td>
    <td>$2.3077^{\phantom{1}53}$</td></tr>
<tr>
    <td>$1/1024$</td>
    <td>$\phantom{100}1$</td>
    <td>$\phantom{1}1.0022511$</td>
    <td>$2.3051^{\phantom{1}26}$</td></tr>
<tr>
    <td></td>
    <td></td>
    <td></td>
    <td>$\phantom{00}\Big\downarrow\hspace 3ex^{26}$</td></tr>
<tr>
    <td>$\Delta/1024$</td>
    <td>$\phantom{102}\Delta$</td>
    <td>$1+0.0022486\Delta\overleftarrow{\kern 1.5em}$</td>
    <td>$\raise.5ex\overline{\kern 1em}2.3025$</td></tr>
<tr>
    <td>$(\Delta\to 0)$</td>
    <td></td>
    <td></td>
    <td></td></tr></tbody>
</table>
</div>

<p>Now let us see how to calculate logarithms. We begin by computing
successive square roots of&nbsp;$10$, by cut and try. The results are shown
in Table&nbsp;<a href="#Ch22-T1">22–1</a>. The powers of&nbsp;$10$ are given in the first
column, and the result, $10^s$, is given in the third column. Thus
$10^1 = 10$. The one-half power of&nbsp;$10$ we can easily work out,
because that is the square root of&nbsp;$10$, and there is a known, simple
process for taking square roots of any number.<a id="footnote_source_1" href="#footnote_1"><sup>1</sup></a> Using this process, we find
the first square root to be&nbsp;$3.16228$. What good is that? It already
tells us something, it tells us how to take $10^{0.5}$, so we now know
at least <em>one</em> logarithm, if we happen to need the logarithm
of&nbsp;$3.16228$, we know the answer is close to&nbsp;$0.50000$. But we must do a
little bit better than that; we clearly need more information. So we
take the square root again, and find $10^{1/4}$, which
is&nbsp;$1.77828$. Now we have the logarithm of more numbers than we had
before, $1.250$ is the logarithm of&nbsp;$17.78$ and, incidentally, if it
happens that somebody asks for $10^{0.75}$, we can get it, because
that is $10^{(0.5+0.25)}$; it is therefore the product of the second
and third numbers. If we can get enough numbers in column&nbsp;$s$ to be
able to make up almost any number, then by multiplying the proper
things in column&nbsp;3, we can get $10$ to any power; that is the plan. So
we evaluate ten successive square roots of&nbsp;$10$, and that is the main
work which is involved in the calculations.</p>
<p>Why don’t we keep on going for more and more accuracy? Because we
begin to notice something. When we raise $10$ to a very small power,
we get $1$ plus a small amount. The reason for this is clear, because
we are going to have to take the $1000$th&nbsp;power of&nbsp;$10^{1/1000}$ to
get back to&nbsp;$10$, so we had better not start with too big a number; it
has to be close to&nbsp;$1$. What we notice is that the small numbers that
are added to&nbsp;$1$ begin to look as though we are merely dividing by&nbsp;$2$
each time; we see $1815$ becomes $903$, then $450$, $225$; so it is
clear that, to an excellent approximation, if we take another root, we
shall get $1.00112$ something, and rather than actually <em>take</em>
all the square roots, we <em>guess</em> at the ultimate limit. When we
take a small fraction&nbsp;$\Delta/1024$ as $\Delta$ approaches zero,
what will the answer be? Of course it will be some number close
to&nbsp;$1+0.0022511\,\Delta$. Not exactly $1+0.0022511\,\Delta$, however—we
can get a better value by the following trick: we subtract the $1$,
and then divide by the power&nbsp;$s$. This ought to correct all the
excesses to the same value. We see that they are very closely
equal. At the top of the table they are not equal, but as they come
down, they get closer and closer to a constant value. What is the
value? Again we look to see how the series is going, how it has
changed with&nbsp;$s$. It changed by $211$, by $104$, by $53$, by
$26$. These changes are obviously half of each other, very closely, as
we go down. Therefore, if we kept going, the changes would be
$13$,&nbsp;$7$, $3$, $2$ and&nbsp;$1$, more or less, or a total of&nbsp;$26$. Thus we have
only&nbsp;$26$ more to go, and so we find that the true number
is&nbsp;$2.3025$. (Actually, we shall later see that the <em>exact</em> number
should be&nbsp;$2.3026$, but to keep it realistic, we shall not alter
anything in the arithmetic.) From this table we can now calculate any
power of&nbsp;$10$, by compounding the power out of&nbsp;$1024$ths.</p>
<p>Let us now actually calculate a logarithm, because the process we
shall use is where logarithm tables actually come from. The procedure
is shown in Table&nbsp;<a href="#Ch22-T2">22–2</a>, and the numerical values are shown
in Table&nbsp;<a href="#Ch22-T1">22–1</a> (columns 2 and 3).</p>



<div id="Ch22-T2">
<p><span>Table 22–2</span>Calculation of a logarithm: $\boldsymbol{\log_{10} 2}$
</p>
<table>
<tbody>
<tr>
    <td></td>
    <td>$2 \div 1.77828 = 1.124682$</td></tr>
<tr>
    <td></td>
    <td>$1.124682 \div 1.074607 = 1.046598$, etc.</td></tr>
<tr>
    <td>$\therefore\,$</td>
    <td>$2=(1.77828)(1.074607)(1.036633)(1.0090350)(1.000573)$</td></tr>
<tr>
    <td></td>
    <td>$\phantom{2}=10^{\biggl[\dfrac{1}{1024}\mbox{(256+32+16+4+0.254)}\biggr]}=10^{\biggl[\dfrac{308.254}{1024}\biggr]}$</td></tr>
<tr>
    <td></td>
    <td>$\phantom{2}=10^{0.30103}\phantom{(256+32+16+4}\biggl(\dfrac{573}{2249}=0.254\biggr)$</td></tr>
<tr>
    <td>$\therefore\,$</td>
    <td>$\log_{10}2=0.30103$</td></tr></tbody>
</table>
</div>

<p>Suppose we want the logarithm of&nbsp;$2$. That is, we want to know to what
power we must raise $10$ to get $2$. Can we raise $10$ to the
$1/2$&nbsp;power?  No; that is too big. In other words, we can see that the
answer is going to be bigger than $1/4$, and less than $1/2$. Let us
take the factor&nbsp;$10^{1/4}$ out; we divide $2$ by&nbsp;$1.778\dots$, and get
$1.124\dots$, and so on, and now we know that we have taken away
$0.250000$ from the logarithm. The number&nbsp;$1.124\dots$, is now the
number whose logarithm we need. When we are finished we shall add back
the $1/4$, or $256/1024$. Now we look in the table for the next number
just below $1.124\dots$, and that is&nbsp;$1.074607$. We therefore divide
by&nbsp;$1.074607$ and get $1.046598$. From that we discover that $2$ can
be made up of a product of numbers that are in Table&nbsp;<a href="#Ch22-T1">22–1</a>,
as follows:
<span>
\begin{equation*}
2 = (1.77828)(1.074607)(1.036633)(1.0090350)(1.000573).
\end{equation*}
</span>
<span>
\begin{gather*}
2 = (1.77828)(1.074607)(1.036633)\;\times\\
(1.0090350)(1.000573).
\end{gather*}
</span>



There was one factor&nbsp;$(1.000573)$ left over, naturally, which is
beyond the range of our table. To get the logarithm of this factor, we
use our result that $10^{\Delta/1024} \approx 1+ 2.3025
\Delta/1024$. We find $\Delta= 0.254$. Therefore our answer is&nbsp;$10$ to
the following power: $(256 + 32 + 16 + 4 + 0.254)/1024$. Adding those
together, we get $308.254/1024$. Dividing, we get $0.30103$, so we
know that the $\log_{10} 2 = 0.30103$, which happens to be right
to&nbsp;$5$ figures!</p>
<p>This is how logarithms were originally computed by Mr.&nbsp;Briggs of Halifax, in&nbsp;1620. He said, “I computed successively
$54$&nbsp;square roots of&nbsp;$10$.” We know he really computed only the first $27$,
because the rest of them can be obtained by this trick with $\Delta$. His work
involved calculating the square root of&nbsp;$10$ twenty-seven times, which is not
much more than the ten times we did; however, it was more work because he
calculated to sixteen decimal places, and then reduced his answer to fourteen
when he published it, so that there were no rounding errors. He made tables of
logarithms to fourteen decimal places by this method, which is quite tedious.
But all logarithm tables for three hundred years were borrowed from
Mr.&nbsp;Briggs’ tables by reducing the number
of decimal places. Only in modern times, with the WPA and computing machines,
have new tables been independently computed. There are much more efficient
methods of computing logarithms today, using certain series expansions.
</p>
<p>In the above process, we discovered something rather interesting, and
that is that for very small powers&nbsp;$\epsilon$ we can calculate
$10^\epsilon$ easily; we have discovered that $10^\epsilon = 1+
2.3025\epsilon$, by sheer numerical analysis. Of course this also
means that $10^{n/2.3025} = 1+ n$ if $n$ is very small. Now logarithms
to any other base are merely multiples of logarithms to the
base&nbsp;$10$. The base&nbsp;$10$ was used only because we have $10$&nbsp;fingers, and
the arithmetic of it is easy, but if we ask for a mathematically
natural base, one that has nothing to do with the number of fingers on
human beings, we might try to change our scale of logarithms in some
convenient and natural manner, and the method which people have chosen
is to redefine the logarithms by multiplying all the logarithms to the
base&nbsp;$10$ by $2.3025\dots$ This then corresponds to using some other
base, and this is called the <em>natural</em> base, or base&nbsp;$e$. Note
that $\log_e (1 + n) \approx n$, or $e^n \approx 1+ n$ as $n\to0$.</p>
<p>It is easy enough to find out what $e$ is: $e = 10^{1/2.3025\dots}$
or&nbsp;$10^{0.434310\dots}$, an irrational power. Our table of the successive
square roots of&nbsp;$10$ can be used to compute, not just logarithms, but
also $10$ to any power, so let us use it to calculate this natural
base&nbsp;$e$. For convenience we transform $0.434310\dots$ into
$444.73/1024$. Now, $444.73$ is $256 + 128 + 32 + 16 + 8 + 4 +
0.73$. Therefore $e$, since it is an exponent of a sum, will be a
product of the numbers
<span>
\begin{equation*}
(1.77828)\!(1.33352)\!(1.074607)\!(1.036633)\!(1.018152)\!
(1.009035)\!(1.001643) = 2.7184.
\end{equation*}
</span>
<span>
\begin{align*}
(1.&amp;77828)\!(1.33352)\!(1.074607)\!(1.036633)\;\times\\
&amp;(1.018152)\!(1.009035)\!(1.001643)= 2.7184.
\end{align*}
</span>
(The only problem is the last one, which is&nbsp;$0.73$, and which is not
in the table, but we know that if $\Delta$ is small enough, the answer
is&nbsp;$1 + 0.0022486\,\Delta$.) When we multiply all these together, we get
$2.7184$ (it should be&nbsp;$2.7183$, but it is good enough). The use of
such tables, then, is the way in which irrational powers and the
logarithms of irrational numbers are all calculated. That takes care
of the irrationals.</p>
</div>
<div id="Ch22-S5">
<h3>
<span>22–5</span>Complex numbers</h3>
<p>Now it turns out that after all that work we <em>still</em> cannot solve
every equation!  For instance, what is the square root of&nbsp;$-1$?
Suppose we have to find $x^2 =-1$.  The square of no rational, of no
irrational, of <em>nothing</em> that we have discovered so far, is equal
to&nbsp;$-1$. So we again have to generalize our numbers to a still wider
class. Let us suppose that a specific solution of&nbsp;$x^2 =-1$ is called
something, we shall call it&nbsp;$i$; $i$ has the property, by definition,
that its square is&nbsp;$-1$. That is about all we are going to say about
it; of course, there is more than one root of the equation $x^2
=-1$. Someone could write $i$, but another could say, “No, I prefer
$-i$. My $i$ is minus your $i$.” It <i>is</i> just as good a solution, and
since the only definition that $i$ has is that $i^2=-1$, it must be
true that any equation we can write is equally true if the sign of&nbsp;$i$
is changed everywhere. This is called taking the <em>complex
conjugate</em>. Now we are going to make up numbers by adding
successive&nbsp;$i$’s, and multiplying $i$’s by numbers, and adding other numbers,
and
so on, according to all of our rules. In this way we find that numbers
can all be written in the form $p + iq$, where $p$ and&nbsp;$q$ are what we
call <em>real</em> numbers, i.e., the numbers we have been defining up
until now. The number&nbsp;$i$ is called the <em>unit imaginary</em> number.
Any real multiple of&nbsp;$i$ is called <em>pure imaginary</em>. The most
general number, $a$, is of the form $p+iq$ and is called a
<em>complex number</em>. Things do not get any worse if, for instance,
we multiply two such numbers, let us say $(r + is)(p + iq)$. Then,
using the rules, we get
\begin{align}
(r + is)(p + iq) &amp;= rp + r(iq) + (is)p + (is)(iq)\notag\\[1ex]
&amp;= rp + i(rq) + i(sp) + (ii)(sq)\notag\\[1ex]
\label{Eq:I:22:4}
&amp;= (rp - sq) + i(rq + sp),
\end{align}
since $ii =$ $i^2 =$ $-1$. Therefore all the numbers that now belong in
the rules&nbsp;(<a href="#mjx-eqn-EqI221">22.1</a>) have this mathematical form.</p>
<p>Now you say, “This can go on forever! We have defined powers of
imaginaries and all the rest, and when we are all finished, somebody
else will come along with another equation which cannot be solved,
like $x^6 + 3x^2 =-2$. Then we have to generalize all over again!”
But it turns out that <em>with this one more invention</em>, just the
square root of&nbsp;$-1$, <em>every algebraic equation can be solved!</em>
This is a fantastic fact, which we must leave to the Mathematics
Department to prove. The proofs are very beautiful and very
interesting, but certainly not self-evident. In fact, the most obvious
supposition is that we are going to have to invent again and again and
again. But the greatest miracle of all is that we do not. This is the
last invention. After this invention of complex numbers, we find that
the rules still work with complex numbers, and we are finished
inventing new things. We can find the complex power of any complex
number, we can solve any equation that is written algebraically, in
terms of a finite number of those symbols. We do not find any new
numbers. The square root of&nbsp;$i$, for instance, has a definite result,
it is not something new; and $i^i$ is something. We will discuss that
now.</p>
<p>We have already discussed multiplication, and addition is also easy;
if we add two complex numbers, $(p + iq) + (r + is)$, the answer is
$(p + r) + i(q + s)$. Now we can add and multiply complex numbers. But
the real problem, of course, is to compute <em>complex powers of
complex numbers</em>. It turns out that the problem is actually no more
difficult than computing complex powers of real numbers. So let us
concentrate now on the problem of calculating $10$ to a complex power,
not just an irrational power, but $10^{(r+is)}$. Of course, we must at
all times use our rules (<a href="#mjx-eqn-EqI221">22.1</a>) and&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>). Thus
\begin{equation}
\label{Eq:I:22:5}
10^{(r+is)}=10^r10^{is}.
\end{equation}
But $10^r$ we already know how to compute, and we can always multiply
anything by anything else; therefore the problem is to compute only
$10^{is}$. Let us call it some complex number, $x + iy$. Problem:
given $s$, find $x$, find $y$. Now if
\begin{equation*}
10^{is}=x+iy,
\end{equation*}
then the complex conjugate of this equation must also be true, so that
\begin{equation*}
10^{-is}=x-iy.
\end{equation*}
(Thus we see that we can deduce a number of things without actually
computing anything, by using our rules.) We deduce another interesting
thing by multiplying these together:
<span>
\begin{equation}
\label{Eq:I:22:6}
10^{is}10^{-is}=10^0=1=(x+iy)(x-iy)=x^2+y^2.
\end{equation}
</span>
<span>
\begin{equation}
\begin{gathered}
\label{Eq:I:22:6}
10^{is}10^{-is}=10^0=1\\
=(x+iy)(x-iy)=x^2+y^2.
\end{gathered}
\end{equation}
</span>
Thus if we find $x$, we have $y$ also.</p>
<p>Now the problem is <em>how</em> to compute $10$ to an imaginary
power. What guide is there? We may work over our rules until we can go
no further, but here is a reasonable guide: if we can compute it for
any particular $s$, we can get it for all the rest. If we know
$10^{is}$ for any one $s$ and then we want it for twice that $s$, we
can square the number, and so on. But how can we find $10^{is}$ for
even one special value of&nbsp;$s$? To do so we shall make one additional
assumption, which is not quite in the category of all the other rules,
but which leads to reasonable results and permits us to make progress:
when the power is small, we shall suppose that the “law”
$10^\epsilon = 1+ 2.3025\epsilon$ is right, as $\epsilon$ gets very
small, not only for real $\epsilon$, <em>but for complex $\epsilon$
as well</em>. Therefore, we begin with the supposition that this law is
true in general, and that tells us that $10^{is} = 1+ 2.3025\cdot is$,
for $s\to0$. So we assume that if $s$ is very small, say one part
in&nbsp;$1024$, we have a rather good approximation to&nbsp;$10^{is}$.</p>
<p>Now we make a table by which we can compute <em>all</em> the imaginary
powers of&nbsp;$10$, that is, compute $x$ and&nbsp;$y$. It is done as
follows. The first power we start with is the $1/1024$ power, which we
presume is very nearly $1+ 2.3025i/1024$. Thus we start with
\begin{equation}
\label{Eq:I:22:7}
10^{i/1024}=1.00000+0.0022486i,
\end{equation}
and if we keep multiplying the number by itself, we can get to a
higher imaginary power. In fact, we may just reverse the procedure we
used in making our logarithm table, and calculate the square, $4$th
power, $8$th power, etc., of&nbsp;(<a href="#mjx-eqn-EqI227">22.7</a>), and thus build up the
values shown in Table&nbsp;<a href="#Ch22-T3">22–3</a>. We notice an interesting
thing, that the $x$ numbers are positive at first, but then swing
negative. We shall look into that a little bit more in a moment. But
first we may be curious to find for what number&nbsp;$s$ the real part
of&nbsp;$10^{is}$ is <em>zero</em>. The $y$-value would be&nbsp;$1$, and so we would
have $10^{is} = 1i$, or $is = \log_{10} i$. As an example of how to use
this table, just as we calculated $\log_{10} 2$ before, let us now use
Table&nbsp;<a href="#Ch22-T3">22–3</a> to find $\log_{10} i$.</p>



<div id="Ch22-T3">
<p><span>Table 22–3</span>Successive Squares of $\boldsymbol{10^{i/1024} = 1 + 0.0022486i}$
</p>
<table>
<thead>
<tr>
    <td>Power $is$</td>
    <td>$1024s$</td>
    <td>$10^{is}$</td></tr></thead>
<tbody>
<tr>
    <td>$i/1024$</td>
    <td>$\phantom{000}1$</td>
    <td>$\phantom{-}1.00000 + 0.00225i$*</td></tr>
<tr>
    <td>$i/512\phantom{0}$</td>
    <td>$\phantom{000}2$</td>
    <td>$\phantom{-}1.00000 + 0.00450i$</td></tr>
<tr>
    <td>$i/256\phantom{0}$</td>
    <td>$\phantom{000}4$</td>
    <td>$\phantom{-}0.99996 + 0.00900i$</td></tr>
<tr>
    <td>$i/128\phantom{0}$</td>
    <td>$\phantom{000}8$</td>
    <td>$\phantom{-}0.99984 + 0.01800i$</td></tr>
<tr>
    <td>$i/64\phantom{00}$</td>
    <td>$\phantom{00}16$</td>
    <td>$\phantom{-}0.99936 + 0.03599i$</td></tr>
<tr>
    <td>$i/32\phantom{00}$</td>
    <td>$\phantom{00}32$</td>
    <td>$\phantom{-}0.99742 + 0.07193i$</td></tr>
<tr>
    <td>$i/16\phantom{00}$</td>
    <td>$\phantom{00}64$</td>
    <td>$\phantom{-}0.98967 + 0.14349i$</td></tr>
<tr>
    <td>$i/8\phantom{000}$</td>
    <td>$\phantom{0}128$</td>
    <td>$\phantom{-}0.95885 + 0.28402i$</td></tr>
<tr>
    <td>$i/4\phantom{000}$</td>
    <td>$\phantom{0}256$</td>
    <td>$\phantom{-}0.83872 + 0.54467i$</td></tr>
<tr>
    <td>$i/2\phantom{000}$</td>
    <td>$\phantom{0}512$</td>
    <td>$\phantom{-}0.40679 + 0.91365i$</td></tr>
<tr>
    <td>$i/1\phantom{000}$</td>
    <td>$1024$</td>
    <td>$-0.66928 + 0.74332i$</td></tr>
<tr>
    <td></td>
    <td colspan="2">* Should be $0.0022486i$</td></tr></tbody>
</table>
</div>

<p>Which of the numbers in Table&nbsp;<a href="#Ch22-T3">22–3</a> do we have to multiply
together to get a pure imaginary result? After a little trial and
error, we discover that to reduce $x$ the most, it is best to multiply
“$512$” by&nbsp;“$128$.” This gives $0.13056 + 0.99159i$. Then we
discover that we should multiply this by a number whose imaginary part
is about equal to the size of the real part we are trying to
remove. Thus we choose “$64$” whose $y$-value is&nbsp;$0.14349$, since
that is closest to&nbsp;$0.13056$. This then gives $-0.01308 +
1.00008i$. Now we have overshot, and must <em>divide</em> by $0.99996 +
0.00900i$. How do we do that? By changing the sign of&nbsp;$i$ and
multiplying by $0.99996 - 0.00900i$ (which works if $x^2 + y^2 =
1$). Continuing in this way, we find that the entire power to which
$10$ must be raised to give $i$ is $i(512 + 128 + 64 - 4 - 2 +
0.20)/1024$, or $698.20i/1024$. If we raise $10$ to that power, we can
get $i$. Therefore $\log_{10} i = 0.68184i$.</p>

</div>
<div id="Ch22-S6">
<h3>
<span>22–6</span>Imaginary exponents</h3>

<div id="Ch22-T4">
<p><span>Table 22–4</span>Successive Powers of $\boldsymbol{10^{i/8}}$
</p>
<table>
<thead>
<tr>
    <td>$p=$ $\text{power}\cdot8/i$</td>
    <td>$10^{ip/8}$</td></tr></thead>
<tbody>
<tr>
    <td>$\phantom{0}0$</td>
    <td>$\phantom{-}1.00000+0.00000i$</td></tr>
<tr>
    <td>$\phantom{0}1$</td>
    <td>$\phantom{-}0.95882+0.28402i$</td></tr>
<tr>
    <td>$\phantom{0}2$</td>
    <td>$\phantom{-}0.83867+0.54465i$</td></tr>
<tr>
    <td>$\phantom{0}3$</td>
    <td>$\phantom{-}0.64944+0.76042i$</td></tr>
<tr>
    <td>$\phantom{0}4$</td>
    <td>$\phantom{-}0.40672+0.91356i$</td></tr>
<tr>
    <td>$\phantom{0}5$</td>
    <td>$\phantom{-}0.13050+0.99146i$</td></tr>
<tr>
    <td>$\phantom{0}6$</td>
    <td>$-0.15647+0.98770i$</td></tr>
<tr>
    <td>$\phantom{0}7$</td>
    <td>$-0.43055+0.90260i$</td></tr>
<tr>
    <td>$\phantom{0}8$</td>
    <td>$-0.66917+0.74315i$</td></tr>
<tr>
    <td>$\phantom{0}9$</td>
    <td>$-0.85268+0.52249i$</td></tr>
<tr>
    <td>$10$</td>
    <td>$-0.96596+0.25880i$</td></tr>
<tr>
    <td>$11$</td>
    <td>$-0.99969-0.02620i$</td></tr>
<tr>
    <td>$12$</td>
    <td>$-0.95104-0.30905i$</td></tr>
<tr>
    <td>$14$</td>
    <td>$-0.62928-0.77717i$</td></tr>
<tr>
    <td>$16$</td>
    <td>$-0.10447-0.99453i$</td></tr>
<tr>
    <td>$18$</td>
    <td>$+0.45454-0.89098i$</td></tr>
<tr>
    <td>$20$</td>
    <td>$+0.86648-0.49967i$</td></tr>
<tr>
    <td>$22$</td>
    <td>$+0.99884+0.05287i$</td></tr>
<tr>
    <td>$24$</td>
    <td>$+0.80890+0.58836i$</td></tr></tbody>
</table>
</div>

<p>To further investigate the subject of taking complex imaginary powers,
let us look at the powers of&nbsp;$10$ taking <em>successive powers</em>, not
doubling the power each time, in order to follow Table&nbsp;<a href="#Ch22-T3">22–3</a>
further and to see what happens to those minus signs. This is shown in
Table&nbsp;<a href="#Ch22-T4">22–4</a>, in which we take $10^{i/8}$, and just keep
multiplying it. We see that $x$ decreases, passes through zero, swings
almost to&nbsp;$-1$ (if we could get in between $p = 10$ and&nbsp;$p = 11$ it
would obviously swing to&nbsp;$-1$), and swings back. The $y$-value is going
back and forth too.</p>
<div id="Ch22-F1">
<p><img data-src="img/FLP_I/f22-01/f22-01_tc_big.svgz"></p><p><span>Figure 22–1</span>
</p>
</div>
<p>In Fig.&nbsp;<a href="#Ch22-F1">22–1</a> the dots represent the numbers that appear
in Table&nbsp;<a href="#Ch22-T4">22–4</a>, and the lines are just drawn to help you
visually. So we see that the numbers $x$ and&nbsp;$y$ oscillate; $10^{is}$
<em>repeats itself</em>, it is a <em>periodic</em> thing, and as such, it
is easy enough to explain, because if a certain power is&nbsp;$i$, then the
fourth power of that would be&nbsp;$i^2$ <em>squared</em>. It would be&nbsp;$+1$
again, and therefore, since $10^{0.68i}$ is equal to&nbsp;$i$, by taking
the fourth power we discover that $10^{2.72i}$ is equal to&nbsp;$+1$.
Therefore, if we wanted $10^{3.00i}$, for instance, we could write it
as $10^{2.72i}$ times&nbsp;$10^{0.28i}$. In other words, it has a period,
it repeats. Of course, we recognize what the curves look like! They
look like the sine and cosine, and we shall call them, for a while,
the algebraic sine and algebraic cosine. However, instead of using the
base&nbsp;$10$, we shall put them into our natural base, which only changes
the horizontal scale; so we denote $2.3025s$ by&nbsp;$t$, and write
$10^{is} = e^{it}$, where $t$ is a real number. Now $e^{it} = x + iy$,
and we shall write this as the algebraic cosine of&nbsp;$t$ plus $i$&nbsp;times
the algebraic sine of&nbsp;$t$. Thus
\begin{equation}
\label{Eq:I:22:8}
e^{it}=\operatorname{\underline{\cos}}t+
i\operatorname{\underline{\sin}}t.
\end{equation}
What are the properties of $\operatorname{\underline{\cos}} t$
and&nbsp;$\operatorname{\underline{\sin}} t$? First, we know, for instance,
that $x^2 + y^2$ must be&nbsp;$1$; we have proved that before, and it is
just as true for base&nbsp;$e$ as for base&nbsp;$10$. Therefore
$\operatorname{\underline{\cos}}^2 t+
\operatorname{\underline{\sin}}^2 t= 1$. We also know that, for small
$t$, $e^{it} = 1+it$, and therefore $\operatorname{\underline{\cos}}
t$ is nearly $1$, and $\operatorname{\underline{\sin}} t$ is nearly
$t$, and so it goes, that <em>all of the various properties of these
remarkable functions</em>, which come from taking imaginary powers,
<em>are the same as the sine and cosine of trigonometry</em>.</p>
<p>Is the period the same? Let us find out. $e$ to what power is equal
to&nbsp;$i$? What is the logarithm of&nbsp;$i$ to the base&nbsp;$e$? We worked it out
before, in the base&nbsp;$10$ it was $0.68184i$, but when we change our
logarithmic scale to&nbsp;$e$, we have to multiply by&nbsp;$2.3025$, and if we
do that it comes out $1.570$. So this will be called “algebraic
$\pi/2$.”  But, we see, it differs from the regular $\pi/2$ by only
one place in the last point, and that, of course, is the result of
errors in our arithmetic! So we have created two new functions in a
purely algebraic manner, the cosine and the sine, which belong to
algebra, and only to algebra. We wake up at the end to discover the
very functions that are natural to geometry. So there is a connection,
ultimately, between algebra and geometry.</p>
<p>We summarize with this, the most remarkable formula in mathematics:
\begin{equation}
\label{Eq:I:22:9}
e^{i\theta}=\cos\theta+i\sin\theta.
\end{equation}
This is our jewel.</p>
<p>We may relate the geometry to the algebra by representing complex
numbers in a plane; the horizontal position of a point is&nbsp;$x$, the
vertical position of a point is&nbsp;$y$ (Fig.&nbsp;<a href="#Ch22-F2">22–2</a>). We
represent every complex number, $x+iy$. Then if the radial distance to
this point is called $r$ and the angle is called $\theta$, the
algebraic law is that $x+iy$ is written in the form $re^{i\theta}$,
where the geometrical relationships between $x$,&nbsp;$y$, $r$,
and&nbsp;$\theta$ are as shown. This, then, is the unification of algebra and
geometry.</p>
<div id="Ch22-F2">
<p><img data-src="img/FLP_I/f22-02/f22-02_tc_big.svgz"></p><p><span>Fig. 22–2.</span>$x + iy = re^{i\theta}$.
</p>
</div>
<p>When we began this chapter, armed only with the basic notions of
integers and counting, we had little idea of the power of the
processes of abstraction and generalization. Using the set of
algebraic “laws,” or properties of numbers, Eq.&nbsp;(<a href="#mjx-eqn-EqI221">22.1</a>),
and the definitions of inverse operations&nbsp;(<a href="#mjx-eqn-EqI222">22.2</a>), we have
been able here, ourselves, to manufacture not only numbers but useful
things like tables of logarithms, powers, and trigonometric functions
(for these are what the imaginary powers of real numbers are), all
merely by extracting ten successive square roots of ten!</p>
</div>
<ol id="footnotes"><li>
  <a id="footnote_1"></a>
  There is a
definite arithmetic procedure, but the easiest way to find the square
root of any number&nbsp;$N$ is to choose some&nbsp;$a$ fairly close, find $N/a$,
average $a' = \tfrac{1}{2}[a + (N/a)]$, and use this average&nbsp;$a'$ for
the next choice for&nbsp;$a$. The convergence is very rapid—the number of
significant figures doubles each time.
  <a href="#footnote_source_1">↩</a>
</li></ol>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Large Balloon Reflector: a potentially game-changing antenna design (221 pts)]]></title>
            <link>https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/</link>
            <guid>38043955</guid>
            <pubDate>Fri, 27 Oct 2023 21:08:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/">https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/</a>, See on <a href="https://news.ycombinator.com/item?id=38043955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Some 30 years ago, a young engineer named Christopher Walker was home in the evening making chocolate pudding when he got what turned out to be a very serendipitous call from his mother.</p>
<p>Taking the call, he shut off the stove and stretched plastic wrap over the pot to keep the pudding fresh. By the time he returned, the cooling air in the pot had drawn the wrap into a concave shape, and in that warped plastic, he saw something – the magnified reflection of an overhead lightbulb – that gave him an idea that could revolutionize space-based sensing and communications.</p>
<p>That idea became the Large Balloon Reflector (LBR), an inflatable device that creates wide collection apertures that weigh a fraction of today’s deployable antennas. Now, with an assist from <a href="https://www.nasa.gov/stmd-the-nasa-innovative-advanced-concepts-niac/">NASA’s Innovative Advanced Concepts</a> (NIAC) program, funded by the agency’s Space Technology Mission Directorate, which supports visionary innovations from diverse sources, Walker’s decades-old vision is coming to fruition.</p>
<p>The concept turns part of the inside surface of an inflated sphere into a parabolic antenna. A section comprising about a third of the balloon’s interior surface is aluminized, giving it reflective properties.</p>
<p>With NIAC funding, and a grant from the U.S. Naval Research Laboratory, Walker was able to develop and demonstrate technologies for a 33-foot-diameter (10 meters) LBR that was carried to the stratosphere by a giant balloon. For comparison, the aperture of NASA’s massive <a href="https://webb.nasa.gov/" rel="noopener">James Webb Space Telescope</a> is over 21 feet (6.5 meters) in diameter.</p>
<p>“There was no place other than NIAC within NASA to get this off the ground,” says Walker, now a astronomy and optical engineering professor at the University of Arizona in Tucson. “At first, I was afraid to share the idea with colleagues because it sounded so crazy. You need a program within NASA that will actually look at the radical ideas, and NIAC is it.”</p>
<p>Parabolic dish antennas use their concave shape to capture and concentrate electromagnetic radiation. The larger the antenna’s diameter, or aperture, the more effective it is for capturing light or radio waves and transmitting radio signals over great distances.</p>
<p>In astronomy, there is a tremendous advantage to placing telescopes above the Earth’s atmosphere, which tends to distort or degrade signals coming from space. The challenge is that traditional large reflector antennas are heavy, unwieldy, and difficult to stow, leading to launch constraints and risky in-space deployment schemes.</p>
<p>The LBR design solves both problems. Made of a thin film structure, it inflates like a beachball, providing a stable parabolic-dish shape without the need for bulky and complex deployable hardware, and can fold into a tiny volume. &nbsp;</p>
<p>In 2018, Freefall Aerospace, a company co-founded by Walker to develop and market the technology, demonstrated the LBR’s potential aboard NASA’s stadium-sized stratospheric balloon, which carried a 3.28-foot scale model to an altitude of 159,000 feet.</p>
<figure>
<p>
<iframe title="CatSat | Student-led CubeSat Project" width="640" height="360" src="https://www.youtube.com/embed/Z4OQG4ABJ_k?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
</figure>
<p>Next up for the technology is a high-speed communications demonstration in low Earth orbit aboard a 6-unit CubeSat, about the size of a shoebox, called CatSat. It was selected for flight in 2019 as part of NASA’s CubeSat Launch Initiative. It is a joint effort involving NASA, Freefall Aerospace, the University of Arizona, and Rincon Research Corporation in Tucson, Arizona.</p>
<p>After reaching low-Earth orbit, CatSat’s inflatable antenna deployment system will deploy from its container, inflate to a diameter of about one-and-a-half feet, and begin transmitting back high-definition Earth photos. The mission is slated for launch with several other CubeSats on Firefly Aerospace’s Alpha rocket as part of the Educational Launch of Nanosatellites (ELaNa) 43 mission.</p>
<p>A more ambitious lunar mission concept is also being explored. NASA’s Goddard Space Flight Center in Greenbelt, Maryland, would use the inflatable antenna in tandem with a new instrument called Terahertz Spectrometer for In-Situ Resource Utilization, a miniature, high-power laser precisely calibrated to detect water, a critical exploration resource.</p>
<p>“The technology demonstrated by CatSat opens the door to the possibility of future lunar, planetary and deep-space missions using CubeSats,” said Walker.</p>
<p>It might be difficult to believe this all started because a young engineer’s idea of dinner one evening was what most would consider dessert. Then again, one could say the proof was in the pudding.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing quaternions (2018) (171 pts)]]></title>
            <link>https://eater.net/quaternions</link>
            <guid>38043644</guid>
            <pubDate>Fri, 27 Oct 2023 20:41:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eater.net/quaternions">https://eater.net/quaternions</a>, See on <a href="https://news.ycombinator.com/item?id=38043644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Android 14's user-profile data bug (172 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/</link>
            <guid>38043574</guid>
            <pubDate>Fri, 27 Oct 2023 20:35:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/">https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/</a>, See on <a href="https://news.ycombinator.com/item?id=38043574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Back up your data!    —
</h4>
            
            <h2 itemprop="description">Users with multiple profiles are getting locked out of local storage and losing data.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2013/10/android-lockup.jpg" alt="Android 14’s user-profile data bug seems indistinguishable from ransomware">
      <figcaption><p>Aurich Lawson</p></figcaption>  </figure>

  




<!-- cache hit 188:single/related:e6e51722ea3a7db761540f225f7d54d6 --><!-- empty -->
<p>Android 14 has a nasty storage bug that seems to be affecting users of the "multiple profiles" feature. The bug is about as bad as you can get, with users having "unusable" devices due to getting locked out of device storage. A few users are <a href="https://issuetracker.google.com/issues/305766503#comment284">likening</a> the experience to getting hit with "ransomware."</p>
<p><a href="https://www.theverge.com/2023/10/16/23919957/pixel-6-android-14-upgrade-bugs-multiple-user-profiles">Earlier reports</a> had this bug limited to the Pixel 6, but Google seemed to ignore those reports, and now with a wider rollout, this does not seem device-specific. Everything upgrading to Android 14 this early seems to be affected: <a href="https://issuetracker.google.com/issues/305766503?pli=1">Pixel 6</a>, <a href="https://issuetracker.google.com/issues/305766503#comment122">6a</a>, <a href="https://issuetracker.google.com/issues/305766503#comment7">7</a>, <a href="https://issuetracker.google.com/issues/305766503#comment104">7a</a>, <a href="https://issuetracker.google.com/issues/305766503#comment121">Pixel Fold</a>, and <a href="https://issuetracker.google.com/issues/305766503#comment353">Pixel Tablet.</a></p>
<p>The Google <a href="https://issuetracker.google.com/issues/305766503?pli=1">issue tracker</a> for this is now up to over 350 replies and has had no response from Google. The bug is languishing at only the medium "P2" priority (P0 is the highest) and remains "unassigned," meaning, assuming the tracker is up to date, no one is looking into it.</p>
<p>Some users have <a href="https://issuetracker.google.com/issues/305766503#comment45">helpfully posted</a> log files full of worrying messages, like, "Failed to open directory /data/media/0: Structure needs cleaning." Being locked out of your own device's data partition causes all sorts of bizarre issues. Some users are boot looping, others are stuck on a "Pixel is starting..." message, while others can get into the phone. If your phone tries to continue trucking with no local storage, you'll be inundated with all sorts of error messages. The camera app claims to be "<a href="https://issuetracker.google.com/issues/305766503#comment38">out of storage</a>," and you can't take screenshots because there's nowhere to store the screenshots. The file manager lists <a href="https://issuetracker.google.com/issues/305766503#comment38">0 bytes</a> for every type of file and empty folder, and the files also aren't viewable from a PC over USB. The System UI and Settings also keep crashing. Basically, computers need storage to function!</p>
<p>Android's user-profile system allows for both multiple users on a single device (which is good for tablets) and splitting up <a href="https://arstechnica.com/information-technology/2015/03/a-review-of-android-for-work-dual-persona-support-comes-to-android/">"home" and "work" profiles</a> to keep your work data separate from your personal data, via duplicate apps. It sounds like the bug is only hitting users who take advantage of this rarely used feature, with lots of reports that the primary profile—that's usually the important one—gets locked out.</p>                                            
                                                        
<p>Several users are complaining about the data lost from all of this, so it's a good time to remind people to always have a backup of everything on their phones. Even straight out of the box, Android has options for Google Photos automatic backups, Play Games storage of your game data, and a million other cloud-based data features (it would be nice if Android phones had a comprehensive whole-phone backup feature, though). While it is totally reasonable to expect your OS to keep running after an update, phones are uniquely vulnerable to getting lost/stolen/damaged, so having everything get stored somewhere else is a great idea. Shockingly, <a href="https://www.reddit.com/r/GooglePixel/comments/178jj3i/need_help_pixel_7_stuck_on_phone_is_starting/">several users</a> report the phone is<a href="https://issuetracker.google.com/issues/305766503#comment43"><em> automatically</em></a> doing a factory reset, which deletes all your data, shutting down any possibility of data recovery. This feature should probably not exist, but it's another sign that phones are not a reliable storage medium for critical data.</p>
<p>What's so strange about how Google is handling this bug is that the company <em>has tools</em> to deal with this. Google delivers software on a slow and often frustrating "roll out" strategy, where a small percentage of users will get an update at first, and as the days pass, more and more users opt in to the update. Google does this to see if any problems pop up via its extensive Android analytics system, and if a problem is detected, the update rollout can be halted, limiting the problem to as few people as possible.</p>
<p>Why didn't that happen here? Surely, a bug where people are locked out of their phones and possibly lose data is worth halting a rollout, but it never happened. Google's entire response to this problem has been lacking. To our knowledge, no one from Google has officially addressed the issue in the 10-ish days it has been around. It hasn't issued statements to the several sites that have already reported on this. No one is replying to the bug tracker, and the issue is unassigned. What's up, Google?</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New devices could change the way we measure blood pressure (107 pts)]]></title>
            <link>https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure</link>
            <guid>38043555</guid>
            <pubDate>Fri, 27 Oct 2023 20:34:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure">https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure</a>, See on <a href="https://news.ycombinator.com/item?id=38043555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Knowable Magazine is from <a href="http://www.annualreviews.org/">Annual Reviews</a>, a nonprofit publisher dedicated to synthesizing and integrating knowledge for the progress of science and the benefit of society.</p>
            <p>© 2023 Annual Reviews</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Images from the 2023 Nikon Small World Photomicrography Competition (157 pts)]]></title>
            <link>https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition</link>
            <guid>38043127</guid>
            <pubDate>Fri, 27 Oct 2023 19:54:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition">https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition</a>, See on <a href="https://news.ycombinator.com/item?id=38043127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <section id="galleryapp" v-touch:swipe.left="pageNext" v-touch:swipe.right="pagePrev">

            <section id="winner_gallery">
    <h2>Top 20</h2>
        
        <hr>
    
        
              </section>
              <section id="honorable-mention_gallery">
    <h2>Honorable Mentions</h2>
        
        <hr>
        
    
        
              </section>
              <section id="image-of-distinction_gallery">
    <h2>Images of Distinction</h2>
        
        <hr>
        
    
            
          </section>
      <section id="judges">
    <h2>Judges</h2>
  <div>
                      <div>
          <h3>Dr. Clare Waterman</h3>
          <p><em>Cell Biologist and Member of the National Academy of Sciences</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Clare-Waterman.jpg" width="120"></p><p>Clare Waterman graduated from Mount Holyoke College with a B.A. in biochemistry in 1989. From there, she received an M.S. in exercise science in 1991 from the <a href="https://www.nikonsmallworld.com/organizations/university-of-massachusetts-boston">University of Massachusetts</a>, and her Ph.D. in cell biology from the <a href="https://www.nikonsmallworld.com/organizations/university-of-pennsylvania">University of Pennsylvania</a> in 1995. She then spent nine years as a professor in the Department of Cell Biology at <a href="https://www.nikonsmallworld.com/organizations/the-scripps-research-institute">The Scripps Research Institute</a> in La Jolla, California. Dr. Waterman has received numerous awards and honors for her work, including election to the National Academy of Sciences and the Arthur S. Flemming Award for Public Service in Basic Science. Dr. Waterman has made fundamental advances in the understanding of cell migration and has authored or co-authored more than 150 papers. She currently serves on the editorial boards of <em>Current Biology</em> and <em>Journal of Microscopy</em>. Dr. Waterman is a member of the American Society for Cell Biology, Royal Microscopical Society, Biophysical Society, and is a Council Member of Gordon Research Conferences Organization.</p>
        </div>
                      <div>
          <h3>James Cutmore</h3>
          <p><em>Picture Editor at BBC Science Focus Magazine</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/James-Cutmore.jpg" width="120"></p><p>James Cutmore has worked for BBC’s science and technology publication <em><a href="https://www.sciencefocus.com/" target="_blank" rel="noreferrer noopener">BBC Science Focus</a></em> since 2004, telling compelling science stories through stunning science imagery. He holds a bachelor’s degree in fine art from the University of West England and was highly commended in 2020, having been nominated for the British Society of Magazine Editors Talent Awards. He is passionate about sourcing images that not only illustrate a range of difficult and complex concepts but highlight positive technology and the natural world. For many years he was a judge for the Wellcome Trust’s image competition, as well as the Royal Photographic Society.</p>
        </div>
                      <div>
          <h3>Ed Cara</h3>
          <p><em>Science and Health Reporter at Gizmodo</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Ed-Cara.jpg" width="120"></p><p>Born and raised in New York City, Ed Cara currently covers the public health and science beat at <em><a href="https://gizmodo.com/author/edcara" target="_blank" rel="noreferrer noopener">Gizmodo</a></em>. His past feature and investigative reporting can be seen in <em>The Atlantic</em>, <em>Pacific Standar</em>d and <em>Undark Magazine</em>.</p>
        </div>
                      <div>
          <h3><a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">Dr. Igor Siwanowicz</a></h3>
          <p><em>Research Scientist at Howard Hughes Medical Institute</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Igor-Siwanowicz.jpg" width="120"></p><p><a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">Igor Siwanowicz</a> began his career in biochemistry, but his love for animals and nature, and a desire to see the bigger picture, drove him to refocus his scientific discipline on neurobiology. This transition was facilitated by his expertise in invertebrate morphology, which he developed through macro photography. A confocal microscope became his key tool of trade, allowing for even more intimate insight into natural forms than a macro lens. Dr. Siwanowicz’s images have placed a total of <a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">25 times in the Nikon Small World</a> and other scientific imaging competitions. In 2020, he received an award from the Royal Photographic Society for scientific imaging. For the past 10 years, Dr. Siwanowicz has been studying the anatomy and biomechanics of movement in invertebrates at the <a href="https://www.nikonsmallworld.com/organizations/howard-hughes-medical-institute-hhmi">Janelia Research Campus of Howard Hughes Medical Institute</a> in Ashburn, Virginia.</p>
        </div>
                      <div>
          <h3>Dr. Gary Laevsky</h3>
          <p><em>Director of the Confocal Imaging Facility at Princeton University</em><br><a href="https://www.nikonsmallworld.com/organizations/princeton-university"></a></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Gary-Laevsky.jpg" width="120"></p><p>Gary Laevsky received his Ph.D. in Cell Biology from the University of Connecticut in 2003 and completed his post-doctoral research at <a href="https://www.nikonsmallworld.com/organizations/the-scripps-research-institute">The Scripps Research Institute</a> in La Jolla, California. Prior to joining the Department of Molecular Biology at <a href="https://www.nikonsmallworld.com/organizations/princeton-university">Princeton University</a>, he was the Product Manager for Olympus, a Senior Biosystems Applications Manager for Nikon, and an Imaging Applications Specialist for Andor Technology. Since joining Princeton University in 2013 as Director of the Confocal Imaging Facility, he has achieved the title of Senior Professional Specialist, the highest non-faculty level position that can be achieved. He has also been selected to sit on the Committee on Appointments and Advancement for Professional Researchers and Professional Specialists. Dr. Laevsky is a co-founder of the North Atlantic Microscopy Society (NAMS), Course Laboratory Director for the Analytical and Quantitative Light Microscopy (AQLM) course at the <a href="https://www.nikonsmallworld.com/organizations/marine-biological-laboratory">Marine Biological Laboratory (MBL)</a>, and co-founder of the Light-Sheet Fluorescence Microscopy (LSFM) Conference and Workshop, also at MBL.</p>
        </div>
          </div>
    
      </section>
  </section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shadow: New browser engine made almost entirely in JavaScript (433 pts)]]></title>
            <link>https://goose.icu/introducing-shadow/</link>
            <guid>38043033</guid>
            <pubDate>Fri, 27 Oct 2023 19:46:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://goose.icu/introducing-shadow/">https://goose.icu/introducing-shadow/</a>, See on <a href="https://news.ycombinator.com/item?id=38043033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h3>&lt;shadow&gt; is a new novel browser engine made almost entirely in JS</h3></p></div><section><p>So I started making a browser engine (for fun) a few days ago, it felt kind of inevitable so here we are. Here’s a short rundown. <a href="https://github.com/canadahonk/shadow" target="_blank" rel="noopener">Source code too!</a></p><h2 id="try-it-in-your-browserhttpsshadowgooseicu"><a href="https://shadow.goose.icu/" target="_blank" rel="noopener">Try it in your browser!</a></h2><h4 id="screenshot-of-ltshadowgts-welcome-page-running-inside-ltshadowgt-as-of-writing">Screenshot of <em><strong>&lt;shadow&gt;</strong></em>’s welcome page running inside <em><strong>&lt;shadow&gt;</strong></em> (as of writing)</h4><p><img src="https://github.com/CanadaHonk/shadow/assets/19228318/9431b624-94aa-4209-87c9-60d2478badf6" alt="Screenshot of shadow&amp;rsquo;s welcome page in shadow"></p><h3 id="what">What?</h3><p>A browser(/web) engine essentially takes in a URL(/etc) and gives you it rendered into a window for you to view and interact with. <em><strong>&lt;shadow&gt;</strong></em> does this too, almost entirely from scratch, made in JS. It runs in your browser! Node backend soon™ too? The host browser(/etc) is only used for networking (<code>fetch</code>) and renderer backend (<code>&lt;canvas&gt;</code>).</p><h4 id="components-of-ltshadowgt">Components of <em><strong>&lt;shadow&gt;</strong></em></h4><p><img src="https://mermaid.ink/svg/pako:eNpVkT1vhDAMhv8KygxVaTuB1Kljp96axSLukQtJUByK0N399zogFJAyOI_f1x_yXXReoWjENcDYF98_rXRF0YH7A6peqs_eU0zEYZx9MNpdj5R6UH6uGEQ7jBAIwwlnV8LKW2YdUVZmH2c4vzbfBYwGWPwUszn_t4hRQKcwbOX2-KQ8wG2vBGcgmzahUXPSemdwOXKjkXDIjR9-jJV2j9vqvlEed42TTbp1Lu3MJS4DFq9lXdb83gqKwRtsAqr2LKnfy_pjT_MFllaUwvI8oBXf5J7UUsQeLUrRcKggGCmke7IOpugvi-tEE8OEpZhGBRG_NHAhK5pfGAif_zYlpas?bgColor=101418" alt="Component flowchart">
(red = external/not me)</p><h3 id="why-make-it">Why make it?</h3><p>It’s just for fun, no really. Learning too. It will probably never work with 90% of websites, and that’s okay. <del>Also it’s funny to see people react in many forms of “wtf?"</del></p><h3 id="screenshot-of-serenityosorg-running-inside-ltshadowgt-left-vs-firefox-right">Screenshot of serenityos.org running inside <em><strong>&lt;shadow&gt;</strong></em> (left) vs Firefox (right)</h3><p>Pretty spot on! No list markers yet. Colors are different as UA/browser defined.</p><h3 id="name">Name</h3><p>As with all my recent projects, the name is because I thought it was kind of funny at the time. <em><strong>&lt;shadow&gt;</strong></em> is named after the defunct &lt;shadow&gt; element. I mostly stole this idea from Blink (was it intentional to name it after a dead HTML element at the time?). Also it sounds spooky and mysterious so that’s a bonus I guess? (It’s also stylized as <em><strong>&lt;shadow&gt;</strong></em> because of this.)</p><h3 id="it-supports-javascript">It supports JavaScript??!</h3><p>Yes, kind of. This is quite complicated (as you can probably imagine), so I’ll do it in a separate future dedicated post if you’re interested (DM/reply me on twitter).</p><h3 id="why-publish-it">Why publish it?</h3><p>Why not? If someone can learn something or just find what I make fun/interesting, that makes me happy :)</p><h3 id="but-making-a-new-browser-engine-is-impossible">But making a new browser engine is impossible!</h3><p><a href="https://ladybird.dev/" target="_blank" rel="noopener">No</a>. <a href="https://servo.org/" target="_blank" rel="noopener">It</a>. <a href="https://www.ekioh.com/flow-browser/" target="_blank" rel="noopener">Isn’t</a>! <em>(Also I don’t really care how possible/feasible something is.)</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The convolution empire strikes back (127 pts)]]></title>
            <link>https://gonzoml.substack.com/p/the-convolution-empire-strikes-back</link>
            <guid>38042954</guid>
            <pubDate>Fri, 27 Oct 2023 19:39:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gonzoml.substack.com/p/the-convolution-empire-strikes-back">https://gonzoml.substack.com/p/the-convolution-empire-strikes-back</a>, See on <a href="https://news.ycombinator.com/item?id=38042954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><strong>Authors</strong><span>: Samuel L. Smith, Andrew Brock, Leonard Berrada, Soham De</span><br><strong>Paper</strong><span>: </span><a href="https://arxiv.org/abs/2310.16764" rel="">https://arxiv.org/abs/2310.16764</a></p><p><span>The empire strikes back for the second time (I’d say the first time was </span><a href="https://arxiv.org/abs/2201.03545" rel="">ConvNeXt</a><span>).</span></p><p><span>There's a common perception that convolutional networks (ConvNets) perform well on small to medium-sized datasets, but when it comes to extremely large datasets, they fall short of transformers, particularly Vision Transformers (ViT) - more on this </span><a href="https://arxiv.org/abs/2010.11929" rel="">here</a><span>. The latest research from DeepMind challenges this notion.</span></p><p><span>It has been believed that the scalability of transformers surpasses that of ConvNets, but there's scant evidence to back this up. Furthermore, many studies that delve into ViT compare them with relatively weak convolutional baselines, sometimes training with enormous computational budgets exceeding 500k TPU-v3 core hours. This equates to approximately $250k based on </span><a href="https://cloud.google.com/tpu/pricing" rel="">current on-demand prices</a><span>. Such a budget is significantly beyond what's typically allocated for training convolutional networks.</span></p><p><span>In this study, the authors utilize the </span><a href="https://arxiv.org/abs/2102.06171" rel="">NFNet (Normalizer-Free ResNets) family</a><span>. They progressively increase the width and depth of these networks. This is a purely convolutional architecture and the latest of its kind to achieve </span><a href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="">state-of-the-art (SoTA) results on ImageNet</a><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png" width="484" height="260.3092105263158" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:327,&quot;width&quot;:608,&quot;resizeWidth&quot;:484,&quot;bytes&quot;:232842,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Without major modifications (except for simple hyperparameter tuning), these architectures are pre-trained on the expansive JFT-4B dataset (with 4 billion labeled images across 30k classes) with computational budgets ranging from 0.4k to 110k TPU-v4 core compute hours. It's noteworthy that the TPU-v4 has about twice the computational power as the v3, but with the same memory capacity. </p><p><span>Subsequently, the pre-trained networks are fine-tuned on ImageNet using </span><a href="https://arxiv.org/abs/2010.01412" rel="">Sharpness-Aware Minimization (SAM)</a><span>. The results show performance on par with ViT models that have comparable budgets. All models consistently improve as computational power is added. The largest model, NFNet-F7+, is pre-trained over 8 epochs (110k TPU-v4 hrs), fine-tuned (1.6k TPU-v4 hrs), and achieves 90.3% top-1 accuracy (90.4% with 4x augmentation).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png" width="1211" height="750" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:1211,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:580050,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>An interesting observation during the training process is the clear linear trend of the validation loss curve. This is consistent with the log-log scaling law between validation loss and the amount of computation during pre-training. This mirrors the same scaling laws observed for transformers in language modeling tasks. The authors identified an optimal scaling regime wherein the model size and training epochs increase at the same rate. They also pinpointed optimal learning rates.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png" width="1204" height="645" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:645,&quot;width&quot;:1204,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:693438,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Another intriguing finding is that models with the lowest validation loss don't always yield the best performance post fine-tuning. A similar phenomenon has been observed with transformers. For fine-tuning, slightly larger models and slightly smaller epoch budgets consistently outperform others. Occasionally, a slightly higher learning rate can also be beneficial.</p><p><span>The takeaway? </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">The bitter lesson</a><span>! Computational power and data remain the key driving factors.</span></p><p>However, it's important to note that models have their own inductive biases. The authors acknowledge that in certain situations, ViT might be a more suitable choice, possibly due to its ability to employ uniform components across different modalities.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is failing to deliver emails that mention newclimate.org (174 pts)]]></title>
            <link>https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the</link>
            <guid>38042114</guid>
            <pubDate>Fri, 27 Oct 2023 18:30:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the">https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the</a>, See on <a href="https://news.ycombinator.com/item?id=38042114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                            <center>
<p><em>(in case you share a link to this page by email, please use this bit.ly link <a href="https://bit.ly/NewClimate_MicrosoftStatement">https://bit.ly/NewClimate_MicrosoftStatement</a> to ensure delivery)</em></p>
</center>

<p>Since 17 October, to our knowledge any email with the NewClimate URL (newclimate dot org) in the email body, link, signature, reply header or contained anywhere in an attachment is unjustifiably quarantined by Microsoft email servers without any notice, regardless of who sends or receives the email.</p>

<p>That our own email flow is disrupted is the least of our problems: all Microsoft tenants are afflicted by the same issue when this URL appears in their emails, or any attachments they share, even when we are not a party to the communications.</p>

<p><strong>Hundreds of governmental and non-governmental organisations working on climate change appear to be experiencing disruption to email communication when their communications contain any reference to the NewClimate URL. </strong></p>

<p>For example, as per our understanding:</p>

<ul><li>No organisation using Microsoft email services can currently send the <strong>IPCC Sixth Assessment Report of Working Group 3 </strong>as an attachment to anyone else (newclimate dot org URL appears 11 times in the report). The same applies to <strong>hundreds of other relevant scientific papers and reports </strong>from any organisations, where NewClimate URLs appear on the reference lists.</li>
	<li>Any <strong>multi-organisation email chain</strong> where <em>any </em>of the participants uses Microsoft email services is breaking down in the case that NewClimate URLs are included. This could arise either because a NewClimate colleague is on the mailing list in the chat history, or if a NewClimate publication is linked to, in the email or the chat history.</li>
	<li>Even <strong>a link to this article on the NewClimate website cannot be spread by email </strong>if the sender or recipient uses Microsoft as email service.</li>
</ul><p>We understand that the majority of our partner organisations within the climate community use Microsoft email services, including the United Nations Framework Convention for Climate Change (UNFCCC) and the Intergovernmental Panel on Climate Change (IPCC).</p>

<p>It came as a surprise to us that this is even possible. It remains unclear whether this is the result of a targeted attack on Microsoft’s infrastructure against NewClimate, or simply a highly unfortunate error on the part of Microsoft. In our consultations with Microsoft and a number of independent IT experts, we have confirmed that we are not on any blacklist and our website is also free of malware.</p>

<p>We are fully dependent on Microsoft to prioritise and solve the issue, but Microsoft support agents have been difficult to engage and – to our understanding – disinclined to prioritise the issue. It is not clear whether Microsoft is aware of the inconvenience and disruption beyond our own organisation.</p>

<p>Beyond being an existential threat to our own organisation, this issue could significantly disrupt communication within the climate community at a time when it is most critical in the run up to COP28 in at the end of 2023 in Dubai. <strong>We greatly appreciate any support to bring this issue to the attention of Microsoft’s senior management so that it can be prioritised and resolved.</strong></p>

<p>This is extremely unfortunate and we apologise for any inconvenience that this may cause or may have caused.</p>

<p>We understand that partner organisations who use Microsoft email exchange services and that are experiencing email deliverability problems because of this issue may be able to apply a band-aid fix for email flow, by reporting to Microsoft that the NewClimate URL should not be blocked (see ‘Submission form for reporting false positives for Microsoft’s URL detonation policy’, available in the Microsoft admin center). This may help individual organisations to improve issues with their own incoming and outgoing email flow, but it does not help to resolve communication issues with other organisations that use Microsoft.</p>

<p>Emails are only quarantined when the text “newclimate(dot)org” appears in the email and not if a diverted link to the NewClimate website through e.g. tinyurl is included instead.</p>

                                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google paid $26.3B in 2021 to be the default search engine everywhere (174 pts)]]></title>
            <link>https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion</link>
            <guid>38041335</guid>
            <pubDate>Fri, 27 Oct 2023 17:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion">https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion</a>, See on <a href="https://news.ycombinator.com/item?id=38041335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The <a href="https://www.theverge.com/23869483/us-v-google-search-antitrust-case-updates"><em>US v. Google</em> antitrust trial</a> is about many things, but more than anything, it’s about the power of defaults. Even if it’s easy to switch browsers or platforms or search engines, the one that appears when you turn it on matters a lot. Google obviously agrees and has paid a staggering amount to make sure it is the default: testimony in the trial revealed that Google spent a total of $26.3 billion in 2021 to be the default search engine in multiple browsers, phones, and platforms.</p><p>That number, the sum total of all of Google’s search distribution deals, came out during the Justice Department’s cross-examination of Google’s search head, Prabhakar Raghavan. It was made public after a debate earlier in the week between the two sides and Judge Amit Mehta over whether the figure should be redacted. Mehta has begun to push for more openness in the trial in general, and this was one of the most significant new pieces of information to be shared openly.</p><p>Just to put that $26.3 billion in context: Alphabet, Google’s parent company, announced in <a href="https://www.theverge.com/2023/10/24/23929496/google-alphabet-q3-2023-earnings-ads-ai-sge">its recent earnings report</a> that Google Search ad business brought in about $44 billion over the last three months and about $165 billion in the last year. Its entire ad business — which also includes YouTube ads — made a bit under $90 billion in profit. This is all back-of-the-napkin math, but essentially, Google is giving up about 16 percent of its search revenue and about 29 percent of its profit to those distribution deals. </p><div><p>Google is giving up about 16 percent of its search revenue and about 29 percent of its profit to those distribution deals</p></div><p>Most of that money, of course, goes to Apple. <em>The New York Times</em> recently reported that Google’s deal to be the default search engine in Safari across Google products cost the company <a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion">about $18 billion</a> in 2021. (Apple’s outsize percentage of the total is why that particular deal <a href="https://www.theverge.com/2023/10/11/23913287/us-v-google-apple-search-deal">has been such a focus</a> of the first weeks of the trial.) In addition, Google pays Mozilla for default placement in Firefox; it pays Samsung for the same on its devices; and it has deals with many device makers, wireless carriers, and other platforms to be the default as well.</p><p>Until now, these numbers have been closely held secrets, leaving competitors and analysts to speculate about exactly what it’s worth to Google to be the near-universal default choice. The information also comes as Google is beginning its defense portion of the trial, which started with Raghavan testifying that Google is at perpetual risk of losing its cool — and its users — to platforms like TikTok and ChatGPT. Raghavan said that some users call his search engine “Grandpa Google.” (Raghavan has been <a href="https://www.theverge.com/23365101/tiktok-search-google-replacement">saying stuff like this</a> for a while now.) He also said that he sees Yelp and Amazon as competitors and that, in such a hot market, Google has to do everything it can to stay relevant and compete. The Justice Department, on the other hand, is making the case that spending $26.3 billion on securing default status everywhere is actually a way to make sure the market <em>isn’t</em> competitive. After a few more weeks of testimony, Mehta will have to decide who’s right.</p></div></div>]]></description>
        </item>
    </channel>
</rss>