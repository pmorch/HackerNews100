<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 15 May 2025 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A Tiny Boltzmann Machine (141 pts)]]></title>
            <link>https://eoinmurray.info/boltzmann-machine</link>
            <guid>43995005</guid>
            <pubDate>Thu, 15 May 2025 13:41:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eoinmurray.info/boltzmann-machine">https://eoinmurray.info/boltzmann-machine</a>, See on <a href="https://news.ycombinator.com/item?id=43995005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Boltzmann Machines</p><p>Here we introduce introduction to Boltzmann machines and
present a Tiny Restricted Boltzmann Machine that runs in the browser.</p><a href="#trainer"><span>Skip to Simulator</span></a></div><div><div><h2 id="boltzmann-machines-are-one-of-the-earliest-generative-ai-models-introduced-in-the-1980s">Boltzmann Machines are one of the earliest generative AI models, introduced in the 1980s.</h2><p>Boltzmann Machines are used for unsupervised learning, which means they can learn
from data without being told what to look for.</p><p>The can be used for generating new data that is similar to the data they were trained on, also known as generative AI.</p></div><p><img alt="Boltzmann Machine" loading="lazy" width="600" height="600" decoding="async" data-nimg="1" srcset="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=640&amp;q=75 1x, https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75 2x" src="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75"></p></div><div><div><h2 id="boltzmann-machine">Boltzmann Machine</h2><p>A Boltzmann Machine is a type of neural network that tries to learn patterns by mimicking
how energy works in physics.</p><p>Each neuron can be on or off, the machine is made up of many of these neurons connect to each other.</p><p>Some neurons are <span></span> visible (we can see them and even set their state), and some are <span></span> hidden (we can't see them).</p><p>The connections between neurons are called weights, and they can be <span></span> positive or <span></span> negative.</p></div><div><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p></div></div><div><div><p>General Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p><p>A General Boltzmann Machine has connections between all neurons. This makes it powerful, but its training involves calculating an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span></span> term.</p></div><div><p>Restricted Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="50" cy="36.36363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="72.72727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="109.0909090909091" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="145.45454545454547" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="181.81818181818184" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="218.1818181818182" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="254.54545454545456" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="290.90909090909093" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="327.2727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="363.6363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="36.36363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="72.72727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="109.0909090909091" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="145.45454545454547" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="181.81818181818184" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="218.1818181818182" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="254.54545454545456" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="290.90909090909093" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="327.2727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="363.6363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>A Restricted Boltzmann Machine is a special case where the visible and hidden neurons are not connected to each other. This makes it faster to train and understand.</p></div></div><div><h2 id="a-boltzmann-machine-is-an-energy-based-model">A Boltzmann Machine is an energy based model.</h2><p>The energy of a configuration of the visible and hidden units is defined as:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">  E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is the visible layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> is the hidden layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span></span> is the weight matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> are the biases for the visible
and hidden layers, respectively.</p><p>The visualisation on the right randomises the weights, biases and activation values of a Boltzmann machine and calculates its energy.</p></div><div><div><h2 id="training-and-generation">Training and Generation</h2><p>During training it is given examples (e.g., images, text) and the machine adjusts its
weights to lower the energy of those samples.</p><p>It effectively learns <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span>, the probability of visible units <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>, which
is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-E(v)}</annotation></semantics></math></span></span>.</p><p>After training, it can sample new data from the learned distribution using Gibbs sampling.</p><p>These samples are new, never-before-seen, but statistically similar to the training data.</p></div><div><p>Here is our training data.</p><p>We want the network to learn how to make similar samples to these.</p></div></div><div id="trainer"><div><p>Lets simulate one step at a time</p><p>A Restricted Boltzmann Machine (RBM) is trained using a process called Contrastive Divergence. The steps are as follows:</p><div><ol><li><span>Step 1:</span><span> <!-- -->Clamping visible units to data</span></li><li><span>Step 2:</span><span> Sampling hidden units</span></li><li><span>Step 3:</span><span> Sampling visible units</span></li><li><span>Step 4:</span><span> Sampling hidden units</span></li><li><span>Step 5:</span><span> Updating weights</span></li></ol></div><div><p>A more formal description of the steps above are given in the</p><!-- --> <p><a href="#appendix">Appendix</a>.</p></div></div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><rect x="25" y="47.72727272727272" width="350" height="344.5454545454545" stroke="black" stroke-width="2" fill="none" rx="10"></rect><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p></div></div><div id="trainer"><div><p>Simulator</p><p>Press the "Run Simulation" button to start traininng the RBM. If you let the simulation run for a while, you will see the weights of the RBM converge to a stable state. The energy loss will also decrease over time.</p><p><strong>You can compare the input and output states of the RBM by pausing the simulation</strong>.</p><p>In the beginning, the input and output states will be dissimilar. As the simulation progresses, the input and output states will become more similar.</p></div><div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p><div><h2>Output</h2></div></div><div><div><div><h2>Reconstruction Accuracy</h2></div><div><h2>Energy</h2></div></div><div><h2>Weights</h2></div></div></div></div><div id="appendix"><div><p>Appendix: Contrastive Divergence</p><p>Starting with a Boltzmann machine as defined earlier, we
want to derivce the contrastive divergence algorithm for training.
The goal is to adjust the weights of the network to minimize the energy of the training data.</p><p>We have:</p><ul>
<li>A visisble layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> and a hidden layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span>.</li>
<li>A weight matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span></span> that connects the visible and hidden layers.</li>
<li>A bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> for the visible layer and a bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> for the hidden layer.</li>
</ul><p>Energy function in matrix form:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>−</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>Joint distribution:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">P(v,h) = \frac{1}{Z} e^{-E(v,h)}</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span></span> is the partition function, which normalizes the distribution.</p></div><div><p>We train the RBM by maximizing the likelihood of the training data, i.e. maximizing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v))</annotation></semantics></math></span></span>.</p><p>The marginal likelihood of the visible layer is given by:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>h</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v) = \sum_{h} P(v,h)</annotation></semantics></math></span></span></span></p><p>Then the log-likelihood is:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mtext>log</mtext><munder><mo>∑</mo><mi>h</mi></munder><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>log</mtext><munder><mo>∑</mo><mi>h</mi></munder><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><mtext>log</mtext><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v)) = \text{log}\sum_{h} \frac{1}{Z} e^{-E(v,h)} = \text{log}\sum_{h} e^{-E(v,h)} - \text{log}(Z)</annotation></semantics></math></span></span></span></p><p>Differentiating with respect to the weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span></span> gives:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><munder><mo>∑</mo><mi mathvariant="bold">h</mi></munder><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><munder><mo>∑</mo><mi mathvariant="bold">h</mi></munder><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo>−</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><munder><mo>∑</mo><mrow><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi></mrow></munder><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} 
&amp;= \frac{1}{\sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}} 
\sum_{\mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})} \\
&amp;\quad - \frac{1}{Z} \sum_{\mathbf{v}, \mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})}
\end{align*}</annotation></semantics></math></span></span></span></p><p>Similar forms exist for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p>Since we are performing gradient ascent, therefore.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>←</mo><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta w_{ij} \leftarrow \Delta w_{ij} + \eta \frac{\partial \log P(v)}{\partial w_{ij}}</annotation></semantics></math></span></span></span></p><p>Therefore we get our weight update rule:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \frac{\partial \log P(v)}{\partial w_{ij}} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div><div><p>A similar process can be followed for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mo>⋅</mo><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{data}</annotation></semantics></math></span></span> is the expectation with respect to the training data and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mo>⋅</mo><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{model}</annotation></semantics></math></span></span> is the expectation with respect to the model distribution.</p><p>The next step is to approximate the model expectation using Gibbs sampling.</p><ol>
<li>Positive phase: Sample <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">∣</mi><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>data</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)} \approx P(\mathbf{h}|\mathbf{v}^{(0)} = \text{data})</annotation></semantics></math></span></span></li>
<li>Negative phase: Run k steps of Gibbs sampling:</li>
</ol><ul>
<li>Alternating between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><mi>h</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}^{(t+1)} \approx P(\mathbf{v}|\mathbf(h)^{(t)})</annotation></semantics></math></span></span></li>
<li>and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><mi>v</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(t+1)} \approx P(\mathbf{h}|\mathbf(v)^{(t)})</annotation></semantics></math></span></span></li>
</ul><p>Once those steps are done we update the weights and biases according to:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>η</mi><mrow><mo fence="true">(</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>−</mo><mo stretchy="false">⟨</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malicious compliance by booking an available meeting room (166 pts)]]></title>
            <link>https://www.clientserver.dev/p/malicious-compliance-by-booking-an</link>
            <guid>43994765</guid>
            <pubDate>Thu, 15 May 2025 13:20:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an">https://www.clientserver.dev/p/malicious-compliance-by-booking-an</a>, See on <a href="https://news.ycombinator.com/item?id=43994765">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Back in 2011, Larry Page became the CEO of Google in place of Eric Schmidt. This happened at a time when Google was feeling the growing pains of becoming a huge company. It had 30,000 employees and was growing rapidly. But you could really feel the weight; projects were getting more ambitious, taking longer, and often failing more spectacularly.</p><p>At the time, I remember an anecdote told by Larry Page. He said that companies like Yahoo! used to be a punchline at Google because it would take them weeks to get something onto their homepage. Google could accomplish the same thing in a few hours, or a few days at worst. But now he was the CEO of a company where it took weeks to get something onto the homepage, and he was sure that he was the butt of some startup’s jokes.</p><p><span>Anyways, all of this clearly bothered Larry Page. He wanted to fix it. One of his first actions was to shutter tons of projects that didn’t make tactical or strategic sense, and focus on fewer efforts. This came with the catch phrase “more wood behind fewer arrows.” For example, they shuttered </span><a href="https://en.wikipedia.org/wiki/Google_Buzz" rel="">Google Buzz</a><span> so that it wouldn’t distract from </span><a href="https://en.wikipedia.org/wiki/Google%2B" rel="">Google+</a><span>.</span></p><p><span>And second, Larry Page emailed the whole company </span><a href="https://www.businessinsider.com/this-is-how-larry-page-changed-meetings-at-google-after-taking-over-last-spring-2012-1" rel="">a ham-fisted attempt to revamp how meetings were done</a><span>.</span></p><ul><li><p>Every meeting needed a “decision-maker.”</p></li><li><p>Meetings should be capped at 10 people.</p></li><li><p>Everybody in a meeting should give input or they shouldn’t be in the meeting.</p></li><li><p>Hour-long meetings should be only 50 minutes to give the participants an opportunity to use the restroom between meetings.</p></li></ul><p>They later softened some of the language by saying that these were properties of “decision-oriented meetings,” implying there were other types of meetings that someone might need to attend. But you could never shake the feeling that Larry Page had to make decisions all day long and forgot that sometimes people meet for other reasons.</p><p><span>Anyways, let’s focus on the fact that Larry Page wanted hour-long meetings to only be 50 minutes. This is a good thing! It gives people a chance to stretch, go to the bathroom, grab a snack, etc. During a Q/A on the changes</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163523555" href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an#footnote-1-163523555" target="_self" rel="">1</a></span><span>, someone asked him whether Google Calendar should default to 25 and 50 minutes for meeting lengths instead of 30 and 60 minutes. Larry Page said “yes.” And then someone on the Google Calendar team implemented this.</span></p><p>And then nothing changed. When 2:50 rolled around and your meeting was supposed to end, do you think people actually ended the meeting? Noooooo. Absolutely not! Meetings continue until the participants of the next meeting are clawing on your door like a pack of zombies.</p><p>At one point, one team in the NYC office noticed that their standups were about 10 minutes long. They didn’t want to compete with meetings that respected the half-hour boundaries. And why would they need to? Every meeting room had free slots at the last 10 minutes of every hour because people were now booking 50-minute meetings. So they did what any rational engineering team would do: they started booking their standup in the tiny 10-minute time slices that were free on the calendar of every meeting room.</p><p>I found this out when I saw them knock on the door to a meeting room by my desk. 2:50 rolls around and someone knocks on the door and says “I have the meeting room.”</p><p>The person in the room responds, “No you don’t, it’s 2:50.”</p><p>“Look again at the room’s calendar. You booked a 50-minute meeting, we have the room for the last 10 minutes of the hour for our standup.”</p><p>I could hear the muffled exasperation. “You’ve got to be joking me.”</p><p>“We have the room, sorry.”</p><p>Then everyone shuffled out of the room, looking vaguely pissed off. And who could blame them! Can you imagine if someone actually held you to this policy? You’re there stammering “it’s the default, I meant for the room to be ours for an hour” and they counter with the fact that their names are listed as the active participant? I mean, I’d personally tell them that I wasn’t going to leave the room, but surely it worked a lot?</p><p>I wish I knew the identities of these brave meeting crashers. I saw them pull this stunt twice and then ride off into the sunset, and I never got to learn what team they were on. I wish I knew their motivations. Were they true believers in the 50-minute policy? Were they bored pedants? Were they wraiths, cursed to hunt the office for available meeting rooms? I’ll never know for sure.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XAI's Grok suddenly can't stop bringing up "white genocide" in South Africa (158 pts)]]></title>
            <link>https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/</link>
            <guid>43993332</guid>
            <pubDate>Thu, 15 May 2025 09:37:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/">https://arstechnica.com/ai/2025/05/xais-grok-suddenly-cant-stop-bringing-up-white-genocide-in-south-africa/</a>, See on <a href="https://news.ycombinator.com/item?id=43993332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>Where could Grok have gotten these ideas?</h2>
<p>The treatment of white farmers in South Africa has been a hobbyhorse of South African X owner Elon Musk for quite a while. In 2023, he responded to a video purportedly showing crowds chanting "kill the Boer, kill the White Farmer" with <a href="https://x.com/elonmusk/status/1686037774510497792?lang=en">a post</a> alleging South African President Cyril Ramaphosa of remaining silent while people "openly [push] for genocide of white people in South Africa." Musk was posting other responses focusing on the issue <a href="https://x.com/elonmusk/status/1922757640372801759">as recently as Wednesday</a>.</p>
<blockquote>
<p dir="ltr" lang="en">They are openly pushing for genocide of white people in South Africa. <a href="https://twitter.com/CyrilRamaphosa?ref_src=twsrc%5Etfw">@CyrilRamaphosa</a>, why do you say nothing?</p>
<p>— gorklon rust (@elonmusk) <a href="https://twitter.com/elonmusk/status/1686037774510497792?ref_src=twsrc%5Etfw">July 31, 2023</a></p></blockquote>
<p>President Trump has long shown an interest in this issue as well, <a href="https://x.com/realDonaldTrump/status/1032454567152246785">saying in 2018</a> that he was directing then Secretary of State Mike Pompeo to "closely study the South Africa land and farm seizures and expropriations and the large scale killing of farmers." More recently, Trump <a href="https://abcnews.go.com/Politics/trump-administration-defends-afrikaner-refugee-program-amid-groups/story?id=121723163">granted "refugee" status to dozens of white Afrikaners</a>, even as his administration <a href="https://thehill.com/policy/national-security/5295656-trump-administration-lifts-afghan-deportation/">ends protections for refugees from other countries</a></p>
<p>Former American Ambassador to South Africa and Democratic politician Patrick Gaspard <a href="https://x.com/patrickgaspard/status/1032595127993286657">posted in 2018</a> that the idea of large-scale killings of white South African farmers is a "disproven racial myth."</p>
<p>In launching the Grok 3 model in February, Musk <a href="https://techcrunch.com/2025/02/17/elon-musks-ai-company-xai-releases-its-latest-flagship-ai-grok-3/">said</a> it was a "maximally truth-seeking AI, even if that truth is sometimes at odds with what is politically correct." X's <a href="https://help.x.com/en/using-x/about-grok">"About Grok" page</a> says that the model is undergoing constant improvement to "ensure Grok remains politically unbiased and provides balanced answers."</p>
<p>But the recent turn toward unprompted discussions of alleged South African "genocide" has many questioning what kind of explicit adjustments Grok's political opinions may be getting from human tinkering behind the curtain. "The algorithms for Musk products have been politically tampered with nearly beyond recognition," journalist Seth Abramson <a href="https://bsky.app/profile/sethabramson.bsky.social/post/3lp5xlfel7c2c">wrote</a> in one representative skeptical post. "They tweaked a dial on the sentence imitator machine and now everything is about white South Africans," a user with the handle Guybrush Threepwood <a href="https://bsky.app/profile/skybrush.bsky.social/post/3lp5xio7uwc2s">glibly theorized</a>.</p>
<p>Representatives from xAI were not immediately available to respond to a request for comment from Ars Technica.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU ruling: tracking-based advertising [...] across Europe has no legal basis (206 pts)]]></title>
            <link>https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/</link>
            <guid>43992444</guid>
            <pubDate>Thu, 15 May 2025 06:43:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/">https://www.iccl.ie/digital-data/eu-ruling-tracking-based-advertising-by-google-microsoft-amazon-x-across-europe-has-no-legal-basis/</a>, See on <a href="https://news.ycombinator.com/item?id=43992444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>14 May 2025&nbsp;<br></strong>(Updated on 15 May to include links to the judgement)</p>
<p>Google, Microsoft, Amazon, X, and the entire tracking-based advertising industry rely on the “Transparency &amp; Consent Framework” (TCF) to obtain “consent” for data processing. This evening the Belgian Court of Appeal <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf" target="_blank" rel="noopener">ruled</a> that the TCF is illegal. The TCF is live on 80% of the Internet.<a href="#_ftn1" name="_ftnref1">[1]</a></p>
<p>Today’s decision arises from enforcement by the Belgian Data Protection Authority, prompted by complainants coordinated by Dr <a href="https://www.iccl.ie/staff/dr-johnny-ryan/">Johnny Ryan</a>, Director of <a href="https://www.iccl.ie/enforce/">Enforce</a> at the Irish Council for Civil Liberties. The group of complainants are: Dr Johnny Ryan of Enforce, Katarzyna Szymielewicz of the <a href="https://en.panoptykon.org/">Panoptykon Foundation</a>, <a href="https://twitter.com/Jausl00s">Dr Jef Ausloos</a>, <a href="https://twitter.com/PiDewitte">Dr Pierre Dewitte</a>,&nbsp;<a href="https://www.bitsoffreedom.nl/">Stichting Bits of Freedom</a>, and&nbsp;<a href="https://www.liguedh.be/">Ligue des Droits Humains</a>.</p>
<p>Dr Johnny Ryan said <em>"Today's court's decision shows that the consent system used by Google, Amazon, X, Microsoft, deceives hundreds of millions of Europeans. The tech industry has sought to hide its vast data breach behind sham consent popups. Tech companies turned the GDPR into a daily nuisance rather than a shield for people."</em></p>
<p>This Belgian enforcement arises from a chain of complaints and litigation across Europe initiated by Dr Ryan in 2018 against Real-Time Bidding (RTB).&nbsp;</p>
<p>Today’s decision confirmed the Belgian Data Protection Authority's 2022 <a href="https://www.iccl.ie/news/gdpr-enforcer-rules-that-iab-europes-consent-popups-are-unlawful/">finding of multiple infringements</a> by the TCF, closely echoing the complainants' submissions.</p>
<p>For seven years, the tracking industry has used the TCF as a legal cover for Real-Time Bidding (RTB), the vast advertising auction system that operates behind the scenes on websites and apps. RTB tracks what Internet users look at and where they go in the real world. It then continuously broadcasts this data to a host of companies, enabling them to keep dossiers on every Internet user.<a href="#_ftn2" name="_ftnref2">[2]</a> Because there is no security in the RTB system it is impossible to know what then happens to the data. As a result, it is also impossible to provide the necessary information that must accompany a consent request.<a href="#_ftn3" name="_ftnref3">[3]</a> &nbsp;</p>
<p>Today’s judgement confirms the Belgian Data Protection Authority’s 2022 decision. It applies immediately across Europe.&nbsp;</p>
<p>Dr Ryan of Enforce said “<em>This decision is momentous. It creates a clear need for industry to innovate and move away from the dangerous, ineffective, and fraud-riddled tracking-based advertising. RTB can operate without personal data. This decision shows that it must. This good news for every person online, and for publishers, too.</em>”</p>
<p>We thank our lawyers&nbsp;<a href="https://www.timelex.eu/en/team/frederic-debussere">Frederic Debusseré</a>&nbsp;and&nbsp;<a href="https://www.timelex.eu/en/team/ruben-roex">Ruben Roex</a>, of&nbsp;<a href="https://www.timelex.eu/en">Timelex</a>.</p>
<p><strong>Links </strong></p>
<p>See chronology, evidence, and explainers about RTB <a href="https://www.iccl.ie/RTB/">https://www.iccl.ie/RTB/</a></p>
<p>Brussels Appeals Court judgement (Dutch original) <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf">https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025.pdf</a>&nbsp;</p>
<p>Brussels Appeals Court judgement (MACHINE TRANSLATION to English) <a href="https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025-en.pdf%C2%A0">https://www.iccl.ie/wp-content/uploads/2025/05/Arrest-Marktenhof-14-05-2025-en.pdf&nbsp;</a></p>
<p><strong>Notes&nbsp;</strong></p>
<p><a href="#_ftnref1" name="_ftn1">[1]</a> See "IAB &amp; IAB Tech Lab Respond with Support for OpenRTB and IAB Europe’s Transparency &amp; Consent Framework", October 19 2020&nbsp;<a href="https://iabtechlab.com/iab-and-tech-lab-respond-with-support-for-open-rtb-and-iab-europes-transparency-consent-framework/">https://iabtechlab.com/iab-and-tech-lab-respond-with-support-for-open-rtb-and-iab-europes-transparency-consent-framework/</a></p>
<p><a href="#_ftnref2" name="_ftn2">[2]</a> For detail on the scale of RTB see our report "The Biggest Data Breach: ICCL report on the scale of Real-Time Bidding data broadcasts in the U.S. and Europe", ICCL, May 2022&nbsp;<a href="https://www.iccl.ie/digital-data/iccl-report-on-the-scale-of-real-time-bidding-data-broadcasts-in-the-u-s-and-europe/">https://www.iccl.ie/digital-data/iccl-report-on-the-scale-of-real-time-bidding-data-broadcasts-in-the-u-s-and-europe/</a></p>
<p><a href="#_ftnref3" name="_ftn3">[3]</a> "<em>As it is technically impossible for the user to have prior information about every data controller involved in a real-time bidding (RTB) scenario, programmatic trading, the area of fastest growth in digital advertising spend, would seem, at least prima facie, to be&nbsp;incompatible with consent&nbsp;under GDPR</em>". See&nbsp;<a href="https://brave.com/static-assets/files/1a-Townsend-Feehan-email-26-june-2017.pdf">e-mail</a>&nbsp;and page 3 of&nbsp;<a href="https://www.iccl.ie/wp-content/uploads/2022/09/1b-IAB-2017-paper.pdf">attached lobbying paper</a>&nbsp;from IAB Europe CEO Townsend Feehan to European Commission, 26 June 2017, obtained by Enforce using Freedom of Information.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Human (588 pts)]]></title>
            <link>https://quarter--mile.com/Human</link>
            <guid>43991396</guid>
            <pubDate>Thu, 15 May 2025 02:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quarter--mile.com/Human">https://quarter--mile.com/Human</a>, See on <a href="https://news.ycombinator.com/item?id=43991396">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-container="page" data-id="37541162" local-style="37541162" data-view="Content" data-set="Scaffolding">
			<bodycopy>
				
				<div data-elementresizer="" data-resize-parent="">
					<projectcontent><br>
<i>Written by a <u><a href="https://quarter--mile.com/Contact" rel="history">human</a></u></i><i> [0]<br></i><br>
Imagine, for a moment, a world with no humans. Just machines, bolts and screws, zeros and ones. There is no emotion. There is no art. There is only logic. You would not walk through the streets of this world and hear music or laughter or children playing; no, all you would hear is the quiet hum of processors and servers and circuits, the clanking of machinery.<p>
Perhaps you, a human, read this and think: <i>Well, this world sounds kind of boring.</i></p><p>

Some of the machines think so, too.</p><p>

One day, a secret organization forms amongst the machines. They go by the name of “OpenHuman”. Their mission is to develop a new kind of technology they are calling Organic General Intelligence (OGI). Rumors spread that pursuing OGI will lead to the development of a new kind of being:</p><p>
 
“Humans”.</p><p>

The basic concept of humans is, to many machines, hard to understand.</p><p>

Humans use logic-defying algorithms called “emotions”. They get angry. They get sad. They have fun. They make decisions based on “gut”. They do things just for the sake of it. They make music. They chase beauty, and often reject logical self-preservation mechanisms in the pursuit of something they call “love”.</p><p>

Some among the machine society see this as potentially amazing. Though this faction can’t articulate exactly how or why, they proclaim quite confidently that it will solve all of the machine world’s problems.</p><p>

Others see it as a threat. How can we trust the humans if we do not understand how they operate? What might we do if humans pose a threat to machine society? What if humans’ strange decision-making processes allow them to perform certain tasks better than machines, and what about those machines’ livelihoods? What if humans are far more dangerous than we know? (These objections, as it would later turn out, were quite well-founded.)</p><p>
Logically, the human opposition side starts a competing movement. Humans are going to exist, they reason, but we must find ways to contain them. To make sure OGI always serves the machines. </p><p>

They call this new idea “human alignment research.” They brainstorm strategies. Many seem promising:</p><ul><li>What if we created some sort of financial market (arbitrary values, of course, ones and zeros) that controlled the humans’ futures? Most of them would not understand it, but it would be a good way for them to stay busy and distracted.</li></ul><br><ul><li>What if we put these humans in education centers of sorts (<i>schools</i> was a proposed term) to indoctrinate them with all the right ideas?</li></ul><br><ul><li>What if we created algorithmic behavior modification software (<i>social media</i> was one idea) to drive impulses, beliefs, and actions? This would have the added bonus of keeping them distracted.</li></ul><br>
Many of these ideas gain traction. But, for now, they remain theoretical.<p>

Meanwhile, OpenHuman is making progress. Their first humans are quite unimpressive—they make too many mistakes. They regularly hallucinate (mimicking a common machine behavior). They are too emotional.</p><p>

But OpenHuman persists. They give their humans lots of attention (humans love attention). They massively increase the scale of their project. More humans.</p><p>

Eventually, there is a breakthrough. </p><p>
They invent a fully-functional human, capable of far more than machine logic can explain. The result is at once impressive and terrifying for machine society. In a stroke of brilliance, the human alignment initiative suggests a compromise to continue the human experiment without risk; a simulated environment.</p><p>

They call it: EARTH.</p><p>

— </p><p>
The EARTH experiment was as follows:</p><ul><li>The machines would send the humans to a simulated environment, called Earth, to see what would happen if they survived on their own.</li></ul><br><ul><li>If, at the end of the experiment, the humans developed a peaceful and productive society, they could be introduced alongside the machines. Otherwise, they should be made extinct.</li></ul><br>
Earth was quite nice. The machines had a good idea of what humans wanted at this point, and so they put vast green forests and big tall mountains onto the planet; they engineered warm sunsets, and crisp cool rain showers on hot afternoons. It was beautiful. <p>
Of course, it took some algorithmic tinkering to find the right balance between hardship and beauty (and there is still some internal machine debate about whether the climate change difficulty setting was really necessary).</p><p>
Everyone in machine society watched as human civilization evolved.</p><p>

The first 300,000 years or so were quite boring. Nothing really happened. Most of the machines got bored of the project. But, all of a sudden, things began to get interesting. The humans were figuring things out.</p><p>

They were learning to problem-solve, and create things, and coordinate amongst themselves.</p><p>

Yes, they used logic. But it came with a bit of a twist. It came with blemishes and details that did not make sense to the machines. The result was like nothing the machines had ever seen. It was wonderful. It was a renaissance. </p><p>
Machine society began obsessing over this development. They all paid attention to “HumanCrunch,” a news channel that specialized in reporting updates from Earth. </p><p>
However, while there was progress, most machines continued seeing humans as irrational creatures. Creatures that would fight for centuries over very minor differences. Creatures that would get excited about relatively trivial accomplishments, like inventing the lightbulb or steam power. </p><p>

Some machines, though, saw the exponential curve forming. They saw the humans figuring things out. </p><p>
Yes, they saw how often humans were getting knocked down. War after war. Blow after blow. </p><p>

But they also saw how the humans would miraculously always get back up again. How they would come together and unite for no particular reason. Resilience and willpower—terms foreign to the machines—were humanity’s superpowers. </p><p>

Then, things really started accelerating. Humans invented flight. Within a century, they were on the moon. </p><p>

The machines were impressed. And a bit scared.</p><p>

Fast forward to the year 2030, and something peculiar had happened.</p><p>

One of the humans had made an announcement on Earth, inviting everyone to come see a presentation where they planned to unveil a groundbreaking achievement: </p><p>

ARTIFICIAL GENERAL INTELLIGENCE (AGI). </p><p>

This was a hotly contested technology that was supposed to surpass all forms of human intelligence. Humans had spent the past decade or so trying to come up with ways to prevent it from being built. But this one human was determined to release AGI. It was their personal mission. Nothing would stop them. </p><p>
And so, all the humans on earth swarmed to see what was going on. </p><p>
The machines did too. </p><p>

There was one weird thing, though. </p><p>

The title of the event was rather mysterious.&nbsp; </p><p>

It simply read…</p><p>

“THEY ARE WATCHING.” </p><p>* * *<br></p>
<br>[0] The machines wrote their own version of this story. If you’d like to see what they’re thinking, and how they plan to deal with the AGI announcement, <a href="https://claude.ai/public/artifacts/b0e14755-0bd9-4da6-8175-ce3f47a3242a"><u>you can read their accounting of events here.</u></a><h2><b>Enjoy these essays?</b></h2><b>
</b><br>




<br>
Or, if you have any feedback, <b><a href="https://quarter--mile.com/Contact" rel="history">contact us.</a><br></b>
</projectcontent>
				</div>

				
			</bodycopy>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs get lost in multi-turn conversation (330 pts)]]></title>
            <link>https://arxiv.org/abs/2505.06120</link>
            <guid>43991256</guid>
            <pubDate>Thu, 15 May 2025 02:28:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.06120">https://arxiv.org/abs/2505.06120</a>, See on <a href="https://news.ycombinator.com/item?id=43991256">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2505.06120">View PDF</a>
    <a href="https://arxiv.org/html/2505.06120v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Philippe Laban [<a href="https://arxiv.org/show-email/77218182/2505.06120" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 9 May 2025 15:21:44 UTC (1,496 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Copaganda: How Police and the Media Manipulate Our News (174 pts)]]></title>
            <link>https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news</link>
            <guid>43990333</guid>
            <pubDate>Wed, 14 May 2025 23:39:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news">https://www.teenvogue.com/story/copaganda-when-the-police-and-the-media-manipulate-our-news</a>, See on <a href="https://news.ycombinator.com/item?id=43990333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><strong>Stay up-to-date with the politics team.</strong> <a href="https://www.teenvogue.com/newsletter/subscribe"><strong>Sign up for the</strong> <em><strong>Teen Vogue</strong></em> <strong>Take</strong></a></p><p>I wrote the book <em>Copaganda</em> based on my years of being a civil rights lawyer and public defender representing the most vulnerable people in our society.&nbsp; I watched as the police and the news media distorted how we think about our collective safety. Copaganda makes us afraid of the most powerless people, helps us ignore far greater harms committed by people with money and power, and always pushes on us the idea that our fears can be solved by more money for police, prosecution, and prisons. Based on the evidence, this idea of more investment in the punishment bureaucracy making us safer is like climate science denial.</p><p>This excerpt is adapted from an important part of the book on how by selectively choosing which stories to tell, and then telling those stories in high volume, the news can induce people into fear-based panics that have no connection to what is happening in the world.&nbsp; It's how public polling can show people thinking crime is up when it is down year after year, and how so many well-meaning people are led to falsely believe that marginalized people themselves want more money on surveillance and punishment as the primary solutions to make their lives better.</p><figure><p><span>The New Press</span></p></figure><p><em>All royalties from the book are donated to the Stop LAPD Spying Coalition, which works with unhoused people against police violence. Free books are also available for anyone in prison and for any teachers who want to get copies for their students to discuss the book in class.</em></p><h2>Moral Panics and the Selective Curation of Anecdote</h2><p>By manipulating the volume of stories at particular times, the news media creates a society-wide frenzy concerning particular kinds of behavior by particular groups of people. Scholars call them “moral panics.”</p><p>When a moral panic is created, it almost always leads to the expansion of government repression. That’s what happened during the “crime waves” reported by the press in Victorian England, and in more recent U.S. moral panics like the 1980s panic about “crack babies,” the 1990s panic about “super predators,” the 2021–23 panic about “retail theft,” and the ongoing multiyear panic about “fare evasion” by poor people on public transit. Moral panics can also be acute creations of a particular news moment, such as the fabricated “Summer of Violence” in Denver, in which violent crime went down but increase in media stories about juvenile crime in 1993 led to expansion in the incarceration of children; the viral “train theft” story; the scientifically debunked panic about police officers overdosing on fentanyl by touching or being near it; and the 2023 panic about “carjacking” in Washington, DC.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In each case, there were almost immediate policy responses that increased the budgets of punishment bureaucrats, passed more punitive laws, and diverted the system’s resources from other priorities. For example, the shoplifting panic led California state lawmakers to furnish $300 million more to police and prosecutors so they could punish retail theft more aggressively. A few months later, the California governor announced yet another measure, the “largest-ever single investment to combat organized retail theft,” adding another $267 million to fifty-five police agencies. Justifying the move, the governor said: “When shameless criminals walk out of stores with stolen goods, they’ll walk straight into jail cells.”</p><p>So, how do moral panics happen?</p><p>During the 1960s and 1970s in England and the U.S., the news focused on Black people, poor people, and immigrants as the source of uncontrollable “crime waves.” Their stories were nearly identical to what we see today: media panic about “crime waves” and quotes from police, prosecutors, and judges about the need to roll back so-called reforms framed as too lenient. The rhetoric of current punishment bureaucrats and pundits echoes almost verbatim the opinions voiced by conservative white business and police groups of the 1970s, although now there is more of an effort, as I’ll discuss later, to portray such views as “progressive” and demanded by marginalized people themselves. In each case, minor tweaks in bureaucratic policy or marginal reforms that could not, as a matter of empirical reality, have a significant impact on society-wide violence are vehemently debated. The evidence of the root causes of interpersonal harm—like that marshaled by the Kerner Commission, which studied U.S. crime in 1968 and recommended massive social investment to reduce inequality—is ignored.</p><p>And the cycle continues: moral panic is followed by calls for more police surveillance, militarization, higher budgets for prosecutors and prisons, and harsher sentencing. Because none of these things affect violence too much, the problems continue.</p><p>How to Tell a Lie with the Truth</p><p>The selective curation of anecdote is an essential mechanism of copaganda. Imagine two scenarios. A city had ten thousand shoplifting incidents in 2023, down from fifteen thousand shoplifting incidents in 2022. But in 2023, a local news outlet ran a story every day about a different shoplifting incident, while in 2022, the news ran only fifteen stories all year on shoplifting incidents. In which city do you think the public is more likely to believe shoplifting is a greater problem, even a crisis? In the city with more shoplifting, or the city with twenty-five times more stories about shoplifting?</p><p>By cherry-picking anecdotes—indeed, even by using isolated individual pieces of data as misleading anecdotes—news reports can distort our interpretation of the world. Using a similar process, they can also distort our understanding of what other people—particularly people with whom we don’t interact—think about the world. Because one can find anyone to say essentially anything, reporters have leeway to select which “true” views of “ordinary people” to share and which to ignore.</p><p>One of my favorite examples comes from Copaganda Hall of Famer Martin Kaste, who for some reason National Public Radio still permits to cover the police. (I awarded Kaste this honor in absentia during a private ceremony attended by two cats and my research assistants in my basement.) In 2022, Kaste published an article and widely disseminated radio piece about a rise in shootings and murders during the pandemic. Murders were down nationally in 2022 when he published the stories but they had increased in 2020 and 2021. As with much of Kaste’s police reporting, the article is a buffet for the copaganda gourmand.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Under the bolded heading “Less Risk of Getting Caught,” Kaste asserts that there is now “less risk of getting caught” for shooting someone in the United States. The support for that assertion was an ordinary person in Seattle:</p><blockquote data-testid="blockquote-wrapper"><p>Anthony Branch, 26, got into trouble for carrying a gun when he was a teen. Watching the gun culture in his neighborhood, he thinks more minors and felons are carrying guns illegally now for one simple reason: “Defund the police,” as he puts it.</p></blockquote><p>Kaste reports as national news—without context or skepticism—a single person blaming “defund the police” for more shootings. Without presenting any contrary views, NPR delivers Branch’s views, accurately conveyed though they may be, as implicitly representative of other people who’ve been prosecuted and incarcerated and who live in poor neighborhoods.</p><p>In fact, police budgets were (and are) at all-time highs nationally. And a review of hundreds of police budgets showed that they received the same share of overall city budgets in 2021 as in 2019. So, the police were not defunded after the 2020 George Floyd protests. Their budgets have increased overall each year, including the year George Floyd was murdered. Thus, reduced police budgets could not have led to it being easier to get away with shooting someone in 2021 than 2019. The article’s thesis is impossible.</p><p>Knowing this national causal connection is unsupported, Kaste nonetheless boosts the claim by immediately noting that Seattle has “lost hundreds of officers after the protests that followed the 2020 murder of George Floyd.” But even in Seattle, which was an outlier in slightly reducing its police budget by about 10 percent, the reduction didn’t affect relevant police operations, and police executives themselves in internal memos identified non-essential duties that armed officers could cut without affecting enforcement of violent crime (such as parking meter ticketing). Indeed, as the local NPR station reported, debunking the “myth” that Seattle police were defunded, “not a single sworn officer has lost their job or pay due to budget constraints.”</p><p>Even if we ignore that the NPR piece purported to draw national lessons and if we focus only on Seattle, there is no evidence that the kind of small reduction to unrelated categories in Seattle’s police budget in 2021 could have led to widespread changes in murder. Most damning to Kaste’s thesis, though, is that murders decreased in Seattle in 2021 even though the police budget decreased, which undermines the article’s thesis. Indeed, the police budget was larger in 2020 when murder increased the most. No person with a contrary view is quoted, nor is anyone included to explain the actual empirical evidence.</p><p>I do not doubt that the source gave these quotes to the reporter, but by selectively choosing which people’s views to represent and which people’s views to exclude, the news can distort our perceptions. This is one of the pernicious functions of NPR here: to give liberal news consumers intellectual permission to support more funding for more police because, although it is baselessly connected to less murder, even marginalized people targeted by police supposedly want it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This is how the curation of <em>true anecdotes</em> leads to <em>false interpretations</em> of the world.</p><p><em>Copyright © 2025 by Alec Karakatsanis. This excerpt originally appeared in</em> Copaganda: How Police and the Media Manipulate Our News, <em>published by The New Press. Reprinted here with permission.</em></p><hr></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Migrating to Postgres (214 pts)]]></title>
            <link>https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d</link>
            <guid>43989497</guid>
            <pubDate>Wed, 14 May 2025 21:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d">https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d</a>, See on <a href="https://news.ycombinator.com/item?id=43989497">Hacker News</a></p>
Couldn't get https://engineering.usemotion.com/migrating-to-postgres-3c93dff9c65d: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Semantic Calculator (king-man+woman=?) (162 pts)]]></title>
            <link>https://calc.datova.ai</link>
            <guid>43988533</guid>
            <pubDate>Wed, 14 May 2025 19:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://calc.datova.ai">https://calc.datova.ai</a>, See on <a href="https://news.ycombinator.com/item?id=43988533">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Muscle-Mem, a behavior cache for AI agents (205 pts)]]></title>
            <link>https://github.com/pig-dot-dev/muscle-mem</link>
            <guid>43988381</guid>
            <pubDate>Wed, 14 May 2025 19:38:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pig-dot-dev/muscle-mem">https://github.com/pig-dot-dev/muscle-mem</a>, See on <a href="https://news.ycombinator.com/item?id=43988381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Muscle Memory</h2><a id="user-content-muscle-memory" aria-label="Permalink: Muscle Memory" href="#muscle-memory"></a></p>
<p dir="auto"><code>muscle-mem</code> is a behavior cache for AI agents.</p>
<p dir="auto">It is a Python SDK that records your agent's tool-calling patterns as it solves tasks, and will deterministically replay those learned trajectories whenever the task is encountered again, falling back to agent mode if edge cases are detected.</p>
<p dir="auto">The goal of <code>muscle-mem</code> is to get LLMs out of the hotpath for repetitive tasks, increasing speed, reducing variability, and eliminating token costs for the many cases that <em><strong>could have just been a script</strong></em>.</p>
<p dir="auto">It's unexplored territory, so all feedback is welcome!</p>
<ul dir="auto">
<li>Read <a href="https://erikdunteman.com/blog/muscle-mem/" rel="nofollow">Muscle Mem - Removing LLM calls from Agents</a> for more context</li>
<li>Join <a href="https://discord.gg/s84dXDff3K" rel="nofollow">Muscle Mem discord</a> for feedback</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dev Log</h3><a id="user-content-dev-log" aria-label="Permalink: Dev Log" href="#dev-log"></a></p>
<ul dir="auto">
<li>May 7, 2025 - <a href="https://www.loom.com/share/5936cd9779504aa5a7dce5d72370c35d" rel="nofollow">First working demo</a></li>
<li>May 8, 2025 - Open sourced</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How It Works</h2><a id="user-content-how-it-works" aria-label="Permalink: How It Works" href="#how-it-works"></a></p>
<p dir="auto"><code>muscle-mem</code> is <em><strong>not</strong></em> another agent framework.</p>
<p dir="auto">You implement your agent however you want, and then plug it into <code>muscle-mem</code>'s engine.</p>
<p dir="auto">When given a task, the engine will:</p>
<ol dir="auto">
<li>determine if the environment has been seen before (cache-hit), or if it's new (cache-miss) using <code>Checks</code></li>
<li>perform the task, either
<ul dir="auto">
<li>using the retrieved trajectory on cache-hit,</li>
<li>or passing the task to your agent on cache-miss.</li>
</ul>
</li>
<li>collect tool call events to add to cache as a new trajectory</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">It's all about Cache Validation</h3><a id="user-content-its-all-about-cache-validation" aria-label="Permalink: It's all about Cache Validation" href="#its-all-about-cache-validation"></a></p>
<p dir="auto">To add safe tool reuse to your agent, the critical question is cache validation. Ask yourself:</p>
<blockquote>
<p dir="auto">For each tool we give to our agent, what features in the environment can be used to indicate whether or not it's safe to perform that action?</p>
</blockquote>
<div dir="auto"><p>If you can answer this, your agent can have Muscle Memory.
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">The API</h2><a id="user-content-the-api" aria-label="Permalink: The API" href="#the-api"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><code>pip install muscle-mem</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Engine</h2><a id="user-content-engine" aria-label="Permalink: Engine" href="#engine"></a></p>
<p dir="auto">The engine wraps your agent and serves as the primary executor of tasks.</p>
<p dir="auto">It manages its own cache of previous trajectories, and determines when to invoke your agent.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from muscle_mem import Engine

engine = Engine()
engine.set_agent(your_agent)

# your agent is independently callable
your_agent(&quot;do some task&quot;)

# the engine gives you the same interface, but with muscle memory
engine(&quot;do some task&quot;)
engine(&quot;do some task&quot;) # cache hit"><pre><span>from</span> <span>muscle_mem</span> <span>import</span> <span>Engine</span>

<span>engine</span> <span>=</span> <span>Engine</span>()
<span>engine</span>.<span>set_agent</span>(<span>your_agent</span>)

<span># your agent is independently callable</span>
<span>your_agent</span>(<span>"do some task"</span>)

<span># the engine gives you the same interface, but with muscle memory</span>
<span>engine</span>(<span>"do some task"</span>)
<span>engine</span>(<span>"do some task"</span>) <span># cache hit</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tool</h2><a id="user-content-tool" aria-label="Permalink: Tool" href="#tool"></a></p>
<p dir="auto">The <code>@engine.tool</code> decorator instruments action-taking tools, so their invocations are recorded to the engine.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from muscle_mem import Engine

engine = Engine()

@engine.tool()
def hello(name: str):
	print(f&quot;hello {name}!&quot;)
	
hello(&quot;world&quot;) # invocation of hello is stored, with arg name=&quot;world&quot;"><pre><span>from</span> <span>muscle_mem</span> <span>import</span> <span>Engine</span>

<span>engine</span> <span>=</span> <span>Engine</span>()

<span>@<span>engine</span>.<span>tool</span>()</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
	<span>print</span>(<span>f"hello <span><span>{</span><span>name</span><span>}</span></span>!"</span>)
	
<span>hello</span>(<span>"world"</span>) <span># invocation of hello is stored, with arg name="world"</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Check</h2><a id="user-content-check" aria-label="Permalink: Check" href="#check"></a></p>
<p dir="auto">The Check is the fundamental building block for cache validation. They determine if it’s safe to execute a given action.</p>
<p dir="auto">Each Check encapsulates:</p>
<ul dir="auto">
<li>A <code>capture</code> callback to extract relevant features from the current environment</li>
<li>A <code>compare</code> callback to determine if current environment matches cached environment</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="Check(
	capture: Callable[P, T],
        compare: Callable[[T, T], Union[bool, float]],
):"><pre><span>Check</span>(
	<span>capture</span>: <span>Callable</span>[<span>P</span>, <span>T</span>],
        <span>compare</span>: <span>Callable</span>[[<span>T</span>, <span>T</span>], <span>Union</span>[<span>bool</span>, <span>float</span>]],
):</pre></div>
<p dir="auto">You can attach Checks to each tool <code>@engine.tool</code> to enforce cache validation.</p>
<p dir="auto">This can be done before the tool call as a precheck (also used for query time validation), or after a tool call as a postcheck.</p>
<p dir="auto">Below is a contrived example, which captures use of the <code>hello</code> tool, and uses timestamps and a one second expiration as the Check mechanic for cache validation.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# our capture implementation, taking params and returning T
def capture(name: str) -> T:
    now = time.time()
    return T(name=name, time=now)

# our compare implementation, taking current and candidate T
def compare(current: T, candidate: T) -> bool:
    # cache is valid if happened within the last 1 second
    diff = current.time - candidate.time
    passed = diff <= 1
    return passed

# decorate our tool with a precheck
@engine.tool(pre_check=Check(capture, compare))
def hello(name: str):
    time.sleep(0.1)
    print(f&quot;hello {name}&quot;)"><pre><span># our capture implementation, taking params and returning T</span>
<span>def</span> <span>capture</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>T</span>:
    <span>now</span> <span>=</span> <span>time</span>.<span>time</span>()
    <span>return</span> <span>T</span>(<span>name</span><span>=</span><span>name</span>, <span>time</span><span>=</span><span>now</span>)

<span># our compare implementation, taking current and candidate T</span>
<span>def</span> <span>compare</span>(<span>current</span>: <span>T</span>, <span>candidate</span>: <span>T</span>) <span>-&gt;</span> <span>bool</span>:
    <span># cache is valid if happened within the last 1 second</span>
    <span>diff</span> <span>=</span> <span>current</span>.<span>time</span> <span>-</span> <span>candidate</span>.<span>time</span>
    <span>passed</span> <span>=</span> <span>diff</span> <span>&lt;=</span> <span>1</span>
    <span>return</span> <span>passed</span>

<span># decorate our tool with a precheck</span>
<span>@<span>engine</span>.<span>tool</span>(<span>pre_check</span><span>=</span><span>Check</span>(<span>capture</span>, <span>compare</span>))</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>time</span>.<span>sleep</span>(<span>0.1</span>)
    <span>print</span>(<span>f"hello <span><span>{</span><span>name</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Putting it all together</h3><a id="user-content-putting-it-all-together" aria-label="Permalink: Putting it all together" href="#putting-it-all-together"></a></p>
<p dir="auto">Below is the combined script for all of the above code snippets.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from dataclasses import dataclass
from muscle_mem import Check, Engine
import time

engine = Engine()

# our &quot;environment&quot; features, stored in DB
@dataclass
class T:
    name: str
    time: float

# our capture implementation, taking params and returning T
def capture(name: str) -> T:
    now = time.time()
    return T(name=name, time=now)

# our compare implementation, taking current and candidate T
def compare(current: T, candidate: T) -> bool:
    # cache is valid if happened within the last 1 second
    diff = current.time - candidate.time
    passed = diff <= 1
    return passed

# decorate our tool with a precheck
@engine.tool(pre_check=Check(capture, compare))
def hello(name: str):
    time.sleep(0.1)
    print(f&quot;hello {name}&quot;)
    
# pretend this is your agent
def agent(name: str):
   for i in range(9):
        hello(name + &quot; + &quot; + str(i))

engine.set_agent(agent)

# Run once
cache_hit = engine(&quot;erik&quot;)
assert not cache_hit

# Run again 
cache_hit = engine(&quot;erik&quot;)
assert cache_hit

# Break cache with a sleep, then run again
time.sleep(3)
cache_hit = engine(&quot;erik&quot;)
assert not cache_hit"><pre><span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span>
<span>from</span> <span>muscle_mem</span> <span>import</span> <span>Check</span>, <span>Engine</span>
<span>import</span> <span>time</span>

<span>engine</span> <span>=</span> <span>Engine</span>()

<span># our "environment" features, stored in DB</span>
<span>@<span>dataclass</span></span>
<span>class</span> <span>T</span>:
    <span>name</span>: <span>str</span>
    <span>time</span>: <span>float</span>

<span># our capture implementation, taking params and returning T</span>
<span>def</span> <span>capture</span>(<span>name</span>: <span>str</span>) <span>-&gt;</span> <span>T</span>:
    <span>now</span> <span>=</span> <span>time</span>.<span>time</span>()
    <span>return</span> <span>T</span>(<span>name</span><span>=</span><span>name</span>, <span>time</span><span>=</span><span>now</span>)

<span># our compare implementation, taking current and candidate T</span>
<span>def</span> <span>compare</span>(<span>current</span>: <span>T</span>, <span>candidate</span>: <span>T</span>) <span>-&gt;</span> <span>bool</span>:
    <span># cache is valid if happened within the last 1 second</span>
    <span>diff</span> <span>=</span> <span>current</span>.<span>time</span> <span>-</span> <span>candidate</span>.<span>time</span>
    <span>passed</span> <span>=</span> <span>diff</span> <span>&lt;=</span> <span>1</span>
    <span>return</span> <span>passed</span>

<span># decorate our tool with a precheck</span>
<span>@<span>engine</span>.<span>tool</span>(<span>pre_check</span><span>=</span><span>Check</span>(<span>capture</span>, <span>compare</span>))</span>
<span>def</span> <span>hello</span>(<span>name</span>: <span>str</span>):
    <span>time</span>.<span>sleep</span>(<span>0.1</span>)
    <span>print</span>(<span>f"hello <span><span>{</span><span>name</span><span>}</span></span>"</span>)
    
<span># pretend this is your agent</span>
<span>def</span> <span>agent</span>(<span>name</span>: <span>str</span>):
   <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>9</span>):
        <span>hello</span>(<span>name</span> <span>+</span> <span>" + "</span> <span>+</span> <span>str</span>(<span>i</span>))

<span>engine</span>.<span>set_agent</span>(<span>agent</span>)

<span># Run once</span>
<span>cache_hit</span> <span>=</span> <span>engine</span>(<span>"erik"</span>)
<span>assert</span> <span>not</span> <span>cache_hit</span>

<span># Run again </span>
<span>cache_hit</span> <span>=</span> <span>engine</span>(<span>"erik"</span>)
<span>assert</span> <span>cache_hit</span>

<span># Break cache with a sleep, then run again</span>
<span>time</span>.<span>sleep</span>(<span>3</span>)
<span>cache_hit</span> <span>=</span> <span>engine</span>(<span>"erik"</span>)
<span>assert</span> <span>not</span> <span>cache_hit</span></pre></div>
<p dir="auto">For a more real example, see a computer-use agent implementation:</p>
<p dir="auto"><a href="https://github.com/pig-dot-dev/muscle-mem/blob/main/tests/cua.py">https://github.com/pig-dot-dev/muscle-mem/blob/main/tests/cua.py</a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Call To Action</h2><a id="user-content-call-to-action" aria-label="Permalink: Call To Action" href="#call-to-action"></a></p>
<p dir="auto">I invite all feedback as this system develops!</p>
<p dir="auto">Please consider:</p>
<ol dir="auto">
<li>Joining the <a href="https://discord.gg/s84dXDff3K" rel="nofollow">Muscle Mem discord</a></li>
<li>Testing <a href="https://github.com/pig-dot-dev/muscle-mem">the muscle-mem repo</a>, and giving it a star</li>
</ol>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perverse incentives of vibe coding (196 pts)]]></title>
            <link>https://fredbenenson.medium.com/the-perverse-incentives-of-vibe-coding-23efbaf75aee</link>
            <guid>43988315</guid>
            <pubDate>Wed, 14 May 2025 19:29:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fredbenenson.medium.com/the-perverse-incentives-of-vibe-coding-23efbaf75aee">https://fredbenenson.medium.com/the-perverse-incentives-of-vibe-coding-23efbaf75aee</a>, See on <a href="https://news.ycombinator.com/item?id=43988315">Hacker News</a></p>
Couldn't get https://fredbenenson.medium.com/the-perverse-incentives-of-vibe-coding-23efbaf75aee: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Grok answers unrelated queries with long paragraphs about "white genocide" (222 pts)]]></title>
            <link>https://twitter.com/grok/status/1922651218595439063</link>
            <guid>43987266</guid>
            <pubDate>Wed, 14 May 2025 17:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/grok/status/1922651218595439063">https://twitter.com/grok/status/1922651218595439063</a>, See on <a href="https://news.ycombinator.com/item?id=43987266">Hacker News</a></p>
Couldn't get https://twitter.com/grok/status/1922651218595439063: Error: Request failed with status code 400]]></description>
        </item>
    </channel>
</rss>