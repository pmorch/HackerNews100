<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 12 May 2025 14:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A crypto founder faked his death. We found him alive at his dad's house (112 pts)]]></title>
            <link>https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/</link>
            <guid>43962503</guid>
            <pubDate>Mon, 12 May 2025 13:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/">https://sfstandard.com/2025/05/08/jeffy-yu-zerebro-fake-death/</a>, See on <a href="https://news.ycombinator.com/item?id=43962503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div id=""><p>Jeffy Yu was days away from his 23rd birthday when he appeared to take his own life Sunday in a video. A flattering obituary appeared, calling the aspiring crypto mogul “a visionary artist, technologist, and cultural force” and “a tech prodigy from an early age.” A memecoin in Yu’s honor went up for sale.</p><p>But in the days that followed, online sleuths raised doubts about the video’s authenticity. The obituary disappeared. Whatever deceptive game was being played came to an indisputable end when The Standard found Yu on Wednesday, holed up at his parents’ Crocker-Amazon home.</p></div><p>Standing outside the two-story house, the supposedly gifted tech developer was agitated and shocked that he had been found after some routine internet searches.</p><p>“I’ve been doxxed. I’ve been harassed. If you can find me, other people can find me,” he said. “Now I have to move my parents out of here this week.”</p><p>Yu is the developer of Zerebro, an obscure cryptocurrency token with a <a href="https://finance.yahoo.com/quote/ZEREBRO-USD/">market capitalization of $44 million</a>. (Bitcoin, by comparison, has a market cap of $2 trillion.) Zerebro was little known until videos of Yu appearing to shoot himself began circulating early Sunday. </p><p>Some online spectators speculated the video came from a livestream on the pump.fun cryptocurrency platform, but a spokesperson for the company denied the event had taken place on its site, calling it “nothing more than a social narrative circulating on X” and “an edited&nbsp;fabrication” in a statement Friday.</p></div><div><p>That full video is no longer available, but snippets are circulating. His obituary, which called him a “Martyr of Imagination and Creativity,” is no longer on Legacy.com.</p><p>“His life, though brief, was lived with intensity, brilliance, and a devotion to creation that he hoped would inspire others for eternity,” the article said.</p></div><div><p>It’s unclear if the other details about Yu’s life can be trusted. The online memorial said he studied computer science at Stanford and attended Northeastern and Arizona State. He reportedly worked full-time as a software engineer in Santa Cruz. His <a href="https://www.linkedin.com/in/jeffyyu/">LinkedIn profile</a> says the same.</p><p>Dressed in a T-shirt, shorts, flip-flops, and wire-rimmed glasses, Yu declined to talk about the false report of his death or how he may have benefited financially from it.</p><p>“You can see the PTSD in my eyes, right?” he said before telling this reporter to leave.</p><p>Although the obituary touted his alleged successes, the more significant self-tribute was Sunday’s release of a memecoin that one of his social media accounts <a href="https://mirror.xyz/jyu.eth/8vyuJuDkAOBmFvPmVy3QWdThtqDiQdg76llPH9IGmGM">promoted</a> in what claimed to be an automated message. “If you’re reading this, it’s because my 72 hour deadman’s switch triggered so i’m not here, at least physically,” the message said. The message described the new coin, dubbed $LLJEFFY, as “my final art piece” and “an eternal grave in cyberspace.”</p></div><figure id=""><div><p><span><span></span><img alt="The image is a halftone portrait of a person with blue and orange tones, featuring their head and shoulders and a neutral expression." loading="lazy" decoding="async" data-nimg="responsive" sizes="(min-width: 1001px) 600px, (min-width: 768px) 700px, 100vw" srcset="https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=640&amp;q=75 640w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=750&amp;q=75 750w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=768&amp;q=75 768w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=828&amp;q=75 828w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1024&amp;q=75 1024w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1080&amp;q=75 1080w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1200&amp;q=75 1200w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=1920&amp;q=75 1920w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=2048&amp;q=75 2048w, https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=3840&amp;q=75 3840w" src="https://content.sfstandard.com/wp-content/uploads/2025/05/jyu_04.jpg?w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><figcaption><span><span>Source: </span>Photo Illustration by The Standard</span></figcaption></figure><div><p>Suspicions emerged quickly when cryptocurrency figures, including Wonderland CEO Daniele Sestagalli, claimed Yu had faked his death. Sestagalli published what he described as a private letter from Yu stating that the “suicide” was “my only viable exit from persistent harassment, blackmail and threats” and that he had to “definitively and permanently disengage.” Attempts to reach Sestagalli were unsuccessful.&nbsp;</p><p>On-chain analysis shared on social media by Bubblemaps, a crypto analytics platform, <a href="https://x.com/bubblemaps/status/1920054468567265306">showed</a> accounts linked to Yu moving up to $1.4 million in cryptocurrency after his supposed death. Several accounts accused Yu of orchestrating an elaborate “pseudocide exit strategy” to cash out his holdings.</p><p>Before his staged death, Yu published a <a href="https://mirror.xyz/jyu.eth/Ekd5RjVGhygQUHUSECslOqzxsoNF9eAKjqGwQ2PCoJU">manifesto</a> introducing the concept of “legacoins” — described as an “evolution of digital assets commonly referred to as memecoins” that function as “a vault or storage, securing and preserving value indefinitely.”</p><p>New, confusing messages about Yu continue to arise. Early Thursday, the X account @eiuge74698713 announced that it would hold “a unique blockchain funeral event” for Yu.</p><p>“A true believer and builder of blockchain, someone who upheld his life’s belief even in death,” <a href="https://x.com/eiuge74698713/status/1920445385522483298">the account said</a>. “He deserves to be honored in a uniquely blockchain way.”</p><p><em>If you or someone you know may be struggling with suicidal thoughts, call or text “988,” day or night, to reach the Suicide and Crisis Lifeline, or </em><a href="https://suicidepreventionlifeline.org/chat/"><em>chat online</em></a><em>.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A community-led fork of Organic Maps (124 pts)]]></title>
            <link>https://www.comaps.app/news/2025-05-12/3/</link>
            <guid>43961908</guid>
            <pubDate>Mon, 12 May 2025 11:40:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.comaps.app/news/2025-05-12/3/">https://www.comaps.app/news/2025-05-12/3/</a>, See on <a href="https://news.ycombinator.com/item?id=43961908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><em>This is a copy of the update to the <strong>Open Letter to Organic Maps Shareholders</strong> from <a rel="noopener" target="_blank" href="https://openletter.earth/open-letter-to-organic-maps-shareholders-a0bf770c">openletter.earth</a>:</em></p>
<hr>
<p>Work on a community fork is progressing rapidly!</p>
<p>The project's core principles are <strong>Transparency, Community Decision-making, Not-for-profit &amp; for Public Interest, Fully Open Source and Privacy-focused</strong>. Read more about the project's essence and direction at our new "home" at <a rel="noopener" target="_blank" href="https://codeberg.org/comaps">https://codeberg.org/comaps</a>.</p>
<p>The project has a strong start! And now we are focusing on building the foundation, setting up the technology, and many areas are in-progress. Work is continuing on the first release!</p>
<p>The name for the project - <em>CoMaps (community, collaborative, common, collective, etc.)</em> - is provisional. We think it's a good one, but would like more people in the community to take part in choosing the final name together!</p>
<p><strong>The project name voting</strong> happens <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/Governance/issues/34">here</a> and <strong>will conclude on May 20th.</strong>
Sign into Codeberg to vote or suggest a name. People are already signing up in Codeberg to get involved and contribute on the project (including discussing features and reporting bugs).</p>
<p>CoMaps is a community-driven project, here is <strong>how to get involved:</strong></p>
<ul>
<li>Join the development effort <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/comaps">on Codeberg</a> (there are many things to do, even just fixing and updating the docs is an important task)</li>
<li>Join organizational and decisions-making activities <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/Governance">in our Governance repo</a></li>
<li>Spread the word!</li>
<li>Get in touch if you'd like to help with texts and graphics for social media promotion</li>
<li>Help build our website in <a rel="noopener" target="_blank" href="https://codeberg.org/comaps/website">our Website repo</a> (check the preview <a rel="noopener" target="_blank" href="https://www.comaps.app/">comaps.app</a>)</li>
<li>Donate at <a rel="noopener" target="_blank" href="https://opencollective.com/comaps/donate">OpenCollective</a> (all donations and spending are handled transparently by Open Collective)</li>
</ul>
<p>In the next update we'll announce the final project name!</p>
<hr>
<p>There was no real progress in negotiations with Organic Maps shareholders.</p>
<p>It appears that Viktor is only open to a guarantee not to sell the project, however besides that he wants to retain full control of Organic Maps.</p>
<p>And Organic Maps future is uncertain still, as the disagreement between shareholders (Viktor and Roman) has not been resolved.</p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Copyright Office found AI companies breach copyright. Its boss was fired (204 pts)]]></title>
            <link>https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/</link>
            <guid>43961247</guid>
            <pubDate>Mon, 12 May 2025 09:49:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/">https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/</a>, See on <a href="https://news.ycombinator.com/item?id=43961247">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The head of the US Copyright Office has reportedly been fired, the day after agency concluded that builders of AI models use of copyrighted material went beyond existing doctrines of fair use.</p>
<p>The office’s opinion on fair use came in a draft of the third part of its report on copyright and artificial intelligence. The first part considered digital replicas and the second tackled whether it is possible to copyright the output of generative AI.</p>
<p>The office published the <a target="_blank" rel="nofollow" href="https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf">draft</a> [PDF] of Part 3, which addresses the use of copyrighted works in the development of generative AI systems, on May 9th.</p>
<div><p><img src="https://regmedia.co.uk/2024/02/16/ai_shutterstock.jpg?x=174&amp;amp;y=115&amp;amp;crop=1" width="174" height="115" alt="AI"></p><h2 title="Recently formed AI Preferences Working Group has August deadline to develop ideas on how to tell crawlers to go away, or come for a feast">Copyright-ignoring AI scraper bots laugh at robots.txt so the IETF is trying to improve it</h2>
<p><a href="https://www.theregister.com/2025/04/09/ietf_ai_preferences_working_group/"><span>READ MORE</span></a></p></div>
<p>The draft notes that generative AI systems “draw on massive troves of data, including copyrighted works” and asks: “Do any of the acts involved require the copyright owners’ consent or compensation?”</p>
<p>That question is the subject of <a target="_blank" href="https://www.theregister.com/2025/01/10/meta_libgen_allegation/">several lawsuits</a>, because developers of AI models have admitted to training their products on content scraped from the internet and other sources without compensating content creators or copyright owners. AI companies have argued fair use provisions of copyright law mean they did no wrong.</p>

    

<p>As the report notes, one test courts use to determine fair use considers “the effect of the use upon the potential market for or value of the copyrighted work”. If a judge finds an AI company’s use of copyrighted material doesn’t impact a market or value, fair use will apply.</p>

        


        

<p>The report finds AI companies can’t sustain a fair use defense in the following circumstances:</p>

<p>The office will soon publish a final version of Part 3 that it expects will emerge “without any substantive changes expected in the analysis or conclusions.”</p>
<p>Tech law professor Blake. E Reid <a target="_blank" rel="nofollow" href="https://bsky.app/profile/chup.blakereid.org/post/3lot4e7onuk2m">described</a> the report as “very bad news for the AI companies in litigation” and “A straight-ticket loss for the AI companies”.</p>
<p>Among the AI companies currently in litigation on copyright matters are Google, Meta, OpenAI, and Microsoft. All four made donations to Donald Trump’s inauguration fund.</p>

        

<p>Reid’s post also pondered the timing of the Part 3 report – despite the office saying it was released “in response to congressional inquiries and expressions of interest from stakeholders” – and wrote “I continue to wonder (speculatively!) if a purge at the Copyright Office is incoming and they felt the need to rush this out.”</p>
<p>Reid looks prescient as the Trump administration <a target="_blank" rel="nofollow" href="https://www.nbcbayarea.com/news/national-international/trump-administration-fires-top-copyright-official/3866143/">reportedly</a> fired the head of the Copyright Office, Shira Perlmutter, on Saturday.</p>
<ul>

<li><a href="https://www.theregister.com/2025/04/22/bad_trip_coming_for_ai/">Bad trip coming for AI hype as humanity tools up to fight back</a></li>

<li><a href="https://www.theregister.com/2025/05/09/tech_titans_wanna_secure_us/">Tech titans: Wanna secure US AI leadership? Stop giving the world excuses to buy Chinese</a></li>

<li><a href="https://www.theregister.com/2025/04/29/take_it_down_act_passes/">TAKE IT DOWN Act? Yes, take the act down before it's too late for online speech</a></li>

<li><a href="https://www.theregister.com/2025/04/03/blair_institute_ai_copyright/">On the issue of AI copyright, Blair Institute favors tech bros over Cool Britannia</a></li>
</ul>
<p>Representative Joe Morelle (D-NY), <a target="_blank" rel="nofollow" href="https://democrats-cha.house.gov/media/press-releases/morelles-statement-abrupt-firing-shira-perlmutter-register-copyrights">wrote</a> the termination was “…surely no coincidence he acted less than a day after she refused to rubber-stamp Elon Musk’s efforts to mine troves of copyrighted works to train AI models.”</p>
<p>Morelle linked the words “she refused to rubber-stamp” to the Part 3 report discussed above.</p>
<p>The remarks about Musk may refer to the billionaire’s recent endorsement of Twitter founder Jack Dorsey’s <a target="_blank" href="https://www.theregister.com/2025/04/22/bad_trip_coming_for_ai/">desire</a> to “Delete all IP law", or the Tesla and SpaceX boss’s plans to <a target="_blank" href="https://www.theregister.com/2025/04/14/ireland_investigation_into_x/">train his own “Grok” AI</a> on X users’ posts.</p>

        

<p>There’s another possible explanation for Perlmutter’s ousting: The Copyright Office is a department of the Library of Congress, whose leader was last week fired on grounds of “quite concerning things that she had done … in the pursuit of DEI [diversity, equity, and inclusion] and putting inappropriate books in the library for children," according to White House press secretary Karoline Leavitt.</p>
<p>So maybe this is just the Trump administration enacting its policy on diversity without regard to the report’s possible impact on donors or Elon Musk. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Cursor or Windsurf? (209 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43959710</link>
            <guid>43959710</guid>
            <pubDate>Mon, 12 May 2025 04:41:50 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43959710">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="43960527"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960527" href="https://news.ycombinator.com/vote?id=43960527&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Neither? I'm surprised nobody has said it yet. I turned off AI autocomplete, and sometimes use the chat to debug or generate simple code but only when I prompt it to. Continuous autocomplete is just annoying and slows me down.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961128"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961128" href="https://news.ycombinator.com/vote?id=43961128&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor/Windsurf and similar IDEs and plugins are more than autocomplete on steroids.</p><p>Sure, you might not like it and think you as a human should write all code, but frequent experience in the industry in the past months is that productivity in the teams using tools like this has greatly increased.</p><p>It is not unreasonable to think that someone deciding not to use tools like this will not be competitive in the market in the near future.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961133"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961133" href="https://news.ycombinator.com/vote?id=43961133&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>We have an internal ban policy on copilot for IP reasons and while I was... missing it initially, now just using neovim without any AI feels fine. Maybe I'll add an avante.nvim for a built-in chat box though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960839"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960839" href="https://news.ycombinator.com/vote?id=43960839&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>This is the way.</p><p>All this IDE churn makes me glad to have settled on Emacs a decade ago. I have adopted LLMs into my workflow via the excellent gptel, which stays out of my way but is there when I need it. I couldn't imagine switching to another editor because of some fancy LLM integration I have no control over. I have tried Cursor and VS Codium with extensions, and wasn't impressed. I'd rather use an "inferior" editor that's going to continue to work exactly how I want 50 years from now.</p><p>Emacs and Vim are editors for a lifetime. Very few software projects have that longevity and reliability. If a tool is instrumental to the work that you do, those features should be your highest priority. Not whether it works well with the latest tech trends.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961064"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43961064" href="https://news.ycombinator.com/vote?id=43961064&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Ironically LLMs have made Emacs even more relevant. The model LLMs use (text) happens to match up with how Emacs represents everything (text in buffers). This opens up Emacs to becoming the agentic editor par excellence. Just imagine, some macro magic acound a defcommand and voila, the agent can do exactly what a user can.  If only such a project could have the funding like Cursor does...</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961109"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43961109" href="https://news.ycombinator.com/vote?id=43961109&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Nothing could be worse for the modern Emacs ecosystem than for the tech industry finance vampires ("VCs," "LPs") to decide there's blood enough there to suck.</p><p>Fortunately, alien space magic seems immune, so far at least. I assume they do not like the taste, and no wonder.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43961090"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961090" href="https://news.ycombinator.com/vote?id=43961090&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I can't even get simple code generation to work for VHDL. It just gives me garbage that does not compile. I have to assume this is not the case for the majority of people using more popular languages? Is this because the training data for VHDL is far more limited? Are these "AIs" not able to consume the VHDL language spec and give me actual legal syntax at least?! Or is this because I'm being cheap and lazy by only trying free chatGPT and I should be using something else?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960845"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960845" href="https://news.ycombinator.com/vote?id=43960845&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>That is interesting. Which tech are you using?</p><p>Are you getting irrelevant suggestions as those autocompletes are meant to predict the things you are about to type.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960875"><td></td></tr>
                  <tr id="43960844"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960844" href="https://news.ycombinator.com/vote?id=43960844&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I shortcut the "cursor tab" and enable or disable it as needed. If only Ai was smart enough to learn when I do and don't want it (like clippy in the ms days) - when you are manually toggling it on/off clear patterns emerge (to me at least) as to when I do and don't want it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960870"><td></td></tr>
                <tr id="43960885"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960885" href="https://news.ycombinator.com/vote?id=43960885&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Bottom right says "cursor tab" you can manually manipulate it there (and snooze for X minutes - interesting feature). For binding shortcuts - Command/Ctrl + Shift + P, then look for "Enable|Disable|Whatever Cursor Tab" and set shortcuts there.</p><p>Old fashioned variable name / function name auto complete is not affected.</p><p>I considered a small macropad to enable / disable with a status light - but honestly don't do enough work to justify avoiding work by finding / building / configuring / rebuilding such a solution. If the future is this sort of extreme autocomplete in everything I do on a computer, I would probably go to the effort.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961045"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43961045" href="https://news.ycombinator.com/vote?id=43961045&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Thanks!</p><p>The thing that bugs me is when Im trying to use tab to indent with spaces, but I get a suggestion instead.</p><p>I tried to disable caps lock, then remap tab to caps lock, but no joy</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960859"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960859" href="https://news.ycombinator.com/vote?id=43960859&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>+100. I’ve found the “chat” interface most productive as I can scope a problem appropriately.</p><p>Cursor, Windsurf, etc tend to feel like code vomit that takes more time to sift through than working through code by myself.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960985"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960985" href="https://news.ycombinator.com/vote?id=43960985&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Having it as tab was a mistake, tab complete for snippets is fine because it’s at the end of a line, tab complete in empty text space means you always have to be aware if it’s in autocomplete context or not before setting an indent.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960860"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960860" href="https://news.ycombinator.com/vote?id=43960860&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yeah, I use IntelliJ with the chat sidebar. I don't use autocomplete, except in trivial cases where I need to write boilerplate code. Other than that, when I need help, I ask the LLM and then write the code based on its response.</p><p>I'm sure it's initially slower than vibe-coding the whole thing, but at least I end up with a maintainable code base, and I know how it works and how to extend it in the future.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961007"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43961007" href="https://news.ycombinator.com/vote?id=43961007&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>AI autocomplete can be infuriating if like me, you like to browse the public methods and properties by dotting the type. The AI autocomplete sometimes kicks in and starts writing broken code using suggestions that don't exist and that prevents quickly exploring the actual methods available.</p><p>I have largely disabled it now, which is a shame, because there are also times it feels like magic and I can see how it could be a massive productivity lever if it needed a tighter confidence threshold to kick in.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43961048"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43961048" href="https://news.ycombinator.com/vote?id=43961048&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>If I can, I map it to ctrl-; so I can bring it up when I need it.</p><p>But I found once it was optional I hardly ever used it.</p><p>I use Deepseek or others as a conversation partner or rubber duck, but I'm perfectly happy writing all my code myself.</p><p>Maybe this approach needs a trendy name to counter the "vibe coding" hype.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960616"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960616" href="https://news.ycombinator.com/vote?id=43960616&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Agreed. You may like the arms-length stuff here: <a href="https://github.com/day50-dev/llmehelp">https://github.com/day50-dev/llmehelp</a> . shell-hook.zsh and screen-query have been life-changing</p><p>I always forget syntax for things like ssh port forwarding.  Now just describe it at the shell:</p><p>$ ssh (take my local port 80 and forward it to 8080 on the machine betsy) user@betsy</p><p>I press ctrl+x x and it will replace the english with a suggested command. It's been a total game changer for git, jq, rsync, ffmpeg, regex..</p><p>For more involved stuff there's screen-query: Confusing crashes, strange terminal errors, weird config scripts, it allows a joint investigation whereas aider and friends just feels like I'm asking AI to fuck around.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960751"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960751" href="https://news.ycombinator.com/vote?id=43960751&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>This never accesses any extradata and works only when explicitly asked? I find terminal as most important part from privacy perspective and I haven’t tried any LLM integration yet…</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960818"><td></td></tr>
                        <tr id="43960550"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960550" href="https://news.ycombinator.com/vote?id=43960550&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yeah</p><p>AI autocomplete is a feature, not a product (to paraphrase SJ)</p><p>I can understand Windsurf getting the valuation as they had their own Codeium model</p><p>$B for a VSCode fork? Lol</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960774"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960774" href="https://news.ycombinator.com/vote?id=43960774&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Microsoft seems to be always winner - maybe they predicted all this and for this reason they made core extensions closed source.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43960674"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960674" href="https://news.ycombinator.com/vote?id=43960674&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Asking HN this is like asking which smartphone to use. You'll get suggestions for obscure Linux-based modular phones that weigh 6 kilos and lack a clock app or wifi. But they're better because they're open source or fully configurable or whatever. Or a smartphone that a fellow HNer created in his basement and plans to sell soon.</p><p>Cursor and Windsurf are both good, but do what most people do and use Cursor for a month to start with.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959899"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959899" href="https://news.ycombinator.com/vote?id=43959899&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Zed. They've upped their game in the AI integration and so far it's the best one I've seen (external from work). Cursor and VSCode+Copilot always felt slow and janky, Zed is much less janky feels like pretty mature software, and I can just plug in my Gemini API key and use that for free/cheap instead of paying for the editor's own integration.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960546"><td></td></tr>
            <tr id="43960506"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960506" href="https://news.ycombinator.com/vote?id=43960506&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Consumes lots of resources on an M4 Macbook. Would love to test it though. If it didn’t freeze my Macbook.</p><p>Edit:</p><p>With the latest update to 0.185.15 it works perfectly smooth. Excellent addition to my setup.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960728" href="https://news.ycombinator.com/vote?id=43960728&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I'll second the zed recommendation, sent from my M4 macbook. I don't know why exactly it's doing this for you but mine is idling with ~500MB RAM (about as little as you can get with a reasonably-sized Rust codebase and a language server) and 0% CPU.</p><p>I have also really appreciated something that felt much less janky, had better vim bindings, and wasn't slow to start even on a very fast computer. You can completely botch Cursor if you type really fast. On an older mid-range laptop, I ran into problems with a bunch of its auto-pair stuff of all things.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960803"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960803" href="https://news.ycombinator.com/vote?id=43960803&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Yeah, same. Zed is <i>incredibly</i> efficient on my M1 Pro. It's my daily driver these days, and my Python setup in it is almost perfect.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960874"><td></td></tr>
                  <tr id="43960069"><td></td></tr>
                <tr id="43960155"><td></td></tr>
                <tr id="43960299"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960299" href="https://news.ycombinator.com/vote?id=43960299&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It would be great if there was an easy way to run their open model (<a href="https://huggingface.co/zed-industries/zeta" rel="nofollow">https://huggingface.co/zed-industries/zeta</a>) locally ( for latency reasons ).</p><p>I don't think Zeta is quite up to windsurf's completion quality/speed.</p><p>I get that this would go against their business model, but maybe people would pay for this - it could in theory be the fastest completion since it would run locally.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960136"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960136" href="https://news.ycombinator.com/vote?id=43960136&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For the agentic stuff I think every solution can be hit or miss. I've tried claude code, aider, cline, cursor, zed, roo, windsurf, etc. To me it is more about using the right models for the job, which is also constantly in flux because the big players are constantly updating their models and sometimes that is good and sometimes that is bad.</p><p>But I daily drive Cursor because the main LLM feature I use is tab-complete, and here Cursor blows the competition out of the water. It understands what I want to do next about 95% of the time when I'm in the middle of something, including comprehensive multi-line/multi-file changes. Github Copilot, Zed, Windsurf, and Cody aren't at the same level imo.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960573"><td></td></tr>
                <tr id="43960635"><td></td></tr>
                        <tr id="43960296"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960296" href="https://news.ycombinator.com/vote?id=43960296&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For a time windsurf was way ahead of cursor in full agentic coding, but now I hear cursor has caught up. I have yet to switch back to try out cursor again but starting to get frustrated with Windsurf being restricted to gathering context only 100-200 lines at a time.</p><p>So many of the bugs and poor results that it can introduce are simply due to improper context. When forcibly giving it the necessary context you can clearly see it’s not a model problem but it’s a problem with the approach of gathering disparate 100 line snippets at a time.</p><p>Also, it struggles with files over 800ish lines which is extremely annoying</p><p>We need some smart deepseek-like innovation in context gathering since the hardware and cost of tokens is the real bottleneck here.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959984"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959984" href="https://news.ycombinator.com/vote?id=43959984&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Aider! Use the editor of your choice and leave your coding assistant separate. Plus, it's open source and will stay like this, so no risk to see it suddenly become expensive or dissappear.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960453"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960453" href="https://news.ycombinator.com/vote?id=43960453&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Approximately how much does it cost in practice to use Aider? My understanding is that Aider itself is free, but you have to pay per token when using an API key for your LLM of choice. I can look up for myself the prices of the various LLMs, but it doesn't help much, since I have no intuition whatsoever about how many tokens I am likely to consume. The attraction of something like Zed or Cursor for me is that I just have a fixed monthly cost to worry about. I'd love to try Aider, as I suspect it suits my style of work better, but without having any idea how much it would cost me, I'm afraid of trying.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960743"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960743" href="https://news.ycombinator.com/vote?id=43960743&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'm using Gemini 2.5 Pro with Aider and Cline for work. I'd say when working for 8 full hours without any meetings or other interruptions, I'd hit around $2. In practice, I average at $0.50 and hit $1 once in the last weeks.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960920"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960920" href="https://news.ycombinator.com/vote?id=43960920&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>This is very inexpensive. What is your workflow and savings techniques! I can spend $10/h or more with very short sessions and few files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960548"><td></td></tr>
                <tr id="43960703"><td></td></tr>
                <tr id="43960980"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43960980" href="https://news.ycombinator.com/vote?id=43960980&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>To be honest I'm using windsurf with openAI/google right now and used deepseek with aider when it was still less crowded.</p><p>My only problem was deepseek occasionally not answering at all, but generally it was fast (non thinking that was).</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960110"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960110" href="https://news.ycombinator.com/vote?id=43960110&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I used to be religiously pro-Aider. But after a while those little frictions flicking backwards and forwards between the terminal and VS Code, and adding and dropping from the context myself, have worn down my appetite to use it. The `--watch` mode is a neat solution but harms performance. The LLM gets distracted by deleting its own comment.</p><p>Roo is less solid but better-integrated.</p><p>Hopefully I'll switch back soon.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960199"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960199" href="https://news.ycombinator.com/vote?id=43960199&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I suspect that if you're a vim user those friction points are a bit different. For me, Aider's git auto commit and /undo command are what sells it for me at this current junction of technology. OpenHands looks promising, though rather complex.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960384"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960384" href="https://news.ycombinator.com/vote?id=43960384&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>The (relative) simplicity is what sells aider for me (it also helps that I use neovim in tmux).</p><p>It was easy to figure out exactly what it's sending to the LLM, and I like that it does one thing at a time. I want to babysit my LLMs and those "agentic" tools that go off and do dozens of things in a loop make me feel out of control.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960675"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_43960675" href="https://news.ycombinator.com/vote?id=43960675&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I like your framing about “feeling out of control”.</p><p>For the occasional frontend task, I don’t mind being out of control when using agentic tools. I guess this is the origin of Karpathy’s vibe coding moniker: you surrender to the LLM’s coding decisions.</p><p>For backend tasks, which is my bread and butter, I certainly want to know what it’s sending to the LLM so it’s just easier to use the chat interface directly.</p><p>This way I am fully in control. I can cherry pick the good bits out of whatever the LLM suggests or redo my prompt to get better suggestions.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960122"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960122" href="https://news.ycombinator.com/vote?id=43960122&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yup, choose your model and pay as you go, like commodities like rice and water. The others played games with me to minimize context and use cheaper models (such as 3 modes, daily credits etc, using most expensive model etc).</p><p>Also the --watch mode is the most productive interface of using your editor, no need of extra textboxes with robot faces.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960191"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960191" href="https://news.ycombinator.com/vote?id=43960191&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>fwiw. Gemini-*, which is available in Aider, isn't Pay As You Go (payg) but post paid, which means you get a bill at the end of the month and not the OpenAI/others model of charging up credits before you can use the service.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960659"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960659" href="https://news.ycombinator.com/vote?id=43960659&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I guess this is a good reason to consider things like openrouter. Turns it into a prepaid service.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43960640"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960640" href="https://news.ycombinator.com/vote?id=43960640&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'm with Cursor for the simple reason it is in practice unlimited. Honestly the slow requests after 500 per month are fast enough. Will I stay with Cursor? No, ill switch the second something better comes along.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960866"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960866" href="https://news.ycombinator.com/vote?id=43960866&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Same. Love the "slow but free" model, I hope they can continue providing it, I love paying only $20/m instead of having a pay by usage.</p><p>I've been building SO MANY small apps and web apps in the latest months, best $20/m ever spent.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960969"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960969" href="https://news.ycombinator.com/vote?id=43960969&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I'm cursor with claude 3.7</p><p>Somehow other models don't work as well with it. ,,auto'' is the work.</p><p>Still, I hate it when it deletes all my unit tests to ,,make them pass''</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960707"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960707" href="https://news.ycombinator.com/vote?id=43960707&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor is acceptable because for the price it's unbeatable. Free, unlimited requests are great. But by itself, Cursor is not anything special. It's only interesting because they pay Claude or Gemini from their pockets.</p><p>Ideally, things like RooCode + Claude are much better, but you need infinite money glitch.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960678"><td></td></tr>
                  <tr id="43959909"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959909" href="https://news.ycombinator.com/vote?id=43959909&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I am betting on myself.</p><p>I built a minimal agentic framework (with editing capability) that works for a lot of my tasks with just seven tools: read, write, diff, browse, command, ask and think.</p><p>One thing I'm proud of is the ability to have it be more proactive in making changes and taking next action by just disabling the `ask` tool.</p><p>I won't say it is better than any of the VSCode forks, but it works for 70% of my tasks in an understandable manner. As for the remaining stuff, I can always use Cursor/Windsurf in a complementary manner.</p><p>It is open, have a look at <a href="https://github.com/aperoc/toolkami">https://github.com/aperoc/toolkami</a> if it interests you.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961130"><td></td></tr>
            <tr id="43960864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960864" href="https://news.ycombinator.com/vote?id=43960864&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>For daily work - neither. They basically promote the style of work where you end up with mediocre code that you don't fully understand, and with time the situation gets worse.</p><p>I get much better result by asking specific question to a model that has huge context (Gemini) and analyzing the generated code carefully. That's the opposite of the style of work you get with Cursor or Windsurf.</p><p>Is it less efficient? If you are paid by LoCs, sure. But for me the quality and long-term maintainability are far more important. And especially the Tab autocomplete feature was driving me nuts, being wrong roughly half of the time and basically just interrupting my flow.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960810"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960810" href="https://news.ycombinator.com/vote?id=43960810&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>VS Code with GitHub Copilot works great, though they are usually a little late to add features compared to Cursor or Windsurf. I use the 'Edit' feature the most.</p><p>Windsurf I think has more features, but I find it slower compared to others.</p><p>Cursor is pretty fast, and I like how it automatically suggests completion even when moving my cursor to a line of code. (Unlike others where you need to 'trigger' it by typing a text first)</p><p>Honorable mention: Supermaven. It was the first and fastest AI autocomplete I used. But it's no longer updated since they were acquired by Cursor.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960189"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960189" href="https://news.ycombinator.com/vote?id=43960189&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I’ve been using Zed Agent with GitHub Copilot’s models, but with GitHub planning to limit usage, I’m exploring alternatives.</p><p>Now I'm testing Claude Code’s $100 Max plan. It feels like magic - editing code and fixing compile errors until it builds. The downside is I’m reviewing the code a lot less since I just let the agent run.</p><p>So far, I’ve only tried it on vibe coding game development, where every model I’ve tested struggles. It says “I rewrote X to be more robust and fixed the bug you mentioned,” yet the bug still remains.</p><p>I suspect it will work better for backend web development I do for work: write a failing unit test, then ask the agent to implement the feature and make the test pass.</p><p>Also, give Zed’s Edit Predictions a try. When refactoring, I often just keep hitting Tab to accept suggestions throughout the file.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960254"><td></td></tr>
                <tr id="43960412"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960412" href="https://news.ycombinator.com/vote?id=43960412&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It feels like magic when it works and it at least gets the code to compile. Other models* would usually return a broken code. Specially when using a new release of a library. All the models use the old function signatures, but Claud Code then sees compile error and fixes it.</p><p>Compared to Zed Agent, Claude Code is: 
- Better at editing files. Zed would sometimes return the file content in the chatbox instead of updating it. Zed Agent also inserted a new function in the middle of the existing function. 
- Better at running tests/compiling. Zed struggled with nix environment and I don't remember it going to the update code -&gt; run code -&gt; update code feedback loop.</p><p>With this you can leave Claude Code alone for a few minutes, check back and give additional instructions. With Zed Agent it was more of a constantly monitoring / copy pasting and manually verifying everything.</p><p>*I haven't tested many of the other tools mentioned here, this is mostly my experience with Zed and copy/pasting code to AI.</p><p>I plan to test other tools when my Claude Code subscription expires next month.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43960966"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960966" href="https://news.ycombinator.com/vote?id=43960966&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor is good for basic stuff but Windsurf consistently solves issues Cursor fails on even after 40+ mins of retries and prompting changes.</p><p>Cursor is very lazy about looking beyond the current context or even context at all sometimes it feels it’s trying to one shot a guess without looking deeper.</p><p>Bad thing about Windsurf is the plans are pretty limited and the unlimited “cascade base” feels dumb the times I used it so ultimately I use Cursor until I hit a wall then switch to Windsurf.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960212"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960212" href="https://news.ycombinator.com/vote?id=43960212&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>OP probably means to keep using vscode. Honestly, best thing you can do is just try each for a few weeks. Feature comparison tables only say so much, particularly because the terminology is still in a state of flux.</p><p>I’ve personally never felt at home in vscode. If you’re open to switching, definitely check out Zed, as others are suggesting.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960897"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960897" href="https://news.ycombinator.com/vote?id=43960897&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Using Windsurf since the start and I am satisfied. Didn't look beyond it. Focused on actually doing the coding. It's impossible to keep up with daily AI news and if something groundbreaking happens it will go viral.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960180"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960180" href="https://news.ycombinator.com/vote?id=43960180&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Cursor has for me had the best UX and results until now. Trae's way of adding context is way too annoying. Windsurf has minor UI-issues all over. Options that are extensions in VSCode do not cut it in turn of providing fantastic UI/UX because of the API not supporting it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960320"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960320" href="https://news.ycombinator.com/vote?id=43960320&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor: Autocomplete is really good. At a time when I compared them, it was without a doubt better than Githib Copilot autocomplete. Cmd-K - insert/edit snippet at cursor - is good when you use good old Sonnet 3.5. ;;; Agent mode, is, honestly, quite disappointing; it doesn't feel like they put a lot of thought into prompting and wrapping LLM calls. Sometimes it just fails to submit code changes. Which is especially bad as they charge you for every request. Also I think they over-charge for Gemini, and Gemini integration is especially poor.</p><p>My reference for agent mode is Claude Code. It's far from perfect, but it uses sub-tasks and summarization using smaller haiku model. That feels way more like a coherent solution compared to Cursor. Also Aider ain't bad when you're OK with more manual process.</p><p>Windsurf: Have only used it briefly, but agent mode seems somewhat better thought out. For example, they present possible next steps as buttons. Some reviews say it's even more expensive than Cursor in agent mode.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960904" href="https://news.ycombinator.com/vote?id=43960904&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Has anyone had any joy using a local model? Or is it still too slow?</p><p>On something like a M4 Macbook Pro can local models replace the connection to OpenAi/Anthropic?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960961"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960961" href="https://news.ycombinator.com/vote?id=43960961&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>It might seem contrary to the current trend, but I've recently returned to using nvim as my daily driver after years with VS Code. This shift wasn't due to resource limitations but rather the unnecessary strain from agentic features consuming high amounts of resources.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960693"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960693" href="https://news.ycombinator.com/vote?id=43960693&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I wish your own coding would just be augmented like somebody looking over your shoulder. The problem with the current AI coding is that you don't know your code base anymore. Basically, like somebody helping you figure out stuff faster, update documentation etc.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959889"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959889" href="https://news.ycombinator.com/vote?id=43959889&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I really like Zed. Have not tried any of the mentioned by op. 
Zed I feel like is getting somewhere that can replace Sublime Text completely (but not there yet).</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960188"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960188" href="https://news.ycombinator.com/vote?id=43960188&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Zed is an editor firslty.. The Ops has mentioned options which are AI development "agents" basically.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960348"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960348" href="https://news.ycombinator.com/vote?id=43960348&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>AI aided development has first class support in Zed.</p><p>Ie. it's not a "plugin" but built-in ecosystem developed by core team.</p><p>Speed of iterations on new features is quite impressive.</p><p>Their latest agentic editing update basically brought claude code cli to the editor.</p><p>Most corporations don't have direct access to arbitrary LLMs but through Microsoft's Github's Copilot they do – and you can use models through copilot and other providers like Ollama – which is great for work.</p><p>With their expertise (team behind pioneering tech like electron, atom, teletype, tree sitter, building their own gpu based cross platform ui etc.) and velocity it seems that they're positioned to outpace competition.</p><p>Personally I'd say that their tech is maybe two orders of magnitude more valuable than windsurf?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960670"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43960670" href="https://news.ycombinator.com/vote?id=43960670&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I don't dispite Zed is great, I actually am using it myself, but it's an editor first and foremost. The OP, to me at least seems to be asking more-so about the AI agent comparisons.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960820"><td></td></tr>
                                    <tr id="43960777"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960777" href="https://news.ycombinator.com/vote?id=43960777&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Cursor. Good price, the predictive next edit is great, good enough with big code bases and with the auto mode i dont even spend all my prem requests.</p><p>I've tried VScode with copilot a couple of times and its frustrating, you have to point out individual files for edits but project wide requests are a pain.</p><p>My only pain is the workflow for developing mobile apps where I have to switch back and forth between Android Studio and Xcode as vscode extensions for mobile are not so good</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960385"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960385" href="https://news.ycombinator.com/vote?id=43960385&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Since this topic is closely related to my new project, I’d love to hear your opinion on it.</p><p>I’m thinking of building an AI IDE that helps engineers write production quality code quickly when working with AI. The core idea is to introduce a new kind of collaboration workflow.</p><p>You start with the same kind of prompt, like “I want to build this feature...”, but instead of the model making changes right away, it proposes an architecture for what it plans to do, shown from a bird’s-eye view in the 2D canvas.</p><p>You collaborate with the AI on this architecture to ensure everything is built the way you want. You’re setting up data flows, structure, and validation checks. Once you’re satisfied with the design, you hit play, and the model writes the code.</p><p>Website (in progress): <a href="https://skylinevision.ai/" rel="nofollow">https://skylinevision.ai</a></p><p>YC Video showing prototype that I just finished yesterday: <a href="https://www.youtube.com/watch?v=DXlHNJPQRtk" rel="nofollow">https://www.youtube.com/watch?v=DXlHNJPQRtk</a></p><p>Karpathy’s post that talks about this:
<a href="https://x.com/karpathy/status/1917920257257459899" rel="nofollow">https://x.com/karpathy/status/1917920257257459899</a></p><p>Thoughts? Do you think this workflow has a chance of being adopted?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960663"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960663" href="https://news.ycombinator.com/vote?id=43960663&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Looks like an antidote for "vibe coding", like it. When are you planning to release something that could be tried? Is this open source?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960729"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960729" href="https://news.ycombinator.com/vote?id=43960729&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I believe we can have a beta release in September, and yes, we plan to open-source the editor.</p><p>PS. I’m stealing the ‘antidote to “vibe coding”’ phrase :)</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960568"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960568" href="https://news.ycombinator.com/vote?id=43960568&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I quite liked the video. Hope you get to launch the product and I could try it out some day.</p><p>The only thing that I kept thinking about was - if there is a correction needed- you have to make it fully by hand. Find everything and map. However, if the first try was way off , I would like to enter from "midpoint" a correction that I want. So instead of fixing 50%, I would be left with maybe 10 or 20. Don't know if you get what I mean.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960603"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960603" href="https://news.ycombinator.com/vote?id=43960603&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Yes, the idea is to ‘speak/write’ to the local model to fix those little things so you don’t have to do them by hand. I actually already have a fine-tuned Qwen model running on Apple’s MLX to handle some of that, but given the hard YC deadline, it didn’t make it into the demo.</p><p>Eventually, you’d say, ‘add an additional layer, TopicsController, between those two files,’ and the local model would do it quickly without a problem, since it doesn’t involve complicated code generation. You’d only use powerful remote models at the end.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960619" href="https://news.ycombinator.com/vote?id=43960619&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Just watched the demo video and thought it is a very interesting approach to development, I will definitely be following this project. Good Luck.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960064"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960064" href="https://news.ycombinator.com/vote?id=43960064&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Amazon Q. Claude Code is great (the best imho, what everything else measures against right now), and Amazon Q seems almost as good and for the first week I've been using it I'm still on the free tier.</p><p>The flat pricing of Claude Code seems tempting, but it's probably still cheaper for me to go with usage pricing. I feel like loading my Anthropic account with the minimum of $5 each time would last me 2-3 days depending on usage. Some days it wouldn't last even a day.</p><p>I'll probably give Open AI's Codex a try soon, and also circle back to Aider after not using it for a few months.</p><p>I don't know if I misundersand something with Cursor or Copilot. It seems so much easier to use Claude Code than Cursor, as Claude Code has many more tools for figuring things out. Cursor also required me to add files to the context, which I thought it should 'figure out' on its own.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960817"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960817" href="https://news.ycombinator.com/vote?id=43960817&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I remember asking Amazon Q something and it wouldn’t reply cuz of security policy or something. It was as far as I can remember a legit question around Iam policy which I was trying to configure. I figured it out back in Google search.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960240"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960240" href="https://news.ycombinator.com/vote?id=43960240&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>&gt; I don't know if I misundersand something with Cursor or Copilot. It seems so much easier to use Claude Code than Cursor, as Claude Code has many more tools for figuring things out. Cursor also required me to add files to the context, which I thought it should 'figure out' on its own.</p><p>Cursor can find files on its own. But if you point it in the right direction it has far better results than Claude code.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960087"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960087" href="https://news.ycombinator.com/vote?id=43960087&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>this is the first time I am seeing someone says good things about Amazon Q</p><p>Do they publish any benchmark sheet on how it compares against others?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960292"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_43960292" href="https://news.ycombinator.com/vote?id=43960292&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>It is currently at top3 in swe bench verified.</p><p>It went through multiple stages of upgrades and I would say at this stage it is better than copilot. Fundamentally it is as good as cursor or windsurf but lacks some features and cannot match their speed of release. If you re on aws tho its a compelling offering.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="43959991"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959991" href="https://news.ycombinator.com/vote?id=43959991&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Windsurf at the moment. It now can run multiple "flows" in parallel, so I can set one cascade off to look into a bug somewhere while another cascade implements a feature elswhere in the code base. The LLMs spit out their tokens in the background, I drop in eventually to reveiew and accept or ask for further changes.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960167" href="https://news.ycombinator.com/vote?id=43960167&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Cursor offers this too - open different tabs in chat and ask for different changes; they’ll run in parallel.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960258"><td></td></tr>
            <tr id="43960103"><td></td></tr>
                  <tr id="43960666"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960666" href="https://news.ycombinator.com/vote?id=43960666&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p><a href="https://nonbios.ai/" rel="nofollow">https://nonbios.ai</a> - [Disclosure: I am working on this.]</p><p>- We are in public beta and free for now.</p><p>- Fully Agentic. Controllable and Transparent. Agent does all the work, but keeps you in the loop. You can take back control anytime and guide it.</p><p>- Not an IDE, so don't compete with VSCode forks. Interface is just a chatbox.</p><p>- More like Replit - but full stack focussed. You can build backend services.</p><p>- Videos are up at youtube.com/@nonbios</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960567"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960567" href="https://news.ycombinator.com/vote?id=43960567&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I just use Copilot (across VS Code, VS etc), it lets you pick the model you want and it's a fixed monthly cost (and there is a free tier). They have most of the core features of these other tools now.</p><p>Cursor, Windsurf et al have no "moat" (in startup speak), in that a sufficiently resourced organization (e.g. Microsoft) can just copy anything they do well.</p><p>VS code/Copilot has millions of users, cursor etc have hundreds of thousands of users. Google claims to have "hundreds of millions" of users but we can be pretty sure that they are quoting numbers for their search product.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960284"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960284" href="https://news.ycombinator.com/vote?id=43960284&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Claude Code.
And... Junie in Jetbrains IDE. It appeared recently and I'm really impressed by its quality. I think it is on the level of Claude Code.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960314"><td></td></tr>
                <tr id="43960473"><td></td></tr>
                <tr id="43961094"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_43961094" href="https://news.ycombinator.com/vote?id=43961094&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Junie in Ask mode:</p><p>&gt; Which LLM are you?</p><p>&gt; I am Claude, an AI assistant created by Anthropic. In this interface, I'm operating as "Junie," a helpful assistant designed to explore codebases and answer questions about projects. I'm built on Anthropic's large language model technology, specifically the Claude model family.</p><p>Jetbrains wider AI tools let you choose the model that gets used but as far as I can tell Junie doesn't. That said, it works great.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="43961040"><td></td></tr>
            <tr id="43960386"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960386" href="https://news.ycombinator.com/vote?id=43960386&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Recently started using Cursor for adding a new feature on a small codebase for work, after a couple of years where I didn't code. It took me a couple of tries to figure out how to work with the tool effectively, but it worked great! I'm now learning how to use it with TaskMaster, it's such a different way to do and play with software. Oh, one important note: I went with Cursor also because of the pricing, that's despite confusing in term of fast vs slow requests, it smells less consumption base.</p><p>BTW There's a new OSS competitor in town that got the front a couple of days ago - Void: Open-source Cursor alternative <a href="https://news.ycombinator.com/item?id=43927926">https://news.ycombinator.com/item?id=43927926</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960871"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960871" href="https://news.ycombinator.com/vote?id=43960871&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Considering Microsoft is closing down on the ecosystem, I'd pick VSCode with Copilot over those two.</p><p>It's a matter of time before they're shuttered or their experience gets far worse.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960880"><td></td></tr>
                  <tr id="43959987"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959987" href="https://news.ycombinator.com/vote?id=43959987&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I evaluated Windsurf at a friend's recommendation around half a year ago and found that it could not produce any useful behaviors on files above a thousand lines or so. I understand this is mostly a property of the model, but certainly also a property of the approach used by the editor of just tossing the entire file in, yeah? I haven't tried any of these products since then, but it might be worth another shot because Gemini might be able to handle these files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960303"><td></td></tr>
                  <tr id="43960047"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960047" href="https://news.ycombinator.com/vote?id=43960047&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I've had trials for both running and tested both on the same codebases.</p><p>Cursor works roughly how I've expected. It reads files and either gets it right or wrong in agent mode.</p><p>Windsurf seems restricted to reading files 50 lines at a time, and often will stop after 200 lines [0]. When dealing with existing code I've been getting poorer results than Cursor.</p><p>As to autocomplete: perhaps I haven't set up either properly (for PHP) but the autocomplete in both is good for pattern matching changes I make, and terrible for anything that require knowledge of what methods an object has, the parameters a method takes etc. They both hallucinate wildly, and so I end up doing bits of editing in Cursor/Windsurf and having the same project open in PhpStorm and making use of its intellisense.</p><p>I'm coming to the end of both trials and the AI isn't adding enough over Jetbrains PhpStorm's built in features, so I'm going back to that until I figure out how to reduce hallucinations.</p><p>0. <a href="https://www.reddit.com/r/Codeium/comments/1hsn1xw/report_from_a_windsurf_user/" rel="nofollow">https://www.reddit.com/r/Codeium/comments/1hsn1xw/report_fro...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960297" href="https://news.ycombinator.com/vote?id=43960297&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I think with the answer, each responder should include their level of coding proficiency. Or, at least whether they are able to (or even bother to) read the code that the tool generates. Preferences would vary wildly based on it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960321" href="https://news.ycombinator.com/vote?id=43960321&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I tested windsurf last week, it installed all dependencies to my global python....it didn't know best practices for Python, and didn't create any virtual env..... I am disappointed. My Cursor experience was slightly better. Still, one issue I had was how to make sure it does not change the part of code I don't want it to change. Every time you ask it to do something for A, it rewrote B in the process, very annoying.</p><p>My best experience so far is v0.dev :)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960400"><td></td></tr>
                <tr id="43960805"><td></td></tr>
                  <tr id="43960576"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960576" href="https://news.ycombinator.com/vote?id=43960576&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I use Windsurf but it's been having ridiculous downtime lately.</p><p>I can't use Cursor because I don't use Ubuntu which is what their Linux packages are compiled against and they don't run on my non-Ubuntu distro of choice.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959880"><td></td></tr>
                <tr id="43959933"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43959933" href="https://news.ycombinator.com/vote?id=43959933&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I love Cline and use it every day. It works the way I think and makes smart decisions about features.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960013"><td></td></tr>
                  <tr id="43960511"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960511" href="https://news.ycombinator.com/vote?id=43960511&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Claude code is the best so far, I am using the 200$ plan. in terms of feature matrix all tools are almost same with some hits and misses but speed is something which claude code wins.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43961068"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43961068" href="https://news.ycombinator.com/vote?id=43961068&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I use neovim now, after getting tired of the feature creep and the constant chasing of shiny new features.</p><p>AI is not useful when it does the thinking for you. It's just advanced snippets at that point. I only use LLMs to explain things or to clarify a topic that doesn't make sense right away to me.  That's when it shows it's real strength.</p><p>sing AI for autocomplete? I turn it off.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960726"><td></td></tr>
            <tr id="43959912"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959912" href="https://news.ycombinator.com/vote?id=43959912&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Currently using cursor. I've found cursor even without the AI features to be a more responsive VS Code. I've found the AI features to be particularly useful when I contain the blast radius to a unit of work.</p><p>If I am continuously able to break down my work into smaller pieces and build a tight testing loop, it does help me be more productive.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="43960993"><td></td></tr>
                  <tr id="43960006"><td></td></tr>
            <tr id="43959964"><td></td></tr>
            <tr id="43959870"><td></td></tr>
            <tr id="43960141"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960141" href="https://news.ycombinator.com/vote?id=43960141&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I’m using Github Copilot in VScode Insiders, mostly because I don’t want yet another subscription. I guess I’m missing out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959885"><td></td></tr>
            <tr id="43959862"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959862" href="https://news.ycombinator.com/vote?id=43959862&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>Personally copilot/code assist for tab autocomplete, if I need longer boilerplate I request it to the LLM. Usually VIM with LSP.</p><p>Anything that’s not boilerplate I still code it</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43959961"><td></td></tr>
            <tr id="43960249"><td></td></tr>
            <tr id="43959876"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43959876" href="https://news.ycombinator.com/vote?id=43959876&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I’d just wait a bit. At current rate of progress winner will be apparent sooner rather than later.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960029"><td></td></tr>
                <tr id="43960487"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960487" href="https://news.ycombinator.com/vote?id=43960487&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>This is the way.</p><p>Getting great results both in chat, edit and now agentic mode. Don’t have to worry about any blocked extensions in the cat and mouse game with MS.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="43960481"><td></td></tr>
            <tr id="43960722"><td></td></tr>
            <tr id="43960113"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960113" href="https://news.ycombinator.com/vote?id=43960113&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>Still on codeium lol! Might give aider another spin. It is never been quite good for my needs but tech evolves.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960545"><td></td></tr>
            <tr id="43959871"><td></td></tr>
            <tr id="43960121"><td></td></tr>
            <tr id="43960201"><td></td></tr>
            <tr id="43960020"><td></td></tr>
                <tr id="43960811"><td></td></tr>
            <tr id="43960522"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_43960522" href="https://news.ycombinator.com/vote?id=43960522&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div>
                  <p>I'd love to try it, could you please share an invite? My email is on my profile page.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960533"><td></td></tr>
                  <tr id="43960004"><td></td></tr>
            <tr id="43960399"><td></td></tr>
            <tr id="43960337"><td></td></tr>
            <tr id="43960373"><td></td></tr>
            <tr id="43960272"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_43960272" href="https://news.ycombinator.com/vote?id=43960272&amp;how=up&amp;goto=item%3Fid%3D43959710"></a></center>    </td><td><br><div><p>I am using both. Windsurf feels complete less clunky. They are very close tho and the pace of major updates is crazy.</p><p>I dont like CLI based tools to code. Dont understand why they are being shilled. Claude code is maybe better at coding from scratch because it is only raw power and eating tokens like there is no tomorrow but it us the wrong interface to build anything serious.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="43960605"><td></td></tr>
            <tr id="43960476"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I ruined my vacation by reverse engineering WSC (269 pts)]]></title>
            <link>https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/</link>
            <guid>43959403</guid>
            <pubDate>Mon, 12 May 2025 03:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/">https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/</a>, See on <a href="https://news.ycombinator.com/item?id=43959403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In this post I will briefly describe the journey I went through while implementing <a href="https://github.com/es3n1n/defendnot">defendnot</a>.</p><p>Even though this is most likely not what you expected to see here, but rather than going into full technical details on how everything works, I will describe what rabbitholes I went through and how painful everything was due to my ✨special✨ environment.</p><p>Beware, most likely this post will be too informal unlike the previous posts of mine, I am pretty sure that all the other posts with <code>irl</code> tag will be written in a style like this. If you are looking for a more detailed technical description of how everything works, a writeup like this will be released a bit later by someone else and I will link it here.</p><h2 id="a-one-year-step-back">A one-year step back</h2><p>Almost exactly one year ago I released a tool <a href="https://github.com/es3n1n/no-defender">no-defender</a>, a project that was disabling windows defender using the special windows api made for antiviruses to let the system know that there is an another antivirus so there is no need to run defender scans.</p><p>The part of the system that manages all this mess is called Windows Security Center - WSC for short. The way how my project worked is that it was using a thirdparty code from some already existing antivirus and forced that av to register the antivirus in WSC.</p><p>Then, after a few weeks after the release, the project blew up quite a bit and gained ~1.5k stars, after that the developers of the antivirus I was using filed a DMCA takedown request and I didn’t really want to do anything with that so just erased everything and called it a day.</p><h2 id="how-it-started">How it started</h2><p>Currently, even while writing this article, I am sitting in an airbnb we rented in Seoul. After numerous trips to other parts of the planet for CTFs and stuff, me and a friend of mine decided that we want to visit Seoul and arrived a few months after that.</p><p>My current main machine for non-ctf things is an M4Pro MacBook, and usually, when I am going for a CTF I bring an another x86 laptop with me to do some extensive reverse engineering/pwn stuff as it is usually built for the x86 cpus. Emulation would kind of work for this task but it is pretty painful so I just use an another laptop for all the x86 stuff.</p><p>And, as you might have guessed, for this trip I did not bring that x86 laptop with me, but I did bring my macbook with me to do some other development stuff in my free-free time. So, I did not have any x86 machine with me to do the x86 reversing.</p><p>And, on May 4th, after a few days spent in South Korea meeting my favorite South Korean CTF friends and drinking alcohol with them, I received a message from MrBruh where they said that they were looking at <a href="https://github.com/es3n1n/no-defender">no-defender</a> and were looking into whether it would be possible to create a “clean” implementation of my project without using any AVs.</p><h2 id="initial-research-day-1">Initial research (Day 1)</h2><p>I am having <em>some</em> troubles with my sleep schedule and I woke up a bit earlier than my friends so I decided to take a look at this while I am waiting for my friends to wake up.</p><p>MrBruh provided me the latest binaries of wsc because I was too lazy to spin up my parallels vm to get the binaries and I started looking into what we got.</p><p>As a reference implementation, I took the WSC registration implementation made by the same AV I was using a year ago. I was somewhat familiar with the internals of their thing and it was a great call.</p><p>Essentially, WSC has a COM API that all antiviruses are using, so I quickly rebuilt everything that AV was doing with it in ~1hr, booted an arm64 windows in parallels and tested the thing. I was greeted with an access denied error.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/access_denied_error.png" alt="access_denied"></p><p>But from my last year’s courtesy I knew that WSC was somehow validating the process that calls these APIs, my guess was that they are validating the signatures, which was indeed a correct guess but I didn’t know that for sure yet.</p><p>My move then was to inject my code into the same process that is doing all the WSC stuff for that AV and register my AV from there, when I did that this is what come out:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/first_success.png" alt="first_success"></p><p>Then, I recreated an another COM call to update the status of my fresh-new antivirus I registered and everything worked like a charm as well!</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/twitter_pic.png" alt="twitter_pic"></p><p>As you might have guessed, this is exactly the image I posted on twitter to let my beloved followers know that I might have something cooking:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/first_tweet.png" alt="tweet"></p><h2 id="trying-to-get-rid-of-the-avs-binary-day-1">Trying to get rid of the AVs binary (Day 1)</h2><p>After my initial research, I spent many hours actually enjoying life and arrived back to airbnb late at night and started tinkering with this again.</p><p>My first idea was to create a legit-signed process, inject my module in it, and execute my shenanigans from there, the exact same thing I was doing except I would use system-provided binaries and not AV’s ones <em>(because I didn’t want my new project to be removed from github by that AV)</em>.</p><p>As a first victim process I chose <code>cmd.exe</code> for no particular reason, just the first thing that came to my mind. However, to my surprise the api rejected my calls and I had to actually dig into the implementation to find out what was causing it.</p><p>After a quick look at <code>wscsvc.dll</code>, I found out that the binary was doing some calls to check the caller process for PPL, but, the binary I was running was created using just simple <code>CreateProcessA</code> call, there is no way it was PPL protected <em>(and it indeed was not)</em>.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/ppl_guessing.png" alt="ppl_guessing"></p><p>It was already pretty late in the morning so I went to sleep.</p><h2 id="setting-up-environment-day-2">Setting up environment (Day 2)</h2><p>When I woke up, I tried a bunch of other system processes, but nothing really worked, so I decided that it is actually time to properly reverse engineer this service, debug it and find out what is the reason behind all this - something I was really trying to avoid because I did not have an x86 machine.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/debug_intro.png" alt="debug_intro"></p><p>As you might still remember, I was working on an arm64 macbook and there currently is no sane solutions how to emulate x86 windows on arm macbooks. I didn’t feel like dealing with arm64 stuff and not being able to use my favorite x64dbg, so I asked a good friend of mine to lend me their pc while they are asleep, so that I can debug the wsc service in a virtual machine that will be running on their pc.</p><p>Luckily my friend <a href="https://github.com/pind0s">pindos</a> agreed to this almost instantly, shared an access to their pc through <a href="https://parsec.app/">parsec</a> and went to sleep. A rather notable thing about all this is that while I am in South Korea, they were in the USA so an average latency from my pc to theirs was around 210ms. Not very convenient, but bearable.</p><p>So, at that time, my setup looked like this:</p><ul><li>Build the module in windows arm64 running in parallels using MSVC</li><li>Share the build artifacts with my host using shared folders</li><li>Copy build artifacts using anydesk to the virtual machine running on pindos’ pc</li><li>Debug the service using parsec with 210ms latency</li></ul><p>As you can probably guess, this was <strong>extremely</strong> painful to proceed with, so the speed of development/researching was really poor.</p><h2 id="debugging-wsc-service-day-2">Debugging WSC service (Day 2)</h2><p>Essentially, WSC service is just a dll that is being run by svchost, the only thing that blocks us from attaching debugger to it right ahead is the PPL protection, which very conveniently can be removed with a few lines of code in kernel mode. So I enabled test mode on the vm, spinned up a driver that removed PPL from a process and we were good to go.</p><p>After looking into what happened after <code>cmd.exe</code> requested to register an antivirus in WSC, I traced down that the function it fails in was a so-called <code>WscServiceUtils::CreateExternalBaseFromCaller</code>.</p><p>Here’s a quick preview of that function:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>v10</span> <span>=</span> <span>RpcImpersonateClient</span>(<span>ClientBinding</span>);
</span></span><span><span><span>if</span> ( <span>ConvertStringSidToSidW</span>(<span>L</span><span>"S-1-5-80-1913148863-3492339771-4165695881-2087618961-4109116736"</span>, <span>Sid</span>) )
</span></span><span><span>{
</span></span><span><span>    <span>if</span> ( <span>!</span><span>CheckTokenMembership</span>(<span>0</span>, <span>Sid</span>[<span>0</span>], <span>&amp;</span><span>IsMember</span>) )
</span></span><span><span>    {
</span></span><span><span>        <span>IsMember</span> <span>=</span> <span>0</span>;
</span></span><span><span>    }
</span></span><span><span>    <span>LocalFree</span>(<span>Sid</span>[<span>0</span>]);
</span></span><span><span>}
</span></span><span><span><span>if</span> ( <span>IsMember</span> )
</span></span><span><span>{
</span></span><span><span>    <span>v8</span> <span>=</span> (<span>*</span>(<span>__int64</span> (<span>__fastcall</span> <span>**</span>)(<span>void</span> <span>*</span>, <span>struct</span> <span>CExternalBase</span> <span>**</span>))(<span>*</span>(<span>_QWORD</span> <span>*</span>)<span>a2</span> <span>+</span> <span>8LL</span>))(<span>a2</span>, <span>a4</span>);
</span></span><span><span>}
</span></span><span><span><span>else</span>
</span></span><span><span>{
</span></span><span><span>    <span>// a bunch of some other stuff 
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>So what it’s doing is checking whether the process that is calling the RPC methods has a <code>WinDefend</code> SID on a token.</p><p>Back then, I had no idea how any of the token-related stuff worked so I spent some time to get a grasp of how it is exactly working and came to a conclusion right ahead that if we impersonate <code>WinDefend</code>, we will pass all these checks and we should be good.</p><p>It was already evening and I wanted to see what happened in that function for the legit av binary I was initially testing my code in, and <em>for some reason, probably due to sleep deprivation</em> I came to the conclusion that IsMember was equals 1 for that binary. Now looking back on this, I have literally no clue why I assumed that, but I guess such things really tend to happen when you’re trying to speedrun stuff.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/sid_mystery.png" alt="sid_mystery"></p><h2 id="impersonating-windefend-day-2">Impersonating WinDefend (Day 2)</h2><p>After three more hours of learning how tokens in windows work, I came up with this theory:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/impersonating_windefend.png" alt="impersonating_windefend"></p><p>I spent some time doing IRL things, then came up with an implementation of the described algo:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/impersonated_windefend.png" alt="impersonated_windefend"></p><p>Right ahead, after chatting with my friends I wanted to test whether my code will work if I run it within the cmd that has WinDefend sid on its token.</p><p>Surprise surprise, while all the COM calls returned <code>STATUS_SUCCESS</code>, nothing really happened. It didn’t register any new AV, it didn’t do anything.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/status_success.png" alt="status_success"></p><h2 id="rebuilding-validation-algorithm-day-3">Rebuilding validation algorithm (Day 3)</h2><p>While I was asleep, the owner of PC I was using told me that they are going to sleep, but they didn’t shutdown the pc so I can connect to it anytime. I proceeded doing silly thingies IRL and once I was tired from that, went back to analyzing what was actually happening.</p><p>A first thing I did was verifying whether the checks I thought passed for that binary, indeed passed - mostly because I don’t trust myself, especially when I am sleep deprived. Man, I really tend to overlook things and misinterpret stuff sometimes.</p><p>As it turned out, in fact, the SID check did not pass for the legit AV binary as I thought it did. And it also, turned out, that if you pass that check you will be operating on the WSC object of windows defender, but this is rather useless because you can’t disable it with WSC calls just like that.</p><p>So I removed everything I had for the WinDefend impersonation and started looking into the second branch of the code.</p><p>And what it revealed to me is that if the windefend check didn’t pass, the service will check whether the calling binary is elevated, and then check for the signature and specific dll characteristics flag in the <code>CSecurityVerificationManager::CreateExternalBaseFromPESettings</code>, the function that gets called later on in within this branch:</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>/// Check DllCharacteristics flag
</span></span></span><span><span><span></span><span>FileW</span> <span>=</span> <span>CreateFileW</span>(<span>a2</span>, <span>0x80000000</span>, <span>1u</span>, <span>0</span>, <span>3u</span>, <span>0x8000000u</span>, <span>0</span>);
</span></span><span><span><span>FileMappingW</span> <span>=</span> <span>CreateFileMappingW</span>(<span>FileW</span>, <span>0</span>, <span>2u</span>, <span>0</span>, <span>0</span>, <span>0</span>);
</span></span><span><span><span>v12</span> <span>=</span> <span>MapViewOfFile</span>(<span>FileMappingW</span>, <span>4u</span>, <span>0</span>, <span>0</span>, <span>0</span>);
</span></span><span><span><span>v14</span> <span>=</span> <span>ImageNtHeader</span>(<span>v12</span>);
</span></span><span><span><span>v6</span> <span>=</span> <span>SLOBYTE</span>(<span>v14</span><span>-&gt;</span><span>OptionalHeader</span>.<span>DllCharacteristics</span>) <span>&lt;</span> <span>0</span>;
</span></span><span><span><span>UnmapViewOfFile</span>(<span>v13</span>);
</span></span><span><span>
</span></span><span><span><span>/// Check the signature
</span></span></span><span><span><span></span><span>CertContext</span> <span>=</span> <span>GetCertContext</span>(<span>a2</span>, <span>&amp;</span><span>pCertContext</span>);
</span></span><span><span><span>if</span> ( <span>CryptHashPublicKeyInfo</span>(
</span></span><span><span>        <span>0</span>,
</span></span><span><span>        <span>0x8003u</span>,
</span></span><span><span>        <span>0</span>,
</span></span><span><span>        <span>1u</span>,
</span></span><span><span>        <span>&amp;</span><span>pCertContext</span><span>-&gt;</span><span>pCertInfo</span><span>-&gt;</span><span>SubjectPublicKeyInfo</span>,
</span></span><span><span>        (<span>BYTE</span> <span>*</span>)<span>&amp;</span><span>SystemTime</span>,
</span></span><span><span>        <span>&amp;</span><span>pcbComputedHash</span>) )
</span></span><span><span>{
</span></span><span><span>    <span>/// Store signature info
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span><span>/// ...
</span></span></span></code></pre></div><p>After taking a look at the structure of <code>DllCharacteristics</code>, I realized that the flag it checks for is <code>ForceIntegrity</code>.</p><p>In debugger, the failing check I saw is the DllCharacteristics, so to get a new victim process, I recreated the checks wsc is doing on a binary (<code>wsc-binary-check</code> folder in <a href="https://github.com/es3n1n/defendnot">defendnot</a> repo) and tested all the System32 binaries against it.</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/recreated_algo.jpg" alt="recreated_algo"></p><h2 id="using-taskmgr-as-a-victim-process-day-3">Using Taskmgr as a victim process (Day 3)</h2><p>At that time my friend has already woken up and had to do some stuff for the uni on their PC, so I connected directly to the vm using <a href="https://parsec.app/">Parsec</a> and my setup immediately got even worse because on top of the latency issues, now the encoding was being done in software which was super slow.</p><p>I tried to just replace <code>cmd.exe</code> with <code>Taskmgr.exe</code> in my code and unfortunately something was still missing, I was still getting errors from RPC.</p><p>After an attempt to debug this on the same vm, I realized that it was pratically impossible to debug anything because everything was lagging super hard, some keys weren’t pressing, sometimes when I pressed a key once it was pressed two times on the machine - something that is super difficult to put up with.</p><p>So what I did, is I spent $30 on a <a href="https://shadow.tech/">shadow.tech</a> subscription after a friend recommended me this service, as it gives you a bare-metal access to the machine which was something I needed.</p><p>After waiting an hour, my VM was created, I hopped in and noticed that because the windows version they are using is much older than the latest one, the code I was interested in wscsvc wasn’t inlined to a single function and the overall “quality” of decompiled code was much better. Too bad I already had everything figured out, but if you are taking a look at wsc and confused about something due to the fact that everything was inlined in a single function, you can take a look at the older versions.</p><p>To cut the chase, the error that was happening on a VM was due to invalid name I passed as the AV name.</p><p>You see, the way how defendnot transfers data from defendnot-loader to defendnot.dll is by creating a <code>ctx.bin</code> file with serialized parameters. While I added a proper IPC mechanism in the project for state tracking, I was too lazy to port this context thingy to use the same IPC buffer and left the config still using this <code>ctx.bin</code> thing. It was a leftover from the initial <a href="https://github.com/es3n1n/no-defender">no-defender</a> code after all.</p><p>So what happened is that my code was reading an invalid <code>ctx.bin</code> file, because the function that deduced the path of path of this file was broken (it used the base folder of the module of <code>Taskmgr.exe</code>, not of the <code>nodefend.dll</code>), it read a bunch of null bytes and passed that as a name of the AV. Of course wsc rejected this buffer and returned an error.</p><p>After figuring it out, I fixed the issue, tested and here is what happened:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/worked.png" alt="worked"></p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/omg.png" alt="omg"></p><h2 id="cleaning-up-code-day-3">Cleaning up code (Day 3)</h2><p>I wanted to finish everything on this day so I stayed up until 8 am cleaning up code and implementing some other features, such as adding itself to autorun.</p><p>At 8 am, I had everything done except the autorun part because it was just not working. I tested numerous ways how to do it, but nothing really worked so I just went to sleep.</p><h2 id="implementing-autorun-day-4">Implementing autorun (Day 4)</h2><p>When I woke up, I immediately started working on the autorun and realized that while I was creating a task, the reason why my autorun code did not work is because of these two check boxes:</p><p><img src="https://blog.es3n1n.eu/posts/how-i-ruined-my-vacation/pics/task_thing.png" alt="task_thing"></p><p>This is exactly what was happening. My laptop was not on the AC power and the task simply was not executing. After unsetting these two flags, it started working.</p><p>I spent a few more hours cleaning up the code, and that was it.</p><h2 id="conclusion">Conclusion</h2><p>While that was a fun experience, I don’t think I would want to repeat everything I went through these past few days. Considering only that diabolical environment I had, it was already enough to make me lose my mind.</p><p>Thanks for reading, a more technical documentation of wsc will be released a bit later by someone else.</p><h2 id="acknowledgements">Acknowledgements</h2><ul><li><a href="https://github.com/pind0s">Pindos</a> for heating up their room by the pc running at night so that I can debug the WSC service</li><li><a href="https://mrbruh.com/">MrBruh</a> for poking me into researching this and listening to my mad ideas while I was working on this</li><li>Everyone else i was texting during these few days</li><li>I love you kimchi</li><li>The graffiti artist that VANDALIZED our wall</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Academic Pipeline Stall: Why Industry Must Stand for Academia (134 pts)]]></title>
            <link>https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/</link>
            <guid>43959129</guid>
            <pubDate>Mon, 12 May 2025 02:34:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/">https://www.sigarch.org/the-academic-pipeline-stall-why-industry-must-stand-for-academia/</a>, See on <a href="https://news.ycombinator.com/item?id=43959129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<h2><b>The Research Pipeline is Stalling</b></h2>
<p><span>The </span><a href="https://www.nature.com/articles/d41586-025-01396-2"><span>U.S. National Science Foundation (NSF) froze all outgoing funding</span></a><span>, including new awards and scheduled payments on active grants. Over</span><a href="https://airtable.com/appGKlSVeXniQZkFC/shrFxbl1YTqb3AyOO?jntvk%3Asort=eyJwZWw4ZlRSME8ycmFObWNUYyI6eyJjb2x1bW5JZCI6ImZsZHlMSEpuZkR6OXE1dEw0IiwiYXNjZW5kaW5nIjpmYWxzZX19"><span> 1,000 NSF research projects</span></a><span> were abruptly canceled in a few days, resulting in roughly $739 million in halted research funding. The directive, issued with little explanation, has created chaos across the academic research ecosystem, part of a broader trend <a href="https://www.nature.com/articles/d41586-025-00562-w">Nature</a> described as an unprecedented assault.</span></p>
<p>Before we go any further, let me be clear: this isn’t about sides or ideologies. Support for education and research should be as fundamental as clean air or safe roads. It is part of the shared infrastructure that holds society together. When that foundation cracks, the consequences ripple far beyond the lab.</p>
<p>The ramifications are profound. Laboratories have been forced to suspend operations.<a href="https://www.nature.com/articles/d41586-025-00608-z"> Graduate students face uncertainty</a> about completing their degrees.<a href="https://www.science.org/content/article/u-s-early-career-researchers-struggling-amid-chaos"> Early-career faculty have lost their first major grants</a>, sometimes just months after starting their jobs and labs.<a href="https://www.insidehighered.com/news/government/politics-elections/2025/02/19/federal-funding-uncertainty-prompts-hiring-freezes"> Departments are freezing hiring</a>, deferring PhD admissions, and scrambling to keep core infrastructure afloat. The<a href="https://www.npr.org/2025/05/02/nx-s1-5371720/national-science-foundation-budget-grant-cuts-turmoil"> entire academic research enterprise is stalling</a>, not because the ideas aren’t there, but because the support has vanished. What was once America’s steady innovation engine is now sputtering under the weight of policy and silence.</p>
<p><span>Meanwhile, where are those who benefited from America’s higher education? The tech giants whose founders and engineers were trained in these institutions, whose core technologies were incubated in these research environments? Universities are left to defend </span><a href="https://www.harvard.edu/president/news/2025/the-promise-of-american-higher-education/"><span>The Promise of American Higher Education</span></a><span> alone. There’s no contingency plan for this disruption, no industry emergency fund to save labs, and no guidance on preserving student funding. Every department, PI, and institution is improvising, trying to patch over a pipeline cracking at every joint—a pipeline that sent talent streaming into industry coffers for decades.</span></p>
<p>This is what system designers recognize as a <em>pipeline stall</em>. The inputs (funding and institutional support) are blocked. The outputs (trained students, published research, working prototypes) are starving for resources, creating a growing bubble of dependency hazards. The pipeline stages that move ideas forward (labs, advisors, infrastructure) are frozen, and values cannot be forwarded to the next stage. When the stall isn’t resolved quickly, latency builds, dependencies break, and failure propagates backward through the entire execution path.</p>
<p><span>This blog is a </span><b>call to action</b><span>. It asks our community of researchers, educators, and—especially—our industry leaders and alumni who have directly benefited from higher education to stand in solidarity with the system that built us. It asks the industry (as a holistic entity), in particular, to move beyond silent support and take visible, vocal responsibility for the research pipeline on which it depends. </span></p>
<p><b>The academic pipeline is stalling, and it will take all of us, especially those who have reaped its rewards, to step up as stewards of science and defend the academic foundations that made their success possible.</b></p>
<h2><b>A Legacy We All Recognize</b></h2>
<p><span>We don’t need to remind the SIGARCH community of our shared history. Every member of this community knows that our field’s landmark innovations emerged not purely from product roadmaps but from university labs with federal funding. You know these stories. You lived them, or you studied under those who did.</span></p>
<p><span>The story of systems innovation is inseparable from public research. RISC, out-of-order execution, speculative prefetching, vector processing, GPGPU, and multicore—all of these breakthroughs took shape in university labs, powered by federal support. These weren’t incremental product features. They were conceptual leaps, sometimes radical, requiring time, freedom, and student talent to explore. </span>And these breakthroughs didn’t stay in the lab. They reshaped entire industries. Public investment has powered innovation through three enduring pathways:</p>
<p><strong data-start="674" data-end="730">Academic Tools That Became Global Infrastructure. </strong><a href="http://riscv.org/">RISC-V</a>, now a worldwide open standard, began as a research project at UC Berkeley. The <a href="http://gem5.org/">gem5 simulator</a>, essential for computer architecture research, emerged from NSF-supported efforts at the University of Wisconsin–Madison and Michigan. It continues receiving federal support as it evolves to meet the research community’s needs. <a href="http://llvm.org/">LLVM</a>, the compiler framework now embedded in everything from iOS to data centers, was launched and developed at the University of Illinois with federal funding support.</p>
<p><strong data-start="1082" data-end="1135">Research Projects That Became Industry Giants. </strong><a href="https://www.nsf.gov/news/origins-google">Google began as an NSF-funded</a> Digital Library project at Stanford. <a href="https://www.akamai.com/company/company-history">Akamai spun out of federally funded MIT research on internet traffic</a>, helping to build the content delivery layer of the modern web. <a href="https://www.sbir.gov/success/qualcomm-inducted-sbir-hall-fame">Qualcomm’s early wireless breakthroughs were rooted in federal funding</a>, thanks to faculty at UCSD and UCLA. <a href="https://www.sri.com/75-years-of-innovation/75-years-of-innovation-siri/#:~:text=The%20development%20of%20Siri%20was,Learns%20and%20Observes.">Siri traces its origins to federally funded work</a> in cognitive assistants. Each of these examples shows how public research has seeded technologies that became transformative companies.</p>
<p><strong data-start="1531" data-end="1583"><strong data-start="1531" data-end="1583">Benchmarks That Became Standards. </strong></strong><a href="http://spec.org/">SPEC</a> and <a href="http://mlcommons.org/">MLPerf</a>, which define fairness and accountability in systems evaluation, were shaped by collaborations among academics, national labs, and industry. SPEC benefited from early academic input and support from labs like LLNL. MLPerf was developed through a broad partnership spanning Stanford, UC Berkeley, and Harvard, alongside national labs and leading industry partners. Federal funding helped support the graduate students and principal investigators who worked across these sectors.</p>
<p>These are just a few examples, but they’re not outliers. They are the natural result of sustained public investment in ideas, people, and infrastructure. When we fund basic research, we don’t just advance knowledge; instead, we lay the groundwork for industries, train the talent that leads them, and shape the technologies that define our future.</p>
<p><b>Industry’s Quiet Debt to Academic Research</b></p>
<p>Amidst the growing chaos, <strong>the silence from industry is deafening</strong>. The tech sector often presents its breakthroughs as the product of internal innovation, as if they emerged fully formed from corporate labs. But as we’ve seen, many of today’s most transformative advances were first conceived, developed, and refined in publicly funded university research, long before they were commercialized by companies that now profit from them.</p>
<p>Moreover, the engineers and researchers who lead research innovation and infrastructure development at places such as Google, Meta, NVIDIA, AMD, and Apple were trained in publicly funded universities. The architects of MapReduce, TensorFlow, CUDA, and countless other industry-defining systems started in academic labs backed by NSF and DARPA grants. They learned their craft in spaces free from quarterly earnings pressure.</p>
<p>Yet, the returns on that public investment have been wildly asymmetric. A few million in research gifts are scattered across universities. Some branded fellowships support a handful of Ph.D. students, while many more are needed to sustain the scale of these programs. Occasionally, nonprofit consortia offer additional support. All of this is valued, but it pales compared to the industry salaries, stock options, and profit margins built on this foundation.</p>
<p>Meanwhile, academia continues to carry the burden: training the next generation, maintaining open infrastructure, and stewarding community benchmarks and standards—all while industry captures the lion’s share of the financial reward. This isn’t just an imbalance; it’s an existential threat to the very pipeline that sustains the industry’s future.</p>
<p><span>At a moment when the research ecosystem that built today’s tech giants is under unprecedented strain, the absence of their collective voice speaks volumes. Industry must step up to support and defend the public pipeline. Otherwise, it risks starving the very source of its future:</span></p>
<ul>
<li>
<ul>
<li aria-level="1"><b>You don’t get applied research without basic research.</b></li>
<li aria-level="1"><b>You don’t get a talent pool without a talent pipeline. </b></li>
<li aria-level="1"><b>You don’t get scale without exploration.</b></li>
</ul>
</li>
</ul>
<p><span>Many of you, our deeply respected colleagues, collaborators, and friends in industry, understand this. However, at this moment, mere understanding isn’t enough. We need you to speak up, show up, and take action. What is one thing you’re willing to do? Say it, do it, and let it count. And make no mistake: the need for action is urgent.</span></p>
<h2><b>The Talent Pipeline Under Threat</b></h2>
<p>Nowhere is action more urgently needed than protecting the talent pipeline, which is the foundation of our success. The pipeline that brought so many of us into this field is no longer just at risk; it’s beginning to collapse. Every chip, compiler, and system we’ve built has a lineage that runs through undergraduates, graduate students, and postdocs.</p>
<p><span>The fallout is already here. Schools are </span><a href="https://www.wesa.fm/health-science-tech/2025-02-21/university-pittsburgh-phd-pause-research-funding-uncertainty"><span>pausing PhD admissions</span></a><span> and are unsure if they can support students. Some <a href="https://www.thedp.com/article/2025/02/penn-graduate-student-class-size-cut-trump-funding">offers are being rescinded</a>. The prestigious</span><a href="https://www.nature.com/articles/d41586-025-01098-9"><span> NSF Graduate Research Fellowship</span></a><span> saw its awards cut in half this year. Undergraduate</span><a href="https://beta.nsf.gov/funding/opportunities/research-experiences-undergraduates-reu"> <span>REU programs</span></a><span>, which are a critical entry point into research for many budding scientists since they support their first exposure to research, have dropped from ~200 to just over 50 funded sites. Postdocs are being <a href="https://www.reddit.com/r/labrats/comments/1jq5wna/just_found_out_my_postdoc_got_terminated/">let go and terminated</a>. One PI described the situation as “<em>being in a war—you’re just trying to survive.</em>”</span></p>
<p><span>Meanwhile, </span><a href="https://ww2.aip.org/fyi/funding-cuts-hit-stem-career-pipelines"><span>65 NSF CAREER grants were canceled</span></a><span>, a blow that will deeply affect early-career faculty. Computer architecture is particularly vulnerable. Research in this field depends on costly infrastructure—simulators, chip tape-outs, and FPGAs. Without stable funding, labs shrink or shutter. The students who might have built the next great system won’t be trained. <strong>Unlike cloud computing infrastructure, academic infrastructure doesn’t scale back up overnight. </strong></span>There’s no autoscaling for talent development or research continuity.</p>
<p>International talent, long a cornerstone of the U.S. research engine, is also at risk. As the academic pathway grows more unstable and funding dries up, many may hesitate to come—or to stay. A growing number of scientists are now considering leaving. <a href="https://sciencebusiness.net/sites/default/files/inline-files/Letter%20to%20Commissioner%20Zaharieva.pdf">Europe is actively positioning itself as a haven for embattled U.S. researchers</a>, with calls for a “solidarity and attractivity boom” to welcome brilliant minds facing “ill-motivated and brutal funding cuts.” The <a href="https://www.nature.com/articles/d41586-025-00992-6">result could be a devastating brain drain</a> that would impoverish American innovation and diminish science.</p>
<p><span>Perhaps the most concerning aspect of this is that it isn’t just a slow-moving crisis; it’s happening now. If we don’t act, the next generation of systems talent may never enter the field.</span></p>
<h2><b>What We Can Do Now: ACT</b></h2>
<p><span>We know the systems community takes pride in solving hard problems. This is one of them. If you care about the future of innovation, education, and research, here’s a to-do:</span></p>
<p><span><b>A</b></span><b>dvocate with </b><a href="https://bit.ly/HigherEdPledge"><b>The People’s Pledge for Higher Education</b></a><b>: </b>No matter where you are, if you benefited from American higher education, make your voice part of something larger. Help build visibility, solidarity, and public pressure. This collective signal will help show our policymakers that research, education, and innovation are values we’re willing to defend. It gives them the moral support they need to fight the battles. Sign the pledge.</p>
<p><span><b>C</b></span><b>all on industry to contribute:</b><span> Ask your colleagues and friends in industry to step up. Those who have benefited should now stand to uphold. If you’re in the industry, ask your company to acknowledge its public reliance on the academic research pipeline. Encourage leadership to issue a statement, provide bridge funding for canceled research, or directly support students and labs. Urge them to engage with policymakers. Industry has influence—help it use that power responsibly. Make that call.</span></p>
<p><span><b>T</b></span><b>alk beyond the echo chamber: </b>If you’re in the U.S., <a href="https://www.congress.gov/members/find-your-member">contact your senator</a>. But no matter where you are, this movement needs more than policy. It needs people. Reach out to someone outside your usual circles: a colleague in industry, a former student, or a friend in product development. If each of us brings just one new voice into the conversation, we multiply our impact. Let your voice be heard.</p>
<p><b>Here’s an example of putting these principles into action:</b><span> Not everything has to be grand. My wife and I hosted a backyard BBQ for people in Boston—just good home-cooked food and an open invitation to folks from industry, academia, and the broader community. Over 100 people showed up, including several members of our SIGARCH community, not for an agenda, but because they care about higher education for their kids and their neighbors’ kids. We simply made space for conversation, connection, and reflection on what’s happening to education. The discussions that started that day are still unfolding, leading to actions. Sometimes, the most meaningful action begins with shared meals (because we all love food), honest dialogue, and the courage to come together.</span></p>
<p><span>Pick one. Reach one person. Do it today. </span></p>
<h2><b>Conclusion</b></h2>
<p>I’ve been surprised by how many people, from industry leaders to scientific citizens, are afraid to speak up. They cite fears of professional or social consequences. I understand that fear. But silence, especially now, is a form of complicity. As that silence reveals a troubling reality: in a field built on boldness and innovation, we have become hesitant to defend the very system that enables our work. <strong>Do something—just don’t do nothing.</strong></p>
<p>We live in a time that demands courage, the courage to speak up when others stay silent, to rise when others remain seated, and to build bridges between academia and industry rather than watch the gap grow wider. That courage is not something we must summon alone. As a community, we can find it together. We are responsible for protecting the ecosystem that enabled our success and ensuring it thrives for the next generation. For those in senior positions, that means using your influence, whether leading a research lab, directing an engineering team, or shaping corporate strategy, to keep our field’s intellectual infrastructure strong and vibrant.</p>
<p>As <a href="https://www.nature.com/articles/d41586-025-00562-w">Nature editorialized in February 2025</a>, “<strong>an assault on science and scientists <em>anywhere</em> is an assault on science and scientists <em>everywhere</em>.</strong>” The <a href="https://royalsociety.org/news/2025/02/science-under-threat/">Royal Society similarly warned</a> that cutting science for ideological reasons threatens not only national progress but the global fabric of evidence-based innovation. Science is collaborative by nature; when it’s weakened in one part of the world, the consequences ripple outward.</p>
<p>The time has come to be stewards of science—to stand up for what we believe in and what’s essential, even when it’s hard. Not to sit back and watch the pipeline stall, or let the system that built us quietly break. Our collective future in computing depends not just on what we invent, but on our willingness to defend the conditions that make invention possible. Find your purpose to summon the courage. I’m doing this to protect the opportunities that shaped my path — for my beautiful daughters, and for your kids. A.C.T. now.</p>
<h2><strong>About the Author</strong></h2>
<p>Dr. Vijay Janapa Reddi is a Professor at Harvard University, Vice President and co-founder of MLCommons, and author of the open-source <a href="https://mlsysbook.ai/">Machine Learning Systems textbook</a> and the <a href="https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning">TinyML edX course series</a>. His work bridges computer architecture, machine learning systems, and edge computing. He is passionate about helping others succeed, just as his mentors, public education, and the academic pipeline once helped him.</p>
<p><strong>[Read the <a href="https://bit.ly/Pipeline-Stalling">Whitepaper</a> and <a href="https://cacm.acm.org/opinion/big-tech-you-need-academia-speak-up/">CACM article</a>] [<a href="https://creators.spotify.com/pod/show/jveejay/episodes/The%20Pipeline%20Is%20Stalling:%20Why%20America%E2%80%99s%20Economic%20and%20Technological%20Future%20Depends%20on%20Its%20Universities-e329lpn">Listen to the Podcast</a>] [✍️ <a href="https://bit.ly/HigherEdPledge">Sign the Pledge</a>]</strong></p>
<p><strong>Disclaimer:</strong> <em>These posts are written by individual contributors to share their thoughts on the Computer Architecture Today blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGARCH or its parent organization, ACM.</em></p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous Thought Machines (236 pts)]]></title>
            <link>https://pub.sakana.ai/ctm/</link>
            <guid>43959071</guid>
            <pubDate>Mon, 12 May 2025 02:21:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pub.sakana.ai/ctm/">https://pub.sakana.ai/ctm/</a>, See on <a href="https://news.ycombinator.com/item?id=43959071">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="no_javasript_warning">
    <h3>This page requires Javascript. Please enable it to view the website.</h3>
  </p><div id="dtbody">


<dt-byline></dt-byline>

<p>
<figcaption>
  tl;dr
</figcaption>
<figcaption>
  Neurons in brains use timing and synchronization in the way that they compute. This property seems essential for the flexibility and adaptability of biological intelligence. Modern AI systems discard this fundamental property in favor of efficiency and simplicity. We found a way of bridging the gap between the existing powerful implementations and scalability of modern AI, and the biological plausibility paradigm where <b>neuron timing matters</b>. The results have been surprising and encouraging.
<br>
</figcaption>
</p>



<div id="maze-demo-container">
    <h2>Interactive demonstration</h2>
    <div id="maze-demo">
            <!-- <div id="maze-load-overlay" class="maze-load-overlay loading-overlay-inactive">
                <div class="overlay-text">Click/Touch to load maze demo</div>
            </div> -->
            <p>Initializing...</p>
            
            <canvas id="mazeCanvas" width="39" height="39"></canvas>
            
            <div id="controls">
                
                <div>
                    <div>
                        <p>
                            <label for="validOnlyCheckbox">Valid Path Only</label>
                        </p>
                        <p>
                            <label for="autoSolveCheckbox">Auto-solve</label>
                        </p>
                    </div>
                    <div>
                        <p>
                            <label for="showPathCheckbox">Show Path</label>
                        </p>
                        <p>
                            <label for="showOverlayCheckbox">Show Attention Overlay</label>
                        </p>
                    </div>
                </div>
                <p><label for="fpsSlider">Animation FPS:</label>
                    
                    <span id="fpsValueDisplay">60</span>
                </p>
            </div> 
            <p>
                Click to move Start/ End (toggle with 'move')
            </p>
            </div> 
        </div>
    
    



<h2>Introduction</h2>
<p>Neural networks (NNs) were originally inspired by biological brains, yet they remain significantly distinct from their biological counterparts. Brains demonstrate complex neural dynamics that evolve over time, but modern NNs intentionally abstract away such temporal dynamics in order to facilitate large-scale deep learning. For instance, the activation functions of standard NNs can be seen as an intentional abstraction of a neuron's firing rate, replacing the temporal dynamics of biological processes with a single, static value. Such simplifications, though enabling significant advancements in large-scale machine learning <dt-cite key="lecun2015deep,goodfellow2016deep,wei2022emergent"></dt-cite>, have resulted in a departure from the fundamental principles that govern biological neural computation.</p>
<p>Over hundreds of millions of years, evolution has endowed biological brains with rich neural dynamics, including spike-timing-dependent plasticity (STDP) <dt-cite key="caporale2008spike"></dt-cite> and neuronal oscillations. Emulating these mechanisms, particularly the temporal coding inherent in spike timing and synchrony, presents a significant challenge. Consequently, modern neural networks do not rely on temporal dynamics to perform compute, but rather prioritize simplicity and computational efficiency. This abstraction, while boosting performance on specific tasks, contributes to a recognized gap between the flexible, general nature of human cognition and current AI capabilities, suggesting fundamental components, potentially related to temporal processing, are missing from our current models <dt-cite key="lake2017building,marcus2018deep,chollet2019measure"></dt-cite>.</p>
<div>
  <h4>Why do this research?</h4>
  <p>Indeed, the notably high performance of modern AI across many fields suggests the emulation of neural dynamics is unwarranted. However, the gap between the highly flexible and general nature of human cognition and the current state of modern AI suggests missing components in our current models.</p>
</div>
<p>For these reasons, we argue that time should be a central component of artificial intelligence in order for it to eventually achieve levels of competency that rival or surpass human brains <dt-cite key="cariani2022time,maass2001relevance"></dt-cite>. Therefore, in this work, we address the strong limitation imposed by overlooking neural activity as a central aspect of intelligence. We introduce the <strong>Continuous Thought Machine (CTM)</strong>, a novel neural network architecture designed to explicitly incorporate neural timing as a foundational element. Our contributions are as follows:</p>
<ul>
<li>We introduce a <strong>decoupled internal dimension</strong>, a novel approach to modeling the temporal evolution of neural activity. We view this dimension as that over which thought can unfold in an artificial neural system, hence the choice of nomenclature.</li>
<li>We provide a mid-level abstraction for neurons, which we call <strong>neuron-level models</strong> (NLMs), where every neuron has its own internal weights that process a history of incoming signals (i.e., pre-activations) to activate (as opposed to a static ReLU, for example).</li>
<li>We use <strong>neural synchronization</strong> directly as the latent representation with which the CTM observes (e.g., through an attention query) and predicts (e.g., via a projection to logits). This biologically-inspired design choice puts forward neural activity as the crucial element for any manifestation of intelligence the CTM might demonstrate.</li>
</ul>
<h4>Reasoning models and recurrence</h4>
<p>The frontier of artificial intelligence faces a critical juncture: moving beyond simple input-output mappings towards genuine reasoning capabilities. While scaling existing models has yielded remarkable advancements, the associated computational cost and data demands are unsustainable and raise questions about the long-term viability of this approach. For sequential data, longstanding recurrent architectures <dt-cite key="hochreiter1997long,dey2017gate,medsker1999recurrent"></dt-cite> have largely been superseded by transformer-based approaches <dt-cite key="vaswani2017attention"></dt-cite>. Nevertheless, recurrence is re-emerging as a natural avenue for extending model complexity. Recurrence is promising because it enables iterative processing and the accumulation of information over time. Modern text generation models (sometimes referred to as 'reasoning models') use intermediate generations as a form of recurrence that enables additional compute during test-time. Recently, other works have demonstrated the benefits of the recurrent application of latent layers <dt-cite key="jaegle2021perceiver,geiping2025scaling,yang2023looped"></dt-cite>. While such methods bring us closer to the recurrent structure of biological brains, a fundamental gap nevertheless remains. <strong>We posit that recurrence, while essential, is merely one piece of the puzzle</strong>. The temporal dynamics unlocked by recurrence -- the precise timing and interplay of neural activity -- are equally crucial. The CTM differs from existing approaches in three ways: (1) the decoupled internal dimension enables sequential thought on any conceivable data modality; (2) private neuron-level models enables the consideration of precise neural timing; and (3) neural synchronization used directly as a representation for solving tasks.</p>
<hr>
<h2>Method</h2>


<p>
<figcaption>
<b>Fig 1.</b> The Continuous Thought Machine: <span>a single step in its internal recurrent process.</span>
</figcaption>
<figcaption>
The CTM unfolds neural activity internally as it thinks about data. At each step (one of which demonstrated above) a truncated history of 'pre activations' are collected and used for the <b>Neuron Level Models</b> (NLMs). The history of 'post activations' produced by all NLMs over time are kept and used to compute neuron-to-neuron synchronization over time. This result is a <b>Synchronization Representation</b>: a new, parameter-efficient, and <i>evidently powerful</i> representation that the CTM uses to observe (via attention) and predict.
<br>
</figcaption>
</p>
<p>The Continuous Thought Machine (CTM) is a neural network architecture that enables a novel approach to thinking about data. It departs from conventional feed-forward models by explicitly incorporating the concept of <strong>Neural Dynamics</strong> as the central component to its functionality. The video above gives a pictorial overview of the internal workings of the CTM. We give all technical details, including additional figures and verbose explanations in our <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>. A <a href="https://github.com/SakanaAI/continuous-thought-machines" target="_blank">GitHub repository</a> is also available. We will provide links to relevant parts of the repository as we explain the model below.</p>
<div>
    <figure>
        <img src="https://pub.sakana.ai/ctm/assets/png/architecture.jpeg" alt="CTM architecture">
        <figcaption>
        <span><b>Fig 2.</b> CTM architecture</span>: The <span>1</span> synapse model (weights depicted as blue lines) models the cross-neuron interactions to produce pre-activations. For each neuron, a <span>2</span> history of pre-activations is kept, the most recent of which are used by the <span>3</span> neuron-level models (weights depicted as red lines) to produce <span>4</span> post-activations. A <span>5</span> history of post-activations is also kept and used to <span>6</span> compute a synchronization matrix. Neuron pairs are <span>7</span> selected from the synchronization matrix, yielding the <span>8</span> latent representations with which the CTM <span>9</span> produces outputs and modulates data through cross-attention. Modulated data (e.g., attention outputs) are <span>10</span> concatenated with post-activations for the next internal tick.
        </figcaption>
    </figure>
    <figure>
        <table>
            <thead>
                <tr>
                    <th scope="col">Variable</th>
                    <th scope="col">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>$\mathbf{z}^t$</td> 
                    <td>Post-activations at internal tick $t$, after neuron-level models have been used.</td> 
                </tr>
                <tr>
                    <td>$\theta_{\text{syn}}$</td>
                    <td>Recurrent (synapse) model weights; U-NET-like architecture that connects neurons at a given internal tick, $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{a}^t$</td>
                    <td>Pre-activations at internal tick $t$.</td>
                </tr>
                <tr>
                    <td>$\mathbf{A}^t$</td>
                    <td>History of <i>most recent</i> pre-activations, designed as a FIFO list so that they are always length $M$; inputs to neuron-level models.</td>
                </tr>
                <tr>
                    <td>$\theta_{\text{d}}$</td>
                    <td>Weights of a <i>single neuron-level model</i>, $d$ of $D$; MLP architecture, unique weights per neuron.</td>
                </tr>
                <tr>
                    <td>$\mathbf{Z}^t$</td>
                    <td>History of <i>all</i> post-activations up to this internal tick, variable length; used as input for synchronization dot products.</td>
                </tr>
                <tr>
                    <td>$\mathbf{S}^t$</td>
                    <td>Synchronization matrix at internal tick $t$. In practice we use far fewer neurons than $D$ for separate $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ synchronization representations.</td>
                </tr>
                <tr>
                    <td>$\mathbf{W}_{\text{out}}$, $\mathbf{W}_{\text{in}}$</td>
                    <td>Linear weight matrices that project from $\mathbf{S}^t_{\text{out}}$ and $\mathbf{S}^t_{\text{action}}$ to attention queries and predictions, respectively.</td>
                </tr>
                <tr>
                    <td>$\mathbf{o}^t$</td> 
                    <td>Cross attention output.</td> 
                </tr>
                </tbody>
        </table>
    </figure>
</div>
<p><strong>The CTM consists of three main ideas</strong>:</p>
<ol>
<li>The use of <a href="#internal-ticks">internal recurrence</a>, enabling a dimension over which a concept analogous to <strong>thought</strong> can occur. The entire process visualised in the video above is a single tick; the <a href="#maze-demo">interactive maze demo</a> at the top of the page uses 75 ticks. This recurrence is completely decoupled from any data dimensions.</li>
<li><a href="#neuron-level-models">Neuron-level models</a>, that compute post-activations by applying private (i.e., on a per-neuron basis) MLP models to a <em>history of incoming pre-activations</em>.</li>
<li><a href="#synchronization-representation">Synchronization as a representation</a>, where the neural activity over time is tracked and used to compute how pairs of neurons synchronize with one another over time. This measure of synchronization is the representation with which the CTM takes action and makes predictions. Listing 3 in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> shows the logic for this, and Appendix K details how we use a recursive computation for efficiency.</li>
</ol>
<div>
  <h4>But what about data?</h4>
  <div>
    <p>While data is undoubtedly crucial for any modeling, the CTM is designed around the idea of internal recurrence and synchronization, where the role of data is somewhat secondary to the internal process itself.</p>
    <p><a href="#from-data">Input data is attended to</a> and ingested at each internal tick based on the current sychronisation, and similarly for predictions.</p>
  </div>
</div>
<video src="https://pub.sakana.ai/ctm/assets/mp4/topvideo.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
<p>
<figcaption><b>Fig 3.</b> Neural Dynamics when thinking about ImageNet: <span>Each subplot is the activity of a single neuron over time. It is the synchronization between these that forms the representation used by the CTM.</span>
</figcaption>
</p>
<h3 id="internal-ticks">Internal ticks: the 'thought' dimension</h3>
<p>We start by introducing the continuous internal dimension: <span><span><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in  \{ 1, \ldots ,T \}</annotation></semantics></math></span></span>. Unlike conventional sequential models -- such as RNNs or Transformers -- that process inputs step-by-step according to the sequence inherent in the data (e.g., words in a sentence or frames in a video), the CTM operates along a self-generated timeline of internal <strong>thought steps</strong>. This internal unfolding allows the model to iteratively build and refine its representations, even when processing static or non-sequential data such as images or mazes. To conform with existing nomenclature used in related works <dt-cite key="kirsch2021meta,pedersen2024structurally,kirsch2022introducing,schwarzschild2021can"></dt-cite>, we refer to these thought steps as 'internal ticks' from here on.</p>
<div>
  <h4>A dimension over which thought can unfold.</h4>
  <p>The CTM's internal dimension is that over which the dynamics of neural activity can unfold. We believe that such dynamics are likely a cornerstone of intelligent thought.</p>
</div>
<h3 id="synapses">Recurrent weights: synapses</h3>
<p>A recurrent multi-layer perceptron (MLP structured in a U-NET fashion <dt-cite key="ronneberger2015u"></dt-cite>) acts as a synapse model for the CTM. At any internal tick <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, the synapse model produces what we consider <strong>pre-activations</strong>:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub><mo>(</mo><mtext><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>)</mo><mo>)</mo><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>D</mi></msup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{a}^t = f_{\theta_{\text{syn}}}(\text{concat}(\bold{z}^t, \bold{o}^t)) \in~\mathbb{R}^D,
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{o}^t</annotation></semantics></math></span></span> is <a href="#from-data">from input data</a>. The <span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> <strong>most recent pre-activations</strong> are then collected into a pre-activation 'history':</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">A</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mrow><mi>t</mi><mo>−</mo><mi>M</mi><mo>+</mo><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">a</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>M</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{A}^t = \begin{bmatrix}
\bold{a}^{t-M+1} &amp; \bold{a}^{t-M+2} &amp; \cdots &amp; \bold{a}^t
\end{bmatrix} \in~\mathbb{R}^{D \times M}.
</annotation></semantics></math></span></span></span></p>
<h3 id="neuron-level-models">Neuron-level models</h3>
<p><span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> effectively defines the length of the <strong>history of pre-activations</strong> that each neuron level model works with. Each neuron, <span><span><math><semantics><mrow><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>D</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">\{1, \ldots, D\}</annotation></semantics></math></span></span>, is then <strong>given its own privately parameterized MLP</strong> that produces what we consider <strong>post-activations</strong>:</p>
<p><span><span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>g</mi><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow></msub><mo>(</mo><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1} = g_{\theta_d}(\bold{A}_d^t),
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msub><mi>θ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\theta_d</annotation></semantics></math></span></span> are the unique parameters for neuron <span><span><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span></span>, and <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">z</mi></mrow><mi>d</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\bold{z}_d^{t+1}</annotation></semantics></math></span></span> is a single unit in the vector that contains all <strong>post-activations</strong>. <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">A</mi></mrow><mi>d</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{A}_d^t</annotation></semantics></math></span></span> is a <span><span><math><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span>-dimensional vector (time series). The full set of neuron post-activations are then concatenated with  <a href="#from-data">attention output</a> and fed recurrently into <span><span><math><semantics><mrow><msub><mi>f</mi><mrow><msub><mi>θ</mi><mrow><mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow></msub></mrow><annotation encoding="application/x-tex">f_{\theta_{\text{syn}}}</annotation></semantics></math></span></span> to produce pre-activations for next step, <span><span><math><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span></span>, in the unfolding thought process.</p>
<h3 id="synchronization-representation">Synchronization as a representation: modulating data</h3>
<p>How should the CTM interact with the outside world? Specifically, how should the CTM consume inputs and produce outputs? We introduced a timing dimension over which something akin to thought can unfold. We also want the CTM's relationship with data (its interaction, so to speak) to depend not on a <em>snapshot</em> of the state of neurons (at some <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>), but rather on the <strong>ongoing temporal dynamics of neuron activities</strong>. By way of solution, we turn again to natural brains for inspiration and find the concept of neural synchronization <dt-cite key="uhlhaas2009neural"></dt-cite> both fitting and powerful. For synchronization we start by collecting the post-activations into a post-activation 'history':</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo fence="true">[</mo><mtable><mtr><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>1</mn></mrow></msup></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow></mtd><mtd><mrow><mo>⋯</mo></mrow></mtd><mtd><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mi>t</mi></msup></mrow></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>t</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{Z}^t = \begin{bmatrix}
\bold{z}^{1} &amp; \bold{z}^{2} &amp; \cdots &amp; \bold{z}^t
\end{bmatrix} \in \mathbb{R}^{D \times t}.
</annotation></semantics></math></span></span></span></p>
<p>The length of <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{Z}^t</annotation></semantics></math></span></span> is equal to the current internal tick, meaning that <strong>this dimension is not fixed</strong> and can be arbitrarily large. We define neural synchronization as the matrix yielded by the inner dot product between post-activation histories:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup><mo>=</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><mo>⋅</mo><mo>(</mo><msup><mrow><mi mathvariant="bold">Z</mi></mrow><mi>t</mi></msup><msup><mo>)</mo><mo>⊺</mo></msup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>D</mi><mo>×</mo><mi>D</mi></mrow></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{S}^t = \bold{Z}^t \cdot (\bold{Z}^t)^\intercal \in~\mathbb{R}^{D\times D}.
</annotation></semantics></math></span></span></span></p>
<p>Since this matrix scales in <span><span><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mi>D</mi><mn>2</mn></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(D^2)</annotation></semantics></math></span></span> it makes practical sense to subsample <span><span><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span></span> row-column pairs, which capture the synchronization between neurons <span><span><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span></span>. To do so we randomly select <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span></span> <span><span><math><semantics><mrow><mo>(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span></span> pairs from <span><span><math><semantics><mrow><mrow><mi mathvariant="bold">S</mi></mrow></mrow><annotation encoding="application/x-tex">\bold{S}</annotation></semantics></math></span></span>, thus collecting two <strong>synchronization representations</strong>, <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out} \in~\mathbb{R}^{D_\text{out}}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup><mo>∈</mo><mtext>&nbsp;</mtext><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action} \in~\mathbb{R}^{D_\text{action}}</annotation></semantics></math></span></span>. <span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{out}</annotation></semantics></math></span></span> can then be projected to an output space as:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext><mi>t</mi></msubsup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bold{y}^t = \bold{W}_{\text{out}} \cdot \bold{S}^t_\text{out}.
</annotation></semantics></math></span></span></span></p>

<div>
  <h4>Synchronization enables a very large representation.</h4>
  <p>
      As the model width, D, grows, the synchronization representation grows with
      \(\frac{D \times (D+1)}{2}\), offering opportunities for improved expressiveness without the need for more parameters in order to project a latent space to this size.
    </p>
</div>

<h4 id="from-data">Modulating input data</h4>
<p><span><span><math><semantics><mrow><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{S}^t_\text{action}</annotation></semantics></math></span></span> can be used to take actions in the world (e.g., via attention as is in our setup):</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo>=</mo><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub><mo>⋅</mo><msubsup><mrow><mi mathvariant="bold">S</mi></mrow><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">\bold{q}^t = \bold{W}_{\text{in}} \cdot \bold{S}^t_\text{action}
</annotation></semantics></math></span></span></span></p>
<p>where <span><span><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{out}}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mrow><mi mathvariant="bold">W</mi></mrow><mrow><mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\bold{W}_{\text{in}}</annotation></semantics></math></span></span> are learned weight matrices that project synchronization into vectors for observation (e.g., attention queries, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{q}^t</annotation></semantics></math></span></span>) or outputs (e.g., logits, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t</annotation></semantics></math></span></span>). Even though there are <span><span><math><semantics><mrow><mo>(</mo><mi>D</mi><mo>×</mo><mo>(</mo><mi>D</mi><mo>+</mo><mn>1</mn><mo>)</mo><mo>)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">(D \times (D+1))/2</annotation></semantics></math></span></span> unique pairings in <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">S</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\bold{S}^t</annotation></semantics></math></span></span>, <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{out}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msub><mi>D</mi><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext></msub></mrow><annotation encoding="application/x-tex">D_\text{action}</annotation></semantics></math></span></span> can be orders of magnitude smaller than this. That said, the full synchronization matrix is a large representation that has high future potential.</p>
<p>In most of our experiments we used standard cross attention <dt-cite key="vaswani2017attention"></dt-cite>:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">A</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mi>Q</mi><mo>=</mo><msup><mrow><mi mathvariant="bold">q</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><mi>K</mi><mi>V</mi><mo>=</mo><mtext><mi mathvariant="normal">F</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mtext><mo>(</mo><mtext><mi mathvariant="normal">d</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi></mtext><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\bold{o}^t = \text{Attention}(Q=\bold{q}^t, KV=\text{FeatureExtractor}(\text{data}))
</annotation></semantics></math></span></span></span></p>
<p>where a 'FeatureExtractor' model, e.g., a ResNet <dt-cite key="he2016deep"></dt-cite>, is first used to build useful local features for the keys and values. <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">o</mi></mrow><mrow><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{o}^{t}</annotation></semantics></math></span></span> is concatenated with <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">z</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{z}^{t+1}</annotation></semantics></math></span></span> for the next cycle of recurrence.</p>
<h3 id="loss-function">Loss function: optimizing across internal ticks</h3>
<p>The CTM produces outputs at each internal tick, <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>. A key question arises: how do we optimize the model across this internal temporal dimension?  Let <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\bold{y}^t \in \mathbb{R}^{C}</annotation></semantics></math></span></span> be the prediction vector (e.g., probabilities of classes) at internal tick <span><span><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, where <span><span><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> is the number of classes.  Let <span><span><math><semantics><mrow><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{true}</annotation></semantics></math></span></span> be the ground truth target. We can compute a loss at each internal tick using a standard loss function, such as cross-entropy:</p>
<p><span><span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup><mo>=</mo><mtext><mi mathvariant="normal">C</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">y</mi></mtext><mo>(</mo><msup><mrow><mi mathvariant="bold">y</mi></mrow><mi>t</mi></msup><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub><mo>)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">    \mathcal{L}^t = \text{CrossEntropy}(\bold{y}^t, y_{true}),
</annotation></semantics></math></span></span></span></p>
<p>and a corresponding certainty measure, <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span></span>. We compute certainty simply as 1 - normalised entropy. We compute <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{L}^t</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><msup><mrow><mi mathvariant="script">C</mi></mrow><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}^t</annotation></semantics></math></span></span> for all <span><span><math><semantics><mrow><mi>t</mi><mo>∈</mo><mo>{</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>T</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">t \in \{1, \ldots, T\}</annotation></semantics></math></span></span>, yielding losses and certainties per internal tick, <span><span><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{L} \in \mathbb{R}^{T}</annotation></semantics></math></span></span> and <span><span><math><semantics><mrow><mrow><mi mathvariant="script">C</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{C} \in \mathbb{R}^{T}</annotation></semantics></math></span></span>.</p>
<p>A natural question arises: how should we reduce <span><span><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span></span> into a scalar loss for learning? Our loss function is designed to optimize CTM performance across the internal thought dimension. Instead of relying on a single step (e.g., the last step), which can incentivize the model to only output at that specific step, we dynamically aggregate information from two internal ticks: the point of minimum loss and the point of maximum certainty:</p>
<ul>
<li>the point of minimum loss: <span><span><math><semantics><mrow><msub><mi>t</mi><mn>1</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mtext><mo>(</mo><mrow><mi mathvariant="script">L</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_1=\text{argmin}(\mathcal{L})</annotation></semantics></math></span></span>; and</li>
<li>the point of maximum certainty: <span><span><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub><mo>=</mo><mtext><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mtext><mo>(</mo><mrow><mrow><mi mathvariant="script">C</mi></mrow></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">t_2=\text{argmax}({\mathcal{C}})</annotation></semantics></math></span></span>.</li>
</ul>
<p>This approach is advantageous because it means that the CTM can perform meaningful computations across multiple internal ticks, naturally facilitates a curriculum effect, and enables the CTM to tailor computation based on problem difficulty. The final loss is computed as:</p>
<p><span><span><span><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>1</mn></msub></mrow></msup><mo>+</mo><msup><mrow><mi mathvariant="script">L</mi></mrow><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">    L = \frac{\mathcal{L}^{t_1} + \mathcal{L}^{t_2}}{2}.
</annotation></semantics></math></span></span></span></p>
<div>
  <h4>More information in our Technical Report.</h4>
  <div>
    <p>Please take a look at our <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> for more information.</p>
    <p>Specifically, it includes additional information on how we enable the CTM to learn short versus long time dependency.</p>
  </div>
</div>
<hr>
<h2>Experiment: ImageNet</h2>
<div id="imagenet-experiment"> 
        <div id="imagenet-demos">
            <h3>Demonstrations</h3>
            
            
            <figcaption>
                <span><b>Fig 4.</b> Thinking about Images</span>: Top left is the average attention weighting (of the 16 heads shown) when the CTM observes the image on the right. Class predictions are shown on the bottom left and the certainty is shown on the bottom right (<span>green</span> denotes a correct prediction). The small images at the bottom are buttons to load other examples, showing a diversity of certainties and correctness.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            
            <div>
                <p>This is a subset of results from our ImageNet experiments (see the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a> for more). Crucially, the CTM enables <b>Adaptive Compute</b> where the internal steps, (<i>how much thought the CTM is putting into the problem</i>) can be cut short. These figures show what can be expected in terms of accuracy when cutting thinking short. Only marginal gains are had past a certain point, but gains nonetheless.</p>
                <p> Fig 4. shows where the CTM looks as it reasons about the data. We show the <b>Attention Weights</b> for all 16 heads and demark where the model is looking for each (and on average at the top). The predictions are shown on the bottom left and certainty over time on the bottom right. Fig 6. shows a visualization of <b>Neural Activity</b> as the CTM thinks about a single image: note the multi-scale structure and how activity seems to 'flow'.
            </p></div>
            
            <figcaption>
                <span><b>Fig 6.</b> Neural activity</span>: visualised in 2D using a <a href="https://umap-learn.readthedocs.io/en/latest/" target="_blank">UMAP</a> projection. Each neuron is shown as an individual dot, scaling in size with absolute magnitude, and color with value (<span>blue</span> for negative, <span>red</span> for positive). We show similar visualizations inside later demonstrations.
            </figcaption>
        </div>
        <div>
            <h3>Discussion</h3>
            <p>We never set out to train a model that achieved some remarkable new state-of-the-art performance on ImageNet. AI researchers already expect high performance on ImageNet after over a decade of research that uses it. Instead, we wanted to show just how different and interesting <b>the CTM's interaction with data</b> can be. The videos on the left/above demonstrate the thought process the CTM undertakes and the figures show its benefits.
            </p><p>Let's contextualize just what's going on here: the CTM is looking around these images, all the while building up its prediction, all by using the <b>synchronization of neural activity</b> directly as a representation. The <a href="#neural-dynamics">neural dynamics</a> we showed earlier are actually examples of dynamics from a CTM observing ImageNet! The paths output by the CTM in <a href="#maze-demo">the maze demo</a> are akin to the class predictions made here.
            </p><h4>The missing ingredient: TIME</h4>
            <p><b>Biological intelligence is still superior to AI in many cases</b> <dt-cite key="chollet2024arc,phan2025humanitysexamshort,lake2017building,ren2024brain"></dt-cite>. Biological brains solve tasks very differently to conventional neural networks, which might explain why this is the case. It might be that <a href="https://www.thetransmitter.org/neuroai/the-brain-holds-no-exclusive-rights-on-how-to-create-intelligence/" target="_blank">biological intelligence pays heed to time</a> in ways that modern AI simply does not. In this work, we aimed to develop a model that approaches problem-solving in a manner more aligned with biological brains, emphasizing the central role of the precise timing and interplay of neural dynamics. The interpretable and intuitive outcome we point at in the video demonstrations is very exciting as it suggests that the CTM is indeed leveraging time to its advantage, in order to reason about data.</p>
            <p>The details on model hyper-parameters can be found in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>.</p>
        </div>
    </div>

<h2>Experiment: Solving 2D Mazes - doing it the hard way</h2>
<div id="maze-experiment"> 
        <div>
            <h3>The why and the how</h3>
            <p>
            Solving mazes is a challenging task for machines <dt-cite key="zhang2025t,schwarzschild2021can,bansal2022end"></dt-cite>, where only the current <a href="https://openai.com/index/thinking-with-images/" target="_blank">bleeding edge models perform well</a> on fairly simple <a href="https://featurecrew.io/tests/maze" target="_blank">mazes</a>. Even so, existing methods either require careful design of the data/objective (e.g., outputs are images instead of a <i>solution</i>), or extensive tool use (e.g., LLMs that perform well at this), indicating that the underlying <b>intelligent reasoning</b> required to solve a maze, step-by-step, is not evidenced by these approaches.
            </p>
            <p>
            We trained n CTM on a new setup, requiring it to directly predict a path (truncated for simplicity) from start to finish in the form of steps: <b>L</b>eft, <b>R</b>ight, <b>U</b>p, <b>D</b>own, or <b>W</b>ait. A small version of the resultant model can be explored in the <a href="#maze-demo">interactive demo at the top of this page</a>. We show a demonstration of larger model here. Remarkably, the attention pattern is intuitive and follows the solution, all while using neural synchronization as a representation. It even generalizes beyond the truncated path! See the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 7.</b> Thinking about mazes</span>: each animation segment shows 75 internal ticks of the CTM when it is provided with the input maze. We show the route as it is constructed through the internal 'thought process', showing only the valid route (i.e., ignoring predictions through walls; see the associated toggle on the <a href="#maze-demo">demo</a>). 16 attention heads' weights are shown at the bottom and the average is overlayed on the maze to show where the CTM is focusing. We 'teleport' the CTM to its resultant predicted location until it lands on the target and then load a new maze.
            </figcaption>
        </div>
        <div>
            <h3 id="maze-results">Results</h3>
            
            <h3>Generalization</h3>
            <p>Each video below shows how well the CTM generalizes to bigger and more complex mazes, while retaining its reasoning prowess. To generate these we used a CTM trained to solve a path up to length 100 on 39 x 39 mazes, but the mazes shown here are of size 99 x 99 and the full paths are roughly 6x as long.</p>
            
        </div>
        <div>
            <h3>Discussion</h3>
            <p>Why run these experiments? We know that neural networks can be tailored to solve 2D mazes if we present the data in the "right" way. But, when presented in a fashion that requires a clear process through which the model must progress, existing methods fall short. Even current SoTA LLMs rely on tool use, which is impressive in its own right, but somewhat unsatisfying: an intelligent machine should be demonstrably intelligent, and humans don't require tools to solve these mazes. </p>
            <p>We set out to show that the CTM has the capacity to learn when complex reasoning is required, unlike the most comparable baseline methods. We also show how the CTM generalizes to larger and more complex mazes, indicating that its internal reasoning is not merely memorization, but rather a more natural and correct way to solve the underlying maze problem. Importantly, we made no specific structural changes to the model compared to the <a href="#imagenet-experiment">CTM we trained for ImageNet</a>; the only meaningful structural change was to output the solution as a 2D class space, applying cross entropy for each step.
            </p><h4>A World Model</h4>
            <p>We chose our setup carefully: (1) we used <b>no positional embedding</b> for attention; and (2) we required that the models predict the routes directly as a string of classes (e.g., go left, left, right, up, etc.). By forgoing positional embedding the CTM must build an <b>internal world model</b> in order to query the data and navigate the maze. The fact that it does so in such a convincing fashion is remarkable. </p>
            <h4>Where to go from here?</h4>
            <p>We have some strong evidence that the CTM is capable of solving challenging problems, and it does so in intuitive and interesting ways. The fact that it can solve mazes by building an internal world model "on the fly" without any positional embedding opens up avenues for future research. For instance, we would like to see how the CTM finds its way around more complex environments (e.g., games or videos) without any explicit positional encodings.</p>
        </div>
    </div>
<h2>Experiment: Parity</h2>
<div id="parity-experiment"> 
        <div>
            <h3>Sequential data, non-sequentially</h3>
            <p>
            The parity of a binary sequence, given by the sign of the product of its elements, can reasonably be predicted by an RNN when the data is fed sequentially - the model need only maintain an internal state, flipping a 'switch' whenever a negative number if encountered. When the entire sequence is provided at once, however, the task is significantly more challenging<dt-cite key="graves2016adaptive"></dt-cite>.
            </p>
            <p>
            We trained CTMs to solve a variant of this parity task: the model is input with a 64-length binary vector, and must predict the <i>cumulative</i> parity at each of the 64 positions.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 9.</b> Determining the cumulative parity of a sequence</span>: shown are the movements of the attention weights from each of the 8 heads. Overlayed on the input sequences is the trajectory of the attention weight argmax. The larger sequences depict the models predictions and targets.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            
                <p>
                We compare the accuracy of CTMs trained with different numbers of internal ticks to parameter matched LSTMs. We found that CTMs with over 75 internal ticks could reliably solve this task, with some runs achieving 100% accuracy. The LSTMs, on the other hand, struggled to learn with over 10 internal ticks, suggesting that LSTMs are not well suited to unfolding an internal thought dimension.
                </p>
                <p>
                The left/above demonstration shows the solving process of the CTM: the movement of the attention weights, as well as their argmax overlayed on the inputs, the models predictions, the target and the neuron activations. Notice how the attention moves <b>backwards</b> through the data and determines the solution after observing the entire input. Some attention heads display interpretable behavior, such as the first attention head which attends to only negative parity positions (\(\blacksquare\)).
                </p>
            </div>
        <div>
   <h3>Learning sequential algorithms</h3>
            <p>We visualise the learned algorithms by plotting the accuracy (top) and attention weights (bottom) over the 75 internal ticks for each position in the 64-length sequence, at different points during training. One model (left) attends to the data in reverse order before predicting the cumulative parity at once; the other attends forward, predicting parity incrementally. Both achieve perfect accuracy.
            </p>
            <p>
            The ability of the CTM to search through the data in reverse order, suggests that the CTM is carrying out some form of planning, building up its understanding of the data before making a final decision -- the CTM is capable of forming and following a strategy.
            </p>
            <p>
                <figure>
                    <video src="https://pub.sakana.ai/ctm/assets/mp4/parity/run1.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
                    <figcaption>
                        <span><b>Fig 11a.</b> 75-Internal Tick CTM 1</span>: learns to attend to the data in reverse order, predicting the parity at the end of the reasoning process.
                    </figcaption>
                </figure>
                <figure>
                    <video src="https://pub.sakana.ai/ctm/assets/mp4/parity/run3.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
                        <figcaption>
                        <span><b>Fig 11b.</b> 75-Internal Tick CTM 2</span>: learns to attend from beginning to end, and with it, increasing its certainty in each prediction.
                    </figcaption>
                </figure>
            </p>
        </div>
    </div>
<h2>Experiment: Q&amp;A MNIST</h2>
<div id="parity-experiment"> 
        <div>
            <h3>Memory via Synchronization</h3>
            <p>
            To assess the CTM’s ability to memorise and recall information, we design a Question and Answering (Q&amp;A) MNIST task. In this task, the model first observes a sequence of MNIST digits, followed by a series of interleaved index and operator embeddings that specify which digits should be recalled and which modular operation should be applied. Once all digits and index/operator embeddings have been presented, a zero-tensor flag signals the model to produce its final answer. An example is shown below.
            </p>
            <figure>
                <img src="https://pub.sakana.ai/ctm/assets/png/qamnist/qamnist_example.svg" alt="Accuracy during training">
                <figcaption>
                    <span><b>Fig 12.</b> Q&amp;A MNIST example</span>: a typical sequence observed by the model.
                </figcaption>
            </figure>
            <p>
            In our experiments, the memory length of the CTMs is such that the MNIST digits will always lie outside of the activation history window used by the neuron-level models. In this way, the CTM must organize its activations such that it can recall digits are later timesteps.
            </p>
            <h3>Demonstration</h3>
            
            <figcaption>
                <span><b>Fig 13.</b>Observing digits and answering questions</span>: the model is shown MNIST digits followed by operator and index embeddings which specifies the modular operation at the top. Shown also is the attention weights for the digits and the models predictions.
            </figcaption>
        </div>
        <div>
            <h3>Results</h3>
            <div>
                <figure>
                    <img src="https://pub.sakana.ai/ctm/assets/png/qamnist/accuracy_comparison.svg" alt="Accuracy during training">
                    <figcaption>
                        <span><b>Fig 14.</b> Accuracy during training</span>: for both CTMs and LSTMs trained with 1 internal tick per input and 10 internal ticks per input.
                    </figcaption>
                </figure>
                </div>
                <p>
                Our results show that, while the LSTM outperforms the CTM when only a single internal tick is used to process each input, the LSTM becomes more unstable when more internal ticks are used. The CTM, on the other hand, exhibits stronger performance with increasing internal ticks, achieving over 95% accuracy in the most challenging in-distribution task. 
                </p>
                <p>
                Furthermore, we highlight the ability of the CTM to recall digit values observed many timesteps in the past, arising purely from the organization and synchronization of neurons. This strong performance suggests that processing timing information through the synchronization of neuron activations may be a powerful mechanism for memorization and recall.
                </p>
            </div>
        <div>
            <h3>Generalization</h3>
            <p>
            We examine the generalization capabilities of the CTM by measuring the accuracy of the model when input with more digits or index-operator embeddings than observed during training, depicted below, with the training regime marked in red. We find that both the CTM and the LSTM baseline can generalize to an increased number of operations. Empirically, we find that this generalization arises from the model’s approach to solving the task: each time a new index embedding is presented, the model computes and stores the result of the specified operation, regardless of whether the answer flag has been given. This enables it to continue processing a stream of index and operator embeddings without needing to wait for a final signal.
            </p>
            
            <figcaption>
                <span><b>Fig 15.</b> Generalization:</span> accuracy of the CTM and LSTM for different numbers of input digits and operations. The red line indicates the training regime. For the CTM, performance scales with the number of internal ticks, while the converse is true for the LSTM.
            </figcaption>
        </div>
        </div> 

<hr>
<h2>Additional experiments</h2>
<h3>CTM versus humans</h3>
<p>In this section we test the CTM using CIFAR-10, comparing it to human performance, a feed-forward baseline, and an LSTM baseline. The purpose of this experiment was to contextualize the performance of the CTM alongside a standard feed-forward baseline, an LSTM baseline that also uses internal ticks for reasoning (potentially), and humans. We used a restricted backbone to highlight the differences between models (details in the <a href="https://arxiv.org/abs/2505.05522" target="_blank">Technical Report</a>).</p>
<p>We used two datasets of human labels for CIFAR-10; we call these CIFAR-10D <dt-cite key="ho2018cifar10"></dt-cite> owing to its calibration of difficulty levels, and CIFAR-10H <dt-cite key="peterson2019human"></dt-cite> originally used to quantify human uncertainty. CIFAR-10D can be found at <a href="https://sites.google.com/site/hophuoctien/projects/virec/cifar10-classification" target="_blank">here</a> and CIFAR-10H can be found <a href="https://github.com/jcpeterson/cifar-10h" target="_blank">here</a>.</p>

<p>For the human calibration we used the probabilities provided in CIFAR-10H, which were computed using guesses from multiple humans using the available human datasets. We computed calibration (Fig 16b.) as we did for ImageNet: we compute the predictive probability as the average probability for the chosen class over all internal ticks (for both CTM and LSTM). The CTM demonstrates the best calibration, even when compared to humans.</p>

<p>Fig 17. shows the neural activities for the CTM and the LSTM baseline. The CTM yields rich, diverse, and complex dynamics with multiple interesting features, including periodic behavior (there is <b>no periodic driving function</b>). The distinct difference between the CTM and LSTM neural activities is evidence that the two novel elements of the CTM (<a href="#neuron-level-models">neuron-level models</a> and <a href="#synchronization-representation">synchronization as a representation</a>) enable neural dynamics as a fundamental computational mechanic.</p>
<h3>CIFAR-100, ablation studies</h3>
<p>Fig 18. shows what happens when we vary the number of neurons (i.e., the model width) while keeping all else constant, including the training time. As with other models, a wider network could evidently benefit from a longer training time or different training hyper-parameters, hence the reduction in accuracy in Fig 18a. For Fig 18b. and Fig 18c. we set out to understand how unique the <a href="#neuron-level-models">Neuron-level models</a> tend to be, and that was related to the model width, as measured by the cosine similarity between the dynamics of different neurons. Fig 18b. shows that with a wider model (i.e., more neurons), we see more diversity instead of less. One might expect that with more neurons there is less 'space' for diversity, but we observed the opposite.</p>

<p>Fig 19. shows the relationship between predictions and the number of internal ticks used by the CTM. We trained several CTMs (again keeping all other variables constant). In Fig 19b. we plot the distributions of the data over which steps the CTM is most certain (i.e., <span><span><math><semantics><mrow><msub><mi>t</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">t_2</annotation></semantics></math></span></span> in <a href="#loss-function">the loss function</a>). What this shows is that the CTM uses a wide range of steps to become most certain about the data it observes. For each setup (25, 50 and 100 internal ticks), there are two concentrated areas in the distributions, indicating that the CTM is following separate internal processes depending on the data.</p>

<h3>Sorting real numbers</h3>
<p>For these experiments we trained a CTM to sort 30 real numbers from <span><span><math><semantics><mrow><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msub><mrow><mi>I</mi></mrow><mrow><mn>3</mn><mn>0</mn></mrow></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0, {I}_{30})</annotation></semantics></math></span></span>. The purpose of this experiment was twofold: (1) to understand if and when the CTM applies more or less compute in a controlled environment; and (2) see if we can train the CTM to output a sequence in sequential order using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html">CTC loss</a>. This CTM could sort a length 30 list of real numbers approximately 80% of the time.</p>

<h3>Reinforcement Learning</h3>
<p>We have shown that the CTM can process non-sequential data via an continuous thought dimension. Here, we extend the CTM to tasks involving interation with an external environment, training CTMs with proximal policy optimization<dt-cite key="schulman2017proximal"></dt-cite> to solve a navigation task and partially observable variants of CartPole and Acrobot<dt-cite key="MinigridMiniworld23,towers2024gymnasium"></dt-cite>. In this setting, the CTM receives an observation, process it using a fixed number of internal thought steps, and outputs the next action. The history of activations is continuous across environment steps, such that activations from past environment steps can affect the present decision making process.</p>
<div>
  <figure>
    <video src="https://pub.sakana.ai/ctm/assets/mp4/rl/activations.mp4" type="video/mp4" autoplay="" muted="" playsinline="" loop=""></video>
    <figcaption>
    <span><b>Fig 21a.</b> CTM solving the MiniGrid Four Rooms task</span>: evidencing that the CTM can use a leverage a continuous history of activations to interact with the world.
    </figcaption>
  </figure>
  <figure>
    <img src="https://pub.sakana.ai/ctm/assets/png/rl/episode_lengths_avg.png" alt="CTM Training Curves of MiniGrid Four Rooms">
    <figcaption>
    <span><b>Fig 21b.</b>Training curves</span>: for this navigation task (episode length during training). Although the LSTM learns slightly faster, both solve the task and converge to the same average episode length.
    </figcaption>
  </figure>
</div>
<p>Although our results show that the CTM achieves a comparable performance to the LSTM baseline, the central goal of this section is provide evidence that the CTM can learn in a continuous environment.</p>
<hr>
<h2>Conclusion</h2>
<p>The Continuous Thought Machine (CTM) represents a novel step towards bridging computational efficiency with biological plausibility in artificial intelligence. By moving beyond traditional pointwise activation functions to private neuron-level models, the CTM cultivates far richer neuron dynamics. Crucially, it leverages neural synchronization as a powerful and fundamentally new type of representation - distinct from the activation vectors prevalent since the early days of neural networks. This direct use of neuron dynamics as a first-class representational citizen allows the CTM to exhibit behaviors qualitatively different from contemporary models.</p>
<p>Our research demonstrates the tangible benefits of this approach. The CTM can dynamically build representations over time for tasks like image classification, form rich internal maps to attend to specific input data without positional embeddings, and naturally exhibit adaptive computation. Furthermore, it learns to synchronize neural dynamics to store and retrieve memories beyond its immediate activation history. This internal processing also lends itself to greater interpretability, as seen in its methodical solving of mazes and parity tasks.</p>
<p>Remarkably, the core CTM architecture remained largely consistent across a diverse range of challenging tasks, requiring only input/output module adjustments. This versatility and trainability were particularly evident in complex scenarios like maze navigation. The CTM succeeded with minimal tuning, where a traditional model like the LSTMs still struggled even after significant tuning efforts.</p>
<p>This work underscores a vital, yet often underexplored, synergy between neuroscience and machine learning. While modern AI is ostensibly brain-inspired, the two fields often operate in surprising isolation. The CTM serves as a testament to the power of drawing inspiration from biological principles. By starting with such inspiration and iteratively following the emergent, interesting behaviors, we developed a model with unexpected capabilities, such as its surprisingly strong calibration in classification tasks, a feature that was not explicitly designed for.</p>
<p>It is crucial to note that our approach advocates for borrowing concepts from biology rather than insisting on strict, literal plausibility; real neurons may not access their activation history as modeled in the CTM, yet emergent phenomena like traveling waves still manifest. This nuanced balance between practicality and biological inspiration opens a landscape of new research directions, which may hold the key to unlocking capabilities currently missing in AI, potentially leading to systems that exhibit more human-like intelligence and address its current limitations.</p>
<p>When we initially asked, "why do this research?", we hoped the journey of the CTM would provide compelling answers. By embracing light biological inspiration and pursuing the novel behaviors observed, we have arrived at a model with emergent capabilities that exceeded our initial designs. We are committed to continuing this exploration, borrowing further concepts to discover what new and exciting behaviors will emerge, pushing the boundaries of what AI can achieve.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intellect-2 Release: The First 32B Model Trained Through Globally Distributed RL (170 pts)]]></title>
            <link>https://www.primeintellect.ai/blog/intellect-2-release</link>
            <guid>43958898</guid>
            <pubDate>Mon, 12 May 2025 01:46:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.primeintellect.ai/blog/intellect-2-release">https://www.primeintellect.ai/blog/intellect-2-release</a>, See on <a href="https://news.ycombinator.com/item?id=43958898">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="sectionFixed"><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png" loading="lazy" alt="" sizes="100vw" srcset="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-500.png 500w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-800.png 800w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1080.png 1080w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-1600.png 1600w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report-p-2000.png 2000w, https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682104ffd9b893407813e24e_intellect-2-technical-report.png 2160w"></p></div><div data-w-id="e5e1e688-1022-43bb-86ee-2597b4f74f82"><div id="blogRichText" fs-toc-element="contents" fs-toc-offsettop="8rem" fs-richtext-element="rich-text"><p>We're excited to release INTELLECT-2, the first 32B parameter model trained via globally distributed reinforcement learning. &nbsp;Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning language model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.</p><p>To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.</p><p>Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B.</p><p>We open-source INTELLECT-2 along with our code and data, hoping to enable more open research in the field of decentralized training</p><ul role="list"><li><strong>Detailed Technical Report: </strong><a href="http://primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></li><li><a href="https://huggingface.co/collections/PrimeIntellect/intellect-2-68205b03343a82eabc802dc2"><strong>INTELLECT-2 on Hugging Face</strong></a><ul role="list"><li>Chat Interface to try it out: <a href="http://chat.primeintellect.ai/">chat.primeintellect.ai</a></li></ul></li><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>prime-rl: – our async RL framework</strong></a></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/68210770ed19e45df800ade8_Screenshot%202025-03-25%20at%2017.46.32%20(2).png" loading="lazy" alt=""></p></figure><h2><strong>Paradigm Shift for Decentralized Training</strong></h2><p>Test-time compute scaling with reinforcement learning has emerged as a new scaling axis for large language models (LLMs), enabling improvements by allowing models to spend more time reasoning.</p><p>However, reinforcement learning training is typically centralized, requiring large clusters of co-located GPUs and fast interconnect speeds. With INTELLECT-2, we showcase a paradigm shift: reinforcement learning is inherently more asynchronous and well suited for decentralized, globally distributed compute.</p><h2>Training Infrastructure</h2><p>We introduce the following key open-source infrastructure components for training INTELLECT-2:</p><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/prime-rl"><strong>PRIME-RL</strong>:</a><ul role="list"><li>Fully asynchronous reinforcement learning framework designed for decentralized training. Decouples rollout generation, model training, and weight broadcasting. It enables training across heterogeneous, unreliable networks.</li><li>The trainer implementation uses PyTorch FSDP2, inference uses vLLM and the verifiers use the <a href="https://github.com/PrimeIntellect-ai/genesys">GENESYS</a> schema introduced in <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li></ul></li><li><strong>SHARDCAST</strong>: A library for distributing large files via a HTTP-based tree-topology network that efficiently propagates updated model weights to the decentralized inference workers.</li></ul><p>‍</p><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/67fc98b6b2ae65e17cf41926_Blog%20Asset.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>TOPLOC</strong>:<ul role="list"><li>A locality-sensitive hashing scheme for efficient verifiable inference. It detects tampering or precision changes in model inference and works reliably across nondeterministic GPU hardware.</li><li>Inference workers generate the rollouts, these rollout files are uploaded via signed URLs, an on-chain event triggers TOPLOC validators to check them; accepted files feed the trainer, while invalid ones slash and remove the submitting node from the pool.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107b3b1d00e49e0e1d232_toploc-validator.png" loading="lazy" alt=""></p></figure><ul role="list"><li><a href="https://github.com/PrimeIntellect-ai/pi-protocol"><strong>Protocol Testnet</strong>:</a> Provides the infrastructure to aggregate and coordinate global computeresources.<ul role="list"><li>Rust-based orchestrator and discovery service coordinate permissionless workers—nodes auto-register with hardware checks, heartbeats, and <em>pull</em> Docker-container tasks while the orchestrator schedules workloads, tracks health, and records pool ownership and contributions.</li></ul></li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107d311028667c6db24ad_image%20(1).png" loading="lazy" alt=""></p></figure><h2>Training Recipe</h2><ul role="list"><li><strong>Training Data &amp; Rewards:</strong><ul role="list"><li>285k verifiable tasks (math &amp; coding) from <strong>NuminaMath-1.5, Deepscaler</strong>, and <a href="https://www.primeintellect.ai/blog/synthetic-1-release">SYNTHETIC-1</a>.</li><li>Binary task reward + length reward lets users budget thinking tokens at inference time.</li></ul></li><li><strong>Two-step asynchronous RL:</strong> The broadcast of new policy weights is fully overlapped with ongoing inference and training—eliminating the communication bottleneck</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682107ef11028667c6db337a_async-rl-schedule.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Two-Sided GRPO Clipping:</strong> Stabilizes training by mitigating gradient spikes with two-sided token probability ratio clipping.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821080742d40ea0c31cfc42_two-sided-clipping.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Advanced Data Filtering:</strong> Combines offline and online filtering to select challenging tasks, significantly enhancing model learning efficiency.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/6821086978179d3909d2a696_data-filtering.png" loading="lazy" alt=""></p></figure><ul role="list"><li><strong>Aggressive Gradient Clipping:</strong> Addresses escalating gradient norms at scale, providing improved training stability.</li></ul><h2>Experiments</h2><p>We report results from two main experiments: TARGET-SHORT, an experimental run with short target lengths to train an efficient reasoning model, and, TARGET-LONG, our main run with longer target lengths.</p><ul role="list"><li><strong>Compute Utilization:</strong> During the two main experiments, we successfully overlapped communication with computation through two-step asynchronous reinforcement learning.</li><li><strong>Reward Trajectories:</strong><ul role="list"><li>Throughout training, we saw significant improvements of our task rewards, indicating that the model improved its performance on our mathematics and coding problems. We also saw a reduction of length penalties, but a much slower one than during our ablation experiments</li></ul></li></ul><p>‍</p><ul role="list"><li><strong>Benchmark Performance:</strong> We were able to increase the performance of QwQ-32B on mathematics and coding benchmarks.</li></ul><figure><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c7e/682108892e5b8c350cb40532_benchmarks.png" loading="lazy" alt=""></p></figure><ul role="list"><li>Overall, as QwQ-32B was already extensively trained with RL, it was difficult to obtain huge amounts of generalized improvement on benchmarks beyond our improvements on the training dataset. To see stronger improvements, it is likely that better base models such as the now available Qwen3, or higher quality datasets and RL environments are needed.</li></ul><h2>Future Work</h2><p>INTELLECT-2 is a first step towards open frontier reasoning models trained in a decentralized fashion. Over the coming months, we’ll work on:</p><ul role="list"><li><strong>Increasing the Ratio of Inference to Training Compute</strong><ul role="list"><li>Inference is embarrassingly parallel and communication-free, so more complex RL environments that devote <em>more</em> FLOPs to inference are a natural fit for decentralized training.</li></ul></li><li><strong>Tool Calls &amp; Multi-Turn RL</strong><ul role="list"><li>To fully leverage inference-time compute for scientific and research progress, we need to give models built-in tools in its reasoning chain—web search, Python interpreters, etc.</li></ul></li><li><strong>Crowdsourcing RL Tasks &amp; Environments</strong><ul role="list"><li>We believe open-source has a unique advantage here. Distributed RL is still in its early days, and with the right community and contributions, open-source AI can outpace the closed labs.</li></ul></li><li><strong>Model Merging &amp; DiLoCo</strong><ul role="list"><li>Fuse independently trained RL models—either at the end or continuously via DiLoCo—to create a single unified model, scaling decentralized RL to a lot more compute.</li></ul></li></ul><p>Check out the technical reports for more details: <a href="https://www.primeintellect.ai/intellect-2">primeintellect.ai/intellect-2</a></p><h2><strong>Join Us</strong></h2><p>INTELLECT-2 demonstrates that globally decentralized RL works. Now, we’re focusing on tool-assisted reasoning, crowdsourcing higher-quality data, and optimizing our infrastructure and training recipe to build frontier open models.</p><p>Join us if you want to build open source and decentralized AGI with us.</p></div><div data="chatbox"><div><p>Eurus-2-PRIME</p><div><div><p>When tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process. [ASSESS] [ADVANCE] [VERIFY] [SIMPLIFY] [SYNTHESIZE] [PIVOT] [OUTPUT] You should strictly follow the format below: [ACTION NAME] # Your action step 1 # Your action step 2 # Your action step 3 ... Next action: [NEXT ACTION NAME]</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><p>

[ASSESS]</p><p>We are given two quadratic polynomials $P(x)$ and $Q(x)$ with leading coefficients $2$ and $-2$, respectively. The graphs of both polynomials pass through the points $(16,54)$ and $(20,53)$. We need to find the value of $P(0) + Q(0)$.</p><p>[ADVANCE]</p><p>Let's express the quadratic polynomials $P(x)$ and $Q(x)$ in the general form:<br>  
$ P(x) = 2x^2 + bx + c $<br>  
$ Q(x) = -2x^2 + dx + e $</p><p>Since the graphs of both polynomials pass through the points $(16,54)$ and $(20,53)$, we can substitute these points into the equations of $P(x)$ and $Q(x)$ to form a system of equations.</p><p>For $P(x)$:<br>  
$ P(16) = 2(16)^2 + 16b + c = 54 $<br>  
$ 512 + 16b + c = 54 $<br>  
$ 16b + c = -458 \quad \text{(Equation 1)} $</p><p>  
$ P(20) = 2(20)^2 + 20b + c = 53 $<br>  
$ 800 + 20b + c = 53 $<br>  
$ 20b + c = -747 \quad \text{(Equation 2)} $</p><p>For $Q(x)$:<br>  
$ Q(16) = -2(16)^2 + 16d + e = 54 $<br>  
$ -512 + 16d + e = 54 $<br>  
$ 16d + e = 566 \quad \text{(Equation 3)} $</p><p>  
$ Q(20) = -2(20)^2 + 20d + e = 53 $<br>  
$ -800 + 20d + e = 53 $<br>  
$ 20d + e = 853 \quad \text{(Equation 4)} $</p><p>[ADVANCE]</p><p>Now, we can solve the system of equations for $P(x)$:<br>Subtract Equation 1 from Equation 2:<br>  
$ (20b + c) - (16b + c) = -747 - (-458) $<br>  
$ 4b = -289 $<br>  
$ b = -\frac{289}{4} $</p><p>Substitute $b = -\frac{289}{4}$ into Equation 1:<br>  
$ 16\left(-\frac{289}{4}\right) + c = -458 $<br>  
$ -1156 + c = -458 $<br>  
$ c = 698 $</p><p>So, for $P(x)$, we have:<br>  
$ P(x) = 2x^2 - \frac{289}{4}x + 698 $<br>  
$ P(0) = 698 $</p><p>Next, we can solve the system of equations for $Q(x)$:<br>Subtract Equation 3 from Equation 4:<br>  
$ (20d + e) - (16d + e) = 853 - 566 $<br>  
$ 4d = 287 $<br>  
$ d = \frac{287}{4} $</p><p>Substitute $d = \frac{287}{4}$ into Equation 3:<br>  
$ 16\left(\frac{287}{4}\right) + e = 566 $<br>  
$ 1148 + e = 566 $<br>  
$ e = -582 $</p><p>So, for $Q(x)$, we have:<br>  
$ Q(x) = -2x^2 + \frac{287}{4}x - 582 $<br>  
$ Q(0) = -582 $</p><p>[ADVANCE]</p><p>Finally, we can find $P(0) + Q(0)$:<br>  
$ P(0) + Q(0) = 698 + (-582) = 116 $</p><p>So, the value of $P(0) + Q(0)$ is $\boxed{116}$.
</p></div></div></div><div><p>INTELLECT-MATH-7B-SFT (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""></p><p>INTELLECT-MATH-7B-SFT</p></div><div><p>
So I've got this problem here about quadratic polynomials $P(x)$ and $Q(x)$. $P(x)$ has a leading coefficient of $2$, and $Q(x)$ has a leading coefficient of $-2$. Both polynomials pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p><p>
First, since both $P(x)$ and $Q(x)$ are quadratic polynomials, I can express them in the general form:</p><p>
$P(x) = 2x^2 + bx + c$</p><p>
$Q(x) = -2x^2 + dx + e$</p><p>
Wait, actually, the leading coefficient is the coefficient of the highest power of $x$, which for a quadratic is $x^2$. So yes, $P(x) = 2x^2 + bx + c$ and $Q(x) = -2x^2 + dx + e$.</p><p>
Now, both polynomials pass through the points $(16,54)$ and $(20,53)$. That means when $x=16$, $P(16)=54$, and when $x=20$, $P(20)=53$. Similarly for $Q(x)$.</p><p>
So, I can set up equations using these points.</p><p>
First, for $P(x)$:</p><p>
$P(16) = 2(16)^2 + b16 + c = 54$</p><p>
Similarly, $P(20) = 2(20)^2 + b20 + c = 53$</p><p>
Let me calculate $16^2$ and $20^2$:</p><p>
$16^2 = 256$</p><p>
$20^2 = 400$</p><p>
So,</p><p>
$P(16): 2256 + 16b + c = 54 \Rightarrow 512 + 16b + c = 54 \Rightarrow 16b + c = 54 - 512 \Rightarrow 16b + c = -458$</p><p>
$P(20): 2400 + 20b + c = 53 \Rightarrow 800 + 20b + c = 53 \Rightarrow 20b + c = 53 - 800 \Rightarrow 20b + c = -747$</p><p>
Now, I have two equations:</p><p>
1) $16b + c = -458$</p><p>
2) $20b + c = -747$</p><p>
I can subtract equation 1 from equation 2 to eliminate $c$:</p><p>
$(20b + c) - (16b + c) = -747 - (-458)$</p><p>
$20b + c - 16b - c = -747 + 458$</p><p>
$4b = -289$</p><p>
So, $b = -289 / 4 = -72.25$</p><p>
Wait, but coefficients are usually integers, but maybe not. Let me check.</p><p>
Now, plug $b$ back into one of the equations to find $c$. Let's use equation 1:</p><p>
$16(-72.25) + c = -458$</p><p>
$-1156 + c = -458$</p><p>
$c = -458 + 1156 = 698$</p><p>
So, $P(x) = 2x^2 - 72.25x + 698$</p><p>
Hmm, $P(0)$ would be $c$, which is $698$.</p><p>
Now, let's do the same for $Q(x)$.</p><p>
$Q(16) = -2*(16)^2 + d16 + e = 54 \Rightarrow -2256 + 16d + e = 54 \Rightarrow -512 + 16d + e = 54 \Rightarrow 16d + e = 54 + 512 \Rightarrow 16d + e = 566$</p><p>
$Q(20) = -2*(20)^2 + d20 + e = 53 \Rightarrow -2400 + 20d + e = 53 \Rightarrow -800 + 20d + e = 53 \Rightarrow 20d + e = 53 + 800 \Rightarrow 20d + e = 853$</p><p>
Now, equations for $Q(x)$:</p><p>
1) $16d + e = 566$</p><p>
2) $20d + e = 853$</p><p>
Subtract equation 1 from equation 2:</p><p>
$(20d + e) - (16d + e) = 853 - 566$</p><p>
$4d = 287$</p><p>
$d = 287 / 4 = 71.75$</p><p>
Now, plug $d$ back into equation 1:</p><p>
$1671.75 + e = 566 \Rightarrow 1148 + e = 566 \Rightarrow e = 566 - 1148 = -582$</p><p>
So, $Q(x) = -2x^2 + 71.75x - 582$</p><p>
Therefore, $Q(0) = e = -582$</p><p>
Now, $P(0) + Q(0) = 698 + (-582) = 116$</p><p>
Wait, but the problem says "present the answer in LaTeX format: \boxed{Your answer}"</p><p>
So, I should write $\boxed{116}$</p><p>
But let me double-check my calculations to make sure I didn't make any mistakes.</p><p>
First, for $P(x)$:</p><p>
$P(16) = 2(256) + 16b + c = 512 + 16b + c = 54 \Rightarrow 16b + c = -458$</p><p>
$P(20) = 2*(400) + 20b + c = 800 + 20b + c = 53 \Rightarrow 20b + c = -747$</p><p>
Subtracting: $4b = -289 \Rightarrow b = -72.25$</p><p>
Then, $c = -458 - 16*(-72.25) = -458 + 1156 = 698$</p><p>
Similarly for $Q(x)$:</p><p>
$Q(16) = -2*(256) + 16d + e = -512 + 16d + e = 54 \Rightarrow 16d + e = 566$</p><p>
$Q(20) = -2*(400) + 20d + e = -800 + 20d + e = 53 \Rightarrow 20d + e = 853$</p><p>
Subtracting: $4d = 287 \Rightarrow d = 71.75$</p><p>
Then, $e = 566 - 16*71.75 = 566 - 1148 = -582$</p><p>
So, $P(0) = 698$, $Q(0) = -582$, sum is $116$.</p><p>
Seems correct.</p><p>
Final Answer</p><p>
$\boxed{116}$
</p></div></div></div></div><div><p>INTELLECT-MATH-7B (ours)</p><div><div><p>Solve the following math problem efficiently and clearly. Think carefully and step by step about your response and reason before providing a final response. Conclude your response with: \n\nTherefore, the final answer is: $\\boxed{answer}$. I hope it is correct.\n\nWhere [answer] is just the final number or expression that solves the problem.</p></div><div><p>Quadratic polynomials $P(x)$ and $Q(x)$ have leading coefficients $2$ and $-2,$ respectively. The graphs of both polynomials pass through the two points $(16,54)$ and $(20,53).$ Find $P(0) + Q(0).$</p></div><div><div><p><img src="https://cdn.prod.website-files.com/66239f0441b09824acb92c75/678edcd5f221a97f3337fc68_Ellipse%201.png" loading="lazy" alt=""></p><p>INTELLECT-MATH-7B</p></div><div><p>
So I have two quadratic polynomials, $P(x)$ and $Q(x)$, with leading coefficients 2 and -2, respectively. Both pass through the points $(16,54)$ and $(20,53)$. I need to find $P(0) + Q(0)$.</p><p>First, since both are quadratic polynomials, I can express them in the general form:</p><p>$P(x) = 2x² + bx + c$</p><p>$Q(x) = -2x² + dx + e$</p><p>But since both pass through the points $(16,54)$ and $(20,53)$, I can set up equations using these points.</p><p>For $P(x)$:</p><p>$P(16) = 2*(16)² + b \cdot 16 + c = 54$</p><p>$P(20) = 2(20)² + b \cdot 20 + c = 53$</p><p>Similarly for $Q(x)$:</p><p>$Q(16) = -2(16)² + d \cdot 16 + e = 54$</p><p>$Q(20) = -2(20)² + d \cdot 20 + e = 53$</p><p>Let's compute the squares:</p><p>$16² = 256$</p><p>$20² = 400$</p><p>So for $P(x)$:</p><p>$2256 + 16b + c = 54 \Rightarrow 512 + 16b + c = 54 \Rightarrow 16b + c = 54 - 512 = -458 \quad ...(1)$</p><p>$2400 + 20b + c = 53 \Rightarrow 800 + 20b + c = 53 \Rightarrow 20b + c = 53 - 800 = -747 \quad ...(2)$</p><p>For $Q(x)$:</p><p>$-2256 + 16d + e = 54 \Rightarrow -512 + 16d + e = 54 \Rightarrow 16d + e = 54 + 512 = 566 \quad ...(3)$</p><p>$-2400 + 20d + e = 53 \Rightarrow -800 + 20d + e = 53 \Rightarrow 20d + e = 53 + 800 = 853 \quad ...(4)$</p><p>Now, I have two systems of linear equations:</p><p>For $P(x)$:</p><p>$16b + c = -458 \quad ...(1)$</p><p>$20b + c = -747 \quad ...(2)$</p><p>Subtract equation (1) from equation (2):</p><p>$(20b + c) - (16b + c) = -747 - (-458)$</p><p>$4b = -289 \Rightarrow b = -\frac{289}{4}$</p><p>Now plug $b$ back into equation (1):</p><p>$16\left(-\frac{289}{4}\right) + c = -458 \Rightarrow -1156 + c = -458 \Rightarrow c = -458 + 1156 = 698$</p><p>So $P(x) = 2x² - \frac{289}{4}x + 698$</p><p>For $Q(x)$:</p><p>$16d + e = 566 \quad ...(3)$</p><p>$20d + e = 853 \quad ...(4)$</p><p>Subtract equation (3) from equation (4):</p><p>$(20d + e) - (16d + e) = 853 - 566$</p><p>$4d = 287 \Rightarrow d = \frac{287}{4}$</p><p>Now plug $d$ back into equation (3):</p><p>$16\left(\frac{287}{4}\right) + e = 566 \Rightarrow 1148 + e = 566 \Rightarrow e = 566 -1148= -582$</p><p>So $Q(x)=-2x^{2}+\frac{287}{4}x-582$. </p><p>Now, $P(0)=c=698$. </p><p>$Q(0)=e=-582$. </p><p>Therefore, $P(0)+Q(0)=698+(-582)=116$. </p><p>Final Answer</p><p>$\boxed{116}$.

</p></div></div></div></div></div></div><div><p>Acknowledgements</p><p>Prime Intellect Research Team: Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Matthew Di Ferrante, Kushal Thaman, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann</p></div></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Avoiding AI is hard – but our freedom to opt out must be protected (205 pts)]]></title>
            <link>https://theconversation.com/avoiding-ai-is-hard-but-our-freedom-to-opt-out-must-be-protected-255873</link>
            <guid>43958382</guid>
            <pubDate>Mon, 12 May 2025 00:09:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/avoiding-ai-is-hard-but-our-freedom-to-opt-out-must-be-protected-255873">https://theconversation.com/avoiding-ai-is-hard-but-our-freedom-to-opt-out-must-be-protected-255873</a>, See on <a href="https://news.ycombinator.com/item?id=43958382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Imagine applying for a job, only to find out that an algorithm powered by artificial intelligence (AI) rejected your resume before a human even saw it. Or imagine visiting a doctor where treatment options are chosen by a machine you can’t question. </p>

<p>This isn’t science fiction. It’s becoming a reality for many people today. AI quietly shapes almost every part of our lives – from the news we read chosen by AI to how our cities manage traffic. AI promises convenience, productivity, innovation and efficiency. </p>

<p>But as it grows more powerful, one urgent question is being overlooked: do people have the right to live free from AI’s influence?</p>

<h2>Excluded from modern life</h2>

<p>Opting out of AI is no simple matter. </p>

<p>AI powers essential systems such as <a href="https://www.investopedia.com/artificial-intelligence-in-us-government-8406703?">healthcare, transport</a> and <a href="https://www.intel.vn/content/www/vn/vi/financial-services-it/fintech/ai-in-financial-services.html?">finance</a>. </p>

<p>It also influences <a href="https://www.youtube.com/watch?v=a3QAgjuI_zY">hiring decisions</a>, <a href="https://www.theguardian.com/technology/2024/dec/14/saferent-ai-tenant-screening-lawsuit?">rental applications</a>, <a href="https://www.aljazeera.com/economy/2024/2/12/as-corporate-america-pivots-to-ai-consumers-rejected-for-loans-jobs?">loans</a>, <a href="https://www.cash-platform.com/vietcombanks-ai-revolution-enhancing-customer-experience-and-operational-efficiency/?">credit scoring</a>, <a href="https://www.news.com.au/technology/online/social/universitys-ai-experiment-reveals-shocking-truth-about-future-of-online-discourse/news-story/3e257b5bb2a90efd9702a0cd0e149bf8?">social media feeds</a>, <a href="https://www2.deloitte.com/us/en/insights/industry/public-sector/automation-and-generative-ai-in-government.html">government services</a> and even what news or information we see when we search online.</p>

<p>Challenging decisions made by AI in your life can be enormously difficult – and <a href="https://www.theguardian.com/technology/2024/dec/14/saferent-ai-tenant-screening-lawsuit?">may only be able to be done in court</a>. It’s even harder to choose to live without AI entirely, as this requires stepping away from much of modern life. For example, Australian users of Meta’s social media platforms <a href="https://www.smh.com.au/technology/meta-admits-australians-cannot-opt-out-of-predatory-ai-data-scrape-20240911-p5k9o0.html">cannot opt out</a> of having their data used to train the company’s AI models. </p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/a3QAgjuI_zY?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            
          </figure>

<h2>A growing divide</h2>

<p>AI-driven systems are <a href="https://www.businessinsider.com/chatgpt-racial-bias-job-hiring-report-2024-3?">biased</a>. Automated hiring tools favour certain <a href="https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/?">demographics</a>, and <a href="https://vietnamnews.vn/economy/1536512/abbank-launches-tech-based-credit-decisioning-system.html?">AI-powered credit scoring</a> can unfairly deny loans. </p>

<p>These biases are not just theoretical but they are real and present in our daily interactions. If AI becomes the gatekeeper for essential services, those who choose to avoid it could face significant disadvantages.</p>

<p>In many countries where digital systems are expanding rapidly, a large portion of the population <a href="https://www.ispionline.it/en/publication/the-digital-divide-a-barrier-to-social-economic-and-political-equity-204564">struggles to adapt</a> to these technologies. For example in India, <a href="https://www.newindianexpress.com/states/telangana/2024/Nov/04/only-12-per-cent-indians-over-15-years-are-computer-literate-cess-study?">only 12%</a> of people over the age of 15 are considered digitally literate highlighting the challenges of technological changes.</p>

<p>Many face exclusion simply because they don’t fit into the AI-defined model. In these cases, opting out of AI isn’t a personal choice anymore, but it’s a matter of survival in a rapidly changing world. The divide between those who embrace AI and those who are left behind is widening and becoming a social barrier.</p>

<p>This isn’t just about bias or inefficiency. It’s about the fundamental transformation of our society, where the connected, the optimised, and the machine-readable dominate – and the rest of us are left in the shadows.</p>

<h2>A lesson in control</h2>

<p>The story of The Sorcerer’s Apprentice, a poem by Johann Wolfgang von Goethe written in 1797, is a powerful reminder of the dangers of unleashing forces we cannot control. </p>

<p>In the tale, an apprentice uses magic to perform tasks but loses control of the enchanted broomstick, leading to chaos and disaster. Today, AI is that uncontrollable force. It has the potential to make life easier but also poses serious risks when it gets out of hand.</p>

<p>The problem isn’t just about safety but about freedom. The Sorcerer’s Apprentice teaches us, as historian Yuval Noah Harari <a href="https://www.theguardian.com/technology/article/2024/aug/24/yuval-noah-harari-ai-book-extract-nexus">has written</a>, to “never summon a power you cannot control” – a reminder that even well-meaning advancements can spiral out of control if we fail to manage them properly. </p>

<p>As AI becomes more embedded in our lives, the issue isn’t just about whether it works. It’s about whether we have the right to decide how much of it we want in our lives. The freedom to choose, to opt out, is essential to preserving our autonomy in the digital age.</p>

<figure>
            <a href="https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="A large blue and white sign featuring Meta's logo." data-src="https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/665703/original/file-20250505-56-wzz20.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Australian users of Meta’s social media platforms cannot opt out of having their data used to train the company’s AI models.</span>
              <span><a href="https://www.shutterstock.com/image-photo/menlo-park-ca-usa-apr-30-2152758887">Tada Images/Shutterstock</a></span>
            </figcaption>
          </figure>

<h2>What needs to change?</h2>

<p>To protect the right to live a life free from the constant influence of AI, we must act now. </p>

<p>While <a href="https://www.wired.com/story/bidens-ai-bill-of-rights-is-toothless-against-big-tech/?">most</a> AI governance <a href="https://www.pdpc.gov.sg/help-and-resources/2020/01/model-ai-governance-framework?">frameworks</a> emphasise <a href="https://aiforgood.itu.int/ai-governance-and-sustainability/">responsible use</a> focusing on fairness, transparency and accountability, they often overlook a vital principle: the right to disengage from AI systems entirely without facing exclusion or disadvantage. </p>

<p>Governments, businesses and communities need to create policies that not only regulate AI but also respect individual freedoms. People should have the option to live without AI, without facing discrimination or exclusion from essential services.</p>

<p>AI decision making also needs to be more transparent. Whether it’s automated hiring, healthcare or financial services, AI should be understandable, accountable and open to scrutiny. No longer can we allow these systems to operate behind closed doors, making decisions that affect people’s lives with no recourse for the individuals involved.</p>

<p>Finally, society must invest in digital literacy. Everyone should understand the systems that impact their lives and have the tools to challenge them when necessary. Ensuring people know how to navigate and control the technologies that shape their world is essential in maintaining freedom in the digital age.</p>

<h2>An urgent question</h2>

<p>Some may ask, why not just add a kill switch to AI and shut it down if needed? But once AI is woven into crucial systems such as healthcare, transport or communication, it’s no longer something we can simply turn off. </p>

<p>Like electricity or the internet, it becomes a core part of modern life, not easily reversible <a href="https://arxiv.org/abs/2405.07406">without major disruption</a>.</p>

<p>So as AI spreads further into every corner of our lives, we must urgently ask: will we still have the freedom to say no? </p>

<p>If we don’t act now to protect the right to choose, we risk a future where personal autonomy is compromised, and the influence of AI goes unchecked. The question isn’t whether we can live with AI but whether we will still have the right to live without it before it’s too late to break the spell.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Bell Labs Worked (266 pts)]]></title>
            <link>https://1517.substack.com/p/why-bell-labs-worked</link>
            <guid>43957010</guid>
            <pubDate>Sun, 11 May 2025 20:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://1517.substack.com/p/why-bell-labs-worked">https://1517.substack.com/p/why-bell-labs-worked</a>, See on <a href="https://news.ycombinator.com/item?id=43957010">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Hallowed is the name of Bell Labs. It falls from many an ambitious lip, seeking to conjure forth lost magic for their pet jar. Some zealots go further.  They attempt the most venerated of summons — to materialize an Apollo.</p><p>The conjuring proceeds with hope. It is extremely exciting and hopeful to be a part of such an endeavor. Naked hope sparks the flame of conviction. Sponsors are found. Press releases are released. Social media is social-ed. Applications are announced... It's real!</p><p>And then there's silence. Louder the start; quieter the end. Many an aspiring titan ends up wandering the plains of human endeavor with an open jar; forever searching for magic to bottle.</p><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>    </pre></div><p><span>Alexander Graham Bell was prolific. His interests spanned hydrofoils (see footage above), metal detectors, optical data transmission, aviation, genetics, acoustics and early electrification. Bell used his first big liquidity event to start Volta Laboratory and Bureau, a lab that Bell led with an enlightened management style that would become Bell Labs' signature. From "[Bell] suggested the basic lines of research, furnished the financial resources, and then allowed his associates to receive the credit for many of the inventions that resulted." </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163345499" href="https://1517.substack.com/p/why-bell-labs-worked#footnote-1-163345499" target="_self" rel="">1</a></span></p><p>Mervin Kelly, the man who built Bell Labs, shared this attitude. Starting in the late 1920s and accelerating in the 1930s, Kelly went about scouting and (indiscriminately) assimilating every talented person he could find. From the Idea Factory,</p><blockquote><div><p><span>It was curious, in a way, who they were, these men coming to Bell Labs in New York. Most [...had been flagged by professors...] and their names had been quietly passed along to Kelly or someone else at the Labs. [Typically, these recruits grew up] with a peculiar desire to know more about the stars or the telephone lines or (most often) the radio, and especially their makeshift home wireless sets. Almost all of them had put one together themselves, and in turn had discovered how sound could be pulled from the air.</span></p><p><span>— </span><em>Gertner, Jon. The Idea Factory: Bell Labs and The Great Age of American Innovation. Penguin, 2012.</em></p></div></blockquote><p>Bell Labs' antecedent was founded by a prolific maker and researcher, and it was led from the very start by makers and researchers. As a working scientist, Mervin Kelly understood the golden rule, "How do you manage genius? You don't." And it worked.</p><p>During WW2, Bell Labs reversed engineered and improved on the British Magnetron within 2 months. Helped create the "Bazooka." Built an electronic computer that semi-autonomously controlled anti-aircraft guns, invented an acoustic homing torpedo, proximity fuzes, echo-ranging SONAR, pulse code modulation, the first anti-aircraft missile (the Nike) and the klystron.</p><p>By all accounts, Kelly stayed true to his philosophy. None of these projects were micro-managed by Kelly. People did things because they wanted to do them. And they kept doing them after the war.</p><p>Bell Labs is the furnace wherein the American century was forged.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png" width="720" height="720" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:720,&quot;width&quot;:720,&quot;resizeWidth&quot;:720,&quot;bytes&quot;:554862,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://1517.substack.com/i/163345499?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f08b12-db1a-487f-becd-0aa171a9e1c5_720x720.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>An ad for Bell Labs, 1945. Popular Mechanics.</figcaption></figure></div><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>    </pre></div><p>Bell Labs died when Ma Bell died, that's how the popular narrative goes. While this account is mostly true, it obscures a deeper truth — Bell Labs was a victim of the information age it helped create.</p><p>There have been non-profit and corporate giants with larger war chests than Ma Bell. AT&amp;T started Bell Labs when its revenue was under $13 B (current USD). During the great depression, when Mervin Kelly laid the foundation for the lab, AT&amp;T's revenue was $22 B (current USD).</p><p>Inflation adjusted, Google has made more than AT&amp;T did at Bell Labs' start since 2006. Microsoft, 1996. Apple, 1992.</p><p>Each has invested in research. None have a Bell Labs.</p><p><span>Academia's worse. Scientists at the height of their careers spend more time writing grants than doing research. Between 1975 and 2005, the amount of time scientists at top tier universities spent on research declined by 20%. Time spent on paperwork increased by 100%. To quote the study, "experienced secular decline in research time, on the order of 10h per week." </span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-163345499" href="https://1517.substack.com/p/why-bell-labs-worked#footnote-2-163345499" target="_self" rel="">2</a></span></p><p>Empirical measures are hard to find, but another study found that principal investigators in astronomy spend more than 110 hours per grant. Or, one month per grant. As grants have a 20% success rate, more successful investigators apply to two or more per year.</p><p>The game is far more brutal if you're young. A young scientist with their own lab who sets their own research direction is unheard of. When was the last time a 20-something ran a lab at a major university?</p><p>People who can survive this system aren't necessarily the same as people who can do great work. Most of the great names of the past would be considered unemployable today;</p><blockquote><div><p><span>"It's difficult to imagine how I would ever have enough peace and quiet in the present sort of climate to do what I did in 1964... Today I wouldn't get an academic job. It's as simple as that. I don't think I would be regarded as productive enough."</span></p><p><em>— Peter Higgs</em></p></div></blockquote><p>Higgs' statement gets to the heart of the problem, MBA culture,</p><p>We live in a metrics obsessed culture that is obsessed with narrowly defined productivity. There's too much focus on accountability and too little focus on creativity.</p><p>The reason why we don't have Bell Labs is because we're unwilling to do what it takes to create Bell Labs — giving smart people radical freedom and autonomy. </p><p>The freedom to waste time. The freedom to waste resources. And the autonomy to decide how.</p><blockquote><div><p><span>"I've always pursued my interests without much regard for final value or value to the world. I've spent lots of time on totally useless things."</span></p><p><em>— Claude Shannon</em></p></div></blockquote><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>    </pre></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg" width="1456" height="943" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:943,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae67060-a683-4886-a635-86b3291e9c59_1500x971.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Bell Labs, 1966</figcaption></figure></div><blockquote><div><p><span>I don't think I was ever motivated by the notion of winning prizes, although I have a couple of dozen of them in the other room. I was motivated more by curiosity. I was never motivated by the desire for money, financial gain. I wasn't trying to do something big so that I could get a bigger salary.</span></p><p><span>— Claude Shannon</span></p></div></blockquote><p>On December 23rd 1947, when the transistor was demo-ed for the first time, Mervin Kelly wasn't invited. In fact, he didn't know what Bardeen and Brattain were doing. Why would he? They'd tell him when they were ready.</p><p>Kelly seems to have believed that the desire to impress your boss was a corrosive force, and so new discoveries and inventions were steadily percolated up to him and other leadership layer-by-layer one to two weeks at a time. From his perspective, it wasn't Kelly's job to micromanage people. Yes, they worked for him, but in his model, he wasn't their employer — he was their patron.</p><p><span>Reportedly, Kelly and others would hand people problems and then check in a few years later.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-163345499" href="https://1517.substack.com/p/why-bell-labs-worked#footnote-3-163345499" target="_self" rel="">3</a></span><span> Most founders and executives I know balk at this idea. After all, "what's stopping someone from just slacking off?" Kelly would contend that's the wrong question to ask. The right question is, "Why would you expect information theory from someone who needs a babysitter?"</span></p><p>As with many things, it – ultimately – comes down to taste. Bell Labs worked because Mervin Kelly had great taste in people and problems. He could identify who had that innate drive and thirst for more, and who didn't. From Richard Hamming's You and Your Research,</p><blockquote><div><p><span>Now for the matter of drive. You observe that most great scientists have tremendous drive. I worked for 10 years with John Tukey at Bell Labs. He had tremendous drive. One day about 3 or 4 years after I joined, I discovered that John Tukey was slightly younger than I was. John was a genius and I clearly was not.</span></p><p><span>Well, I went storming into Bode's office and said, "How can anybody my age know as much as John Tukey does?" He leaned back in his chair, put his hands behind his head, grinned slightly, and said, "You would be surprised Hamming, how much you would know if you worked as hard as he did that many years." I simply slunk out of the office!</span></p></div></blockquote><p><span>Bell Labs' pantheon was built on the backs of those who can't escape having dark nights of the soul. People who wake up in the middle of the night every night and ask "what am I doing with my life? I've accomplished nothing worthwhile.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-163345499" href="https://1517.substack.com/p/why-bell-labs-worked#footnote-4-163345499" target="_self" rel="">4</a></span><span> People with an exotic mix of conscientiousness, high openness and highly-directional neuroticism. Kelly and Bob Taylor (PARC) were gifted at identifying such people, because (I suspect) they were such people.</span></p><p>Like seeks like.</p><p>Sadly, freedom and patience alone isn’t enough. The Bell Labs formula can be briefly described as,</p><ul><li><p>Use good taste to find great, ambitious people.</p></li><li><p>Surround them with other great, ambitious people.</p></li><li><p>Hire smart, technical makers to be around them.</p><ul><li><p>Cross-pollinate between the two groups as necessary.</p></li></ul></li><li><p>Make sure people talk to each other every day.</p></li><li><p>Create a school so they teach one another.</p><ul><li><p>Encourage everyone to study / improve.</p></li></ul></li></ul><p>Once the group is humming with activity and has a sense of self identity, the smart leader can then,</p><ul><li><p>Use taste to curate problems for the researchers.</p></li><li><p>Give freedom to think — for years, if necessary. Trust the taste.</p></li><li><p>Make explicit hand-offs to the makers.</p><ul><li><p>Once you have something that works from the makers; emphasize fast scaling and execution.</p></li></ul></li><li><p>Scale outwards as necessary.</p></li></ul><p>Proving and expanding on the above formula is left as an exercise to the reader.</p><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>    </pre></div><p><span>One of the reasons why I love working at 1517 is because I believe the general partners (Danielle Strachman and Michael Gibson) do </span><em>understand</em><span> why Bell Labs worked. I see it in how they deal with me and the people around them. Danielle is a teacher first, and has the patience of one combined with the resources of a VC.</span></p><p><span>And while a VC fund is limited in what it can do in providing open-ended freedom. It can try to provide a meaningful simulacrum of that space and community, which is why I’m so excited about programs like </span><a href="https://1517.substack.com/p/1517-garage-science-flux-capacitor" rel="">1517’s Flux</a><span> that invests $100k in people, no questions asked and lets them explore for a few months without demanding KPIs or instantaneous progress.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-163345499" href="https://1517.substack.com/p/why-bell-labs-worked#footnote-5-163345499" target="_self" rel="">5</a></span></p><p>I am hopeful that – as more organizations try such approaches – the idea will spread and we’ll get to see another Bell Labs.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Burrito Now, Pay Later (147 pts)]]></title>
            <link>https://enterprisevalue.substack.com/p/burrito-now-pay-later</link>
            <guid>43956723</guid>
            <pubDate>Sun, 11 May 2025 20:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://enterprisevalue.substack.com/p/burrito-now-pay-later">https://enterprisevalue.substack.com/p/burrito-now-pay-later</a>, See on <a href="https://news.ycombinator.com/item?id=43956723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><strong>tl;dr: Burrito bonds represent finance doing what it does best: unbundling risk, pricing it granularly, and allocating capital more efficiently. BNPL unbundles consumer credit into tradeable assets to let the market decide who should hold the risk. The result is lower borrowing costs, and a credit market that’s more effective than the status quo.</strong></em></p><p><span>Despite skepticism from </span><a href="https://www.azquotes.com/author/21652-Paul_Volcker" rel="">Volcker</a><span> and </span><a href="https://www.trendfollowing.com/whitepaper/buffett.pdf" rel="">Buffet</a><span>, financial innovation has been and will continue to be a massive net positive for humanity. I will write a separate, longer piece on why </span><em>financial engineering is good akshually</em><span>. For now, I’ll describe why the development of the BNPL</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-161211911" href="https://enterprisevalue.substack.com/p/burrito-now-pay-later#footnote-1-161211911" target="_self" rel="">1</a></span><span> securities market will improve the current state of credit and lending. It’s a win-win-win-win-win. </span><em><a href="https://www.youtube.com/watch?v=Ryvkwx8BFss" rel="">We're gonna win so much, you may even get tired of winning.</a></em></p><p><span>In March 2025, </span><a href="https://about.doordash.com/en-us/news/doordash-partners-with-klarna" rel="">DoorDash and Klarna announced a deal</a><span> to let consumers pay for restaurant food, groceries and other delivery orders in four equal, interest-free installments, or “at a more convenient time, such as a date that aligns with their paycheck schedules”. The </span><a href="https://www.zerohedge.com/markets/burrito-now-pay-later-doordash-klarna-deal-feeds-us-debt-addiction" rel="">commentary</a><span> and </span><a href="https://x.com/PirateWires/status/1903191186304799123" rel="">memes</a><span> about “burrito bonds” and Shake Shack securitization were funny. Most takes implied or directly stated it was a manifestation of late-stage capitalism. In 1983, people lamented that one had to use a “credit” card at Burger King. On some level, payments and credit are on the same spectrum of value transfer.</span></p><p>To take a few steps back, why does credit exist at all? One reason is that someone’s or a company’s income doesn’t always align perfectly with when they want to buy things. Credit bridges this gap by allowing the borrower to purchase now and the lender to be repaid later, with the lender earning compensation for the time value of money, repayment risk, and liquidity risks. This logic doesn’t change just because the thing being financed is a burrito.</p><p><span>Is financing your lunch a sign of societal decay? Maybe, maybe not. But it’s definitely an evolution in Market Completion. As a financial engineering and market completion </span><em>enjoyer</em><span>, I think this is great. A </span><a href="https://en.wikipedia.org/wiki/Complete_market" rel="">Complete Market</a><span> is one where every risk can be priced, traded, or hedged. i.e. every risk has a price, every future has a counterparty. If you think the price of oil will be $120 after a year, you can use futures or options on futures to make this bet. If you think the S&amp;P 500 will be over 6,500 within 2 years </span><em>and</em><span> the yield on 10Y Treasury bonds will be 4.25%, investment bankers will help you express this view for a fee.</span></p><p>Small, short-term loans provided by BNPL providers (Klarna, Affirm, etc.) get bundled together as financial instruments and are sold to investors looking for yield. This is called securitization. This does a few useful things for the different parties involved:</p><ul><li><p><strong>For Borrowers:</strong><span> A simple, often zero-interest way to smooth out small expenses, if they pay on time. This contrasts with credit cards, where average rates are 15-30%, and even super-prime borrowers have a 7.22% spread (if the Fed Funds rate is 4% and the credit card APR is 10%, the spread is 6%) over the Fed Funds rate. Additionally, consumers have to only forecast the next six weeks of their life, something even people with </span><a href="https://en.wikipedia.org/wiki/Time_preference" rel="">high time preference</a><span> can handle (I think).</span></p></li></ul><ul><li><p><strong>For Restaurants/Merchants:</strong><span> BNPL fees (2-6%) are higher than credit card fees (1.5-3.5%), reducing the merchant’s net payout per transaction but BNPL drives higher sales volumes, offsetting the fee impact. BNPL boosts conversion, order sizes, and frequency (what else is left for a marketer to do?). So BNPL is really a customer acquisition and loyalty play funded by the merchant, not the Consumer. With BNPL, merchants do not have customer default risk since the provider pays upfront and manages collections. Unlike credit cards, where chargebacks from fraud or disputes can delay or reverse payments weeks or months later, BNPL offers merchants predictable, risk-free payouts.</span></p></li></ul><ul><li><p><strong>BNPL Provider:</strong><span> The BNPL provider finances the $100 loan using its own funds, a bank credit line, or investor-backed warehouse financing.</span></p><ul><li><p>BNPL Economics: The BNPL provider pays the merchant $95 (after a $5 fee) and collects $100 from the customer over time, earning the $5 fee as revenue. To free up capital, the provider bundles many $100 loans and sells them to investors for 95 cents on the dollar through securitization. This allows the provider to recycle funds into new loans, continuing to earn fees. To align interests with investors, the provider retains a ‘first-loss’ portion of the securitized loans, so BNPL providers have skin in the game to ensure responsible lending. After paying $0.57 for capital costs and operations, the provider’s net profit per $100 loan is $4.43. The more $100 loans they can make and get off their balance sheet, the more profits they make.</p></li></ul></li></ul><ul><li><p><strong>For Banks &amp; Investors:</strong><span> It creates a new, short-duration segment within the Asset-Backed Securities market. BNPL loans are 6-8 weeks long. That’s a tight risk-feedback loop. In exchange for fees, </span><a href="https://www.spglobal.com/market-intelligence/en/news-insights/articles/2023/3/buy-now-pay-later-platforms-turn-to-interest-bearing-lending-via-bank-partners-74223673" rel="">banks structure these loans and quickly move them off their balance sheets and to investors</a><span>. This is one of the primary roles of finance–to transfer risk to those who are best equipped to hold it. The rough math is that the merchant fee funds a loan yielding an APR of 22.8% for the investor (capital provider) while the consumer pays zero interest. This is the Golden Age of Credit™ after all.</span></p><ul><li><p><span>Investor Economics:</span><strong> </strong><span>Assume a $100 BNPL loan. $25 is paid upfront by the Consumer, so an Investor pays $73 for a $75 loan, discounted for risk, fees, and return expectations. The Investor receives $75 from customer repayments over 6 weeks minus servicing fees of $0.25. A $1.75 profit on $73 investment over 6 weeks is a 2.4% return, or 22.8% annualized (52 weeks/6 weeks = 8.67 periods each year; annualized return = (1+0.024)</span><sup>8.67 </sup><span>- 1).</span></p></li></ul></li></ul><ul><li><p><strong>For the Economy:</strong><span> This process is another step toward a Complete Market. It converts a financing tool into a discrete financial instrument that can be priced and traded based on the views of financially-motivated participants, i.e. it converts a balance sheet item into a tradeable security. The allocation of capital becomes more efficient as the number of heterogeneous market participants increases via the Wisdom of Crowds. Thus, this over time reduces the costs of credit for consumers and businesses.</span></p></li></ul><p>BNPL providers have fairly clear terms and retain the flexibility to adjust their own risk. For borrowers:</p><ul><li><p><strong>Late Fees</strong><span>: A modest fee, often capped to keep things reasonable. Alice forgets a $25 installment on her $100 DoorDash transaction, she’s hit with a $7 late fee, tacked onto her next payment. Annoying, but it probably won’t push her down a debt spiral.</span></p></li><li><p><strong>Credit Impact</strong><span>: Alice’s credit score takes a hit if the BNPL provider reports it (many don’t for short-term plans). Most BNPL defaults stay off the radar of the credit bureaus, unlike credit card delinquencies, but </span><a href="https://www.bankrate.com/credit-cards/news/buy-now-pay-later-and-credit-reports/" rel="">this is changing</a><span>.</span></p></li><li><p><strong>Account Lockout</strong><span>: BNPL platforms can freeze account her account if it gets  over-leveraged and moves into the ‘high-risk borrower’ category.</span></p></li><li><p><strong>Collections</strong><span>: If payments stay delinquent for months, providers will escalate to collections. </span></p></li><li><p><strong>APRs:</strong><span> Most BNPL plans start at 0% APR for short-term deals, and missing a payment doesn’t change the rates. However, some providers might revoke the 0% rate, slapping a standard rate of, say 20%, on the remaining balance. Late fees mimic interest even if called interest-fee loans.</span></p></li></ul><p><span>This is a fair concern, with </span><a href="https://libertystreeteconomics.newyorkfed.org/2023/09/who-uses-buy-now-pay-later/" rel="">the data</a><span> showing that those with lower incomes, lower education, and worse credit (redundancies here) are more likely to use BNPL. Even with adverse selection for BNPL, the underwriting is for each transaction, not for all monthly spending like that in credit cards; so if a consumer misses a payment, the BNPL provider can stop lending immediately, as opposed to the credit card company which has to underwrite the person’s full ability and willingness to repay their debts. This tech-enabled granularity allows for legibility and hence greater precision and predictability. Asking your neighbor to watch your dog for a few days is a much lower bar than asking them to raise your toddler kids if you get hit by a bus because the stakes and duration of your ask are very different. BNPL is granular, precise, and agile. Credit cards are monolithic and inert. See the next section for more on legibility.</span></p><p>In Design, the principle of Universal Design focuses on creating inclusive systems and tools that improve usability for all. Autocorrect, text-to-speech readers, dark mode, and subtitles all came from Universal Design. Similarly, lending that makes credit more affordable and accessible for lower-income individuals will reduce credit costs for all borrowers.</p><p><span>I’m a </span><a href="https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/" rel="">James C. Scott appreciator</a><span>. Credit cards account for 63% of all US retail purchases but bundle all sorts of non-discretionary and discretionary purchases together. Your $30 takeout order is bundled with “financing” for a new coffee machine, Uber rides, airline tickets, gym membership, dry cleaning, a tire change, and maybe even your monthly rent. Additionally, your credit card rate is also obscured by complex fee structures, variable interest rates, reward points, hotel points, and airline miles. So it’s hard to precisely price the risk of that $30 loan.</span></p><p>BNPL disentangles these purchases. Each loan is for a specific item, with a fixed repayment schedule over a very short term (usually 6-8 weeks).</p><p>Underwriting happens at the point of sale for that specific transaction. Investors buying securities backed by these loans get exposure to a pool of very similar, short-term risks, making it easier to model and understand compared to a portion of a bank's diversified credit card portfolio. The granular control lets providers nip risks in the bud without punishing borrowers into financial destitution. Unlike credit cards, where a few missed payments could balloon to a 5-figure limit, BNPL’s transaction-level underwriting caps the damage. This is legibility in action—providers see exactly what’s gone wrong and adjust.</p><p>For investors, defaults are baked into the yield math: the 22.8% annualized return must also account for the burrito deadbeats. This improved legibility means the risk premium that’s embedded in a blended credit card rate can be isolated, traded, and hence compressed.</p><p><span>While defaults happen, the short duration means they won't cascade through and clog up the financial system in the same way longer-term, larger debts like mortgages did during 2006-2008. But even if BNPL scales massively, the risk sits on the balance sheets of credit funds and hedge funds, not deposit-backed banks. This unbundling makes the specific risk of financing everyday consumption more visible and manageable. Banks are already using </span><a href="https://www.kkr.com/insights/asset-based-finance-srt" rel="">Significant Risk Transfers</a><span> to transfer risk to non-bank investors (SRTs move loans’ risks and returns from banks’ balance sheets to credit investors so banks keep the customer relationship while the risks &amp; returns are transferred to investors). Thus, even if the BNPL securitization market grows exponentially, risks will be shifted toward investors who can stomach them, rather than banks dependent on retail deposits. Moral hazard, counterparty risks, etc. etc. remain, but </span><a href="https://www.youtube.com/watch?v=ag14Ao_xO4c" rel="">willing buyers are transacting with willing sellers</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png" width="498" height="280" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:280,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04a50002-90de-4c84-810f-a37f79dc77f8_498x280.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Losses across different FICO score portfolios tend to move together, rising during economic downturns, and currently cannot be diversified within the credit card market. Additionally, credit card losses (charge-off rates) are highly correlated with losses on other types of loans and corporate bonds. Which is to say that credit card lending risks are undiversifiable across other lending markets and therefore banks/investors have to be compensated to hold this risk. Hence, consumers must pay more to borrow because their risk of default cannot be diversified.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png" width="1186" height="880" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:880,&quot;width&quot;:1186,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe23d000e-e996-42a6-94df-3516f76c2c9d_1186x880.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>At the moment, for credit cards, </span><a href="https://libertystreeteconomics.newyorkfed.org/2025/03/why-are-credit-card-rates-so-high/" rel="">the average interest spread</a><span> over the Fed Funds rate (the risk-free rate) is 14.5%, and ranges from 21% for borrowers with a low FICO score of 600, to 7.22% for those with the highest score of 850. The </span><a href="https://libertystreeteconomics.newyorkfed.org/2025/03/why-are-credit-card-rates-so-high/" rel="">estimated non-diversifiable risk premium is 5.3% per year</a><span>, or 37% of the average risk premium of 14.5%, which seems really high. I expect this component of the credit card spread will get compressed as the BNPL securitization market matures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png" width="1330" height="852" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:852,&quot;width&quot;:1330,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30ed0887-ab6f-4222-8e54-2b5d1827fd93_1330x852.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As the BNPL-Backed Securities (“BBS”) market develops, I expect to see the creation and growth of BBS Indexes and associated derivatives and BBS ratings from the credit ratings agencies. ABS indexes already exist for mortgage-backed securities, CDS indexes for corporate bonds, etc. Investment banks will create futures, options, swaps, etc. so investors can manage their BBS portfolios. If a fund holds a portfolio of BBS and is worried about an increase in takeout BNPL credit losses, they could short the “Food Delivery BBS Index”, and similarly the “Clothing BBS Index”, the “Electronics BBS Index”, and so on.</p><p><span>Do I want to see a </span><a href="http://sezzle.com/shop/draftkings/" rel="">Sports Betting BBS Index</a><span>? No. Will it happen? Definitely. Sports betting </span><a href="https://www.investopedia.com/americans-sports-betting-losing-8768618" rel="">does a lot of damage</a><span> to the finances of American households but when the loans backing them are securitized, they will make for a great fixed income product because because gambling is a somewhat recession-resilient industry, much like other ‘sin sectors’ like alcohol and tobacco (BBS indexes for alcohol and tobacco will also happen, and around here is where I may get tired of winning).</span></p><p><span>Perhaps, a case can be made for some paternalism i.e. to not increase financing for harmful products. And of course, we should also expect to see a Rental Market BBS Index because BNPL for monthly rent payments </span><a href="https://www.prnewswire.com/news-releases/splitit-brings-its-installments-as-a-service-to-the-rental-market-through-its-partnership-with-letus-301598624.html" rel="">is</a><span> </span><a href="https://getflex.com/" rel="">already</a><span> </span><a href="https://zip.co/us/store/payrent" rel="">commonplace</a><span>.</span></p><blockquote><p><em>One argument made is that BNPLs, by extension of credit to people who don’t have traditional credit available, allow customers to overextend themselves. (This is, for social reasons, very rarely articulated as “I am better at math than women who shop at Sephora, think they will not make good choices, and accordingly think they should have less choices presented to them.” But it is that argument.)</em><span>—</span><a href="https://www.bitsaboutmoney.com/archive/buy-now-pay-later/" rel="">Patrick McKenzie</a></p></blockquote><p>Here is where market completion comes in. An investor can bet on the rental market holding up by buying the Rental Market BBS index futures and selling the broader BBS index if they think rentals will outperform. Similarly, DoorDash can sell the Food Delivery index to protect against a potential drop in their earnings if it sees softening demand. These index prices can serve as a signal and guide lending decisions in the BNPL market and the wider credit market because they will aggregate the opinion of the market’s participants.</p><p>When investors can hedge non-diversifiable risks, they require a lower return/yield for taking on that exposure. This translates back through the capital allocation chain, lowering the fees merchants pay, or allowing BNPL providers to offer even better terms i.e. cheaper credit for borrowers, and making the whole economy more efficient at allocating resources. While financial engineering doesn’t eliminate risk; it transfers it to those best equipped to handle it, making credit cheaper and capital allocation more effective.</p><p>More speculative but still plausible: by providing flexible, short-term financing with manageable risks, BNPL will make the economy more resilient to recessions by supporting consumption and business stability. BNPL’s short duration and non-systemic risks limit cascading defaults. This flexibility could buffer downturns as consumers &amp; merchants and their financiers adapt more quickly to changing conditions.</p><p>This process–underwriting individual loans, securitizing them, and creating indexes and derivatives–is a classic pattern in financial markets. So while “burrito bonds” sound absurd, absurdity is common at the frontiers of innovation.</p><p><span>In this case, tech-enabled financial engineering leads to more efficient credit, granular risk transfer, and deeper markets. And that’s a win.</span><br></p><p data-attrs="{&quot;url&quot;:&quot;https://enterprisevalue.substack.com/p/burrito-now-pay-later?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://enterprisevalue.substack.com/p/burrito-now-pay-later?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ToyDB rewritten: a distributed SQL database in Rust, for education (111 pts)]]></title>
            <link>https://github.com/erikgrinaker/toydb</link>
            <guid>43956547</guid>
            <pubDate>Sun, 11 May 2025 19:49:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/erikgrinaker/toydb">https://github.com/erikgrinaker/toydb</a>, See on <a href="https://news.ycombinator.com/item?id=43956547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/erikgrinaker/toydb/blob/main/docs/architecture/images/toydb.svg"><img src="https://github.com/erikgrinaker/toydb/raw/main/docs/architecture/images/toydb.svg" height="40"></a> toyDB</h2><a id="user-content--toydb" aria-label="Permalink:  toyDB" href="#-toydb"></a></div>
<p dir="auto">Distributed SQL database in Rust, built from scratch as an educational project. Main features:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/src/raft/mod.rs">Raft distributed consensus</a> for linearizable state machine replication.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/src/storage/mvcc.rs">ACID transactions</a> with MVCC-based snapshot isolation.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/src/storage/engine.rs">Pluggable storage engine</a> with <a href="https://github.com/erikgrinaker/toydb/blob/main/src/storage/bitcask.rs">BitCask</a> and <a href="https://github.com/erikgrinaker/toydb/blob/main/src/storage/memory.rs">in-memory</a> backends.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/src/sql/execution/executor.rs">Iterator-based query engine</a> with <a href="https://github.com/erikgrinaker/toydb/blob/main/src/sql/planner/optimizer.rs">heuristic optimization</a> and time-travel
support.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/src/sql/parser/parser.rs">SQL interface</a> including joins, aggregates, and transactions.</p>
</li>
</ul>
<p dir="auto">I originally wrote toyDB in 2020 to learn more about database internals. Since then, I've spent
several years building real distributed SQL databases at
<a href="https://github.com/cockroachdb/cockroach">CockroachDB</a> and
<a href="https://github.com/neondatabase/neon">Neon</a>. Based on this experience, I've rewritten toyDB as a
simple illustration of the architecture and concepts behind distributed SQL databases.</p>
<p dir="auto">toyDB is intended to be simple and understandable, and also functional and correct. Other aspects
like performance, scalability, and availability are non-goals -- these are major sources of
complexity in production-grade databases, and obscure the basic underlying concepts. Shortcuts have
been taken where possible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/docs/architecture/index.md">Architecture guide</a>: a guided tour of toyDB's code and architecture.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/docs/examples.md">SQL examples</a>: walkthrough of toyDB's SQL features.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/docs/sql.md">SQL reference</a>: reference documentation for toyDB's SQL dialect.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/docs/references.md">References</a>: research materials used while building toyDB.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">With a <a href="https://www.rust-lang.org/tools/install" rel="nofollow">Rust compiler</a> installed, a local five-node
cluster can be built and started as:</p>
<div data-snippet-clipboard-copy-content="$ ./cluster/run.sh
Starting 5 nodes on ports 9601-9605 with data under cluster/*/data/.
To connect to node 1, run: cargo run --release --bin toysql

toydb4 21:03:55 [INFO] Listening on [::1]:9604 (SQL) and [::1]:9704 (Raft)
toydb1 21:03:55 [INFO] Listening on [::1]:9601 (SQL) and [::1]:9701 (Raft)
toydb2 21:03:55 [INFO] Listening on [::1]:9602 (SQL) and [::1]:9702 (Raft)
toydb3 21:03:55 [INFO] Listening on [::1]:9603 (SQL) and [::1]:9703 (Raft)
toydb5 21:03:55 [INFO] Listening on [::1]:9605 (SQL) and [::1]:9705 (Raft)
toydb2 21:03:56 [INFO] Starting new election for term 1
[...]
toydb2 21:03:56 [INFO] Won election for term 1, becoming leader"><pre><code>$ ./cluster/run.sh
Starting 5 nodes on ports 9601-9605 with data under cluster/*/data/.
To connect to node 1, run: cargo run --release --bin toysql

toydb4 21:03:55 [INFO] Listening on [::1]:9604 (SQL) and [::1]:9704 (Raft)
toydb1 21:03:55 [INFO] Listening on [::1]:9601 (SQL) and [::1]:9701 (Raft)
toydb2 21:03:55 [INFO] Listening on [::1]:9602 (SQL) and [::1]:9702 (Raft)
toydb3 21:03:55 [INFO] Listening on [::1]:9603 (SQL) and [::1]:9703 (Raft)
toydb5 21:03:55 [INFO] Listening on [::1]:9605 (SQL) and [::1]:9705 (Raft)
toydb2 21:03:56 [INFO] Starting new election for term 1
[...]
toydb2 21:03:56 [INFO] Won election for term 1, becoming leader
</code></pre></div>
<p dir="auto">A command-line client can be built and used with node 1 on <code>localhost:9601</code>:</p>
<div data-snippet-clipboard-copy-content="$ cargo run --release --bin toysql
Connected to toyDB node n1. Enter !help for instructions.
toydb> CREATE TABLE movies (id INTEGER PRIMARY KEY, title VARCHAR NOT NULL);
toydb> INSERT INTO movies VALUES (1, 'Sicario'), (2, 'Stalker'), (3, 'Her');
toydb> SELECT * FROM movies;
1, 'Sicario'
2, 'Stalker'
3, 'Her'"><pre><code>$ cargo run --release --bin toysql
Connected to toyDB node n1. Enter !help for instructions.
toydb&gt; CREATE TABLE movies (id INTEGER PRIMARY KEY, title VARCHAR NOT NULL);
toydb&gt; INSERT INTO movies VALUES (1, 'Sicario'), (2, 'Stalker'), (3, 'Her');
toydb&gt; SELECT * FROM movies;
1, 'Sicario'
2, 'Stalker'
3, 'Her'
</code></pre></div>
<p dir="auto">toyDB supports most common SQL features, including joins, aggregates, and transactions. Below is an
<code>EXPLAIN</code> query plan of a more complex query (fetches all movies from studios that have released any
movie with an IMDb rating of 8 or more):</p>
<div data-snippet-clipboard-copy-content="toydb> EXPLAIN SELECT m.title, g.name AS genre, s.name AS studio, m.rating
  FROM movies m JOIN genres g ON m.genre_id = g.id,
    studios s JOIN movies good ON good.studio_id = s.id AND good.rating >= 8
  WHERE m.studio_id = s.id
  GROUP BY m.title, g.name, s.name, m.rating, m.released
  ORDER BY m.rating DESC, m.released ASC, m.title ASC;

Remap: m.title, genre, studio, m.rating (dropped: m.released)
└─ Order: m.rating desc, m.released asc, m.title asc
   └─ Projection: m.title, g.name as genre, s.name as studio, m.rating, m.released
      └─ Aggregate: m.title, g.name, s.name, m.rating, m.released
         └─ HashJoin: inner on m.studio_id = s.id
            ├─ HashJoin: inner on m.genre_id = g.id
            │  ├─ Scan: movies as m
            │  └─ Scan: genres as g
            └─ HashJoin: inner on s.id = good.studio_id
               ├─ Scan: studios as s
               └─ Scan: movies as good (good.rating > 8 OR good.rating = 8)"><pre><code>toydb&gt; EXPLAIN SELECT m.title, g.name AS genre, s.name AS studio, m.rating
  FROM movies m JOIN genres g ON m.genre_id = g.id,
    studios s JOIN movies good ON good.studio_id = s.id AND good.rating &gt;= 8
  WHERE m.studio_id = s.id
  GROUP BY m.title, g.name, s.name, m.rating, m.released
  ORDER BY m.rating DESC, m.released ASC, m.title ASC;

Remap: m.title, genre, studio, m.rating (dropped: m.released)
└─ Order: m.rating desc, m.released asc, m.title asc
   └─ Projection: m.title, g.name as genre, s.name as studio, m.rating, m.released
      └─ Aggregate: m.title, g.name, s.name, m.rating, m.released
         └─ HashJoin: inner on m.studio_id = s.id
            ├─ HashJoin: inner on m.genre_id = g.id
            │  ├─ Scan: movies as m
            │  └─ Scan: genres as g
            └─ HashJoin: inner on s.id = good.studio_id
               ├─ Scan: studios as s
               └─ Scan: movies as good (good.rating &gt; 8 OR good.rating = 8)
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">toyDB's architecture is fairly typical for a distributed SQL database: a transactional
key/value store managed by a Raft cluster with a SQL query engine on top. See the
<a href="https://github.com/erikgrinaker/toydb/blob/main/docs/architecture/index.md">architecture guide</a> for more details.</p>
<p dir="auto"><a href="https://github.com/erikgrinaker/toydb/blob/main/docs/architecture/index.md"><img src="https://github.com/erikgrinaker/toydb/raw/main/docs/architecture/images/architecture.svg" alt="toyDB architecture"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests</h2><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto">toyDB mainly uses <a href="https://github.com/erikgrinaker/goldenscript">Goldenscripts</a> for tests. These
script various scenarios, capture events and output, and later assert that the behavior remains the
same. See e.g.:</p>
<ul dir="auto">
<li><a href="https://github.com/erikgrinaker/toydb/tree/main/src/raft/testscripts/node">Raft cluster tests</a></li>
<li><a href="https://github.com/erikgrinaker/toydb/tree/main/src/storage/testscripts/mvcc">MVCC transaction tests</a></li>
<li><a href="https://github.com/erikgrinaker/toydb/tree/main/src/sql/testscripts">SQL execution tests</a></li>
<li><a href="https://github.com/erikgrinaker/toydb/tree/main/tests/scripts">End-to-end tests</a></li>
</ul>
<p dir="auto">Run tests with <code>cargo test</code>, or have a look at the latest
<a href="https://github.com/erikgrinaker/toydb/actions/workflows/ci.yml">CI run</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">toyDB is not optimized for performance, but comes with a <code>workload</code> benchmark tool that can run
various workloads against a toyDB cluster. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Start a 5-node toyDB cluster.
$ ./cluster/run.sh
[...]

# Run a read-only benchmark via all 5 nodes.
$ cargo run --release --bin workload read
Preparing initial dataset... done (0.179s)
Spawning 16 workers... done (0.006s)
Running workload read (rows=1000 size=64 batch=1)...

Time   Progress     Txns      Rate       p50       p90       p99      pMax
1.0s      13.1%    13085   13020/s     1.3ms     1.5ms     1.9ms     8.4ms
2.0s      27.2%    27183   13524/s     1.3ms     1.5ms     1.8ms     8.4ms
3.0s      41.3%    41301   13702/s     1.2ms     1.5ms     1.8ms     8.4ms
4.0s      55.3%    55340   13769/s     1.2ms     1.5ms     1.8ms     8.4ms
5.0s      70.0%    70015   13936/s     1.2ms     1.5ms     1.8ms     8.4ms
6.0s      84.7%    84663   14047/s     1.2ms     1.4ms     1.8ms     8.4ms
7.0s      99.6%    99571   14166/s     1.2ms     1.4ms     1.7ms     8.4ms
7.1s     100.0%   100000   14163/s     1.2ms     1.4ms     1.7ms     8.4ms

Verifying dataset... done (0.002s)"><pre><span><span>#</span> Start a 5-node toyDB cluster.</span>
$ ./cluster/run.sh
[...]

<span><span>#</span> Run a read-only benchmark via all 5 nodes.</span>
$ cargo run --release --bin workload <span>read</span>
Preparing initial dataset... <span>done</span> (0.179s)
Spawning 16 workers... <span>done</span> (0.006s)
Running workload <span>read</span> (rows=1000 size=64 batch=1)...

Time   Progress     Txns      Rate       p50       p90       p99      pMax
1.0s      13.1%    13085   13020/s     1.3ms     1.5ms     1.9ms     8.4ms
2.0s      27.2%    27183   13524/s     1.3ms     1.5ms     1.8ms     8.4ms
3.0s      41.3%    41301   13702/s     1.2ms     1.5ms     1.8ms     8.4ms
4.0s      55.3%    55340   13769/s     1.2ms     1.5ms     1.8ms     8.4ms
5.0s      70.0%    70015   13936/s     1.2ms     1.5ms     1.8ms     8.4ms
6.0s      84.7%    84663   14047/s     1.2ms     1.4ms     1.8ms     8.4ms
7.0s      99.6%    99571   14166/s     1.2ms     1.4ms     1.7ms     8.4ms
7.1s     100.0%   100000   14163/s     1.2ms     1.4ms     1.7ms     8.4ms

Verifying dataset... <span>done</span> (0.002s)</pre></div>
<p dir="auto">The available workloads are:</p>
<ul dir="auto">
<li><code>read</code>: single-row primary key lookups.</li>
<li><code>write</code>: single-row inserts to sequential primary keys.</li>
<li><code>bank</code>: bank transfers between various customers and accounts. To make things interesting, this
includes joins, secondary indexes, sorting, and conflicts.</li>
</ul>
<p dir="auto">For more information about workloads and parameters, run <code>cargo run --bin workload -- --help</code>.</p>
<p dir="auto">Example workload results are listed below. Write performance is atrocious, due to
<a href="https://en.wikipedia.org/wiki/Sync_(Unix)" rel="nofollow">fsync</a> and a lack of write batching in the Raft layer.
Disabling fsync, or using the in-memory engine, significantly improves write performance (at the
expense of durability).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Workload</th>
<th>BitCask</th>
<th>BitCask w/o fsync</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>read</code></td>
<td>14163 txn/s</td>
<td>13941 txn/s</td>
<td>13949 txn/s</td>
</tr>
<tr>
<td><code>write</code></td>
<td>35 txn/s</td>
<td>4719 txn/s</td>
<td>7781 txn/s</td>
</tr>
<tr>
<td><code>bank</code></td>
<td>21 txn/s</td>
<td>1120 txn/s</td>
<td>1346 txn/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Debugging</h2><a id="user-content-debugging" aria-label="Permalink: Debugging" href="#debugging"></a></p>
<p dir="auto"><a href="https://code.visualstudio.com/" rel="nofollow">VSCode</a> and the <a href="https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb" rel="nofollow">CodeLLDB</a>
extension can be used to debug toyDB, with the debug configuration under <code>.vscode/launch.json</code>.</p>
<p dir="auto">Under the "Run and Debug" tab, select e.g. "Debug executable 'toydb'" or "Debug unit tests in
library 'toydb'".</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">The toyDB logo is courtesy of <a href="https://github.com/jonasmerlin">@jonasmerlin</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2024 sea level 'report cards' map futures of U.S. coastal communities (101 pts)]]></title>
            <link>https://news.wm.edu/2025/05/06/2024-sea-level-report-cards-map-futures-of-u-s-coastal-communities/</link>
            <guid>43956115</guid>
            <pubDate>Sun, 11 May 2025 18:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.wm.edu/2025/05/06/2024-sea-level-report-cards-map-futures-of-u-s-coastal-communities/">https://news.wm.edu/2025/05/06/2024-sea-level-report-cards-map-futures-of-u-s-coastal-communities/</a>, See on <a href="https://news.ycombinator.com/item?id=43956115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

    
<main>
	
		
		
		<!-- .page-header -->

		<div id="primary">
						

<article id="post-19488">

	
	<span>

		

		<div>
			
<p><em>The following story originally appeared on the <a href="https://www.vims.edu/newsandevents/topstories/2025/slrc.php">website</a> for W&amp;M’s Batten School &amp; VIMS. – Ed.</em></p>



<p><a href="https://www.wm.edu/">William &amp; Mary</a>’s&nbsp;<a href="https://www.vims.edu/academics/index.php">Batten School</a>&nbsp;&amp;&nbsp;<a href="https://www.vims.edu/academics/index.php">VIMS</a>&nbsp;have released their 2024 U.S. sea level “report cards,” providing updated analyses of sea level trends and projections for 36 coastal communities. Encompassing 55 years of historical data, the report cards aid planning and adaptation efforts by analyzing rates of sea level rise and acceleration at each locality and forecasting 2050 water levels.&nbsp;</p>



<p>This year, the report cards are consolidated in an interactive&nbsp;<a href="https://www.arcgis.com/apps/dashboards/30ce98512c5b429f82d04d67f0d6b5db">dashboard</a>&nbsp;and add data from tide gauge stations in Annapolis, Maryland; Solomons Island, Maryland; Yorktown, Virginia; and Fort Myers, Florida.</p>


<div>
<figure><img decoding="async" width="720" height="348" src="https://wmit-news-prod.s3.amazonaws.com/wp-content/uploads/2025/05/06085735/slrc-dashboard-720.jpg" alt="A graphic shows a map with red dots on the coasts of America and a graph with projections for Eastport, Maine." srcset="https://wmit-news-prod.s3.amazonaws.com/wp-content/uploads/2025/05/06085735/slrc-dashboard-720.jpg 720w, https://wmit-news-prod.s3.amazonaws.com/wp-content/uploads/2025/05/06085735/slrc-dashboard-720-360x174.jpg 360w" sizes="(max-width: 720px) 100vw, 720px"><figcaption>A new, interactive dashboard created by the Center for Coastal Resources Management at W&amp;M’s Batten School &amp; VIMS consolidates sea level rise and acceleration data for 36 U.S. coastal communities.</figcaption></figure></div>


<p>Most sea level projections are based on an understanding of average global sea level rise. However, sea levels do not rise uniformly across the world. Factors such as geological uplift, land subsidence, ocean currents and other processes all impact regional sea level trends.&nbsp;</p>



<p>“Many people who live near the coast want to know what they can reasonably expect over the next few decades, giving them time to make actionable plans and decisions,” says Molly Mitchell, an assistant professor at the Batten School of Coastal &amp; Marine Sciences &amp; VIMS. “Compared to other predictions based on satellite data and global computer models, our reports are created using observed tide gauge data from the past 55 years and reflect the exact experience at the location of the gauge. The annual release of the report cards allows coastal regions to examine if past trends are changing and alter their planning accordingly.”</p>



<p>The reports group localities into East Coast, Gulf Coast, West Coast and Alaskan Coast regions. Each report card shows values for monthly sea level averages along with high-and low-water levels caused by storms and other transient events, as well as a decadal signal showing the influence of longer-term climate patterns such as El Niño. Observed rates of acceleration are factored into future projections and are displayed in comparison to a linear trendline that does not account for acceleration.&nbsp;</p>



<p>The projections also show the range of sea level rise within the 95% confidence interval, which allows individuals and municipalities to plan adequately for the highest predicted rates of sea level rise caused by things like storm surge and tidal flooding.&nbsp;</p>



<p>Overall, most locations continue a trend of accelerating sea level rise. However, Mitchell notes that projections have remained mostly uniform since reporting began in 2018, apart from a few notable exceptions.&nbsp;</p>



<p>“One interesting new trend is the acceleration occurring in southeastern states such as South Carolina and Georgia,” said Mitchell. “We continue to see the fastest rates of sea level rise in Gulf states like Texas and Louisiana, but many of the East Coast stations are accelerating quite quickly, likely due to patterns of water distribution related to glacial melt from the Greenland ice sheet.”</p>



<p>Mitchell also notes that most West Coast localities have been fairly stable, despite past predictions that they would increase rapidly. “This has led to some questions about why,” she said.&nbsp;</p>



<p>Information about the processes most affecting regional sea levels is listed on the Batten School &amp; VIMS&nbsp;<a href="https://www.vims.edu/research/products/slrc/processes/">website</a>.&nbsp;Emeritus Professor John Boon launched the sea level report cards in 2018 following the publication of the study&nbsp;<a href="https://scholarworks.wm.edu/reports/1111/">Anthropocene Sea Level Change: A History of Recent Trends Observed in the U.S. East, Gulf and West Coast Regions</a>, which showed a notable increase in sea level acceleration rates beginning in 2013-2014.</p>
		
		</div><!-- .entry-content -->

	</span><!-- .entry-text -->



			<!-- .entry-footer -->
	
	
		<!-- duplicate social share buttons in right column and footer -->
		<!-- don't show column buttons on video template -->
					
		
					
				
	
</article><!-- #post-## -->


											</div>

	
</main>










					
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scraperr – A Self Hosted Webscraper (221 pts)]]></title>
            <link>https://github.com/jaypyles/Scraperr</link>
            <guid>43955842</guid>
            <pubDate>Sun, 11 May 2025 18:29:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jaypyles/Scraperr">https://github.com/jaypyles/Scraperr</a>, See on <a href="https://news.ycombinator.com/item?id=43955842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/jaypyles/www-scrape/blob/master/docs/logo_picture.png"><img src="https://github.com/jaypyles/www-scrape/raw/master/docs/logo_picture.png" alt="Scraperr Logo" width="250px"></a></p><p dir="auto"><strong>A powerful self-hosted web scraping solution</strong></p>
  <p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ec9b2bbaccf6915a29050ce24c10cd9b481b0c41b0bf5194add3e69f49a9be3c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f6e676f44422d2532333465613934622e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6d6f6e676f6462266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/ec9b2bbaccf6915a29050ce24c10cd9b481b0c41b0bf5194add3e69f49a9be3c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f6e676f44422d2532333465613934622e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6d6f6e676f6462266c6f676f436f6c6f723d7768697465" alt="MongoDB" data-canonical-src="https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&amp;logo=mongodb&amp;logoColor=white"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e39929a378d51fbc2b819c60b41e25514a8e24e2e42889cd936e9365dc0d438d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666f722d7468652d6261646765266c6f676f3d66617374617069"><img src="https://camo.githubusercontent.com/e39929a378d51fbc2b819c60b41e25514a8e24e2e42889cd936e9365dc0d438d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f466173744150492d3030353537313f7374796c653d666f722d7468652d6261646765266c6f676f3d66617374617069" alt="FastAPI" data-canonical-src="https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&amp;logo=fastapi"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4ff95c6c85e810b4acfe5dbf01bf2b44680cf75945b21a7e5438c87b473f2c6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6578742d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/d4ff95c6c85e810b4acfe5dbf01bf2b44680cf75945b21a7e5438c87b473f2c6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e6578742d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d6e6578742e6a73266c6f676f436f6c6f723d7768697465" alt="Next JS" data-canonical-src="https://img.shields.io/badge/Next-black?style=for-the-badge&amp;logo=next.js&amp;logoColor=white"></a>
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b2eac0f505dfd05c25acf8c285b5eb346916090126c8836c6cbf9aeb754eac32/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7461696c77696e646373732d2532333338423241432e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d7461696c77696e642d637373266c6f676f436f6c6f723d7768697465"><img src="https://camo.githubusercontent.com/b2eac0f505dfd05c25acf8c285b5eb346916090126c8836c6cbf9aeb754eac32/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7461696c77696e646373732d2532333338423241432e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d7461696c77696e642d637373266c6f676f436f6c6f723d7768697465" alt="TailwindCSS" data-canonical-src="https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?style=for-the-badge&amp;logo=tailwind-css&amp;logoColor=white"></a>
  </p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📋 Overview</h2><a id="user-content--overview" aria-label="Permalink: 📋 Overview" href="#-overview"></a></p>
<p dir="auto">Scraperr enables you to extract data from websites with precision using XPath selectors. This self-hosted application provides a clean interface to manage scraping jobs, view results, and export data.</p>
<blockquote>
<p dir="auto">📚 <strong><a href="https://scraperr-docs.pages.dev/" rel="nofollow">Check out the docs</a></strong> for a comprehensive quickstart guide and detailed information.</p>
</blockquote>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/jaypyles/www-scrape/blob/master/docs/main_page.png"><img src="https://github.com/jaypyles/www-scrape/raw/master/docs/main_page.png" alt="Scraperr Main Interface" width="800px"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Key Features</h2><a id="user-content--key-features" aria-label="Permalink: ✨ Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>XPath-Based Extraction</strong>: Precisely target page elements</li>
<li><strong>Queue Management</strong>: Submit and manage multiple scraping jobs</li>
<li><strong>Domain Spidering</strong>: Option to scrape all pages within the same domain</li>
<li><strong>Custom Headers</strong>: Add JSON headers to your scraping requests</li>
<li><strong>Media Downloads</strong>: Automatically download images, videos, and other media</li>
<li><strong>Results Visualization</strong>: View scraped data in a structured table format</li>
<li><strong>Data Export</strong>: Export your results in various formats</li>
<li><strong>Notifcation Channels</strong>: Send completion notifcations, through various channels</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Getting Started</h2><a id="user-content--getting-started" aria-label="Permalink: 🚀 Getting Started" href="#-getting-started"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">⚖️ Legal and Ethical Guidelines</h2><a id="user-content-️-legal-and-ethical-guidelines" aria-label="Permalink: ⚖️ Legal and Ethical Guidelines" href="#️-legal-and-ethical-guidelines"></a></p>
<p dir="auto">When using Scraperr, please remember to:</p>
<ol dir="auto">
<li><strong>Respect <code>robots.txt</code></strong>: Always check a website's <code>robots.txt</code> file to verify which pages permit scraping</li>
<li><strong>Terms of Service</strong>: Adhere to each website's Terms of Service regarding data extraction</li>
<li><strong>Rate Limiting</strong>: Implement reasonable delays between requests to avoid overloading servers</li>
</ol>
<blockquote>
<p dir="auto"><strong>Disclaimer</strong>: Scraperr is intended for use only on websites that explicitly permit scraping. The creator accepts no responsibility for misuse of this tool.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <a href="https://github.com/jaypyles/Scraperr/blob/master/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">👏 Contributions</h2><a id="user-content--contributions" aria-label="Permalink: 👏 Contributions" href="#-contributions"></a></p>
<p dir="auto">Development made easier with the <a href="https://github.com/jaypyles/webapp-template">webapp template</a>.</p>
<p dir="auto">To get started, simply run <code>make build up-dev</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Car companies are in a billion-dollar software war (395 pts)]]></title>
            <link>https://insideevs.com/features/759153/car-companies-software-companies/</link>
            <guid>43955525</guid>
            <pubDate>Sun, 11 May 2025 17:51:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insideevs.com/features/759153/car-companies-software-companies/">https://insideevs.com/features/759153/car-companies-software-companies/</a>, See on <a href="https://news.ycombinator.com/item?id=43955525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                    
                        <p>Ford announced this week that&nbsp;<a href="https://www.reuters.com/business/autos-transportation/ford-kills-project-develop-tesla-like-electronic-brain-2025-04-30/" target="_blank" rel="noopener noreferrer">it is merging FNV4</a>, its project to develop a next-generation electrical architecture designed to unlock new functionality and upgradeability for both EVs and internal-combustion cars, with its existing architecture, seemingly confirming reports that the project has not gone as planned. It’s another in a long, long line of false starts in the race for legacy automakers to develop true “<a href="https://insideevs.com/features/755649/software-defined-vehicle-explainer-101/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22755649%22%2C%22section%22%3A%223%22%2C%22alias%22%3A%22software-defined-vehicle-explainer-101%22%7D">software-defined vehicles</a>” (SDVs), and further proof of just how hard the task is.</p>
<p>It’ll be worth the effort. Software-first architectures reduce costs, increase flexibility and—perhaps most importantly—allow automakers to move faster, without being slowed down by a complex web of suppliers creating their own code. Though becoming a leader in SDVs will be just as challenging as becoming an electric vehicle powerhouse, it’s also equally important. Consumers have had it with clunky, slow automotive technology, and the modern car is so computerized that a seamless electronic interface is an absolute necessity.</p>
<p>None of the legacy automakers has solved this problem yet. All of them have thrown billions at it. Only some of them will complete the transformation.</p>

<h2><span><svg><use xlink:href=""></use></svg></span>What Is A Software-Defined Vehicle?</h2>
<p><a href="https://insideevs.com/tesla/" data-inline-widget="internal-links">Tesla</a> invented the software-defined vehicle with the launch of the original <a href="https://insideevs.com/tesla/model-s/" data-inline-widget="internal-links">Model S</a>. While previous cars had plenty of software onboard, they all used a different approach. Electronic control units (ECUs) with supplier-sourced software would control groups of features or individual modules. Cars would have one computer handling, say, HVAC systems, and another for the lighting. Each of these computers was networked via <a href="https://www.csselectronics.com/pages/can-bus-simple-intro-tutorial" target="_blank" rel="noopener noreferrer">CAN bus</a>, an old-school network with limited bandwidth.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="special_image" data-align="center" data-source="{&quot;source_id&quot;:&quot;895&quot;,&quot;title&quot;:&quot;Tesla&quot;}"><span>
    <svg>
        <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
    </svg>
</span> <p><a href="https://cdn.motor1.com/images/custom/tesla-model-s-interior.jpeg"> <img draggable="false" src="https://cdn.motor1.com/images/custom/thumbnail/tesla-model-s-interior.jpeg" alt="An original Tesla Model S interior at night" width="1420" height="752" loading="lazy"> </a></p> <p>This was the original software-defined vehicle, the Tesla Model S.</p> <p>Photo by: Tesla</p> </section>
<p>Updates were done at dealers, and since that process was expensive, automakers only updated things when they had to, for safety, security or reliability reasons. This meant that software had to be fully validated and finalized before the product entered production, according to Sam Abuelsamid, vice president of market research at auto intel firm Telemetry.&nbsp;</p>
<p>But Tesla changed the game. The Model S was designed from the get-go to be updatable over-the-air, and since it didn’t rely on legacy suppliers, it used far fewer ECUs than its competitors. Most software duties were handled by a centralized computer, with only certain safety-critical systems getting their own computers. This simplified wiring, reduced production costs and allowed Tesla to make the car better over time.</p>
<p>The updatability was both broad and deep. One example Abuesamid offers: When the <a href="https://insideevs.com/tesla/model-3/" data-inline-widget="internal-links">Model 3</a> first came out, it took far too long to stop in <a href="https://www.consumerreports.org/hybrids-evs/tesla-model-3-review-falls-short-of-consumer-reports-recommendation/#:~:text=Editor's%20Note%3A%20In%20response%20to,more%20affordable%20mass%2Dmarket%20car." target="_blank" rel="noopener noreferrer">Consumer Reports testing</a>, thanks to bad anti-lock braking system (ABS) calibration. Tesla was able to fix this with a software update over the air, something no one else could do for a braking system. That was impressive, but the example presented a worrying question: Did engineers not do stopping-distance testing before they shipped the car to customers?&nbsp;&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="5917237">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/WmQrj/s5/2018-tesla-model-3.webp 213w,
            https://cdn.motor1.com/images/mgl/WmQrj/s6/2018-tesla-model-3.webp 445w,
            https://cdn.motor1.com/images/mgl/WmQrj/s4/2018-tesla-model-3.webp 889w,
            https://cdn.motor1.com/images/mgl/WmQrj/s3/2018-tesla-model-3.webp 1280w,
            https://cdn.motor1.com/images/mgl/WmQrj/s2/2018-tesla-model-3.webp 1440w,
            https://cdn.motor1.com/images/mgl/WmQrj/s1/2018-tesla-model-3.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/WmQrj/s5/2018-tesla-model-3.jpg 213w,
            https://cdn.motor1.com/images/mgl/WmQrj/s6/2018-tesla-model-3.jpg 445w,
            https://cdn.motor1.com/images/mgl/WmQrj/s4/2018-tesla-model-3.jpg 889w,
            https://cdn.motor1.com/images/mgl/WmQrj/s3/2018-tesla-model-3.jpg 1280w,
            https://cdn.motor1.com/images/mgl/WmQrj/s2/2018-tesla-model-3.jpg 1440w,
            https://cdn.motor1.com/images/mgl/WmQrj/s1/2018-tesla-model-3.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="2018 Tesla Model 3" width="16" height="9" loading="lazy">
    </picture>
    <p>The Tesla Model 3 brought the SDV to a mass-market audience, but its early quality issues show why the move-fast and fix-it-later approach can be problematic.</p>
    
</section>
<p>Thus, the double-edged sword of SDVs. They are more upgradeable and flexible than their predecessors, but that advantage allows companies to deliver under-baked software with a “fix it later” approach.</p>
<h2><span><svg><use xlink:href=""></use></svg></span>Teething Issues</h2>
<p>Evidence of that dichotomy is not hard to find. As automakers have introduced vehicles with more advanced computing and electrical architectures, they have also struggled to deliver bug-free software on time.</p>
<p>General Motors <a href="https://insideevs.com/features/709703/gm-ultium-problems-software-batteries/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22709703%22%2C%22section%22%3A%223%22%2C%22alias%22%3A%22gm-ultium-problems-software-batteries%22%7D">provided America’s most salient example</a>. Its <a href="https://www.theautopian.com/a-history-of-gm-software-and-how-a-new-quality-lab-aims-to-fix-the-companys-recent-problems/" target="_blank" rel="noopener noreferrer">Vehicle Intelligence Platform</a> (VIP) uses a CAN bus system alongside ethernet connections. That allows for more networked features, including things like Super Cruise hands-free driving and key modules that can be updated over-the-air. But troubles with the platform also ruined the launch of products like the <a href="https://insideevs.com/gmc/hummer-ev/" data-inline-widget="internal-links">Hummer EV</a>, <a href="https://insideevs.com/reviews/729569/cadillac-lyriq-ev-review-2024/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22729569%22%2C%22section%22%3A%222%22%2C%22alias%22%3A%22cadillac-lyriq-ev-review-2024%22%7D">Cadillac Lyriq</a> and <a href="https://insideevs.com/chevrolet/blazer-ev/" data-inline-widget="internal-links">Chevy Blazer EV</a>. All of them were wracked with software issues, <a href="https://insideevs.com/reviews/701169/2024-blazer-ev-stranded-broken/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22701169%22%2C%22section%22%3A%222%22%2C%22alias%22%3A%222024-blazer-ev-stranded-broken%22%7D">one of which stranded our man Kevin Williams</a>. And a truly centralized platform remains on the horizon.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="related-content" data-widget-size="content" data-params="%7B%22type_id%22%3A0%2C%22title_id%22%3A%22%22%2C%22items%22%3A%5B%7B%22article_edition_id%22%3A%22709703%22%2C%22title%22%3A%22How%20GM's%20Ultium%20Electric%20Car%20Revolution%20Went%20Off%20The%20Rails%22%2C%22alias%22%3A%22gm-ultium-problems-software-batteries%22%2C%22section%22%3A%223%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2F9myYZ1%2Fs5%2Fgm-ultium-top-3.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22701169%22%2C%22title%22%3A%22The%202024%20Chevrolet%20Blazer%20EV%20Left%20Me%20Stranded%20In%20Rural%20Virginia%22%2C%22alias%22%3A%222024-blazer-ev-stranded-broken%22%2C%22section%22%3A%222%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2F0eNXkj%2Fs5%2Fblazer-ev-trip-1.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22753673%22%2C%22title%22%3A%22VW%20Group%E2%80%99s%20Cariad%20Software%20Division%20Had%20A%20Bad%20Year.%20Again%22%2C%22alias%22%3A%22vw-group-cariad-billions-losses-2024%22%2C%22section%22%3A%221%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2FZnKqeo%2Fs5%2Fvolkswagen-id.7-interior-prototype.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22729645%22%2C%22title%22%3A%22A%20Volvo%20EX30%20Impressed%20This%20Reviewer.%20Then%20It%20Suddenly%20Died%22%2C%22alias%22%3A%22volvo-ex30-dies-during-review%22%2C%22section%22%3A%222%22%2C%22is_video%22%3A%221%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2FNGeRXG%2Fs5%2Fral43bcr3te.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22755649%22%2C%22title%22%3A%22From%20Tesla%20To%20Toyota%3A%20How%20Software%20Is%20Hitting%20Reset%20On%20Cars%22%2C%22alias%22%3A%22software-defined-vehicle-explainer-101%22%2C%22section%22%3A%223%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2F8Ax3Ae%2Fs5%2Fsdv-top-1.jpg%22%7D%7D%2C%7B%22article_edition_id%22%3A%22752270%22%2C%22title%22%3A%22Rivian%20Will%20Unlock%20More%20Range%20For%20The%20R1S%20And%20R1T%20Via%20A%20Paid%20Software%20Upgrade%22%2C%22alias%22%3A%22rivian-paid-range-unlock-performance-upgrade%22%2C%22section%22%3A%221%22%2C%22is_video%22%3A%220%22%2C%22images%22%3A%7B%22s5%22%3A%22https%3A%2F%2Fcdn.motor1.com%2Fimages%2Fmgl%2F3W43eK%2Fs5%2F2025-rivian-r1t-review.jpg%22%7D%7D%5D%7D"> <p>More Software Stories</p>  </section>
<p>Volvo also struggled with scaling its platform. It’s proud of the fact that the <a href="https://insideevs.com/volvo/ex90/" data-inline-widget="internal-links">EX90</a> and <a href="https://insideevs.com/news/740089/volvo-ex30-us-debut-price/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22740089%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22volvo-ex30-us-debut-price%22%7D">EX30</a> are true software-defined EVs. They have a centralized computing system, a simpler electrical architecture and a software stack that is designed to get better continuously. But delivering it was a nightmare. The company delayed the EX30 and EX90, and then shipped them with <a href="https://insideevs.com/reviews/729645/volvo-ex30-dies-during-review/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22729645%22%2C%22section%22%3A%222%22%2C%22alias%22%3A%22volvo-ex30-dies-during-review%22%7D">plenty of bugs</a> and <a href="https://insideevs.com/reviews/731827/2025-volvo-ex90-first-drive/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22731827%22%2C%22section%22%3A%222%22%2C%22alias%22%3A%222025-volvo-ex90-first-drive%22%7D">missing features</a>. Still, the Volvo EX30, EX90 and related Polestar 4 are SDVs you can buy in the U.S. today.</p>
<p>Ford’s FNV4 was its next big move, and according to Abuelsamid, it was supposed to be out already. The platform was going to underpin vehicles like Ford’s planned electric three-row SUV, which the company <a href="https://insideevs.com/news/730891/ford-three-row-electric-suv-dead/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22730891%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22ford-three-row-electric-suv-dead%22%7D">abruptly canceled last year.</a> According to Abuelsamid, the software wasn’t ready. Now, some improvements from FNV4 are being folded into Ford’s existing FNV3 architecture.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="8026107">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/1ZEMxL/s5/chevy-blazer-ev-long-term-owner-review.webp 213w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s6/chevy-blazer-ev-long-term-owner-review.webp 445w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s4/chevy-blazer-ev-long-term-owner-review.webp 889w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s3/chevy-blazer-ev-long-term-owner-review.webp 1280w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s2/chevy-blazer-ev-long-term-owner-review.webp 1440w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s1/chevy-blazer-ev-long-term-owner-review.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/1ZEMxL/s5/chevy-blazer-ev-long-term-owner-review.jpg 213w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s6/chevy-blazer-ev-long-term-owner-review.jpg 445w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s4/chevy-blazer-ev-long-term-owner-review.jpg 889w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s3/chevy-blazer-ev-long-term-owner-review.jpg 1280w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s2/chevy-blazer-ev-long-term-owner-review.jpg 1440w,
            https://cdn.motor1.com/images/mgl/1ZEMxL/s1/chevy-blazer-ev-long-term-owner-review.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="Chevy Blazer EV Long term owner review" width="16" height="9" loading="lazy">
    </picture>
    <p>My Blazer EV has gotten a few software updates since I got it eight months ago, but one of them still had to happen at the dealer.</p>
    <p>Photo by: Mack Hogan/InsideEVs</p>
</section>
<p>Still, Abuelsamid says Ford’s current position is “way better than it was two years ago.”</p>
<p>Then there’s Volkswagen, which made a big bet on software development with its captive software arm, Cariad. Despite the early start and plenty of money, though, <a href="https://insideevs.com/news/753673/vw-group-cariad-billions-losses-2024/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22753673%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22vw-group-cariad-billions-losses-2024%22%7D">Cariad was a disaster</a>. VW tried to <a href="https://insideevs.com/news/693654/volkswagen-group-cariad-job-cuts/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22693654%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22volkswagen-group-cariad-job-cuts%22%7D">reshuffle the company a few times</a> and utterly failed to turn it around. Now, with its SDV plans way off track, VW is outsourcing key software tasks to Mobileye, Chinese partners and Rivian. It is, as Abuelsamid put it, a “clusterf—.”</p>
<p><a href="https://insideevs.com/news/740871/rivian-volkswagen-software-venture-cm/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22740871%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22rivian-volkswagen-software-venture-cm%22%7D">The Rivian deal</a> will at least allow the company to get its own “<a href="https://insideevs.com/features/724945/zonal-architecture-software-define-electric-car/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22724945%22%2C%22section%22%3A%223%22%2C%22alias%22%3A%22zonal-architecture-software-define-electric-car%22%7D">zonal architecture</a>,” the latest buzzword that refers to a new, more efficient way to lay out electrical architecture in SDVs. But Volkswagen needs to learn how to do this itself, so relyihng on others isn’t the best option in the long run.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="7813227">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/eoOkbW/s5/rivian-zonal-architecture.webp 213w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s6/rivian-zonal-architecture.webp 445w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s4/rivian-zonal-architecture.webp 889w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s3/rivian-zonal-architecture.webp 1280w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s2/rivian-zonal-architecture.webp 1440w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s1/rivian-zonal-architecture.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/eoOkbW/s5/rivian-zonal-architecture.jpg 213w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s6/rivian-zonal-architecture.jpg 445w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s4/rivian-zonal-architecture.jpg 889w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s3/rivian-zonal-architecture.jpg 1280w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s2/rivian-zonal-architecture.jpg 1440w,
            https://cdn.motor1.com/images/mgl/eoOkbW/s1/rivian-zonal-architecture.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="Rivian Zonal Architecture" width="16" height="9" loading="lazy">
    </picture>
    <p>"Zonal architectures" are the next big thing in electrical system design, allowing companies to reduce complexity by running almost all functions through one of a few core computers distributed throughout the vehicle.</p>
    
</section>
<p>“It’s not an ideal solution,” Abuelsamid said.</p>
<p>As for everyone else, they’re all a few years away from having any proper SDVs on the road. Stellantis has been cautious with its rollout, but it’s working hard on its STLA Brain project. BMW is promising a generational leap in <a href="https://insideevs.com/news/746351/bmw-panoramic-idrive-vision-more-screens/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22746351%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22bmw-panoramic-idrive-vision-more-screens%22%7D">SDV tech with its Neue Class</a> line of Vehicles. Mercedes has its true SDV platform launching on the CLA later this year, even if that car is looking a little less exciting after <a href="https://insideevs.com/news/758859/mercedes-cla-ev-400v-charging-impossible/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22758859%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22mercedes-cla-ev-400v-charging-impossible%22%7D">this week’s news</a>.</p>
<p>Japanese and Korean companies remain further behind. Hyundai and Kia are transitioning to more software-driven vehicles, but their existing products feel a generation behind what GM’s doing and two generations behind Tesla. Toyota has brought much of its software team in-house and set up a major operation in the U.S., but is far from offering a true SDV. Honda’s existing products are legacy through and through, but it says the 0 Series EVs will use a <a href="https://insideevs.com/news/746798/honda-0-series-ai-driving/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22746798%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22honda-0-series-ai-driving%22%7D">true SDV platform with AI integration</a> and a smart assistant, Asimo.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="8001491">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/wlqN1Q/s5/06-asimo-os-copy.webp 213w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s6/06-asimo-os-copy.webp 445w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s4/06-asimo-os-copy.webp 889w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s3/06-asimo-os-copy.webp 1280w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s2/06-asimo-os-copy.webp 1440w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s1/06-asimo-os-copy.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/wlqN1Q/s5/06-asimo-os-copy.jpg 213w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s6/06-asimo-os-copy.jpg 445w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s4/06-asimo-os-copy.jpg 889w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s3/06-asimo-os-copy.jpg 1280w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s2/06-asimo-os-copy.jpg 1440w,
            https://cdn.motor1.com/images/mgl/wlqN1Q/s1/06-asimo-os-copy.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="06 ASIMO OS copy" width="16" height="9" loading="lazy">
    </picture>
    <p>Honda's 0 Series Saloon concept previews its upcoming software-defined vehicle.</p>
    <p>Photo by: Honda</p>
</section>
<p>All of these future products will likely have teething issues. Because even with deep pockets, this is not an easy transition. But automakers need to push through if they want to create affordable, upgradeable EVs with streamlined user experiences. As customers of Rivian, Tesla and many Chinese brands have proven, once you use a true <a href="https://insideevs.com/news/677548/brand-loyalty-is-dying-but-not-tesla/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22677548%22%2C%22section%22%3A%221%22%2C%22alias%22%3A%22brand-loyalty-is-dying-but-not-tesla%22%7D">SDV you rarely go back to a legacy product</a>. It’s a smoother, more modern experience.</p>
<h2><span><svg><use xlink:href=""></use></svg></span>Why It’s So Hard</h2>
<p>From the outside, it may be hard to understand why creating a software-defined vehicle is still hard in 2025. These legacy companies have poached big hitters from Apple, Tesla and Google. They’ve sunk billions into it. They have decades of experience with software-controlled modules. From the clouds, it all seems doable.</p>
<p>Yet on the ground, it’s a nightmare. Automakers are not redesigning software. They are reworking how their entire organizations approach software.</p>
<p>These are companies that have typically seen software as a problem to be solved, not a design to be experienced. Engineers at GM, Ford and Toyota have spent decades using an approach to software that was silo’d, minimally tolerant of risk and designed to remain unchanged through the vehicle’s 20-year life cycle.</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="8075677">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/BXyyYM/s5/hyundai-motor-group-s-pleos-software-brand.webp 213w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s6/hyundai-motor-group-s-pleos-software-brand.webp 445w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s4/hyundai-motor-group-s-pleos-software-brand.webp 889w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s3/hyundai-motor-group-s-pleos-software-brand.webp 1280w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s2/hyundai-motor-group-s-pleos-software-brand.webp 1440w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s1/hyundai-motor-group-s-pleos-software-brand.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/BXyyYM/s5/hyundai-motor-group-s-pleos-software-brand.jpg 213w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s6/hyundai-motor-group-s-pleos-software-brand.jpg 445w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s4/hyundai-motor-group-s-pleos-software-brand.jpg 889w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s3/hyundai-motor-group-s-pleos-software-brand.jpg 1280w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s2/hyundai-motor-group-s-pleos-software-brand.jpg 1440w,
            https://cdn.motor1.com/images/mgl/BXyyYM/s1/hyundai-motor-group-s-pleos-software-brand.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="Hyundai Motor Group's Pleos software brand" width="16" height="9" loading="lazy">
    </picture>
    <p>Hyundai will launch a new software experience under the "Pleos Connect" brand.</p>
    <p>Photo by: Hyundai</p>
</section>
<p>Now, they need to make compelling apps, slick new features and all-new electrical architectures that neither the companies nor their suppliers are used to using. They need to build Tesla-level upgradeability with far less willingness to ship unfinished goods, all while tucking it behind a military-grade firewall to ensure your car can’t be remotely hacked.</p>
<p>“The challenging part is these companies need to merge their development philosophies,” Abuelsamid says, keeping the safety-first mindset while also getting faster, more creative and bolder.</p>
<p>Plus, as Ford SDV project boss <a href="https://www.theverge.com/ford-motor-company/661205/ford-doug-field-interview-software-zonal-domain-fnv4" target="_blank" rel="noopener noreferrer">Doug Field tells <em>The Verge</em></a>, slower-than-expected EV sales mean they need to build systems that can also work with internal-combustion vehicles. That’s a major challenge, as EVs have a big battery that can power their electronics round-the-clock, enabling big over-the-air updates. When you only have a puny 12-volt to keep things moving, you’re going to run out of juice pretty quick.&nbsp;&nbsp;</p>
<p>Finally, they have to navigate all of these issues while not scaring off customers. GM has been more aggressive and, I’d argue, successful with SDVs than competitors, but <a href="https://insideevs.com/reviews/726471/gm-carplay-cadillac-lyriq-review/" data-inline-widget="internal-links" data-type-id="0" data-params="%7B%22article_edition_id%22%3A%22726471%22%2C%22section%22%3A%222%22%2C%22alias%22%3A%22gm-carplay-cadillac-lyriq-review%22%7D">its anti-Apple CarPlay</a> stance has also scared off plenty of buyers. Plus, as automakers have moved more controls into their central displays, they’ve faced more consumer complaints and confusion.</p>
<h2><span><svg><use xlink:href=""></use></svg></span>So Who Wins?</h2>
<p>The clear leaders here are the companies that weren’t already locked into the old-world approach to automotive software. Tesla, Rivian, Lucid and almost all of the Chinese automakers have built ground-up systems that work without legacy bloat.&nbsp;</p>
<section contenteditable="false" draggable="true" data-widget="image" data-border="" data-id="7501943">
        <span>
            <svg>
                <use xlink:href="https://insideevs.com/design/dist/critical/icons/sprite-common-0-c22b9e1b69b591f36313a4760a61b994.svg#semidir"></use>
            </svg>
        </span>
    <picture>
        <source type="image/webp" srcset="
            https://cdn.motor1.com/images/mgl/7Zv14q/s5/rivian-software-updates.webp 213w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s6/rivian-software-updates.webp 445w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s4/rivian-software-updates.webp 889w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s3/rivian-software-updates.webp 1280w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s2/rivian-software-updates.webp 1440w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s1/rivian-software-updates.webp 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <source type="image/jpeg" srcset="
            https://cdn.motor1.com/images/mgl/7Zv14q/s5/rivian-software-updates.jpg 213w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s6/rivian-software-updates.jpg 445w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s4/rivian-software-updates.jpg 889w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s3/rivian-software-updates.jpg 1280w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s2/rivian-software-updates.jpg 1440w,
            https://cdn.motor1.com/images/mgl/7Zv14q/s1/rivian-software-updates.jpg 1920w
        " sizes="(max-width: 767px) calc(100vw - 24px), (max-width: 1023px) calc(100vw - 48px), 644px">
        <img src="https://cdn.motor1.com/images/static/16x9-tr.png" alt="Rivian software updates" width="16" height="9" loading="lazy">
    </picture>
    <p>Rivian's software may not be as great as Tesla's, but it's far ahead of legacy automaker tech.</p>
    
</section>
<p>Everyone else is having to stumble their way towards that future, with varying levels of success. GM had the most high-profile struggles, but like BMW with early iDrive, the company’s willingness to stick it out seems to have produced an organization that’s ahead of its legacy peers on software. But it remains to be seen whether the company can take its approach further down the path to true SDVs.</p>
<p>BMW and Mercedes are close to launching theirs, while the fruits of the VW-Rivian deal and Ford’s “skunkworks” next-gen EV project remain further out. As for the Korean and Japanese companies, all of the hard work is still ahead of them.</p>
<p>Tesla proved that a software-defined vehicle was a viable and desirable product. Chinese automakers proved that the software-first approach could be replicated and improved upon. Now, we need a legacy automaker to prove that you can turn a century-old hardware brand into a true software company. It’s possible, but we know now that it won’t be easy.</p>
<p>Contact the author: <a href="mailto:Mack.Hogan@insideevs.com">Mack.Hogan@insideevs.com</a>.&nbsp;</p>                        <!-- new gallery place, attached gallery -->
                        
                                                                            
                                                    

                                                    

                                                            
                                                        
                            
                            
                                                            
                                
                            
                                
                            
                                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LSP client in Clojure in 200 lines of code (154 pts)]]></title>
            <link>https://vlaaad.github.io/lsp-client-in-200-lines-of-code</link>
            <guid>43955397</guid>
            <pubDate>Sun, 11 May 2025 17:38:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vlaaad.github.io/lsp-client-in-200-lines-of-code">https://vlaaad.github.io/lsp-client-in-200-lines-of-code</a>, See on <a href="https://news.ycombinator.com/item?id=43955397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><time datetime="2025-05-10 00:00:00 +0000">May 10, 2025</time>
    
  </p>

  
  

  <p>Awhile ago I was prototyping integrating LLMs with <a href="https://microsoft.github.io/language-server-protocol/">LSP</a> to enable a language model to answer questions about code while having access to code navigation tools provided by language servers. I wasn’t that successful with this prototype, but I found it cool that I could write a minimal LSP client in around 200 lines of code. Of course, it was very helpful that I previously wrote a much more featureful <a href="https://github.com/defold/defold/blob/dev/editor/src/clj/editor/lsp.clj">LSP client for the Defold editor</a>… So let me share with you a minimal LSP client, written in Clojure, in under 200 lines. Also, at the end of the post, I’ll share my thoughts on the LSP.</p>

<p>Who is the target audience of this blog post? I don’t even know… Clojure developers writing code editors? There are, like, 3 of us! Okay, let’s try to change the scope of this exercise a bit: let’s build a command line linter that uses a language server to do the work. Surely that wouldn’t be a problem…</p>

<h2 id="the-what">The what</h2>

<p>Some terminology and scope first. LSP stands for Language Server Protocol, a standard that defines how some text editor (a language client) should talk to some language-specific tool (a language server) that knows the semantics of a programming language and may provide contextual information like code navigation, refactoring, linting etc.</p>

<p>The main benefit of LSP is that the so called MxN problem of IDEs and languages becomes M+N with LSP. <a href="https://langserver.org/">Here is a good explanation</a>. In short, as a language author, previously you had to write integration for every code editor. Or, as an IDE author, you had to write a separate integration for every language. Now there is a common interface — LSP — and both language authors and IDE authors only need to support this interface.</p>

<p>In 200 LoC, we will implement essential blocks of the <a href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/">LSP Specification</a> that supports programmatic read-only querying of language servers. We will implement:</p>
<ol>
  <li>base communication layer between language client and server processes. It is similar to HTTP protocol: client and server talk to each other using byte streams with messages formatted as headers + JSON message bodies. The base layer establishes a way to exchange JSON blobs.</li>
  <li><a href="https://www.jsonrpc.org/">JSON-RPC</a> — a layer on top of the base layer that adds meaning to JSON blobs, turning them into either requests/responses, or notifications.</li>
  <li>A wrapper around JSON-RPC connection that is a leaving breathing language server we can talk to.</li>
</ol>

<p>We will use Java 24 with <a href="https://openjdk.org/jeps/425">virtual threads</a>: writing blocking code that performs and scales well is nice, sweet, and performant. Now, here are few things we will not implement:</p>
<ul>
  <li>JSON parser. I mean come on. We will just use a dependency. I picked <a href="https://github.com/metosin/jsonista/">jsonista</a> because it’s fast and has a cool name.</li>
  <li>Document syncing. When the user opens a file in a text editor and makes some changes to it without saving, the editor notifies running language servers about the new text of the open files. We are not building a text editor here, just a small PoC, so we’ll skip this.</li>
</ul>

<p>Now, Let’s go!</p>

<h2 id="the-how">The how</h2>

<p>If you just want to look at the code, <a href="https://github.com/vlaaad/lsp-clj-client/blob/main/src/io/github/vlaaad/lsp.clj">here it is</a>. Now I’ll walk you through it.</p>

<h2 id="base-layer">Base layer</h2>

<p>First, we start with a base communication layer. Language server runs in another process, so the communication happens over InputStream + OutputStream pair. We will run the language server as a process and we will communicate via stdin/stdout, so java Process will provide us the pair. Both client and server send and receive HTTP-like requests with JSON blobs. Each individual message looks like this:</p>
<div><pre><code>Content-Length: 14\r\n
\r\n
{"json": true}
</code></pre></div>
<p>First, there are 1 or more headers with a required <code>Content-Length</code> header, separated with <code>\r\n</code>. Then, an empty line. Then comes a JSON string. The headers are serialized using ASCII encoding (so 1 byte is always 1 char), the JSON blob uses UTF-8.</p>

<p>We start with <a href="https://github.com/vlaaad/lsp-clj-client/blob/main/src/io/github/vlaaad/lsp.clj#L12-L23">a function that reads a line of ascii text from InputStream</a>:</p>
<div><pre><code><span>(</span><span>defn-</span><span> </span><span>read-ascii-line</span><span> </span><span>[</span><span>^</span><span>InputStream</span><span> </span><span>in</span><span>]</span><span>
  </span><span>(</span><span>let</span><span> </span><span>[</span><span>sb</span><span> </span><span>(</span><span>StringBuilder.</span><span>)]</span><span>
    </span><span>(</span><span>loop</span><span> </span><span>[</span><span>carriage-return</span><span> </span><span>false</span><span>]</span><span>
      </span><span>(</span><span>let</span><span> </span><span>[</span><span>ch</span><span> </span><span>(</span><span>.read</span><span> </span><span>in</span><span>)]</span><span>
        </span><span>(</span><span>if</span><span> </span><span>(</span><span>=</span><span> </span><span>-1</span><span> </span><span>ch</span><span>)</span><span>
          </span><span>(</span><span>if</span><span> </span><span>(</span><span>zero?</span><span> </span><span>(</span><span>.length</span><span> </span><span>sb</span><span>))</span><span> </span><span>nil</span><span> </span><span>(</span><span>.toString</span><span> </span><span>sb</span><span>))</span><span>
          </span><span>(</span><span>let</span><span> </span><span>[</span><span>ch</span><span> </span><span>(</span><span>char</span><span> </span><span>ch</span><span>)]</span><span>
            </span><span>(</span><span>.append</span><span> </span><span>sb</span><span> </span><span>ch</span><span>)</span><span>
            </span><span>(</span><span>cond</span><span>
              </span><span>(</span><span>=</span><span> </span><span>ch</span><span> </span><span>\r</span><span>eturn</span><span>)</span><span> </span><span>(</span><span>recur</span><span> </span><span>true</span><span>)</span><span>
              </span><span>(</span><span>and</span><span> </span><span>carriage-return</span><span> </span><span>(</span><span>=</span><span> </span><span>ch</span><span> </span><span>\n</span><span>ewline</span><span>))</span><span> </span><span>(</span><span>.substring</span><span> </span><span>sb</span><span> </span><span>0</span><span> </span><span>(</span><span>-</span><span> </span><span>(</span><span>.length</span><span> </span><span>sb</span><span>)</span><span> </span><span>2</span><span>))</span><span>
              </span><span>:else</span><span> </span><span>(</span><span>recur</span><span> </span><span>false</span><span>))))))))</span><span>
</span></code></pre></div>
<p>So, we read characters byte by byte into a string until we get to <code>\r\n</code>. If we reached end of stream, we return <code>nil</code>. We can’t use BufferedReader’s <code>readLine</code> here for a few reasons:</p>
<ul>
  <li>it buffers, meaning it might read more than we want.</li>
  <li>it uses both <code>\n</code> and <code>\r\n</code> as line separators, while we only want <code>\r\n</code>.</li>
  <li>it uses a single encoding, while the communication channel uses a mix of ASCII and UTF-8.</li>
</ul>

<p>The next step is a single function that implements the whole <a href="https://github.com/vlaaad/lsp-clj-client/blob/main/src/io/github/vlaaad/lsp.clj#L25-L56">base communication layer</a>:</p>
<div><pre><code><span>(</span><span>defn-</span><span> </span><span>lsp-base</span><span> </span><span>[</span><span>^</span><span>InputStream</span><span> </span><span>in</span><span> </span><span>^</span><span>BlockingQueue</span><span> </span><span>server-in</span><span> </span><span>^</span><span>OutputStream</span><span> </span><span>out</span><span> </span><span>^</span><span>BlockingQueue</span><span> </span><span>server-out</span><span>]</span><span>
  </span><span>(</span><span>-&gt;</span><span> </span><span>(</span><span>Thread/ofVirtual</span><span>)</span><span>
      </span><span>(</span><span>.name</span><span> </span><span>"lsp-base-in"</span><span>)</span><span>
      </span><span>(</span><span>.start</span><span>
        </span><span>#</span><span>(</span><span>loop</span><span> </span><span>[]</span><span>
           </span><span>(</span><span>when-some</span><span> </span><span>[</span><span>headers</span><span> </span><span>(</span><span>loop</span><span> </span><span>[</span><span>acc</span><span> </span><span>{}]</span><span>
                                 </span><span>(</span><span>when-let</span><span> </span><span>[</span><span>line</span><span> </span><span>(</span><span>read-ascii-line</span><span> </span><span>in</span><span>)]</span><span>
                                   </span><span>(</span><span>if</span><span> </span><span>(</span><span>=</span><span> </span><span>""</span><span> </span><span>line</span><span>)</span><span>
                                     </span><span>acc</span><span>
                                     </span><span>(</span><span>if-let</span><span> </span><span>[[</span><span>_</span><span> </span><span>field</span><span> </span><span>value</span><span>]</span><span> </span><span>(</span><span>re-matches</span><span> </span><span>#</span><span>"^([^:]+):\s*(.+?)\s*$"</span><span> </span><span>line</span><span>)]</span><span>
                                       </span><span>(</span><span>recur</span><span> </span><span>(</span><span>assoc</span><span> </span><span>acc</span><span> </span><span>(</span><span>string/lower-case</span><span> </span><span>field</span><span>)</span><span> </span><span>value</span><span>))</span><span>
                                       </span><span>(</span><span>throw</span><span> </span><span>(</span><span>IllegalStateException.</span><span> </span><span>(</span><span>str</span><span> </span><span>"Can't parse header: "</span><span> </span><span>line</span><span>)))))))]</span><span>
             </span><span>(</span><span>let</span><span> </span><span>[</span><span>^</span><span>String</span><span> </span><span>content-length</span><span> </span><span>(</span><span>or</span><span> </span><span>(</span><span>get</span><span> </span><span>headers</span><span> </span><span>"content-length"</span><span>)</span><span>
                                              </span><span>(</span><span>throw</span><span> </span><span>(</span><span>IllegalStateException.</span><span> </span><span>"Required header missing: Content-Length"</span><span>)))</span><span>
                   </span><span>len</span><span> </span><span>(</span><span>Integer/valueOf</span><span> </span><span>content-length</span><span>)</span><span>
                   </span><span>bytes</span><span> </span><span>(</span><span>.readNBytes</span><span> </span><span>in</span><span> </span><span>len</span><span>)]</span><span>
               </span><span>(</span><span>if</span><span> </span><span>(</span><span>=</span><span> </span><span>(</span><span>alength</span><span> </span><span>bytes</span><span>)</span><span> </span><span>len</span><span>)</span><span>
                 </span><span>(</span><span>do</span><span> </span><span>(</span><span>.put</span><span> </span><span>server-in</span><span> </span><span>(</span><span>json/read-value</span><span> </span><span>(</span><span>String.</span><span> </span><span>bytes</span><span> </span><span>StandardCharsets/UTF_8</span><span>)</span><span> </span><span>json/keyword-keys-object-mapper</span><span>))</span><span>
                     </span><span>(</span><span>recur</span><span>))</span><span>
                 </span><span>(</span><span>throw</span><span> </span><span>(</span><span>IllegalStateException.</span><span> </span><span>"Couldn't read enough bytes"</span><span>))))))))</span><span>
  </span><span>(</span><span>-&gt;</span><span> </span><span>(</span><span>Thread/ofVirtual</span><span>)</span><span>
      </span><span>(</span><span>.name</span><span> </span><span>"lsp-base-out"</span><span>)</span><span>
      </span><span>(</span><span>.start</span><span>
        </span><span>#</span><span>(</span><span>while</span><span> </span><span>true</span><span>
           </span><span>(</span><span>let</span><span> </span><span>[</span><span>^</span><span>bytes</span><span> </span><span>message-bytes</span><span> </span><span>(</span><span>json/write-value-as-bytes</span><span> </span><span>(</span><span>.take</span><span> </span><span>server-out</span><span>))]</span><span>
             </span><span>(</span><span>doto</span><span> </span><span>out</span><span>
               </span><span>(</span><span>.write</span><span> </span><span>(</span><span>.getBytes</span><span> </span><span>(</span><span>str</span><span> </span><span>"Content-Length: "</span><span>
                                       </span><span>(</span><span>alength</span><span> </span><span>message-bytes</span><span>)</span><span>
                                       </span><span>"\r\nContent-Type: application/vscode-jsonrpc; charset=utf-8\r\n\r\n"</span><span>)</span><span>
                                  </span><span>StandardCharsets/UTF_8</span><span>))</span><span>
               </span><span>(</span><span>.write</span><span> </span><span>message-bytes</span><span>)</span><span>
               </span><span>(</span><span>.flush</span><span>)))))))</span><span>
</span></code></pre></div>
<p>This function converts the client/server communication from InputStream+OutputStream pair (bytes) to input+output BlockingQueues of json blobs. The <code>"lsp-base-in"</code> part reads headers from the InputStream, then reads a JSON object and finally puts it onto a <code>server-in</code> queue. This way, whenever a language server sends us something, we’ll get it as a JSON in a queue. The <code>"lsp-base-out"</code> is an inverse: it reads JSON objects from <code>server-out</code> and writes them to the server. This way, when we will want to send a message to the language server, we will only need to put a JSON value onto a <code>server-out</code> queue.</p>

<h2 id="json-rpc-layer">JSON-RPC layer</h2>

<p>LSP client and server exchange JSON blobs in a special format called <a href="https://www.jsonrpc.org/specification">JSON-RPC</a>. The main idea is to agree on the shape and meaning of the exchanged data so that exchanging JSON objects supports these use cases:</p>
<ol>
  <li>send a request to perform a specific action and receive a response for this request (aka “remote procedure call”)</li>
  <li>send a notification that does not expect a response</li>
</ol>

<p>This use case is achieved by exchanging JSON objects with special combinations of fields, i.e.:</p>
<ol>
  <li>to send a request, use a JSON object with fields <code>id</code> (request identifier) and <code>method</code> (action identifier). Optionally, you can provide <code>params</code>, i.e. an “argument” to the “method call”.</li>
  <li>to send a notification, use a request, but without the <code>id</code> field</li>
  <li>to respond to a request, send a JSON object with <code>id</code> of the received request, and either an <code>error</code> or a <code>result</code> field, depending on whether we got an error or a successfully produced a result. The error has to be an object with <code>code</code> and <code>message</code> fields.</li>
</ol>

<p>Now I’ll walk you through the implementation of JSON-RPC protocol, which happens to be <a href="https://github.com/vlaaad/lsp-clj-client/blob/main/src/io/github/vlaaad/lsp.clj#L58-L121">a single function</a>.</p>

<p>We start with this argument list:</p>
<div><pre><code><span>(</span><span>defn-</span><span> </span><span>lsp-jsonrpc</span><span> </span><span>[</span><span>^</span><span>BlockingQueue</span><span> </span><span>client-in</span><span> </span><span>^</span><span>BlockingQueue</span><span> </span><span>server-in</span><span> </span><span>^</span><span>BlockingQueue</span><span> </span><span>server-out</span><span> </span><span>handlers</span><span>]</span><span> 
  </span><span>...</span><span>)</span><span>
</span></code></pre></div>
<p><code>server-in</code> and <code>server-out</code> are the base layer of the LSP commucation. We will put JSON-RPC objects to <code>server-out</code> to send messages to the language server. We will read from <code>server-in</code> to receive language server JSON-RPC objects from the language servers. So, what are <code>client-in</code> and <code>handlers</code>?</p>

<p><code>client-in</code> is another queue that we will use to send requests and notifications to the language server. Our <code>lsp-jsonrpc</code> function will take objects from <code>client-in</code>, perform some pre-processing, and then will post the JSON-RPC objects to <code>server-out</code>. This will enable us to write a simple API to send messages to the language server.</p>

<p><code>handler</code> is a map from JSON-RPC “method name” to a function. When language server decides to notify us about something, or sends us a request, we will lookup a function to handle this notification in the <code>handlers</code> map. This enables us to respond to requests from language servers.</p>

<p>The next bit of code in the function “merges” <code>client-in</code> and <code>server-in</code> into a single queue (<code>in</code>):</p>
<div><pre><code><span>  </span><span>(</span><span>let</span><span> </span><span>[</span><span>in</span><span> </span><span>(</span><span>SynchronousQueue.</span><span>)]</span><span>
    </span><span>(</span><span>-&gt;</span><span> </span><span>(</span><span>Thread/ofVirtual</span><span>)</span><span>
        </span><span>(</span><span>.name</span><span> </span><span>"lsp-jsonrpc-client"</span><span>)</span><span>
        </span><span>(</span><span>.start</span><span> </span><span>#</span><span>(</span><span>while</span><span> </span><span>true</span><span> </span><span>(</span><span>.put</span><span> </span><span>in</span><span> </span><span>[</span><span>:client</span><span> </span><span>(</span><span>.take</span><span> </span><span>client-in</span><span>)]))))</span><span>
    </span><span>(</span><span>-&gt;</span><span> </span><span>(</span><span>Thread/ofVirtual</span><span>)</span><span>
        </span><span>(</span><span>.name</span><span> </span><span>"lsp-jsonrpc-server"</span><span>)</span><span>
        </span><span>(</span><span>.start</span><span> </span><span>#</span><span>(</span><span>while</span><span> </span><span>true</span><span> </span><span>(</span><span>.put</span><span> </span><span>in</span><span> </span><span>[</span><span>:server</span><span> </span><span>(</span><span>.take</span><span> </span><span>server-in</span><span>)]))))</span><span>
    </span><span>...</span><span>)</span><span>
</span></code></pre></div>
<p>Now, we can write a single sequential loop that take messages from <code>in</code> and handles both messages from “us”, i.e. the client, and “them”, i.e. remote language server. With virtual threads, this blocking code stays lightweight and performant. On a side note, I think the only reason for <a href="https://github.com/clojure/core.async/">core.async</a> to exist post JDK 24 is the observability tooling that <a href="https://clojure.github.io/core.async/flow.html">flow</a> provides. And, maybe, sliding buffers — AFAIK, there are no blocking alternatives to them in the JDK.</p>

<p>Okay, let’s move on. The next piece of code in the JSON-RPC implementation is the loop:</p>
<div><pre><code><span>    </span><span>(</span><span>-&gt;</span><span> </span><span>(</span><span>Thread/ofVirtual</span><span>)</span><span>
        </span><span>(</span><span>.name</span><span> </span><span>"lsp-jsonrpc"</span><span>)</span><span>
        </span><span>(</span><span>.start</span><span>
          </span><span>#</span><span>(</span><span>loop</span><span> </span><span>[</span><span>next-id</span><span> </span><span>0</span><span>
                  </span><span>requests</span><span> </span><span>{}]</span><span>
             </span><span>(</span><span>let</span><span> </span><span>[[</span><span>src</span><span> </span><span>message</span><span>]</span><span> </span><span>(</span><span>.take</span><span> </span><span>in</span><span>)]</span><span>
               </span><span>(</span><span>case</span><span> </span><span>src</span><span>
                  </span><span>...</span><span>)))))</span><span>
</span></code></pre></div>
<p>We start another lightweight process that handles incoming messages from both language server and client. We need <code>next-id</code> and <code>requests</code> to support sending requests and then handling the incoming responses to these requests. We are taking from <code>in</code>, so <code>src</code> is either <code>:client</code> or <code>:server</code>, and message is a JSON-RPC message. Now, let’s start handling stuff! First we’ll handle the <code>:client</code> case, i.e. messages that we send to the server:</p>
<div><pre><code><span>                 </span><span>:client</span><span> </span><span>(</span><span>let</span><span> </span><span>[</span><span>out-message</span><span> </span><span>(</span><span>cond-&gt;</span><span> </span><span>{</span><span>:jsonrpc</span><span> </span><span>"2.0"</span><span>
                                                    </span><span>:method</span><span> </span><span>(</span><span>:method</span><span> </span><span>message</span><span>)}</span><span>
                                             </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:params</span><span>)</span><span>
                                             </span><span>(</span><span>assoc</span><span> </span><span>:params</span><span> </span><span>(</span><span>:params</span><span> </span><span>message</span><span>)))]</span><span>
                           </span><span>(</span><span>if-let</span><span> </span><span>[</span><span>response-queue</span><span> </span><span>(</span><span>:response</span><span> </span><span>message</span><span>)]</span><span>
                             </span><span>(</span><span>do</span><span>
                               </span><span>(</span><span>.put</span><span> </span><span>server-out</span><span> </span><span>(</span><span>assoc</span><span> </span><span>out-message</span><span> </span><span>:id</span><span> </span><span>next-id</span><span>))</span><span>
                               </span><span>(</span><span>recur</span><span> </span><span>(</span><span>inc</span><span> </span><span>next-id</span><span>)</span><span> </span><span>(</span><span>assoc</span><span> </span><span>requests</span><span> </span><span>next-id</span><span> </span><span>response-queue</span><span>)))</span><span>
                             </span><span>(</span><span>do</span><span>
                               </span><span>(</span><span>.put</span><span> </span><span>server-out</span><span> </span><span>out-message</span><span>)</span><span>
                               </span><span>(</span><span>recur</span><span> </span><span>next-id</span><span> </span><span>requests</span><span>))))</span><span>
</span></code></pre></div>
<p>Remember, we need to support both notifications (don’t expect a response) and requests (need a response). We will differentiate between them by using <code>:response</code> key on the client messages. The value for the key is going to be a <code>BlockingQueue</code> — once we receive a response from the language server, we will put the response value onto this queue. If we are sending a response, we increment the <code>next-id</code> counter and store the queue that awaits a response in the in-flight <code>requests</code> map. If we are sending a notification, we simply send a JSON-RPC object and continue.</p>

<p>That’s it for the client! Now we handle incoming messages from server. There are 3 possible message types:</p>
<ol>
  <li>responses to our requests: those have an <code>id</code> and either <code>result</code> or <code>error</code>.</li>
  <li>notifications: those have <code>method</code>, but not <code>id</code></li>
  <li>requests: those have both <code>method</code> and <code>id</code></li>
</ol>

<p>Here is the <code>:server</code> case:</p>
<div><pre><code><span>                 </span><span>:server</span><span> </span><span>(</span><span>cond</span><span>
                           </span><span>;; response?</span><span>
                           </span><span>(</span><span>and</span><span> </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:id</span><span>)</span><span>
                                </span><span>(</span><span>or</span><span> </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:result</span><span>)</span><span>
                                    </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:error</span><span>)))</span><span>
                           </span><span>(</span><span>let</span><span> </span><span>[</span><span>id</span><span> </span><span>(</span><span>:id</span><span> </span><span>message</span><span>)</span><span>
                                 </span><span>^</span><span>BlockingQueue</span><span> </span><span>response-out</span><span> </span><span>(</span><span>get</span><span> </span><span>requests</span><span> </span><span>id</span><span>)]</span><span>
                             </span><span>(</span><span>.put</span><span> </span><span>response-out</span><span> </span><span>message</span><span>)</span><span>
                             </span><span>(</span><span>recur</span><span> </span><span>next-id</span><span> </span><span>(</span><span>dissoc</span><span> </span><span>requests</span><span> </span><span>id</span><span>)))</span><span>

                           </span><span>;; notification?</span><span>
                           </span><span>(</span><span>and</span><span> </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:method</span><span>)</span><span>
                                </span><span>(</span><span>not</span><span> </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:id</span><span>)))</span><span>
                           </span><span>(</span><span>do</span><span>
                             </span><span>(</span><span>when-let</span><span> </span><span>[</span><span>handler</span><span> </span><span>(</span><span>get</span><span> </span><span>handlers</span><span> </span><span>(</span><span>:method</span><span> </span><span>message</span><span>))]</span><span>
                               </span><span>(</span><span>handler</span><span> </span><span>(</span><span>:params</span><span> </span><span>message</span><span>)))</span><span>
                             </span><span>(</span><span>recur</span><span> </span><span>next-id</span><span> </span><span>requests</span><span>))</span><span>

                           </span><span>;; request?</span><span>
                           </span><span>(</span><span>and</span><span> </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:method</span><span>)</span><span>
                                </span><span>(</span><span>contains?</span><span> </span><span>message</span><span> </span><span>:id</span><span>))</span><span>
                           </span><span>(</span><span>do</span><span>
                             </span><span>(</span><span>.put</span><span>
                               </span><span>server-out</span><span>
                               </span><span>(</span><span>try</span><span>
                                 </span><span>{</span><span>:jsonrpc</span><span> </span><span>"2.0"</span><span>
                                  </span><span>:id</span><span> </span><span>(</span><span>:id</span><span> </span><span>message</span><span>)</span><span>
                                  </span><span>:result</span><span> </span><span>((</span><span>get</span><span> </span><span>handlers</span><span> </span><span>(</span><span>:method</span><span> </span><span>message</span><span>))</span><span> </span><span>(</span><span>:params</span><span> </span><span>message</span><span>))}</span><span>
                                 </span><span>(</span><span>catch</span><span> </span><span>Throwable</span><span> </span><span>e</span><span>
                                   </span><span>{</span><span>:jsonrpc</span><span> </span><span>"2.0"</span><span>
                                    </span><span>:id</span><span> </span><span>(</span><span>:id</span><span> </span><span>message</span><span>)</span><span>
                                    </span><span>:error</span><span> </span><span>{</span><span>:code</span><span> </span><span>-32603</span><span> </span><span>:message</span><span> </span><span>(</span><span>or</span><span> </span><span>(</span><span>ex-message</span><span> </span><span>e</span><span>)</span><span> </span><span>"Internal Error"</span><span>)}})))</span><span>
                             </span><span>(</span><span>recur</span><span> </span><span>next-id</span><span> </span><span>requests</span><span>))</span><span>

                           </span><span>:else</span><span>
                           </span><span>(</span><span>do</span><span>
                             </span><span>(</span><span>.put</span><span> </span><span>server-out</span><span> </span><span>{</span><span>:jsonrpc</span><span> </span><span>"2.0"</span><span> </span><span>:id</span><span> </span><span>(</span><span>:id</span><span> </span><span>message</span><span>)</span><span> </span><span>:error</span><span> </span><span>{</span><span>:code</span><span> </span><span>-32600</span><span> </span><span>:message</span><span> </span><span>"Invalid Request"</span><span>}})</span><span>
                             </span><span>(</span><span>recur</span><span> </span><span>next-id</span><span> </span><span>requests</span><span>))))))))))</span><span>
</span></code></pre></div>
<p>When we receive a response to our request, we put it on the queue stored in the in-flight <code>requests</code> map, and remove the queue from the map. When we get a notification, we simply invoke the handler if it exists. Handling requests is a bit different, because we want to ensure the server will always receive a response. So we do a try/catch and always send back something. We do the request handling on the JSON-RPC process thread, so if it blocks for a long time, no other messages are processed. That’s actually a downside. So let’s just say I kept things simple for illustrative purposes, and spawning one more virtual thread to compute and send a response to the server is left as an exercise for the reader :D</p>

<p>Finally, there is an <code>:else</code> branch that responds to unexpected messages with an error. Which, I guess, is unnecessarily defensive given the lack of error handling and validations in other places.</p>

<h2 id="the-api">The API</h2>

<p>Now that all communication is implemented, it’s time to create an API. We will only need 3 functions:</p>
<ol>
  <li><code>start!</code> to start a language server.</li>
  <li><code>request!</code> to send a request to the language server and get a result back</li>
  <li><code>notify!</code> to send a notification to the language server and get nothing back</li>
</ol>

<p>Let’s start with <code>start!</code>-ing a server:</p>
<div><pre><code><span>(</span><span>defn</span><span> </span><span>start!</span><span>
  </span><span>([</span><span>^</span><span>Process</span><span> </span><span>process</span><span> </span><span>handlers</span><span>]</span><span>
   </span><span>(</span><span>start!</span><span> </span><span>(</span><span>.getInputStream</span><span> </span><span>process</span><span>)</span><span> </span><span>(</span><span>.getOutputStream</span><span> </span><span>process</span><span>)</span><span> </span><span>handlers</span><span>))</span><span>
  </span><span>([</span><span>^</span><span>InputStream</span><span> </span><span>in</span><span> </span><span>^</span><span>OutputStream</span><span> </span><span>out</span><span> </span><span>handlers</span><span>]</span><span>
   </span><span>(</span><span>let</span><span> </span><span>[</span><span>client-in</span><span> </span><span>(</span><span>ArrayBlockingQueue.</span><span> </span><span>16</span><span>)</span><span>
         </span><span>server-in</span><span> </span><span>(</span><span>ArrayBlockingQueue.</span><span> </span><span>16</span><span>)</span><span>
         </span><span>server-out</span><span> </span><span>(</span><span>ArrayBlockingQueue.</span><span> </span><span>16</span><span>)]</span><span>
     </span><span>(</span><span>lsp-jsonrpc</span><span> </span><span>client-in</span><span> </span><span>server-in</span><span> </span><span>server-out</span><span> </span><span>handlers</span><span>)</span><span>
     </span><span>(</span><span>lsp-base</span><span> </span><span>in</span><span> </span><span>server-in</span><span> </span><span>out</span><span> </span><span>server-out</span><span>)</span><span>
     </span><span>client-in</span><span>)))</span><span>
</span></code></pre></div>

<p>I made 2 arities for the <code>start!</code> function:</p>
<ol>
  <li>Helper process arity specifically for process stdio, since this is what is used in 99% of LSP client/server communication implementations. We are going to use it to start the server.</li>
  <li>Generic arity over InputStream+OutputStream pair. This arity is the one that does the work. LSP allows various transports, e.g. pipes, network sockets, or stdio communication between processes. The generic arity supports it all, you only need to provide the input and output streams. In the setup, I allocate small buffers so if some part of the commucation consumes too slow (or produces too fast), there is some buffering and then backpressure. I don’t know if these buffer sizes are any good to be honest, I just made them up. Anyway, here, we call <code>lsp-jsonrpc</code> and <code>lsp-base</code> to wire everything together, and finally return the <code>client-in</code>. Yes, the LSP client object is just a queue. Yes, it probably should be something else, like a custom type, in a proper implementation.</li>
</ol>

<p>Next step is sending a notification. This is simpler than sending a request because we don’t get a response back:</p>
<div><pre><code><span>(</span><span>defn</span><span> </span><span>notify!</span><span>
  </span><span>([</span><span>^</span><span>BlockingQueue</span><span> </span><span>lsp</span><span> </span><span>method</span><span>]</span><span>
   </span><span>(</span><span>.put</span><span> </span><span>lsp</span><span> </span><span>{</span><span>:method</span><span> </span><span>method</span><span>}))</span><span>
  </span><span>([</span><span>^</span><span>BlockingQueue</span><span> </span><span>lsp</span><span> </span><span>method</span><span> </span><span>params</span><span>]</span><span>
   </span><span>(</span><span>.put</span><span> </span><span>lsp</span><span> </span><span>{</span><span>:method</span><span> </span><span>method</span><span> </span><span>:params</span><span> </span><span>params</span><span>})))</span><span>
</span></code></pre></div>

<p>Finally, sending a request. If you remember, back when we were implementing the <code>lsp-jsonrpc</code> function, we agreed that LSP request maps will use a <code>:response</code> key with a queue value. Now is the time to do it:</p>
<div><pre><code><span>(</span><span>defn</span><span> </span><span>request!</span><span>
  </span><span>([</span><span>lsp</span><span> </span><span>method</span><span>]</span><span>
   </span><span>(</span><span>request!</span><span> </span><span>lsp</span><span> </span><span>method</span><span> </span><span>nil</span><span>))</span><span>
  </span><span>([</span><span>^</span><span>BlockingQueue</span><span> </span><span>lsp-client</span><span> </span><span>method</span><span> </span><span>params</span><span>]</span><span>
   </span><span>(</span><span>let</span><span> </span><span>[</span><span>queue</span><span> </span><span>(</span><span>SynchronousQueue.</span><span>)]</span><span>
     </span><span>(</span><span>.put</span><span> </span><span>lsp-client</span><span> </span><span>(</span><span>cond-&gt;</span><span> </span><span>{</span><span>:method</span><span> </span><span>method</span><span> </span><span>:response</span><span> </span><span>queue</span><span>}</span><span> </span><span>params</span><span> </span><span>(</span><span>assoc</span><span> </span><span>:params</span><span> </span><span>params</span><span>)))</span><span>
     </span><span>(</span><span>let</span><span> </span><span>[</span><span>m</span><span> </span><span>(</span><span>.take</span><span> </span><span>queue</span><span>)]</span><span>
       </span><span>(</span><span>if-let</span><span> </span><span>[</span><span>e</span><span> </span><span>(</span><span>:error</span><span> </span><span>m</span><span>)]</span><span>
         </span><span>(</span><span>throw</span><span> </span><span>(</span><span>ex-info</span><span> </span><span>(</span><span>:message</span><span> </span><span>e</span><span>)</span><span> </span><span>e</span><span>))</span><span>
         </span><span>(</span><span>:result</span><span> </span><span>m</span><span>))))))</span><span>
</span></code></pre></div>
<p><code>SynchronousQueue</code> is a queue with a buffer of size 0. This means every blocking <code>.take</code> (which we do here) will wait until someone else (<code>lsp-jsonrpc</code> function) puts a value onto the queue. So this is like a promise that we await here. This implementation creates a request map, submits it to the lsp client, and then blocks until a response arrives from the language server. What’s extra nice here is that JSON-RPC errors are thrown as java exceptions, and successful results are simply returned as values. As if this is some sort of synchronous “method call”. That also performs well because virtual threads. Java 24 is really nice.</p>

<p>Anyway, that’s it! We now can start language servers and do stuff with them! Yay, we implemented an LSP client, all in 150 (not even 200) lines of code!</p>

<p>Yay?</p>

<p>You might feel a bit let down now because everything we did — base and jsonrpc layer — although required for the LSP, don’t actually have anything to do with actual language servers. But it’s so nice and short and focused! Oh, well. Now, I guess, the time has come to destroy all this beauty by actually trying to use a language server. After all, we still have a budget for 50 more LoC.</p>

<h2 id="the-ugly-linter">The ugly linter</h2>

<p>Let’s discuss the language server lifecycle first. When client starts a language server, it’s not actually immediately ready for use. Now we are entering the real LSP integration territory. We have to initialize it (a request), then notify it that it’s initialized (a notification), then use it (issue 0 or more requests or notifications), then shut it down (a request), and then finally notify it so it can exit (one more notification). The initialization process is necessary to exchange <strong>capabilities</strong>: the client says what it can do, and then the server says what it can do, and LSP demands both client and server to honor what they said to each other. For example, a proper language client (like a text editor, not the toy that we build here) might say “I will ask you about code completion, but please don’t notify me about your linting since I don’t support displaying squiggly lines yet”, and the server might say “I can provide both code completion and notify you about code issues as you type, but I won’t do that since you don’t support it”.</p>

<p>All capabilities are defined in the LSP specification, and almost all of them are optional to implement. This allows for both LSP client and server developers to build the support gradually over time. For example, in <a href="https://defold.com/">Defold editor</a>, the LSP support story started only with displaying <em>diagnostics</em> (this is the term LSP specification uses for linting squigglies), and then was gradually expanded to code completion, hovers and symbol renaming.</p>

<p>Let’s see what we have in stock for diagnostics. A diagnostic is a data describing the code issue. It has a text range (something like “from line 20 char 5 to line 20 char 10”), severity (warning/error etc.) and a text message.
LSP specification defines these 2 methods that we could use to get diagnostics from the language server:</p>
<ol>
  <li><a href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_diagnostic">document diagnostics</a>: a client may request a server to lint a particular file and return a result.</li>
  <li><a href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#workspace_diagnostic">workspace diagnostics</a>: a client may request a server to lint the whole project and return a result.</li>
</ol>

<p>So, with these 2 methods at hand, and with our nice LSP client implementation, we could sketch a linting function that does linting using roughly this algorithm:</p>
<ol>
  <li>start a server</li>
  <li>initialize it, telling the server that we may ask it for workspace and document diagnostics</li>
  <li>if server supports workspace diagnostics, we use that; if server supports document diagnostics, we list all files in a project and ask it to lint them; otherwise, we report an error that the server can’t do what we want it to do.</li>
  <li>we shut down the server</li>
</ol>

<p>Should be easy. Really, it should be this easy! It should be easy!!! Why isn’t it this easy?!?!..</p>

<p>Okay.</p>

<p>Here comes the ugly part.</p>

<p>When preparing this post, I went through a lot of language servers to use as an example. I only needed one of them to implement either of the methods. But no. Not single one of them did. All these language servers that boast that they provide diagnostics. They are not even lying. But! They don’t actually implement diagnostics on request. You see, there is a third way language servers can use to provide these pesky little squigglies. They can post them, out of the blue, whenever they want, as a notification. No way to ask them about it. And that’s what they do. All of them. And they do it, mostly, as a response to 2 specific notifications from the client: when the client notifies the server that it opened a document, and when the client notifies the server that a text of an open document has changed. This notification approach existed first, and every language server implementor just uses it because it’s easy and it works and everything else is unnecessary. It makes total sense for a text editor: most of the time, you are only interested in squigglies for the file you are editing, while you are editing it. But unfortunately it means that I can’t make a nice example of using our tiny language client to do something useful without building a full-blown text editor — all the other features only make sense in the code editing context where we have cursors and text selection and we can ask a language server about this thing on this line.</p>

<p>So. It’s going to be ugly. But this not a problem of the LSP specification. It’s just that I got unlucky with the example that I wanted to use. Instead of this simple straightforward request/response thing I’m going to do something awful. I’ll start a language server. I will initialize it, saying only that I am open to receiving diagnostic notifications. I will ignore server capabilities completely because at this point why bother. And then I will open every file in a project, and then I’ll wait a bit to receive diagnostic notifications, and then I’ll shut this abomination down. I’m not going to explain all the code, because it’s so awful, but <a href="https://github.com/vlaaad/lsp-clj-client/blob/main/src/io/github/vlaaad/lsp.clj#L156-L200">here it is in all it’s glory</a>. Here, I’ll only show the good parts.</p>

<p>We start with a function signature:</p>
<div><pre><code><span>(</span><span>defn</span><span> </span><span>lint</span><span> </span><span>[</span><span>&amp;</span><span> </span><span>{</span><span>:keys</span><span> </span><span>[</span><span>cmd</span><span> </span><span>path</span><span> </span><span>ext</span><span>]}]</span><span> 
  </span><span>...</span><span>)</span><span>
</span></code></pre></div>
<p>The function takes an LSP shell <code>cmd</code> to run (either a string or a coll of strings), a directory <code>path</code> to lint, and a file <code>ext</code>ension to select the files to lint. Since the function accepts kv-args, and it’s on github, and you are using an up-to-date <code>clj</code> tool (aren’t you?), you can actually try to run it. Maybe it will even work! For example, you can download <a href="https://github.com/clojure-lsp/clojure-lsp/releases">clojure-lsp</a>, and then run the following command in your project:</p>
<div><pre><code>clj <span>-Sdeps</span> <span>'{:deps {io.github.vlaaad/lsp-clj-client {:git/sha "57c618d7ecfc9f94fbef9157cfe4534a4816be45"}}}'</span> <span>\</span>
    <span>-X</span> io.github.vlaaad.lsp/lint <span>\</span>
    :cmd <span>'"/Users/vlaaad/Downloads/clojure-lsp.exe"'</span> <span>\</span>
    :path <span>'"."'</span> <span>\ </span>
    :ext <span>'"clj"'</span>
</code></pre></div>
<p>For the code that we discussed in this post, the output will look like this:</p>
<div><pre><code>file:///Users/vlaaad/Projects/lsp-clj-client/src/io/github/vlaaad/lsp.clj at 168:22:  Redundant let expression.
</code></pre></div>
<p>Turns out there is a warning in the <code>lint</code> function implementation! But the warning is in a bad, messy part of the code, so there is no point in fixing it in the function. Nothing can fix this function… Anyway, we start a process and then make it a server:</p>
<div><pre><code><span>(</span><span>let</span><span> </span><span>[</span><span>...</span><span> </span><span>...</span><span>
      </span><span>^</span><span>Process</span><span> </span><span>process</span><span> </span><span>(</span><span>apply</span><span> </span><span>process/start</span><span> </span><span>{</span><span>:err</span><span> </span><span>:inherit</span><span>}</span><span> </span><span>(</span><span>if</span><span> </span><span>(</span><span>string?</span><span> </span><span>cmd</span><span>)</span><span> </span><span>[</span><span>cmd</span><span>]</span><span> </span><span>cmd</span><span>))</span><span>
      </span><span>...</span><span> </span><span>...</span><span>
      </span><span>server</span><span> </span><span>(</span><span>start!</span><span> </span><span>process</span><span> </span><span>{</span><span>"textDocument/publishDiagnostics"</span><span> </span><span>(</span><span>fn</span><span> </span><span>[</span><span>diagnostics</span><span>]</span><span> </span><span>...</span><span>)})]</span><span>
  </span><span>...</span><span>)</span><span>
</span></code></pre></div>
<p>We are only going to listen to <code>textDocument/publishDiagnostics</code> notification that might be sent by the language server when we open files. At this point, the server is not initialized yet, so we do it next:</p>
<div><pre><code><span>(</span><span>request!</span><span> </span><span>server</span><span> </span><span>"initialize"</span><span> </span><span>{</span><span>:processId</span><span> </span><span>(</span><span>.pid</span><span> </span><span>(</span><span>ProcessHandle/current</span><span>))</span><span>
                               </span><span>:rootUri</span><span> </span><span>(</span><span>uri</span><span> </span><span>path</span><span>)</span><span>
                               </span><span>:capabilities</span><span> </span><span>{</span><span>:textDocument</span><span> </span><span>{</span><span>:publishDiagnostics</span><span> </span><span>{}}}})</span><span>
</span></code></pre></div>
<p>We issue a blocking <code>initialize</code> call, and tell the server our process id (so it can exit if we die before stopping it), which directory is the project root, and what are our capabilities. You are expected to take the return value and check if it e.g. supports the diagnostics, but I decided to skip it in this example.</p>

<p>Next step: we notify the server that it’s <code>initialized</code>:</p>
<div><pre><code><span>(</span><span>notify!</span><span> </span><span>server</span><span> </span><span>"initialized"</span><span>)</span><span>
</span></code></pre></div>
<p>Not sure why it’s necessary, but the protocol demands it. Then we use the server and print the results (horrors omitted). Then we shut it down:</p>
<div><pre><code><span>(</span><span>request!</span><span> </span><span>server</span><span> </span><span>"shutdown"</span><span>)</span><span>
</span><span>(</span><span>notify!</span><span> </span><span>server</span><span> </span><span>"exit"</span><span>)</span><span>
</span></code></pre></div>

<p>And that’s it!</p>

<h2 id="discussion">Discussion</h2>

<p>Okay, let’s take a deep breath. I took a deep breath and spent some time reflecting on all this. I like LSP. It’s great for the ecosystem: IDEs get better support for more programming languages, and programming languages are easier to integrate into more IDEs. It’s not a great protocol for building command line linters: even though the protocol supports it, in reality it’s going to be hard to find a server that has the necessary capabilites. But it’s much better for building text editors, I promise :)</p>

<p>I built LSP support for the Defold editor. Now that I also spent a bit of time reflecting on it, I’d like to share my opinions on the matter. First of all, integrating diagnostics into the text editor was actually pretty easy, since there was no requirement to explicitly request diagnostics, they just appear and get displayed. That wasn’t the complex part. Defold LSP support is much more complex than our toy implementation because a text editor needs to manage the whole zoo of language servers, each with it’s own lifecycle, initialization process and capabilities. When implementing the LSP support in a text editor, I found that most of the complexity comes from having to manage this zoo, where each server has different runtime state (starting, running, stopped), and where each of language server processes might decide to die at any point. This complicates, for example, the following:</p>
<ul>
  <li>Tracking open files with unsaved changes. Not only does the text editor need to notify running language servers when the user opens a files, it should also notify a freshly started (or restarted) servers about all currently open documents. There needs to be book-keeping of open (visible to the user) and unsaved files (not necessarily visible to the user).</li>
  <li>Sending requests to multiple servers at the same time. This might be not immediately obvious, but LSP does not get in the way of running multiple language servers — for the same language, in the same project — simultaneously. VSCode does it. Defold editor does it too. When the editor asks for code completions to present a completion popup, the LSP integration actually asks all capable running language servers for the target language, and then merges the results. Same applies for displaying diagnostics. Having multiple language servers per file is very useful. For example, you might run a language server dedicated to the code analysis and, additionally, a spell checking language server that highlights typos, and the editor will display diagnostics from both in the same file. So, implementing support for sending a request to multiple language servers at once, with different capabilities, where every server might die at any moment, but we still wan’t to receive a response from all matching servers, within a time frame, wasn’t easy.</li>
</ul>

<p>Compared to that, here is a critique of LSP that I’ve <a href="https://www.michaelpj.com/blog/2024/09/03/lsp-good-bad-ugly.html">read about before</a>, but don’t find convincing:</p>
<ol>
  <li>Missing causality. The editor changes the code, then immediately asks for something like code actions from the server. It’s possible that the server won’t have a chance to update it’s internal state and will return results for an outdated text state. Or it will post diagnostics that no longer apply. But then it will post the correct ones a bit later. I think it doesn’t matter since the problem is easily recoverable with e.g. an undo in a text editor, or with repeating a request, or it will recover itself automatically a bit later. There is no need for strong causalty/consistency guarantees: interactions with language servers are mostly read-only, there is no harm in the thing being a bit lax/late.</li>
  <li>Different endpoints encode data slightly differently. For example, unsaved changes to text files are communicated incrementally (as diffs), but text document outline (i.e. list of defined classes/functions/modules etc.) is always refreshed in full. I think inconsistencies here don’t matter: writing a pre/post processing is easy. Different state synchronization approaches are dictated by the context and there are trade-offs. Text state synchronization should be fast, therefore requiring support for incremental text synchronization for clients and servers is reasonable — we might be editing very large files, we shouldn’t constantly send them in full on every change. Outline refreshes, on the other hand, are requested as needed, and not on typing, so there is no need for incremental diffs there.</li>
  <li>Specification is big. It is, but it doesn’t matter: we can opt into into parts of it using capabilities.</li>
  <li>Weird type definitions. A lot of JSON schema of requests/response is written using Typescript types. Truth be told, I was perplexed by it initially, but I quicky got used to it. It communicates the data shape well enough.</li>
</ol>

<p>LSP has it’s warts and inconsistencies, as every successful protocol that has grown over time. If it was designed from scratch now, it would be simpler, particularly around request and response data shapes. But that’s not as hard as e.g. managing the state of the servers, which is an unfortunate consequence of the fact that language servers are separate stateful processes. Perhaps, LSP successor will be not a better protocol for inter-process communication, but a WASM “interface” that will allow writing language servers in-process, synchronous, in whatever language, as long as it compiles to WASM. And then, every code editor will run some WASM runtime. Meanwhile, LSP is infinitely better than building bespoke language integrations, so I’m happy to use it.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Klarna changes its AI tune and again recruits humans for customer service (237 pts)]]></title>
            <link>https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/</link>
            <guid>43955374</guid>
            <pubDate>Sun, 11 May 2025 17:35:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/">https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/</a>, See on <a href="https://news.ycombinator.com/item?id=43955374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
                        
<div>
        <p>
            This audio is auto-generated. Please let us know if you have <a href="https://www.customerexperiencedive.com/contact/">feedback</a>.
        </p>
    </div>

                        
                        

<p><span><span><span><span><span><span>A year after claiming that its AI chatbot could do the work of </span></span></span></span></span></span><a href="https://www.customerexperiencedive.com/news/klarna-generative-ai-customer-service-call-center/709072/"><span><span><span><span><span><span><span><span><span>700 representatives</span></span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>, Klarna is turning back to people to handle more of its customer service work.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The buy now, pay later firm now wants customers to always have the option to speak with a human, Klarna spokesperson </span></span></span></span></span></span><span><span><span><span><span><span><span>Clare Nordstrom</span></span></span></span></span></span></span><span><span><span><span><span><span> told CX Dive.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“While Klarna pioneered the use of AI in customer service </span></span></span></span></span></span><a href="https://url.us.m.mimecastprotect.com/s/V8saC5ylW5FpMv6OqSzf3fkS7op?domain=klarna.com/"><span><span><span><span><span><span><span><span>with groundbreaking results</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>, this strategy will now evolve to the next level,” Nordstrom said in an email. “AI gives us speed. Talent gives us empathy. Together, we can deliver service that’s fast when it should be, and emphatic and personal when it needs to be.”</span></span></span></span></span></span></p>



<p><span><span><span><span><span><span>It’s a drastic change from a year ago, when the firm went all in on AI, </span></span></span></span></span></span><a href="https://www.bbc.com/news/articles/c80e1gp9m9zo"><span><span><span><span><span><span><span><span>laid off workers</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> and </span></span></span></span></span></span><a href="https://www.bloomberg.com/news/articles/2024-12-12/klarna-stopped-all-hiring-a-year-ago-to-replace-workers-with-ai?sref=Hhue1scO"><span><span><span><span><span><span><span><span>paused hiring</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>. The shift highlights the need for the option to speak to a human in customer service — and to use AI as a supplement, not a replacement, for staff, according to </span></span></span></span></span></span><span><span><span><span><span><span><span>Julie Geller</span></span></span></span></span></span></span><span><span><span><span><span><span>, principal research director at Info-Tech Research Group.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“The key takeaway is that AI should augment human agents — not replace them,” </span></span></span></span></span></span><span><span><span><span><span><span><span>Geller</span></span></span></span></span></span></span><span><span><span><span><span><span> said in an email. “Automate the routine to drive efficiency, but always ensure customers have a clear, easy path to a human, especially when emotions or complexity come into play.”</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Klarna CEO Sebastian Siemiatkowski</span></span></span></span></span></span></span><span><span><span><span><span><span> said just as much in an </span></span></span></span></span></span><a href="https://www.bloomberg.com/news/articles/2025-05-08/klarna-turns-from-ai-to-real-person-customer-service?sref=Hhue1scO"><span><span><span><span><span><span><span><span>interview with Bloomberg</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span> Thursday.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“From a brand perspective, a company perspective … I just think it’s so critical that you are clear to your customer that there will be always a human if you want,” Siemiatkowski said.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Klarna is now recruiting workers for what Siemiatkowski referred to as an Uber-type customer service setup. Starting with a pilot program, the firm will offer customer service talent “competitive pay and full flexibility to attract the best,” with staff able to work remotely, according to Nordstrom.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“AI solves the easy stuff — our experts handle the moments that matter,” Nordstrom said. “That’s why we’re running this pilot, bringing in highly educated students, professionals and entrepreneurs for a new kind of role that blends frontline excellence with real-time product feedback.”&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>One of the goals is to replace the thousands of workers that Klarna currently outsources, Siemiatkowski told Bloomberg.&nbsp;</span></span></span></span></span></span></p>


<p><span><span><span><span><span><span>Back in February 2024, Klarna touted impressive figures: The AI assistant had taken over </span></span></span></span></span></span><span><span><span><span><span><span><span>two-thirds </span></span></span></span></span></span></span><span><span><span><span><span><span>of customer service chats —</span></span></span></span></span></span><span><span><span><span><span><span><span> 2.3 million</span></span></span></span></span></span></span><span><span><span><span><span><span> in total — in its first month. Its handling of customer service chats resulted in average resolution times of </span></span></span></span></span></span><span><span><span><span><span><span><span>less than 2 minutes</span></span></span></span></span></span></span><span><span><span><span><span><span>.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The AI still plays a crucial role, according to Nordstrom. The chatbot still handles two-thirds of all customer inquiries. Since launch, response times have improved by </span></span></span></span></span></span><span><span><span><span><span><span><span>82%</span></span></span></span></span></span></span><span><span><span><span><span><span>, and Klarna has seen a </span></span></span></span></span></span><span><span><span><span><span><span><span>25%</span></span></span></span></span></span></span><span><span><span><span><span><span> drop in repeat issues.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“But in a world of automation, nothing is more valuable than a truly great human interaction. That’s why we’re doubling down — investing in the human side of service: empathy, expertise, and real conversations,” Nordstrom said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Like many companies, Klarna had turned to AI as a form of cost-cutting, Siemiatkowski told Bloomberg.&nbsp;</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“Initially, Klarna embraced AI with an eye toward cost savings and efficiency — but perhaps underestimated the tradeoff,” Geller said. “As customers increasingly voiced frustration over impersonal interactions and limited access to human help, it became clear this approach risked undermining the very experience they aimed to improve. That kind of friction doesn’t just erode satisfaction; it carries real financial and reputational consequences.”</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Siemiatkowski acknowledged that Klarna had gone too far in the wrong direction with AI.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“As cost unfortunately seems to have been a too predominant evaluation factor when organizing this, what you end up having is lower quality,” Siemiatkowski told Bloomberg. “Really investing in the quality of the human support is the way of the future for us.”</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>The firm’s shifting customer service strategy is perhaps unsurprising considering consumers’ overall frustrations with chatbots and the technology’s shortcomings.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>A Verint survey released last year found more than two-thirds customers have had a </span></span></span></span></span></span><a href="https://www.customerexperiencedive.com/news/consumer-frustration-self-service-live-agent-ivr-chatbot/724620/"><span><span><span><span><span><span><span><span><span>bad chatbot experience</span></span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>, and in that group, the top complaint was inability to answer questions.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>Customers still </span></span></span></span></span></span><a href="https://www.customerexperiencedive.com/news/consumers-human-connection-speed-customer-service/743364/"><span><span><span><span><span><span><span><span>want to talk to humans</span></span></span></span></span></span></span></span></a><span><span><span><span><span><span>, especially for complex or sensitive issues. A Five9 survey from March found that </span></span></span></span></span></span><span><span><span><span><span><span><span>86%</span></span></span></span></span></span></span><span><span><span><span><span><span> of customers believe empathy and human connection are more important than a quick response in providing excellent customer experience.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>AI can mimic emotional intelligence in simple, routine interactions, Geller said.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“But when situations get more complex, subtle gaps can start to show,” she said. “AI can also misread customer intent, leading to irrelevant — or worse, misleading — responses that frustrate people instead of solving their issue. And when companies lean too heavily on automation, they risk building rigid systems that make customers feel trapped, with no clear way to reach a human when it matters most.”</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>While Klarna has been an early, vocal adopter of AI, Geller is encouraged to see Klarna’s leadership course-correct from its earlier all-or-nothing approach.</span></span></span></span></span></span></p>
<p><span><span><span><span><span><span>“They recognized that trust and satisfaction aren’t purely transactional — they’re emotional,” she said. “And to sustain loyalty, especially in complex or sensitive moments, customers still expect, and deserve, the option of a human touch.”</span></span></span></span></span></span></p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An online exhibition of pretty software bugs (108 pts)]]></title>
            <link>https://glitchgallery.org/</link>
            <guid>43955333</guid>
            <pubDate>Sun, 11 May 2025 17:29:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://glitchgallery.org/">https://glitchgallery.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43955333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://glitchgallery.org/assets/images/glitchgallery.svg"></p><p>Welcome to the Glitch Gallery, an <strong>online exhibition of pretty software bugs</strong>! This is a museum of accidental art, an enthusiastic embrace of mistakes, a celebration of emergent beauty. <a href="https://glitchgallery.org/about/">Learn more?</a></p>
<p>Below you can browse the exhibits in our collection. If you have ever encountered such an accidental artwork yourself, we <strong>invite you to <a href="https://glitchgallery.org/submit/">submit it to us</a>!</strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plain Vanilla Web – Guide for de-frameworking yourself (1239 pts)]]></title>
            <link>https://plainvanillaweb.com/index.html</link>
            <guid>43954896</guid>
            <pubDate>Sun, 11 May 2025 16:31:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://plainvanillaweb.com/index.html">https://plainvanillaweb.com/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=43954896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <section>
            <h2>
                About this site
            </h2>
            <p>
                This is an overview of the major techniques used to make web sites and web applications
                without making use of build tools or frameworks, with just an editor, a browser, and web standards.
            </p>
            <p>
                Specifically, that means these topics:
            </p>
            <dl>
                <dt><a href="https://plainvanillaweb.com/pages/components.html">Components</a></dt>
                <dd>The use of Web Components as the basic building block for making higher-level primitives out of plain HTML, JavaScript and CSS, replacing the components approach from frameworks like React or Vue.</dd>
                <dt><a href="https://plainvanillaweb.com/pages/styling.html">Styling</a></dt>
                <dd>Fully leveraging the abilities of modern CSS to replace the conveniences of CSS Modules, PostCSS, or SASS.</dd>
                <dt><a href="https://plainvanillaweb.com/pages/sites.html">Sites</a></dt>
                <dd>Making a web project based on web components and deploying it for production without involving build tools, frameworks or server-side logic.</dd>
                <dt><a href="https://plainvanillaweb.com/pages/applications.html">Applications</a></dt>
                <dd>How to build single-page web applications with vanilla techniques, how to do routing, and how to manage state.</dd>
            </dl>
            <p>
                This tutorial is meant for those who already know their way around HTML, CSS and JavaScript. If you're just starting out learning web development and are looking for resources to get going, then this is not the site for you yet. 
                You can get started at <a href="https://www.theodinproject.com/paths/foundations/courses/foundations">The Odin Project Foundations Course</a> 
                or the <a href="https://developer.mozilla.org/en-US/docs/Learn">MDN Learn web development</a> learning path. 
            </p>
        </section>
        <section>
            <h2>
                Wait, but why
            </h2>
            <p>
                Modern web development frameworks are powerful in their ability to develop rich well-structured web applications quickly,
                and for this reason alone they are well worth your time to learn.
                However, this rich functionality comes at the cost of framework and tooling complexity,
                and the resulting projects often require regular maintenance to keep secure and up to date.
            </p>
            <p>
                The plain vanilla style of web development makes a different choice,
                trading off a few short term comforts for long term benefits like simplicity and being effectively zero-maintenance.
                This approach is made possible by today's browser landscape, which offers excellent web standards support.
            </p>
        </section>
        <section>
            <hr>
            <h2>Up next</h2>
            Learn about using <a href="https://plainvanillaweb.com/pages/components.html">Web Components</a> as the way to compose content, style and behavior.
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I built a native Windows Todo app in pure C (278 KB, no frameworks) (340 pts)]]></title>
            <link>https://github.com/Efeckc17/simple-todo-c</link>
            <guid>43954649</guid>
            <pubDate>Sun, 11 May 2025 15:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Efeckc17/simple-todo-c">https://github.com/Efeckc17/simple-todo-c</a>, See on <a href="https://news.ycombinator.com/item?id=43954649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Simple Todo (C / WinAPI)</h2><a id="user-content--simple-todo-c--winapi" aria-label="Permalink: 🌟 Simple Todo (C / WinAPI)" href="#-simple-todo-c--winapi"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40b57bd6443be706eec66a7aad9bb92cde60772fc1c44958fc99a64552598b76/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d77696e646f77732d626c7565"><img src="https://camo.githubusercontent.com/40b57bd6443be706eec66a7aad9bb92cde60772fc1c44958fc99a64552598b76/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d77696e646f77732d626c7565" alt="Platform" data-canonical-src="https://img.shields.io/badge/platform-windows-blue"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/54e7575c73d075ee3a666a4c2fa589d8b1084658ffe4774f6677cf3b9d3bd393/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d432d627269676874677265656e"><img src="https://camo.githubusercontent.com/54e7575c73d075ee3a666a4c2fa589d8b1084658ffe4774f6677cf3b9d3bd393/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d432d627269676874677265656e" alt="Language" data-canonical-src="https://img.shields.io/badge/language-C-brightgreen"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0412748680b88a40cce8a326d28a0ce988ab923b710d3e7c0955674e9d3cdf43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d79656c6c6f77"><img src="https://camo.githubusercontent.com/0412748680b88a40cce8a326d28a0ce988ab923b710d3e7c0955674e9d3cdf43/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d79656c6c6f77" alt="License" data-canonical-src="https://img.shields.io/badge/license-MIT-yellow"></a></p>
<p dir="auto">A modern, native Windows Todo application built with C and Win32 API. This project demonstrates advanced Windows GUI programming and system integration capabilities.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Efeckc17/simple-todo-c/blob/main/img/home.png"><img src="https://github.com/Efeckc17/simple-todo-c/raw/main/img/home.png" alt="Todo App Screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Features</h2><a id="user-content--features" aria-label="Permalink: ✨ Features" href="#-features"></a></p>
<ul dir="auto">
<li>📝 Create, edit, and delete todo items</li>
<li>✅ Mark tasks as complete</li>
<li>💾 Persistent storage in AppData</li>
<li>🔔 System tray integration (minimize to tray)</li>
<li>🌙 Native Windows look and feel</li>
<li>🚀 Auto-start with Windows option</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛠️ Technical Details</h2><a id="user-content-️-technical-details" aria-label="Permalink: 🛠️ Technical Details" href="#️-technical-details"></a></p>
<ul dir="auto">
<li>Written in pure C</li>
<li>Uses Win32 API for GUI</li>
<li>System tray integration</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">💾 Data Storage</h2><a id="user-content--data-storage" aria-label="Permalink: 💾 Data Storage" href="#-data-storage"></a></p>
<p dir="auto">The application stores todos in a binary file:</p>
<ul dir="auto">
<li><strong>Location</strong>: <code>%APPDATA%\TodoApp\todos.dat</code></li>
<li><strong>Format</strong>: Binary file containing the entire todo list</li>
<li><strong>Capacity</strong>: Maximum 100 todos</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📋 Requirements</h2><a id="user-content--requirements" aria-label="Permalink: 📋 Requirements" href="#-requirements"></a></p>
<ul dir="auto">
<li>Windows OS</li>
<li>MinGW-w64 (GCC compiler)</li>
<li>Windows SDK (included with MinGW)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Building from Source</h2><a id="user-content--building-from-source" aria-label="Permalink: 🚀 Building from Source" href="#-building-from-source"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install MinGW-w64:</p>
<ul dir="auto">
<li>Download from <a href="https://www.mingw-w64.org/downloads/" rel="nofollow">MinGW-w64 website</a></li>
<li>Or use <a href="https://www.msys2.org/" rel="nofollow">MSYS2</a></li>
<li>Add MinGW's bin directory to your system PATH</li>
</ul>
</li>
<li>
<p dir="auto">Clone the repository:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Efeckc17/simple-todo-c.git
cd simple-todo-c"><pre>git clone https://github.com/Efeckc17/simple-todo-c.git
<span>cd</span> simple-todo-c</pre></div>
</li>
<li>
<p dir="auto">Build the project:</p>

</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎮 Usage</h2><a id="user-content--usage" aria-label="Permalink: 🎮 Usage" href="#-usage"></a></p>
<ol dir="auto">
<li>Run <code>bin/todo.exe</code></li>
<li>Use the interface to:
<ul dir="auto">
<li>Add new todos with the "Add" button</li>
<li>Edit existing todos by selecting and clicking "Edit"</li>
<li>Delete todos with the "Delete" button</li>
<li>Mark todos as complete with the "Complete" button</li>
<li>Set priority levels for each todo</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏗️ Project Structure</h2><a id="user-content-️-project-structure" aria-label="Permalink: 🏗️ Project Structure" href="#️-project-structure"></a></p>
<div data-snippet-clipboard-copy-content=".
├── src/
│   ├── main.c      # Application entry point
│   ├── todo.c      # Todo management logic
│   ├── todo.h      # Todo data structures and declarations
│   └── gui.c       # GUI implementation
├── bin/            # Compiled executable
├── build.bat       # Build script
└── README.md       # This file"><pre><code>.
├── src/
│   ├── main.c      # Application entry point
│   ├── todo.c      # Todo management logic
│   ├── todo.h      # Todo data structures and declarations
│   └── gui.c       # GUI implementation
├── bin/            # Compiled executable
├── build.bat       # Build script
└── README.md       # This file
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Development</h2><a id="user-content--development" aria-label="Permalink: 🔧 Development" href="#-development"></a></p>
<p dir="auto">The project uses the following key components:</p>
<ul dir="auto">
<li><strong>Win32 API</strong>: For window management and GUI</li>
<li><strong>Common Controls</strong>: For modern UI elements</li>
<li><strong>UXTheme</strong>: For Windows visual styles</li>
<li><strong>File I/O</strong>: For data persistence</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📝 License</h2><a id="user-content--license" aria-label="Permalink: 📝 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/Efeckc17/simple-todo-c/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📫 Contact</h2><a id="user-content--contact" aria-label="Permalink: 📫 Contact" href="#-contact"></a></p>
<p dir="auto">Toxi360 - <a href="https://github.com/Efeckc17">@Efeckc17</a></p>
<p dir="auto">Project Link: <a href="https://github.com/Efeckc17/simple-todo-c">https://github.com/Efeckc17/simple-todo-c</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[High-school shop students attract skilled-trades job offers (212 pts)]]></title>
            <link>https://www.wsj.com/lifestyle/careers/skilled-trades-high-school-recruitment-fd9f8257</link>
            <guid>43954488</guid>
            <pubDate>Sun, 11 May 2025 15:31:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/lifestyle/careers/skilled-trades-high-school-recruitment-fd9f8257">https://www.wsj.com/lifestyle/careers/skilled-trades-high-school-recruitment-fd9f8257</a>, See on <a href="https://news.ycombinator.com/item?id=43954488">Hacker News</a></p>
Couldn't get https://www.wsj.com/lifestyle/careers/skilled-trades-high-school-recruitment-fd9f8257: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[DNS piracy blocking orders: Google, Cloudflare, and OpenDNS respond differently (173 pts)]]></title>
            <link>https://torrentfreak.com/dns-piracy-blocking-orders-google-cloudflare-and-opendns-respond-differently-250511/</link>
            <guid>43954459</guid>
            <pubDate>Sun, 11 May 2025 15:26:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/dns-piracy-blocking-orders-google-cloudflare-and-opendns-respond-differently-250511/">https://torrentfreak.com/dns-piracy-blocking-orders-google-cloudflare-and-opendns-respond-differently-250511/</a>, See on <a href="https://news.ycombinator.com/item?id=43954459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          <p>    
                 <!-- Breadcrumb NavXT 7.4.1 -->
<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Anti-Piracy category archives." href="https://torrentfreak.com/category/anti-piracy/"><span property="name">Anti-Piracy</span></a><meta property="position" content="2"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Site Blocking category archives." href="https://torrentfreak.com/category/anti-piracy/site-blocking/"><span property="name">Site Blocking</span></a><meta property="position" content="3"></span> &gt; <span></span>
	</p>

        <p>
                      <span> </span>
                    Pirate site blocking orders issued against DNS resolvers including OpenDNS, Google, and Cloudflare are stacking up in Europe. Since similar demands may eventually surface in other countries, including the U.S., we take a look at how these companies respond to blocking orders. Our research reveals that their responses differ quite a bit, which adds to an already confusing situation.
        </p>
      </div><div>
        <p><img loading="lazy" decoding="async" src="https://torrentfreak.com/images/stop-danger.jpg" alt="stop danger" width="300" height="220">The frontline of online piracy liability keeps moving, and core internet infrastructure providers are increasingly finding themselves in the crosshairs.</p>
<p>For rightsholders, site blocking remains the go-to response in many cases. Until recently, the majority of blockades were implemented by consumer ISPs, but expanded legal efforts are now targeting standalone DNS resolvers. </p>
<p>Over the past year, courts in <a href="https://torrentfreak.com/french-court-orders-cloudflare-to-dynamically-block-motogp-streaming-piracy-250405/">France</a>, <a href="https://torrentfreak.com/court-orders-google-to-poison-public-dns-to-prevent-iptv-piracy-250321/">Italy</a>, and <a href="https://torrentfreak.com/dazns-new-pirate-iptv-blocking-order-confidential-secret-sauce-250502/">Belgium</a> have ordered OpenDNS, Cloudflare, and Google to alter their responses to certain DNS queries. Instead of leading visitors to the domains of pirate sites, the companies are required to intercept queries and redirect them elsewhere. </p>
<p>The main rationale for targeting public DNS resolvers is their growing use for bypassing blocking measures rolled out by Internet providers. However, the American tech companies now being targeted are not happy with the role of ‘Internet police’ and appeals of these orders are still pending.</p>
<h2>OpenDNS Says Farewell</h2>
<p>While these legal battles play out in court, the DNS resolvers still have to comply one way or another. This has resulted in different responses, with Cisco’s OpenDNS taking by far the most drastic action.</p>
<p>When OpenDNS was first ordered to block pirate sites in France, the company made a <a href="https://torrentfreak.com/opendns-suspends-service-in-france-due-to-canal-piracy-blocking-order-240629/">simple but drastic</a> decision to leave the country entirely, effectively affecting all French users. Last week, it <a href="https://torrentfreak.com/opendns-quits-belgium-under-threat-of-piracy-blocks-or-fines-of-e100k-per-day-250416/">repeated this response</a> in Belgium following a similar court order. </p>
<p>Instead of blocking access to more than 100 sports piracy sites, as the Belgian order requires, OpenDNS announced its departure; at least temporarily.  </p>
<p>“Due to a court order in Belgium requiring the implementation of blocking measures to prevent access within Belgium to certain domains, the OpenDNS service is not currently available to users in Belgium,” the company <a href="https://community.cisco.com/t5/security-knowledge-base/opendns-service-currently-not-available-to-users-in-belgium/ta-p/5283479/redirect_from_archived_page/true#M18273">said</a>. </p>
<h2>Cloudflare Complies Using ‘Alternate Mechanisms’</h2>
<p>Not all DNS resolvers take such drastic measures. Cloudflare chooses to comply with court orders in its own way. Cloudflare DNS (1.1.1.1) users who try to access the targeted domains in countries where blocking orders are issued, see the following notice instead. </p>
<center><em>Error HTTP 451</em></center><br><center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/livescooerclou.jpg.webp 1032w, https://torrentfreak.com/images/livescooerclou-300x165.jpg.webp 300w" sizes="(max-width: 600px) 100vw, 600px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/livescooerclou.jpg" alt="cloudflare blocked" width="600" height="330" srcset="https://torrentfreak.com/images/livescooerclou.jpg 1032w, https://torrentfreak.com/images/livescooerclou-300x165.jpg 300w, https://torrentfreak.com/images/livescooerclou-600x330.jpg 600w, https://torrentfreak.com/images/livescooerclou-150x82.jpg 150w" sizes="(max-width: 600px) 100vw, 600px">
</picture>
</center>
<p>Interestingly, Cloudflare maintains in its <a href="https://torrentfreak.com/cloudflare-blocked-400-sports-piracy-domains-in-france-last-year-250303/">transparency report</a> that it is not blocking content through its public DNS resolver. Instead, it points out that it uses “alternate mechanisms”.</p>
<p>“Given the extraterritorial effect as well as the different global approaches to DNS-based blocking, Cloudflare […] identified alternate mechanisms to comply with relevant court orders. To date, Cloudflare has not blocked content through the 1.1.1.1 Public DNS Resolver,” the company writes.   </p>
<center><em>Not Blocked</em></center><center><br></center><center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/dns-blocked.jpg.webp 900w, https://torrentfreak.com/images/dns-blocked-300x54.jpg.webp 300w" sizes="(max-width: 600px) 100vw, 600px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/dns-blocked.jpg" alt="not blocked" width="600" height="109" srcset="https://torrentfreak.com/images/dns-blocked.jpg 900w, https://torrentfreak.com/images/dns-blocked-300x54.jpg 300w, https://torrentfreak.com/images/dns-blocked-600x109.jpg 600w, https://torrentfreak.com/images/dns-blocked-150x27.jpg 150w" sizes="(max-width: 600px) 100vw, 600px">
</picture>
</center>
<p>The result for Cloudflare DNS users appears to be the same, however. Those who try to access the blocked domains in the applicable countries will be redirected to the HTTP 451 error. </p>
<p>The good news is that affected users are informed about the reason for this technical blockade via the <a href="http://lumendatabase.org/">Lumen Database</a>. That doesn’t appear to be the case with Google. </p>
<h2>Google’s DNS Blackout</h2>
<p>After running tests in both Belgium and France, using various blocked domains, it’s clear that the targeted websites are no longer accessible through Google’s public DNS resolver (8.8.8.8). However, unlike Cloudflare, there is no notification whatsoever. </p>
<p>Instead, Google appears to simply refuse the DNS query, which means that the domain lookup is not linked to any IP address.</p>
<center><em>Query refused</em></center><br><center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/refused.jpg.webp 739w, https://torrentfreak.com/images/refused-300x32.jpg.webp 300w" sizes="(max-width: 600px) 100vw, 600px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/refused.jpg" alt="refused" width="600" height="63" srcset="https://torrentfreak.com/images/refused.jpg 739w, https://torrentfreak.com/images/refused-300x32.jpg 300w, https://torrentfreak.com/images/refused-600x63.jpg 600w, https://torrentfreak.com/images/refused-150x16.jpg 150w" sizes="(max-width: 600px) 100vw, 600px">
</picture>
</center>
<p>While this is effective in the sense that the pirate sites are no longer available, it’s not very transparent. Users who try to access the domains will simply see a browser error, which could be caused by various DNS issues. </p>
<center><em>Not resolved</em></center><br><center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/livesoccerblocked.jpg.webp 1034w, https://torrentfreak.com/images/livesoccerblocked-300x191.jpg.webp 300w" sizes="(max-width: 600px) 100vw, 600px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/livesoccerblocked.jpg" alt="google blocked" width="600" height="383" srcset="https://torrentfreak.com/images/livesoccerblocked.jpg 1034w, https://torrentfreak.com/images/livesoccerblocked-300x191.jpg 300w, https://torrentfreak.com/images/livesoccerblocked-600x383.jpg 600w, https://torrentfreak.com/images/livesoccerblocked-150x96.jpg 150w" sizes="(max-width: 600px) 100vw, 600px">
</picture>
</center>
<p>Google’s basic response is not limited to the recent Belgian court order. We observed the same query refusal for domain names that were included in French blocking orders over the past several months.</p>
<h2>Transparency is Paramount</h2>
<p>While the different responses from DNS resolvers are interesting, Google’s approach doesn’t make blocking efforts more transparent. These orders are still relatively new, so it’s possible that the company is working on offering more transparency in the future, but currently it only adds to the confusion. </p>
<p>Google’s response also appears to go against the advice of the Belgian court, which required the DNS providers to redirect users to a dedicated page, presumably to provide further detail.</p>
<center><em>Redirect</em></center><br><center><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://torrentfreak.com/images/redirect.jpg.webp 1089w, https://torrentfreak.com/images/redirect-300x32.jpg.webp 300w" sizes="(max-width: 600px) 100vw, 600px">
<img loading="lazy" decoding="async" src="https://torrentfreak.com/images/redirect.jpg" alt="redirect" width="600" height="64" srcset="https://torrentfreak.com/images/redirect.jpg 1089w, https://torrentfreak.com/images/redirect-300x32.jpg 300w, https://torrentfreak.com/images/redirect-600x64.jpg 600w, https://torrentfreak.com/images/redirect-150x16.jpg 150w" sizes="(max-width: 600px) 100vw, 600px">
</picture>
</center>
<p>If these blocking orders are upheld by various courts, a more streamlined approach will be welcome. Interfering with DNS is a big step that can’t be taken lightly, so transparency is paramount. That’s relevant for the United States too, where a new <a href="https://torrentfreak.com/new-bill-aims-to-block-foreign-pirate-sites-in-the-u-s-250129/">site-blocking bill</a> also proposes public DNS resolver blockades.</p>
<p><em>—</em></p><p><em>For context, a copy of the recent Belgian court order shared by Cloudflare is availableb<a href="https://torrentfreak.com/images/Belgian_Pro_League_Order.pdf">here (pdf)</a></em></p>

      </div></div>]]></description>
        </item>
    </channel>
</rss>