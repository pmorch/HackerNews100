<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 20 Dec 2024 23:30:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[OpenAI O3 breakthrough high score on ARC-AGI-PUB (771 pts)]]></title>
            <link>https://arcprize.org/blog/oai-o3-pub-breakthrough</link>
            <guid>42473321</guid>
            <pubDate>Fri, 20 Dec 2024 18:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">https://arcprize.org/blog/oai-o3-pub-breakthrough</a>, See on <a href="https://news.ycombinator.com/item?id=42473321">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            <h2>ARC Prize remains undefeated.<br>New ideas still needed<span>.</span></h2>
        </p><div>


<p>OpenAI's new o3 system - trained on the ARC-AGI-1 Public Training set - has scored a breakthrough <strong>75.7%</strong> on the Semi-Private Evaluation set at our stated public leaderboard $10k compute limit. A high-compute (172x) o3 configuration scored <strong>87.5%</strong>.</p>

<p><img src="https://arcprize.org/media/images/blog/o-series-performance.jpg" alt="o Series Performance"></p>

<p>This is a surprising and important step-function increase in AI capabilities, showing novel task adaptation ability never seen before in the GPT-family models. For context, ARC-AGI-1 took 4 years to go from 0% with GPT-3 in 2020 to 5% in 2024 with GPT-4o. All intuition about AI capabilities will need to get updated for o3.</p>

<p>The mission of ARC Prize goes beyond our first benchmark: to be a North Star towards AGI. And we're excited to be working with the OpenAI team and others next year to continue to design next-gen, enduring AGI benchmarks.</p>

<p>ARC-AGI-2 (same format - verified easy for humans, harder for AI) will launch alongside ARC Prize 2025. We're committed to running the Grand Prize competition until a high-efficiency, open-source solution scoring 85% is created.</p>

<p>Read on for the full testing report.</p>

<hr>

<h2 id="openai-o3-arc-agi-results">OpenAI o3 ARC-AGI Results</h2>

<p>We tested o3 against two ARC-AGI datasets:</p>

<ul>
  <li><strong>Semi-Private Eval</strong>: 100 private tasks used to assess overfitting</li>
  <li><strong>Public Eval</strong>: 400 public tasks</li>
</ul>

<p>At OpenAI's direction, we tested at two levels of compute with variable sample sizes: 6 (high-efficiency) and 1024 (low-efficiency, 172x compute).</p>

<p>Here are the results.</p>

<div>
    <table>
    <thead>
        <tr>
        <th>Set</th>
        <th>Tasks</th>
        <th>Efficiency</th>
        <th>Score</th>
        <th>Retail Cost</th>
        <th>Samples</th>
        <th>Tokens</th>
        <th>Cost/Task</th>
        <th>Time/Task (mins)</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td>Semi-Private</td>
        <td>100</td>
        <td>High</td>
        <td>75.7%</td>
        <td>$2,012</td>
        <td>6</td>
        <td>33M</td>
        <td>$20</td>
        <td>1.3</td>
        </tr>
        <tr>
        <td>Semi-Private</td>
        <td>100</td>
        <td>Low</td>
        <td>87.5%</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        </tr>
        <tr>
        <td>Public</td>
        <td>400</td>
        <td>High</td>
        <td>82.8%</td>
        <td>$6,677</td>
        <td>6</td>
        <td>111M</td>
        <td>$17</td>
        <td>N/A</td>
        </tr>
        <tr>
        <td>Public</td>
        <td>400</td>
        <td>Low</td>
        <td>91.5%</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>-</td>
        <td>--</td>
        </tr>
    </tbody>
    </table>
</div>

<ul>
  <li>Note: OpenAI has requested that we not publish the high-compute costs. The amount of compute was roughly 172x the low-compute configuration.</li>
</ul>

<p>Due to variable inference budget, efficiency (e.g., compute cost) is now a required metric when reporting performance. We've documented both the total costs and the cost per task as an initial proxy for efficiency. As an industry, we'll need to figure out <a href="https://x.com/mikeknoop/status/1868760635716386864" target="_blank">what metric best tracks efficiency</a>, but directionally, cost is a solid starting point.</p>

<p>The high-efficiency score of 75.7% is within the budget rules of ARC-AGI-Pub (costs &lt;$10k) and therefore qualifies as 1st place on the public leaderboard!</p>

<p>The low-efficiency score of 87.5% is quite expensive, but still shows that performance on novel tasks does improve with increased compute (at least up to this level.)</p>

<p>Despite the significant cost per task, these numbers aren't just the result of applying brute force compute to the benchmark. OpenAI's new o3 model represents a significant leap forward in AI's ability to adapt to novel tasks. This is not merely incremental improvement, but a genuine breakthrough, marking a qualitative shift in AI capabilities compared to the prior limitations of LLMs. o3 is a system capable of adapting to tasks it has never encountered before, arguably approaching human-level performance in the ARC-AGI domain.</p>

<p>Of course, such generality comes at a steep cost, and wouldn't quite be economical yet: you could pay a human to solve ARC-AGI tasks for roughly $5 per task (we know, we did that), while consuming mere cents in energy. Meanwhile o3 requires $17-20 per task in the low-compute mode. But cost-performance will likely improve quite dramatically over the next few months and years, so you should plan for these capabilities to become competitive with human work within a fairly short timeline.</p>

<p>o3's improvement over the GPT series proves that architecture is everything. You couldn't throw more compute at GPT-4 and get these results. Simply scaling up the things we were doing from 2019 to 2023 – take the same architecture, train a bigger version on more data – is not enough. Further progress is about new ideas.</p>

<hr>

<h3 id="so-is-it-agi">So is it AGI?</h3>

<p>ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI – as we've repeated dozens of times this year. It's a research tool designed to focus attention on the most challenging unsolved problems in AI, a role it has fulfilled well over the past five years.</p>

<p>Passing ARC-AGI does not equate to achieving AGI, and, as a matter of fact, I don't think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence.</p>

<p>Furthermore, early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.</p>

<h3 id="whats-different-about-o3-compared-to-older-models">What's different about o3 compared to older models?</h3>

<p>Why does o3 score so much higher than o1? And why did o1 score so much higher than GPT-4o in the first place? I think this series of results provides invaluable data points for the ongoing pursuit of AGI.</p>

<p>My mental model for LLMs is that they work as <a href="https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering" target="_blank">a repository of vector programs</a>. When prompted, they will fetch the program that your prompt maps to and "execute" it on the input at hand. LLMs are a way to store and operationalize millions of useful mini-programs via passive exposure to human-generated content.</p>

<p>This "memorize, fetch, apply" paradigm can achieve arbitrary levels of skills at arbitrary tasks given appropriate training data, but it cannot adapt to novelty or pick up new skills on the fly (which is to say that there is no fluid intelligence at play here.) This has been exemplified by the low performance of LLMs on ARC-AGI, the only benchmark specifically designed to measure adaptability to novelty – GPT-3 scored 0, GPT-4 scored near 0, GPT-4o got to 5%. Scaling up these models to the limits of what's possible wasn't getting ARC-AGI numbers anywhere near what basic brute enumeration could achieve years ago (up to 50%).</p>

<p>To adapt to novelty, you need two things. First, you need knowledge – a set of reusable functions or programs to draw upon. LLMs have more than enough of that. Second, you need the ability to recombine these functions into a brand new program when facing a new task – a program that models the task at hand. Program synthesis. LLMs have long lacked this feature. The o series of models fixes that.</p>

<p>For now, we can only speculate about the exact specifics of how o3 works. But o3's core mechanism appears to be natural language program search and execution within token space – at test time, the model searches over the space of possible Chains of Thought (CoTs) describing the steps required to solve the task, in a fashion perhaps not too dissimilar to AlphaZero-style Monte-Carlo tree search. In the case of o3, the search is presumably guided by some kind of evaluator model. To note, Demis Hassabis hinted back in <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/" target="_blank">a June 2023 interview</a> that DeepMind had been researching this very idea – this line of work has been a long time coming.</p>

<p>So while single-generation LLMs struggle with novelty, o3 overcomes this by generating and executing its own programs, where the program itself (the CoT) becomes the artifact of knowledge recombination. Although this is not the only viable approach to test-time knowledge recombination (you could also do test-time training, or search in latent space), it represents the current state-of-the-art as per these new ARC-AGI numbers.</p>

<p>Effectively, o3 represents a form of <em>deep learning-guided program search</em>. The model does test-time search over a space of "programs" (in this case, natural language programs – the space of CoTs that describe the steps to solve the task at hand), guided by a deep learning prior (the base LLM). The reason why solving a single ARC-AGI task can end up taking up tens of millions of tokens and cost thousands of dollars is because this search process has to explore an enormous number of paths through program space – including backtracking.</p>

<p>There are however two significant differences between what's happening here and what I meant when I previously described "deep learning-guided program search" as the best path to get to AGI. Crucially, the programs generated by o3 are <em>natural language instructions</em> (to be "executed" by a LLM) rather than <em>executable symbolic programs</em>. This means two things. First, that they cannot make contact with reality via execution and direct evaluation on the task – instead, they must be evaluated for fitness via another model, and the evaluation, lacking such grounding, might go wrong when operating out of distribution. Second, the system cannot autonomously acquire the ability to generate and evaluate these programs (the way a system like AlphaZero can learn to play a board game on its own.) Instead, it is reliant on expert-labeled, human-generated CoT data.</p>

<p>It's not yet clear what the exact limitations of the new system are and how far it might scale. We'll need further testing to find out. Regardless, the current performance represents a remarkable achievement, and a clear confirmation that intuition-guided test-time search over program space is a powerful paradigm to build AI systems that can adapt to arbitrary tasks.</p>

<h3 id="what-comes-next">What comes next?</h3>

<p>First of all, open-source replication of o3, facilitated by the ARC Prize competition in 2025, will be crucial to move the research community forward. A thorough analysis of o3's strengths and limitations is necessary to understand its scaling behavior, the nature of its potential bottlenecks, and anticipate what abilities further developments might unlock.</p>

<p>Moreover, ARC-AGI-1 is now saturating – besides o3's new score, the fact is that a large ensemble of low-compute Kaggle solutions can now score 81% on the private eval.</p>

<p>We're going to be raising the bar with a new version – ARC-AGI-2 - which has been in the works since 2022. It promises a major reset of the state-of-the-art. We want it to push the boundaries of AGI research with hard, high-signal evals that highlight current AI limitations.</p>

<p>Our early ARC-AGI-2 testing suggests it will be useful and extremely challenging, even for o3. And, of course, ARC Prize's objective is to produce a <em>high-efficiency</em> and <em>open-source</em> solution in order to win the Grand Prize. We currently intend to launch ARC-AGI-2 alongside ARC Prize 2025 (estimated launch: late Q1).</p>

<p>Going forward, the ARC Prize Foundation will continue to create new benchmarks to focus the attention of researchers on the hardest unsolved problems on the way to AGI. We've started work on a third-generation benchmark which departs completely from the 2019 ARC-AGI format and incorporates some exciting new ideas.</p>

<hr>

<h2 id="get-involved-open-source-analysis">Get Involved: Open-Source Analysis</h2>

<p>Today, we're also releasing high-compute, o3-labeled tasks and would like your help to analyze them. In particular, we are very curious about the ~9% set of Public Eval tasks o3 was unable to solve, even with lots of compute, yet are straightforward for humans.</p>

<p>We invite the community to help us assess the characteristics of both solved and unsolved tasks.</p>

<p>To get your ideas flowing, here are 3 examples of tasks unsolved by high-compute o3.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-c6e1b8da.png" alt="ARC-AGI Task Id: c6e1b8da">
  <figcaption>ARC-AGI Task ID: c6e1b8da</figcaption>
</figure>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-0d87d2a6.png" alt="ARC-AGI Task Id: 0d87d2a6">
  <figcaption>ARC-AGI Task ID: 0d87d2a6</figcaption>
</figure>

<figure>
  <img src="https://arcprize.org/media/images/blog/arc-agi-task-b457fec5.png" alt="ARC-AGI Task Id: b457fec5">
  <figcaption>ARC-AGI Task ID: b457fec5</figcaption>
</figure>

<p><a href="https://github.com/arcprizeorg/model_baseline/tree/main/results" target="_blank">See our full set of o3 testing data.</a></p>

<p>We've also created a new channel in our Discord named <code>oai-analysis</code> and we'd love to hear your analysis and insights there. Or tag us on X/Twitter <a href="https://x.com/arcprize" target="_blank">@arcprize</a>.</p>

<hr>

<h2 id="conclusions">Conclusions</h2>

<p>To sum up – o3 represents a significant leap forward. Its performance on ARC-AGI highlights a genuine breakthrough in adaptability and generalization, in a way that no other benchmark could have made as explicit.</p>

<p>o3 fixes the fundamental limitation of the LLM paradigm – the inability to recombine knowledge at test time – and it does so via a form of LLM-guided natural language program search. This is not just incremental progress; it is new territory, and it demands serious scientific attention.</p>

<p><span></span> <a href="#" data-modal-id="newsletter">Sign up to get updates</a></p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grayjay Desktop App (258 pts)]]></title>
            <link>https://grayjay.app/desktop/</link>
            <guid>42473032</guid>
            <pubDate>Fri, 20 Dec 2024 17:33:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grayjay.app/desktop/">https://grayjay.app/desktop/</a>, See on <a href="https://news.ycombinator.com/item?id=42473032">Hacker News</a></p>
Couldn't get https://grayjay.app/desktop/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why are UK electricity bills so expensive? (133 pts)]]></title>
            <link>https://climate.benjames.io/uk-electricity-bills/</link>
            <guid>42472247</guid>
            <pubDate>Fri, 20 Dec 2024 16:05:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://climate.benjames.io/uk-electricity-bills/">https://climate.benjames.io/uk-electricity-bills/</a>, See on <a href="https://news.ycombinator.com/item?id=42472247">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>I recently built a website that breaks down the cost of a UK electricity bill.</p><figure><a href="https://electricitybills.uk/?ref=climate.benjames.io"><img src="https://climate.benjames.io/content/images/2024/12/image.png" alt="" loading="lazy" width="2000" height="1121" srcset="https://climate.benjames.io/content/images/size/w600/2024/12/image.png 600w, https://climate.benjames.io/content/images/size/w1000/2024/12/image.png 1000w, https://climate.benjames.io/content/images/size/w1600/2024/12/image.png 1600w, https://climate.benjames.io/content/images/2024/12/image.png 2124w" sizes="(min-width: 720px) 720px"></a><figcaption><a href="http://electricitybills.uk/?ref=climate.benjames.io">electricitybills.uk</a></figcaption></figure><p>It's interactive, and I'd recommend visiting it before reading this post. Check it out here: <a href="http://electricitybills.uk/?ref=climate.benjames.io">electricitybills.uk</a></p><p>Here are three interesting things about the data.</p><h3 id="1-the-wholesale-power-cost-is-only-one-third-of-an-electricity-bill">#1: The wholesale power cost is only one third of an electricity bill.</h3><p>The wholesale price is the actual cost of buying electricity on the open market. But the average bill is <strong>triple</strong> that amount.</p><figure><img src="https://climate.benjames.io/content/images/2024/12/bill_breakdown_full_manual--1-.png" alt="" loading="lazy" width="1204" height="1107" srcset="https://climate.benjames.io/content/images/size/w600/2024/12/bill_breakdown_full_manual--1-.png 600w, https://climate.benjames.io/content/images/size/w1000/2024/12/bill_breakdown_full_manual--1-.png 1000w, https://climate.benjames.io/content/images/2024/12/bill_breakdown_full_manual--1-.png 1204w" sizes="(min-width: 720px) 720px"></figure><p>The remaining 2/3 of the bill is made up of three parts:</p><ul><li><strong>Network costs:</strong> paying for the wires and substations of the power grid</li><li><strong>Generation costs:</strong> subsidising strategically important generation, like offshore wind, household solar, and firm gas.</li><li><strong>Miscellaneous: </strong>running a utility company customer service department, various taxes, etc.</li></ul><h3 id="2-these-charges-are-about-to-rise-a-lot">#2: These charges are about to rise, a lot.</h3><p><strong>Network costs </strong>are about to skyrocket. Investment in the UK power grid has been stagnant for 20 years, because UK power demand has been flat for 20 years. But now, the UK urgently needs to expand the grid. Energy that used to flow through pipelines will need to flow through wires.</p><p><strong>Contracts for Difference </strong>are the UK's flagship scheme for supporting renewables, and they will add an increasing cost to electricity bills. More than half of the contracts already allocated have yet to be activated, and the next contract allocation round is expected to be the biggest yet.</p><p><em>Note: CfDs do insulate consumers from high wholesale prices (during the energy crisis, CfDs reduced consumer bills), but on average they add cost.</em></p><figure><img src="https://climate.benjames.io/content/images/2024/12/cfd_svg--2-.png" alt="" loading="lazy" width="1087" height="514" srcset="https://climate.benjames.io/content/images/size/w600/2024/12/cfd_svg--2-.png 600w, https://climate.benjames.io/content/images/size/w1000/2024/12/cfd_svg--2-.png 1000w, https://climate.benjames.io/content/images/2024/12/cfd_svg--2-.png 1087w" sizes="(min-width: 720px) 720px"></figure><p>The <strong>Capacity Market</strong> pays firm generation &amp; demand response to be on standby, to prevent blackouts. Contracts are mostly allocated four years in advance, and the cost will ~<em>triple</em><strong> </strong>to 2028.</p><figure><img src="https://climate.benjames.io/content/images/2024/12/CM-clearing-prices--1-.png" alt="" loading="lazy" width="942" height="427" srcset="https://climate.benjames.io/content/images/size/w600/2024/12/CM-clearing-prices--1-.png 600w, https://climate.benjames.io/content/images/2024/12/CM-clearing-prices--1-.png 942w" sizes="(min-width: 720px) 720px"></figure><h3 id="3-existing-costs-are-locked-in-for-a-long-time">#3: Existing costs are locked in for a long time.</h3><p>The UK ran two pretty expensive green subsidies in the 2010s.</p><p>The <strong>Renewables Obligation</strong> mandates utilities to buy credits from wind and solar farms. It closed to new projects in 2017, but payments to existing projects will continue until 2037. It makes up around 10% of an average bill.</p><div><p>The Renewables Obligation is, in my opinion, wild. Renewable generators who got accredited before 2017 essentially get paid an ever-rising inflation-linked price until 2037, regardless of the market price of electricity.&nbsp;</p></div><p>The <strong>Feed in Tariff</strong> pays households with solar panels a very tasty rate for exported energy. It closed to new applications in 2019, but payments will continue up to 2044 for some projects.</p><p>Being an early adopter of renewables has been expensive for the UK. You might argue that we should have waited an extra decade, since renewables would now be <a href="https://climate.benjames.io/solar-off-grid">much cheaper</a>. But the reality of learning curves means that renewables only got cheap because people built them. If everyone waits for someone else to decarbonise first, we won't get very far.</p><p>But we can learn from policy mistakes in the past. Schemes like the Feed in Tariff did not correct quickly enough when solar prices fell, leading to spiralling policy costs that were completely decoupled from market dynamics.</p><h2 id="the-future-of-cheap-clean-power">The future of cheap, clean power.</h2><p>Let's say that we want electricity to be radically cheap in future - say £50 / MWh.</p><p>Well, network costs are already ~£70 / MWh and will increase steadily. We've missed our target, before we've even paid to generate electricity. There are only two solutions here:</p><ul><li>Ditch the power grid. Use local solar generation, and a lot of batteries. This works well in most of the world, but less so in northern Europe (it's not very sunny).</li><li>Utilise the existing power grid better. Instead of expanding the grid just to service peak demand, improve our grid utilisation by "filling in the rectangle" throughout the day. (This is part of what we're working on at <a href="https://axle.energy/?ref=climate.benjames.io">Axle</a>)</li></ul><p>The UK has achieved the fastest rate of grid decarbonisation among advanced economies. A lot of this progress occurred when renewables were still expensive, so we are stuck with a cost hangover. Luckily, renewables are getting <em>much</em> cheaper, so the tradeoffs in future policy are very different.</p><hr><p>Thanks for reading, I'd love to hear your thoughts on the site. <a href="https://climate.benjames.io/uk-electricity-bills/electricitybills.uk">electricitybills.uk</a></p><p>Warmly,</p><p>Ben</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A bestiary of exotic hadrons (105 pts)]]></title>
            <link>https://cerncourier.com/a-bestiary-of-exotic-hadrons/</link>
            <guid>42471927</guid>
            <pubDate>Fri, 20 Dec 2024 15:29:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cerncourier.com/a-bestiary-of-exotic-hadrons/">https://cerncourier.com/a-bestiary-of-exotic-hadrons/</a>, See on <a href="https://news.ycombinator.com/item?id=42471927">Hacker News</a></p>
Couldn't get https://cerncourier.com/a-bestiary-of-exotic-hadrons/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Artemis, a Calm Web Reader (186 pts)]]></title>
            <link>https://artemis.jamesg.blog/</link>
            <guid>42471913</guid>
            <pubDate>Fri, 20 Dec 2024 15:28:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artemis.jamesg.blog/">https://artemis.jamesg.blog/</a>, See on <a href="https://news.ycombinator.com/item?id=42471913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            
            

            
            
<article>
    <h2>Artemis</h2>
    <p>Artemis is a calm web reader.</p>
    <p>You can use Artemis to follow your favorite websites.</p>
    <p>Artemis updates once per day, at approximately 12am in your timezone.</p>
    <p><a href="https://artemis.jamesg.blog/features" target="_blank">See a list of features.</a></p>
    <figure>
        <img src="https://artemis.jamesg.blog/assets/demo.png" alt="The Artemis dashboard, showing the titles and domains from for two posts from feeds." height="243" width="543">
        <figcaption>The Artemis dashboard.</figcaption>
    </figure>
    <h2>Design</h2>
    <p>Artemis is designed to be slow and minimal. It's a calm place to see what's new on your favorite websites, with no urgency.</p>
    <p><a href="https://jamesg.blog/2024/11/30/designing-a-calm-web-reader/">Read more about the project design philosophy.</a></p>
    <h3>Data Storage</h3>
    <p><a href="https://artemis.jamesg.blog/data">Read how we store your data.</a></p>
    <h3>Accessibility</h3>
    <p>I have tried my best to make this service accessible. If you notice any issues, please feel free to contact me.</p>
    <p><a href="https://artemis.jamesg.blog/accessibility">Read our accessibility statement.</a></p>
    <h2>Pricing</h2>
    <p>Artemis is free to use, although <a href="https://github.com/sponsors/capjamesg/">donations are appreciated</a>!</p>
    <h2>About</h2>
    <p>Artemis is made by <a href="https://jamesg.blog/">capjamesg</a>.</p>
    <p>Need tech support? Contact <a href="mailto:readers@jamesg.blog">jamesg@jamesg.blog</a></p>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Matt Mullenweg temporarily shuts down some Wordpress.org functions (119 pts)]]></title>
            <link>https://wordpress.org/news/2024/12/holiday-break/</link>
            <guid>42469708</guid>
            <pubDate>Fri, 20 Dec 2024 10:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordpress.org/news/2024/12/holiday-break/">https://wordpress.org/news/2024/12/holiday-break/</a>, See on <a href="https://news.ycombinator.com/item?id=42469708">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>In order to give myself and the many tired volunteers around WordPress.org a break for the holidays, we’re going to be pausing a few of the free services currently offered:</p>



<ul>
<li>New account registrations on WordPress.org (clarifying so press doesn’t confuse this: people can still make their own WordPress installs and accounts)</li>



<li>New <a href="https://wordpress.org/plugins/">plugin directory</a> submissions</li>



<li>New plugin reviews</li>



<li>New <a href="https://wordpress.org/themes/">theme directory</a> submissions</li>



<li>New <a href="https://wordpress.org/photos/">photo directory</a> submissions</li>
</ul>



<p>We’re going to leave things like localization and the forums open because these don’t require much moderation.</p>



<p>As you may have heard, <a href="https://www.theverge.com/2024/12/10/24318350/automattic-restore-wp-engine-access-wordpress">I’m legally compelled to provide free labor and services to WP Engine thanks to the success of their expensive lawyers</a>, so in order to avoid bothering the court I will say that none of the above applies to WP Engine, so if they need to bypass any of the above please just have your high-priced attorneys talk to my high-priced attorneys and we’ll arrange access, or just reach out directly to me on Slack and I’ll fix things for you.</p>



<p>I hope to find the time, energy, and money to reopen all of this sometime in the new year. Right now much of the time I would spend making WordPress better is being taken up defending against WP Engine’s legal attacks. Their attacks are against Automattic, but also me individually as the owner of WordPress.org, which means if they win I can be personally liable for millions of dollars of damages.</p>



<p>If you would like to fund legal attacks against me, I would encourage you to sign up for WP Engine services, <a href="https://wpengine.com/plans/">they have great plans and pricing starting at $50/mo and scaling all the way up to $2,000/mo</a>. If not, you can use <a href="https://wordpress.org/news/2024/10/wp-engine-promotions/">literally any other web host in the world that isn’t suing me and is offering promotions and discounts for switching away from WP Engine</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo achieves 92% reduction in bodily injury claims compared to human drivers (175 pts)]]></title>
            <link>https://waymo.com/research/do-autonomous-vehicles-outperform-latest-generation-human-driven-vehicles-25-million-miles/</link>
            <guid>42469264</guid>
            <pubDate>Fri, 20 Dec 2024 08:26:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/research/do-autonomous-vehicles-outperform-latest-generation-human-driven-vehicles-25-million-miles/">https://waymo.com/research/do-autonomous-vehicles-outperform-latest-generation-human-driven-vehicles-25-million-miles/</a>, See on <a href="https://news.ycombinator.com/item?id=42469264">Hacker News</a></p>
Couldn't get https://waymo.com/research/do-autonomous-vehicles-outperform-latest-generation-human-driven-vehicles-25-million-miles/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tldraw Computer (423 pts)]]></title>
            <link>https://computer.tldraw.com</link>
            <guid>42469074</guid>
            <pubDate>Fri, 20 Dec 2024 07:42:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.tldraw.com">https://computer.tldraw.com</a>, See on <a href="https://news.ycombinator.com/item?id=42469074">Hacker News</a></p>
Couldn't get https://computer.tldraw.com: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[My favourite colour is Chuck Norris red (500 pts)]]></title>
            <link>https://htmhell.dev/adventcalendar/2024/20/</link>
            <guid>42468318</guid>
            <pubDate>Fri, 20 Dec 2024 04:35:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmhell.dev/adventcalendar/2024/20/">https://htmhell.dev/adventcalendar/2024/20/</a>, See on <a href="https://news.ycombinator.com/item?id=42468318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="content"><div><p>by <a href="https://vale.rocks/">Declan Chidlow</a> published on <time datetime="2024-12-20">Dec 20, 2024</time></p><p>Setting the colour of text on a webpage is usually a simple affair involving whipping it out the good ol' CSS <code>color</code> property. But this is HTMHell, dammit. None of that wishy-washy CSS nonsense here. No siree. We use HTML as the good lord intended and shalln't stray into the sins of cascading sheets lest we end up some non-HTML variant of hell where they define page structure with JavaScript vars.</p><p>But HTML isn't great for defining styles -- or at least, it isn't anymore. If we wind back the clocks a few years to HTML versions of old, we find the colour attribute. If you've been around for a while, you've no doubt seen it. Something like this:</p><pre><code><span><span><span><span>&lt;</span>font</span> <span>color</span><span><span>=</span><span>"</span>#d72b2b<span>"</span></span><span>&gt;</span></span>HTMHell rules!<span><span><span>&lt;/</span>font</span><span>&gt;</span></span></span></code></pre><p><span color="#d72b2b">HTMHell rules!</span></p><p>If we render that in a browser, we get some text in the lovely HTMHell red. That's great. That's what we'd expect. Next we'll choose another colour. Something a bit different. Let's try 'chucknorris'.</p><pre><code><span><span><span><span>&lt;</span>font</span> <span>color</span><span><span>=</span><span>"</span>chucknorris<span>"</span></span><span>&gt;</span></span>But... Chuck Norris isn't a colour.<span><span><span>&lt;/</span>font</span><span>&gt;</span></span></span></code></pre><p><span color="chucknorris">But... Chuck Norris isn't a colour.</span></p><p>If you go through the effort of loading <em>that</em> up in a browser, you might notice it makes the text red. Why?</p><h2 id="some-funny-character-parsing">Some funny character parsing</h2><p>HTML generally doesn't have an error state, at least not one akin to what would happen if writing something like invalid JavaScript. Browsers are very forgiving when parsing HTML (which explains how people have gotten away with the crimes documented throughout this website) and generally do their best to make up for user error. If you leave a dangling <code>&lt;div&gt;</code>, the browser will do its best to close it up and render it out.</p><p>This forgiveness is the reason behind the funkiness. Browsers simply try to forge ahead with the invalid value and hope it'll work. In the past web browsers all handled invalid values a bit differently, but now it's all outlined in the <a href="https://html.spec.whatwg.org/multipage/common-microsyntaxes.html#rules-for-parsing-a-legacy-colour-value">"rules for parsing a legacy color value" part of the HTML spec</a>. A surmised version of the parsing outlined there is as follows:</p><ol><li><p>Initial Cleanup:</p><ul><li>If an octothorpe (#) is located at the start of the value, it's removed.</li><li>The colour attribute only accepts hexes, so there isn't a point keeping it.</li><li>Example: "#FF0000" becomes "FF0000".</li></ul></li><li><p>Replace Invalid Characters:</p><ul><li>Any non-hexadecimal characters (anything not 0-9 or A-F/a-f) are removed and replaced with '0'.</li><li>Example: 'abcxyz123' becomes 'abc000123'.</li></ul></li><li><p>Standardise Length:</p><ul><li>While the string's length is 0 or not divisible by 3, append '0'.</li><li>Examples:<ul><li>"F" becomes "F00" (padded to length 3).</li><li>"FFFF" becomes "FFFF00" (padded to length 6).</li><li>"FFFFFF0" becomes "FFFFFF000" (padded to length 9).</li></ul></li></ul></li><li><p>Split into Red, Green, and Blue:</p><ul><li>The first third becomes the red value.</li><li>The second third becomes the green value.</li><li>The last third becomes the blue value.</li><li>Example: "FFFFFF000" becomes ["FFF", "FFF", "000"].</li></ul></li><li><p>Handle Length:</p><ul><li>If any component is longer than 8 characters, remove the characters from the left until it's 8 characters long.<ul><li>Example: "123456789" → "23456789"</li></ul></li><li>While the length is greater than 2, and all components start with '0', remove the leading '0' from each component.<ul><li>Example: ["000F", "000F", "000F"] becomes ["00F", "00F", "00F"] which then becomes ["0F", "0F", "0F"].</li></ul></li><li>If length is still greater then 2 keep only the first 2 characters of each component.<ul><li>Example: ["ABC", "DEF", "123"] becomes ["AB", "DE", "12"].</li></ul></li></ul></li><li><p>Putting It Together:</p><ul><li>Get the final red, blue, and green components, then put them together in that order to create the colour.</li><li>Example: ["AB", "DE", "12"] becomes ABDE12.</li></ul></li></ol><p>I've written a small tool over on CodePen that will take any inputted value, break down the processing step by step, and output the colour as it would be handled. Go have a bit of fiddle!</p><p data-height="300" data-default-tab="result" data-slug-hash="yLmKBpN" data-pen-title="Legacy HTML Colour Parsing Demo" data-user="OuterVale"><span>See the Pen <a href="https://codepen.io/OuterVale/pen/yLmKBpN">Legacy HTML Colour Parsing Demo</a> by Declan Chidlow (<a href="https://codepen.io/OuterVale">@OuterVale</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p><h2 id="some-fun-examples">Some fun examples</h2><p>So, we know this happens and why. The next task is obviously to have some fun with it. Finding words whose computed colours correlate with them is great fun. For example, 'Sonic' gives us a lovely blue like the hedgehog. I've put together a little table of some of these coincidental match ups:</p><p data-height="300" data-default-tab="result" data-slug-hash="wvLbjpZ" data-pen-title="Word Correlations With HTML Colour Parsing" data-user="OuterVale"><span>See the Pen <a href="https://codepen.io/OuterVale/pen/wvLbjpZ">Word Correlations With HTML Colour Parsing</a> by Declan Chidlow (<a href="https://codepen.io/OuterVale">@OuterVale</a>) on <a href="https://codepen.io/">CodePen</a>.</span></p><h2 id="interesting-parsing-in-the-modern-era">Interesting parsing in the modern era</h2><p>So, that's all well and good, but it's old news. The <code>color</code> and <code>bgcolor</code> attributes that permitted our parsing adventures are relics of HTML 4. They're obsolete (though still in active use on a disturbingly high number of websites). That isn't to say quirks like that have disappeared completely though. CSS has its own set of fascinating peculiarities when it comes to handling invalid colour values. Most modern browsers will clamp values rather than reject them outright -– throw rgb(300, -50, 1000) at a browser and it won't fail; it'll helpfully transform it into rgb(255, 0, 255).</p><p>The web's foundational principle of forgiveness -– the inherent flexibility that allows "chucknorris" to be parsed as red, even though the reason it does so is old, silly, and unsupported –- hasn't gone anywhere. Modern browsers still bend over backward to make our code work, even when we throw nonsense at them. It doesn't take long to see this forgiveness in action within the cursed examples held within the pages of HTMHell. Each horrifying snippet, each questionable hack, each "it works but why" moment exists because browsers simply refuse to give up on rendering our 'mistakes'.</p><p>The web is built on this foundation of resilience, both in technology and <a href="https://www.w3.org/blog/2022/a-letter-from-our-ceo-the-web-as-the-ultimate-tool-of-resilience-for-the-world">ethos</a>. It's what allows a website from 1996 to still render in a modern browser. It's what lets a page load even when half the CSS is invalid. It's what makes it magic.</p><p>I've heard people quip that browsers should be less forgiving and enforce perfection. That allowing jank makes the web somehow 'bad'. I think a perfect web would be a boring web. I certainly wouldn't be here writing were it 'perfect'. It's about making the web work, no matter what we throw at it, and I wouldn't have it any other way.</p><p>After all, in a perfect web, "chucknorris" would just be another error message -– and where's the fun in that?</p><h2 id="resources">Resources</h2><ul><li><a href="http://scrappy-do.blogspot.com/2004/08/little-rant-about-microsoft-internet.html">Sam's Place - A little rant about Microsoft Internet Explorer's color parsing</a></li><li><a href="https://html.spec.whatwg.org/multipage/common-microsyntaxes.html#rules-for-parsing-a-legacy-colour-value">HTML Standard</a></li><li><a href="https://stackoverflow.com/q/8318911">Why does HTML think "chucknorris" is a color?</a></li></ul><h2 id="about-declan-chidlow">About Declan Chidlow</h2><p>Front-end developer, designer, dabbler, and avid user of the superpowered information superhighway.</p><p>Website: <a href="https://vale.rocks/">vale.rocks</a><br>Fediverse: <a href="https://fedi.vale.rocks/vale">@vale@fedi.vale.rocks</a><br>Bluesky: <a href="https://bsky.app/profile/vale.rocks">@vale.rocks</a></p><h2 id="more-articles">More articles</h2><nav aria-label="Select next or previous entry"><ol><li><a href="https://htmhell.dev/adventcalendar/2024/19/" rel="prev"><p>Previous day (19)</p>Getting Oriented with HTML Video</a></li></ol></nav></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Gentle Introduction to Graph Neural Networks (291 pts)]]></title>
            <link>https://distill.pub/2021/gnn-intro/</link>
            <guid>42468214</guid>
            <pubDate>Fri, 20 Dec 2024 04:10:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://distill.pub/2021/gnn-intro/">https://distill.pub/2021/gnn-intro/</a>, See on <a href="https://news.ycombinator.com/item?id=42468214">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><em>This article is one of two Distill publications about graph neural networks. Take a look at <a href="https://distill.pub/2021/understanding-gnns/">Understanding Convolutions on Graphs</a><d-cite key="daigavane2021understanding"></d-cite> to understand how convolutions over images generalize naturally to convolutions over graphs.</em></p>
<p>Graphs are all around us; real world objects are often defined in terms of their connections to other things. A set of objects, and the connections between them, are naturally expressed as a <em>graph</em>. Researchers have developed neural networks that operate on graph data (called graph neural networks, or GNNs) for over a decade<d-cite key="Scarselli2009-ku"></d-cite>. Recent developments have increased their capabilities and expressive power. We are starting to see practical applications in areas such as antibacterial discovery <d-cite key="Stokes2020-az"></d-cite>, physics simulations  <d-cite key="Sanchez-Gonzalez2020-yo"></d-cite>, fake news detection <d-cite key="Monti2019-tf"></d-cite>, traffic prediction <d-cite key="undated-sy"></d-cite> and recommendation systems <d-cite key="Eksombatchai2017-il"></d-cite>.</p>
<p>This article explores and explains modern graph neural networks. We divide this work into four parts. First, we look at what kind of data is most naturally phrased as a graph, and some common examples. Second, we explore what makes graphs different from other types of data, and some of the specialized choices we have to make when using graphs. Third, we build a modern GNN, walking through each of the parts of the model, starting with historic modeling innovations in the field. We move gradually from a bare-bones implementation to a state-of-the-art GNN model. Fourth and finally, we provide a GNN playground where you can play around with a real-word task and dataset to build a stronger intuition of how each component of a GNN model contributes to the predictions it makes.</p>
<p>To start, let’s establish what a graph is. A graph represents the relations (<em>edges</em>) between a collection of entities (<em>nodes</em>). </p>
<figure>
<figcaption>
Three types of attributes we might find in a graph, hover over to highlight each attribute. Other types of graphs and attributes are explored in the <a href="#other-types-of-graphs-multigraphs-hypergraphs-hypernodes">Other types of graphs</a> section.
</figcaption></figure>


<p>To further describe each node, edge or the entire graph, we can store information in each of these pieces of the graph. </p>
<figure>
<figcaption>
Information in the form of scalars or embeddings can be stored at each graph node (left) or edge (right).
</figcaption></figure>

<p>We can additionally specialize graphs by associating directionality to edges (<em>directed, undirected</em>). </p>
<figure><img src="https://distill.pub/2021/gnn-intro/directed_undirected.e4b1689d.png" '="">
<figcaption>
The edges can be directed, where an edge $e$ has a source node, $v_{src}$, and a destination node $v_{dst}$. In this case, information flows from $v_{src}$ to $v_{dst}$. They can also be undirected, where there is no notion of source or destination nodes, and information flows both directions. Note that having a single undirected edge is equivalent to having one directed edge from $v_{src}$ to $v_{dst}$, and another directed edge from $v_{dst}$ to $v_{src}$.
</figcaption></figure>

<p>Graphs are very flexible data structures, and if this seems abstract now, we will make it concrete with examples in the next section. </p>
<h2 id="graphs-and-where-to-find-them">Graphs and where to find them</h2>
<p>You’re probably already familiar with some types of graph data, such as social networks. However, graphs are an extremely powerful and general representation of data, we will show two types of data that you might not think could be modeled as graphs: images and text. Although counterintuitive, one can learn more about the symmetries and structure of images and text by viewing them as graphs,, and build an intuition that will help understand other less grid-like graph data, which we will discuss later.</p>
<h3 id="images-as-graphs">Images as graphs</h3>
<p>We typically think of images as rectangular grids with image channels, representing them as arrays (e.g., 244x244x3 floats). Another way to think of images is as graphs with regular structure, where each pixel represents a node and is connected via an edge to adjacent pixels. Each non-border pixel has exactly 8 neighbors, and the information stored at each node is a 3-dimensional vector representing the RGB value of the pixel.</p>
<p>A way of visualizing the connectivity of a graph is through its <em>adjacency matrix</em>. We order the nodes, in this case each of 25 pixels in a simple 5x5 image of a smiley face, and fill a matrix of $n_{nodes} \times n_{nodes}$ with an entry if two nodes share an edge. Note that each of these three representations below are different views of the same piece of data. </p>
<figure>

<figcaption>
Click on an image pixel to toggle its value, and see how the graph representation changes.
</figcaption>
</figure>

<h3 id="text-as-graphs">Text as graphs</h3>
<p>We can digitize text by associating indices to each character, word, or token, and representing text as a sequence of these indices. This creates a simple directed graph, where each character or index is a node and is connected via an edge to the node that follows it.</p>
<figure>

<figcaption>
Edit the text above to see how the graph representation changes.
</figcaption>
</figure>

<p>Of course, in practice, this is not usually how text and images are encoded: these graph representations are redundant since all images and all text will have very regular structures. For instance, images have a banded structure in their adjacency matrix because all nodes (pixels) are connected in a grid. The adjacency matrix for text is just a diagonal line, because each word only connects to the prior word, and to the next one. </p>


<h3 id="graph-valued-data-in-the-wild">Graph-valued data in the wild</h3>
<p>Graphs are a useful tool to describe data you might already be familiar with. Let’s move on to data which is more heterogeneously structured. In these examples, the number of neighbors to each node is variable (as opposed to the fixed neighborhood size of images and text). This data is hard to phrase in any other way besides a graph.</p>
<p><strong>Molecules as graphs.</strong> Molecules are the building blocks of matter, and are built of atoms and electrons in 3D space. All particles are interacting, but when a pair of atoms are stuck in a stable distance from each other, we say they share a covalent bond. Different pairs of atoms and bonds have different distances (e.g. single-bonds, double-bonds). It’s a very convenient and common abstraction to describe this 3D object as a graph, where nodes are atoms and edges are covalent bonds. <d-cite key="Duvenaud2015-yc"></d-cite> Here are two common molecules, and their associated graphs.</p>
<figure>
<figcaption>
(Left) 3d representation of the Citronellal molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.
</figcaption>
</figure>

<figure>
<figcaption>
(Left) 3d representation of the Caffeine molecule (Center) Adjacency matrix of the bonds in the molecule (Right) Graph representation of the molecule.
</figcaption>
</figure>


<p><strong>Social networks as graphs.</strong> Social networks are tools to study patterns in collective behaviour of people, institutions and organizations. We can build a graph representing groups of people by modelling individuals as nodes, and their relationships as edges. </p>
<figure>
<figcaption>
(Left) Image of a scene from the play “Othello”. (Center) Adjacency matrix of the interaction between characters in the play. (Right) Graph representation of these interactions.
</figcaption>
</figure>

<p>Unlike image and text data, social networks do not have identical adjacency matrices. </p>
<figure>
<figcaption>
(Left) Image of karate tournament. (Center) Adjacency matrix of the interaction between people in a karate club. (Right) Graph representation of these interactions.
</figcaption>
</figure>

<p><strong>Citation networks as graphs.</strong> Scientists routinely cite other scientists’ work when publishing papers. We can visualize these networks of citations as a graph, where each paper is a node, and each <em>directed</em> edge is a citation between one paper and another. Additionally, we can add information about each paper into each node, such as a word embedding of the abstract. (see <d-cite key="Mikolov2013-vr"></d-cite>,  <d-cite key="Devlin2018-mi"></d-cite>&nbsp;,  <d-cite key="Pennington2014-kg"></d-cite>). </p>
<p><strong>Other examples.</strong> In computer vision, we sometimes want to tag objects in visual scenes. We can then build graphs by treating these objects as nodes, and their relationships as edges. <a href="https://www.tensorflow.org/tensorboard/graphs">Machine learning models</a>, <a href="https://openreview.net/pdf?id=BJOFETxR-">programming code</a> <d-cite key="Allamanis2017-kz"></d-cite> and <a href="https://openreview.net/forum?id=S1eZYeHFDS">math equations</a><d-cite key="Lample2019-jg"></d-cite> can also be phrased as graphs, where the variables are nodes, and edges are operations that have these variables as input and output. You might see the term “dataflow graph” used in some of these contexts.</p>
<p>The structure of real-world graphs can vary greatly between different types of data — some graphs have many nodes with few connections between them, or vice versa. Graph datasets can vary widely (both within a given dataset, and between datasets) in terms of the number of nodes, edges, and the connectivity of nodes.</p>
<figure>

<figcaption>

<p>Summary statistics on graphs found in the real world. Numbers are dependent on featurization decisions. More useful statistics and graphs can be found in KONECT<d-cite key="Kunegis2013-er"></d-cite></p>
</figcaption></figure>

<h2 id="what-types-of-problems-have-graph-structured-data">What types of problems have graph structured data?</h2>
<p>We have described some examples of graphs in the wild, but what tasks do we want to perform on this data? There are three general types of prediction tasks on graphs: graph-level, node-level, and edge-level. </p>
<p>In a graph-level task, we predict a single property for a whole graph. For a node-level task, we predict some property for each node in a graph. For an edge-level task, we want to predict the property or presence of edges in a graph.</p>
<p>For the three levels of prediction problems described above (graph-level, node-level, and edge-level), we will show that all of the following problems can be solved with a single model class, the GNN. But first, let’s take a tour through the three classes of graph prediction problems in more detail, and provide concrete examples of each.</p>


<h3 id="graph-level-task">Graph-level task</h3>
<p>In a graph-level task, our goal is to predict the property of an entire graph. For example, for a molecule represented as a graph, we might want to predict what the molecule smells like, or whether it will bind to a receptor implicated in a disease.</p>
<figure>

</figure>

<p>This is analogous to image classification problems with MNIST and CIFAR, where we want to associate a label to an entire image. With text, a similar problem is sentiment analysis where we want to identify the mood or emotion of an entire sentence at once.</p>
<h3 id="node-level-task">Node-level task</h3>
<p>Node-level tasks are concerned with predicting the identity or role of each node within a graph.</p>
<p>A classic example of a node-level prediction problem is Zach’s karate club.<d-cite key="Zachary1977-jg"></d-cite> The dataset is a single social network graph made up of individuals that have sworn allegiance to one of two karate clubs after a political rift. As the story goes, a feud between Mr. Hi (Instructor) and John H (Administrator) creates a schism in the karate club. The nodes represent individual karate practitioners, and the edges represent interactions between these members outside of karate. The prediction problem is to classify whether a given member becomes loyal to either Mr. Hi or John H, after the feud. In this case, distance between a node to either the Instructor or Administrator is highly correlated to this label.</p>
<figure>

<figcaption>
On the left we have the initial conditions of the problem, on the right we have a possible solution, where each node has been classified based on the alliance. The dataset can be used in other graph problems like unsupervised learning. 
</figcaption></figure>

<p>Following the image analogy, node-level prediction problems are analogous to <em>image segmentation</em>, where we are trying to label the role of each pixel in an image. With text, a similar task would be predicting the parts-of-speech of each word in a sentence (e.g. noun, verb, adverb, etc).</p>
<h3 id="edge-level-task">Edge-level task</h3>
<p>The remaining prediction problem in graphs is <em>edge prediction</em>. </p>
<p>One example of edge-level inference is in image scene understanding. Beyond identifying objects in an image, deep learning models can be used to predict the relationship between them. We can phrase this as an edge-level classification: given nodes that represent the objects in the image, we wish to predict which of these nodes share an edge or what the value of that edge is. If we wish to discover connections between entities, we could consider the graph fully connected and based on their predicted value prune edges to arrive at a sparse graph.</p>
<figure>
<img src="https://distill.pub/2021/gnn-intro/merged.0084f617.png" '="">
<figcaption>
In (b), above, the original image (a) has been segmented into five entities: each of the fighters, the referee, the audience and the mat. (C) shows the relationships between these entities. 
</figcaption></figure>

<figure>
<img src="https://distill.pub/2021/gnn-intro/edges_level_diagram.c40677db.png" '="">
<figcaption>
On the left we have an initial graph built from the previous visual scene. On the right is a possible edge-labeling of this graph when some connections were pruned based on the model’s output.
</figcaption></figure>

<h2 id="the-challenges-of-using-graphs-in-machine-learning">The challenges of using graphs in machine learning</h2>
<p>So, how do we go about solving these different graph tasks with neural networks? The first step is to think about how we will represent graphs to be compatible with neural networks.</p>
<p>Machine learning models typically take rectangular or grid-like arrays as input. So, it’s not immediately intuitive how to represent them in a format that is compatible with deep learning. Graphs have up to four types of information that we will potentially want to use to make predictions: nodes, edges,  global-context and connectivity. The first three are relatively straightforward: for example, with nodes we can form a node feature matrix $N$ by assigning each node an index $i$ and storing the feature for $node_i$ in $N$. While these matrices have a variable number of examples, they can be processed without any special techniques.</p>
<p>However, representing a graph’s connectivity is more complicated. Perhaps the most obvious choice would be to use an adjacency matrix, since this is easily tensorisable. However, this representation has a few drawbacks. From the <a href="#table">example dataset table</a>, we see the number of nodes in a graph can be on the order of millions, and the number of edges per node can be highly variable. Often, this leads to very sparse adjacency matrices, which are space-inefficient.</p>
<p>Another problem is that there are many adjacency matrices that can encode the same connectivity, and there is no guarantee that these different matrices would produce the same result in a deep neural network (that is to say, they are not permutation invariant).</p>


<p>For example, the <a href="https://distill.pub/2021/gnn-intro/mols-as-graph-othello"> Othello graph </a> from before can be described equivalently with these two adjacency matrices.  It can also be described with every other possible permutation of the nodes.</p>
<figure>
<p><img src="https://distill.pub/2021/gnn-intro/othello1.246371ea.png">
<img src="https://distill.pub/2021/gnn-intro/othello2.6897c848.png">
</p>
<figcaption>
Two adjacency matrices representing the same graph.
</figcaption>
</figure>

<p>The example below shows every adjacency matrix that can describe this small graph of 4 nodes. This is already a significant number of adjacency matrices–for larger examples like Othello, the number is untenable.</p>
<figure>
<figcaption>
All of these adjacency matrices represent the same graph. Click on an edge to remove it on a “virtual edge” to add it and the matrices will update accordingly.
</figcaption>
</figure>

<p>One elegant and memory-efficient way of representing sparse matrices is as adjacency lists. These describe the connectivity of edge $e_k$ between nodes $n_i$ and $n_j$ as a tuple (i,j) in the k-th entry of an adjacency list. Since we expect the number of edges to be much lower than the number of entries for an adjacency matrix ($n_{nodes}^2$), we avoid computation and storage on the disconnected parts of the graph. </p>


<p>To make this notion concrete, we can see how information in different graphs might be represented under this specification:</p>
<figure>

<figcaption>
Hover and click on the edges, nodes, and global graph marker to view and change attribute representations. On one side we have a small graph and on the other the information of the graph in a tensor representation.
</figcaption></figure>

<p>It should be noted that the figure uses scalar values per node/edge/global, but most practical tensor representations have vectors per graph attribute. Instead of a node tensor of size $[n_{nodes}]$ we will be dealing with node tensors of size $[n_{nodes}, node_{dim}]$. Same for the other graph attributes.</p>
<h2 id="graph-neural-networks">Graph Neural Networks</h2>
<p>Now that the  graph’s description is in a matrix format that is permutation invariant, we will describe using graph neural networks (GNNs) to solve graph prediction tasks. <strong>A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances).</strong> We’re going to build GNNs using the “message passing neural network” framework proposed by Gilmer et al.<d-cite key="Gilmer2017-no"></d-cite> using the Graph Nets architecture schematics introduced by Battaglia et al.<d-cite key="Battaglia2018-pi"></d-cite>  GNNs adopt a “graph-in, graph-out” architecture meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph. </p>
<h3 id="the-simplest-gnn">The simplest GNN</h3>
<p>With the numerical representation of graphs that <a href="#graph-to-tensor">we’ve constructed above</a> (with vectors instead of scalars), we are now ready to build a GNN. We will start with the simplest GNN architecture, one where we learn new embeddings for all graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph.</p>


<p>This GNN uses a separate multilayer perceptron (MLP) (or your favorite differentiable model) on each component of a graph; we call this a GNN layer. For each node vector, we apply the MLP and get back a learned node-vector. We do the same for each edge, learning a per-edge embedding, and also for the global-context vector, learning a single embedding for the entire graph.</p>


<figure>
<img src="https://distill.pub/2021/gnn-intro/arch_independent.0efb8ae7.png" '="">
<figcaption>
A single layer of a simple GNN. A graph is the input, and each component (V,E,U) gets updated by a MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of a GNN model.
</figcaption></figure>

<p>As is common with  neural networks modules or layers, we can stack these GNN layers together. </p>
<p>Because a GNN does not update the connectivity of the input graph, we can describe the output graph of a GNN with the same adjacency list and the same number of feature vectors as the input graph. But, the output graph has updated embeddings, since the GNN has updated each of the node, edge and global-context representations.</p>
<h3 id="gnn-predictions-by-pooling-information">GNN Predictions by Pooling Information</h3>
<p>We have built a simple GNN, but how do we make predictions in any of the tasks we described above?</p>
<p>We will consider the case of binary classification, but this framework can easily be extended to the multi-class or regression case. If the task is to make binary predictions on nodes, and the graph already contains node information, the approach is straightforward — for each node embedding, apply a linear classifier.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/prediction_nodes_nodes.c2c8b4d0.png" '=""></figure>



<p>However, it is not always so simple. For instance, you might have information in the graph stored in edges, but no information in nodes, but still need to make predictions on nodes. We need a way to collect information from edges and give them to nodes for prediction. We can do this by <em>pooling</em>. Pooling proceeds in two steps:</p>
<ol>
<li><p>For each item to be pooled, <em>gather</em> each of their embeddings and concatenate them into a matrix.</p>
</li>
<li><p>The gathered embeddings are then <em>aggregated</em>, usually via a sum operation.</p>
</li>
</ol>


<p>We represent the <em>pooling</em> operation by the letter $\rho$, and denote that we are gathering information from edges to nodes as $p_{E_n \to V_{n}}$. </p>
<figure>
<figcaption>
Hover over a node (black node) to visualize which edges are gathered and aggregated to produce an embedding for that target node.</figcaption>
</figure>


<p>So If we only have edge-level features, and are trying to predict binary node information, we can use pooling to route (or pass) information to where it needs to go. The model looks like this. </p>
<figure><img src="https://distill.pub/2021/gnn-intro/prediction_edges_nodes.e6796b8e.png" '="">
<figcaption>
</figcaption>
</figure>

<p>If we only have node-level features, and are trying to predict binary edge-level information, the model looks like this.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/prediction_nodes_edges.26fadbcc.png" '=""></figure>



<p>If we only have node-level features, and need to predict a binary global property, we need to gather all available node information together and aggregate them. This is similar to <em>Global Average Pooling</em> layers in CNNs. The same can be done for edges.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/prediction_nodes_edges_global.7a535eb8.png" '=""></figure>



<p>In our examples, the classification model <em>$c$</em> can easily be replaced with any differentiable model, or adapted to multi-class classification using a generalized linear model.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/Overall.e3af58ab.png" '="">
<figcaption>
An end-to-end prediction task with a GNN model.
</figcaption>
</figure>

<p>Now we’ve demonstrated that we can build a simple GNN model, and make binary predictions by routing information between different parts of the graph. This pooling technique will serve as a building block for constructing more sophisticated GNN models. If we have new graph attributes, we just have to define how to pass information from one attribute to another. </p>
<p>Note that in this simplest GNN formulation, we’re not using the connectivity of the graph at all inside the GNN layer. Each node is processed independently, as is each edge, as well as the global context. We only use connectivity when pooling information for prediction. </p>
<h3 id="passing-messages-between-parts-of-the-graph">Passing messages between parts of the graph</h3>
<p>We could make more sophisticated predictions by using pooling within the GNN layer, in order to make our learned embeddings aware of graph connectivity. We can do this using <em>message passing</em><d-cite key="Gilmer2017-no"></d-cite>, where neighboring nodes or edges exchange information and influence each other’s updated embeddings.</p>
<p>Message passing works in three steps: </p>
<ol>
<li><p>For each node in the graph, <em>gather</em> all the neighboring node embeddings (or messages), which is the $g$ function described above.</p>
</li>
<li><p>Aggregate all messages via an aggregate function (like sum).</p>
</li>
<li><p>All pooled messages are passed through an <em>update function</em>, usually a learned neural network.</p>
</li>
</ol>


<p>Just as pooling can be applied to either nodes or edges, message passing can occur between either nodes or edges.</p>
<p>These steps are key for leveraging the connectivity of graphs. We will build more elaborate variants of message passing in GNN layers that yield GNN models of increasing expressiveness and power. </p>
<figure>
<figcaption>
Hover over a node, to highlight adjacent nodes and visualize the adjacent embedding that would be pooled, updated and stored.
</figcaption>
</figure>

<p>This sequence of operations, when applied once, is the simplest type of message-passing GNN layer.</p>
<p>This is reminiscent of standard convolution: in essence, message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value. In graphs, the element is a node, and in images, the element is a pixel. However, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.</p>
<p>By stacking message passing GNN layers together, a node can eventually incorporate information from across the entire graph: after three layers, a node has information about the nodes three steps away from it.</p>
<p>We can update our architecture diagram to include this new source of information for nodes:</p>
<figure><img src="https://distill.pub/2021/gnn-intro/arch_gcn.40871750.png" '="">
<figcaption>
Schematic for a GCN architecture, which updates node representations of a graph by pooling neighboring nodes at a distance of one degree.
</figcaption></figure>

<h3 id="learning-edge-representations">Learning edge representations</h3>
<p>Our dataset does not always contain all types of information (node, edge, and global context). 
When we want to make a prediction on nodes, but our dataset only has edge information, we showed above how to use pooling to route information from edges to nodes, but only at the final prediction step of the model. We can share information between nodes and edges within the GNN layer using message passing.</p>
<p>We can incorporate the information from neighboring edges in the same way we used neighboring node information earlier, by first pooling the edge information, transforming it with an update function, and storing it.</p>
<p>However, the node and edge information stored in a graph are not necessarily the same size or shape, so it is not immediately clear how to combine them. One way is to learn a linear mapping from the space of edges to the space of nodes, and vice versa. Alternatively, one may concatenate them together before the update function.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/arch_mpnn.a13c2294.png">
<figcaption>
Architecture schematic for Message Passing layer. The first step “prepares” a message composed of information from an edge and it’s connected nodes and then “passes” the message to the node.
</figcaption></figure>

<p>Which graph attributes we update and in which order we update them is one design decision when constructing GNNs. We could choose whether to update node embeddings before edge embeddings, or the other way around. This is an open area of research with a variety of solutions– for example we could update in a ‘weave’ fashion<d-cite key="Kearnes2016-rl"></d-cite> where we have four updated representations that get combined into new node and edge representations: node to node (linear), edge to edge (linear), node to edge (edge layer), edge to node (node layer).</p>
<figure><img src="https://distill.pub/2021/gnn-intro/arch_weave.352befc0.png">
<figcaption>
Some of the different ways we might combine edge and node representation in a GNN layer.
</figcaption></figure>

<h3 id="adding-global-representations">Adding global representations</h3>
<p>There is one flaw with the networks we have described so far: nodes that are far away from each other in the graph may never be able to efficiently transfer information to one another, even if we apply message passing several times. For one node, If we have k-layers, information will propagate at most k-steps away.  This can be a problem for situations where the prediction task depends on nodes, or groups of nodes, that are far apart.  One solution would be to have all nodes be able to pass information to each other. 
Unfortunately for large graphs, this quickly becomes computationally expensive (although this approach, called ‘virtual edges’, has been used for small graphs such as molecules).<d-cite key="Gilmer2017-no"></d-cite></p>
<p>One solution to this problem is by using the global representation of a graph (U) which is sometimes called a <strong>master node</strong> <d-cite key="Battaglia2018-pi"></d-cite><d-cite key="Gilmer2017-no"></d-cite> or context vector. This global context vector is connected to all other nodes and edges in the network, and can act as a bridge between them to pass information, building up a representation for the graph as a whole. This creates a richer and more complex representation of the graph than could have otherwise been learned. </p>
<figure><img src="https://distill.pub/2021/gnn-intro/arch_graphnet.b229be6d.png">
<figcaption>Schematic of a Graph Nets architecture leveraging global representations.
</figcaption></figure>

<p>In this view all graph attributes have learned representations, so we can leverage them during pooling by conditioning the information of our attribute of interest with respect to the rest. For example, for one node we can consider information from neighboring nodes, connected edges and the global information. To condition the new node embedding on all these possible sources of information, we can simply concatenate them. Additionally we may also map them to the same space via a linear map and add them or apply a feature-wise modulation layer<d-cite key="Dumoulin2018-tb"></d-cite>, which can be considered a type of featurize-wise attention mechanism.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/graph_conditioning.3017e214.png">
<figcaption>Schematic for conditioning the information of one node based on three other embeddings (adjacent nodes, adjacent edges, global). This step corresponds to the node operations in the Graph Nets Layer. 
</figcaption></figure>

<h2 id="gnn-playground">GNN playground</h2>
<p>We’ve described a wide range of GNN components here, but how do they actually differ in practice? This GNN playground allows you to see how these different components and architectures contribute to a GNN’s ability to learn a real task. </p>
<p>Our playground shows a graph-level prediction task with small molecular graphs. We use the the Leffingwell Odor Dataset<d-cite key="Sanchez-Lengeling2020-qq"></d-cite><d-cite key="Sanchez-Lengeling2019-vs"></d-cite>, which is composed of molecules with associated odor percepts (labels). Predicting the relation of a molecular structure (graph) to its smell is a 100 year-old problem straddling chemistry, physics, neuroscience, and machine learning.</p>
<p>To simplify the problem,  we consider only a single binary label per molecule, classifying if a molecular graph smells “pungent” or not, as labeled by a professional perfumer. We say a molecule has a “pungent” scent if it has a strong, striking smell. For example, garlic and mustard, which might contain the molecule <em>allyl alcohol</em> have this quality. The molecule <em>piperitone</em>, often used for peppermint-flavored candy, is also described as having a pungent smell.</p>
<p>We represent each molecule as a graph, where atoms are nodes containing a one-hot encoding for its atomic identity (Carbon, Nitrogen, Oxygen, Fluorine) and bonds are edges containing a one-hot encoding its bond type (single, double, triple or aromatic). </p>
<p>Our general modeling template for this problem will be built up using sequential GNN layers, followed by a linear model with a sigmoid activation for classification. The design space for our GNN has many levers that can customize the model:</p>
<ol>
<li><p>The number of GNN layers, also called the <em>depth</em>.</p>
</li>
<li><p>The dimensionality of each attribute when updated. The update function is a 1-layer MLP with a relu activation function and a layer norm for normalization of activations. </p>
</li>
<li><p>The aggregation function used in pooling: max, mean or sum.</p>
</li>
<li><p>The graph attributes that get updated, or styles of message passing: nodes, edges and global representation. We control these via boolean toggles (on or off). A baseline model would be a graph-independent GNN (all message-passing off) which aggregates all data at the end into a single global attribute. Toggling on all message-passing functions yields a GraphNets architecture.</p>
</li>
</ol>
<p>To better understand how a GNN is learning a task-optimized representation of a graph, we also look at the penultimate layer activations of the GNN. These ‘graph embeddings’ are the outputs of the GNN model right before prediction.  Since we are using a generalized linear model for prediction, a linear mapping is enough to allow us to see how we are learning representations around the decision boundary. </p>
<p>Since these are high dimensional vectors, we reduce them to 2D via principal component analysis (PCA). 
A perfect model would visibility separate labeled data, but since we are reducing dimensionality and also have imperfect models, this boundary might be harder to see.</p>
<p>Play around with different model architectures to build your intuition. For example, see if you can edit the molecule on the left to make the model prediction increase. Do the same edits have the same effects for different model architectures?</p>


<figure>
<figcaption>Edit the molecule to see how the prediction changes, or change the model params to load a different model. Select a different molecule in the scatter plot.</figcaption></figure>


<h3 id="some-empirical-gnn-design-lessons">Some empirical GNN design lessons</h3>
<p>When exploring the architecture choices above, you might have found some models have better performance than others. Are there some clear GNN design choices that will give us better performance? For example, do deeper GNN models perform better than shallower ones? or is there a clear choice between aggregation functions? The answers are going to depend on the data, <d-cite key="Dwivedi2020-xm"></d-cite> <d-cite key="You2020-vk"></d-cite>, and even different ways of featurizing and constructing graphs can give different answers.</p>
<p>With the following interactive figure, we explore the space of GNN architectures and the performance of this task across a few major design choices:  Style of message passing, the dimensionality of embeddings, number of layers, and aggregation operation type.</p>
<p>Each point in the scatter plot represents a model: the x axis is the number of trainable variables, and the y axis is the performance. Hover over a point to see the GNN architecture parameters.</p>
<figure>


<figcaption>Scatterplot of each model’s performance vs its number of trainable variables. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>

<p>The first thing to notice is that, surprisingly, a higher number of parameters does correlate with higher performance. GNNs are a very parameter-efficient model type: for even a small number of parameters (3k) we can already find models with high performance. </p>
<p>Next, we can look at the distributions of performance aggregated based on the dimensionality of the learned representations for different graph attributes.</p>
<figure>


<figcaption>Aggregate performance of models across varying node, edge, and global dimensions.</figcaption>
</figure>

<p>We can notice that models with higher dimensionality tend to have better mean and lower bound performance but the same trend is not found for the maximum. Some of the top-performing models can be found for smaller dimensions. Since higher dimensionality is going to also involve a higher number of parameters, these observations go in hand with the previous figure.</p>
<p>Next we can see the breakdown of performance based on the number of GNN layers.</p>
<figure>


<figcaption> Chart of number of layers vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by the number of layers. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>


<p>The box plot shows a similar trend, while the mean performance tends to increase with the number of layers, the best performing models do not have three or four layers, but two. Furthermore, the lower bound for performance decreases with four layers. This effect has been observed before, GNN with a higher number of layers will broadcast information at a higher distance and can risk having their node representations ‘diluted’ from many successive iterations <d-cite key="Corso2020-py"></d-cite>.</p>
<p>Does our dataset have a preferred aggregation operation? Our following figure breaks down performance in terms of aggregation type.</p>
<figure>


<figcaption>Chart of aggregation type vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by aggregation type. Hover over a point to see the GNN architecture parameters.</figcaption>
</figure>

<p>Overall it appears that sum has a very slight improvement on the mean performance, but max or mean can give equally good models. This is useful to contextualize when looking at the <a href="#comparing-aggregation-operations"> discriminatory/expressive capabilities</a> of aggregation operations&nbsp;.</p>
<p>The previous explorations have given mixed messages. We can find mean trends where more complexity gives better performance but we can find clear counterexamples where models with fewer parameters, number of layers, or dimensionality perform better. One trend that is much clearer is about the number of attributes that are passing information to each other.</p>
<p>Here we break down performance based on the style of message passing. On both extremes, we consider models that do not communicate between graph entities (“none”) and models that have messaging passed between nodes, edges, and globals.</p>
<figure>


<figcaption>Chart of message passing vs model performance, and scatterplot of model performance vs number of parameters. Each point is colored by message passing. Hover over a point to see the GNN architecture parameters</figcaption>
</figure>

<p>Overall we see that the more graph attributes are communicating, the better the performance of the average model. Our task is centered on global representations, so explicitly learning this attribute also tends to improve performance. Our node representations also seem to be more useful than edge representations, which makes sense since more information is loaded in these attributes.</p>
<p>There are many directions you could go from here to get better performance. We wish two highlight two general directions, one related to more sophisticated graph algorithms and another towards the graph itself.</p>
<p>Up until now, our GNN is based on a neighborhood-based pooling operation. There are some graph concepts that are harder to express in this way, for example a linear graph path (a connected chain of nodes). Designing new mechanisms in which graph information can be extracted, executed and propagated in a GNN is one current research area <d-cite key="Markowitz2021-rn"></d-cite>, <d-cite key="Du2019-hr"></d-cite>, <d-cite key="Xu2018-hq"></d-cite>, <d-cite key="Velickovic2019-io"></d-cite>.</p>
<p>One of the frontiers of GNN research is not making new models and architectures, but “how to construct  graphs”, to be more precise, imbuing graphs with additional structure or relations that can be leveraged. As we loosely saw, the more graph attributes are communicating the more we tend to have better models. In this particular case, we could consider making molecular graphs more feature rich, by adding additional spatial relationships between nodes, adding edges that are not bonds, or explicit learnable relationships between subgraphs.</p>


<h2 id="into-the-weeds">Into the Weeds</h2>
<p>Next, we have a few sections on a myriad of graph-related topics that are relevant for GNNs.</p>
<h3 id="other-types-of-graphs-multigraphs-hypergraphs-hypernodes-hierarchical-graphs">Other types of graphs (multigraphs, hypergraphs, hypernodes, hierarchical graphs)</h3>
<p>While we only described graphs with vectorized information for each attribute, graph structures are more flexible and can accommodate other types of information. Fortunately, the message passing framework is flexible enough that often adapting GNNs to more complex graph structures is about defining how information is passed and updated by new graph attributes. </p>
<p>For example, we can consider multi-edge graphs or <em>multigraphs</em><d-cite key="Harary1969-qo"></d-cite>, where a pair of nodes can share multiple types of edges, this happens when we want to model the interactions between nodes differently based on their type. For example with a social network, we can specify edge types based on the type of relationships (acquaintance, friend, family). A GNN can be adapted by having different types of message passing steps for each edge type. 
We can also consider nested graphs, where for example a node represents a graph, also called a hypernode graph.<d-cite key="Poulovassilis1994-bt"></d-cite> Nested graphs are useful for representing hierarchical information. For example, we can consider a network of molecules, where a node represents a molecule and an edge is shared between two molecules if we have a way (reaction) of transforming one to the other <d-cite key="Zitnik2018-uk"></d-cite>  <d-cite key="Stocker2020-tr"></d-cite>.
In this case, we can learn on a nested graph by having a GNN that learns representations at the molecule level and another at the reaction network level, and alternate between them during training.</p>
<p>Another type of graph is a hypergraph<d-cite key="Berge1976-ss"></d-cite>, where an edge can be connected to multiple nodes instead of just two. For a given graph, we can build a hypergraph by identifying communities of nodes and assigning a hyper-edge that is connected to all nodes in a community.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/multigraphs.1bb84306.png">
<figcaption>Schematic of more complex graphs. On the left we have an example of a multigraph with three edge types, including a directed edge. On the right we have a three-level hierarchical graph, the intermediate level nodes are hypernodes.
</figcaption></figure>

<p>How to train and design GNNs that have multiple types of graph attributes is a current area of research <d-cite key="Yadati2018-de"></d-cite>, <d-cite key="Zhong2020-mv"></d-cite>.</p>
<h3 id="sampling-graphs-and-batching-in-gnns">Sampling Graphs and Batching in GNNs</h3>
<p>A common practice for training neural networks is to update network parameters with gradients calculated on randomized constant size (batch size) subsets of the training data (mini-batches). This practice presents a challenge for graphs due to the variability in the number of nodes and edges adjacent to each other, meaning that we cannot have a constant batch size. The main idea for batching with graphs is to create subgraphs that preserve essential properties of the larger graph. This graph sampling operation is highly dependent on context and involves sub-selecting nodes and edges from a graph. These operations might make sense in some contexts (citation networks) and in others, these might be too strong of an operation (molecules, where a subgraph simply represents a new, smaller molecule). How to sample a graph is an open research question.<d-cite key="Rozemberczki2020-lq"></d-cite> 
If we care about preserving structure at a neighborhood level, one way would be to randomly sample a uniform number of nodes, our <em>node-set</em>. Then add neighboring nodes of distance k adjacent to the node-set, including their edges.<d-cite key="Leskovec2006-st"></d-cite> Each neighborhood can be considered an individual graph and a GNN can be trained on batches of these subgraphs. The loss can be masked to only consider the node-set since all neighboring nodes would have incomplete neighborhoods.
A more efficient strategy might be to first randomly sample a single node, expand its neighborhood to distance k, and then pick the other node within the expanded set. These operations can be terminated once a certain amount of nodes, edges, or subgraphs are constructed.
If the context allows, we can build constant size neighborhoods by picking an initial node-set and then sub-sampling a constant number of nodes (e.g randomly, or via a random walk or Metropolis algorithm<d-cite key="Hubler2008-us"></d-cite>).</p>
<figure><img src="https://distill.pub/2021/gnn-intro/sampling.968003b3.png">
<figcaption>Four different ways of sampling the same graph. Choice of sampling strategy depends highly on context since they will generate different distributions of graph statistics (# nodes, #edges, etc.). For highly connected graphs, edges can be also subsampled. 
</figcaption></figure>

<p>Sampling a graph is particularly relevant when a graph is large enough that it cannot be fit in memory. Inspiring new architectures and training strategies such as Cluster-GCN <d-cite key="Chiang2019-yh"></d-cite>  and GraphSaint <d-cite key="Zeng2019-eh"></d-cite>. We expect graph datasets to continue growing in size in the future.</p>
<h3 id="inductive-biases">Inductive biases</h3>
<p>When building a model to solve a problem on a specific kind of data, we want to specialize our models to leverage the characteristics of that data. When this is done successfully, we often see better predictive performance, lower training time, fewer parameters and better generalization.  </p>
<p>When labeling on images, for example, we want to take advantage of the fact that a dog is still a dog whether it is in the top-left or bottom-right corner of an image. Thus, most image models use convolutions, which are translation invariant. For text, the order of the tokens is highly important, so recurrent neural networks process data sequentially. Further, the presence of one token (e.g. the word ‘not’) can affect the meaning of the rest of a sentence, and so we need components that can ‘attend’ to other parts of the text, which transformer models like BERT and GPT-3 can do. These are some examples of inductive biases, where we are identifying symmetries or regularities in the data and adding modelling components that take advantage of these properties.</p>
<p>In the case of graphs, we care about how each graph component (edge, node, global) is related to each other so we seek models that have a relational inductive bias.<d-cite key="Battaglia2018-pi"></d-cite> A model should preserve explicit relationships between entities (adjacency matrix) and preserve graph symmetries (permutation invariance). We expect problems where the interaction between entities is important will benefit from a graph structure. Concretely, this means designing transformation on sets: the order of operation on nodes or edges should not matter and  the operation should work on a variable number of inputs. </p>
<h3 id="comparing-aggregation-operations">Comparing aggregation operations</h3>
<p>Pooling information from neighboring nodes and edges is a critical step in any reasonably powerful GNN architecture. Because each node has a variable number of neighbors, and because we want a differentiable method of aggregating this information, we want to use a smooth aggregation operation that is invariant to node ordering and the number of nodes provided.</p>
<p>Selecting and designing optimal aggregation operations is an open research topic.<d-cite key="Xu2018-sf"></d-cite> A desirable property of an aggregation operation is that similar inputs provide similar aggregated outputs, and vice-versa. Some very simple candidate permutation-invariant operations are sum, mean, and max. Summary statistics like variance also work. All of these take a variable number of inputs, and provide an output that is the same, no matter the input ordering. Let’s explore the difference between these operations.</p>
<figure>
<figcaption>
No pooling type can always distinguish between graph pairs such as max pooling on the left and sum / mean pooling on the right. 
</figcaption></figure>

<p>There is no operation that is uniformly the best choice. The mean operation can be useful when nodes have a highly-variable number of neighbors or you need a normalized view of the features of a local neighborhood. The max operation can be useful when you want to highlight single salient features in local neighborhoods. Sum provides a balance between these two, by providing a snapshot of the local distribution of features, but because it is not normalized, can also highlight outliers. In practice, sum is commonly used. </p>
<p>Designing aggregation operations is an open research problem that intersects with machine learning on sets.<d-cite key="Skianis2019-ds"></d-cite> New approaches such as Principal Neighborhood aggregation<d-cite key="Corso2020-py"></d-cite> take into account several aggregation operations by concatenating them and adding a scaling function that depends on the degree of connectivity of the entity to aggregate. Meanwhile, domain specific aggregation operations can also be designed. One example lies with the “Tetrahedral Chirality” aggregation operators <d-cite key="Pattanaik2020-jj"></d-cite>.</p>
<h3 id="gcn-as-subgraph-function-approximators">GCN as subgraph function approximators</h3>
<p>Another way to see GCN (and MPNN) of k-layers with a 1-degree neighbor lookup is as a neural network that operates on learned embeddings of subgraphs of size k.<d-cite key="Liu2018-kf"></d-cite><d-cite key="Xu2018-sf"></d-cite></p>
<p>When focusing on one node, after k-layers, the updated node representation has a limited viewpoint of all neighbors up to k-distance, essentially a subgraph representation. Same is true for edge representations.</p>
<p>So a GCN is collecting all possible subgraphs of size k and learning vector representations from the vantage point of one node or edge. The number of possible subgraphs can grow combinatorially, so enumerating these subgraphs from the beginning vs building them dynamically as in a GCN, might be prohibitive.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/arch_subgraphs.197f9b0e.png">
<figcaption>
</figcaption></figure>

<h3 id="edges-and-the-graph-dual">Edges and the Graph Dual</h3>
<p>One thing to note is that edge predictions and node predictions, while seemingly different, often reduce to the same problem: an edge prediction task on a graph $G$ can be phrased as a node-level prediction on $G$’s dual.</p>
<p>To obtain $G$’s dual, we can convert nodes to edges (and edges to nodes). A graph and its dual contain the same information, just expressed in a different way. Sometimes this property makes solving problems easier in one representation than another, like frequencies in Fourier space.  In short, to solve an edge classification problem on $G$, we can think about doing graph convolutions on $G$’s dual (which is the same as learning edge representations on $G$), this idea was developed with Dual-Primal Graph Convolutional Networks.<d-cite key="Monti2018-ov"></d-cite></p>
<!--[TODO: Image sketch of a graph and its dual]-->


<h3 id="graph-convolutions-as-matrix-multiplications-and-matrix-multiplications-as-walks-on-a-graph">Graph convolutions as matrix multiplications, and matrix multiplications as walks on a graph</h3>
<p>We’ve talked a lot about graph convolutions and message passing, and of course, this raises the question of how do we implement these operations in practice? For this section, we explore some of the properties of matrix multiplication, message passing, and its connection to traversing a graph. </p>
<p>The first point we want to illustrate is that the matrix multiplication of an adjacent matrix $A$  $n_{nodes} \times n_{nodes}$ with a node feature matrix $X$ of size $n_{nodes} \times node_{dim}$ implements an simple message passing with a summation aggregation.
Let the matrix be $B=AX$, we can observe that any entry $B_{ij}$ can be expressed as $&lt;A_{row_i} \dot X_{column_j}&gt;= A_{i,1}X_{1,j}+A_{i,2}X_{2, j}+…+A_{i,n}X_{n, j}=\sum_{A_{i,k}&gt;0} X_{k,j}$. Because $A_{i,k}$ are binary entries only when a edge exists between $node_i$ and $node_k$, the inner product is essentially “gathering” all node features values of dimension $j$” that share an edge with $node_i$. It should be noted that this message passing is not updating the representation of the node features, just pooling neighboring node features. But this can be easily adapted by passing $X$ through your favorite differentiable transformation (e.g. MLP) before or after the matrix multiply.</p>
<p>From this view, we can appreciate the benefit of using adjacency lists. Due to the expected sparsity of $A$ we don’t have to sum all values where $A_{i,j}$ is zero. As long as we have an operation to gather values based on an index, we should be able to just retrieve positive entries. Additionally, this matrix multiply-free approach frees us from using summation as an aggregation operation. </p>
<p>We can imagine that applying this operation multiple times allows us to propagate information at greater distances. In this sense, matrix multiplication is a form of traversing over a graph. This relationship is also apparent when we look at powers $A^K$ of the adjacency matrix.  If we consider the matrix $A^2$, the term $A^2_{ij}$ counts all walks of length 2 from $node_{i}$ to $node_{j}$ and can be expressed as the inner product $&lt;A_{row_i}, A_{column_j}&gt; = A_{i,1}A_{1, j}+A_{i,2}A_{2, j}+…+A_{i,n}A{n,j}$. The intuition is that the first term $a_{i,1}a_{1, j}$ is only positive under two conditions, there is edge that connects $node_i$ to $node_1$ and another edge that connects $node_{1}$ to $node_{j}$. In other words, both edges form a path of length 2 that goes from $node_i$ to $node_j$ passing by $node_1$. Due to the summation, we are counting over all possible intermediate nodes. This intuition carries over when we consider $A^3=A \matrix A^2$.. and so on to $A^k$. </p>
<p>There are deeper connections on how we can view matrices as graphs to explore <d-cite key="noauthor_undated-qq"></d-cite><d-cite key="Bapat2014-fk"></d-cite><d-cite key="Bollobas2013-uk"></d-cite>.</p>
<h3 id="graph-attention-networks">Graph Attention Networks</h3>
<p>Another way of communicating information between graph attributes is via attention.<d-cite key="Vaswani2017-as"></d-cite> For example, when we consider the sum-aggregation of a node and its 1-degree neighboring nodes we could also consider using a weighted sum.The challenge then is to associate weights in a permutation invariant fashion. One approach is to consider a scalar scoring function that assigns weights based on pairs of nodes ( $f(node_i, node_j)$). In this case, the scoring function can be interpreted as a function that measures how relevant a neighboring node is in relation to the center node. Weights can be normalized, for example with a softmax function to focus most of the weight on a neighbor most relevant for a node in relation to a task. This concept is the basis of Graph Attention Networks (GAT) <d-cite key="Velickovic2017-hf"></d-cite> and Set Transformers<d-cite key="Lee2018-ti"></d-cite>. Permutation invariance is preserved, because scoring works on pairs of nodes. A common scoring function is the inner product and nodes are often transformed before scoring into query and key vectors via a linear map to increase the expressivity of the scoring mechanism. Additionally for interpretability, the scoring weights can be used as a measure of the importance of an edge in relation to a task. </p>
<figure><img src="https://distill.pub/2021/gnn-intro/attention.3c55769d.png">
<figcaption>Schematic of attention over one node with respect to it’s adjacent nodes. For each edge an interaction score is computed, normalized and used to weight node embeddings.
</figcaption></figure>

<p>Additionally, transformers can be viewed as GNNs with an attention mechanism <d-cite key="Joshi2020-ze"></d-cite>. Under this view, the transformer models several elements (i.g. character tokens) as nodes in a fully connected graph and the attention mechanism is assigning edge embeddings to each node-pair which are used to compute attention weights. The difference lies in the assumed pattern of connectivity between entities, a GNN is assuming a sparse pattern and the Transformer is modelling all connections.</p>
<h3 id="graph-explanations-and-attributions">Graph explanations and attributions</h3>
<p>When deploying GNN in the wild we might care about model interpretability for building credibility, debugging or scientific discovery. The graph concepts that we care to explain vary from context to context. For example, with molecules we might care about the presence or absence of particular subgraphs<d-cite key="McCloskey2018-ml"></d-cite>, while in a citation network we might care about the degree of connectedness of an article. Due to the variety of graph concepts, there are many ways to build explanations. GNNExplainer<d-cite key="Ying2019-gk"></d-cite> casts this problem as extracting the most relevant subgraph that is important for a task. Attribution techniques<d-cite key="Pope2019-py"></d-cite> assign ranked importance values to parts of a graph that are relevant for a task. Because realistic and challenging graph problems can be  generated synthetically, GNNs can serve as a rigorous and repeatable testbed for evaluating attribution techniques <d-cite key="NEURIPS2020_6054"></d-cite>.</p>
<figure><img src="https://distill.pub/2021/gnn-intro/graph_xai.bce4532f.png">
<figcaption>Schematic of some explanability techniques on graphs. Attributions assign ranked values to graph attributes. Rankings can be used as a basis to extract connected subgraphs that might be relevant to a task.
</figcaption></figure>


<h3 id="generative-modelling">Generative modelling</h3>
<p>Besides learning predictive models on graphs, we might also care about learning a generative model for graphs. With a generative model we can generate new graphs by sampling from a learned distribution or by completing a graph given a starting point. A relevant application is in the design of new drugs, where novel molecular graphs with specific properties are desired as candidates to treat a disease.</p>
<p>A key challenge with graph generative models lies in modelling the topology of a graph, which can vary dramatically in size and has $N_{nodes}^2$ terms. One solution lies in modelling the adjacency matrix directly like an image with an autoencoder framework.<d-cite key="Kipf2016-ky"></d-cite> The prediction of the presence or absence of an edge is treated as a binary classification task. The $N_{nodes}^2$ term can be avoided by only predicting known edges and a subset of the edges that are not present. The graphVAE learns to model positive patterns of connectivity and some patterns of non-connectivity in the adjacency matrix.</p>
<p>Another approach is to build a graph sequentially, by starting with a graph and applying discrete actions such as addition or subtraction of nodes and edges iteratively. To avoid estimating a gradient for discrete actions we can use a policy gradient. This has been done via an auto-regressive model, such a RNN<d-cite key="You2018-vx"></d-cite>, or in a reinforcement learning scenario.<d-cite key="Zhou2019-ko"></d-cite> Furthermore, sometimes graphs can be modeled as just sequences with grammar elements.<d-cite key="Krenn2019-gg"></d-cite><d-cite key="Goyal2020-wl"></d-cite></p>
<!--[TODO: Image sketch of a graph generation]-->

<h2 id="final-thoughts">Final thoughts</h2>
<p>Graphs are a powerful and rich structured data type that have strengths and challenges that are very different from those of images and text. In this article, we have outlined some of the milestones that researchers have come up with in building neural network based models that process graphs. We have walked through some of the important design choices that must be made when using these architectures, and hopefully the GNN playground can give an intuition on what the empirical results of these design choices are. The success of GNNs in recent years creates a great opportunity for a wide range of new problems, and we are excited to see what the field will bring. 
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doctors Without Borders declares the war in Gaza as genocide (255 pts)]]></title>
            <link>https://www.doctorswithoutborders.org/latest/gaza-death-trap-msf-report-exposes-israels-campaign-total-destruction</link>
            <guid>42467375</guid>
            <pubDate>Fri, 20 Dec 2024 01:09:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.doctorswithoutborders.org/latest/gaza-death-trap-msf-report-exposes-israels-campaign-total-destruction">https://www.doctorswithoutborders.org/latest/gaza-death-trap-msf-report-exposes-israels-campaign-total-destruction</a>, See on <a href="https://news.ycombinator.com/item?id=42467375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <div>
      <p>Repeated Israeli military attacks on <a href="https://www.doctorswithoutborders.org/what-we-do/where-we-work/palestine" data-entity-type="node" data-entity-uuid="89bade8a-5775-4fce-9ca4-060ade776857" data-entity-substitution="canonical" title="Palestine">Palestinian</a> civilians over the last 14 months, the dismantling of the health care system and other essential <a href="https://www.doctorswithoutborders.org/latest/how-year-war-has-devastated-gazas-civilian-infrastructure" data-entity-type="node" data-entity-uuid="9b3b38b2-c69e-4543-8861-d1c461faefb9" data-entity-substitution="canonical" title="How a year of war has devastated Gaza’s civilian infrastructure">infrastructure</a>, the suffocating siege, and the systematic <a href="https://www.doctorswithoutborders.org/latest/palestinians-northern-gaza-are-desperate-need-aid" data-entity-type="node" data-entity-uuid="05c48515-8e9d-46a8-a520-85d400c922fa" data-entity-substitution="canonical" title=" Palestinians in northern Gaza are in desperate need of aid">denial</a> of humanitarian assistance are destroying the conditions of life in Gaza, according to a new Doctors Without Borders/Médecins Sans Frontières (MSF) report, "<a href="https://www.doctorswithoutborders.org/sites/default/files/documents/MSF_REPORT_Gaza%20Life%20in%20a%20death%20trap%20Report_20241229.pdf">Gaza: Life in a Death Trap</a>."&nbsp;</p>
</div>
                        <div><p>The international medical humanitarian organization is urgently calling on all parties, once again, for an <a href="https://www.doctorswithoutborders.org/latest/doctors-without-borders-calls-immediate-ceasefire-gaza" data-entity-type="node" data-entity-uuid="0a57b5b1-1f3e-4aac-a965-e8c4efac6070" data-entity-substitution="canonical" title="Doctors Without Borders calls for immediate ceasefire in Gaza">immediate ceasefire</a> to save lives and enable the flow of humanitarian aid. Israel must stop its targeted and indiscriminate attacks against civilians, and its allies must act without delay to protect the lives of Palestinians and uphold the rules of war.</p><p>"People in Gaza are struggling to survive apocalyptic conditions, but nowhere is safe, no one is spared, and there is no exit from this shattered enclave," said Christopher Lockyear, MSF secretary general, who visited Gaza earlier this year. The recent <a href="https://www.doctorswithoutborders.org/latest/whats-happening-northern-gaza" data-entity-type="node" data-entity-uuid="eae15551-d701-4eab-8d81-7f074f6aae00" data-entity-substitution="canonical" title="What’s happening in northern Gaza? ">military offensive in the north</a> is a stark illustration of the brutal war the Israeli forces are waging on Gaza, and we are seeing clear signs of ethnic cleansing as Palestinians are forcibly displaced, trapped, and bombed.”&nbsp;</p></div>
    <div>
      
      <div>
              
              <div><h2>Bearing witness in Gaza</h2><p>"What our medical teams have witnessed on the ground throughout this conflict is consistent with the descriptions provided by an increasing number of legal experts and organizations concluding that genocide is taking place in Gaza,” Lockyear said. “While we don't have legal authority to establish intentionality, the signs of ethnic cleansing and the ongoing devastation—including mass killings, severe physical and <a href="https://www.doctorswithoutborders.org/what-we-do/medical-issues/mental-health" data-entity-type="node" data-entity-uuid="dbe372ca-7cce-4d31-a84e-5bd5297f1f22" data-entity-substitution="canonical" title="Mental health">mental health</a> injuries, forced displacement, and impossible conditions of life for Palestinians under siege and bombardment—are undeniable."</p><p>In response to the horrific attacks carried out by Hamas and other armed groups in Israel on October 7, 2023—in which 1,200 people were killed and 251 people were taken hostage—Israeli forces are crushing the entire population of Gaza. Israel's all-out war on Gaza has reportedly <a href="https://www.ochaopt.org/content/humanitarian-situation-update-247-gaza-strip">killed more than 45,000 people</a>, according to the Ministry of Health, <a href="https://www.doctorswithoutborders.org/latest/remembering-our-colleagues-killed-gaza" data-entity-type="node" data-entity-uuid="daf39f9f-0731-49d5-ae0b-8ddd0b98b14e" data-entity-substitution="canonical" title="Remembering our colleagues killed in Gaza">including eight MSF colleagues</a>. The number of excess deaths related to the war is likely much higher due to the impacts of a collapsed health care system, disease outbreaks, and severely limited access to food, water, and shelter. The United Nations estimated earlier this year that more than 10,000 bodies remained buried under the rubble. &nbsp;</p></div>
              <div>
    <blockquote><p>What our medical teams have witnessed on the ground throughout this conflict is consistent with the descriptions provided by an increasing number of legal experts and organizations concluding that genocide is taking place in Gaza.</p><figcaption>Christopher Lockyear, MSF secretary general</figcaption></blockquote>
  </div>
              <div>
    <p>Israeli forces have on numerous occasions prevented essential items such as food, water, and medical supplies from entering the Strip, as well as <a href="https://www.doctorswithoutborders.org/latest/impossible-task-getting-lifesaving-supplies-gaza" data-entity-type="node" data-entity-uuid="862e46fb-d7f8-4188-a31b-34983c61f433" data-entity-substitution="canonical" title="What it takes to get lifesaving supplies into Gaza">blocked, denied, and delayed</a> humanitarian assistance, as documented in the report. Some 1.9 million people—90 percent of the entire population of the Strip—have been forcibly displaced, many forced to move multiple times.&nbsp;</p>
</div>
              <div>

  <figure>
        <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/04/MSB190917.jpg?itok=uf6HoRII 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/04/MSB190917.jpg?itok=uf6HoRII 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_tablet_768x512/public/image_base_media/2024/04/MSB190917.jpg?itok=5lzScnQW 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="768" height="512">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_large_mobile_480x320/public/image_base_media/2024/04/MSB190917.jpg?itok=45W6yHJV 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="780" height="520">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_mobile_335x250/public/image_base_media/2024/04/MSB190917.jpg?itok=odENLaRF 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="550" height="410">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/04/MSB190917.jpg?itok=uf6HoRII" width="1340" height="893" alt="Destruction inside Nasser Hospital in Gaza.">

  </picture>


      <p><span>
          <figcaption>
            Destruction inside Nasser Hospital in Gaza on March 13. <span> | </span> Palestine 2024 ©&nbsp;MSF
          </figcaption>
        </span>
      </p>

  </figure>
  </div>
              <div><h2>A decimated health care system</h2><p>Fewer than half of Gaza's 36 hospitals are even partially functional, and the health care system lies in ruins. During the one-year period covered by the report—from October 2023 to October 2024—MSF staff alone have endured 41 attacks and violent incidents, including airstrikes, shelling, and <a href="https://www.doctorswithoutborders.org/latest/how-israeli-army-besieged-nasser-hospital" data-entity-type="node" data-entity-uuid="39b38326-a49b-4ae7-924f-4c5c9510b898" data-entity-substitution="canonical" title="How the Israeli army besieged Nasser Hospital">violent incursions in health facilities</a>; direct fire on the organization’s shelters and <a href="https://www.doctorswithoutborders.org/latest/msf-convoy-attack-gaza-all-elements-point-israeli-army-responsibility" data-entity-type="node" data-entity-uuid="7b372d87-bc4b-421b-8a86-b1f10e56687e" data-entity-substitution="canonical" title="MSF convoy attack in Gaza: All elements point to Israeli army responsibility">convoys</a>; and arbitrary detention of colleagues by Israeli forces. MSF medical personnel and patients have been <a href="https://www.doctorswithoutborders.org/latest/gaza-msf-condemns-israels-forced-evacuation-nasser-hospital" data-entity-type="node" data-entity-uuid="bae6a195-9f92-4100-a372-55d450752aa6" data-entity-substitution="canonical" title="Gaza: MSF condemns Israel’s forced evacuation of Nasser Hospital">forced to evacuate</a> hospitals and health facilities on 17 separate occasions, often literally running for their lives. Warring parties have conducted hostilities near medical facilities, endangering patients, caretakers, and medical staff.</p><p>Meanwhile, Palestinians' physical and mental health injuries are overwhelming, and the needs continue to grow. MSF-supported facilities have carried out at least 27,500 consultations for violence-related injuries and 7,500 surgical interventions. People are suffering from war wounds as well as chronic diseases, made worse when they cannot access essential health care services and medicines. &nbsp;</p></div>
              <div>

  <figure>
        <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB215029%28High%29.png?itok=7-17duiI 1x" media="all and (min-width: 1600px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB215029%28High%29.png?itok=7-17duiI 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_tablet_768x512/public/MSB215029%28High%29.png?itok=WRcJiWfK 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/png" width="768" height="512">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_large_mobile_480x320/public/MSB215029%28High%29.png?itok=Q-oxTpSg 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/png" width="780" height="520">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_mobile_335x250/public/MSB215029%28High%29.png?itok=etRM0bCq 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/png" width="550" height="410">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB215029%28High%29.png?itok=7-17duiI" width="1340" height="893" alt="A displaced Palestinian family lights a fire in Gaza.">

  </picture>


      <p><span>
          <figcaption>
            Yasmin (right) lights a fire using plastic to prepare a meal for her children on November 7. Fuel is among other vital supplies that have been blocked or impeded from entering Gaza in sufficient quantities. <span> | </span> Palestine 2024 ©&nbsp;Ibrahim Nofal
          </figcaption>
        </span>
      </p>

  </figure>
  </div>
              <div><h2>Unbearable conditions of displacement</h2><p>Israel’s forced displacement has pushed people into<a href="https://www.doctorswithoutborders.org/latest/building-accessible-sanitation-facilities-displaced-people-gaza" data-entity-type="node" data-entity-uuid="2f959615-eb8f-4feb-a1e8-332e1d20d3cd" data-entity-substitution="canonical" title="Building accessible sanitation facilities for displaced people in Gaza"> unbearable and unhygienic living conditions</a> in which diseases can spread rapidly. As a result, MSF teams are treating high numbers of people for illnesses like skin diseases, respiratory infections, and diarrhea—all of which are expected to increase as winter temperatures drop. Children are missing out on crucial immunizations, leaving them vulnerable to diseases like <a href="https://www.doctorswithoutborders.org/what-we-do/medical-issues/measles" data-entity-type="node" data-entity-uuid="334ab892-5617-4f8d-b082-7d57936452e5" data-entity-substitution="canonical" title="Measles">measles</a> and <a href="https://www.doctorswithoutborders.org/latest/msf-supports-polio-vaccination-campaign-gaza" data-entity-type="node" data-entity-uuid="b1347d64-f6d8-4221-b5f1-a65c92ea3471" data-entity-substitution="canonical" title="MSF supports polio vaccination campaign in Gaza">polio</a>. MSF has observed an increase in the number of <a href="https://www.doctorswithoutborders.org/what-we-do/medical-issues/malnutrition" data-entity-type="node" data-entity-uuid="c2e2d193-6936-4385-b170-7b974436888e" data-entity-substitution="canonical" title="Malnutrition">malnutrition</a> cases; however, it is impossible to carry out a full malnutrition screening in Gaza due to widespread insecurity and the lack of proper deconfliction measures.&nbsp;</p></div>
              <div>

  <figure>
        <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB214906.png?itok=196lPheB 1x" media="all and (min-width: 1600px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB214906.png?itok=196lPheB 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_tablet_768x512/public/MSB214906.png?itok=hQT1JdvI 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/png" width="768" height="512">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_large_mobile_480x320/public/MSB214906.png?itok=zV9rcaGw 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/png" width="780" height="520">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_mobile_335x250/public/MSB214906.png?itok=gkt4Ok7Y 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/png" width="550" height="410">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB214906.png?itok=196lPheB" width="1340" height="893" alt="Malnourished child in Gaza.">

  </picture>


      <p><span>
          <figcaption>
            A young girl suffering from severe malnutrition who was admitted to MSF's intensive therapeutic feeding center for urgent care. The health of children in Gaza has been severely impacted by the lack of food, medical care, and basic necessities available. <span> | </span> Palestine 2024 ©&nbsp;MSF
          </figcaption>
        </span>
      </p>

  </figure>
  </div>
              <div><h2>Medical evacuations denied</h2><p>As medical care options dwindle in Gaza, Israel has made it even more difficult for people to be <a href="https://www.doctorswithoutborders.org/latest/long-road-recovery-gazas-war-wounded-children" data-entity-type="node" data-entity-uuid="679e4a53-c2f3-40e7-ac9e-5ba5b445e087" data-entity-substitution="canonical" title="The long road to recovery for Gaza’s war-wounded children">medically evacuated</a>. Between the closure of the Rafah crossing in early May 2024 and September 2024, Israeli authorities have only authorized the evacuations of 229 patients—which amounts to 1.6 percent of those who needed it at that time. This is a drop in the ocean of needs.&nbsp;</p></div>
              <div>

  <figure>
        <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB212618%28High%29.png?itok=kSeRb8QH 1x" media="all and (min-width: 1600px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB212618%28High%29.png?itok=kSeRb8QH 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/png" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_tablet_768x512/public/MSB212618%28High%29.png?itok=GuLSdt8J 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/png" width="768" height="512">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_large_mobile_480x320/public/MSB212618%28High%29.png?itok=wXpB3ki3 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/png" width="780" height="520">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_mobile_335x250/public/MSB212618%28High%29.png?itok=-D_zN-XG 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/png" width="550" height="410">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/MSB212618%28High%29.png?itok=kSeRb8QH" width="1340" height="893" alt="Body bags in northern Gaza.">

  </picture>


      <p><span>
          <figcaption>
            Victims are placed in body bags in northern Gaza on October 20. <span> | </span> Palestine 2024 ©&nbsp;MSF
          </figcaption>
        </span>
      </p>

  </figure>
  </div>
              <div><h2>The siege of northern Gaza</h2><p>The <a href="https://www.doctorswithoutborders.org/latest/whats-happening-northern-gaza" data-entity-type="node" data-entity-uuid="eae15551-d701-4eab-8d81-7f074f6aae00" data-entity-substitution="canonical" title="What’s happening in northern Gaza? ">situation in northern Gaza</a> is especially dire following Israel's recent scorched earth military offensive that has depopulated large areas and reportedly killed almost 2,000 people. The northern part of the Strip, particularly <a href="https://www.doctorswithoutborders.org/latest/urgent-thousands-trapped-jabalia-northern-gaza-amid-israeli-forces-attack" data-entity-type="node" data-entity-uuid="27490958-78ee-4d4b-a08e-585c0f52ce8b" data-entity-substitution="canonical" title="Urgent: Thousands trapped in Jabalia, northern Gaza, amid Israeli forces’ attack">Jabalia camp</a>, has been besieged again by Israeli forces since October 6, 2024. Israeli authorities have <a href="https://www.doctorswithoutborders.org/latest/palestinians-northern-gaza-are-desperate-need-aid" data-entity-type="node" data-entity-uuid="05c48515-8e9d-46a8-a520-85d400c922fa" data-entity-substitution="canonical" title=" Palestinians in northern Gaza are in desperate need of aid">dramatically reduced the quantity of essential aid</a> authorized to enter the north. In October 2024, the amount of supplies reaching the whole Gaza Strip hit its lowest point since the war escalated in October 2023: a daily average of 37 humanitarian trucks entered in October 2024, well below the 500 humanitarian trucks entering Gaza each day before October 7, 2023.</p></div>
              <div>
    <blockquote><p>Palestinians have been killed in their homes and in hospital beds. They have been forcibly displaced time and time again to areas that are not safe or healthy. People cannot find even the most basic necessities like food, clean water, medicines, and soap.</p><figcaption>Christopher Lockyear, MSF secretary general</figcaption></blockquote>
  </div>
              <div>
    <p>“For more than a year, our medical staff in Gaza have witnessed a relentless campaign by the Israeli forces marked by massive destruction, devastation, and dehumanization,” said Lockyear. “Palestinians have been killed in their homes and in hospital beds. They have been forcibly displaced time and time again to areas that are not safe or healthy. People cannot find even the most basic necessities like food, clean water, medicines, and soap amid a punishing siege and blockade."&nbsp;</p>
</div>
              <div>

  <figure>
        <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/06/MSB198600.jpg?itok=sRcGHrUp 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/06/MSB198600.jpg?itok=sRcGHrUp 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="1340" height="893">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_tablet_768x512/public/image_base_media/2024/06/MSB198600.jpg?itok=xUonE19h 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="768" height="512">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_large_mobile_480x320/public/image_base_media/2024/06/MSB198600.jpg?itok=DU4E4REe 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="780" height="520">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_mobile_335x250/public/image_base_media/2024/06/MSB198600.jpg?itok=P5Pj9Qrh 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="550" height="410">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/large_image_1340_893/public/image_base_media/2024/06/MSB198600.jpg?itok=sRcGHrUp" width="1340" height="893" alt="A mass grave unearthed at Nasser Hospital in April 2024.">

  </picture>


      <p><span>
          <figcaption>
            A mass grave unearthed at Nasser Hospital in April 2024 (before the Nuseirat attacks). <span> | </span> Palestine 2024 ©&nbsp;Ben Milpas/MSF
          </figcaption>
        </span>
      </p>

  </figure>
  </div>
              <div><h2>States must take urgent action</h2><p>MSF calls on states, particularly Israel’s closest allies, to end their unconditional support for Israel and fulfill their obligation to prevent genocide in Gaza. Nearly a year ago, on January 26, the International Court of Justice (ICJ) <a href="https://www.doctorswithoutborders.org/latest/doctors-without-borders-responds-icj-order-halt-military-operations-rafah" data-entity-type="node" data-entity-uuid="d995a16d-e277-4a77-a148-827b1bb44a2c" data-entity-substitution="canonical" title="Doctors Without Borders responds to ICJ order to halt military operations in Rafah">ordered Israel</a> to take “immediate and effective measures to enable the provision of urgently needed basic services and humanitarian assistance to address the adverse conditions of life faced by Palestinians in the Gaza Strip.” Israel has taken no meaningful action to comply with the court order. Instead, Israeli authorities continue to actively block MSF and other humanitarian organizations from providing lifesaving assistance to people trapped under siege and bombardment.</p></div>
              <div>
  <div>
    <p>
        <iframe title="Lockyear breaks down MSF's report" frameborder="0" allowfullscreen="allowfullscreen" src="https://www.youtube.com/shorts/zSENeHozWIE"></iframe>
    </p>
  </div>
  <div>
          <h3>Lockyear breaks down MSF's report</h3>
            
  </div>
</div>
              <div>
    <p>States must leverage their influence to alleviate the suffering of the population and enable a massive scale-up of humanitarian assistance across the Gaza Strip. As the occupying power, Israeli authorities are responsible for ensuring the rapid, unimpeded, and safe delivery of humanitarian aid at the level sufficient to address people’s needs. Instead, Israel's blockade and continued obstruction of aid have made it close to impossible for people in Gaza to access essential goods, including fuel, food, water, and medicines. At the same time, Israel has decided to <a href="https://www.doctorswithoutborders.org/latest/israels-decision-ban-unrwa-will-significantly-worsen-humanitarian-catastrophe" data-entity-type="node" data-entity-uuid="2684c0f0-8173-4eb3-98fc-751be638a5d5" data-entity-substitution="canonical" title="Israel’s decision to ban UNRWA will significantly worsen humanitarian catastrophe ">effectively ban</a> the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA), which is the largest provider of aid, health care, and other vital services for Palestinians.&nbsp;</p>
</div>
              <div>
			<h2>How MSF is responding in Gaza</h2>
			<p>What to know about the humanitarian catastrophe in Gaza and how MSF teams are providing medical and humanitarian aid.</p>
			  
<p><a href="https://www.doctorswithoutborders.org/latest/our-response-israel-gaza-war" aria-label="Learn more">  Learn more </a>
		</p></div>
              <div><h2>We need a ceasefire</h2><p>MSF repeats its call for an immediate and sustained ceasefire. The total <a href="https://www.doctorswithoutborders.org/latest/one-year-war-without-rules-leaves-gaza-shattered" data-entity-type="node" data-entity-uuid="6f63a8ca-6d7a-4c03-b385-739388c2ad70" data-entity-substitution="canonical" title="One year of a war without rules leaves Gaza shattered">destruction of Palestinian life</a> in Gaza must stop. MSF is also calling for immediate and safe access to northern Gaza to allow the delivery of humanitarian aid and medical supplies to hospitals. While MSF continues to provide lifesaving care in central and southern Gaza, we call on Israel to end its siege on the territory and open vital land borders, including the <a href="https://www.doctorswithoutborders.org/latest/whats-happening-rafah" data-entity-type="node" data-entity-uuid="4208cfce-efbb-4897-b981-3a5adf86e597" data-entity-substitution="canonical" title="What’s happening in Rafah? ">Rafah crossing</a>, to enable a massive scale-up of humanitarian and medical aid.</p></div>
              <div>
    <blockquote><p>Even if the Israeli military offensive on Gaza ended today, its long-term impacts would be unprecedented, given the scale of the destruction and the extraordinary challenges of providing health care across the Strip.</p></blockquote>
  </div>
              <div>
    <p>The MSF report notes that even if the Israeli military offensive on Gaza ended today, its long-term impacts would be unprecedented, given the scale of the destruction and the extraordinary challenges of providing health care across the Strip. A staggering number of war-wounded people are at risk of infection, amputation, and permanent disability, and many will require years of rehabilitative care. The cumulative physical toll and mental trauma caused by the extreme violence, loss of family members and homes, repeated forced displacement, and inhumane living conditions will scar generations.&nbsp;</p>
</div>
              <div id="article-63726">
    
          <h2>Related stories</h2>
                
<div>
    <div>
            <h3>Your questions about our work in Gaza, answered</h3>
            <p>Here are some of the questions frequently asked about our work and mission as it pertains to the ongoing crisis.</p>
            
<p><a href="https://www.doctorswithoutborders.org/your-questions-about-our-work-gaza-answered" aria-label="Read more about Your questions about our work in Gaza, answered">Read more</a>
    </p></div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/MSB185085_Small.jpg?itok=EhTwhAZ6 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/MSB185085_Small.jpg?itok=EhTwhAZ6 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/MSB185085_Small.jpg?itok=EhTwhAZ6 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_large_mobile_480x360/public/MSB185085_Small.jpg?itok=UrzoNtIL 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="480" height="360">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_mobile_335_251/public/MSB185085_Small.jpg?itok=ITRSabfx 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="335" height="251">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/MSB185085_Small.jpg?itok=EhTwhAZ6" width="327" height="245" alt="Two men walk in the sand outside a camp for displaced people in Gaza.">

  </picture>

      </div>
      </div>

          
<div>
    <div>
                        <p>
	<span>    December 19 <span></span> 12:00 AM
  </span>
  </p>
                    <h3>Life in the death trap that is Gaza</h3>
            <p>Israel’s war on Gaza is unraveling the fabric of society in the Strip. </p>
            
<p><a href="https://www.doctorswithoutborders.org/latest/life-death-trap-gaza" aria-label="Read more about Life in the death trap that is Gaza">Read More</a>
    </p></div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=0TFamZTY 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=0TFamZTY 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=0TFamZTY 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_large_mobile_480x360/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=sYZX3BYs 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="480" height="360">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_mobile_335_251/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=coQqCZxY 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="335" height="251">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=0TFamZTY" width="327" height="245" alt="Destruction at Al-Shifa, Gaza’s largest hospital, which is now out of service. ">

  </picture>

      </div>
      </div>

          
<div>
    <div>
                        <p>
	<span>    November 04 <span></span> 10:11 AM
  </span>
  </p>
                    <h3>Timeline: One year of bearing witness to all-out war in Gaza</h3>
            <p>A grim milestone for Palestinians in Gaza suffering under siege and bombardment. </p>
            
<p><a href="https://www.doctorswithoutborders.org/latest/timeline-one-year-bearing-witness-all-out-war-gaza" aria-label="Read more about Timeline: One year of bearing witness to all-out war in Gaza">Read More</a>
    </p></div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/image_base_media/2024/04/MSB183741.jpg?itok=JPeowa_n 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/image_base_media/2024/04/MSB183741.jpg?itok=JPeowa_n 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/image_base_media/2024/04/MSB183741.jpg?itok=JPeowa_n 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="327" height="245">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_large_mobile_480x360/public/image_base_media/2024/04/MSB183741.jpg?itok=0a1Xa-Ny 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="480" height="360">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_mobile_335_251/public/image_base_media/2024/04/MSB183741.jpg?itok=Mlc12_7_ 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="335" height="251">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/horizontal_card_desktop_327_245/public/image_base_media/2024/04/MSB183741.jpg?itok=JPeowa_n" width="327" height="245" alt="A man carries a child to MSF's clinic at Rafah Indonesian Hospital in Gaza.">

  </picture>

      </div>
      </div>

      </div>
              <div>
    <h2>We speak out. Get updates.</h2>
    
    
      </div>
          </div>
  
    
          <div>
      <p><span></span> Developing story </p>    
  <h2>How we're responding to the war in Gaza</h2>
  
</div>                
    <div>
					  <div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=qmG5kFo7 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=qmG5kFo7 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=qmG5kFo7 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=7Hbqq6vJ 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="105" height="88">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=7Hbqq6vJ 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="105" height="88">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/Al%20Shifa%20compound_2%C2%A9MSF.jpeg?itok=qmG5kFo7" width="279" height="235" alt="Destruction at Al-Shifa, Gaza’s largest hospital, which is now out of service. ">

  </picture>

      </div>
    		<div>
			<p>
	<span>Story</span>
  	  <span>Dec 19, 2024</span>
  </p>
			<p>Life in the death trap that is Gaza</p>
            
<p><a>Read More</a>
		</p></div>
	</div>
					  <div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB213926.jpg?itok=CC8KpfYS 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB213926.jpg?itok=CC8KpfYS 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB213926.jpg?itok=CC8KpfYS 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/image_base_media/2024/12/MSB213926.jpg?itok=iyYALjnu 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="105" height="88">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/image_base_media/2024/12/MSB213926.jpg?itok=iyYALjnu 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="105" height="88">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB213926.jpg?itok=CC8KpfYS" width="279" height="235" alt="An MSF nurse performs a rapid malaria test on a girl in Ethiopia. ">

  </picture>

      </div>
    		<div>
			<p>
	<span>Story</span>
  	  <span>Dec 18, 2024</span>
  </p>
			<p>Unprecedented surge of malaria cases in Ethiopia</p>
            
<p><a>Read More</a>
		</p></div>
	</div>
					  <div>
          <div>
            <picture>
                  <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB209078.jpg?itok=1ufl28_i 1x" media="all and (min-width: 1600px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB209078.jpg?itok=1ufl28_i 1x" media="all and (min-width: 1170px) and (max-width: 1599px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB209078.jpg?itok=1ufl28_i 1x" media="all and (min-width: 768px) and (max-width: 1169px)" type="image/jpeg" width="279" height="235">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/image_base_media/2024/12/MSB209078.jpg?itok=0QFTub3p 1x" media="all and (min-width: 480px) and (max-width: 767px)" type="image/jpeg" width="105" height="88">
              <source srcset="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_mobile_105x88/public/image_base_media/2024/12/MSB209078.jpg?itok=0QFTub3p 1x" media="all and (min-width: 320px) and (max-width: 479px)" type="image/jpeg" width="105" height="88">
                  <img loading="lazy" src="https://www.doctorswithoutborders.org/sites/default/files/styles/recirculation_card_275_235/public/image_base_media/2024/12/MSB209078.jpg?itok=1ufl28_i" width="279" height="235" alt="MSF health promoter Aisha B., a refugee from El Geneina who fled to Adré, Chad">

  </picture>

      </div>
    		<div>
			<p>
	<span>Story</span>
  	  <span>Dec 17, 2024</span>
  </p>
			<p>Everyone has felt the bitterness of loss in Sudan's war. I have, too.</p>
            
<p><a>Read More</a>
		</p></div>
	</div>
			</div>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The era of open voice assistants (669 pts)]]></title>
            <link>https://www.home-assistant.io/blog/2024/12/19/voice-preview-edition-the-era-of-open-voice/</link>
            <guid>42467194</guid>
            <pubDate>Fri, 20 Dec 2024 00:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.home-assistant.io/blog/2024/12/19/voice-preview-edition-the-era-of-open-voice/">https://www.home-assistant.io/blog/2024/12/19/voice-preview-edition-the-era-of-open-voice/</a>, See on <a href="https://news.ycombinator.com/item?id=42467194">Hacker News</a></p>
Couldn't get https://www.home-assistant.io/blog/2024/12/19/voice-preview-edition-the-era-of-open-voice/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>