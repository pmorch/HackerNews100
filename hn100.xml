<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 17 May 2025 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[JavaScript's New Superpower: Explicit Resource Management (189 pts)]]></title>
            <link>https://v8.dev/features/explicit-resource-management</link>
            <guid>44012227</guid>
            <pubDate>Sat, 17 May 2025 05:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://v8.dev/features/explicit-resource-management">https://v8.dev/features/explicit-resource-management</a>, See on <a href="https://news.ycombinator.com/item?id=44012227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>The <em>Explicit Resource Management</em> proposal introduces a deterministic approach to explicitly manage the lifecycle of resources like file handles, network connections, and more. This proposal brings the following additions to the language: the <code>using</code> and <code>await using</code> declarations, which automatically calls dispose method when a resource goes out of scope; <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code> symbols for cleanup operations; two new global objects <code>DisposableStack</code> and <code>AsyncDisposableStack</code> as containers to aggregate disposable resources; and <code>SuppressedError</code> as a new type of error (contain both the error that was most recently thrown, as well as the error that was suppressed) to address the scenario where an error occurs during the disposal of a resource, and potientially masking an existing error thrown from the body, or from the disposal of another resource. These additions enable developers to write more robust, performant, and maintainable code by providing fine-grained control over resource disposal.</p><h2 id="using-and-await-using-declarations" tabindex="-1"><code>using</code> and <code>await using</code> declarations <a href="#using-and-await-using-declarations">#</a></h2><p>The core of the Explicit Resource Management proposal lies in the <code>using</code> and <code>await using</code> declarations. The <code>using</code> declaration is designed for synchronous resources, ensuring that the <code>[Symbol.dispose]()</code> method of a disposable resource is called when the scope in which it's declared exits. For asynchronous resources, the <code>await using</code> declaration works similarly, but ensures that the <code>[Symbol.asyncDispose]()</code> method is called and the result of this calling is awaited, allowing for asynchronous cleanup operations. This distinction enables developers to reliably manage both synchronous and asynchronous resources, preventing leaks and improving overall code quality. The <code>using</code> and <code>await using</code> keywords can be used inside braces <code>{}</code> (such as blocks, for loops and function bodies), and cannot be used in top-levels.</p><p>For example, when working with <a href="https://developer.mozilla.org/en-US/docs/Web/API/ReadableStreamDefaultReader"><code>ReadableStreamDefaultReader</code></a>, it's crucial to call <code>reader.releaseLock()</code> to unlock the stream and allow it to be used elsewhere. However, error handling introduces a common problem: if an error occurs during the reading process, and you forget to call <code>releaseLock()</code> before the error propagates, the stream remains locked. Let's start with a naive example:</p><pre><code><span>let</span> responsePromise <span>=</span> <span>null</span><span>;</span><p><span>async</span> <span>function</span> <span>readFile</span><span>(</span><span>url</span><span>)</span> <span>{</span>  <br>    <span>if</span> <span>(</span><span>!</span>responsePromise<span>)</span> <span>{</span><br>        <span>// Only fetch if we don't have a promise yet</span><br>        responsePromise <span>=</span> <span>fetch</span><span>(</span>url<span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> response <span>=</span> <span>await</span> responsePromise<span>;</span><br>    <span>if</span> <span>(</span><span>!</span>response<span>.</span>ok<span>)</span> <span>{</span><br>      <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span><span>`</span><span>HTTP error! status: </span><span><span>${</span>response<span>.</span>status<span>}</span></span><span>`</span></span><span>)</span><span>;</span><br>    <span>}</span><br>    <span>const</span> processedData <span>=</span> <span>await</span> <span>processData</span><span>(</span>response<span>)</span><span>;</span></p><p>    <span>// Do something with processedData</span><br>    <span>...</span><br> <span>}</span></p><p><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span></p><p>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span></p><p>        <span>// Because the error is thrown before this line, the stream remains locked.</span><br>    reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>So it is crucial for developers to have <code>try...finally</code> block while using streams and put <code>reader.releaseLock()</code> in <code>finally</code>. This pattern ensures that <code>reader.releaseLock()</code> is always called.</p><pre><code><span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><p>        <span>try</span> <span>{</span><br>        <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>            <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>            <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>                <span>// Process data and save the result in processedData</span><br>                <span>...</span><br>                <span>// An error is thrown here!</span><br>            <span>}</span><br>        <span>}</span><br>    <span>}</span> <span>finally</span> <span>{</span><br>        <span>// The reader's lock on the stream will be always released.</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>    <span>}</span></p><p>    <span>return</span> processedData<span>;</span><br>  <span>}</span></p><p>  <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><p>An alternative to write this code is to create a disposable object <code>readerResource</code>, which has the reader (<code>response.body.getReader()</code>) and the <code>[Symbol.dispose]()</code> method that calls <code>this.reader.releaseLock()</code>. The <code>using</code> declaration ensures that <code>readerResource[Symbol.dispose]()</code> is called when the code block exits, and remembering to call <code>releaseLock</code> is no longer needed because the using declaration handles it. Integration of <code>[Symbol.dispose]</code> and <code>[Symbol.asyncDispose]</code> in web APIs like streams may happen in the future, so developers do not have to write the manual wrapper object.</p><pre><code> <span>async</span> <span>function</span> <span>processData</span><span>(</span><span>response</span><span>)</span> <span>{</span><br>    <span>const</span> reader <span>=</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>;</span><br>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><p>    <span>// Wrap the reader in a disposable resource</span><br>    using readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>const</span> <span>{</span> reader <span>}</span> <span>=</span> readerResource<span>;</span></p><p>    <span>let</span> done <span>=</span> <span>false</span><span>;</span><br>    <span>let</span> value<span>;</span><br>    <span>let</span> processedData<span>;</span><br>    <span>while</span> <span>(</span><span>!</span>done<span>)</span> <span>{</span><br>        <span>(</span><span>{</span> done<span>,</span> value <span>}</span> <span>=</span> <span>await</span> reader<span>.</span><span>read</span><span>(</span><span>)</span><span>)</span><span>;</span><br>        <span>if</span> <span>(</span>value<span>)</span> <span>{</span><br>            <span>// Process data and save the result in processedData</span><br>            <span>...</span><br>            <span>// An error is thrown here!</span><br>        <span>}</span><br>    <span>}</span><br>    <span>return</span> processedData<span>;</span><br>  <span>}</span><br> <span>// readerResource[Symbol.dispose]() is called automatically.</span></p><p> <span>readFile</span><span>(</span><span>'https://example.com/largefile.dat'</span><span>)</span><span>;</span></p></code></pre><h2 id="disposablestack-and-asyncdisposablestack" tabindex="-1"><code>DisposableStack</code> and <code>AsyncDisposableStack</code> <a href="#disposablestack-and-asyncdisposablestack">#</a></h2><p>To further facilitate managing multiple disposable resources, the proposal introduces <code>DisposableStack</code> and <code>AsyncDisposableStack</code>. These stack-based structures allow developers to group and dispose of multiple resources in a coordinated manner. Resources are added to the stack, and when the stack is disposed, either synchronously or asynchronously, the resources are disposed of in the reverse order they were added, ensuring that any dependencies between them are handled correctly. This simplifies the cleanup process when dealing with complex scenarios involving multiple related resources. Both structures provide methods like <code>use()</code>, <code>adopt()</code>, and <code>defer()</code> to add resources or disposal actions, and a <code>dispose()</code> or <code>asyncDispose()</code> method to trigger the cleanup. <code>DisposableStack</code> and <code>AsyncDisposableStack</code> have <code>[Symbol.dispose]()</code> and <code>[Symbol.asyncDispose]()</code>, respectively, so they can be used with <code>using</code> and <code>await using</code> keywords. They offer a robust way to manage the disposal of multiple resources within a defined scope.</p><p>Let’s take a look at each method and see an example of it:</p><p><code>use(value)</code> adds a resource to the top of the stack.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>adopt(value, onDispose)</code> adds a non-disposable resource and a disposal callback to the top of the stack.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><p><code>defer(onDispose)</code> adds a disposal callback to the top of the stack. It's useful for adding cleanup actions that don't have an associated resource.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>defer</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"done."</span><span>)</span><span>)</span><span>;</span><br><span>}</span><br><span>// done.</span></code></pre><p><code>move()</code> moves all resources currently in this stack into a new <code>DisposableStack</code>. This can be useful if you need to transfer ownership of resources to another part of your code.</p><pre><code><span>{</span><br>    using stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>adopt</span><span>(</span><br>      response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span> reader <span>=</span> <span>&gt;</span> <span>{</span><br>        reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>        console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>      <span>}</span><span>)</span><span>;</span><br>    using newStack <span>=</span> stack<span>.</span><span>move</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Here just the newStack exists and the resource inside it will be disposed.</span><br><span>// Reader lock released.</span></code></pre><p><code>dispose()</code> in DisposableStack and <code>asyncDispose()</code> in AsyncDisposableStack dispose the resources within this object.</p><pre><code><span>{</span><br>    <span>const</span> readerResource <span>=</span> <span>{</span><br>        <span>reader</span><span>:</span> response<span>.</span>body<span>.</span><span>getReader</span><span>(</span><span>)</span><span>,</span><br>        <span>[</span>Symbol<span>.</span>dispose<span>]</span><span>(</span><span>)</span> <span>{</span><br>            <span>this</span><span>.</span>reader<span>.</span><span>releaseLock</span><span>(</span><span>)</span><span>;</span><br>            console<span>.</span><span>log</span><span>(</span><span>'Reader lock released.'</span><span>)</span><span>;</span><br>        <span>}</span><span>,</span><br>    <span>}</span><span>;</span><br>    <span>let</span> stack <span>=</span> <span>new</span> <span>DisposableStack</span><span>(</span><span>)</span><span>;</span><br>    stack<span>.</span><span>use</span><span>(</span>readerResource<span>)</span><span>;</span><br>    stack<span>.</span><span>dispose</span><span>(</span><span>)</span><span>;</span><br><span>}</span><br><span>// Reader lock released.</span></code></pre><h2 id="availability" tabindex="-1">Availability <a href="#availability">#</a></h2><p>Explicit Resource Management is shipped in Chromium 134 and V8 v13.8.</p><ul><li><a href="https://chromestatus.com/feature/5071680358842368"><span>Chrome:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://v8.dev/features/(nightly)"><span>Firefox:</span> <span>supported since version <span>134</span></span></a></li><li><a href="https://bugs.webkit.org/show_bug.cgi?id=248707"><span>Safari:</span> <span>no support</span></a></li><li><span>Node.js:</span> <span>no support</span></li><li><a href="https://github.com/zloirock/core-js#explicit-resource-management"><span>Babel:</span> <span>supported</span></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A kernel developer plays with Home Assistant (136 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</link>
            <guid>44011669</guid>
            <pubDate>Sat, 17 May 2025 02:50:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/">https://lwn.net/SubscriberLink/1017720/7155ecb9602e9ef2/</a>, See on <a href="https://news.ycombinator.com/item?id=44011669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
Those of us who have spent our lives playing with computers naturally see
the appeal of deploying them though the home for both data acquisition and
automation.  But many of us who have watched the evolution of the
technology industry are increasingly unwilling to entrust critical
household functions to cloud-based servers run by companies that may not
have our best interests at heart.  The Apache-licensed <a href="https://www.home-assistant.io/">Home Assistant</a> project offers a
welcome alternative: locally controlled automation with free software.
This two-part series covers roughly a year of Home Assistant use, starting
with a set of overall observations about the project.
</p><p>
This is not the first time that LWN has looked at this project, of course;
<a href="https://lwn.net/Articles/822350/">this review</a> gives a snapshot of what Home
Assistant looked like five years ago, while <a href="https://lwn.net/Articles/947843/">this 2023 article</a> gives a good overview of the
project's history, governance, and overall direction.  I will endeavor to
not duplicate that material here.
</p><h4>Project health</h4>
<p>
At a first glance, Home Assistant bears some of the hallmarks of a
company-owned project.  The company in question, <a href="https://www.nabucasa.com/">Nabu Casa</a>, was formed around the
project and employs a number of its key developers.  One of the ways in
which the company makes money is with a $65/year subscription service, providing
remote access to Home Assistant servers installed on firewalled residential
networks.  Home Assistant has support for that remote option, and no
others.  It would be interesting to see what would happen to a pull request
adding support for, say, <a href="https://opensprinklershop.de/en/2023/01/22/opensprinkler-fernzugriff-mit-openthings-cloud-otc-token/">OpenThings
Cloud</a> as an alternative.  The fate of that request would say a lot
about how open the project really is.
</p><blockquote>
<b>No slop, all substance: subscribe to LWN today</b>
<p>
LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering <a href="https://lwn.net/Promo/no-slop/claim">a free one-month trial subscription</a> to get you started.
</p></blockquote>
<p>
(For the record, I have bought the Nabu Casa subscription rather than, say,
using WireGuard to make a port available on an accessible system; it is a
hassle-free way to solve the problem and support the development of this
software).
</p><p>
That said, most of the warning signs that accompany a corporate-controlled
project are not present with Home Assistant.  The project's <a href="https://github.com/home-assistant/core/blob/dev/CLA.md">contributor
license agreement</a> is a derivative of the kernel's developer certificate
of origin; contributors retain their copyright on their work.  Since the <a href="https://www.home-assistant.io/blog/2024/04/03/release-20244/">2024.4
release</a>, the Home Assistant core repository has acquired over 17,000
changesets from over 900 contributors.  While a number of Nabu Casa
employees (helpfully listed on <a href="https://www.nabucasa.com/about/">this page</a>) appear in the top ten
contributors, they do not dominate that list.
</p><p>
Home Assistant is clearly an active project with a wide developer base.  In
2024, overall responsibility for this project was transferred to <a href="https://www.openhomefoundation.org/blog/announcing-the-open-home-foundation/">the
newly created Open Home Foundation</a>.  This project is probably here to
stay, and seems unlikely to take a hostile turn in the future.  For a
system that sits at the core of one's home, those are important
characteristics.
</p><h4>Installation and setup</h4>
<p>
Linux users tend to be somewhat spoiled; installing a new application is
typically a matter of a single package-manager command.  Home Assistant
does not really fit into that model.  The first three options on <a href="https://www.home-assistant.io/installation/">the installation
page</a> involve dedicated computers — two of which are sold by Nabu Casa.
For those wanting to install it on a general-purpose computer, the
recommended course is to install the <a href="https://github.com/home-assistant/operating-system">Home Assistant
Operating System</a>, a bespoke Linux distribution that runs Home Assistant
within a Docker container.  There is also a container-based method that can
run on another distribution, but this installation does not support <a href="https://www.home-assistant.io/addons/">the add-ons feature</a>.
</p><p>
Home Assistant, in other words, is not really set up to be just another
application on a Linux system.  If one scrolls far enough, though, one will
find, the instructions to install onto a "normal" Linux system, suitably
guarded with warnings about how it is an "<q>advanced</q>" method.
Of course, that is what I did, putting the software onto an existing system
running Fedora.  The whole thing subsequently broke when a
distribution upgrade replaced Python, but that was easily enough repaired.
As a whole, the installation has worked as expected.
</p><p>
Out of the box, though, a new Home Assistant installation does not do much.
Its job, after all, is to interface with the systems throughout the house,
and every house is different.  While Home Assistant can find some systems
automatically (it found the Brother printer and dutifully informed me that
the device was, inevitably, low on cyan toner), it usually needs to be
told about what is installed in the house.  Thus, the user quickly delves
into the world of "integrations" — the device drivers of Home Assistant.
</p><p>
For each remotely accessible device in the house, there is, hopefully, at
least one integration available that allows Home Assistant to work with it.
Many integrations are packaged with the system itself, and can be found by
way of a simple search screen in the Home Assistant web interface.  A much
larger set is packaged separately, usually in the <a href="https://www.hacs.xyz/">Home Assistant Community Store</a>, or HACS;
it is fair to say that most users will end up getting at least some
integrations from this source.  Setting up HACS requires a few steps and,
unfortunately, requires the user to have a GitHub account for full
integration.  It <i>is</i> possible to install HACS integrations without
that account, but it is a manual process that loses support for features
like update tracking.
</p><p>
Most integrations, at setup time, will discover any of the appropriate
devices on the network — if those devices support that sort of discovery,
of course.  Often, using an integration will require the credentials to log
into the cloud account provided by the vendor of the devices in question.
When possible, integrations mostly strive to operate entirely locally; some
only use the cloud connection for the initial device discovery.  When there
is no alternative, though, integrations will remain logged into the cloud
account and interact with their devices that way; this mode may or may not
be supported (or condoned) by the vendor.  There are, of course, some
vendors that <a href="https://www.bleepingcomputer.com/news/security/haier-hits-home-assistant-plugin-dev-with-takedown-notice/">are
actively hostile</a> to integration with Home Assistant.
</p><p>
As might be expected, the quality of integrations varies widely.  Most of
the integrations I have tried have worked well enough.  The OpenSprinkler
(<a href="https://lwn.net/Articles/940509/">reviewed here</a> in 2023) integration,
instead, thoroughly corrupted the device configuration, exposing me to the
shame of being seen with a less-than-perfect lawn; it was quickly
removed.  It is an especially nice surprise when a device comes with Home
Assistant support provided by the vendor, but that is still a relatively
rare occurrence.  Home Assistant now is in a position similar to Linux
25&nbsp;years ago; many devices are supported, but often in spite of their
vendor, and one has to choose components carefully.
</p><h4>Security</h4>
<p>
Home Assistant sits at the core of the home network; it has access to
sensors that can reveal a lot about the occupants of the home, and it
collects data in a single location.  An installation will be exposed to the
Internet if its owner needs remote access.  There is clearly potential for
a security disaster here.
</p><p>
The project has <a href="https://www.home-assistant.io/security/">a posted
security policy</a> describing the project's stance; it asks for a 90-day
embargo on the reporting of any security issues.  Authors writing about the
project's security are encouraged to run their work past the project "<q>so
we can ensure that all claims are correct</q>".  The security policy
explicitly excludes reports regarding third-party integrations (the core
project cannot fix those, after all).  The project is also uninterested in
any sort of privilege escalation by users who are logged into Home
Assistant, assuming that anybody who has an account is fully trusted.
</p><p>
The project has only issued <a href="https://github.com/home-assistant/core/security/advisories/GHSA-m3pm-rpgg-5wj6">one
security advisory</a> since the beginning of 2024.  There were several in
2023, mostly as the result of <a href="https://github.blog/security/vulnerability-research/securing-our-home-labs-home-assistant-code-review/">a
security audit</a> performed by GitHub.
</p><p>

There is no overall vetting of third-party integrations, which are, in the
end, just more Python code.  So loading an unknown integration is similar
to importing an unknown module from PyPI; it will probably work, but the
potential for trouble is there.  The project has occasionally <a href="https://www.home-assistant.io/blog/2021/01/23/security-disclosure2/">reported
security problems in third-party integrations</a>, but such reports are
rare.  I am unable to find any reports of actively malicious integrations
in the wild, but one seems destined to appear sooner or later.
</p><h4>Actually doing something with Home Assistant</h4>
<p>
The first step for the owner of a new Home Assistant installation is,
naturally, to seek out integrations for the devices installed in the home.
On successful installation and initialization, an integration will add one
or more "devices" to the system, each of which has some number of "sensors"
for data it reports, and possible "controls" to change its operating state.
A heat-pump head, for example, may have sensors for the current temperature
and humidity, and controls for its operating mode, fan speed, vane
direction, and more.
</p><p>
It is worth noting that the setup of these entities seems a bit
non-deterministic at times.  My solar system has 22&nbsp;panels with
inverters, each of which reports nearly a dozen parameters (voltage,
current, frequency, temperature, etc.).  There is no easy way to determine
which panel is reporting, for example, <tt>sensor_amps_12</tt>, especially
since <tt>sensor_frequency_12</tt> almost certainly corresponds to a
<i>different</i> panel.  My experience is that Home Assistant is a system
for people who are willing to spend a lot of time fiddling around with
things to get them to a working state.  Dealing with these sensors was an
early introduction to that; it took some time to figure out the mapping
between names and rooftop positions, then to rename each sensor to
something more helpful.
</p><p>
The next level of fiddling around is setting up dashboards.  Home Assistant
offers a great deal of flexibility in the information and controls it
provides to the user; it is possible to set up screens focused on, say,
energy production or climate control.  Happily, the days when this
configuration had to be done by writing YAML snippets are mostly in the
past at this point; one occasionally still has to dip into YAML, but it
does not happen often.  The interface is not always intuitive,
but it is fairly slick, interactive, and functional.
</p><p>
Another part of Home Assistant that I have not yet played with much
is automations and scenes.  Automations are simple rule-triggered programs
that make changes to some controls.  They can carry out actions like
"turn on the front light when it gets dark" or "play scary music if
somebody rings the doorbell and nobody is home".  Scenes are sets of canned
device configurations.  One might create a scene called "in-laws visiting"
that plays loud punk music, sets the temperature to just above freezing,
disables all voice control, and tunes all of the light bulbs to 6000K, for
example.
</p><p>
The good news is that, unless the fiddling itself is the point (and it can
be a good one), there comes a time when things just work and the fiddling
can stop.  A well-configured Home Assistant instance provides detailed
information about the state of the home — and control where the devices
allow it — to any web browser that can reach it and log in.  There are
(open-source) apps that bring this support to mobile devices in a way that
is nearly indistinguishable from how the web interface works.
</p><p>
All told, it is clear why Home Assistant has a strong and growing
following.  It is an open platform that brings control to an industry that
is doing its best to keep a firm grasp on our homes and the data they
create.  Home Assistant shows that we can do nicely without all of these
fragile, non-interoperable, rug-pull-susceptible cloud systems.  Just like
Linux proved that we can have control over our computers, Home Assistant
shows that we do not have to surrender control over our homes.
</p><p>
This article has gotten long, and is remarkably short on interesting things
that one can actually <i>do</i> with Home Assistant.  There are some
interesting stories to be told along those lines; they will appear shortly
in <a href="https://lwn.net/Articles/1017945/">the second, concluding part</a> of this series.<br clear="all"></p>
               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XTool – Cross-platform Xcode replacement (155 pts)]]></title>
            <link>https://github.com/xtool-org/xtool</link>
            <guid>44011515</guid>
            <pubDate>Sat, 17 May 2025 02:10:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xtool-org/xtool">https://github.com/xtool-org/xtool</a>, See on <a href="https://news.ycombinator.com/item?id=44011515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">xtool</h2><a id="user-content-xtool" aria-label="Permalink: xtool" href="#xtool"></a></p>
<p dir="auto">Cross-platform Xcode replacement. Build and deploy iOS apps with SwiftPM on Linux, Windows, and macOS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">xtool is a cross-platform (Linux/WSL/macOS) tool that replicates Xcode functionality with open standards.</p>
<p dir="auto">✅ Build a SwiftPM package into an iOS app</p>
<p dir="auto">✅ Sign and install iOS apps</p>
<p dir="auto">✅ Interact with Apple Developer Services programmatically</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<ol dir="auto">
<li>Follow the guide to install <code>xtool</code>
<ul dir="auto">
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-linux" rel="nofollow">Installation (Linux/Windows)</a></li>
<li><a href="https://swiftpackageindex.com/xtool-org/xtool/documentation/xtool/installation-macos" rel="nofollow">Installation (macOS)</a></li>
</ul>
</li>
<li>Create and run your first xtool-powered app by following the <a href="https://swiftpackageindex.com/xtool-org/xtool/tutorials/xtool/first-app" rel="nofollow">tutorial</a>!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Screenshot</h3><a id="user-content-screenshot" aria-label="Permalink: Screenshot" href="#screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/xtool-org/xtool/blob/main/Sources/xtool/Documentation.docc/Resources/Cover.png"><img src="https://github.com/xtool-org/xtool/raw/main/Sources/xtool/Documentation.docc/Resources/Cover.png" alt="A screenshot of xtool being invoked from VSCode"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Command line interface</h3><a id="user-content-command-line-interface" aria-label="Permalink: Command line interface" href="#command-line-interface"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <subcommand>

OPTIONS:
  -h, --help              Show help information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool for iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See 'xtool help <subcommand>' for detailed help."><pre>$ xtool --help
OVERVIEW: Cross-platform Xcode replacement

USAGE: xtool <span>&lt;</span>subcommand<span>&gt;</span>

OPTIONS:
  -h, --help              Show <span>help</span> information.

CONFIGURATION SUBCOMMANDS:
  setup                   Set up xtool <span>for</span> iOS development
  auth                    Manage Apple Developer Services authentication
  sdk                     Manage the Darwin Swift SDK

DEVELOPMENT SUBCOMMANDS:
  new                     Create a new xtool SwiftPM project
  dev                     Build and run an xtool SwiftPM project
  ds                      Interact with Apple Developer Services

DEVICE SUBCOMMANDS:
  devices                 List devices
  install                 Install an ipa file to your device
  uninstall               Uninstall an installed app
  launch                  Launch an installed app

  See <span><span>'</span>xtool help &lt;subcommand&gt;<span>'</span></span> <span>for</span> detailed help.</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Library</h3><a id="user-content-library" aria-label="Permalink: Library" href="#library"></a></p>
<p dir="auto">xtool includes a library that you can use to interact with Apple Developer Services, iOS devices, and more from your own app. You can use this by adding <code>XKit</code> as a SwiftPM dependency.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// package dependency:
.package(url: &quot;https://github.com/xtool-org/xtool&quot;, .upToNextMinor(from: &quot;1.2.0&quot;))
// target dependency:
.product(name: &quot;XKit&quot;, package: &quot;xtool&quot;)"><pre>// package dependency:
<span>.</span><span>package</span><span>(</span>url<span>:</span> <span>"</span><span>https://github.com/xtool-org/xtool</span><span>"</span><span>,</span> <span>.</span>upToNextMinor<span>(</span>from<span>:</span> <span>"</span><span>1.2.0</span><span>"</span><span>)</span><span>)</span>
// target dependency:
<span>.</span><span>product</span><span>(</span>name<span>:</span> <span>"</span><span>XKit</span><span>"</span><span>,</span> <span>package</span><span>:</span> <span>"</span><span>xtool</span><span>"</span><span>)</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wow@Home – Network of Amateur Radio Telescopes (156 pts)]]></title>
            <link>https://phl.upr.edu/wow/outreach</link>
            <guid>44011489</guid>
            <pubDate>Sat, 17 May 2025 02:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phl.upr.edu/wow/outreach">https://phl.upr.edu/wow/outreach</a>, See on <a href="https://news.ycombinator.com/item?id=44011489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" dir="ltr"><div id="h.f6d8ee3d198bc6f_1" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p role="main" tabindex="0"><h2 id="h.m56w2pmswrzx" dir="ltr"><span>Wow@Home</span></h2></p></div><div id="h.f6d8ee3d198bc6f_27" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>A </span><span>network of small</span><span> </span><span>radio telescopes</span><span> offers several distinct advantages compared to large professional observatories. These systems are low</span><span>-</span><span>cost an</span><span>d</span><span> can operate autonomously around the clock, making them ideal for continuous monitoring of transient events or long-duration signals that professional telescopes cannot commit to observing full-time.</span></p><p dir="ltr"><span>Their geographic distribution enables global sky coverage and coordinated observations across different time zones, which is especially valuable for </span><span>validating repeating or time-variable signals</span><span>. Coincidence detection across multiple stations helps </span><span>reject local radio frequency interference (RFI)</span><span>, increasing confidence in true astrophysical or technosignature candidates.</span></p><p dir="ltr"><span>These networks are also highly scalable, resilient to single-point failures, and capable of </span><span>rapid response to external alerts</span><span>. Furthermore, they are cost-effective, engaging, and accessible, </span><span>ideal for education, citizen science, and expanding participation in radio astronomy</span><span>.</span></p><p dir="ltr"><span>However, these systems also come with notable limitations when compared to professional telescopes. They have </span><span>significantly lower sensitivity</span><span>, limiting their ability to detect faint or distant sources. Their angular resolution is poor due to smaller dish sizes and wide beamwidths, making </span><span>precise source localization difficult</span><span>.</span></p><p dir="ltr"><span>Calibration can be inconsistent across stations</span><span>, and frequency stability or dynamic range may not match the performance of professional-grade equipment. Additionally, without standardized equipment and protocols, data quality and interoperability can vary across the network. Despite these constraints, when thoughtfully coordinated, such networks can </span><span>provide valuable complementary observations to professional facilities</span><span>.</span></p></div><div id="h.3e7c17c5694c8634_48" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><h2 id="h.xu5ifbv830af_l" dir="ltr"><div jscontroller="Ae65rd" jsaction="touchstart:UrsOsc; click:KjsqPd; focusout:QZoaZ; mouseover:y0pDld; mouseout:dq0hvd;fv1Rjc:jbFSOd;CrfLRd:SzACGe;"><p><span>The </span><span>Wow@Home Radio Telescope</span></p></div></h2></div><div id="h.3e7c17c5694c8634_44" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>This page presents a test of our first </span><span>Wow@Home Radio Telescope</span><span> hardware and software configuration (Figure 1). The system is tested for a network of small radio telescopes designed to emulate, as closely as possible, the observation protocol of the meridian radio telescope </span><span>Big Ear</span><span> used by the </span><span>Ohio SETI project</span><span> in the 1970s. As in the </span><a href="http://www.bigear.org/Wow30th/wow30th.htm" target="_blank"><span>original setup</span></a><span>, we use a </span><span>10 kHz channel width</span><span> and a </span><span>12-second integration time</span><span>. However, our system differs in several ways: it features 256 channels instead of 50, a much larger beam size, but significantly lower sensitivity.</span></p><p dir="ltr"><span>The telescope is </span><span>fixed at a constant elevation</span><span>, pointed south, and scans a specific celestial declination over the course of one or more days using a wide field of view of approximately 25° (HPBW or its beamwidth). As the Earth rotates, this configuration allows the telescope to capture a </span><span>continuous 360° strip of the sky</span><span> at that declination. After completing three or more full-sky passes, the telescope is adjusted to a new elevation to begin scanning a different declination, gradually building up </span><span>full-sky coverage over time</span><span>.</span></p><p dir="ltr"><span>While optimized for </span><span>educational use</span><span>, this configuration also yields valuable data on RFI near the H I line in urban environments, helping us assess the likelihood of RFI mimicking a Wow!-like signal. Additionally, it serves as a practical platform for a </span><span>wide-field search for strong transient events</span><span>, whether of astrophysical origin or potential technosignatures.</span></p><p dir="ltr"><span>For events that persist longer than a day, </span><span>multiple observing passes</span><span> can be used to validate their presence, detect weaker features, improve overall sensitivity, and help distinguish them from RFI. Additionally, </span><span>simultaneous observations</span><span> by two or more telescopes pointed at the same location can further aid in rejecting local interference and confirming the reality of signals that last less than 24 hours.</span></p><p dir="ltr"><span>The Wow@Home Radio Telescope operates autonomously, 24/7, as a meridian-style instrument, conducting a continuous </span><span>all-sky survey</span><span> </span><span>for transient events</span><span>. The hardware required to build these telescopes is both </span><span>inexpensive and widely accessible</span><span>, relying on readily available components. </span><span>The critical element lies in the software</span><span>, which must be capable of analyzing data effectively, whether from a single station or across a coordinated network of telescopes.</span></p><p dir="ltr"><span>Future expansions could include the integration of </span><span>multibeam systems</span><span> to enable simultaneous ON–OFF observations </span><span>to improve sensitivity</span><span>, </span><span>tracking capability</span><span> to perform targeted observations of specific sources, </span><span>multi-site detection</span><span> for signal validation, higher sensitivity, and RFI discrimination, </span><span>interferometric capabilities</span><span> for improved angular resolution, and </span><span>phased array configurations</span><span> to enhance sensitivity and enable electronic beam steering.</span></p></div><div id="h.3e7c17c5694c8634_3" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh3.googleusercontent.com/A5qS8iP4yHHZob04FJu8ja9Yf1wfndGkZ_jZD5ng32OkIaWIn0i2yyo47SUkBpoWr64Mxka8U4MbdyZMRsQGyIkk_K8z5--u0LU3PokF8Y1snW6TSvEkKvvducYt9A4RMw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_13" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 1: </span><span>Components of our first Wow@Home Radio Telescope. The </span><a href="https://github.com/tedcline/ezRA" target="_blank"><span>Easy Radio Astronomy (ezRA)</span></a><span> software is an excellent starter package for getting this configuration up and running for radio astronomy. We plan to test additional configurations in the coming months, including the </span><a href="https://www.crowdsupply.com/krakenrf/discovery-dish" target="_blank"><span>Discovery Dish</span></a><span>, which integrates the frontend into the antenna, and the </span><a href="https://airspy.com/airspy-mini/" target="_blank"><span>Airspy Mini</span></a><span> as the backend, offering a 12-bit ADC for improved dynamic range.</span></p></div><div id="h.3e7c17c5694c8634_30" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>The </span><span>Wow@Home Software</span><span> is the core of our project. It serves as the data acquisition and analysis platform designed to search for transient events caused by astrophysical phenomena, potential technosignatures, and RFI characterization, using data from any small radio telescope. The software is built on the analysis methods we are developing to detect Wow-like signals in the archive data of professional observatories, as part of our </span><a href="https://phl.upr.edu/wow"><span>Arecibo Wow! Project</span></a><span>. We are currently developing the software in </span><a href="https://www.nv5geospatialsoftware.com/Products/IDL" target="_blank"><span>IDL</span></a><span>, with example outputs shown in Figures 2, 3, and 4. It will later be translated to Python to ensure cross-platform compatibility and broader accessibility.</span></p></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_8"><div id="h.331927cc8f5c07b8_4"><p><img src="https://lh4.googleusercontent.com/zhgcn2e7vYwMCacemRJOlqkF5VcZcPjS6d8s5sf80ZYdLPNsoSpT0vNAZs7RIVnrGzhbHlboR0S0chVQrL1nGlPG6eynsKj6hH73NCSuuGDFrYGboTSbP8jtSwohUJC2vw=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_4"><p dir="ltr"><span>Figure </span><span>2</span><span>: </span><span>This is a test run of the Wow@Home Radio Telescope</span><span>. </span><span>The top panel shows the </span><span>relative</span><span> power as a function of time. The </span><span>next</span><span> panel </span><span>is</span><span> the signal-to-noise ratio (SNR). Most RFI here originates from continuum sources, which are relatively easy to filter out. </span><span>The following dynamic spectra images show three different ways to analyze the data, depending on the type of signal of interest. The broadband SNR is suitable for detecting continuum sources, but RFI heavily contaminates it. A second telescope at a different location could be used to cross-correlate astronomical signals. The mediumband SNR is good for highlighting the</span><span> Galactic center transiting </span><span>after</span><span> 6 hours and the Galactic anticenter </span><span>about</span><span> </span><span>12</span><span> hours later. The narrowband SNR is more sensitive to signals oc</span><span>curring in only one channel. </span><span>The horizontal line at channel 224 </span><span>is an injected</span><span> test signal spanning the telescope’s beamwidth. An actual narrowband RFI event is visible near channel 0 after 15 hours.</span></p></div></div><div jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1" id="h.f6d8ee3d198bc6f_23"><div id="h.5ad76efa4068bcdf_14"><p><img src="https://lh4.googleusercontent.com/wurJ-qrx1IZw_zkvp20ykcD736bQVpzjvXIlsXw0zhCGxcojIRQMiB-Fv8DJtt_9LSaYWyN1IN8daAZTjlNhwWg905MYGhiU7TKfwLVL3GZ7zC_mIz-T7OOS0t9ro8Qi=w1280" role="img"></p></div><div id="h.f6d8ee3d198bc6f_20"><p dir="ltr"><span>Figure 3: </span><a href="https://www.britannica.com/science/hydrogen-cloud" target="_blank"><span>Neutral Hydrogen</span></a><span> (H I) spectral profile of the Galactic center, extracted from the data in Figure 2 at 6.5 hours. Error bars represent the 1σ uncertainty in each frequency channel.</span></p></div></div><div id="h.3e7c17c5694c8634_36" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p><img src="https://lh6.googleusercontent.com/hMIAxkutySlccvKGun3Pd3CByn7EsvLmj8TkK5o3QYFmBoSQwug-dYthHbmg3DS0Dtn1e9C_INbNOE4iyTW3kj_czt590rnQ-rgksTYY7dY1bQPvL-8StNajaFB2AIkL7Q=w1280" role="img"></p></div><div id="h.3e7c17c5694c8634_40" jscontroller="sGwD4d" jsaction="zXBUYb:zTPCnb;zQF9Uc:Qxe3nd;" jsname="F57UId" tabindex="-1"><p dir="ltr"><span>Figure 4: </span><span>In addition to the modern analysis tools available with today’s radio telescopes, we also aim to incorporate into our software the ability to generate a live preview of the data in the style of the original Ohio State SETI project printouts. This feature is intended to provide historical context and connect current efforts to the legacy of early SETI research. Above is an example using the original Wow! Signal data.</span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Will AI systems perform poorly due to AI-generated material in training data? (103 pts)]]></title>
            <link>https://cacm.acm.org/news/the-collapse-of-gpt/</link>
            <guid>44010705</guid>
            <pubDate>Fri, 16 May 2025 23:27:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/news/the-collapse-of-gpt/">https://cacm.acm.org/news/the-collapse-of-gpt/</a>, See on <a href="https://news.ycombinator.com/item?id=44010705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Ever since ChatGPT was released to the public in November 2022, people have been using it to generate text, from emails to blog posts to bad poetry, much of which they post online. Since that release, the companies that build the large language models (LLMs) on which such chatbots are based—such as OpenAI’s GPT 3.5, the technology underlying ChatGPT—have also continued to put out newer versions of their models, training them with new text data, some of which they scraped off the Web. That means, inevitably, that some of the training data used to create LLMs did not come from humans, but from the LLMs themselves.</p><p id="p-2">That has led computer scientists to worry about a phenomenon they call model collapse. Basically, model collapse happens when the training data no longer matches real-world data, leading the new LLM to produce gibberish, in a 21st-century version of the classic computer aphorism “garbage in, garbage out.”</p><p id="p-3">LLMs work by learning the statistical distribution of so-called tokens—words or parts of words—within a language by examining billions of sentences garnered from sources including book databases, Wikipedia, and the Common Crawl dataset, a collection of material gathered from the Internet. An LLM, for instance, will figure out how often the word “president” is associated with the word “Obama” versus “Trump” versus “Hair Club for Men.” Then, when prompted by a request, it will produce words that it reasons have the highest probability of meeting that request and of following from previous words. The results bear a credible resemblance to human-written text.</p><p id="p-4">Model collapse is basically a statistical problem, said Sanmi Koyejo, an assistant professor of computer science at Stanford University. When machine-generated text replaces human-generated text, the distribution of tokens no longer matches the natural distribution produced by humans. As a result, the training data for a new round of modeling does not match the real world, and the new model’s output gets worse. “The thing we’re worried about is that the distribution of your data that you end up with, if you’re trying to fit your model, ends up really far from the actual distribution that generated the data,” he said.</p><p id="p-5">The problem arises because whatever text the LLM generates would be, at most, a subsample of the sentences on which it was trained. “Because you generate a finite sample, you have some probability of not sampling them,” said Yarin Gal, an associate professor of machine learning at Oxford University. “Once you don’t sample, then they disappear. They will never appear again. So every time you generate data, you basically start forgetting more and more of the tail events and therefore that leads to the concentration of the higher probability events.” Gal and his colleagues published a study in <i>Nature</i> in July that showed indiscriminate use of what they called ‘recursively generated data’ caused the models to fail.</p><p id="p-6">The problem is not limited to LLMs. Any generative model that is iteratively trained can suffer the same fate if it starts ingesting machine-produced data, Gal says. That includes stable diffusion models that create images, such as Dall-E. The issue also can affect variational autoencoders, which create new data samples by producing variations of their original data. It can apply to Gaussian mixture models, a form of unsupervised machine learning that sorts subpopulations of data into clusters; they are used to analyze customer preferences, predict stock prices, and analyze gene expression.</p><p id="p-7">Collapse is not a danger for models that incorporate synthetic data but only do so once, such as neural networks used to identify cancer in medical images, where synthetic data was used to augment rare or expensive real data. “The main distinction is that model collapse happens when you have multiple steps, where each step depends on the output from the previous step,” Gal said.</p><p id="p-8">The theory that replacing training data with synthetic data will quickly lead to the demise of LLMs is sound, Koyejo said. In practice, however, not all human data gets replaced immediately. Instead, when the generated text is scraped from the Internet, it gets mixed in with human text. “You create synthetic data, you add that to real data, so you now have more data, which is real data plus synthetic data,” he said. What is actually happening, he said, is not data replacement, but data accumulation. That slows the degradation of the dataset.</p><p id="p-9">Simply accumulating data may stop model collapse but can cause other problems if done without thought, said Yunzhen Feng, a Ph.D. student at the Center for Data Science at New York University. As a rule, the performance of neural networks improves as their size increases. Naively mixing real and synthetic data together, however, can slow that improvement. “You can still obtain similar performance, but you need much more data. That means you’re using much more compute and much more money to achieve that,” he said.</p><p id="p-10">One challenge is that there is no easy way to tell whether text found on the Internet is synthetic or human-generated. Though there have been attempts to automatically identify text from LLMs, none have been entirely successful. Research into this problem is ongoing, Gal said.</p></section><section id="sec2"><h2>Solving with curation</h2><p id="p-11">There are ways, however, to make the addition of synthetic data less of a problem.</p><p id="p-12">One approach is to curate the synthetic data to make sure it is of good quality. Some curation happens naturally, Gal said; people do not post everything their chatbot creates to the Internet, weeding out the material that contains false information or simply does not make sense, so that improves the training set.</p><p id="p-13">Curation can also be a deliberate process to make sure high-quality data goes into a training set. Feng, for instance, has experimented with asking the LLM to assess the quality of its own output. LLMs naturally select the words they think have the highest probability of fitting into a context. In doing so, they internally generate a score rating how confident they are that they are pairing the best words together. That same mechanism can be used to assess already generated text to rate its quality, with low-scoring results removed or the highest-scoring result of several attempts selected as the best. The idea is similar to a method used to fine-tune LLMs called reinforcement learning from human feedback (RLHF), in which people provide examples of good results, thereby pushing the models toward producing similar results. In this case, though, the LLM is generating its own feedback.</p><p id="p-14">How well that works varies by case, Feng said. The feedback can be improved by having other LLMs assess the same text and combining the results from different models. Including human assessments also improves the outcomes, as does applying some pre-written rules about what the output should look like. Eliminating lower-quality results from the synthetic data makes the generated data more closely resemble original data, he said. “It’s like you have a distribution of the synthetic data, you have a distribution of the real data, and you want to close the gap between them as much as possible,” he said.</p><p id="p-15">Improving the quality of synthetic data could also help with another challenge LLMs are facing as they try to improve: a dearth of new data on which to train. Scientists from Epoch AI, a research institute that focuses on trends in AI, have predicted the world will run out of new text to train on sometime between 2026 and 2032. With no new data on which to train future generations of LLMs, progress could stagnate. “The interesting question is, can synthetic data lead to not just stagnation but actual improvement in the model?” asked Pablo Villalobos, a staff researcher at Epoch.</p><p id="p-16">With curation of high-quality synthetic data, he said, the question becomes “whether this can be done iteratively so that each model generates better data that is used to train another model in basically the opposite of model collapse, in some virtuous circle.” He is not yet sure whether such improvement is possible, but sees some signs it could be.</p><p id="p-17">Other issues arise from training new models on generated data that do not quite reach the level of model collapse. For instance, Koyejo said, synthetic data could increase the likelihood that LLMs will discriminate against people in minority groups. Because any minority is by definition a smaller part of the data distribution, losing the tails of the distribution could make minorities disappear entirely. “Data tends to anchor on majority subgroups,” he said. “It tends to be good at capturing the most popular themes and less good at capturing tails. So less represented demographics can get erased in various ways.”</p><p id="p-18">While such erasure is something that could happen, he added, the issue has not been well studied. His colleague Diyi Yang, an assistant professor in the natural language processing group at Stanford, said there has been very little research into the question of how model collapse affects diversity issues. “Part of the reason is that, if you think about any existing big models, a lot of the training dynamics or checkpoints of those models actually are not really transparent or publicly available,” she said.</p><p id="p-19">In the end, Gal argued, model collapse is an important consideration, but not the matter of imminent disaster that some news coverage has made it out to be. “It’s a matter for the tech companies who build these models to be aware of how the models are being used and how the models are being trained, in order to avoid training on synthetic data that they themselves generated.”</p><h2 id="FurtherReading">Further Reading</h2><ul id="ref-list1"><li><p><span data-jats-publication-type="other"><em>Shumailov, I., Shumaylov, Z., Zhao, Y., Papernot, N., Anderson, R., and Gal, Y.</em> <br><strong>AI models collapse when trained on recursively generated data, <em><span>Nature</span></em>, 2024, DOI: 10.1038/s41586-024-07566-y </strong><a href="https://www.nature.com/articles/s41586-024-07566-y" data-jats-ext-link-type="uri"><strong>https://www.nature.com/articles/s41586-024-07566-y</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Feng, Y., Dohmatob, E., Yang, P., Charton, and F. Kempe, J.</em> <br><strong>Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement, <em>arXiv</em>, 2024, </strong><a href="https://doi.org/10.48550/arXiv.2406.07515" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2406.07515</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Gerstgrasser, M., Schaeffer, R., Dey, A., et al.</em> <br><strong>Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data, <em>arXiv</em>, 2024, </strong><a href="https://doi.org/10.48550/arXiv.2404.01413" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2404.01413</strong></a></span></p></li><li><p><span data-jats-publication-type="other"><em>Villalobos, P., Ho, A., Sevilla, J., Besiroglu, T., Heim, L., and Hobbhahn, M.</em> <br><strong>Will we run out of data? Limits of LLM scaling based on human-generated data, <em>arXiv</em>, 2022, </strong><a href="https://doi.org/10.48550/arXiv.2211.04325" data-jats-ext-link-type="uri"><strong>https://doi.org/10.48550/arXiv.2211.04325</strong></a></span></p></li><li></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moody’s strips U.S. of triple-A credit rating (261 pts)]]></title>
            <link>https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4</link>
            <guid>44009999</guid>
            <pubDate>Fri, 16 May 2025 21:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4">https://www.ft.com/content/e456ea34-c6ad-43fe-abe9-d4ce781c07b4</a>, See on <a href="https://news.ycombinator.com/item?id=44009999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div data-component="electionsHeader" data-component-unique-name="electionsHeader"><h2>US Politics 2025</h2></div><div data-component="topicHeroOffer" data-component-unique-name="topicsHeroOffer"><div><div data-o-grid-colspan="12 XL6"><p><span></span><span></span><span></span><span>Register to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 XL5"><p><h2><span><span>Continue reading and get the indispensable White House Watch newsletter for free.</span></span></h2></p></div></div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/hero_banner_us_election_white_house_watch.png?source=next-barrier-page" alt="WhiteHouseWatch"></p></div><div data-component="electionsFeatures" data-component-unique-name="electionsFeatures"><h2>We’ve got you covered</h2><div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_countdown.png?source=next-barrier-page"></p><h3>White House Watch newsletter</h3><p>Sign up for your free, indispensable guide to what Trump’s second term means for Washington, business and the world.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_hub_2024.png?source=next-barrier-page"></p><h3>Trump tracker: US tariffs</h3><p>As the president threatens a trade war, follow the latest on tariffs and executive orders</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_expert_opinion_analysis.png?source=next-barrier-page"></p><h3>US politics &amp; policy</h3><p>Stay on top of the latest events in US politics with the FT’s trusted and impartial coverage.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_democracy_2024.png?source=next-barrier-page"></p><h3>Expert Opinion &amp; Analysis</h3><p>Insight and analysis on US politics from commentators such as Ed Luce and James Politi</p></div></div><div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_countdown.png?source=next-barrier-page"></p><h3>White House Watch newsletter</h3><p>Sign up for your free, indispensable guide to what Trump’s second term means for Washington, business and the world.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_hub_2024.png?source=next-barrier-page"></p><h3>Trump tracker: US tariffs</h3><p>As the president threatens a trade war, follow the latest on tariffs and executive orders</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_expert_opinion_analysis.png?source=next-barrier-page"></p><h3>US politics &amp; policy</h3><p>Stay on top of the latest events in US politics with the FT’s trusted and impartial coverage.</p></div><div><p><img alt="" src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/images/us_election_2024/feature_democracy_2024.png?source=next-barrier-page"></p><h3>Expert Opinion &amp; Analysis</h3><p>Insight and analysis on US politics from commentators such as Ed Luce and James Politi</p></div></div></div><div id="recommendedOffers-recommendedOffers" data-component="recommendedOffers" data-component-unique-name="recommendedOffers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_digital_edition.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>FT Digital Edition</h3></p></div><p><span><span>€42.99</span><span> per 3 months</span></span></p><p><span><span>The new FT Digital Edition: today’s FT, cover to cover on any device. This subscription does not include access to ft.com or the FT App.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_standard.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Standard Digital</h3></p></div><p><span><span>€45</span><span> per month</span></span></p><p><span><span>Essential digital access to quality FT journalism on any device. Pay a year upfront and save 20%.</span></span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Premium Digital</h3></p></div><p><span><span>€69</span><span> per month</span></span></p><p><span><span>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</span></span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="subscriptionsOptions" data-o3-theme="inverse"><h2>Explore our full range of subscriptions.</h2><div><div><div><h3>For individuals</h3></div><p>Discover all the plans currently available in your country</p></div><div><div><h3> For multiple readers</h3></div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div><div><p>Check whether you already have access via your <span><a data-trackable="edu-finder" href="https://find-your-subscription.ft.com/?segmentId=a0e9a794-4c6d-bb35-e4dc-8bd409e0f54f&amp;ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">university</a></span> or <span><a data-trackable="licence-finder" href="https://subs.enterprise.ft.com/en-gb/licence-finder/?segmentId=9fb23d7d-afe4-12f3-3eaa-ff7a41e9d073&amp;ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">organisation.</a></span></p></div></div><div data-component="whyFT" data-component-unique-name="whyFT" data-o3-theme="inverse"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=e456ea34-c6ad-43fe-abe9-d4ce781c07b4">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting AI to write good SQL (425 pts)]]></title>
            <link>https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql</link>
            <guid>44009848</guid>
            <pubDate>Fri, 16 May 2025 21:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql">https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql</a>, See on <a href="https://news.ycombinator.com/item?id=44009848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Organizations depend on fast and accurate data-driven insights to make decisions, and SQL is at the core of how they access that data. With Gemini, Google can generate SQL directly from natural language — a.k.a. text-to-SQL. This capability increases developer and analysts’ productivity and empowers non-technical users to interact directly with the data they need.</span></p>
<p><span>Today, you can find text-to-SQL capabilities in many Google Cloud products:</span></p>
<ul>
<li>
<p><strong>BigQuery Studio</strong><span> in the </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_sql_from_a_comment"><span>SQL Editor</span></a><span> and </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span>SQL Generation tool</span></a><span>, and within the </span><strong>Data Canvas</strong><span> </span><a href="https://cloud.google.com/blog/products/data-analytics/using-bigquery-data-canvas-a-deep-dive?e=48754805#:~:text=powered%20by%20Gemini-,2.%20Generate%20SQL,-You%20can%20also"><span>SQL node</span></a></p>
</li>
<li>
<p><span>"Help me code" functionality in </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><strong>Cloud SQL Studio</strong></a><strong> (</strong><span>Postgres, MySQL and SQLServer), </span><strong>AlloyDB Studio</strong><span> and </span><strong>Cloud Spanner Studio</strong></p>
</li>
<li>
<p><strong>AlloyDB AI</strong><span> with its direct natural language interface to the database, currently available as a public preview</span></p>
</li>
<li>
<p><span>Through </span><strong>Vertex AI</strong><span>, which lets you access the Gemini models that are the basis for these products directly</span></p>
</li>
</ul>
<p><span>Recently, powerful large language models (LLMs) like Gemini, with their abilities to reason and synthesize, have driven remarkable advancements in the field of text-to-SQL. In this blog post, the first entry in a series, we explore the technical internals of Google Cloud's text-to-SQL agents. We will cover state-of-the-art approaches to context building and table retrieval, how to do effective evaluation of text-to-SQL quality with LLM-as-a-judge techniques, the best approaches to LLM prompting and post-processing, and how we approach techniques that allows the system to offer virtually certified correct answers.</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Text-to-SQL_at_Google_Cloud.max-1800x1800.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Text-to-SQL_at_Google_Cloud.max-1800x1800.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>The ‘Help me code’ feature in Cloud SQL Studio generates SQL from a text prompt</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>The challenges of text-to-SQL technology</strong></h3>
<p><span>Current state-of-the-art LLMs like Gemini 2.5 have reasoning capabilities that make them good at translating complex questions posed in natural language to functioning SQL, complete with joins, filters, aggregations and other difficult concepts.</span></p>
<p><span>To see this in action you can do a simple test in </span><a href="https://cloud.google.com/generative-ai-studio"><span>Vertex AI Studio</span></a><span>. Given the prompt </span><span>"I have a database schema that contains products and orders. Write a SQL query that shows the number of orders for shoes"</span><span>, Gemini produces SQL for a hypothetical schema:</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Great, this is a good looking query. But what happens when you move beyond this trivial example, and use Gemini for text-to-SQL against a real world database and on real-world user questions? It turns out that the problem is more difficult. The model needs to be complemented with methods to:&nbsp;</span></p>
<ol>
<li>
<p><span>provide business-specific context</span></p>
</li>
<li>
<p><span>understand user intent</span></p>
</li>
<li>
<p><span>manage differences in SQL dialects</span></p>
</li>
</ol>
<p><span>Let’s take a look at each of these challenges.&nbsp;</span></p>
<p><strong>Problem #1: Provide business-specific context</strong></p>
<p><span>Just like data analysts or engineers, LLMs need significant amounts of knowledge or "context" to generate accurate SQL. The context can be both explicit (what does the schema look like, what are the relevant columns, and what does the data itself look like?) or more implicit (what is the precise semantic meaning of a piece of data? what does it mean for the specific business case?).</span></p>
<p><span>Specialized model training, or fine tuning, is typically not a scalable solution to this problem. Training on the shape of every database or dataset, and keeping up with schema or data changes, is both difficult and cost-prohibitive. Business knowledge and semantics are often not well documented in the first place, and difficult to turn into training data.</span></p>
<p><span>For example, even the best DBA in the world would not be able to write an accurate query to track shoe sales if they didn't know that </span><code>cat_id2 = 'Footwear'</code><span> in a </span><code>pcat_extension</code><span> table means that the product in question is a kind of shoe. The same is true for&nbsp; LLMs.</span></p>
<p><strong>Problem #2: Understanding user intent</strong></p>
<p><span>Natural language is less precise than SQL. An engineer or analyst faced with an ambiguous question can detect that they need more information and go back and ask the right follow-up questions. An LLM, on the other hand, tends to try to give you an answer, and when the question is ambiguous, can be prone to hallucinating.</span></p>
<p><span>Example: Take a question like "What are the best-selling shoes?" Here, one obvious point of ambiguity is what "best selling" actually means in the context of the business or application — the most ordered shoes? The shoe brand that brought in the most money? Further, should the SQL count returned orders? And how many kinds of shoes do you want to see in the report? etc.&nbsp;</span></p>
<p><span>Further, different users need different kinds of answers. If the user is a technical analyst or a developer asking a vague question, giving them a reasonable, but perhaps not 100% correct SQL query is a good starting point. On the other hand, if the user is less technical and does not understand SQL, providing precise, correct SQL is more important. Being able to reply with follow-up questions to disambiguate, explaining the reasoning that went into an answer, and guiding the user to what they are looking for is key.</span></p>
<p><strong>Problem #3: Limits of LLM generation</strong></p>
<p><span>Out of the box, LLMs are particularly good at tasks like creative writing, summarizing or extracting information from documents. But some models can struggle with following precise instructions and getting details exactly right, particularly when it comes to more obscure SQL features. To be able to produce correct SQL, the LLM needs to adhere closely to what can often turn into complex specifications.</span></p>
<p><span>Example: Consider the differences between SQL dialects, which are more subtle than differences between programming languages like Python and Java. As a simple example, if you're using BigQuery SQL, the correct function for extracting a month from a timestamp column is </span><code>EXTRACT(MONTH FROM timestamp_column)</code><span>. But if you are using MySQL, you use </span><code>MONTH(timestamp_column)</code><span>.</span></p>
<h3><strong>Text-to-SQL techniques</strong></h3>
<p><span>At Google Cloud, we’re constantly evolving our text-to-SQL agents to improve their quality. To address the problems listed above, we apply a number of techniques.</span></p>
<div><table><colgroup><col><col></colgroup>
<tbody>
<tr>
<td>
<p><strong>Problem</strong></p>
</td>
<td>
<p><strong>Solutions</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>Understanding schema, data and business concepts</span></p>
</td>
<td>
<ul>
<li>
<p><strong>Intelligent retrieval</strong><span> and ranking of datasets, tables and columns, based on semantic similarity.</span></p>
</li>
<li>
<p><strong>In-context-learning</strong><span> with business specific examples</span></p>
</li>
<li>
<p><span>Data linking and sampling</span></p>
</li>
<li>
<p><span>Semantic layer over raw data. This provides a bridge between complex data structures and the everyday language used by the customer</span></p>
</li>
<li>
<p><span>Usage pattern analysis and query history</span></p>
</li>
</ul>
</td>
</tr>
<tr>
<td>
<p><span>Understanding user intent</span></p>
</td>
<td>
<p><strong>Disambiguation using LLMs</strong></p>
<ul>
<ul>
<li>
<p><span>Entity resolution</span></p>
</li>
</ul>
</ul>
<p><strong>SQL-aware foundation models</strong></p>
</td>
</tr>
<tr>
<td>
<p><span>Limits of LLM generation</span></p>
</td>
<td>
<p><strong>Self-consistency</strong></p>
<p><strong>Validation and rewriting</strong></p>
<ul>
<li>
<p><span>Strong foundation models</span></p>
</li>
<li>
<p><span>In-context-learning with dialect specific examples</span></p>
</li>
<li>
<p><span>Model finetuning</span></p>
</li>
</ul>
</td>
</tr>
</tbody>
</table></div></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Text-to-SQL_at_Google_Cloud.max-2200x2200.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Text-to-SQL_at_Google_Cloud.max-2200x2200.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>The text-to-SQL architecture</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Let’s take a closer look at some of these techniques.</span></p>
<p><strong>SQL-aware models<br></strong><span>Strong LLMs are the foundation of text-to-SQL solutions, and the Gemini family of models has a proven track record of high-quality code and SQL generation. Depending on the particular SQL generation task, we mix and match model versions, including some cases where we employ customized fine-tuning, for example to ensure that models provide sufficiently good SQL for certain dialects.</span></p>
<p><strong>Disambiguation using LLMs<br></strong><span>Disambiguation involves getting the system to respond with a clarifying question when faced with a question that is not clear enough (in the example above of "What is the best selling shoe?" should lead to a follow-up question like "Would you like to see the shoes ordered by order quantity or revenue?" from the text-to-SQL agent). Here we typically orchestrate LLM calls to first try to identify if a question can actually be answered given the available schema and data, and if not, to generate the necessary follow-up questions to clarify the user's intent.</span></p>
<p><strong>Retrieval and in-context-learning<br></strong><span>As mentioned above, providing models with the context they need to generate SQL is critical. We use a variety of indexing and retrieval techniques — first to identify relevant datasets, tables and columns, typically using vector search for multi-stage semantic matching, then to load additional useful context. Depending on the product, this may include things like user-provided schema annotations, examples of similar SQL or how to apply specific business rules, or samples of recent queries that a user has run against the same datasets. All of this data is organized into prompts then passed to the model. Gemini's support for long context windows unlocks new capabilities here by allowing the model to handle large schemas and other contextual information.</span></p>
<p><strong>Validation and reprompting<br></strong><span>Even with a high-quality model, there is still some level of non-determinism or unpredictability involved in LLM-driven SQL generation. To address this we have found that non-AI approaches like query parsing or doing a dry run of the generated SQL complements model-based workflows well. We can get a clear, deterministic signal if the LLM has missed something crucial, which we then pass back to the model for a second pass. When provided an example of a mistake and some guidance, models can typically address what they got wrong.</span></p>
<p><strong>Self-consistency<br></strong><span>The idea of self-consistency is to not depend on a single round of generation, but to generate multiple queries for the same user question, potentially using different prompting techniques or model variants, and picking the best one from all candidates. If several models agree that one answer looks particularly good, there is a greater chance that the final SQL query will be accurate and matches what the user is looking for.</span></p>
<h3><strong>Evaluation and measuring improvements</strong></h3>
<p><span>Improving AI-driven capabilities depends on robust evaluation. The text-to-SQL benchmarks developed in the academic community, like the popular </span><a href="https://bird-bench.github.io/" rel="noopener" target="_blank"><span>BIRD-bench</span></a><span>, have been a very useful baseline to understand model and end-to-end system performance. However, these benchmarks are often lacking when it comes to representing broad real-world schemas and workloads. To address this we have developed our own suite of synthetic benchmarks that augment the baseline in many ways.</span></p>
<p><strong>Coverage:</strong><span> We make sure to have benchmarks that cover a broad list of SQL engines and products, both dialects and engine-specific features. This includes not only queries, but also DDL, DML and other administrative needs, and questions that are representative for common usage patterns, including more complex queries and schemas.</span></p>
<p><strong>Metrics:</strong><span> We combine user metrics and offline eval metrics, and employ both human and automated evaluation, particularly using LLM-as-a-judge techniques, which reduce cost but still allow us to understand performance on ambiguous and unclear tasks.</span></p>
<p><strong>Continuous evals:</strong><span> Our engineering and research teams use evals to quickly be able to test out new models, new prompting techniques and other improvements. It can give us signals quickly to tell if an approach is showing promise and is worth pursuing.</span></p>
<p><span>Taken together, using these techniques are driving the remarkable improvements in text-to-SQL that we are witnessing in our labs, as well as in customers’ environments. As you get ready to incorporate text-to-SQL in your own environments, stay tuned for more deep dives into our text-to-SQL solutions. Try Gemini text-to-SQL in </span><a href="https://cloud.google.com/bigquery/docs/write-sql-gemini#use_the_sql_generation_tool"><span>BigQuery Studio</span></a><span>, </span><a href="https://cloud.google.com/sql/docs/mysql/write-sql-gemini"><span>CloudSQL, AlloyDB and Spanner Studio</span></a><span>, and in </span><a href="https://cloud.google.com/blog/products/databases/alloydb-ai-drives-innovation-from-the-database"><span>AlloyDB AI</span></a><span> today.</span></p></div><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/databases" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/databases" track-metadata-module="tag list" track-metadata-module_headline="posted in">Databases</a></li><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ClojureScript 1.12.42 (182 pts)]]></title>
            <link>https://clojurescript.org/news/2025-05-16-release</link>
            <guid>44009464</guid>
            <pubDate>Fri, 16 May 2025 20:20:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clojurescript.org/news/2025-05-16-release">https://clojurescript.org/news/2025-05-16-release</a>, See on <a href="https://news.ycombinator.com/item?id=44009464">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p><em>16 May 2025</em><br>
         <em>ClojureScript Team</em></p>

      <div id="preamble">
<p>We’re happy to announce a new release of ClojureScript. If you’re an existing
user of ClojureScript please read over the following release notes carefully.</p>
<p>This release features two significant dependency changes. First, Google Closure
Compiler has been updated to <code>v20250402</code>. This change makes Java 21 a
requirement for ClojureScript. The other significant change is that this release
now depends on the Clojure fork of Google Closure Library. Please read on for
more details about these changes.</p>
<p>For a complete list of fixes, changes, and enhancements to
ClojureScript see
<a href="https://github.com/clojure/clojurescript/blob/master/changes.md#1.12.42">here</a></p>
</div>
<div>
<h2 id="_google_closure_compiler_java_21"><a href="#_google_closure_compiler_java_21"></a>Google Closure Compiler &amp; Java 21</h2>
<div>
<p>Last year we noted that updating Google Closure Compiler would mean losing Java
8 support. Google Closure now requires Java 21. From our perspective this change
doesn’t seem strictly necessary, but Google is a large organization and this
change is likely to due to internal requirements which are hard to influence from
the outside. The general enthusiasm in the Clojure community around adopting more
recent Java releases hopefully softens the overall impact of this change.</p>
<p>So far, the burden of staying current with Google Closure has been manageable.
If for some reason that calculus changes, we could adopt the strategy we have taken
with Google Closure Library.</p>
</div>
</div>
<div>
<h2 id="_clojures_fork_of_google_closure_library"><a href="#_clojures_fork_of_google_closure_library"></a>Clojure’s Fork of Google Closure Library</h2>
<div>
<p>The incredible stability of Google Closure Library started declining around
2019. Google was both trying many things with respect to their internal
JavaScript strategy as well as becoming less concerned about the impact on outside
consumers. Finally, Google stopped contributing to Google Closure Library
last August.</p>
<p>We have forked Google Closure Library (GCL) and taken up maintenance. We backed out a
few years of needless breaking changes and aligned the codebase with the latest
Google Closure Compiler release.</p>
<p>One of the biggest benefits of GCL is that it makes ClojureScript a complete
solution for a variety of JavaScript contexts, not limited to the browser.
Taking on additional dependencies always comes with a cost. One of
ClojureScript’s original value propositions was a rock solid set of readily
available JavaScript tools as dependable as <code>clojure.core</code>.</p>
<p>We are working on restoring that original stability. With this release, you’ll
find that quite a few old ClojureScript libraries work again today as well
as they did <strong>14 years</strong> ago.</p>
<p>ClojureScript is not and never was only just for rich web applications. Even in the
post React-world, a large portion of the web is (sensibly) still using jQuery. If you need
robust DOM manipulation, internationalization, date/time handling, color
value manipulation, mathematics, programmatic animation, browser history management,
accessibility support, graphics, and much more, all without committing to a framework
and without bloating your final JavaScript artifact - ClojureScript is a one
stop shop.</p>
<p>Give it a try!</p>
</div>
</div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: KVSplit – Run 2-3x longer contexts on Apple Silicon (258 pts)]]></title>
            <link>https://github.com/dipampaul17/KVSplit</link>
            <guid>44009321</guid>
            <pubDate>Fri, 16 May 2025 20:04:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dipampaul17/KVSplit">https://github.com/dipampaul17/KVSplit</a>, See on <a href="https://news.ycombinator.com/item?id=44009321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 KVSplit</h2><a id="user-content--kvsplit" aria-label="Permalink: 🚀 KVSplit" href="#-kvsplit"></a></p>
<p dir="auto"><strong>Differentiated KV Cache Quantization for Apple Silicon</strong></p>
<p dir="auto"><a href="https://github.com/dipampaul17/KVSplit/stargazers"><img src="https://camo.githubusercontent.com/e0303260b83f266fb26524b0c11e415c6878324211d0c8edeae0fc1c0b3c9b2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f646970616d7061756c31372f4b5653706c69743f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562" alt="GitHub Stars" data-canonical-src="https://img.shields.io/github/stars/dipampaul17/KVSplit?style=for-the-badge&amp;logo=github"></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/e16a8865515749158a69c1e57a8fb9df3373386b8715b5d087517c0d3d887845/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f646970616d7061756c31372f4b5653706c69743f7374796c653d666f722d7468652d6261646765" alt="License" data-canonical-src="https://img.shields.io/github/license/dipampaul17/KVSplit?style=for-the-badge"></a>
<a href="https://github.com/dipampaul17/KVSplit/blob/main"><img src="https://camo.githubusercontent.com/8b0dd35d753ba4b14face3f93eafee56c2a5d58f05ee9eae4c8d00e55ca22e02/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f506c6174666f726d2d4170706c6525323053696c69636f6e2d626c61636b3f7374796c653d666f722d7468652d6261646765266c6f676f3d6170706c65" alt="Platform" data-canonical-src="https://img.shields.io/badge/Platform-Apple%20Silicon-black?style=for-the-badge&amp;logo=apple"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/kv_cache_memory_usage.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/kv_cache_memory_usage.png" alt="KV Cache Memory Usage" width="70%"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📌 Overview</h2><a id="user-content--overview" aria-label="Permalink: 📌 Overview" href="#-overview"></a></p>
<p dir="auto">Run <strong>larger context windows</strong> and <strong>heavier LLMs</strong> on your Mac by applying different quantization precision to keys vs values in the attention mechanism's KV cache. KVSplit enables you to:</p>
<ul dir="auto">
<li><strong>Reduce memory usage by up to 72%</strong> with minimal quality loss</li>
<li><strong>Run 2-3x longer contexts</strong> in the same memory budget</li>
<li><strong>Maintain or improve inference speed</strong> compared to FP16</li>
<li><strong>Optimize for Apple Silicon</strong> with full Metal support</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Findings</h2><a id="user-content-key-findings" aria-label="Permalink: Key Findings" href="#key-findings"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Tokens/sec</th>
<th>Perplexity Change</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB (100%)</td>
<td>54,360</td>
<td>--</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB (47%)</td>
<td>51,503</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB (41%)</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB (41%)</td>
<td>58,690</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB (28%)</td>
<td>55,193</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Savings by Sequence Length</h3><a id="user-content-memory-savings-by-sequence-length" aria-label="Permalink: Memory Savings by Sequence Length" href="#memory-savings-by-sequence-length"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>128 tokens</th>
<th>2048 tokens</th>
<th>4096 tokens</th>
<th>8192 tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (baseline)</td>
<td>5.50 MB</td>
<td>44.00 MB</td>
<td>88.00 MB</td>
<td>176.00 MB</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>2.92 MB</td>
<td>23.38 MB</td>
<td>46.75 MB</td>
<td>93.50 MB</td>
</tr>
<tr>
<td>K8V4 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V8 (mixed)</td>
<td>2.23 MB</td>
<td>17.88 MB</td>
<td>35.75 MB</td>
<td>71.50 MB</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>1.55 MB</td>
<td>12.38 MB</td>
<td>24.75 MB</td>
<td>49.50 MB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Independent quantization of keys and values in the KV cache</li>
<li>Optimized for Apple Silicon with Metal support</li>
<li>Comprehensive benchmarking suite with perplexity measurement</li>
<li>Memory usage and performance analysis tools</li>
<li>Publication-quality visualization tools</li>
<li>Easy setup and usage</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>macOS (tested on Apple Silicon)</li>
<li>Homebrew package manager</li>
<li>Xcode Command Line Tools</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ One-Command Installation</h2><a id="user-content--one-command-installation" aria-label="Permalink: ⚡ One-Command Installation" href="#-one-command-installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository
git clone https://github.com/dipampaul17/KVSplit.git
cd kvsplit

# Run the installer script
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh"><pre><span><span>#</span> Clone the repository</span>
git clone https://github.com/dipampaul17/KVSplit.git
<span>cd</span> kvsplit

<span><span>#</span> Run the installer script</span>
chmod +x scripts/install_kvsplit.sh
./scripts/install_kvsplit.sh</pre></div>
<p dir="auto">The installer will:</p>
<ul dir="auto">
<li>Set up the project structure</li>
<li>Clone and build llama.cpp with Metal support</li>
<li>Configure for differentiated KV cache quantization</li>
<li>Download a small test model (optional)</li>
<li>Set up Python environment for visualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏎️ Quick Comparison</h2><a id="user-content-️-quick-comparison" aria-label="Permalink: 🏎️ Quick Comparison" href="#️-quick-comparison"></a></p>
<p dir="auto">Want to see the benefits immediately? Run a quick comparison with your model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run quick comparison with different configurations
python scripts/quick_compare.py --model models/your-model.gguf"><pre><span><span>#</span> Run quick comparison with different configurations</span>
python scripts/quick_compare.py --model models/your-model.gguf</pre></div>
<p dir="auto">This will show you a side-by-side comparison of FP16, K8V8, K8V4, K4V8, and K4V4 with memory usage, speed, and quality metrics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">📊 Impressive Results</h2><a id="user-content--impressive-results" aria-label="Permalink: 📊 Impressive Results" href="#-impressive-results"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/memory_vs_quality.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/memory_vs_quality.png" alt="Memory vs Quality" width="50%"></a>
</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">📉 Memory Reduction</h3><a id="user-content--memory-reduction" aria-label="Permalink: 📉 Memory Reduction" href="#-memory-reduction"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>VRAM @ 8K tokens</th>
<th>Memory Savings</th>
<th>Quality Impact</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 (base)</td>
<td>176.00 MB</td>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>K8V8 (8-bit)</td>
<td>93.50 MB</td>
<td>47%</td>
<td>+0.03%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>71.50 MB</strong></td>
<td><strong>59%</strong></td>
<td><strong>+0.86%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>71.50 MB</td>
<td>59%</td>
<td>+6.06%</td>
</tr>
<tr>
<td>K4V4 (4-bit)</td>
<td>49.50 MB</td>
<td>72%</td>
<td>+6.15%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">📈 Performance Impact</h3><a id="user-content--performance-impact" aria-label="Permalink: 📈 Performance Impact" href="#-performance-impact"></a></p>
<p dir="auto">Using KVSplit doesn't just save memory—it often <strong>improves inference speed</strong> by 5-15%!</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Configuration</th>
<th>Tokens/sec (8K ctx)</th>
<th>Speedup vs FP16</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>54,360</td>
<td>—</td>
</tr>
<tr>
<td>K8V8</td>
<td>51,503</td>
<td>-5.3%</td>
</tr>
<tr>
<td><strong>K8V4</strong></td>
<td><strong>57,438</strong></td>
<td><strong>+5.7%</strong></td>
</tr>
<tr>
<td>K4V8</td>
<td>58,690</td>
<td>+8.0%</td>
</tr>
<tr>
<td>K4V4</td>
<td>55,193</td>
<td>+1.5%</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧠 Project Structure</h2><a id="user-content--project-structure" aria-label="Permalink: 🧠 Project Structure" href="#-project-structure"></a></p>
<div data-snippet-clipboard-copy-content="kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file"><pre><code>kvsplit/
├── llama.cpp/      # Optimized llama.cpp build
├── models/         # LLM model files
├── scripts/        # Utility scripts
│   ├── benchmark_kvsplit.py    # Comprehensive benchmark tool
│   ├── install_kvsplit.sh      # One-command installer
│   ├── quick_compare.py        # Quick comparison utility
│   ├── capture_memory.sh       # GIF creation for memory visualization
│   └── visualize_results.py    # Generate publication-quality plots
├── results/        # Benchmark results (CSV/JSON)
├── plots/          # Generated visualizations
└── README.md       # This file
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔬 Scientific Insight</h2><a id="user-content--scientific-insight" aria-label="Permalink: 🔬 Scientific Insight" href="#-scientific-insight"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/dipampaul17/KVSplit/blob/main/plots/configuration_summary.png"><img src="https://github.com/dipampaul17/KVSplit/raw/main/plots/configuration_summary.png" alt="Configuration Summary" width="80%"></a>
</p>
<p dir="auto">KV cache memory is dominated by storing key and value vectors for each token. Our research has revealed a critical insight: <strong>keys are significantly more sensitive to quantization than values</strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">🔑 Key Findings</h3><a id="user-content--key-findings" aria-label="Permalink: 🔑 Key Findings" href="#-key-findings"></a></p>
<ul dir="auto">
<li><strong>Asymmetric Impact</strong>: Keys require higher precision than values for maintaining quality</li>
<li><strong>Sweet Spot</strong>: K8V4 (8-bit keys, 4-bit values) provides optimal balance
<ul dir="auto">
<li>Only 0.86% perplexity degradation vs. FP16</li>
<li>59% memory reduction</li>
<li>Faster inference than FP16</li>
</ul>
</li>
<li><strong>Confirmation</strong>: K4V8 configuration shows 7x more quality degradation than K8V4, despite using the same total bits</li>
</ul>
<p dir="auto">This asymmetry allows for more efficient memory usage without compromising model quality, enabling longer context windows and larger models on consumer hardware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">💻 Usage Examples</h2><a id="user-content--usage-examples" aria-label="Permalink: 💻 Usage Examples" href="#-usage-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running with Different KV Cache Precisions</h3><a id="user-content-running-with-different-kv-cache-precisions" aria-label="Permalink: Running with Different KV Cache Precisions" href="#running-with-different-kv-cache-precisions"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Baseline (FP16)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn

# ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) 
# Best balance of quality and memory savings
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq 8

# 4-bit keys, 8-bit values (K4V8)
# Shows why key precision matters more than value precision
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

# 4-bit keys and values (K4V4)
# Maximum memory savings (72% reduction) with acceptable quality
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p &quot;Your prompt&quot; \
  -t 8 --flash-attn --kvq 4"><pre><span><span>#</span> Baseline (FP16)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn

<span><span>#</span> ⭐ RECOMMENDED: 8-bit keys, 4-bit values (K8V4) </span>
<span><span>#</span> Best balance of quality and memory savings</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq 8

<span><span>#</span> 4-bit keys, 8-bit values (K4V8)</span>
<span><span>#</span> Shows why key precision matters more than value precision</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq-key 4 --kvq-val 8

<span><span>#</span> 4-bit keys and values (K4V4)</span>
<span><span>#</span> Maximum memory savings (72% reduction) with acceptable quality</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p <span><span>"</span>Your prompt<span>"</span></span> \
  -t 8 --flash-attn --kvq 4</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Long Context Example (32K)</h3><a id="user-content-long-context-example-32k" aria-label="Permalink: Long Context Example (32K)" href="#long-context-example-32k"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt"><pre><span><span>#</span> Run with a 32K context (would require ~1.4GB in FP16, only ~400MB with K8V4)</span>
./llama.cpp/build/bin/llama-cli -m models/your-model.gguf \
  -c 32768 -n 4096 -t 8 --flash-attn --kvq 8 \
  -f your-long-document.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">🚩 Command-Line Arguments</h3><a id="user-content--command-line-arguments" aria-label="Permalink: 🚩 Command-Line Arguments" href="#-command-line-arguments"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Flag</th>
<th>Description</th>
<th>Recommendation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-t 8</code></td>
<td>Number of threads</td>
<td>8 is optimal for most Apple Silicon chips</td>
</tr>
<tr>
<td><code>--flash-attn</code></td>
<td>Enables optimized attention</td>
<td>Recommended for Apple Silicon</td>
</tr>
<tr>
<td><code>--kvq N</code></td>
<td>Sets both key and value bits to N</td>
<td>Use <code>--kvq 8</code> for K8V4 configuration</td>
</tr>
<tr>
<td><code>--kvq-key N</code></td>
<td>Sets key bits only</td>
<td>Key precision has major quality impact</td>
</tr>
<tr>
<td><code>--kvq-val N</code></td>
<td>Sets value bits only</td>
<td>Value precision has minor quality impact</td>
</tr>
<tr>
<td><code>-c N</code></td>
<td>Context size in tokens</td>
<td>Longer contexts benefit more from KVSplit</td>
</tr>
<tr>
<td><code>-n N</code></td>
<td>Number of tokens to generate</td>
<td>Adjust based on your needs</td>
</tr>
<tr>
<td><code>-f FILE</code></td>
<td>Input file</td>
<td>For processing documents</td>
</tr>
<tr>
<td><code>-m MODEL</code></td>
<td>Model path</td>
<td>Path to your .gguf model file</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">📏 Advanced Benchmarking</h2><a id="user-content--advanced-benchmarking" aria-label="Permalink: 📏 Advanced Benchmarking" href="#-advanced-benchmarking"></a></p>
<p dir="auto">For comprehensive performance analysis, use our full benchmark suite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Run the full benchmark suite (all configurations and sequence lengths)
python scripts/benchmark_kvsplit.py

# Run a specific configuration test
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

# Generate publication-quality visualizations
python scripts/visualize_results.py"><pre><span><span>#</span> Run the full benchmark suite (all configurations and sequence lengths)</span>
python scripts/benchmark_kvsplit.py

<span><span>#</span> Run a specific configuration test</span>
python scripts/benchmark_kvsplit.py --config K8V4 --seq-len 4096

<span><span>#</span> Generate publication-quality visualizations</span>
python scripts/visualize_results.py</pre></div>
<p dir="auto">The benchmarking script provides thorough measurements of:</p>
<ul dir="auto">
<li>📊 <strong>Memory Usage</strong>: VRAM and KV cache specifically</li>
<li>⚡ <strong>Performance</strong>: Tokens per second across different sequence lengths</li>
<li>🎯 <strong>Quality</strong>: Perplexity measurement using llama-perplexity</li>
<li>📈 <strong>Scaling</strong>: How memory usage and performance scale with sequence length</li>
</ul>
<p dir="auto">Results are saved in CSV/JSON formats with automatic summary statistics, and the visualization script generates publication-quality plots showing key insights.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎬 Visual Memory Savings</h2><a id="user-content--visual-memory-savings" aria-label="Permalink: 🎬 Visual Memory Savings" href="#-visual-memory-savings"></a></p>
<p dir="auto">You can visualize memory savings with our capture tool:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Capture memory reduction in Activity Monitor
./scripts/capture_memory.sh"><pre><span><span>#</span> Capture memory reduction in Activity Monitor</span>
./scripts/capture_memory.sh</pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">🍎 Apple Silicon Optimization</h2><a id="user-content--apple-silicon-optimization" aria-label="Permalink: 🍎 Apple Silicon Optimization" href="#-apple-silicon-optimization"></a></p>
<ul dir="auto">
<li><strong>Metal Performance</strong>: Fully optimized for Apple's Metal framework</li>
<li><strong>Memory Efficiency</strong>: Critical for memory-constrained M1/M2/M3 devices</li>
<li><strong>Activity Monitor</strong>: Use our <code>capture_memory.sh</code> script to visualize real-time memory reductions</li>
<li><strong>Alignment</strong>: 256B page alignment in llama.cpp means actual memory savings might differ slightly from theoretical calculations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Key Features</h2><a id="user-content--key-features" aria-label="Permalink: ⭐ Key Features" href="#-key-features"></a></p>
<ul dir="auto">
<li><strong>Differentiated Precision</strong>: Independent key and value bit precision (K8V4, K4V8, etc)</li>
<li><strong>Apple Silicon Optimization</strong>: Full Metal support for M1/M2/M3 chips</li>
<li><strong>Comprehensive Benchmarking</strong>: Memory, speed, and quality metrics</li>
<li><strong>Publication-Quality Visualization</strong>: Beautiful plots for analysis</li>
<li><strong>Simple User Interface</strong>: One-command install and quick comparison tools</li>
<li><strong>Memory Visualization</strong>: Tools to capture and visualize memory savings</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Acknowledgments</h2><a id="user-content--acknowledgments" aria-label="Permalink: 🙏 Acknowledgments" href="#-acknowledgments"></a></p>
<p dir="auto">This project implements ideas from recent research including:</p>
<ul dir="auto">
<li>"More for Keys, Less for Values: Adaptive KV Cache Quantization" (2024)</li>
<li>"Unifying KV Cache Compression for Large Language Models with LeanKV" (2025)</li>
</ul>
<p dir="auto">Additional credits:</p>
<ul dir="auto">
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> - Base implementation</li>
<li><a href="https://huggingface.co/TinyLlama" rel="nofollow">TinyLlama</a> - Test model</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🧠 Configuration Recommendations</h2><a id="user-content--configuration-recommendations" aria-label="Permalink: 🧠 Configuration Recommendations" href="#-configuration-recommendations"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best Overall</strong>: 🌟 <strong>K8V4</strong> 🌟 (8-bit keys, 4-bit values)</p>
<ul dir="auto">
<li>59% memory reduction with only 0.86% quality loss</li>
<li>Improved inference speed (+5.7% vs FP16)</li>
<li>Great balance of quality and efficiency</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Absolute Maximum Memory Savings</strong>: K4V4 (4-bit keys and values)</p>
<ul dir="auto">
<li>72% memory reduction with ~6% quality loss</li>
<li>Good for memory-constrained devices</li>
<li>Acceptable for less sensitive applications</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Best for Very Long Contexts</strong>: K8V4 or K4V4</p>
<ul dir="auto">
<li>Memory savings compound with context length</li>
<li>Run 2-3x longer contexts in the same memory budget</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔮 Future Roadmap</h2><a id="user-content--future-roadmap" aria-label="Permalink: 🔮 Future Roadmap" href="#-future-roadmap"></a></p>
<ul>
<li> <strong>Adaptive Precision</strong>: Dynamic precision based on token importance</li>
<li> <strong>Layer-Specific Quantization</strong>: Different precision for different model layers</li>
<li> <strong>Model-Specific Optimizations</strong>: Tailored for Mistral, Phi-3, etc.</li>
<li> <strong>Web Demo</strong>: Interactive testing environment</li>
<li> <strong>Mobile Support</strong>: Adapting for iOS and iPadOS</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📜 License</h2><a id="user-content--license" aria-label="Permalink: 📜 License" href="#-license"></a></p>
<p dir="auto">MIT</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions are welcome! Please open an issue or submit a pull request.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thoughts on thinking (556 pts)]]></title>
            <link>https://dcurt.is/thinking</link>
            <guid>44008843</guid>
            <pubDate>Fri, 16 May 2025 19:09:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dcurt.is/thinking">https://dcurt.is/thinking</a>, See on <a href="https://news.ycombinator.com/item?id=44008843">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">
  <article id="4txa6fWsfsW1TNkdx4esmW">
	<time datetime="2025-05-16">May 16, 2025</time>
  <h2>
    <a href="https://dcurt.is/thinking">Thoughts on thinking</a>
  </h2>
	<p>I have been stuck. Every time I sit down to write a blog post, code a feature, or start a project, I come to the same realization: in the context of AI, what I’m doing is a waste of time. It’s horrifying. The fun has been sucked out of the process of creation because nothing I make organically can compete with what AI already produces—or soon will. All of my original thoughts feel like early drafts of better, more complete thoughts that simply haven’t yet formed inside an LLM. </p>

<p>I used to write prolifically. I’d have ideas, write them down, massage them slowly and carefully into cohesive pieces of work over time, and then–when they were ready–share them with the world. I’d obsess for hours before sharing anything, working through the strengths and weaknesses of my thinking. Early in my career, that process brought a lot of external validation. And because I think when I write, and writing is how I form opinions and work through holes in my arguments, my writing would lead to more and better thoughts over time. Thinking is compounding–the more you think, the better your thoughts become. </p>

<p>But now, when my brain spontaneously forms a tiny sliver of a potentially interesting concept or idea, I can just shove a few sloppy words into a prompt and almost instantly get a fully reasoned, researched, and completed thought. Minimal organic thinking required. This has had a dramatic and profound effect on my brain. My thinking systems have atrophied, and I can feel it–I can sense my slightly diminishing intuition, cleverness, and rigor. And because AI can so easily flesh out ideas, I feel less inclined to share my thoughts–no matter how developed.</p>

<p>I thought I was using AI in an incredibly positive and healthy way, as a bicycle for my mind and a way to vastly increase my thinking capacity. But LLMs are insidious–using them to explore ideas feels like work, but it’s not real work. Developing a prompt is like scrolling Netflix, and reading the output is like watching a TV show. Intellectual rigor comes from the journey: the dead ends, the uncertainty, and the internal debate. Skip that, and you might still get the insight–but you’ll have lost the infrastructure for meaningful understanding. Learning by reading LLM output is cheap. Real exercise for your mind comes from building the output yourself. </p>

<p>The irony is that I now know more than I ever would have before AI. But I feel slightly dumber. A bit more dull. LLMs give me finished thoughts, polished and convincing, but none of the intellectual growth that comes from developing them myself. The output from AI answers questions. It teaches me facts. But it doesn’t really help me <em>know</em> anything new. </p>

<p>While using AI feels like a superhuman brain augmentation, when I look back on the past couple of years and think about how I explore new thoughts and ideas today, it looks a lot like sedation instead. </p>

<p>And I’m still stuck. But at least I’m here, writing this, and conveying my raw thoughts directly into your brain. And that means something, I think, even though an AI could probably have written this post far more quickly, eloquently, and concisely. It’s horrifying. </p>

<hr>

<p>This post was written entirely by a human, with no assistance from AI. (Other than spell- and grammar-checking.)</p>

  <figure id="kudo_4txa6fWsfsW1TNkdx4esmW">
    <a href="#kudo">
      
    </a>
    <p>399</p>
    <p>Kudos</p>
  </figure>
  <figure id="kudo_side_4txa6fWsfsW1TNkdx4esmW">
    <a href="#kudo">
      
    </a>
    <p>399</p>
    <p>Kudos</p>
  </figure>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X X^t can be faster (188 pts)]]></title>
            <link>https://arxiv.org/abs/2505.09814</link>
            <guid>44006824</guid>
            <pubDate>Fri, 16 May 2025 15:45:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2505.09814">https://arxiv.org/abs/2505.09814</a>, See on <a href="https://news.ycombinator.com/item?id=44006824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
    </channel>
</rss>