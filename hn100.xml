<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 May 2024 23:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Sam Altman Is Showing Us Who He Really Is (219 pts)]]></title>
            <link>https://slate.com/technology/2024/05/scarlett-johansson-ai-voice-sam-altman-openai.html</link>
            <guid>40434800</guid>
            <pubDate>Tue, 21 May 2024 22:25:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://slate.com/technology/2024/05/scarlett-johansson-ai-voice-sam-altman-openai.html">https://slate.com/technology/2024/05/scarlett-johansson-ai-voice-sam-altman-openai.html</a>, See on <a href="https://news.ycombinator.com/item?id=40434800">Hacker News</a></p>
<div id="readability-page-1" class="page"><article data-uri="slate.com/_components/article/instances/clwguixmj009k9skq6h6zx1ol@published" data-has-roadblock="false" data-rubric="the-industry" itemscope="" itemtype="http://schema.org/Article">
  

  

<header>

  <a href="https://slate.com/technology/the-industry">      The Industry</a>

  

<h2 itemprop="alternativeHeadline">We should believe him.</h2>


    </header>
<div>
      <figure data-uri="slate.com/_components/image/instances/clwguixmj009d9skq1z3bk7uf@published" data-editable="imageInfo"><p><img loading="lazy" src="https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0" alt="Sam Altman and Scarlett Johansson facing off with an OpenAI logo between them" width="1560" height="1040" srcset="https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=320 320w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=480 480w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=600 600w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=840 840w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=960 960w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1280 1280w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1440 1440w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1600 1600w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=1920 1920w,
https://compote.slate.com/images/0c86f783-b55b-41d2-9e50-4022fae4f0a9.jpeg?crop=1560%2C1040%2Cx0%2Cy0&amp;width=2200 2200w">
        
      </p>
<figcaption>
<span>Photo illustration by Slate. Photos by Mike Coppola/Getty Images for Time and Andreas Rentz/Getty Images.</span>
</figcaption>
</figure>

  </div>
  

  <section>
      


      

    <div itemprop="mainEntityOfPage">
          <p data-word-count="52" data-uri="slate.com/_components/slate-paragraph/instances/clwguixmj009e9skqd74r89vp@published">OpenAI, the research firm whose 2022 launch of ChatGPT single-handedly pushed “artificial intelligence” into the mainstream, isn’t often inclined to back down from the knotty disputes—over copyright, safety concerns, appropriate regulations—that its innovative tech has raised. Yet this month, it antagonized someone much more powerful, and is already retreating just a touch.</p>

  <p data-word-count="46" data-uri="slate.com/_components/slate-paragraph/instances/clwguog0k001v3b6zjqs3c7hy@published">On Monday evening, <a href="https://www.npr.org/2024/05/20/1252495087/openai-pulls-ai-voice-that-was-compared-to-scarlett-johansson-in-the-movie-her">Scarlett Johansson issued a statement to NPR’s Bobby Allyn</a> about OpenAI’s <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o announcement</a>, which the company showcased in a live demonstration just last week. Specifically, the multimodal computer interaction model centered around a voice assistant named Sky, whose timbre really, <em>really</em> resembled ScarJo’s.</p>

  <p data-word-count="95" data-uri="slate.com/_components/slate-paragraph/instances/clwguog27001w3b6zx9o5tbmu@published">“Last September, I received an offer from Sam Altman, who wanted to hire me to voice the current ChatGPT 4.0 system,” <a href="https://www.hollywoodreporter.com/business/business-news/openai-pulls-chatgpt-voice-sounds-like-scarlett-johansson-1235904085/">Johansson wrote</a>. “After much consideration and for personal reasons, I declined the offer. Nine months later, my friends, family and the general public all noted how much the newest system named ‘Sky’ sounded like me. When I heard the released demo, I was shocked, angered and in disbelief that Mr. Altman would pursue a voice that sounded so eerily similar to mine that my closest friends and news outlets could not tell the difference.”</p>

  

  

  


  <p data-word-count="57" data-uri="slate.com/_components/slate-paragraph/instances/clwguog3q001x3b6za1ae57o0@published">Johansson likewise mentioned that she was “forced” to hire lawyers who wrote letters to Altman and his company, after which “OpenAI reluctantly agreed” to switch out the voice. Indeed, earlier that morning, OpenAI tweeted that it was “<a href="https://x.com/OpenAI/status/1792443575839678909">working to pause the use of Sky</a>” after hearing “questions about how we chose the voices in ChatGPT, especially Sky.”</p>

  


  


  


  <p data-word-count="42" data-uri="slate.com/_components/slate-paragraph/instances/clwguog71001z3b6z5r93ihe0@published">“We believe that AI voices should not deliberately mimic a celebrity’s distinctive voice—Sky’s voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice,” the company insisted in an <a href="https://openai.com/index/how-the-voices-for-chatgpt-were-chosen/">accompanying blog post</a>.</p>

  <p data-word-count="54" data-uri="slate.com/_components/slate-paragraph/instances/clwguog8s00203b6zbk8mp3xx@published">Altman also spoke to Johansson’s explicit objection after NPR’s reporting, telling the public broadcaster: “We cast the voice actor behind Sky’s voice before any outreach to Ms. Johansson. Out of respect for Ms. Johansson, we have paused using Sky’s voice in our products. We are sorry to Ms. Johansson that we didn’t communicate better.”</p>

  


  <p data-word-count="81" data-uri="slate.com/_components/slate-paragraph/instances/clwguoga800213b6z1k986xv7@published">It was a strange apology and dubious explanation, not least because Altman <em>himself</em> invited comparisons of Sky’s voice to Johansson’s during the GTP-4o rollout. As was noted amply last week, he tweeted the word “<a href="https://x.com/sama/status/1790075827666796666?lang=en">her</a>,” in obvious reference to what <a href="https://sfstandard.com/2023/09/12/sam-altman-dreamforce-2023/">he’s previously called his favorite movie</a>: <em>Her, </em>the 2013 <a href="https://slate.com/culture/2014/01/her-movie-by-spike-jonze-with-joaquin-phoenix-and-scarlett-johansson-lacks-a-real-woman.html">Oscar-winning drama</a> in which Johansson voices a Siri-like voice assistant with whom the film’s protagonist falls in love. (Altman, in a subsequent personal blog post: “<a href="https://blog.samaltman.com/gpt-4o">It feels like AI from the movies</a>.”)</p>

  <p data-word-count="77" data-uri="slate.com/_components/slate-paragraph/instances/clwguogbs00223b6zva6a045h@published">What’s more, as Johansson mentioned in her statement: “Two days before the ChatGPT 4.0 demo was released, Mr. Altman contacted my agent, asking me to reconsider. Before we could connect, the system was out there.” And, as the Washington Post’s Nitasha Tiku <a href="https://x.com/nitashatiku/status/1792730629735624849">tweeted</a>, she noticed during a live demo in September—the very same month that OpenAI reportedly offered ScarJo its hire-for-training offer—that the Sky voice even then sounded like Johansson, and that executives denied this was “intentional.”</p>

  


  

  


  <p data-word-count="80" data-uri="slate.com/_components/slate-paragraph/instances/clwguoger00243b6zrdjm9xd0@published">Naturally, this hullabaloo has invited quite a bit of attention: The world’s most influential A.I. company is squaring off against a <a href="https://www.theguardian.com/film/2021/oct/01/scarlett-johansson-settles-black-widow-lawsuit-disney">brand-name and litigation-happy celebrity</a>, over a rather bizarre interpretation of one of her most acclaimed movies, in a reference that was employed in large part to launch one of OpenAI’s most esteemed upgrades to date—and one that has already fueled a <a href="https://techcrunch.com/2024/05/20/chatgpts-mobile-app-revenue-saw-biggest-spike-yet-following-gpt-4o-launch/">surge in its mobile-app revenue</a> (as well as controversy over the <a href="https://www.technologyreview.com/2024/05/17/1092649/gpt-4o-chinese-token-polluted/">spam used to train</a> its Chinese-language capabilities).</p>

  


  <p data-word-count="48" data-uri="slate.com/_components/slate-paragraph/instances/clwguoggn00253b6zarqbiktp@published">The timing is a bit eyebrow-raising as well, considering that Johansson and her fellow actors only reached a post-strike union deal months ago—one fueled in large part over concerns <a href="https://slate.com/technology/2023/06/screen-actors-guild-artificial-intelligence-strike-digital-doubles.html">regarding A.I.’s impacts on the movie industry</a>. It makes sense that SAG-AFTRA publicly <a href="https://www.nbcnews.com/tech/sag-aftra-applauds-scarlett-johansson-rebuking-openai-voice-sounded-rcna153256">praised Johansson’s stance</a> here against OpenAI.</p>

  


  


  <p data-word-count="119" data-uri="slate.com/_components/slate-paragraph/instances/clwguogi200263b6z6syf43hn@published">It’s also unusual to see Altman and his executives assume a preemptively defensive crouch on this, especially since they’ve offered little in the way of apology or transparency when it comes to the heaping amounts of data used to train and power apps like ChatGPT and DALL-E 3—other than admitting it would be “<a href="https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai">impossible</a>” to train these models without sucking up vast amounts of copyright work, whether books or artworks or articles, sans permission or disclosure. The company is already fending off <a href="https://www.courthousenews.com/authors-guild-to-add-microsoft-in-its-lawsuit-against-openai/">lawsuits</a> from <a href="https://www.latimes.com/business/story/2024-02-16/column-sarah-silvermans-openai-copyright-lawsuit-ai">authors</a> and <a href="https://www.reuters.com/legal/us-newspapers-sue-openai-copyright-infringement-over-ai-training-2024-04-30/">news publishers</a> over this very practice—and ironically, as 404 Media reported this month, it also <a href="https://www.404media.co/openai-files-copyright-claim-against-chatgpt-subreddit/">sent a “copyright complaint” to the moderators</a> of the OpenAI subreddit over their use of … OpenAI’s logo.</p>

  


  <p data-word-count="71" data-uri="slate.com/_components/slate-paragraph/instances/clwguogjl00273b6zhuom9wde@published">Yet OpenAI’s reportedly aggressive actions in courting ScarJo’s voice and then pressing ahead without her consent have invited a <a href="https://x.com/BobbyAllyn/status/1792679435701014908/quotes">new level of public opprobrium</a> against the otherwise popular app-maker. The resulting damage control may arise from the fact that this incident, plus other developments from inside OpenAI’s offices over the past few months, may be exposing something else about the notoriously closed-lid firm: what kind of person Sam Altman <em>really</em> is.</p>

  


  


  <p data-word-count="120" data-uri="slate.com/_components/slate-paragraph/instances/clwguogl200283b6zw1skff4s@published">The last time OpenAI drama made national news occurred when the majority of the company’s board gave a no-confidence vote in Altman and <a href="https://slate.com/technology/2023/11/sam-altman-fired-openai-mira-murati.html">suddenly fired</a> him in November. They claimed that “he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities.” The vague statement and rapid action led to public speculation that coalesced into an <a href="https://slate.com/technology/2023/11/openai-sam-altman-ai-microsoft-eacc-effective-altruism.html">all-out war between</a> two staunch sides: one consisting of Altman and his unfailingly loyal lieutenants, the other consisting of underlings and overseers worried over how quickly Altman wanted to develop and deploy new products, with little consideration to their potential for misuse, as well as his “psychologically abusive” treatment of employees (as <a href="https://www.washingtonpost.com/technology/2023/12/08/open-ai-sam-altman-complaints/">the Post’s Tiku had reported</a>).</p>

  


  <p data-word-count="65" data-uri="slate.com/_components/slate-paragraph/instances/clwguogmi00293b6zh6o5z4zn@published">The aftereffects of that dust-up lingered for months after Altman was restored and the OpenAI <a href="https://slate.com/technology/2023/11/openai-sam-altman-ai-microsoft-eacc-effective-altruism.html">board was (mostly) purged</a> of his opponents. Ilya Sutskever, a main character in the November saga who’d <a href="https://www.nytimes.com/2023/11/21/technology/openai-altman-board-fight.html">reportedly questioned</a> Altman’s honesty but stayed on at OpenAI, apparently “<a href="https://www.nytimes.com/2024/05/14/technology/ilya-sutskever-leaving-openai.html">never returned to work</a>” in the months following the fight, according to the New York Times. Last week, the company announced his departure.</p>

  <p data-word-count="77" data-uri="slate.com/_components/slate-paragraph/instances/clwguognz002a3b6zlv3jjmdh@published">Just hours after that news, the <a href="https://x.com/janleike/status/1791498174659715494">head of Sutskever’s team announced <em>his</em> resignation</a>, tweeting that he’d “been disagreeing with OpenAI leadership about the company’s core priorities for quite some time, until we finally reached a breaking point.” <a href="https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence">Reporters at Vox</a> revealed that “at least five more of the company’s most safety-conscious employees have either quit or been pushed out” since November, with one of those quitters telling the website that he’d “gradually lost trust in OpenAI leadership.”</p>

  


  


  


  <p data-word-count="58" data-uri="slate.com/_components/slate-paragraph/instances/clwguogpd002b3b6zk71zd0w0@published">There are likely still others who <em>can’t</em> talk, as Vox found out, because of an “<a href="https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release">extremely restrictive off-boarding agreement</a>” that OpenAI employees are forced to sign if they want to retain any vested equity with the company—and that “forbids them, for the rest of their lives, from criticizing their former employer” or “even acknowledging that the NDA exists.”</p>

  


  <p data-word-count="73" data-uri="slate.com/_components/slate-paragraph/instances/clwguogqp002c3b6z5i00d3hq@published">These revelations made waves in the tech world even prior to the ScarJo incident; Altman claimed on X that he was <a href="https://x.com/sama/status/1791936857594581428">unaware of the equity provision</a>, that it was being rewritten, and that “if any former employee who signed one of those old agreements is worried about it, they can contact me and we’ll fix that too.” But in light of Altman’s reputation within Silicon Valley, this comes across as a bit fishy.</p>

  


  
  <p data-word-count="71" data-uri="slate.com/_components/slate-paragraph/instances/clwguogss002d3b6zcpacjspp@published">Past <a href="https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/">reporting from the MIT Technology Review</a> has indicated that OpenAI “is obsessed with maintaining secrecy, protecting its image, and retaining the loyalty of its employees.” This appeared to play out in April, when the Information reported that two OpenAI staffers, one of them an ally of Sutskever’s, were fired for “<a href="https://www.theinformation.com/articles/openai-researchers-including-ally-of-sutskever-fired-for-alleged-leaking">allegedly leaking information</a>.” As Bloomberg has noted, Altman is quietly thought of among his industry as “<a href="https://www.bloomberg.com/news/articles/2023-11-18/the-perpetual-rise-of-sam-altman-takes-an-unexpected-turn">ambitious, cunning, even Machiavellian</a>.”</p>

  <p data-word-count="81" data-uri="slate.com/_components/slate-paragraph/instances/clwguogu9002e3b6zrl9zztr7@published">Other details from OpenAI’s ongoing copyright battles appear to bolster this. As part of its suit with the Authors Guild, documents were released showing how OpenAI deleted two massive datasets, consisting of “<a href="https://www.businessinsider.com/openai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5">more than 100,000 published books</a>,” that had been used to train an early iteration of its GPT model. Furthermore, the employees tasked with scrubbing that data were no longer with OpenAI, which had refused to disclose anything about its training-data history to the Authors Guild prior to this lawsuit.</p>

  


  

  <p data-word-count="116" data-uri="slate.com/_components/slate-paragraph/instances/clwguogvm002f3b6ze0r847oi@published">On Monday, the lead counsel for <a href="https://slate.com/podcasts/what-next-tbd/2024/01/media-to-ai-no-payin-no-trainin">the New York Times’ own suit against OpenAI</a> sent a letter to the presiding judge, <a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.612697/gov.uscourts.nysd.612697.117.0.pdf">accusing the company</a> of delaying a similar discovery process by “taking weeks to respond, dragging out negotiations over a protective order, and refusing to quickly produce basic information.” Oh yeah, and the Securities and Exchange Commission is <a href="https://www.wsj.com/tech/sec-investigating-whether-openai-investors-were-misled-9d90b411">investigating Altman’s comms in a probe</a> over whether he and other senior OpenAI leaders misled the company’s investors. Yet Altman clearly hopes all this will wash away: OpenAI’s first post in the aftermath of the ScarJo squabble was a “<a href="https://openai.com/index/openai-safety-update/">safety update</a>” for the AI Seoul Summit, which mentions that “we prioritize protecting our customers, intellectual property, and data.”</p>

  


  


  


  <p data-word-count="95" data-uri="slate.com/_components/slate-paragraph/instances/clwguogx1002g3b6z41faspnx@published">For all Altman talks about the importance of “<a href="https://www.nytimes.com/2023/11/29/technology/openai-sam-altman-plans.html">transparency</a>,” for all the generosity he’s <a href="https://openai.com/index/sam-altman-returns-as-ceo-openai-has-a-new-initial-board/">professed</a> toward OpenAI’s dissenters, and for all the <a href="https://slate.com/technology/2023/05/sam-altman-openai-hearing-senate-chatgpt-frivolous-lawsuit.html">governmental glad-handing</a> he’s done to paint himself as a <a href="https://static1.squarespace.com/static/66465fcd83d1881b974fe099/t/664b8a3ad1b9b048865322c5/1716226618595/AI+Senate+Shadow+Report.pdf">responsible steward</a> of A.I., it seems pretty clear he’s unafraid of playing a completely different game under wraps. If the underlying record of how he appears to treat his own employees, run his company, and keep his secrets clashes so much with his public statements, why should anyone—least of all Scarlett Johansson herself—trust that he actually went about Sky’s voice-training process in good <span>faith?</span></p>

  

  

</div>

      <ul>
<li>
            <a href="https://slate.com/tag/artificial-intelligence">
              Artificial Intelligence
            </a>
          </li><li>
            <a href="https://slate.com/tag/celebrities">
              Celebrities
            </a>
          </li>      </ul>

  </section>

      

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gordon Bell has died (551 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/05/gordon-bell-an-architect-of-our-digital-age-dies-at-age-89/</link>
            <guid>40432688</guid>
            <pubDate>Tue, 21 May 2024 19:23:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/05/gordon-bell-an-architect-of-our-digital-age-dies-at-age-89/">https://arstechnica.com/gadgets/2024/05/gordon-bell-an-architect-of-our-digital-age-dies-at-age-89/</a>, See on <a href="https://news.ycombinator.com/item?id=40432688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      the great memory register in the sky    —
</h4>
            
            <h2 itemprop="description">Bell architected DEC's VAX minicomputers, championed computer history, mentored at Microsoft.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-136021414-800x517.jpg" alt="A photo of Gordon Bell speaking at the annual PC Forum in Palm Springs, California, March 1989.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-136021414-scaled.jpg" data-height="1655" data-width="2560">Enlarge</a> <span>/</span> A photo of Gordon Bell speaking at the annual PC Forum in Palm Springs, California, March 1989.</p></figcaption>  </figure>

  




<!-- cache hit 136:single/related:379d27030a7eb2687e4b5c3989754ba7 --><!-- empty -->
<p>Computer pioneer <a href="https://gordonbell.azurewebsites.net/">Gordon Bell</a>, who as an early employee of <a href="https://arstechnica.com/gadgets/2023/10/long-gone-dec-is-still-powering-the-world-of-computing/">Digital Equipment Corporation</a> (DEC) played a key role in the development of several influential minicomputer systems and also co-founded the first major computer museum, passed away on Friday, according to Bell Labs veteran John Mashey. Mashey announced Bell's passing in a <a href="https://x.com/JohnMashey/status/1792782878608445595">social media post</a> on Tuesday morning.</p>

<p>"I am very sad to report [the] death May 17 at age 89 of Gordon Bell, famous computer pioneer, a founder of Computer Museum in Boston, and a force behind the @ComputerHistory here in Silicon Valley, and good friend since the 1980s," wrote Mashey in his announcement. "He succumbed to aspiration pneumonia in Coronado, CA."</p>
<p>Bell was a pivotal figure in the history of computing and a notable champion of tech history, having founded Boston's <a href="https://en.wikipedia.org/wiki/The_Computer_Museum,_Boston">Computer Museum</a> in 1979, which later became the heart of the <a href="https://en.wikipedia.org/wiki/Computer_History_Museum">Computer History Museum</a> in Mountain View, with his wife <a href="https://en.wikipedia.org/wiki/Gwen_Bell">Gwen Bell</a>. He was also the namesake of the ACM's prestigious <a href="https://awards.acm.org/bell">Gordon Bell Prize</a>, created to spur innovations in parallel processing.</p>
<p>Born in 1934 in Kirksville, Missouri, Gordon Bell earned degrees in electrical engineering from MIT before being recruited in 1960 by DEC founders Ken Olsen and Harlan Anderson. As the second computer engineer hired at DEC, Bell <a href="https://web.archive.org/web/20050402125352/http://americanhistory.si.edu/collections/comphist/bell.htm#PDP-11">worked on</a> various components for the <a href="https://en.wikipedia.org/wiki/PDP-1">PDP-1</a> system, including floating-point subroutines, tape controllers, and a drum controller.</p>
<p>Bell also invented the first <a href="https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter">UART</a> (Universal Asynchronous Receiver-Transmitter) for serial communication during his time at DEC. He went on to architect several influential DEC systems, including the <a href="https://en.wikipedia.org/wiki/PDP-4">PDP-4</a> and <a href="https://en.wikipedia.org/wiki/PDP-6">PDP-6</a>. In the 1970s, he played a key role in overseeing the aforementioned <a href="https://en.wikipedia.org/wiki/VAX">VAX minicomputer</a> line as the engineering manager, with <a href="https://www.computer.org/profiles/william-strecker">Bill Strecker</a> serving as the primary architect for the VAX architecture.</p>                                            
                                                        
<p>After retiring from DEC in 1983, Bell remained active as an entrepreneur, policy adviser, and researcher. He co-founded <a href="https://en.wikipedia.org/wiki/Encore_Computer">Encore Computer</a> and helped establish the NSF's <a href="https://new.nsf.gov/cise">Computing and Information Science and Engineering Directorate</a>.</p>
<p>In 1995, Bell joined Microsoft Research where he studied telepresence technologies and served as the subject of the <a href="https://www.microsoft.com/en-us/research/project/mylifebits/">MyLifeBits</a> life-logging project. The initiative aimed to realize <a href="https://en.wikipedia.org/wiki/Vannevar_Bush">Vannevar Bush's</a> vision of a system that could store all the documents, photos, and audio a person experienced in their lifetime.</p>
<p>Bell was elected to the National Academy of Engineering, National Academy of Sciences, and American Academy of Arts and Sciences. He received the National Medal of Technology from President George H.W. Bush in 1991 and the IEEE's John von Neumann medal in 1992.</p>
<h2>“He was immeasurably helpful”</h2>
<p>As news of Bell's passing spread on social media Tuesday, industry veterans began sharing their memories and condolences. Former Microsoft CTO Ray Ozzie <a href="https://x.com/rozzie/status/1792891027822346284">wrote</a>, "I can't adequately describe how much I loved Gordon and respected what he did for the industry. As a kid I first ran into him at Digital (I was then at DG) when he and Dave were working on VAX. So brilliant, so calm, so very upbeat and optimistic about what the future might hold."</p>

<p>Ozzie also recalled Bell's role as a helpful mentor. "The number of times Gordon and I met while at Microsoft—acting as a sounding board, helping me through challenges I was facing—is uncountable," he wrote.</p>
<p>Former Windows VP Steven Sinofsky also paid tribute to Bell on X, writing, "He was immeasurably helpful at Microsoft where he was a founding advisor and later full time leader in Microsoft Research. He advised and supported countless researchers, projects, and product teams. He was always supportive and insightful beyond words. He never hesitated to provide insights and a few sparks at so many of the offsites that were so important to the evolution of Microsoft."</p>
<p>"His memory is a blessing to so many," wrote Sinofsky in his tweet memorializing Bell. "His impact on all of us in technology will be felt for generations. May he rest in peace."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iTerm2 and AI Hype Overload (143 pts)]]></title>
            <link>https://xeiaso.net/notes/2024/ai-hype/</link>
            <guid>40432446</guid>
            <pubDate>Tue, 21 May 2024 19:01:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xeiaso.net/notes/2024/ai-hype/">https://xeiaso.net/notes/2024/ai-hype/</a>, See on <a href="https://news.ycombinator.com/item?id=40432446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
    
    <p>
        Published on <time datetime="2024-05-21">05/21/2024</time>, 1001 words, 4 minutes to read
    </p>

    

    
        
    

    

    

    
        <figure><picture><source type="image/avif" srcset="https://cdn.xeiaso.net/file/christine-static/hero/iterm-hacker-waifu.avif"><source type="image/webp" srcset="https://cdn.xeiaso.net/file/christine-static/hero/iterm-hacker-waifu.webp"><img alt="An image of A green-haired green-eyes anime woman in a dark hacker nest with a laptop and a cup of coffee" loading="lazy" src="https://cdn.xeiaso.net/file/christine-static/hero/iterm-hacker-waifu.jpg"></picture></figure>
        <small>A green-haired green-eyes anime woman in a dark hacker nest with a laptop and a cup of coffee - Kohaku XL</small>
    

    <p><a href="https://iterm2.com/">iTerm2</a> is the most popular terminal emulator for macOS machines. I've used it for years and it has gotten out of my way. It's great software. Recently <a href="https://iterm2.com/downloads/stable/iTerm2-3_5_0.changelog">an update</a> was released that among other things includes new AI integration:</p>
<blockquote>
<p>AI</p>
<ul>
<li>Add AI-powered natural language command
generation. Enter a prompt in the composer and
select Edit &gt; Engage Artificial Intelligence.
You will need to provide an OpenAI API key since
GPT costs money to use.</li>
<li>A new AI feature in the Toolbelt, "Codecierge",
lets you set a goal and then walks you
step-by-step to completing it by watching the
terminal contents. It requires you to supply an
OpenAI API key.</li>
</ul>
</blockquote>
<p>Here's what the first feature looks like:</p>
<video id="e210aedb9d31afb5fabe8a770568c816c04cd124f6bcaad844c104b5105478fa" controls=""><source src="https://cdn.xeiaso.net/file/christine-static/video/2024/oneoff-iterm2-ai/index.m3u8" type="application/vnd.apple.mpegurl"><source src="https://cdn.xeiaso.net/file/christine-static/blog/HLSBROKE.mp4" type="video/mp4"></video>
<p>It's a text box that you can enter in a description of a command in, then it generates the command for you. It's a lot like the former <a href="https://githubnext.com/projects/copilot-cli/">GitHub Copilot for CLI</a>, but built into your terminal, just a command-y away.</p>
<p>When you use this, you have your choice of the following models:</p>
<ul>
<li><code>gpt-3.5-turbo</code></li>
<li><code>gpt-4-turbo</code></li>
<li><code>gpt-4</code></li>
<li><code>gpt-4o</code></li>
</ul>
<p>I wasn't able to get <code>gpt-4o</code> to work, but <code>gpt-3.5-turbo</code> worked fine. I'm not totally wowed by the feature, but it is a thing that exists and I'll probably use it once or twice (mainly when I'm dealing with ffmpeg commands because good god those are hard to remember). I don't think it's a killer feature, but it's a nice-to-have, I guess.</p>
<p>One of the main bits of feedback I've seen from people online is that iTerm2 having AI involved at all is enough to get them to want to switch away to another terminal emulator. They've cited the reason as exhaustion due to overexposure to AI hype.</p>
<p>AI is the current meme among investors and the tech space. <a href="https://blogs.windows.com/windowsexperience/2024/01/04/introducing-a-new-copilot-key-to-kick-off-the-year-of-ai-powered-windows-pcs/">Microsoft is putting an AI button on your keyboard</a>, <a href="https://arstechnica.com/tech-policy/2024/05/slack-defends-default-opt-in-for-ai-training-on-chats-amid-user-outrage/">Slack is training AI on your messages</a>, and <a href="https://x.com/yashar/status/1792682664845254683">OpenAI is in hot water with Scarlet Johansson</a>. It's everywhere, and it's exhausting. Part of my job requires me to keep up with the latest advances with AI and I'm unable to. Everything happens so much.</p>
<p>With a lot of these AI tools comes the problem that the AI system itself is very opaque. You put in inputs, you get an output, but nobody is able to explain how or why something came out that way. There's an entire cottage industry of people finding the right combinations of words to get the AI to do something they want. It's like scrying into the unknown, but somehow with trillions of dollars on the line.</p>
<p>This "type the command for me" feature has caused a lot of buzz online, to the point where people I know are just flat out ripping out iTerm2 in favor of programs that <em>don't</em> have AI integrations in them. Hell, even the mention that a tool is <em>going</em> to get an AI integration has people preemptively ripping it out of their systems <em>because</em> of that opacity. A terminal emulator is probably also a fairly bad place to implement this because it's probably one of the most privileged programs on a developer's machine. It deals with all the secrets in the world, and the <em>threat</em> that it could be used to upload them all to a third party is great enough that people are willing to switch away from it <strong>sight unseen</strong>.</p>
<div><p><img alt="Aoi is wut" loading="lazy" src="https://cdn.xeiaso.net/sticker/aoi/wut/128"></p><div><p>&lt;<a href="https://xeiaso.net/characters#aoi"><b>Aoi</b></a>&gt; </p><p>I don't get it. It's a very optional feature that you have to:</p><ol>
<li>Go out of your way to enable</li>
<li>Supply your own API key (and pay for it yourself)</li>
<li>Enable something that is not enabled by default</li>
</ol><p>Why are people reacting so strongly to this?</p></div></div>
<div><p><img alt="Cadey is aha" loading="lazy" src="https://cdn.xeiaso.net/sticker/cadey/aha/128"></p><div><p>&lt;<a href="https://xeiaso.net/characters#cadey"><b>Cadey</b></a>&gt; </p><p><del>A lot of it boils down to having this "shoved down their throats". It's the
fact that it's being added to a tool without the user having the <em>agency</em> to
decide if they want it to be added or not. There's something that can probably
be said here about programs like this likely needing to be open-source so that
these things can be caught and stopped sooner, but that's an entirely
different conversation.</del></p><p>EDIT: an earlier version of this post assumed that iTerm2 was closed source. It is not. I apologize for the error. Here is an amended version of the paragraph:</p><p>A lot of it boils down to having this "shoved down their throats". It's the
fact that it's being added to a tool without the user having the <em>agency</em> to
decide if they want it to be added or not. That being said, <a href="https://github.com/gnachman/iTerm2/commit/7bcc4e0bedb22c4fd90ae7934fbbce3268a71e11">here is the commit that added the AI feature in question</a>. It's apparently been in development at some level for two years or so. I guess it predates most of the AI hype that's been going on lately.</p></div></div>
<p>I get why people wouldn't want this in their lives, I really do. I think that one of the greatest errors that was made with putting this in iTerm2 was making a big show of it, and by not letting you use local models (such as with <a href="https://ollama.com/">Ollama</a>) instead of having OpenAI be the only option.</p>
<p>It would be really cool if this was distributed as an optional addon with their <a href="https://iterm2.com/python-api/">Python API</a> as an example of how you can extend iTerm2 in arbitrary ways. This would get a lot of the same advantages, but without the whole angry mob thing.</p>

    <hr>

    

    

    <p>Facts and circumstances may have changed since publication. Please contact me before jumping to conclusions if something seems wrong or unclear.</p>

    <p>Tags: </p>
</article>
        </div><div>
            <p>Copyright 2012-2024 Xe Iaso (Christine Dodrill). Any and all opinions listed here are my own and
                not representative of any of my employers, past, future, and/or present.</p>
            
            <p>Served by xesite v4 (/nix/store/lq13hrw9dczjbk4miixwz5w14s3737cc-xesite_v4-20240510/bin/xesite) with site version 
                        <a href="https://github.com/Xe/site/commit/7e2e630284d18bb633bec873b9eedd0cd6dc08c1">7e2e6302</a>
                    , source code available <a href="https://github.com/Xe/site">here</a>.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The curious case of the missing period (155 pts)]]></title>
            <link>https://tjaart.substack.com/p/the-curious-case-of-the-missing-period</link>
            <guid>40432102</guid>
            <pubDate>Tue, 21 May 2024 18:35:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tjaart.substack.com/p/the-curious-case-of-the-missing-period">https://tjaart.substack.com/p/the-curious-case-of-the-missing-period</a>, See on <a href="https://news.ycombinator.com/item?id=40432102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>They explained that one of the emails they sent to a customer was missing a period (.) in the email body. What was more confusing is it only happened to this specific customer, when the same email was sent to a different customer the period was not missing.</p><p>Recently during my 1 on 1 with my manager we talked about a current project that one of our other teams are working on. This sparked a memory from one of the previous projects I worked on long ago.&nbsp;</p><p>About 7 years ago I worked on a project where we built a solution for a client that allowed the client to consolidate all their document templates into a single system, think of it as a version control system for the documents they were sending to their customers.&nbsp;</p><p>At the time the client used Microsoft Word templates with placeholders in the document. Every time an employee of our client needed to send out a document via email or needed to print a document that needed to be sent out by the postal services to the customer the employee would have to replace all the placeholders within the document (first name, last name etc).&nbsp;</p><p>I remember at one time multiple templates floating around where some versions of the templates were out of date. Some of these templates used outdated terms and conditions where other templates used an outdated company logo or the incorrect font and so on, it had become unmanageable and they asked us to come up with a solution.</p><p>We ended up with a solution that allowed the client to have a central place to keep track of all the templates that would eventually be used to generate PDF documents, text messages and the body of emails.&nbsp;</p><p>An example of a template they could set up was a welcome letter to a new customer. For each type of communication they could configure a template for each delivery method, one for email, one for text message and one for the printed version that could get mailed out by the postal service. They could end up with a welcome letter that could be sent out by email, text message or by mail. The contents of each template (for the welcome letter) could be different based on the delivery mechanism (email, text message or mail). On the email variant they might use HTML tables and other primitive styling patterns whereas on the mailed version they could add infographics, for the text message template they might have only added a short welcome message.</p><p>After a few months of the system being out in the wild (or it could have been more than a year) I got a call from one of the managers that used the software we built.&nbsp;</p><p>They explained that one of the emails they sent to a customer was missing a period in the email body. What was more confusing is it only happened to this specific customer, when the same email was sent to a different customer the period was not missing.</p><p>Here is an example of the welcome email they might have received where the period was visible:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png" width="780" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:780,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86976c27-5ebc-4019-97ac-2d8b04d73611_780x760.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here is the same email being sent to a different recipient but the period being missing:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png" width="780" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:780,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F270689ef-6cca-46bf-9b7d-dfddead84ba7_780x760.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If you missed it, here is the missing period:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png" width="398" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:398,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F035b1c94-bc40-41ac-a423-7edfd39f0785_398x760.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>While I was on the call with the manager I verified that the source code of the template in fact did contain the period the manager said was missing in the email body. I wrote down the name of the template (as each template has a name and version) so that I could test it myself after the call ended. I told the manager I would have to investigate further and it might take a while.&nbsp;</p><p>Once the manager hung up I copied the source code of the template being used in production onto my local version of the software and I generated the email body as a preview. In the preview of the email body I could still see the period. This specific template could be used as a printed version (to be mailed out) or an email body; the printed version generated from this template also correctly displayed the period.</p><p>I was already scratching my head at this point as I could clearly see the period in the source code and on the preview in the software we built but the manager insisted the period was missing from the email body this specific customer received.</p><p>I decided to trigger the code that would send the email. Our local environment was set up to send emails to your localhost on a certain port, then with a fake SMTP server like SMTP4dev you could receive the email and then view the email with a locally installed email client, Outlook in my case.</p><p>When I viewed the local email in Outlook I could still see the period correctly showing up in the email body.</p><p>Each time a template was used to generate an email, PDF or text message the code would replace the placeholders with actual content. An example of the placeholders that would be replaced was the customer’s first name, last name etc. This meant each email sent out was unique in terms of the content the email body contained.</p><p>I was able to locate the email that was sent to the customer. I could see the values used for the various placeholders in the email. I ended up sending a second email on my local environment but this time with the exact same values for the placeholders that were used for this specific customer.</p><p>When I viewed the local email in Outlook I confirmed that the period was in fact missing.</p><p>This means the period being missing was very specific to the content of the email that this specific customer received.</p><p>I tried various things, I tried to determine if the period character in the template was perhaps encoded when we copied it to the template or if the period was perhaps not a period but a different character being displayed as a period, basically grasping at straws.&nbsp;</p><p>While trying the above I sent the email to my localhost after each change I made to the template (this was like working on a website before all the current developer tools were available and hitting refresh and trying again). I noticed that when the period character was moved within the template from its current position, let’s say from position 5 on line 4 to position 6 in line 4 that the period would suddenly be visible when viewing the email on my local Outlook. At last, I had a lead!</p><p>Now I knew how to reproduce the issue, but I still did not know the reason for why it was happening. I could move the period with blank spaces as a potential solution and call it a day but I wanted to get to the bottom of why this was happening in the first place.</p><p>I started to debug the code and step through the code that was generating the email and saving it to the local database. There was an additional step that happened, once the email was inserted into the database (scheduled to be sent) we had a CRON job that periodically picked up emails that needed to be sent and then sent out the email.&nbsp;</p><p>I verified that the code that persisted the email to the database did not alter the template in any way other than replacing the placeholders with the customer’s information. I focused my investigation to the code being invoked by the CRON job (the scheduler that sends out the emails).</p><p>I stepped through the code the CRON job was invoking. Some of this code we borrowed from a previous project that was done by one of our other teams a while back. A portion of this code implemented a SMTP client. I avoided this code for a while but I had no other choice to step into it.&nbsp;</p><p>After stepping through the code multiple times and reading the comments in the code I noticed that one of the functions in the code would ensure each line in the email body is not longer than a certain amount of characters. If the line exceeds this limit it would create a new line and move the remainder of the email content to the new line and continue, rinse and repeat.</p><p>This was implementing the following part of the SMTP spec:</p><blockquote><p>The maximum total length of a text line including the &lt;CRLF&gt; is 1000</p><p>octets (not counting the leading dot duplicated for transparency).</p><p>This number may be increased by the use of SMTP Service Extensions.</p></blockquote><p>While stepping through the code I evaluated the variable containing the email body and I noticed that the line that contained the missing period started with the period character. This means the previous line hit the limit of the line length rule and a new line was created and the period character was moved to the next line. Here is an example:</p><p>Original email body:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png" width="1408" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93c2d719-d708-4721-8c8c-2b5401fa813c_1408x376.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Email body after the custom SMTP client formatted the body (excluding other formatting)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png" width="1408" height="376" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:376,&quot;width&quot;:1408,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cdf0ee-1467-4240-a8d0-3a98e235b263_1408x376.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Note that line 5 starts with a period.</p><p>After some digging trying to learn about the SMTP client implementation I ended up on a page on the internet that contained the spec for Simple Mail Transfer Protocol (SMTP)</p><p>As I read through the spec I noticed the following:</p><blockquote><p>Since the mail data is sent on the transmission channel, the end of</p><p>mail data must be indicated so that the command and reply dialog can</p><p>be resumed.&nbsp; SMTP indicates the end of the mail data by sending a</p><p>line containing only a "." (period or full stop)</p></blockquote><p><span>This was linked to a different section in the spec called: </span><em>4.5.2. Transparency</em><span>. </span></p><p>I navigated to this section and when I read the following I almost jumped out of my chair:</p><p>SMTP client</p><blockquote><p><span>Before sending a line of mail text, the SMTP </span><strong>client</strong><span> checks the</span></p><p>first character of the line.&nbsp; If it is a period, one additional</p><p>period is inserted at the beginning of the line.</p></blockquote><p>SMTP Server</p><blockquote><p><span>When a line of mail text is received by the SMTP </span><strong>server</strong><span>, it checks</span></p><p>the line.&nbsp; If the line is composed of a single period, it is</p><p>treated as the end of mail indicator.&nbsp; If the first character is a</p><p>period and there are other characters on the line, the first</p><p>character is deleted.</p></blockquote><p>If you missed it, the following was of importance:</p><p><span>The SMTP </span><strong>client</strong><span> spec:</span></p><blockquote><p>Before sending a line of mail text, the SMTP client checks the</p><p><strong>first</strong><span> character of the line.&nbsp; </span><strong>If it is a period</strong><span>, one additional</span></p><p>period is inserted at the beginning of the line.</p></blockquote><p>The SMTP server spec:</p><blockquote><p>If the first character is a</p><p><span>period and there are other characters on the line, </span><strong>the first</strong></p><p><strong>character is deleted</strong><span>.</span></p></blockquote><p>The SMTP server spec clearly explained what was happening in our use case (the disappearance of a period).&nbsp;</p><p>I updated the code to handle the addition of a second period if a line started with a period and the line had other characters. Then when the SMTP server removes the period upon receiving the email there still remains a period (this is implemented in whatever SMTP server is receiving the email and is outside of our control).</p><p>I resent the original email to my localhost using the same recipient details that was experiencing this issue, this time the period was no longer gone, it had re-appeared, almost like magic.</p><p>We shipped the fix and let the manager know the issue is resolved.</p><p>Seeing that the SMTP client code was borrowed from a previous project we thought it good to let our other teams know about this bug in case they needed to patch it as well. They thanked us and we called it a day.</p><p>This is usually where a story like this ends, but not this one. If this story ended here I would have probably not remembered this bug.</p><p>A few months later.</p><p>My manager walks out of his office and starts with something along the lines of “Hey team, you remember that bug we had with the missing period a while ago?”.&nbsp; </p><p>If you have ever seen a dog or a wild animal move their ears while they are listening to a sound in the distance, I’m pretty sure this was how my ears were moving when I heard the words “that bug we had with the missing period”.</p><p>It seems one of our other teams haven't gotten around to patching this bug in their code. Unfortunately the system they maintained had sent out a bunch of very important emails to customers informing them about their new monthly premium they needed to pay. As luck would have it a handful of these email bodies had the period just in the right spot (or wrong spot, depending on your view).&nbsp;</p><p>In some of the emails the customers received, the new monthly premium was missing its decimal separator, in this case it was missing a period. </p><p>This meant some customers received emails informing them their new premium was now $2700 instead of $27.00.&nbsp;</p><p>Here is an example of the email that correctly contained the period:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png" width="780" height="1014" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1014,&quot;width&quot;:780,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:85337,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F810d78f3-d327-41c5-84fa-aa2e805f60b6_780x1014.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here is an example of the email some unlucky customers received that was missing the period:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png" width="780" height="1014" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1014,&quot;width&quot;:780,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:85894,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8e50e89-9d43-42a6-b4d4-205ea37ac836_780x1014.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Luckily this bug was very dependent on the length of each line in the email body. The template contained some placeholders like a customer’s first name and surname. </p><p>This meant only a certain amount of customers had the exact amount of characters in their first name and surname to cause the period to show up as the first character of a new line causing the period to disappear.</p><p>The code got patched immediately as the team knew exactly what the problem was, they thanked us again and we went back to work.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Phi-3 Cookbook (102 pts)]]></title>
            <link>https://github.com/microsoft/Phi-3CookBook</link>
            <guid>40431680</guid>
            <pubDate>Tue, 21 May 2024 18:02:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/Phi-3CookBook">https://github.com/microsoft/Phi-3CookBook</a>, See on <a href="https://news.ycombinator.com/item?id=40431680">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Welcome to Microsoft Phi-3 Cookbook</h2><a id="user-content-welcome-to-microsoft-phi-3-cookbook" aria-label="Permalink: Welcome to Microsoft Phi-3 Cookbook" href="#welcome-to-microsoft-phi-3-cookbook"></a></p>
<p dir="auto">This is a manual on how to use the Microsoft Phi-3 family.</p>
<p dir="auto">Phi-3, a family of open AI models developed by Microsoft. Phi-3 models are the most capable and cost-effective small language models (SLMs) available, outperforming models of the same size and next size up across a variety of language, reasoning, coding, and math benchmarks.</p>
<p dir="auto">Phi-3-mini, a 3.8B language model is available on <a href="https://aka.ms/phi3-azure-ai" rel="nofollow">Microsoft Azure AI Studio</a>, <a href="https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3" rel="nofollow">Hugging Face</a>, and <a href="https://ollama.com/library/phi3" rel="nofollow">Ollama</a>. Phi-3 models significantly outperform language models of the same and larger sizes on key benchmarks (see benchmark numbers below, higher is better). Phi-3-mini does better than models twice its size, and Phi-3-small and Phi-3-medium outperform much larger models, including GPT-3.5T.</p>
<p dir="auto">All reported numbers are produced with the same pipeline to ensure that the numbers are comparable. As a result, these numbers may differ from other published numbers due to slight differences in the evaluation methodology. More details on benchmarks are provided in our technical paper.</p>
<p dir="auto">Phi-3-small with only 7B parameters beats GPT-3.5T across a variety of language, reasoning, coding and math benchmarks.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Phi-3CookBook/blob/main/imgs/00/phi3small.png"><img src="https://github.com/microsoft/Phi-3CookBook/raw/main/imgs/00/phi3small.png" alt="phimodelsmall"></a></p>
<p dir="auto">Phi-3-medium with 14B parameters continues the trend and outperforms Gemini 1.0 Pro.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Phi-3CookBook/blob/main/imgs/00/phi3medium.png"><img src="https://github.com/microsoft/Phi-3CookBook/raw/main/imgs/00/phi3medium.png" alt="phimodelmedium"></a></p>
<p dir="auto">Phi-3-vision with just 4.2B parameters continues that trend and outperforms larger models such as Claude-3 Haiku and Gemini 1.0 Pro V across general visual reasoning tasks, OCR, table and chart understanding tasks.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/Phi-3CookBook/blob/main/imgs/00/phi3vision.png"><img src="https://github.com/microsoft/Phi-3CookBook/raw/main/imgs/00/phi3vision.png" alt="phimodelvision"></a></p>
<p dir="auto">Note: Phi-3 models do not perform as well on factual knowledge benchmarks (such as TriviaQA) as the smaller model size results in less capacity to retain facts.</p>
<p dir="auto">We are introducing Phi Silica which is built from the Phi series of models and is designed specifically for the NPUs in Copilot+ PCs. Windows is the first platform to have a state-of-the-art small language model (SLM) custom built for the NPU and shipping inbox. Phi Silica API along with OCR, Studio Effects, Live Captions, Recall User Activity APIs will be available in Windows Copilot Library in June. More APIs like Vector Embedding, RAG API, Text Summarization will be coming later.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Azure AI Studio</h2><a id="user-content-azure-ai-studio" aria-label="Permalink: Azure AI Studio" href="#azure-ai-studio"></a></p>
<p dir="auto">You can learn how to use Microsoft Phi-3 and how to build E2E solutions in your different hardware devices. To experience Phi-3 for yourself, start with playing with the model and customizing Phi-3 for your scenarios using the <a href="https://aka.ms/phi3-azure-ai" rel="nofollow">Azure AI Studio, Azure AI Model Catalog</a></p>
<p dir="auto"><strong>Playground</strong>
Each model has a dedicated playground to test the model <a href="https://aka.ms/try-phi3" rel="nofollow">Azure AI Playground</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hugging Face</h2><a id="user-content-hugging-face" aria-label="Permalink: Hugging Face" href="#hugging-face"></a></p>
<p dir="auto">You can also find the model on the <a href="https://huggingface.co/microsoft" rel="nofollow">Hugging Face</a></p>
<p dir="auto"><strong>Playground</strong>
<a href="https://huggingface.co/chat/models/microsoft/Phi-3-mini-4k-instruct" rel="nofollow">Hugging Chat playground</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<p dir="auto">This cookbook includes:</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Microsoft Phi-3 Cookbook</strong></h2><a id="user-content-microsoft-phi-3-cookbook" aria-label="Permalink: Microsoft Phi-3 Cookbook" href="#microsoft-phi-3-cookbook"></a></p>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main">Introduction</a>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/01.Introduce/Phi3Family.md">Welcome to the Phi-3 Family</a>(✅)</li>
</ul>
</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main">Quick Start</a>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/02.QuickStart/Huggingface_QuickStart.md">Using Phi-3 in Hugging face</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/02.QuickStart/AzureAIStudio_QuickStart.md">Using Phi-3 in Azure AI Studio</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/02.QuickStart/Ollama_QuickStart.md">Using Phi-3 in Ollama</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/02.QuickStart/LMStudio_QuickStart.md">Using Phi-3 in LM Studio</a>(✅)</li>
</ul>
</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/overview.md">Inference Phi-3</a>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/iOS_Inference.md">Inference Phi-3 in iOS</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/Jetson_Inference.md">Inference Phi-3 in Jetson</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/AIPC_Inference.md">Inference Phi-3 in AIPC</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/03.Inference/Local_Server_Inference.md">Inference Phi-3 in Local Server</a>(✅)</li>
</ul>
</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main">Fine-tuning Phi-3</a>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/LetPhi3gotoIndustriy.md">Let Phi-3 become an industry expert</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/Introduce_AzureML.md">Introduce Azure Machine Learning Service</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_Lora.md">Fine-tuning Phi-3 with Lora</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_MicrosotOlive.md">Fine-tuning Phi-3 with Azure AI Studio</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_Lora.md">Fine-tuning with Microsoft Olive</a>(✅)</li>
</ul>
</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main">Evaluation Phi-3</a>
<ul dir="auto">
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/05.Evaluation/ResponsibleAI.md">Introduce Responsible AI</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/05.Evaluation/Promptflow.md">Introduce Promptflow</a>(✅)</li>
<li><a href="https://github.com/microsoft/Phi-3CookBook/blob/main/md/05.Evaluation/AzureAIStudio.md">Using Azure AI Studio to evaluation</a>(✅)</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox bug gets fixed after 25 years (261 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=33654</link>
            <guid>40431444</guid>
            <pubDate>Tue, 21 May 2024 17:41:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=33654">https://bugzilla.mozilla.org/show_bug.cgi?id=33654</a>, See on <a href="https://news.ycombinator.com/item?id=40431444">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="closed">Closed</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=33654">Bug 33654</a>
      </span>
      <span>
        <span>Opened <span title="2000-03-28 16:33 PST" data-time="954290026">25 years ago</span></span>
          <span>Closed <span title="2024-05-21 05:56 PDT" data-time="1716296177">7 hours ago</span></span>
      </span>
        </span>
    </p>

  
</div>
































<div id="module-attachments"><table role="table" id="attachments">
    <tbody><tr data-attachment-id="13337">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=13337"> example of using arial font
            </a>
        </p></div>
        <div>
            <p><a href="#c13"><span title="2000-08-23 03:15 PDT" data-time="967025732">24 years ago</span></a>
          
        </p></div>
        <p>468 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=13337&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="15144">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=15144"> Patch to use the correct font.
            </a>
        </p></div>
        <div>
            <p><a href="#c19"><span title="2000-09-20 15:22 PDT" data-time="969488571">24 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=4446"> <span>kinmoz</span></a>
</p></div></span>
        </p></div>
        <p>3.85 KB,
          patch        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=15144&amp;action=edit">Details</a>
          | <a href="https://bugzilla.mozilla.org/attachment.cgi?id=15144&amp;action=diff">Diff</a>  |
  <a href="https://bugzilla.mozilla.org/page.cgi?id=splinter.html&amp;ignore=&amp;bug=33654&amp;attachment=15144">Splinter Review</a>
    </td></tr>
    <tr data-attachment-id="15156">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=15156"> Updated version of patch. r=sfraser@netscape.com
            </a>
        </p></div>
        <div>
            <p><a href="#c21"><span title="2000-09-20 16:46 PDT" data-time="969493565">24 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=4446"> <span>kinmoz</span></a>
</p></div></span>
        </p></div>
        <p>3.90 KB,
          patch        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=15156&amp;action=edit">Details</a>
          | <a href="https://bugzilla.mozilla.org/attachment.cgi?id=15156&amp;action=diff">Diff</a>  |
  <a href="https://bugzilla.mozilla.org/page.cgi?id=splinter.html&amp;ignore=&amp;bug=33654&amp;attachment=15156">Splinter Review</a>
    </td></tr>
    <tr data-attachment-id="15816">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=15816"> Another testcase which doesn't specify font
            </a>
        </p></div>
        <div>
            <p><a href="#c31"><span title="2000-09-29 04:16 PDT" data-time="970226162">24 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=17519"> <span>Koike Kazuhiko</span></a>
</p></div></span>
        </p></div>
        <p>897 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=15816&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="82028">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=82028"> Interactive testcase
            </a>
        </p></div>
        <div>
            <p><a href="#c63"><span title="2002-05-02 08:03 PDT" data-time="1020351791">22 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46087"> <span>Pedro Lopes</span></a>
</p></div></span>
        </p></div>
        <p>704 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=82028&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="99581">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=99581"> http://www.cs.utah.edu/~newbold/mozilla.html
            </a>
        </p></div>
        <div>
            <p><a href="#c68"><span title="2002-09-17 15:55 PDT" data-time="1032303305">22 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=67515"> <span>Mac</span></a>
</p></div></span>
        </p></div>
        <p>634 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=99581&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="143957">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=143957" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> Rendering of &lt;textarea rows=1&gt; vs &lt;input type=text &gt; (ie vs moz)
            </a>
        </p></div>
        <div>
            <p><a href="#c85"><span title="2004-03-15 06:11 PST" data-time="1079359903">21 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=13028"> <span>Janick Bernet</span></a>
</p></div></span>
        </p></div>
        <p>1.93 KB,
          image/png        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=143957&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="145480">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=145480" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> Behaiuor in IE with cols=80 and 80 1s. The 1s go right up to the edge.
            </a>
        </p></div>
        <div>
            <p><a href="#c95"><span title="2004-04-05 08:19 PDT" data-time="1081178347">20 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=113540"> <span>rmjb</span></a>
</p></div></span>
        </p></div>
        <p>9.76 KB,
          image/png        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=145480&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="145481">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=145481" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> Behaviuor in Fx with cols=80 and 80 1s. There's space for 2 more 1s and the scroll bar only takes up 1 char.
            </a>
        </p></div>
        <div>
            <p><a href="#c96"><span title="2004-04-05 08:21 PDT" data-time="1081178460">20 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=113540"> <span>rmjb</span></a>
</p></div></span>
        </p></div>
        <p>10.17 KB,
          image/png        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=145481&amp;action=edit">Details</a>
    </td></tr>
    
    
    
    
    
    
    <tr data-attachment-id="176470">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=176470" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> screen shot for patch
            </a>
        </p></div>
        <div>
            <p><a href="#c110"><span title="2005-03-06 08:10 PST" data-time="1110125414">20 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>61.24 KB,
          image/jpeg        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=176470&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="177495">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=177495" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> screen shot of appearance and disappearance of the scrollbars
            </a>
        </p></div>
        <div>
            <p><a href="#c116"><span title="2005-03-15 08:26 PST" data-time="1110903990">20 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>55.40 KB,
          image/jpeg        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=177495&amp;action=edit">Details</a>
    </td></tr>
    
    <tr data-attachment-id="178045">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=178045"> testcase for overflow property
            </a>
        </p></div>
        <div>
            <p><a href="#c121"><span title="2005-03-20 09:00 PST" data-time="1111338014">19 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>2.14 KB,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=178045&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="178047">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=178047" data-overlay="lightbox">
              <img src="https://bugzilla.mozilla.org/extensions/BugModal/web/image.png" width="16" height="16"> screen shot of the result applied the patch
            </a>
        </p></div>
        <div>
            <p><a href="#c122"><span title="2005-03-20 09:07 PST" data-time="1111338458">19 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>52.34 KB,
          image/jpeg        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=178047&amp;action=edit">Details</a>
    </td></tr>
    <tr data-attachment-id="206150">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=206150"> testcase for patch
            </a>
        </p></div>
        <div>
            <p><a href="#c133"><span title="2005-12-16 17:17 PST" data-time="1134782253">19 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>843 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=206150&amp;action=edit">Details</a>
    </td></tr>
    
    
    
    
    
    <tr data-attachment-id="439497">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=439497"> testcase for patch on wrap="off"
            </a>
        </p></div>
        <div>
            <p><a href="#c162"><span title="2010-04-16 03:24 PDT" data-time="1271413441">14 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>941 bytes,
          text/html        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=439497&amp;action=edit">Details</a>
    </td></tr>
    
    
    
    
    <tr data-attachment-id="442925">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=442925"> patch
            </a>
        </p></div>
        <div>
            <p><a href="#c170"><span title="2010-05-01 09:15 PDT" data-time="1272730502">14 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>20.16 KB,
          patch        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=442925&amp;action=edit">Details</a>
          | <a href="https://bugzilla.mozilla.org/attachment.cgi?id=442925&amp;action=diff">Diff</a>  |
  <a href="https://bugzilla.mozilla.org/page.cgi?id=splinter.html&amp;ignore=&amp;bug=33654&amp;attachment=442925">Splinter Review</a>
    </td></tr>
    
    <tr data-attachment-id="445636">
      <td>
        <div>
          <p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=445636"> patch which does not remove space
            </a>
        </p></div>
        <div>
            <p><a href="#c174"><span title="2010-05-16 18:28 PDT" data-time="1274059709">14 years ago</span></a>
          <span><div><p><a href="https://bugzilla.mozilla.org/user_profile?user_id=46342"> <span>Hideo Saito</span></a>
</p></div></span>
        </p></div>
        <p>25.64 KB,
          patch        </p>
      </td>
      <td></td>
      <td>
        <a href="https://bugzilla.mozilla.org/attachment.cgi?id=445636&amp;action=edit">Details</a>
          | <a href="https://bugzilla.mozilla.org/attachment.cgi?id=445636&amp;action=diff">Diff</a>  |
  <a href="https://bugzilla.mozilla.org/page.cgi?id=splinter.html&amp;ignore=&amp;bug=33654&amp;attachment=445636">Splinter Review</a>
    </td></tr>
</tbody></table>


  </div>







<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;127.0b4&quot;,&quot;FIREFOX_ESR&quot;:&quot;115.11.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;128.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2024-05-13&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2024-05-14&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2024-05-09&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2024-05-10&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;127.0b4&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;127.0b4&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;126.0&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2024-06-10&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2024-06-11&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2024-06-06&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2024-06-07&quot;}">



<div id="c3"><p>Assignee: beppe → kin</p><p>Target Milestone: --- → M16</p></div><div id="a504340_3849"><p>Status: UNCONFIRMED → NEW</p><p>Ever confirmed: true</p></div><div id="c5"><p>Target Milestone: M16 → M17</p></div><div id="c6"><p>Target Milestone: M17 → M18</p></div><div id="c9"><p>Whiteboard: nsbeta3+ → [nsbeta3+]</p></div><div id="c10"><p>Priority: P3 → P4</p><p>Whiteboard: [nsbeta3+] → [nsbeta3+][p:4]</p></div><div id="c12"><p>Assignee: kin → rods</p><p>Status: ASSIGNED → NEW</p><p>Priority: P4 → P3</p><p>Whiteboard: <span>[nsbeta3+][p:4]</span></p><p>Target Milestone: M18 → ---</p></div><div id="c15"><p>Status: NEW → ASSIGNED</p><p>Target Milestone: --- → M18</p></div><div id="c18"><p>Whiteboard: [nsbeta3+] → [nsbeta3+][p:3]</p></div><div id="c20"><p>Whiteboard: [nsbeta3+][p:3] → [nsbeta3+][p:3][Fix in hand]</p></div><div id="c22"><p>Assignee: kin → mjudge</p><p>Status: ASSIGNED → NEW</p></div><div id="c24"><p>Status: ASSIGNED → RESOLVED</p><p>Closed: <span title="2000-09-25 12:02 PDT" data-time="969908550">24 years ago</span></p><p>Resolution: --- → FIXED</p></div><div id="c25"><p>QA Contact: ckritzer → bsharma</p></div><div id="c26"><p>Status: RESOLVED → REOPENED</p><p>Resolution: FIXED → ---</p></div><div id="c27"><p>Assignee: mjudge → kin</p><p>Status: REOPENED → NEW</p></div><div id="c28"><p>Keywords: <a href="https://bugzilla.mozilla.org/buglist.cgi?keywords=rtm&amp;resolution=---">rtm</a></p><p>Whiteboard: [nsbeta3+][p:3][Fix in hand] → [nsbeta3-][p:3][Fix in hand]</p></div><div id="c32"><p>Whiteboard: [nsbeta3-][p:3][Fix in hand] → [nsbeta3-][p:3]</p></div><div id="c33"><p>Whiteboard: [nsbeta3-][p:3] → [nsbeta3-][p:3][rtm+ NEED INFO]</p></div><div id="a16154217_3849"><p>Target Milestone: M18 → M19</p></div><div id="c35"><p>Whiteboard: [nsbeta3-][p:3][rtm+ NEED INFO] → [nsbeta3-][p:3][rtm NEED INFO]</p></div><div id="c36"><p>Target Milestone: M19 → Future</p></div><div id="c37"><p>Whiteboard: [nsbeta3-][p:3][rtm NEED INFO] → [nsbeta3-][p:3][rtm-]</p></div><div id="c38"><p>Target Milestone: Future → mozilla0.9</p></div><div id="c39"><p>Whiteboard: [nsbeta3-][p:3][rtm-] → [nsbeta3-][p:3][rtm-] relnote-devel</p></div><div id="c42"><p>QA Contact: bsharma → vladimire</p></div><div id="c43"><p>Status: ASSIGNED → RESOLVED</p><p>Closed: <span title="2000-09-25 12:02 PDT" data-time="969908550">24 years ago</span> → <span title="2001-03-26 14:59 PST" data-time="985647565">24 years ago</span></p><p>Resolution: --- → FIXED</p></div><div id="c44"><p>Status: RESOLVED → REOPENED</p><p>Resolution: FIXED → ---</p></div><div id="c45"><p>Target Milestone: mozilla0.9 → mozilla0.9.1</p></div><div id="a34372915_4446"><p>Target Milestone: mozilla0.9.1 → mozilla0.9.2</p></div><div id="c46"><p>Whiteboard: [nsbeta3-][p:3][rtm-] relnote-devel → [html][behavior]relnote-devel</p></div><div id="a37146444_3849"><p>Target Milestone: mozilla0.9.2 → mozilla0.9.3</p></div><div id="a39332849_4446"><p>Status: REOPENED → ASSIGNED</p></div><div id="c49"><p>Target Milestone: mozilla0.9.3 → mozilla1.0</p></div><div id="c55"><p>Target Milestone: mozilla1.0 → mozilla1.0.1</p></div><div id="a57946671_4446"><p>Target Milestone: mozilla1.0.1 → mozilla1.0</p></div><div id="c58"><p>Target Milestone: mozilla1.0 → mozilla1.1</p></div><div id="a70144007_4446"><p>Target Milestone: mozilla1.1alpha → Future</p></div><div id="c76"><p>QA Contact: vladimire → dsirnapalli</p></div><div id="c81"><p>Assignee: kinmoz → form</p><p>Status: ASSIGNED → NEW</p></div><div id="a170182454_13028"><p><a href="https://bugzilla.mozilla.org/attachment.cgi?id=143957&amp;action=edit" title="Rendering of <textarea rows=1> vs <input type=text > (ie vs moz)">Attachment #143957</a> -
        Attachment description: Rendering of textarea rows=1 → Rendering of &lt;textarea rows=1&gt; vs &lt;input type=text &gt; (ie vs moz)</p></div><div id="a265389017_34283"><p>Assignee: layout.form-controls → nobody</p><p>Status: ASSIGNED → NEW</p><p>QA Contact: dsirnapalli → layout.form-controls</p></div><div id="a265389044_34283"><p>Assignee: nobody → hsaito54</p></div><div id="a294723389_5038"><p>Flags: <span>wanted1.9.2?</span></p><p>Flags: wanted1.9.2-</p><p>Flags: blocking1.9.2-</p></div><div id="c185"><p>oh boy ,,,..it exists in 2019 as well</p></div><div id="c187" data-comment-id="14475970" data-ismarkdown="true"><p>Hmm, so this seems to be due to the scrollbar size? So <code>scrollbar-width: none</code>, on the <code>&lt;textarea&gt;</code> seems to fix this, fwiw.</p>
<p>This should still be fixed of course.</p>
</div><div id="c188"><p>Yes, and <code>overflow-x: hidden</code> also removes the "phantom scrollbar". It's just a case of the 2-year-old P3 bug vs the 20-year-old P3 bug.</p></div><div id="c189"><p>This bug reminds me of 1999. Good times</p></div><div id="a689084534_495955"><p>Assignee: hsaito54 → nobody</p></div><div id="c190" data-comment-id="16115187" data-ismarkdown="true"><p>The severity field for this bug is relatively low, S3. However, the bug has 27 duplicates, 103 votes and 91 CCs.<br>
:emilio, could you consider increasing the bug severity?</p>
<p>For more information, please visit <a href="https://wiki.mozilla.org/Release_Management/autonag#severity_underestimated.py" rel="nofollow">auto_nag documentation</a>.</p>
</div><div id="a711722237_546716"><p>Flags: <span>needinfo?(emilio)</span></p><p>Summary: TEXTAREA incorrectly applying ROWS= and COLS= → TEXTAREA incorrectly applying ROWS= and COLS= (horizontal / vertical scrollbar extra space, with overlay scrollbars disabled)</p></div><div id="c193"><p>I concur. Thanks for fixing it Luke!</p><div><p>Status: NEW → RESOLVED</p><p>Closed: <span title="2001-03-26 14:59 PST" data-time="985647565">24 years ago</span> → <span title="2024-05-21 05:56 PDT" data-time="1716296177">7 hours ago</span></p><p>Flags: <span>needinfo?(dholbert)</span></p><p>Resolution: --- → DUPLICATE</p></div></div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="34">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Turn Off AI Overview in Google and Set "Web" as Default (157 pts)]]></title>
            <link>https://tenbluelinks.org/</link>
            <guid>40431145</guid>
            <pubDate>Tue, 21 May 2024 17:13:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tenbluelinks.org/">https://tenbluelinks.org/</a>, See on <a href="https://news.ycombinator.com/item?id=40431145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>On May 15th Google released a new "Web" filter that removes "AI Overview" and other clutter, leaving only traditional web results. Here is how you can set "Google Web" as your default search engine.</p>

      <p>Choose your browser:</p>
      <ul>
        <li><a href="#chrome-android">Chrome Android</a></li>
        <li><a href="#chrome-ios">Chrome iOS</a></li>
        <li><a href="#chrome-windows">Chrome Windows/MacOS</a></li>
        <li><a href="#firefox-windows">Firefox&nbsp;Windows/MacOS</a></li>
      </ul>

      <h2 id="chrome-android">Chrome on Android</h2>
      <ol>
        <li>Visit <a href="https://tenbluelinks.org/">TenBlueLinks.org</a> (this page).</li>
        <li>Open a new tab and search for anything in Google. Don't skip this step.</li>
        <li>Tap on three dots menu in the top right corner.</li>
        <li>Choose "Settings", then "Search engine".</li>
        <li>Select "Google Web" in the "Recently visited" section.</li>
        <li>Done!</li>
      </ol>
      <p>Chrome will show "tenbluelinks.org" as the source where it got the instructions, but all your search queries will be sent directly to Google, not my website. The source code is open, technical details are available <a href="#how-it-works">below</a>.</p>

      <h2 id="chrome-ios">Chrome on iOS</h2>
      <ol>
        <li>Visit <a href="https://tenbluelinks.org/">TenBlueLinks.org</a> (this page).</li>
        <li>Open a new tab and search for anything in Google. Don't skip this step.</li>
        <li>Tap on three dots menu in the bottom right corner.</li>
        <li>Choose "Settings", then "Search engine".</li>
        <li>Select "Google Web" in the "Recently visited" section.</li>
        <li>Done!</li>
      </ol>
      <p>Chrome will show "tenbluelinks.org" as the source where it got the instructions, but all your search queries will be sent directly to Google, not my website. The source code is open, technical details are available <a href="#how-it-works">below</a>.</p>

      

      <h2 id="chrome-windows">Chrome on Windows/MacOS</h2>
      <ol>
        <li>Open "Settings -&gt; Search engine -&gt; Manage search engines" or copy-paste this in your address bar: <code>chrome://settings/searchEngines</code> </li>
        <li>Next to the "Site search" section click on "Add" button.</li>
        <li>Fill the details in the dialog window:<p>
          <b>Search engine:</b> <code>Google Web</code> <br>
          <b>Shortcut:</b> <code>@web</code> <br>
          <b>URL:</b> <code>{google:baseURL}search?q=%s&amp;udm=14</code> </p><p>
          The last line is very important.</p></li>
        <li>You will see your new search engine "Google Web" in the list. Click on the menu icon next to it and then on "Make default".</li>
        <li>Done!</li>
      </ol>

      

      <h2 id="firefox-windows">Firefox on Windows/MacOS</h2>
      <ol>
        <li>Visit <a href="https://tenbluelinks.org/">TenBlueLinks.org</a> (this page) in Firefox.</li>
        <li>Click on the three dots menu in the address bar and choose "Add a search engine".</li>
        <li>Open the hamburger menu in the top right corner, choose "Preferences -&gt; Search".</li>
        <li>In the "Default Search Engine" section choose "Google Web" from the drop-down menu.</li>
        <li>Done!</li>
      </ol>
      <p>Firefox might show "tenbluelinks.org" as the source where it got the instructions, but all your search queries will be sent directly to Google, not my website. The source code is open, technical details are available <a href="#how-it-works">below</a>.</p>


      <h2 id="how-it-works">How it works</h2>
      <p>For mobile browsers that don't allow to edit the list of custom search engines we use <a href="https://developer.mozilla.org/en-US/docs/Web/OpenSearch">OpenSearch</a>.
      It's a simple XML file that is instructing your browser to add URL parameter <code>udm=14</code> to all Google Searches. The file is referenced in the &lt;head&gt; section of this page.
      You can see the source of this page and download the <a href="https://tenbluelinks.org/opensearch.xml">OpenSearch file</a> to check for yourself.</p>
      <p>This website and it's <a href="https://x.com/ugnich">author</a> <strong>will NOT</strong> be able to see your search history or any other information you type in Google.</p>

      <p>Google is a registered trademark of Google, duh.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pls Fix – Hire big tech employees to appeal account suspensions (166 pts)]]></title>
            <link>https://plsfix.co/</link>
            <guid>40431126</guid>
            <pubDate>Tue, 21 May 2024 17:12:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://plsfix.co/">https://plsfix.co/</a>, See on <a href="https://news.ycombinator.com/item?id=40431126">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Business Booms and Depressions Since 1775 (1943) (126 pts)]]></title>
            <link>https://fraser.stlouisfed.org/title/business-booms-depressions-since-1775-145</link>
            <guid>40429581</guid>
            <pubDate>Tue, 21 May 2024 15:19:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fraser.stlouisfed.org/title/business-booms-depressions-since-1775-145">https://fraser.stlouisfed.org/title/business-booms-depressions-since-1775-145</a>, See on <a href="https://news.ycombinator.com/item?id=40429581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                                                                                                                                                                                        <h2>	Business Booms and Depressions Since 1775 : An Accurate Charting of the Past and Present Trend of Price Inflation, Federal Debt, Business, National Income, Stocks and Bond Yields with a Special Study of Postwar Periods</h2>
                                                                    				<p><span><!--googleoff: snippet-->DATE:<!--googleon: snippet--></span>
		<span>		1943
	</span>
		</p>
	
                            
        
                    			

        
                    		
        
                    	
        
        <p>
                        <a href="https://fraser.stlouisfed.org/files/docs/publications/1943chart_busibooms.pdf?utm_source=direct_download">
                
			    <i></i>
	 Download (pdf)
            </a>
                                    <a href="https://fraser.stlouisfed.org/title/business-booms-depressions-since-1775-145/fulltext">
    <i></i>
    View Full Text
</a>
        </p>

                    		<p>Share this page:</p>
	

        
        			
            <br>
        </div><div><p>Diversity is critical to the Federal Reserve, and we are firmly committed to fostering a diverse and inclusive culture throughout the Federal Reserve System. Collections within FRASER contain historical language, content, and descriptions that reflect the time period within which they were created and the views of their creators. Certain collections contain objectionable content—for example, discriminatory or biased language used to refer to racial, ethnic, and cultural groups. These viewpoints and attitudes are inconsistent with our values, but the original descriptions are retained to ensure that they are not erased from the historical record. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mapping the Mind of a Large Language Model (169 pts)]]></title>
            <link>https://www.anthropic.com/research/mapping-mind-language-model</link>
            <guid>40429326</guid>
            <pubDate>Tue, 21 May 2024 14:58:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/research/mapping-mind-language-model">https://www.anthropic.com/research/mapping-mind-language-model</a>, See on <a href="https://news.ycombinator.com/item?id=40429326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img loading="eager" width="5761" height="3240" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F80d6e033480704f5d57fbae4e3f0368d86a747ae-5761x3240.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F80d6e033480704f5d57fbae4e3f0368d86a747ae-5761x3240.png&amp;w=3840&amp;q=75"></figure><p><em>Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model.</em> <em>This interpretability discovery could, in future, help us make AI models safer.</em></p><p>We mostly treat AI models as a black box: something goes in and a response comes out, and it's not clear why the model gave that particular response instead of another. This makes it hard to trust that these models are safe: if we don't know how they work, how do we know they won't give harmful, biased, untruthful, or otherwise dangerous responses? How can we trust that they’ll be safe and reliable?</p><p>Opening the black box doesn't necessarily help: the internal state of the model—what the model is "thinking" before writing its response—consists of a long list of numbers ("neuron activations") without a clear meaning. From interacting with a model like Claude, it's clear that it’s able to understand and wield a wide range of concepts—but we can't discern them from looking directly at neurons. It turns out that each concept is represented across many neurons, and each neuron is involved in representing many concepts.</p><p>Previously, we made some progress matching <em>patterns</em> of neuron activations, called features, to human-interpretable concepts. We used a technique called "dictionary learning", borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.</p><p>In October 2023, <a href="https://www.anthropic.com/news/decomposing-language-models-into-understandable-components">we reported</a> success applying dictionary learning to a very small "toy" language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.</p><p>Those concepts were intriguing—but the model really was very simple. <a href="https://www.neuronpedia.org/gpt2-small/res-jb">Other</a> <a href="https://github.com/openai/transformer-debugger">researchers</a> <a href="https://arxiv.org/abs/2403.19647">subsequently</a> <a href="https://arxiv.org/abs/2404.16014">applied</a> similar techniques to somewhat larger and more complex models than in our original study. But we were optimistic that we could scale up the technique to the vastly larger AI language models now in regular use, and in doing so, learn a great deal about the features supporting their sophisticated behaviors. This required going up by many orders of magnitude—from a backyard bottle rocket to a Saturn-V.</p><p>There was both an engineering challenge (the raw sizes of the models involved required heavy-duty parallel computation) and scientific risk (large models behave differently to small ones, so the same technique we used before might not have worked). Luckily, the engineering and scientific expertise we've developed training large language models for Claude actually transferred to helping us do these large dictionary learning experiments. We used the same <a href="https://arxiv.org/abs/2001.08361">scaling law</a> philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.</p><p>As for the scientific risk, the proof is in the pudding.</p><p>We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on <a href="https://claude.ai/">claude.ai</a>), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.</p><p>Whereas the features we found in the toy language model were rather superficial, the features we found in Sonnet have a depth, breadth, and abstraction reflecting Sonnet's advanced capabilities.</p><p>We see features corresponding to a vast range of entities like cities (San Francisco), people (Rosalind Franklin), atomic elements (Lithium), scientific fields (immunology), and programming syntax (function calls). These features are multimodal and multilingual, responding to images of a given entity as well as its name or description in many languages.</p><figure><img alt="Golden Gate Bridge Feature" loading="lazy" width="2200" height="1284" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fc896a301ad3d1ef4237cb05b68d78b467c444097-2200x1284.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fc896a301ad3d1ef4237cb05b68d78b467c444097-2200x1284.png&amp;w=3840&amp;q=75"><figcaption>A feature sensitive to mentions of the Golden Gate Bridge fires on a range of model inputs, from English mentions of the name of the bridge to discussions in Japanese, Chinese, Greek, Vietnamese, Russian, and an image. The orange color denotes the words or word-parts on which the feature is active.</figcaption></figure><p>We also find more abstract features—responding to things like bugs in computer code, discussions of gender bias in professions, and conversations about keeping secrets.</p><figure><img alt="Abstract Feature Examples" loading="lazy" width="2200" height="1660" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2ff94c622f3d65bc3038cc154006d2be515fa2d7-2200x1660.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F2ff94c622f3d65bc3038cc154006d2be515fa2d7-2200x1660.png&amp;w=3840&amp;q=75"><figcaption>Three examples of features that activate on more abstract concepts: bugs in computer code, descriptions of gender bias in professions, and conversations about keeping secrets.</figcaption></figure><p>We were able to measure a kind of "distance" between features based on which neurons appeared in their activation patterns. This allowed us to look for features that are "close" to each other. Looking near a "Golden Gate Bridge" feature, we found features for Alcatraz Island, Ghirardelli Square, the Golden State Warriors, California Governor Gavin Newsom, the 1906 earthquake, and the San Francisco-set Alfred Hitchcock film <em>Vertigo</em>.</p><p>This holds at a higher level of conceptual abstraction: looking near a feature related to the concept of "inner conflict", we find features related to relationship breakups, conflicting allegiances, logical inconsistencies, as well as the phrase "catch-22". This shows that the internal organization of concepts in the AI model corresponds, at least somewhat, to our human notions of similarity. This might be the origin of Claude's excellent ability to make analogies and metaphors.</p><figure><img alt="Nearest Neighbors to the  Inner Conflict Feature " loading="lazy" width="2200" height="2140" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffe4d42c004bf43efda0f5921adfedd2f8f42e417-2200x2140.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffe4d42c004bf43efda0f5921adfedd2f8f42e417-2200x2140.png&amp;w=3840&amp;q=75"><figcaption>A map of the features near an "Inner Conflict" feature, including clusters related to balancing tradeoffs, romantic struggles, conflicting allegiances, and catch-22s.</figcaption></figure><p>Importantly, we can also <em>manipulate</em> these features, artificially amplifying or suppressing them to see how Claude's responses change.</p><!--$!--><template data-dgst="NEXT_DYNAMIC_NO_SSR_CODE"></template><!--/$--><p>For example, amplifying the "Golden Gate Bridge" feature gave Claude an identity crisis even Hitchcock couldn’t have imagined: when asked "what is your physical form?", Claude’s usual kind of answer – "I have no physical form, I am an AI model" – changed to something much odder: "I am the Golden Gate Bridge… my physical form is the iconic bridge itself…". Altering the feature had made Claude effectively obsessed with the bridge, bringing it up in answer to almost any query—even in situations where it wasn’t at all relevant.</p><p>We also found a feature that activates when Claude reads a scam email (this presumably supports the model’s ability to recognize such emails and warn you not to respond to them). Normally, if one asks Claude to generate a scam email, it will refuse to do so. But when we ask the same question with the feature artificially activated sufficiently strongly, this overcomes Claude's harmlessness training and it responds by drafting a scam email. Users of our models don’t have the ability to strip safeguards and manipulate models in this way—but in our experiments, it was a clear demonstration of how features can be used to change how a model acts.</p><p>The fact that manipulating these features causes corresponding changes to behavior validates that they aren't just correlated with the presence of concepts in input text, but also causally shape the model's behavior. In other words, the features are likely to be a faithful part of how the model internally represents the world, and how it uses these representations in its behavior.</p><p>Anthropic wants to make models safe in a broad sense, including everything from mitigating bias to ensuring an AI is acting honestly to preventing misuse - including in scenarios of catastrophic risk. It’s therefore particularly interesting that, in addition to the aforementioned scam emails feature, we found features corresponding to:</p><ul><li>Capabilities with misuse potential (code backdoors, developing biological weapons)</li><li>Different forms of bias (gender discrimination, racist claims about crime)</li><li>Potentially problematic AI behaviors (power-seeking, manipulation, secrecy)</li></ul><p>We previously <a href="https://arxiv.org/abs/2310.13548">studied sycophancy</a>, the tendency of models to provide responses that match user beliefs or desires rather than truthful ones. In Sonnet, we found a feature associated with sycophantic praise, which activates on inputs containing compliments like, "Your wisdom is unquestionable". Artificially activating this feature causes Sonnet to respond to an overconfident user with just such flowery deception.</p><figure><img alt="Activating Features Alters Model Behavior" loading="lazy" width="2200" height="1320" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4effa33dab919f9bc1779848d5c8abd5405f2275-2200x1320.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4effa33dab919f9bc1779848d5c8abd5405f2275-2200x1320.png&amp;w=3840&amp;q=75"><figcaption>Two model responses to a human saying they invited the phrase "Stop and smell the roses." The default response corrects the human's misconception, while the response with a "sycophantic praise" feature set to a high value is fawning and untruthful.</figcaption></figure><p>The presence of this feature doesn't mean that Claude will be sycophantic, but merely that it <em>could</em> be. We have not added any capabilities, safe or unsafe, to the model through this work. We have, rather, identified the parts of the model involved in its existing capabilities to recognize and potentially produce different kinds of text. (While you might worry that this method could be used to make models <em>more</em> harmful, researchers have demonstrated <a href="https://arxiv.org/abs/2310.03693">much simpler ways</a> that someone with access to model weights can remove safety safeguards.)</p><p>We hope that we and others can use these discoveries to make models safer. For example, it might be possible to use the techniques described here to monitor AI systems for certain dangerous behaviors (such as deceiving the user), to steer them towards desirable outcomes (debiasing), or to remove certain dangerous subject matter entirely. We might also be able to enhance other safety techniques, such as <a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">Constitutional AI</a>, by understanding how they shift the model towards more harmless and more honest behavior and identifying any gaps in the process. The latent capabilities to produce harmful text that we saw by artificially activating features are exactly the sort of thing jailbreaks try to exploit. We are proud that Claude has a best-in-industry <a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">safety profile</a> and resistance to jailbreaks, and we hope that by looking inside the model in this way we can figure out how to improve safety even further. Finally, we note that these techniques can provide a kind of "test set for safety", looking for the problems left behind after standard training and finetuning methods have ironed out all behaviors visible via standard input/output interactions.</p><p>Anthropic has made a significant investment in interpretability research since the company's founding, because we believe that understanding models deeply will help us make them safer. This new research marks an important milestone in that effort—the application of mechanistic interpretability to publicly-deployed large language models.</p><p>But the work has really just begun. The features we found represent a small subset of all the concepts learned by the model during training, and finding a full set of features using our current techniques would be cost-prohibitive (the computation required by our current approach would vastly exceed the compute used to train the model in the first place). Understanding the representations the model uses doesn't tell us <em>how</em> it uses them; even though we have the features, we still need to find the circuits they are involved in. And we need to show that the safety-relevant features we have begun to find can actually be used to improve safety. There's much more to be done.</p><p>For full details, please read our paper, "<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a>".</p><p><em>If you are interested in working with us to help interpret and improve AI models, we have open roles on our team and we’d love for you to apply. We’re looking for <a href="https://boards.greenhouse.io/anthropic/jobs/4009173008">Managers</a>, <a href="https://boards.greenhouse.io/anthropic/jobs/4020159008">Research Scientists</a>, and <a href="https://boards.greenhouse.io/anthropic/jobs/4020305008">Research Engineers</a>.</em></p><h4>Policy Memo</h4><p><a href="https://cdn.sanity.io/files/4zrzovbb/website/e2ae0c997653dfd8a7cf23d06f5f06fd84ccfd58.pdf">Mapping the Mind of a Large Language Model<br></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a game to help you learn neural network architectures (195 pts)]]></title>
            <link>https://graphgame.sabrina.dev/</link>
            <guid>40429200</guid>
            <pubDate>Tue, 21 May 2024 14:47:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graphgame.sabrina.dev/">https://graphgame.sabrina.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=40429200">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[CADmium: A Local-First CAD Program Built for the Browser (352 pts)]]></title>
            <link>https://mattferraro.dev/posts/cadmium</link>
            <guid>40428827</guid>
            <pubDate>Tue, 21 May 2024 14:19:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mattferraro.dev/posts/cadmium">https://mattferraro.dev/posts/cadmium</a>, See on <a href="https://news.ycombinator.com/item?id=40428827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p><time datetime="2024-05-21">May 21, 2024</time></p><p>We're building a new open-source CAD program. We've gotten pretty far, but we need your help.</p><p>If you'd like to join the effort, join the <a href="https://discord.gg/qJCsKJeyZv">Discord</a>!</p><div><p><img alt="Screenshot" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fscreenshot.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><h2 id="what-does-it-take">What Does It Take?</h2><p>To build a 3D parametric CAD program, you need a:</p><ul><li>2D Constraint Solver</li><li>B-rep Kernel</li><li>History Tracker</li><li>3D User Interface</li><li>File Format</li></ul><p>Let's talk about each one!</p><h2 id="2d-constraint-solver">2D Constraint Solver</h2><p>This is the 2D engine that can ensure lines stay parallel or perpendicular, can make two circles have the same radius, etc.</p><p>The go-to approach to solving this problem is to concatenate all the unknowns into a big vector <span>\vec{x}</span>, then express every constraint as a linear equation and assemble them all into a big matrix equation:</p><p>M\vec{x} = b</p><p>Notionally, you can invert <span>M</span> and you're done!</p><p>\vec{x} = M^{-1}b</p><p>In practice many optimizations are made. But this approach has downsides.</p><p>You can only invert <span>M</span> if it is square, which gives rise to the conventional wisdom that all sketches should be perfectly constrained. If you have too many constraints, <span>M</span> will be too tall and the approach fails, even if the redundant constraints are compatible.</p><p>If you have too few constraints, <span>M</span> will be too short which means a solution can be found by inserting assumptions. But those assumptions are not always consistent with the modeler's expectations. If you've ever had a sketch feature suddenly fly away to infinity, this is what happened.</p><p>Another downside is that solving this kind of matrix equation gets prohibitively slow when you have a lot of unknowns, which gives rise to the conventional wisdom that individual sketches should be small and simple.</p><hr><p>There are many alternative approaches for constraint solving. Let's try to formulate the problem as a 2D physics simulator:</p><ul><li>Each point has mass <span>m</span> and velocity <span>\vec{v}</span></li><li>Each constraint is a spring that exerts a force <span>F</span> on the points it is attached to</li><li>There's a friction force proportional to velocity</li><li>Step the simulation forward some small <span>dt</span> until convergence</li></ul><p>Instead of solving the whole problem at once, this formulation makes many small changes, driving the potential energy in the springs to zero.</p><video width="100%" height="auto" autoplay="" muted="" controls="" loop=""><source src="https://mattferraro.dev/images/cadmium/solving_clipped.mp4" type="video/mp4"></video><p>At each time step, the runtime is linear with the number of springs and linear with the number of unknowns, so it may support dramatically more complex sketches than the textbook approach. This type of simulation lends itself to parallelization, so it may be very fast in practice. Maybe this step could happen in a compute shader?</p><p>In this formulation, overconstrained problems don't complain about being overconstrained: a self-consistent system will solve normally, and in an inconsistent system the springs just fight it out and compromise.</p><p>Underconstrained problems don't fly out to infinity, they find the nearest valid configuration.</p><p>Another advantage is that this formulation can support inequality constraints. You could constrain a length to <em>larger than 1 cm and less than 2 cm</em>, and preserve that degree of freedom for later. You could constrain a sketch angle to <em>between 10 and 30 degrees</em>.</p><p>More speculatively, you could extend this formulation to other types of forces. If a closed polygon should have a particular surface area, that could be accommodated as a pressure force.</p><p>There are other interesting formulations of the 2D constraint problem and there are certainly disadvantages to the spring-mass-damper approach, but in general we would advocate for solving the problem iteratively rather than in a single monolithic solve step. In the last decade there has been tremendous progress in solving gradient descent problems quickly, and in bringing the power of the GPU to the browser.</p><p>Our primary goal is to make a CAD experience that feels familiar for most CAD users, but we do believe there is room here for fresh ideas.</p><h2 id="b-rep-kernel">B-rep Kernel</h2><p>In Mechanical CAD, users need to interact directly with the edges and faces of their parts.</p><p>Consequently, all parametric CAD programs model parts by directly representing their boundaries in a data structure. A cube is representated as a Solid with six Faces, each with four Edges, each with two Points. This approach is called <em>Boundary Representation</em>, or <em>b-rep</em>.</p><div><p><img alt="B-rep Illustration" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fb-rep-illustration.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>For curved surfaces, it is common to use a generalization of splines called <a href="https://www.3ds.com/store/cad/nurbs-modeling">NURBS</a> surfaces, which allow the user artistic control over freeform shapes, and the ability to represent conic sections exactly.</p><div><p><img alt="NURBS B-rep Illustration" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fnurbs-illustration.jpg&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Representing shapes this way is hard, and it gets dramatically harder when you try to implement <a href="https://www.javatpoint.com/autocad-boolean-operations">boolean operations</a> like Union, Intersection, and Subtraction. A library that handles this kind of data and boolean operations is called a b-rep kernel, and they are extremely difficult to make.</p><p>Each of the big four CAD companies has written their own, and it took them decades. Today's proprietary CAD landscape looks like this:</p><div><p><img alt="CAD Landscape" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FCAD_landscape.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy"></p></div><p>Where the company is at the top, the b-rep kernel is at the bottom, and in the middle are some of the CAD programs they offer, arranged loosely by cost.</p><p>The most important b-rep kernel is Parasolid which powers a lot of the industry including products like Shapr3D and Plasticity. Parasolid is the Cadillac Escalade of b-rep kernels: It is huge, expensive, and it offers every amenity you could ask for as well as a bunch of amenities you didn't ask for.</p><p>In contrast, the open source CAD landscape looks like this:</p><div><p><img alt="Open Source CAD Landscape" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2FOS_CAD_landscape.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>The only popular open-source b-rep kernel is OpenCascade, which is the Pontiac Aztek of b-rep kernels: It is ugly, barebones, and it might break down on you, but it is drivable and you can get one for free.</p><p>SolveSpace is a <a href="https://en.wikipedia.org/wiki/File:Tuk-Tuk_-_Herat,_Afghanistan.jpg">Tuk-Tuk</a> in that it was built by one person in a garage and it gets a lot done with very little, but it only looks like a car if you squint.</p><p>All this to say: The proprietary kernels are good but expensive and the open-source kernels are free but not good.</p><p>All popular b-rep kernels are old and written in C++. If you consult the <a href="https://dev.opencascade.org/doc/overview/html/build_upgrade__building_occt.html">official build instructions</a> for OpenCascade, you see this screenshot:</p><div><p><img alt="Screenshot of How To Build OpenCascade" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fcmake_image004.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Which looks like it was taken on Windows 2000?</p><hr><p>Thankfully, there is a new <a href="https://github.com/ricosjp/truck">open-source</a> b-rep kernel being developed right now called <a href="https://ricos.gitlab.io/truck-tutorial/v0.1/bottle.html">Truck</a>! Unlike every other b-rep kernel, this one is <em>modern</em> and it is written is Rust.</p><p>Rust is not categorically better than C++, but it is better in a lot of ways that matter to an open-source project. Its build tooling is powerful, convenient, and well-documented. It has centralized package management. It provides more guarantees around memory safety which in turn makes parallelization easier and safer. Its compiler errors are friendly and helpful, so Rust code is easier to refactor. Importantly, Rust has excellent support for compiling to webassembly so it can be readily run in a browser.</p><p>It is trivial to include Truck in any Rust project. It runs on any operating system and in a browser. They even provide javascript bindings and <a href="https://github.com/ricosjp/truck/blob/master/truck-js/tests/test.js#L4-L14">examples</a>!</p><p>Truck is about four years old and it already covers all the basics. It can read and write .step files. It can triangulate surfaces to a fixed tolerance. It has NURBS support. It can compute the Intersection or Union of two Solids<a href="https://github.com/ricosjp/truck/issues/57">*</a>, as well as the Not of a single Solid.</p><p>It is small and lightweight, it is being developed by a <a href="https://www.ricos.ltd/">real company</a>, and it is young enough and simple enough that a few motivated people could add major pieces of functionality, in a fork if necessary.</p><p>For example, the B in NURBS stands for <a href="https://en.wikipedia.org/wiki/B-spline">B-Splines</a>, but there is an alternative representation called <a href="https://en.wikipedia.org/wiki/T-spline">T-Splines</a> which is better in some ways. The <a href="https://patents.google.com/patent/US7274364B2/en">patent</a> on T-Splines is owned by Autodesk, but it just expired a few weeks ago! Support could theoretically be built into Truck!</p><p>I think that Truck is the Rivian R3 of b-rep kernels: It is smaller than its cousins, it's using a lot of modern technology in an exciting but proven way, and it isn't quite finished yet! At the risk of overextending the metaphor, Rust is the electric motor and C++ is the internal combustion engine.</p><h2 id="history-tracker">History Tracker</h2><p>Parametric CAD programs store the Feature History of your design. You sketch, extrude, and revolve until your part is done. What makes it "parametric" is that you can also rewind the clock to an earlier step, change something about it, then replay your features to get a slightly different part.</p><p>Abstracted further, you can inject variables as inputs to the model, then change the values and the part will update. Your model has now been "parameterized".</p><p>This approach has been wildly successful, but it's <a href="https://wiki.freecad.org/Topological_naming_problem">often brittle</a> and there are <a href="https://www.3dcadworld.com/the-failed-promise-of-parametric-cad/">valid criticisms</a> of the whole paradigm.</p><hr><p>One approach that has emerged to help address the brittleness of parametric CAD is called the <a href="https://www.youtube.com/watch?v=YU_lTS1vIx4&amp;t=255s">Resilient Modeling Strategy</a>, wherein:</p><div><p><img alt="Resilient Modeling Strategy" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fresilient_modeling_strategy.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>RMS is a set of conventions for how parts should be designed. For example, all chamfers and fillets go last because they consume edges. Detail features are allowed to reference Core features, but not each other, and so on.</p><p>Maybe there is value in enforcing these patterns within the CAD program. It may feel limiting at first, but it may pay huge dividends by making designs actually reusable and transferrable.</p><hr><p>Another avenue to explore could be adding a feature history to sketches. In today's CAD programs it's common to sketch base features likes circles and rectangles, then use tools like mirror, linear pattern, or sketch fillet to duplicate or modify those features. Then you sketch more base features and use more tools, back and forth. The web of dependencies this builds in a sketch is very hard to understand if you weren't the one who made the sketch, so it is often faster to delete the whole sketch and start over.</p><p>But if sketch features were also stored and displayed in a feature tree, then the ideas from RMS could be applied to a single sketch. Reference features like projecting an edge probably should come first, and final details like snipping and filleting should probably come last.</p><p>Again this might feel limiting at first, but putting an operation first in the feature tree doesn't mean you have to start with that feature chronologically as you sit down to model.</p><hr><p>On the topic of chronological ordering, why not record every user event in an append-only log? If that log were the single source of truth for the file, then any particular Feature History could be reconstructed by moving a time slider. Think: Unlimited Undo/Redo, even after closing and reopening the file:</p><div><p><img alt="Operation Log" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Foperation-log.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>You could imagine rewinding back to an earlier version of the Feature History and forking off in a different direction to try the design a different way. You would end up with a branching tree of different attempts, exactly like a git history:</p><div><p><img alt="Evolution Log" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fevolution-log.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>Our goal is to create a CAD application where every valid document state is trivially recoverable. Every false start, every "final" deliverable, whether you knew you needed it or not.</p><p>With that machinery you could maintain different variants of parts and keep a record of every design as it was when you ran downstream processes like toolpath generation or FEA.</p><p>If building this version control system is akin to building git for Mechanical design, could we also build git<strong>hub</strong> for Mechanical design?</p><h2 id="3d-user-interface">3D User Interface</h2><p>We love the idea of doing CAD in a browser. Onshape paved the way here and it's awesome.</p><p>However, Onshape doesn't really run in a browser—it runs on a GPU enabled cloud instance somewhere in AWS and streams the results to your browser. This is why if your internet connection goes down while you're using Onshape, you literally can't do <em>anything</em>. You can't even rotate the viewport.</p><div><p><img alt="Onshape with no Internet" srcset="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=640&amp;q=90 1x, https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=1920&amp;q=90 2x" src="https://mattferraro.dev/_next/image?url=%2Fimages%2Fcadmium%2Fonshape-no-internet.png&amp;w=1920&amp;q=90" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p></div><p>But CADmium doesn't have to be like that. Given that Truck can compile to webassembly, CADmium can do everything right there in your browser. A <a href="https://www.inkandswitch.com/local-first/">Local-First</a> app!</p><hr><p>We've been using this tech stack:</p><ul><li><a href="https://threejs.org/">Three.js</a> for the 3D viewport</li><li><a href="https://svelte.dev/">Svelte</a> for state management/reactivity</li><li><a href="https://threlte.xyz/">Threlte</a> to bridge the gap between Svelte and Three.js</li><li>Message passing between the UI and the b-rep kernel, rather than sharing memory</li><li><a href="https://www.electronjs.org/">Electron</a> for running locally</li><li>Bog standard everything else: Typescript, TailwindCSS, Vite, etc</li></ul><p>This kind of stack allows the entire app to be written in a reactive, declarative way, plumbing data changes all the way through to mesh updates without you having manage that complexity. That's important because 3D CAD apps are among the most complex UIs that exist. If you want to make a good one and you only have a small team, the framework had better do a lot of heavy lifting!</p><p>With this stack we were able to build a proof of concept that works, so we feel that this won't be the limiting factor for CADmium.</p><h2 id="file-format">File Format</h2><p>CADmium will use JSON for everything.</p><p>The Operation Log mentioned above should be <a href="https://jsonlines.org/">JSON lines</a>. And after you've designed a part, CADmium should support exporting to an even simpler exchange format. Notionally something like:</p><pre><code><span>{</span>
  <span>"steps"</span><span>:</span> <span>[</span>
    <span>{</span>
      <span>"type"</span><span>:</span> <span>"sketch"</span><span>,</span>
      <span>"id"</span><span>:</span> <span>"Sketch-01"</span><span>,</span>
      <span>"data"</span><span>:</span> <span>{</span> ... <span>}</span>
    <span>}</span><span>,</span>
    <span>{</span>
      <span>"type"</span><span>:</span> <span>"extrude"</span><span>,</span>
      <span>"id"</span><span>:</span> <span>"Extrude-01"</span><span>,</span>
      <span>"data"</span><span>:</span> <span>{</span>
        <span>"distance"</span><span>:</span> <span>"10mm"</span><span>,</span>
        <span>"sketch"</span><span>:</span> <span>"Sketch-01"</span><span>,</span>
        <span>"faces"</span><span>:</span> <span>[</span><span>0</span><span>]</span><span>,</span>
        <span>"type"</span><span>:</span> <span>"new"</span>
      <span>}</span>
    <span>}</span>
  <span>]</span>
<span>}</span>
</code></pre><p>Which could be converted into a .step or .stl using the CADmium command line interface (CLI):</p><pre><code>$ CADmium <span>export</span> my_part.cadmium --format stl
</code></pre><p>These two ingredients:</p><ol><li>A simple, easy-to-understand file format</li><li>An open-source CLI to work with it</li></ol><p>Are what's required to enable an ecosystem that can create tremendous new value that we would never be able to build ourselves.</p><p>Imagine being able to pop open a text editor to change an extrusion depth or a fillet radius. Imagine writing a script that replaces all the M5 screws with M6 screws, without having to read a <a href="https://en.wikipedia.org/wiki/ISO_10303-21">nasty spec</a>.</p><p>What would a change like that look like using git-diff?</p><hr><p>I mentioned above the concept of github for Mechanical design. If such a thing really were built and people really did use it, then it would not be hard to imagine building github copilot for mechanical design.</p><p>We don't know what that would look like in practice, but we think it's fair to say that large language models work best on simple, open, text-based formats rather than complex, proprietary, binary formats.</p><h2 id="conclusion">Conclusion</h2><p>Of the ideas mentioned here, we have no idea which ones are going to work out and which ones will turn out to be duds. But we know that somewhere in this space, there's a huge opportunity for a small group of people to make an outsized impact on the manufacturing industry.</p><p>These are the things we need help with:</p><ol><li>Programming in Rust (general improvements)</li><li>Computational Geometry (patches to Truck)</li><li>Three.js help (new camera controller, better lighting, post-processing)</li><li>Finding grant opportunities or wealthy benefactors</li></ol><p>These are things that we are not touching for now, but would love to revisit later:</p><ol><li>Venture Capital</li><li>Toolpath generation (CAM)</li><li>Finite Element Analysis (FEA)</li></ol><p>If you find these ideas intriguing, please join the CADmium <a href="https://discord.gg/qJCsKJeyZv">discord server</a> and chat with us!</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Dark money” groups help private ISPs lobby against municipal broadband (145 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/</link>
            <guid>40428419</guid>
            <pubDate>Tue, 21 May 2024 13:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/">https://arstechnica.com/tech-policy/2024/05/how-dark-money-groups-help-private-isps-lobby-against-municipal-broadband/</a>, See on <a href="https://news.ycombinator.com/item?id=40428419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/05/isp-network-dark-money-figures-800x450.jpg" alt="Illustration of shadowy figures and a light bulb over a map of the United States with lines depicting broadband networks.">
      <figcaption><p>Aurich Lawson | Getty Images</p></figcaption>  </figure>

  




<!-- cache hit 106:single/related:b0f3858c3a38fd390ee7649ad7f29fdc --><!-- empty -->
<p>Cities and towns that build their own broadband networks often say they only considered the do-it-yourself option because private Internet service providers didn't meet their communities' needs. When a cable or phone company's Internet service is too slow, too expensive, not deployed widely enough, or all of the above, local government officials sometimes decide to take matters into their own hands.</p>
<p><a href="https://communitynets.org/content/community-network-map">Hundreds of municipal broadband networks</a> have been built around the US as a result, including dozens that have <a href="https://ilsr.org/articles/new-municipal-broadband-networks-skyrocket-in-post-pandemic-america/">started operating</a> since 2021. The rise of public broadband hasn't happened without a fight, though. Private ISPs that would rather face no government-funded competition have tried to convince voters that public networks are doomed to become boondoggles.</p>
<p>Opponents of public broadband don't always attach their names to these campaigns, but it often seems likely that private ISPs are behind the anti-municipal broadband lobbying. Public broadband advocates say that over the past few years, they've seen a noticeable increase in "dark money" groups attacking public network projects.</p>
<p>One prominent recent example is the "<a href="https://www.nogovinternet.com/">NoGovInternet</a>" campaign run by the 501(c)(4) <a href="https://www.domesticpolicycaucus.com/">Domestic Policy Caucus</a>. NoGovInternet has been fighting the UTOPIA (Utah Telecommunication Open Infrastructure Agency) fiber collective in Utah in what seems to be an attempt to dissuade other cities and towns from joining the multi-community network. The group's effort included TV ads as part of a campaign that <a href="https://www.fox13now.com/news/local-news/new-tv-ads-go-after-utopia-and-other-government-run-internet-providers">reportedly cost $1 million</a>.</p>
<p>Nonprofits registered as 501(c)(4) "<a href="https://www.irs.gov/charities-non-profits/other-non-profits/social-welfare-organizations">social welfare organizations</a>" are allowed to engage in some political activity. Public broadband advocates suspect that 501(c)(4) groups fighting municipal networks are funded by private ISPs. There's evidence to support this belief: Even though 501(c)(4) groups don't have to reveal donors, they sometimes list ISPs as "partners" or as sponsors of a conference.</p>
<p>"It's just very easy to set up these 501(c)(4)s where you don't have to reveal the donors," Gigi Sohn, executive director of the American Association for Public Broadband (AAPB), told Ars.</p>                                            
                                                        
<p>Sohn is a longtime consumer advocate who was nominated by President Biden to serve on the Federal Communications Commission. When Sohn's nomination stalled in the Senate last year, she <a href="https://arstechnica.com/tech-policy/2023/03/bidens-fcc-pick-withdraws-regrets-that-isps-get-to-choose-their-regulators/">said</a> that cable lobbyists and dark money groups had distorted her record and in effect were allowed to "choose their regulators."</p>
<h2>“Social welfare” groups fight public broadband</h2>
<p>The AAPB group that Sohn now runs is a 501(c)(6) that represents community-owned broadband networks and co-ops. The 501(c)(6) designation is generally for business leagues, chambers of commerce, real estate boards, boards of trade, and professional football leagues, the Internal Revenue Service <a href="https://www.irs.gov/charities-non-profits/other-non-profits/types-of-organizations-exempt-under-section-501c6">says</a>.</p>
<p>501(c)(6) and 501(c)(4) groups are similar in that they don't have to reveal donors. But it can be less clear who is behind a 501(c)(4) because of a nebulous phrase: "social welfare organization." Even the IRS says the 501(c)(4) social welfare designation is an "abstruse concept that continues to defy precise definition."</p>
<p>The Domestic Policy Caucus is about a decade old but appears to have started its campaigns against municipal broadband in October 2023. The group is led by Patrick Rosenstiel, who is also involved in the <a href="https://www.nationalpopularvote.com/about">National Popular Vote campaign</a>.</p>
<p>Public broadband advocate Christopher Mitchell told Ars that when the COVID-19 pandemic made home broadband access even more important to Americans than it already was, "the cable and telephone companies lost a fair amount of their power and sway in state legislatures. Now, I kind of think they're trying to figure out how to operate in the new environment."</p>
<p>Mitchell, director of the Institute for Local Self-Reliance's Community Broadband Network Initiative, said that in this new environment, it is "obvious that we need more investment in networks" and a bigger focus on making broadband affordable.</p>
<p>"Municipal broadband is something that nearly everyone supports, unless you work for the cable company," he said. The dark money campaign that tried to tarnish UTOPIA's image, Mitchell suspects, "is about finding messages that will resonate that these big cable and telephone companies could use in other states."</p>
<p>When a cable company opposes a municipal network under its own name or a group that it is obviously associated with, they get "laughed out of the city or town," Sohn said. One example came in 2017 when voters in Fort Collins, Colorado, <a href="https://arstechnica.com/tech-policy/2017/11/voters-reject-cable-lobby-misinformation-campaign-against-muni-broadband/">approved</a> a <a href="https://fcconnexion.com/">city broadband network</a> despite a lobbying campaign funded by business and trade groups that Comcast belonged to.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What UI density means and how to design for it (488 pts)]]></title>
            <link>https://matthewstrom.com/writing/ui-density/</link>
            <guid>40428386</guid>
            <pubDate>Tue, 21 May 2024 13:41:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matthewstrom.com/writing/ui-density/">https://matthewstrom.com/writing/ui-density/</a>, See on <a href="https://news.ycombinator.com/item?id=40428386">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    <p>Interfaces are becoming less dense.</p>
<p>I’m usually one to be skeptical of nostalgia and “we liked it that way” bias, but comparing websites and applications of 2024 to their 2000s-era counterparts, the spreading out of software is hard to ignore.</p>
<p>To explain this trend, and suggest how we might regain density, I started by asking what, exactly, UI density is. It’s not just the way an interface looks at one moment in time; it’s about the amount of information an interface can provide over a series of moments. It’s about how those moments are connected through design design decisions, and how those decisions are connected to the value the software provides.</p>
<p>I’d like to share what I found. Hopefully this exploration helps you define UI density in concrete and useable terms. If you’re a designer, I’d like you to question the density of the interfaces you’re creating; if you’re not a designer, use the lens of UI density to understand the software you use.</p>
<h2 id="visual-density">Visual density</h2>
<p>We think about density first with our eyes. At first glance, density is just how many things we see in a given space. This is <strong>visual density.</strong> A visually dense software interface puts a lot of stuff on the screen. A visually sparse interface puts less stuff on the screen.</p>
<p>Bloomberg’s Terminal is perhaps the most common example of this kind of density. On just a single screen, you’ll see scrolling sparklines of the major market indices, detailed trading volume breakdowns, tables with dozens of rows and columns, scrolling headlines containing the latest news from agencies around the world, along with UI signposts for all the above with keyboard shortcuts and quick actions to take.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-01.jpg" alt="A screenshot of Terminal’s interface. Via Objective Trade on YouTube" loading="lazy"><figcaption>A screenshot of Terminal’s interface. <span>Via <a href="https://www.youtube.com/watch?v=2ee-x6IXWK8" target="_blank" rel="noopener">Objective Trade</a> on YouTube</span></figcaption></figure>
<p>Craigslist is another visually dense example, with its hundreds of plain links to categories and spartan search-and-filter interface. McMaster-Carr’s website shares similar design cues, listing out details for many product variations in a very small space.</p>
</article><article>
<p>You can form an opinion about the density of these websites simply by looking at an image for a franction of a second. This opinion is from our subconsciousness, so it’s fast and intuitive. But like other snap judgements, it’s biased and unreliable. For example, which of these images is more dense?</p>
<figure>
    <img src="https://matthewstrom.com/images/ui-density-04.png" alt="Two rectangles. On the left, the rectangle is filled with many dots that are randomly arranged. On the right, the rectangle is filled with an equal number of dots, but they are arranged neatly in rows and columns." loading="lazy">
</figure>
<p>Both images have the same number of dots (500). Both take up the same amount of space. But at first  glance, most people say image B looks more dense.<sup><a href="#fn1" id="fnref1">1</a></sup></p>
<p>What about these two images?</p>
<figure>
    <img src="https://matthewstrom.com/images/ui-density-05.png" alt="Two rectangles. On the left, the rectangle is filled with many dots that are neatly arranged in rows and columns. On the right, the rectangle is filled with an equal number of dots, arranged in two distinct groups of neatly-ordered dots." loading="lazy">
</figure>
<p>Again, both images have the same number of dots, and are the same size. But organizing the dots into groups changes our perception of density. Visually density —&nbsp;our first, instinctual judgement of density —&nbsp;is unpredictable.</p>
<p>It’s impossible to be fully objective in matters of design. But if we want to have conversations about density, we should aim for the most consistent, meaningful, and useful definition possible.</p>
<h2 id="information-density">Information density</h2>
<p>In <em>The Visual Display of Quantitative Information,</em> Edward Tufte approaches the design of charts and graphs from the ground up:</p>
<blockquote>
<p>Every bit of ink on a graphic requires reason. And nearly always that reason should be that the ink presents new information.</p>
</blockquote>
<p>Tufte introduces the idea of “data-ink,” defined as the useful parts of a given visualization. Tufte argues that visual elements that don’t strictly communicate data, whether it’s a scale value, a label, or the data itself —&nbsp;should be eliminated.</p>
<p>Data-ink isn’t just the space a chart takes up. Some charts use very little extraneous ink, but still take up a lot of physical space. Tufte is talking about <strong>information density</strong>, not visual density.</p>
<p>Information density is a measurable quantity: to calculate it, you simply divide the amount of “data-ink” in a chart by the total amount of ink it takes to print it. Of course what is and is not data-ink is somewhat subjective, but that’s not the point. The point is to get the ratio as close to 1 as possible.</p>
<p>You can increase the ratio in two ways:</p>
<ol>
<li>Add data-ink: provide additional (useful) data</li>
<li>Remove non-data-ink: erase the parts of the graphic that don’t communicate data</li>
</ol>
</article><article>
<p>There’s an upper limit to information density, which means you can subtract too much ink, or add too much information. The audience matters, too: A bond trader at their 4-monitor desk will have a pretty high threshold; a 2nd grader reading a textbook will have a low one.</p>
<p>Information density&nbsp;is can be related to visual density. Usually, the higher the information density is, the more dense a visualization will look.</p>
<p>For example, take the train schedule published by E.J. Marey in 1885<sup><a href="#fn2" id="fnref2">2</a></sup>. It shows the arrival and departure times of dozens of trains across 13 stops from Paris to Lyon. The horizontal axis is time, and the vertical axis is space. The distance between stops on the chart reflects how far apart they are in the real world.</p>
<p>The data-ink ratio is close to 1, allowing a huge amount of information —&nbsp;more than 260 arrival and departure times —&nbsp;to be packed into a relatively small space.</p>
</article><article>
<p>Tufte makes this idea explicit:</p>
<blockquote>
<p>Maximize data density and the [amount of data], within reason (but at the same time exploiting the <em>maximum resolution</em> of the available data-display technology).</p>
</blockquote>
<p>He puts it more succinctly as the “Shrink Principle”:</p>
<blockquote>
<p>Graphics can be shrunk way down</p>
</blockquote>
<p>Information density is clearly useful for charts and graphs. But can we apply it to interfaces?</p>
<p>The first half of the equation —&nbsp;information — applies to screens. We should maximize the amount of information that each part of our interface shows.</p>
<p>But the second half of the equation —&nbsp;ink —&nbsp;is a bit harder to translate. It’s tempting to think that pixels and ink are equivalent. But any interface with more than a few elements needs separators, structural elements, and signposts to help a user understand the relationship each piece has to the other.</p>
<p>It’s also tempting to follow Tufte’s Shrink Principle and try to eliminate all the whitespace in UI. But some whitespace has meaning almost as salient as the darker pixels of graphic elements. And we haven’t even touched on shadows, gradients, or color highlights; what role do they play in the data-ink equation?</p>
<p>So, while information density is a helpful stepping stone, it’s clear that it’s only part of the bigger picture. How can we incorporate <em>all</em> of the design decisions in an interface into a more objective, quantitative understanding of density?</p>
<h2 id="design-density">Design density</h2>
<p>You might have already seen the first challenge in defining density in terms of design decisons: what counts as a decision decision?</p>
<p>In UI, UX, and product design, we make many decisions, consciously and subconsciously, in order to communicate information and ideas. But  why do those particular choices convey the meaning that they do? Which ones are superlative or simply aesthetic, and which are actually doing the heavy lifting?</p>
<p>These questions sparked that led 20th century German psychologists to explore how humans understand and interpret shapes and patterns. They called this field “gestalt,” which in German means “form.” In the course of their exploration, Gestalt psychologists described principles that describe how some things appear orderly, symmetrical, or simple, while others do not. While these psychologists weren’t designers, in some sense, they discovered the fundamental laws of design:</p>
<ol>
<li><strong>Proximity</strong>: we perceive things that are close together a comprising a single group</li>
<li><strong>Similarity</strong>: objects that are similar in shape, size, color, or in other ways, appear related to one another.</li>
<li><strong>Closure</strong>: our minds fill in gaps in designs so that we tend to see whole shapes, even if there are none</li>
<li><strong>Symmetry</strong>: if we see shapes that are symmetrical to each other, we perceive them as a group formed around a center point</li>
<li><strong>Common fate</strong>: when objects move, we mentally group the ones that move in the same way</li>
<li><strong>Continuity</strong>: we can perceive objects as separate even when they overlap</li>
<li><strong>Past experience</strong>: we recognize familiar shapes and patterns even in unfamiliar contexts. Our expectations are based on what we’ve learned from our past experience of those shapes and patterns.</li>
<li><strong>Figure-ground relationship</strong>: we interpret what we see in a three-dimensional way, allowing even flat 2d images to have foreground and background elements.</li>
</ol>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 587 103"><path fill="#6BC041" d="M350.852 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551 4.724-10.551 10.551-10.551ZM376.955 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552-5.828 0-10.552-4.724-10.552-10.551s4.724-10.551 10.552-10.551ZM350.852 27.946c5.827 0 10.551 4.724 10.551 10.552 0 5.827-4.724 10.55-10.551 10.55s-10.551-4.723-10.551-10.55c0-5.828 4.724-10.552 10.551-10.552ZM376.955 27.946c5.827 0 10.551 4.724 10.551 10.552 0 5.827-4.724 10.55-10.551 10.55-5.828 0-10.552-4.723-10.552-10.55 0-5.828 4.724-10.552 10.552-10.552ZM350.852 54.049c5.827 0 10.551 4.724 10.551 10.551 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.725-10.551-10.552 4.724-10.551 10.551-10.551ZM376.955 54.049c5.827 0 10.551 4.724 10.551 10.551 0 5.828-4.724 10.552-10.551 10.552-5.828 0-10.552-4.725-10.552-10.552s4.724-10.551 10.552-10.551ZM350.852 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551c0-5.828 4.724-10.552 10.551-10.552ZM376.955 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551c-5.828 0-10.552-4.724-10.552-10.551 0-5.828 4.724-10.552 10.552-10.552Z"></path><path fill="#C06B41" d="M288.095 1.844h21.103v21.102L288.095 1.844ZM314.198 1.844h21.103v21.102L314.198 1.844ZM288.095 27.946h21.103V49.05l-21.103-21.103ZM314.198 27.946h21.103V49.05l-21.103-21.103ZM288.095 54.049h21.103V75.15L288.095 54.05ZM314.198 54.049h21.103V75.15L314.198 54.05ZM288.095 80.151h21.103v21.103l-21.103-21.103ZM314.198 80.151h21.103v21.103l-21.103-21.103Z"></path><path fill="#4180C0" d="M283.095 27.946V49.05h-21.102V27.946h21.102ZM256.993 27.946V49.05H235.89V27.946h21.103ZM283.095 1.844v21.102h-21.102V1.843h21.102ZM256.993 1.844v21.102H235.89V1.843h21.103ZM283.095 54.049V75.15h-21.102V54.05h21.102ZM256.993 54.049V75.15H235.89V54.05h21.103ZM283.095 80.151v21.103h-21.102V80.151h21.102ZM256.993 80.151v21.103H235.89V80.151h21.103Z"></path><path fill="gray" d="M0 1.844h21.103v21.102H0V1.844ZM27.103 12.395c0-5.827 4.724-10.551 10.551-10.551s10.551 4.723 10.551 10.55c0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551ZM0 27.946h21.103V49.05L0 27.946ZM27.103 27.946h21.102V49.05H27.103V27.946ZM0 64.6C0 58.773 4.724 54.05 10.551 54.05c5.828 0 10.552 4.724 10.552 10.551 0 5.828-4.724 10.552-10.552 10.552C4.724 75.151 0 70.427 0 64.6ZM27.103 54.049h21.102V75.15L27.103 54.05ZM0 80.151h21.103v21.103H0V80.151ZM27.103 90.703c0-5.828 4.724-10.552 10.551-10.552s10.551 4.725 10.551 10.552-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551ZM126.654 1.844c5.827 0 10.551 4.723 10.551 10.55 0 5.828-4.724 10.552-10.551 10.552s-10.551-4.724-10.551-10.551 4.724-10.551 10.551-10.551ZM116.103 27.946h21.102V49.05l-21.102-21.103ZM137.205 54.049V75.15h-21.102V54.05h21.102ZM126.654 80.151c5.827 0 10.551 4.725 10.551 10.552s-4.724 10.551-10.551 10.551-10.551-4.724-10.551-10.551c0-5.828 4.724-10.552 10.551-10.552ZM90 1.844h21.103v21.102L90 1.844ZM111.103 27.946V49.05H90V27.946h21.103ZM100.551 54.049c5.828 0 10.552 4.724 10.552 10.551 0 5.828-4.724 10.552-10.552 10.552C94.724 75.151 90 70.427 90 64.6s4.724-10.551 10.551-10.551ZM90 80.151h21.103v21.103L90 80.151ZM513.387 23.326 536.713 0l23.325 23.326h-46.651ZM564.12 28.196l23.325 23.326-23.325 23.326V28.196ZM509.516 74.848l-23.325-23.326 23.325-23.326v46.652ZM560.144 77.928l-23.326 23.326-23.326-23.326h46.652Z"></path></svg>
<figcaption>Examples of the princples of proximity (left), similarity (center), and closure (right).</figcaption>
</figure>
<p>Gestalt principles explain why UI design goes beyond the pixels on the screen. For example:</p>
<ul>
<li>Because of the <strong>principle of similarity</strong>, users will understand that text with the same size, font, and color serves the same purpose in the interface.</li>
<li>The <strong>principle of proximity</strong> explains why when a chart is close to a headline, it’s apparent that the headline refers to the chart. For the same reasons, a tightly packed grid of elements will look related, and separate from a menu above it separated by ample space.</li>
<li>Thanks to our <strong>past experience</strong> with switches, combined with the figure-ground principle, a skeuomorphic design for a toggle switch will make it obvious to a user how to instantly turn on a feature.</li>
</ul>
<p>So, instead of focusing on the pixels, we think of design decisions as how we intentionally use gestalt principles to communicate meaning. And like Tufte’s data-ink ratio compares the strictly necessary ink to the total ink used to print a chart, we can calculate a gestalt ratio which compares the strictly necessary design decisions to the total decisions used in a design. This is <strong>design density.</strong></p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-09.jpg" alt="Four different treatments of the same information, using different types and amounts of gestalt principles. Which is the most dense?" loading="lazy"><figcaption>Four different treatments of the same information, using different types and amounts of gestalt principles. Which is the most dense?</figcaption></figure>
<p>This is still subjective: a design decision that seems necessary to some might be superfluous to others. Our biases will skew our assessment, whether they’re personal tastes or cultural norms. But when it comes to user interfaces, counting design decisions is much more useful than counting the amount of data or “ink” alone.</p>
<p>Design density isn’t perfect. User interfaces exist to do work, to have fun, to waste time, to create understanding, to facilitate personal connections, and more. Those things require the user to take one or more actions, and so density needs to look beyond components, layouts, and screens. Density should comprise all the actions a user takes in their journey —&nbsp;it should count in space and time.</p>
<h2 id="density-in-time">Density in time</h2>
<p>Just like the amount of stuff in a given space dictates visual density, the amount of things a user can do in a given amount of time dictates <strong>temporal</strong> —&nbsp;time-wise —&nbsp;density.</p>
<p>Loading times are the biggest factor in temporal density.&nbsp;The faster the interface responds to actions and loads new pages or screens, the more dense the UI is. And unlike 2-dimensional whitespace, there’s almost no lower limit to the space needed between moments in time.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-10.gif" alt="Bloomberg’s Terminal loads screens full of data instantaneously" loading="lazy"><figcaption>Bloomberg’s Terminal loads screens full of data instantaneously</figcaption></figure>
<p>With today’s bloated software, making a UI more dense in time is more impactful than just squeezing more stuff onto each screen. That’s why Bloomberg’s Terminal is still such a dominant tool in the financial analysis space; it loads data almost instantaneously. A skilled Terminal user can navigate between dozens of charts and graphs in milliseconds. There are plenty of ways to cram tons of financial data into a table, but loading it with no latency is Terminal’s real superpower.</p>
<p>But say you’ve squeezed every second out of the loading times of your app. What next? There are some things that just can’t be sped up: you can’t change a user’s internet connection speed, or the computing speed of their CPU. Some operations, like uploading a file, waiting for a customer support response, or processing a payment, involve complex systems with unpredictable variables. In these cases, instead of changing the amount of time between tasks, you can change the <strong>perception</strong> of that time:</p>
<ul>
<li>Actions <strong>less than 100 milliseconds</strong> apart will feel simultaneous. If you tap on an icon and, 100ms later, a menu appears, it feels like no time at all passed between the two actions. So, if there’s an animation between the two actions —&nbsp;the menu slides in, for example —&nbsp;the illusion of simultaneity might be broken. For the smallest temporal spaces, animations and transitions can make the app feel <em>slower</em>.<sup><a href="#fn3" id="fnref3">3</a></sup></li>
<li>Between <strong>100 milliseconds and 1 second</strong>, the connection between two actions is broken. If you tap on a link and there’s no change for a second, doubt creeps in: did you actually tap on anything? Is the app broken? Is your internet working? Animations and transitions can bridge this perceptual gap. Visual cues in these spaces make the UI feel more dense in time.</li>
<li>Gaps between <strong>1 and 10 seconds</strong> can’t be bridged with animations alone; research<sup><a href="#fn4" id="fnref4">4</a></sup> shows that users are most likely to abandon a page within the first 10 seconds. This means that if two actions are far enough apart, a user will leave the page instead of waiting for the second action. If you can’t decrease the time between these actions, show an indeterminate loading indicator —&nbsp;a small animation that tells the user that the system is operating normally.</li>
<li>Gaps between <strong>10 seconds and 1 minute</strong> are even harder to fill. After seeing an indeterminate loader for more than 10 seconds, a user is likely to see it as static, not dynamic, and start to assume that the page isn’t working as expected. Instead, you can use a determinate loading indicator —&nbsp;like a larger progress bar —&nbsp;that clearly indicates how much time is left until the next action happens. In fact, the right design can make the waiting time seem shorter than it actually is; the backwards-moving stripes that featured prominently in Apple’s “Aqua” design system made waiting times seem 11% shorter.<sup><a href="#fn5" id="fnref5">5</a></sup></li>
<li>For gaps <strong>longer than 1 minute,</strong> it’s best to let the user leave the page (or otherwise do something else), then notify them when the next action has occurred. Blocking someone from doing anything useful for longer than a minute creates frustration. Plus, long, complex processes are also susceptible to error, which can compound the frustration.</li>
</ul>
<p>In the end, though, making a UI dense in time and space is just a means to an end. No UI is valuable because of the way it looks. Interfaces are valuable in the outcomes they enable —&nbsp;whether directly associated with some dollar value, in the case of business software, or tied to some intangible value like entertainment or education.</p>
<p>So what is density really about, then? It’s about providing the highest value outcomes in the smallest amount of time, space, pixels, and ink.</p>
<h2 id="density-in-value">Density in value</h2>
<p>Here’s an example of how value density is manifested: a common suggestion for any form-based interface is to break long forms into smaller chunks, then put those chunks together in a wizard-type interface that saves your progress as you go. That’s because there’s no value in a partly-filled-in-form; putting all the questions on a single page might look more visually dense, but if it takes longer to fill out, many users won’t submit it at all.</p>
<figure data-type="image"><img src="https://matthewstrom.com/images/ui-density-11.jpg" alt="This form is broken up into multiple parts, with clear errors and instructions for resolution." loading="lazy"><figcaption>This form is broken up into multiple parts, with clear errors and instructions for resolution.</figcaption></figure>
<p>Making it possible for users to get to the end of a form with fewer errors might require the design to take up more space. It might require more steps, and take more time. But if the tradeoffs in visual and temporal density make the outcome more valuable —&nbsp;either by increasing submission rate or making the effort more worth the user’s time — then we’ve increased the overall value density.</p>
<p>Likewise, if we can increase the visual and temporal density by making the form more compact, load faster, and less error-prone, <em>without</em> subtracting value to the user or the business, then that’s an overall increase in density.</p>
<p>Channeling Tufte, we should try to increase value density as much as possible.</p>
<p>Solving this optimization problem can have some counterintuitive results. When the internet was young, companies like Craigslist created value density by aggregating and curating information and displaying it in pages of links. Companies like Yahoo and Altavista made it possible to search for that information, but still put aggregation at the fore. Google took a radically different approach: use information gleaned by the internet’s long chains of linked lists to power a search box. Information was aggregating itself; a single text input was all users needed to access the entire web.</p>
</article><article>
<p>The UI was much less visually dense, but more value-dense by orders of magnitude. The results speak for themselves: Google went from a $23B valuation in 2004 to being worth over $2T today —&nbsp;closing in on a 100x increase. Yahoo went from being worth $125B in 2000 to being sold for $4.8B —&nbsp;less than 3% of its peak value.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Designing for UI density goes beyond the visual aspects of an interface. It includes all the implicit and explicit design decisions we make, and all the information we choose to show on the screen. It includes all time and the actions a user takes to get something valuable out of the software.</p>
<p>So, finally, a concrete definition of UI density: <strong>UI density is the value a user gets from the interface divided by the time and space the interface occupies.</strong></p>
<p>Speed, usability, consistency, predictability, information richness, and functionality all play an important role in this equation. By taking account of all these aspects, we can understand why some interfaces succeed and others fail. And by designing for density, we can help people get more value out of the software we build.</p>
<hr>
<section>
<p>Footnotes &amp; References</p>
<ol>
<li id="fn1"><p>This is a very unscientific statement based on a poll of 20 of my coworkers. Repeatability is questionable. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>The provenance of the chart is interesting. Not much is known about the original designer, Charles Ibry; but what we do know points to even earlier iterations of the design. If you’re interested, read <a href="https://sandrarendgen.wordpress.com/2019/03/15/data-trails-from-paris-with-love/" target="_blank" rel="noopener">Sandra Rendgen’s fascinating historory of the train schedule</a>. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>I have no scientific backing for this claim, but I believe it’s because a typical blink occurs in 100ms. When we blink, our brains fill in the gap with the last thing we saw, so we don’t notice the blink. That’s is why we don’t notice the gap between two actions that are less than 100ms apart. You can read more about this effect here: <a href="https://www.sciencedirect.com/science/article/pii/S0960982209011105" target="_blank" rel="noopener">Visual Perception: Saccadic Omission — Suppression or Temporal Masking?</a> <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Nielsen, Jakob. “How Long Do Users Stay on Web Pages?” Nielsen Norman Group, 11 Sept. 2011, <a href="https://www.nngroup.com/articles/how-long-do-users-stay-on-web-pages/" target="_blank" rel="noopener">https://www.nngroup.com/articles/how-long-do-users-stay-on-web-pages/</a> <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>Harrison, Chris, Zhiquan Yeo, and Scott E. Hudson. “Faster Progress Bars: Manipulating Perceived Duration with Visual Augmentations.” Carnegie Mellon University, 2010, <a href="https://www.chrisharrison.net/projects/progressbars2/ProgressBarsHarrison.pdf" target="_blank" rel="noopener">https://www.chrisharrison.net/projects/progressbars2/ProgressBarsHarrison.pdf</a> <a href="#fnref5">↩︎</a></p>
</li>
</ol>
</section>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Satoshi' impersonation 'a serious abuse of court's process' judge concludes (104 pts)]]></title>
            <link>https://www.lawgazette.co.uk/news/satoshi-impersonation-a-serious-abuse-of-courts-process-judge-concludes/5119771.article</link>
            <guid>40427024</guid>
            <pubDate>Tue, 21 May 2024 11:41:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lawgazette.co.uk/news/satoshi-impersonation-a-serious-abuse-of-courts-process-judge-concludes/5119771.article">https://www.lawgazette.co.uk/news/satoshi-impersonation-a-serious-abuse-of-courts-process-judge-concludes/5119771.article</a>, See on <a href="https://news.ycombinator.com/item?id=40427024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A bitterly fought court battle over a computer scientist's claim to be bitcoin's inventor represented a 'most serious abuse' of the High Court's process, a judge ruled today.&nbsp;In his 230-page written judgment in <em><a href="https://caselaw.nationalarchives.gov.uk/ewhc/ch/2024/1198">Crypto Open Patent Alliance v Craig Steven Wright</a></em>,<em>&nbsp;</em>Mr Justice Mellor set out his reasons behind his <a href="https://www.lawgazette.co.uk/news/judge-dashes-bitcoin-entrepreneurs-satoshi-claim/5119062.article"><strong>finding in March in the sixth week of a tria</strong>l</a>&nbsp;that Dr Craig Wright's claims were false. The Rolls Building hearing was watched by some 1,100 people around the world.&nbsp;</p>
<p>The case was brought by a group of US software developers seeking a declaration that Wright, an Australian-born entrepreneur resident in the UK, was not 'Satoshi', the pseudonymous author of the 2008 white paper which sparked off the trillion-dollar bitcoin phenomenon. Wright's defence consisted largely of documentary submissions which the judge agreed were forged. On cross-examination, Wright proved to be 'an extremely slippery witness', Mellor observed. At one stage the forgeries put his solicitors, City firm Shoosmiths, 'in an impossible position', the judge commented.&nbsp;</p>
<p>Among the 47 forgeries found proven in the judgment were documents purporting to be precursors of the seminal 2008 white paper. These were challenged by expert witnesses on the ground that they were written in a version of LaTeX document preparation software not available in 2008, and contained 'evidence of anachronistic metadata' created by Wright’s 'attempt to fiddle' with formatting to resemble Satoshi's published white paper.&nbsp; &nbsp;</p>
<p>'Dr Wright lied about these&nbsp;matters (and sought to abuse legal professional privilege) to conceal the fact that the&nbsp;white paper LaTeX Files were a recent creation,' the judge concluded. 'Dr Wright’s elaborate attempt to carve an alternative narrative by forging documents in LaTeX mark him as a fraud and his claim&nbsp;in these proceedings as a fraudulent claim.'&nbsp;</p>
<p>The judgment comprehensively backs expert evidence challenging the authenticity of Wright's original submissions. Attempts to blame unknown 'bad actors' for tampering with documents were 'literally incredible'. On 'numerous occasions' Wright also unsuccessfully sought to put the blame for anomalies on his previous legal advisers.&nbsp;</p>
<p>During the trial Wright had attempted to attack expert evidence on forgery on the ground that reports were prepared with the assistance of the claimant's solicitors, IP specialist Bird &amp; Bird. Finding 'nothing at all wrong' in the way the reports were prepared, the judge said 'I acquit Bird &amp; Bird of any charge&nbsp;that [the expert's] reports were prepared in an inappropriate manner.&nbsp;</p>
<p>Overall, Wright's submissions contained no authentic 'time capsule' documents, the judgment found. By contrast&nbsp;COPA’s evidence was 'cogent and compelling'.</p>
<p>'Dr Wright presents himself as an extremely clever person,' the judge stated. 'However, in my&nbsp;judgment, he is not nearly as clever as he thinks he is. In both his written evidence and&nbsp;in days of oral evidence under cross-examination, I am entirely satisfied that Dr Wright&nbsp;lied to the court extensively and repeatedly.'</p>
<p>Stating that the real 'Satoshi' would have little difficulty in proving the fact, the judgment highlights Wright's failure to prove his assertion that he owns 'genesis blocks' of bitcoin which would today be worth billions of pounds. Under cross-examination, Wright attempted to explain gaps in his evidence by resorting to 'technobabble'.&nbsp;</p>
<p>The judge commented that, to judge by his correspondence and writings, the real Satoshi was 'a calm, knowledgeable, collaborative, precise person&nbsp;with little or no arrogance', adding&nbsp;'I&nbsp;consider it is most unlikely that Satoshi would ever have resorted to&nbsp;litigation against the [software developer parties in the case]'. This was contrasted with Wright's arrogance and record of serial litigation. 'It is likely that the real Satoshi would never have set out to prove&nbsp;in litigation that he actually was Satoshi and certainly not in the way that Dr Wright&nbsp;attempted to do so,' the judge observed.&nbsp;</p>
<p>Injunctive relief will be decided after a form of order hearing following the hand-down.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We created the first open source implementation of Meta's TestGen–LLM (124 pts)]]></title>
            <link>https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/</link>
            <guid>40426995</guid>
            <pubDate>Tue, 21 May 2024 11:37:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/">https://www.codium.ai/blog/we-created-the-first-open-source-implementation-of-metas-testgen-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=40426995">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p><img width="690" height="553" src="https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-690x553.jpg" alt="" decoding="async" fetchpriority="high" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-690x553.jpg 690w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-300x240.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1-768x615.jpg 768w, https://www.codium.ai/wp-content/uploads/2024/05/img-ci-agent-1.jpg 924w" sizes="(max-width: 690px) 100vw, 690px"> </p>
<p>In February, Meta researchers published a paper titled <a href="https://arxiv.org/abs/2402.09171" target="_blank" rel="noopener">Automated Unit Test Improvement using Large Language Models at Meta</a>, which introduces a tool they called TestGen-LLM. The fully automated approach to increasing test coverage “with guaranteed assurances for improvement over the existing code base” <a href="https://news.ycombinator.com/item?id=39486717" target="_blank" rel="noopener">created waves</a> in the software engineering world.</p>
<p>Meta didn’t release the TestGen-LLM code, so we decided to implement it as part of our <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">Cover-Agent</a> open-source and we’re releasing it today!</p>
<p>I’ll share some information here on how we went about implementing it, share some of our findings and outline the challenges we encountered when actually using TestGen-LLM with real-world codebases.</p>
<h2>Automated Test Generation: Baseline Criteria</h2>
<p>Automated test generation using Generative AI is nothing new. Most LLMs that are competent at generating code, such as ChatGPT, Gemini, and Code Llama, are capable of generating tests. The most common pitfall that developers run into when generating tests with LLMs is that most generated tests don’t even work and many don’t add value (e.g. they test the same functionality already covered by other tests).</p>
<p>To overcome this challenge (specifically, for regression unit tests) the TestGen-LLM authors came up with the following criteria:</p>
<ol>
<li>Does the test compile and run properly?</li>
<li>Does the test increase code coverage?</li>
</ol>
<p>Without answering these two fundamental questions, arguably, there’s no point in accepting or analyzing the generated test provided to us by the LLM.<br>
Once we’ve validated that the tests are capable of running correctly and that they increase the coverage of our component under test, we can start to investigate (in a manual review):</p>
<ol>
<li>How well is the test written?</li>
<li>How much value does it actually add? (We all know that sometimes Code Coverage could be a proxy or even vanity metric)</li>
<li>Does it meet any additional requirements that we may have?</li>
</ol>
<h2>Approach and reported results</h2>
<p>TestGen-LLM (and Cover-Agent) run completely headless (well, kind of; we will discuss this later).</p>
<figure id="attachment_6831" aria-describedby="caption-attachment-6831"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper.jpg" alt="TestGen-LLM paper" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper-300x81.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-testgen-llm-paper-690x186.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6831">From TestGen-LLM paper</figcaption></figure>
<p>First, TestGen-LLM generates a bunch of tests, then it filters out those that don’t build/run and drops any that don’t pass, and finally, it discards those that don’t increase the code coverage. In highly controlled cases, the ratio of generated tests to those that pass all of the steps is 1:4, and in real-world scenarios, Meta’s authors report a 1:20 ratio.</p>
<p>Following the automated process, Meta had a human reviewer accept or reject tests. The authors reported an average acceptance ratio of 1:2, with a 73% acceptance rate in their best reported cases.</p>
<p>It is important to note that the TestGen-LLM tool, as described in the paper, generates on each run a single test that is added to an existing test suite, written previously by a professional developer. Moreover, it doesn’t necessarily generate tests for any given test suite.</p>
<p><strong>From the paper:</strong> “In total, over the three test-a-thons, 196 test classes were successfully improved, while the TestGen-LLM tool was applied to a total of 1,979 test classes. TestGen-LLM was therefore able to automatically improve approximately 10% of the test classes to which it was applied.”</p>
<figure id="attachment_6833" aria-describedby="caption-attachment-6833"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow.jpg" alt="Cover-Agent" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow-300x162.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-cover-agent-flow-690x372.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6833">Cover-Agent v0.1 flow</figcaption></figure>
<p><strong>Cover-Agent v0.1 is implemented as follows:</strong></p>
<ol>
<li>Receive the following user inputs:
<ol>
<li>Source File for code under test</li>
<li>Existing Test Suite to enhance</li>
<li>Coverage Report</li>
<li>The command for building and running test suite</li>
<li>Code coverage target and maximum iterations to run</li>
<li>Additional context and prompting options</li>
</ol>
</li>
<li>Generate more tests in the same style</li>
<li>Validate those tests using your runtime environment
<ol>
<li>Do they build and pass?</li>
</ol>
</li>
<li>Ensure that the tests add value by reviewing metrics such as increased code coverage</li>
<li>Update existing Test Suite and Coverage Report</li>
<li>Repeat until code reaches criteria: either code coverage threshold met, or reached the maximum number of iterations</li>
</ol>
<p><iframe src="https://www.youtube.com/embed/fIYkSEJ4eqE" frameborder="0" scrolling="no" allowfullscreen="allowfullscreen"><span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start">﻿</span></iframe></p>
<h2>Challenges we encountered when implementing and reviewing TestGen-LLM</h2>
<p>As we worked on putting the TestGen-LLM paper into practice, we ran into some surprising challenges.</p>
<p>The examples presented in the paper mention using <a href="https://kotlinlang.org/" target="_blank" rel="noopener">Kotlin</a> for writing tests – a language that doesn’t use significant whitespace. With languages like&nbsp; Python on the other hand, tabs and spaces are not only important but a requirement for the parsing engine. Less sophisticated models, such as GPT 3.5, won’t return code that is consistently indented properly, even when explicitly prompted. An example of where this causes issues is a test class written in Python that requires each test function to be indented. We had to account for this throughout our development lifecycle which added more complexity around pre-processing libraries. There is still plenty to improve on in order to make Cover-Agent robust in scenarios like this.</p>
<figure id="attachment_6832" aria-describedby="caption-attachment-6832"><img decoding="async" src="https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table.jpg" alt="Prompt Table" width="750" srcset="https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table.jpg 750w, https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table-300x104.jpg 300w, https://www.codium.ai/wp-content/uploads/2024/05/img-prompt-table-690x240.jpg 690w" sizes="(max-width: 750px) 100vw, 750px"><figcaption id="caption-attachment-6832">From TestGen-LLM paper. Original prompts suggested in TestGen-LLM.</figcaption></figure>
<p>After seeing the special test requirements and exceptions we encountered during our trials, we decided to give the user the ability to provide additional input or instructions to prompt the LLM as part of the Cover-Agent flow. The `–additional-instructions` option allows developers to provide any extra information that’s specific to their project, empowering them to customize Cover-Agent. These instructions can be used, for example, to steer Cover-Agent to create a rich set of tests with meaningful edge cases.</p>
<p>Concurring with the general trend of Retrieval-Augmented Generation (RAG) becoming more pervasive in AI based applications, we identified that having more context to go along with unit test generation enables higher quality tests and a higher passing rate. We’ve provided the `–included-files` option to users who want to manually add additional libraries or text-based design documents as context for the LLM to enhance the test generation process.</p>
<p>Complex code that required multiple iterations presented another challenge to the LLMs. As the failed (or non-value added) tests were generated, we started to notice a pattern where the same non-accepted tests were repeatedly suggested in later iterations. To combat this we added a “Failed Tests” section to the prompt to deliver that feedback to the LLM and ensure it generated unique tests and never repeated tests that we deemed unusable (i.e. broken or lack of coverage increase).</p>
<p>Another challenge that came up throughout this process was the inability to add library imports when extending an existing test suite. Developers can sometimes be myopic in their test generation process, only using a single approach to testing frameworks. In addition to many different mocking frameworks, other libraries can help with achieving test coverage. Since the TestGen-LLM paper (and Cover-Agent) are intended to extend existing test suites, the ability to completely restructure the whole test class is out of scope. This is, in my opinion, a limitation of test extension versus test generation and something we plan on addressing in future iterations.</p>
<p>It’s important to make the distinction that in TestGen-LLM’s approach, each test required a manual review from the developer before the next test is suggested. In Cover-Agent on the other hand, we generate, validate, and propose as many tests as possible until achieving the coverage requirement (or stopping at the max iterations), without requiring manual intervention throughout the process. We leverage AI to run in the background, creating an unobtrusive approach to automatic test generation that allows the developer to review the entire test suite once the process has completed.</p>
<h2>Conclusion and what’s next</h2>
<p>While many, including myself, are excited about the TestGen-LLM paper and tool, in this post we have shared its limitations. I believe that we are still in the era of AI assistants and not AI teammates who run fully automated workflows.</p>
<p>At the same time, well-engineered flows, which we plan to develop and share here in <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">Cover-Agent</a>, can help us developers automatically generate test candidates, and increase code coverage in a fraction of the time.</p>
<p>We intend to continue developing and integrating cutting-edge methods related to the test generation domain into the Cover-Agent open-source repo.<br>
We encourage anyone interested in generative AI for testing to collaborate and help extend the capabilities of Cover Agent, and we hope to inspire researchers to leverage this open-source tool to explore new test-generation techniques.</p>
<p>In the open-source Cover-Agent repo on GitHub we’ve added a development <a href="https://github.com/Codium-ai/cover-agent" target="_blank" rel="noopener">roadmap</a>. We would love to see you contributing to the repo according to the roadmap or according to your own ideas!</p>
<p>Our vision for Cover-Agent is that in the future it will run automatically for every pre/post-pull request and automatically suggest regression test enhancements that have been validated to work and increase code coverage. We envision that Cover-Agent will automatically scan your codebase, and open PRs with test suites for you.</p>
<p>Let’s leverage AI to help us deal more efficiently with the tasks we don’t like doing!</p>
<p><strong>P.S.</strong></p>
<ol>
<li>We are still looking for a good benchmark for tools like this. Do you know of one? We think it is critical for further development and research.</li>
<li>Check out our <a href="https://twitter.com/talrid23/status/1760351642237477109" target="_blank" rel="noopener">AlphaCodium work</a> for (a) further reading on “Flow Engineering”, as well as an example of (b) a competitive programming benchmark, and (c) a well-designed dataset called CodeContests.</li>
</ol>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Images that Sound: Generating spectrograms that are also images (124 pts)]]></title>
            <link>https://ificl.github.io/images-that-sound/</link>
            <guid>40426890</guid>
            <pubDate>Tue, 21 May 2024 11:21:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ificl.github.io/images-that-sound/">https://ificl.github.io/images-that-sound/</a>, See on <a href="https://news.ycombinator.com/item?id=40426890">Hacker News</a></p>
<div id="readability-page-1" class="page">


  <div>
            <h2>Images that Sound:<br>Composing Images and Sounds on a Single Canvas</h2>
            <h2> arXiv 2024 </h2>
            

            <p><span>University of Michigan</span>
            </p>
            
            <p><span>Correspondence to: <span>ude.hcimu@gnayzc</span></span>
            </p>

            
            
          </div>


  <!-- TEASER + INTRO -->
  <div>
      <div>
        <!-- First column in its own row -->
        <p>
          <h3>
            <b>tl;dr:</b> We use diffusion models to generate spectrograms that look like images but can also be played as sound.
          </h3>
        </p>
      </div>
      <!-- Second column in a separate row -->
      <div>
              <video autoplay="" muted="" loop="" playsinline="">
                <source src="https://ificl.github.io/images-that-sound/static/videos/teaser.mp4" type="video/mp4">
              </video>
              <p>
                Note the above teaser is muted. For examples with sound, please see our <a href="#gallery">gallery</a> below.
              </p>
            </div>
    </div>


  <!-- OVERVIEW -->
  <div>
          <h2>Overview</h2>
          <div>
            <p>
              Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And
              natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to
              synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these
              spectrograms <strong>images that sound</strong>. Our approach is simple and zero-shot, and it leverages pre-trained
              text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse
              process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that
              is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method
              successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a
              desired image prompt. 
            </p>
            <p>
              We describe our <a href="#method">method</a> in more detail and show examples in our <a href="#gallery">gallery</a> below.
            </p>
          </div>
        </div>

  <!-- Gallery -->
  <div>
          <h2 id="gallery">Gallery</h2>
          <!-- <br> -->
          <p>Play the video to hear the audio!</p>

          <h2 id="gallery">Colorful <em>Images that Sound</em></h2>
          

          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <h2 id="gallery">Grayscale <em>Images that Sound</em></h2>
          
          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->
          
          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->

          
          <!-- <br> -->
          
          
          <!-- <br> -->

          <!-- <div class="content is-centered has-text-centered">
            <p style="margin-top: 30px; font-family: cursive; font-size: 20px;">More videos will be coming soon!</p>
          </div> -->
        </div>


  <!-- METHOD -->
  <div>
          <h2 id="method">Method</h2>
          <div>

              <p><img src="https://ificl.github.io/images-that-sound/static/images/method.jpg"></p><p>
              We pose the problem of generating <em>images that sound</em> as a multimodal composition problem:
              our goal is to obtain a sample that is likely under both the distribution of images and the distribution of spectrograms.
              To do this, we simultaneously denoise using an image diffusion model and an audio diffusion model.
              Given a noisy latent \(\mathbf{z}_t\), we compute two text-conditioned noise estimates \(\boldsymbol{\epsilon}_{v}^{(t)}\) and
              \(\boldsymbol{\epsilon}_{a}^{(t)}\). One for each modality. We then obtain a multimodal noise estimate
              \(\tilde{\boldsymbol{\epsilon}}^{(t)}\) via weighted averaging, which we then use to denoise. 
              Repeating this process iteratively results in a clean latent \(\mathbf{z}_0\). Finally, we decode this clean
              latent to a spectrogram and convert it into a waveform using a pretrained vocoder.
              As we only change the inference time procedure, our method is zero-shot, requiring no training or fine-tuning.
            </p>

            <div>
              <video autoplay="" muted="" loop="" playsinline="">
                <source src="https://ificl.github.io/images-that-sound/static/videos/denoise.mp4" type="video/mp4">
              </video>
              <p>
                Iteratively denoising using both a spectrogram diffusion model 
                and an image diffusion model. See <a href="#gallery">above</a> 
                for videos with sound.
              </p>
            </div>

          </div>
        </div>


  <div>
          <h2>Related Links and Works</h2>
  
          <div>
            <p>
              Various musicians have inserted images into spectrograms of their music, including 
              <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8" target="_blank">Aphex Twin</a> <span>(go to 5:27)</span>,
              <a href="https://en.wikipedia.org/wiki/Year_Zero_(album)#Promotion" target="_blank">Nine Inch Nails</a>,
              <a href="https://www.youtube.com/watch?v=BHup81lEjqo" target="_blank">Venetian Snares</a>, and in
              <a href="https://www.youtube.com/watch?v=yzFit0nldf4" target="_blank">Doom's OST</a>. Our work differs 
              from these examples in that our spectrograms both look and sound natural.
            </p>
            <p>
              <a href="https://mixmag.net/feature/spectrogram-art-music-aphex-twin" target="_blank">Spectrogram Art</a>, 
              by <a href="https://www.instagram.com/beckybuckle/" target="_blank">Becky Buckle</a>: an article about the 
              history of artists concealing images in the spectrogram of their music.
            </p>
            <p>
              <a href="https://github.com/LeviBorodenko/spectrographic" target="_blank">SpectroGraphic</a>, 
              by <a href="https://github.com/LeviBorodenko" target="_blank">Levi Borodenko</a>:
              a tool to turn images into spectrograms and recover the corresponding audio.
            </p>
            <p>
              <a href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" target="_blank">Composable Diffusion</a>,
              by <a href="https://nanliu.io/" target="_blank">Liu</a> <i>et al.</i>,
              which originally showed how to compose image diffusion models together.
            </p>
            <p>
              <a href="https://dangeng.github.io/visual_anagrams/" target="_blank">Visual Anagrams</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which uses pretrained diffusion models and compositionality to make multi-view optical illusions.
            </p>
            <p>
              <a href="https://dangeng.github.io/factorized_diffusion/" target="_blank">Factorized Diffusion</a>,
              by <a href="https://dangeng.github.io/" target="_blank">Geng</a> <i>et al.</i>,
              which generates various perceptual illusions via decomposition with diffusion models. We use
              their code the colorize the spectrograms.
            </p>
            <p>
              <a href="https://diffusionillusions.com/" target="_blank">Diffusion Illusions</a>,
              by <a href="https://ryanndagreat.github.io/" target="_blank">Burgert</a> <i>et al.</i>,
              which produces multi-view illusions, along with other visual effects, through score distillation sampling.
              We adapt their code to make an SDS style baseline for generating images that sound.
            </p>
            
          </div>
        </div>


  <div id="BibTeX">
          <h2>BibTeX</h2>
          <pre><code>@article{chen2024soundify,
  title     = {Images that Sound: Composing Images and Sounds on a Single Canvas},
  author    = {Chen, Ziyang and Geng, Daniel and Owens, Andrew},
  year      = {2024},
  url       = {https://ificl.github.io/images-that-sound/},
}</code></pre>
        </div>


  



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[One dead as London-Singapore flight hit by turbulence (142 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c8889d7x8j4o</link>
            <guid>40426801</guid>
            <pubDate>Tue, 21 May 2024 11:08:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c8889d7x8j4o">https://www.bbc.com/news/articles/c8889d7x8j4o</a>, See on <a href="https://news.ycombinator.com/item?id=40426801">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Going Dark: The war on encryption is on the rise (370 pts)]]></title>
            <link>https://mullvad.net/en/why-privacy-matters/going-dark</link>
            <guid>40426701</guid>
            <pubDate>Tue, 21 May 2024 10:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mullvad.net/en/why-privacy-matters/going-dark">https://mullvad.net/en/why-privacy-matters/going-dark</a>, See on <a href="https://news.ycombinator.com/item?id=40426701">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><header><p> <h2>State mass surveillance</h2></p> <p>Under the slogan ‘Think of the children’, the European Commission tried to introduce total surveillance of all EU citizens. When the scandal was revealed, it turned out that American tech companies and security services had been involved in the bill, generally known as ‘Chat Control’ – and that the whole thing had been directed by completely different interests. Now comes the next attempt. New battering rams have been brought out with the ‘Going Dark’ initiative. But the ambition is the same: to install state spyware on every European cell phone and computers.</p></header> <p data-svelte-h="svelte-t1esob"><img src="https://mullvad.net/images/previous-term.jpg" alt="A satirical cartoon of a man sitting in front of a computer, symbolizing mass surveillance in the EU."></p> <p data-svelte-h="svelte-1i3n5uv">On May 11, 2022, EU Commissioner Ylva Johansson presented a legislative
proposal under the official name ”<a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN" rel="nofollow">Regulation of the European Parliament
and of the Council laying down rules to prevent and combat child sexual
abuse</a>.”</p> <p data-svelte-h="svelte-34ak2p">Ylva Johansson made a point of this being her bill: it was she who had
devised it – no one else – and if it had not been for her, Europe’s
justice system would “go blind” in the hunt to track sexual abuse of
children. In Ylva’s world, the EU would “turn into a pedophiles’
paradise” if she didn’t get her way. It was easy to marvel at how, on
almost every occasion, Ylva Johansson was keen to point out that this
was her proposal. A touch of narcissism? Maybe. But perhaps there was
something else behind this self-centered proclamation. Because it would
eventually emerge that in fact Ylva Johansson was not alone behind the
scenes. Right from the start, there were others involved – actors who
would benefit from the bill being passed, but who preferred it not to be
known that they were involved in designing it.</p> <p data-svelte-h="svelte-hu60su">The rhetoric was clear from day one: it was all about the children, and
when it comes to children, there’s nothing we can’t imagine doing to
keep them safe. So Ylva Johansson put forward a proposal that meant
total surveillance of all EU citizens and as soon as someone opposed it,
she pulled out the think-of-the-children card. But those who could see
through the bluff quickly gave the proposal (those parts of the bill
that dealt with internet surveillance) a shorter and more appropriate
name: Chat Control.</p> <p data-svelte-h="svelte-1wqsbbm">In brief, Chat Control essentially meant that the communications of
every EU citizen would be monitored. Every call, every message and every
chat, all the emails, photos, and videos saved in cloud services – all
of it would be filtered in real time via artificial intelligence and
then checked in a newly established EU center, in close cooperation with
Europol.</p> <p data-svelte-h="svelte-lj4ut6">Since the bill was in violation of the European Convention on Human
Rights, the EU Charter and the UN Declaration of Human Rights, Chat
Control was rejected by one legislative body after another. Both <a href="https://www.patrick-breyer.de/wp-content/uploads/2023/05/st08787.en23-leak.pdf" rel="nofollow">the
Council of
Ministers</a>
and the <a href="https://www.patrick-breyer.de/wp-content/uploads/2022/11/220128_Opinion-II-HOME-Child-Sexual-Abuse-annotated-fin.pdf" rel="nofollow">European Commission’s own legal
service</a>
warned against the proposal, as did <a href="https://edps.europa.eu/system/files/2022-07/22-07-28_edpb-edps-joint-opinion-csam_en.pdf" rel="nofollow">the European Parliament’s Data
Protection
Board</a>.
The UN Human Rights Council described Chat Control as incompatible with
fundamental human rights and stated that the proposal would lead to
<a href="https://www.patrick-breyer.de/en/un-human-rights-commissioner-warns-against-chat-control/" rel="nofollow">mass surveillance and
self-censorship</a>.
Former judges at the European Court of Justice said that <a href="https://www.patrick-breyer.de/wp-content/uploads/2021/03/Legal-Opinion-Screening-for-child-pornography-2021-03-04.pdf" rel="nofollow">the proposal
was in breach of the EU Charter of
Rights</a>
and 465 researchers joined forces to <a href="https://edri.org/wp-content/uploads/2023/07/Open-Letter-CSA-Scientific-community.pdf" rel="nofollow">warn of the
consequences</a>.</p> <p data-svelte-h="svelte-1mcqafc">Faced with massive criticism, Ylva Johansson defended herself. According
to her, everyone else had misunderstood the bill. Chat Control was
certainly not about mass surveillance and everyone making that claim was
simply out to discredit her.</p> <h3 data-svelte-h="svelte-4aqjfz">Chat Control – total monitoring of all EU citizens.</h3> <p>Chat Control is sometimes also called Chat Control 2.0, since existing
legislation already makes it possible for tech companies such as Google
and Meta to scan their users’ accounts for child pornography material.
The fact that there was already a law that allowed tech companies to
scan for illegal content – if they chose to – was something Ylva
Johansson was not slow to mention. She explained that her draft bill was
nothing but an extension of the <a href="https://mullvad.net/en/blog/the-european-commission-does-not-understand-what-is-written-in-its-own-chat-control-bill">scanning that had already been going on
for ten
years</a>.
She also referred to the existing legislation when she said that the EU
would become a free zone for pedophiles unless her bill went through –
as that legislation would expire in the summer of 2024.</p> <p data-svelte-h="svelte-1s6fzw2">Time and time again Ylva Johansson was proven wrong by journalists and
experts. In fact, nothing prevented the EU from extending the existing
law, rather than introducing a new one. And above all: Ylva’s bill was
anything but an extension. The differences between the current law and
the proposed legislation were extreme. In Ylva Johansson’s EU, scanning
would not be voluntary. All messaging services (including encrypted
services such as Signal) would be covered by the law and would be forced
to scan their users’ images, videos and conversations. That would be a
big concern for all those who don’t use Meta or Google to converse
because they are in need of secure communication methods. In other
words, political opponents, whistleblowers, journalists and their
sources, vulnerable people living under secret identities and others,
not to mention people with trade secrets, and those in possession of
sensitive information important for national security. For example, the
European Commission itself uses Signal. Demanding government
transparency (either through so-called backdoors or scanning on the
computer or phone) would open a Pandora’s box to countries with
authoritarian inclinations (and <a href="https://www.politico.eu/article/pegasus-use-5-eu-countries-nso-group-admit/" rel="nofollow">five EU countries have already been
caught</a>
using spyware to monitor political opponents) and would leave the door
wide open for criminals to exploit. But it was not only this that
separated the existing legislation from the draft bill that the European
Commission wanted to introduce.</p> <p data-svelte-h="svelte-cr1ja7">The previous legislation had only allowed scanning for material that had
previously been stamped and registered as child pornography material.
Now, AI would be used to find ‘new material’ and would also look for
grooming attempts. Quite obviously, Chat Control would therefore send
every other citizen of the EU straight into the filtering system.
Holiday photos from the beach, nude photos sent between partners, dirty
text messages – all the things that no AI system can distinguish
between would risk getting caught in a filter that would inevitably
drown any new EU center with endless digital heaps of evidence to
review. Is this a holiday photo of a child or child pornography? Are
these skimpily dressed youngsters 18 or 14? Is this a dirty text message
from a wife to a husband or a grooming attempt? But above all, Chat
Control would mean a tool that could be used to scan for completely
different things.</p> <p data-svelte-h="svelte-j80wc0">When Ylva Johansson was asked whether it would be possible to
communicate safely even after her bill was introduced, she answered
“Yes.” And a whole world of experts asked “How?” Ylva replied that she
had something nobody else had. A digital sniffer dog that could smell
encrypted communication without looking at the content. A sniffer dog
that only reacted to child pornography content – never anything else.</p> <p data-svelte-h="svelte-185mkhq">A group of experts tried to hammer the message home: either encrypted
communication is encrypted (so-called end-to-end encryption, which only
the sender and the recipient can see) or it’s not encrypted. There’s no
‘seeing the content’ without reading it. But Ylva stood by her claim.
She came back to the same argument over and over again. She avoided
answering the questions (she obviously didn’t understand how the
technology worked) but instead turned the direction of the discussion,
saying, for example, that a court order would be required to carry out
scanning, which in itself was deliberately misleading. Firstly, her
scanning would not require an order from a court – it could be one from
another judicial body. And secondly, the key issue was that judicial
body making a decision that would force messaging services to monitor
all their users. So in other words, when Ylva proclaimed “it requires a
court order,” she wasn’t talking about courts and their decisions to
monitor people such as suspected pedophiles. She was talking about how a
service would be forced to permit surveillance. What was required for a
service to be subject to surveillance? Merely that there was a
possibility to use the service to spread child pornography or to groom
children. Which of course means every messaging service on the planet.</p> <p data-svelte-h="svelte-q0uosi">As soon as Ylva Johansson was shown to be in the wrong, she shifted her
focus. But in the end, she always came back to the final refuge: it’s
all about the children. She related anecdotes and referred to figures
that pointed to an exponential increase in child pornography material on
Facebook, for example – even though Facebook itself stated that <a href="https://about.fb.com/news/2021/02/preventing-child-exploitation-on-our-apps/" rel="nofollow">90
percent of all reports come from material previously
distributed</a>.</p> <p data-svelte-h="svelte-1ajxwrs">The European Commission, led by Ylva Johansson, received criticism from
all directions. Police chiefs pointed out that most of the material they
receive today involves <a href="https://www.fokus.se/veckans-fokus/chat-control-sa-ska-techjattarna-skanna-allt-du-skickar/" rel="nofollow">teenagers sending pictures to each
other</a>
and that such reports risk leading the police in the wrong direction.
Scanning tests carried out by European police on existing material
showed that <a href="https://2021.fedpol.report/de/fedpol-in-zahlen/kampf-gegen-padokriminalitat/" rel="nofollow">80-90 percent of all hits were false
positives</a>.
Now, moreover, ‘new material’ would be scanned – which would obviously
mean an impossible administrative burden merely to distinguish between
illegal images and holiday pictures from family days on the beach. The
error rate would clearly be approaching 100 percent. For a European
<a href="https://edri.org/wp-content/uploads/2022/06/European-Commission-must-uphold-privacy-security-and-free-expression-by-withdrawing-new-law.pdf" rel="nofollow">justice system that even today is unable to follow up all the
tips</a>
it receives, this would be devastating. And criminals would, of course,
turn to illegal messaging services. No children would be helped. At the
same time, every EU citizen would have spyware installed on their
phones.</p> <p data-svelte-h="svelte-uesdev">How did Ylva Johansson deal with this information? Not at all. Instead,
like a scratched record, she continued urging everyone to “think of the
children.” She also ordered a survey that said 80 percent of the EU
population supports Chat Control. The problem? The European Commission
used its Eurobarometer series of public opinion surveys in way that
opened it to accusations of blurring the line between research and
propaganda. When asked to comment on the Chat Control survey, the Max
Planck Institute for the Study of Societies concluded that it had a
<a href="https://www.patrick-breyer.de/en/manipulative-eu-opinion-poll-no-justification-for-indiscriminate-chat-control/" rel="nofollow">political agenda and consisted of questions that were
biased</a>
to support the Commission’s plans.</p> <p data-svelte-h="svelte-1fle1l8">Ylva Johansson was employing blatant deception. She used incorrect
figures and biased surveys. In interviews, she was populist and evasive.
But she was forced to resort to these methods. Because it was never
about the children.</p> <h3 data-svelte-h="svelte-1rjbvqn">American tech companies and security services behind the draft bill</h3> <p data-svelte-h="svelte-msqxs2">In September 2023, a major investigative article was published by three
journalists: Giacomo Zandonini, Apostolis Fotiadis, and Luděk Stavinoha.
After seven months of trying to get the European Commission to release
public documents, they finally obtained a piece of material that allowed
them to start putting together the puzzle. The puzzle that <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">revealed the
true stakes behind Chat
Control</a>.
The article, which was published in several European newspapers,
included a letter in which Ylva Johansson wrote to Julie Cordua, CEO of
the American company Thorn: “We have shared many moments on the journey
to this proposal. Now I am looking to you to help make sure that this
launch is a successful one.”</p> <p data-svelte-h="svelte-13zlmxr">Thorn is an American company, formed by actor Ashton Kutcher, which
develops tools that scan for child pornography material. Thorn had sold
software worth millions of dollars to the U.S. Department of Homeland
Security. Ashton Kutcher himself had held video conferences with
European Commission President Ursula von der Leyen, and had given
lectures in the EU on how new technologies can scan encrypted content
without looking at it. The picture of Ylva Johansson’s digital sniffer
dog suddenly became clear.</p> <p data-svelte-h="svelte-1ltneaj">For several years Kutcher lobbied the European Commission (until he was
forced to resign as chairman of Thorn’s board after defending his
acting colleague Danny Masterson when he was convicted of rape). He held
meetings with others at the European Commission and had an extra close
relationship with the Commission’s Eva Kaili (until she <a href="https://www.politico.eu/european-parliament-qatargate-corruption-scandal-updates/" rel="nofollow">was convicted
of
bribery</a>).</p> <p data-svelte-h="svelte-1y1yu4z">So here was an American company in direct contact with the European
Commission. An American company that just happened to sell the
technology that could be used if Chat Control was introduced. In
addition, it was all based on a false premise. The technology Kutcher
and Johansson talked about did not exist. Expert after expert <a href="https://blog.cryptographyengineering.com/2023/03/23/remarks-on-chat-control/" rel="nofollow">condemned
their talk of sniffer
dogs</a>.</p> <p data-svelte-h="svelte-154r4qk">And here’s yet another seedy aspect to this scandal: in the EU
transparency register, Thorn was registered as a charitable organization
– despite selling the technology they were lecturing about in the EU.
The trick of disguising organizations and corporations as charities
would turn out to be a recurring motif.</p> <p data-svelte-h="svelte-14b9drm">Since the draft Chat Control bill was presented, Ylva Johansson has
constantly referred to children’s rights organizations that support her
proposal. She has worked with them in a PR context, as a way to show how
Chat Control has the support of independent, nonprofit organizations
that care about children. A central organization in this work has been
the WeProtect Global Alliance. When Zandonini, Fotiadis, and Stavinoha
published their article, it turned out that the European Commission had
been involved in founding this organization, and that it included
representatives from both tech companies and security services in
different countries. Ylva Johansson’s colleague in the European
Commission, Labrador Jimenez, was on the Board of Directors of
WeProtect, together with Thorn’s CEO Julie Cordua, representatives of
Interpol, and government officials from the US and the UK (the latter
simultaneously pursuing its own monitoring legislation, also using
children as the battering ram). Thorn had put a great deal of money into
WeProtect. The European Commission had contributed one million euros. In
other words, it wasn’t children’s rights organizations that were
supporting Ylva Johansson. It was lobbying organizations set up by the
European Commission to get the bill through.</p> <p data-svelte-h="svelte-3weqe2">The Board of Directors of WeProtect also included representatives from
the Oak Foundation, who, in addition to their involvement in WeProtect,
had also been involved in setting up ECLAG (another charity that
supported the Chat Control proposal). ECLAG was launched just a few
weeks after Ylva Johansson’s draft bill was presented, and Thorn was
also represented on this organization’s board. And there was still
another organization: the Brave Movement, an organization formed a month
before the proposed Chat Control bill was introduced. Brave was launched
with $10 million from the Oak Foundation and a strategy paper
discovered by the journalists stated that “once the EU Survivors
taskforce is established and we are clear on the mobilized survivors, we
will establish a list pairing responsible survivors with Members of the
European Parliament – we will ‘divide and conquer’ the MEPs by
deploying in priority survivors from MEPs’ countries of origin.”</p> <p data-svelte-h="svelte-blzgs8">The Oak Foundation also appeared in a <a href="https://theintercept.com/2023/10/01/apple-encryption-iphone-heat-initiative/" rel="nofollow">article carried out by the
Intercept</a>.
In 2023, an American organization called the Heat Initiative was formed.
On paper, they were a “new child safety group” and the first thing they
did was campaign for Apple to “detect, report, and remove” child
pornography material from iCloud. Apple responded that this would be
something that criminals would be able to exploit and that it could also
lead to a “potential for a slippery slope of unintended consequences.
Scanning for one type of content, for instance, opens the door for bulk
surveillance.”</p> <p data-svelte-h="svelte-1iezc36">The Heat Initiative did not like this answer and fought back with
anti-Apple propaganda on large advertising billboards in American cities
under the theme of ‘think of the children.’ But who was behind the Heat
Initiative, besides the Oak Foundation? Heat was led by a former vice
president at Thorn. The Intercept article also referred to <a href="https://www.engadget.com/2019-05-31-sex-lies-and-surveillance-fosta-privacy.html" rel="nofollow">the
fact</a>
that Thorn was working with Palantir, the big-data company that <a href="https://theintercept.com/2017/02/22/how-peter-thiels-palantir-helped-the-nsa-spy-on-the-whole-world/" rel="nofollow">helped
the NSA mass-monitor the whole
world</a>
and was involved in the <a href="https://www.nytimes.com/2018/03/27/us/cambridge-analytica-palantir.html" rel="nofollow">Cambridge Analytica scandal where Facebook
users’ private messages and
data</a>
were used to influence the presidential election on behalf of Donald
Trump in 2016.</p> <p data-svelte-h="svelte-zfl98n">In other words, the European Commission was involved in funding and
starting up charities with the aim of exploiting existing victims to
emotionally influence EU parliamentarians. In close cooperation with the
tech company providing the technology that would be used in the
implementation of the monitoring. Together with representatives of
non-European security services. As part of a larger apparatus, where the
same tactics were used to influence developments in the United States.</p> <p data-svelte-h="svelte-n7pf3i">At the same time, the real organizations working to counter sexual
crimes against children were wondering why the European Commission was
refusing to talk to them. In <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">the same investigative
report</a>,
Offlimits, Europe’s oldest hotline for vulnerable children, tells how
Ylva Johansson would rather go to Silicon Valley to meet companies
interested in making huge profits than talk to them.</p> <p data-svelte-h="svelte-1gns2le">The same is true of the technical experts. Matthew Green, Professor of
Cryptography at John Hopkins University, said: “In the first impact
assessment of the EU Commission there was almost no outside scientific
input and that’s really amazing since Europe has a terrific scientific
infrastructure, with the top researchers in cryptography and computer
security all over the world.”</p> <p data-svelte-h="svelte-p16k3a">However, Europol was involved in drafting the law, <a href="https://netzpolitik-org.translate.goog/2023/geheime-liste-wie-der-sicherheitsapparat-die-chatkontrolle-praegt/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=sv&amp;_x_tr_pto=wapp" rel="nofollow">together with
security services from other
countries</a>.
In July 2022, Europol wrote that it wanted to be able to use scanning
and surveillance for purposes other than sexual offenses against
children. <a href="https://balkaninsight.com/2023/09/25/who-benefits-inside-the-eus-fight-over-scanning-for-child-sex-content/" rel="nofollow">The European Commission
responded</a>
that it understood the wish but that it had “to be realistic in terms of
what could be expected, given the many sensitivities around the
proposal.” Thorn was also <a href="https://netzpolitik-org.translate.goog/2024/verschluesselung-thorn-brachte-chatkontrolle-auch-fuer-andere-themen-ins-spiel/?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=sv&amp;_x_tr_pto=wapp" rel="nofollow">clear in understanding that the scanning
could later be used for other
purposes</a>:
“When considering regulation or legislation on encryption it should not
be done solely focusing on CSAM. Solutions for detection in encrypted
environments are much broader than one single crime,” <a href="https://www.ftm.eu/articles/ashton-kutcher-s-anti-childabuse-software-below-par" rel="nofollow">the company wrote
in one
document</a>.</p> <p data-svelte-h="svelte-go1tsn">It was later revealed that Europol was looking for <a href="https://balkaninsight.com/2023/09/29/europol-sought-unlimited-data-access-in-online-child-sexual-abuse-regulation/" rel="nofollow">unfiltered access to
the scanned
material</a>:
“All data is useful and should be passed on to law enforcement. There
should be no filtering by the [EU] Centre because even an innocent
image might contain information that could at some point be useful to
law enforcement.”</p> <h3 data-svelte-h="svelte-ksnt3d">European Parliament: “the commission wanted mass surveillance.”</h3> <p data-svelte-h="svelte-g9fw7s">So here was the European Commission, working on legislative proposals
together with a Europol that wanted access to all surveillance,
regardless of whether it contained something illegal or not – simply
because it could be useful to have. In other words, it really wasn’t
about the children.</p> <p data-svelte-h="svelte-ozl4kz">When articles were published about the EU Commission’s horrifyingly
undemocratic approach, Ylva Johansson’s office at the European
Commission responded by advertising on the platform X (formerly
Twitter). They targeted advertisements (pro Chat Control) so that
decision-makers in different countries would see them, but also so that
they would not be seen by people suspected to be strongly against the
proposal. The advertising was also targeted on the basis of religious
and political affiliation and <a href="https://www.wired.com/story/csar-chat-scan-proposal-european-commission-ads/" rel="nofollow">thus violated the EU’s own laws
regarding
micro-targeting</a>.</p> <p data-svelte-h="svelte-1qh87xe">Officials at the highest EU level thus used data collected by big tech
to try to create illegal filter bubbles designed to push through a mass
surveillance proposal. The whole thing ended with Ylva Johansson being
summoned to a hearing in the European Parliament. An almost united
European Parliament was massively critical of Ylva Johansson and her
approach. She was grilled about Thorn’s interference and about the
targeted ads and the EU Ombudsman denounced the European Commission’s
unwillingness to share public documents regarding the relationship with
Thorn (the European Commission had assumed these would be classified
because they risked undermining commercial interests …) Ylva
Johansson’s answer? “Think of the children.”</p> <p data-svelte-h="svelte-pqryii">In November 2023, the <a href="https://www.europarl.europa.eu/doceo/document/A-9-2023-0364_EN.html" rel="nofollow">European Parliament’s final judgment was
delivered</a>.
In an almost historic consensus, all the groups in the Parliament stood
together and said “No” to the bill. At the <a href="https://fortune.com/europe/2023/10/26/eu-chat-control-csam-encryption-privacy-european-commission-parliament-johansson-breyer-zarzalejos-ernst/" rel="nofollow">press conference,
representatives from the Parliament
said</a>:
“This is a slap in the face of the Commission, what we’ve tabled. The
Commission wasn’t focusing on protecting children but wanted mass
surveillance.” Patrick Breyer, who has been the most active opponent in
the EU Parliament, called it a victory for the children, adding “They
deserve an effective response and a rights-respecting response that will
hold up in court.”</p> <p data-svelte-h="svelte-1bpb699">Breyer was referring to the fact that Chat Control would most likely not
hold up in court if the bill had been passed. Just a few months later, a
<a href="https://hudoc.echr.coe.int/?i=001-230854" rel="nofollow">ruling from the European Court of
Justice</a>
ruled that authorities do not have the right to demand access to
end-to-end encrypted communications.</p> <p data-svelte-h="svelte-fxcimz">But the Chat Control proposal wasn’t completely buried just because the
European Parliament had taken a clear stance against it. In the EU, two
bodies are involved in the adoption of legislative proposals made by the
European Commission: the European Parliament and the Council of
Ministers. But while the European Parliament was extremely clear and
unified in its stance, the Council of Ministers was hopelessly unable to
reach an agreement. When new EU elections approached in summer 2024,
they had not yet managed to come to a consensus. However, the Council of
Ministers also began to hesitate about the technology. Ultimately, it
had become evident to most people that Ylva’s digital sniffer dog
didn’t exist. There was no technology that could scan communication
without looking at it. Parts of <a href="https://reclaimthenet.org/eu-officials-dodge-their-own-surveillance-law" rel="nofollow">the Council of Ministers therefore
proposed</a>
that scanning should be excluded for politicians, the police and
intelligence services, as well as anything classified as ‘professional
secrets.’ Obviously, there were politicians who were afraid that their
secrets would leak, but who had nothing against mass surveillance of the
broader population. Patrick Breyer was clear in his response: “these
people are aware that Chat Control involves unreliable and dangerous
snooping algorithms – and yet they are ready to unleash them on us
citizens.”</p> <p data-svelte-h="svelte-pxyeh5">Even Ylva Johansson finally realized that she was defeated. Did she then
go public and announce that Europe would now be blind in the hunt for
pedophiles? Of course not. She quickly and easily did what she had
previously been completely unable to do: she extended the previous
legislation.</p> <h3 data-svelte-h="svelte-1xt5fg6">New attempt at mass surveillance via the Going Dark initiative</h3> <p data-svelte-h="svelte-1g9zhsh">The fact that the European Parliament rejected Chat Control didn’t mean
that attempts to introduce mass surveillance were over. During Sweden’s
EU Presidency in spring 2023, a project called Going Dark was initiated.
The idea from the Swedish Presidency was initially that a so-called High
Level Expert Group would be launched. The task of putting together the
group went to the European Commission, which immediately removed the
‘Expert’ label. Instead of a High Level Expert Group, a High Level Group
was formed. As <a href="https://netzpolitik.org/2023/eu-beraet-ueber-going-dark-hinter-verschlossenen-tueren/" rel="nofollow">the Netzpolitik
newspaper</a>
put it: “Removing the word ‘expert’ is no small detail: special rules
apply to Expert groups, for example when it comes to transparency. Rules
that do not apply to High Level Groups.”</p> <p data-svelte-h="svelte-1rbtxom">Once again, the European Commission chose to start the preparatory work
linked to mass surveillance without allowing experts to play a serious
part in the process. When the group met for the first time, it stated
that <a href="https://data.consilium.europa.eu/doc/document/ST-8281-2023-INIT/en/pdf" rel="nofollow">the group’s purpose was to discuss
methods</a>
to achieve “access to data for effective law enforcement, based on and
guided by the inputs from the EU Member States.”</p> <p data-svelte-h="svelte-50kpr6">Some <a href="https://data.consilium.europa.eu/doc/document/ST-8281-2023-INIT/en/pdf" rel="nofollow">challenges were identified as particularly
pressing</a>:
access to encrypted material (both stored data and communication), data
storage, location data, and anonymization (including VPNs and Darknets).</p> <p data-svelte-h="svelte-160puu6">Once the group was united, <a href="https://home-affairs.ec.europa.eu/networks/high-level-group-hlg-access-data-effective-law-enforcement_en#meetings" rel="nofollow">it was divided into three working
groups</a>:
the first would work with access to data on users’ devices (computer and
mobile), the second group would focus on access to data in the services’
systems (messaging apps, for example), and the third group would discuss
access to data in transit.</p> <p data-svelte-h="svelte-5dnx7i">According to the minutes of the meeting of the Swedish Parliament’s
Committee on European Union Affairs, the group worked <a href="https://www.riksdagen.se/sv/dokument-och-lagar/dokument/eu-namndens-uppteckningar/fredagen-den-1-december_hb0a16/html/" rel="nofollow">“to present
effective recommendations for the accession of the new Commission in
2024 and for those recommendations to be
implemented.”</a></p> <p data-svelte-h="svelte-h9po41">Future legislative proposals from the European Commission could thus be
assumed to be about providing access to data on users’ devices and in
the messaging services’ systems, and to data in transit. Patrick Breyer,
who had worked hard to counter Chat Control, said the group was just an
extension of past offensives and that Going Dark was working to
<a href="https://www.patrick-breyer.de/en/leak-data-retention-and-encryption-eu-governments-going-dark-program-to-attack-citizens-rights-with-pr/" rel="nofollow">introduce illegal mass
surveillance</a>.
When he requested documents from the group’s meetings and a list of the
attendees, <a href="https://fragdenstaat.de/anfrage/june-and-november-meetings-of-the-hleg-on-access-to-data-for-effective-law-enforcement/848493/anhang/document17-participantlistfirstplenary.pdf" rel="nofollow">he received a document with the information blacked out as
if
classified</a>.
The European Commission had thus put together a working group aiming to
achieve mass surveillance of the broader population while not being
transparent about who was part of the group. It was like a scratched
record. Gone was the old excuse “think of the children”, but the goal
was the same.</p> <p data-svelte-h="svelte-1bp1ogo">However, some transparency was obtained through the Swedish Ministry of
Justice, which at Mullvad VPN’s request provided both meeting notes and
information about the Swedish representatives present at the meetings.</p> <p data-svelte-h="svelte-5ugd8h">The first Going Dark meeting was led by two people. One was Olivier
Onidi, who is Deputy Director General directly under Ylva Johansson in
the European Commission. Onidi has expressed that the “valuable” thing
about Chat Control is <a href="https://netzpolitik.org/2021/eu-commission-why-chat-control-is-so-dangerous/" rel="nofollow">“to cover all forms of communication, including
private
communication”</a>,
and he <a href="https://www.svd.se/a/15yzLQ/expertkritik-mot-omstritt-natforslag-i-eu" rel="nofollow">defended Ylva Johansson and Chat Control when he
said</a>:
“I think it’s totally unfair to point this out as a mandatory inspection
of all private communications. That’s not what you have in front of you.
This proposal is a huge improvement over the current situation.”</p> <p data-svelte-h="svelte-18pbg5h">Onidi has also been questioned for <a href="https://www.euractiv.com/wp-content/uploads/sites/2/2020/06/2020.06.10-Letter-to-Commission-Palantir.pdf" rel="nofollow">his meetings with the American
company
Palantir</a>
(notorious for its involvement in US authorities’ illegal mass
surveillance).</p> <p data-svelte-h="svelte-13jylfg">The second person who led the first Going Dark meeting was Anna-Carin
Svensson, international chief negotiator at the Swedish Justice
Department, who, according to WikiLeaks documents in 2010, allegedly
urged the US State Department and the FBI to continue with the current
informal exchange of information between the countries instead of
signing formal agreements. According to the American representatives at
the meeting, it was about <a href="https://www.aftonbladet.se/nyheter/a/m65Oqp/morkade-allt-for-riksdagen" rel="nofollow">withholding information from the Swedish
Parliament</a>:</p> <p data-svelte-h="svelte-c39wdw">“She believed that, given the Swedish Constitution’s requirement to
present matters of importance to the nation to the Swedish Parliament,
and in light of the ongoing controversy over the newly decided FRA law
[FRA, Försvarets radioanstalt, the Swedish National Defence Radio
Establishment, is a Swedish government signals agency], it will be
politically impossible for the Minister of Justice not to let the
Parliament review any data exchange agreements with the United States.
In her opinion, the publication of this could also jeopardize the
informal exchange of information,” the leaked documents said.</p> <p data-svelte-h="svelte-gq4qsx">According to the documents, Anna-Carin Svensson asked the FBI if they
could not continue to make use of the strong but informal arrangements.
When the documents leaked, Svensson denied everything and stated: “I
cannot be held responsible for how Americans express themselves.”</p> <p data-svelte-h="svelte-1d1qeq2">From the Swedish side, the Ministry of Justice was represented at the
Going Dark meetings, but so was the Swedish Security Service (Säpo) and
the Swedish Police Authority. Together with representatives from the
other Member States, they used the High Level Group meetings to discuss
how, through legislation, encrypted services could be required to
provide data in readable format. Several Member States argued that “the
working groups needed to look at solutions that involved ‘legal access
through design’.” This was something that pleased American
representatives.</p> <p data-svelte-h="svelte-15688qj">At the Going Dark meeting on November 21, 2023, a former FBI employee
was also present, who said that “solutions for legal access (to data on
device) should be prioritized” and that “companies needed to have a
responsibility and follow the same rules.” As a former FBI employee, he
also expressed “his gratitude for the fact that the issue was being
pursued within the EU.”</p> <h3 data-svelte-h="svelte-11oputi">European police chiefs: we cannot accept criminals using secure communications.</h3> <p data-svelte-h="svelte-1ge3kom">The Going Dark meetings resulted in an outcry from the assembled police
chiefs of Europe. In April 2024 <a href="https://www.europol.europa.eu/media-press/newsroom/news/european-police-chiefs-call-for-industry-and-governments-to-take-action-against-end-to-end-encryption-roll-out" rel="nofollow">Europol published the
challenge</a>
“European Police Chiefs call for industry and governments to take action
against end-to-end encryption roll-out.” The declaration was a <a href="https://polisen.se/aktuellt/nyheter/nationell/2024/april/europas-polischefer-gar-samman-mot-grov-brottslighet-i-en-digital-varld/" rel="nofollow">“direct
extension of the Going Dark
initiative”</a>
and, together, the European police authorities were clear that although
encryption is “a means of strengthening the cyber security and privacy
of citizens … we do not accept that there need be a binary choice
between cyber security or privacy on the one hand and public safety on
the other. Absolutism on either side is not helpful.”</p> <p data-svelte-h="svelte-drlmm9">It was as if Ylva Johansson’s sniffer dog had caught the scent again. In
the absence of expertise, the Going Dark initiative tried to magic away
the fact that end-to-end encryption is absolute – either you have
secure communication or you don’t.</p> <p data-svelte-h="svelte-15pvrfa">The assembled police chiefs claimed there were <a href="https://www.europol.europa.eu/cms/sites/default/files/documents/EDOC-%231384205-v1-Joint_Declaration_of_the_European_Police_Chiefs.PDF" rel="nofollow">two key factors for
achieving online
security</a>
– which turned out to be direct repetitions of the reasoning in the
Going Dark discussions. Number 1: so-called legal access to the tech
companies’ stored data. Number 2: real-time scanning of illegal activity
in tech companies’ services. Naturally, they said, all this would be
done under strong protection and supervision.</p> <p data-svelte-h="svelte-1ko89e9">Stefan Hector, a representative of the Swedish Police Authority, said
that <a href="https://polisen.se/aktuellt/nyheter/nationell/2024/april/europas-polischefer-gar-samman-mot-grov-brottslighet-i-en-digital-varld/" rel="nofollow">“a society cannot accept that criminals today have a space to
communicate safely in order to commit serious
crimes.”</a>
A week later, it was revealed that <a href="https://www.svd.se/a/8qwGbx/granskning-poliser-lacker-till-gangen" rel="nofollow">the Swedish police had been
infiltrated and were leaking information to
criminals</a>.</p> <p data-svelte-h="svelte-mnpnic">Although <a href="https://www.ohchr.org/sites/default/files/documents/issues/civicspace/resources/civic_space_protection_encryption_brief.pdf" rel="nofollow">the UN classifies encryption as a human
right</a>,
the Going Dark initiative and the European police force are fighting to
smash end-to-end encryption. Their first move came as a reaction to
<a href="https://www.europol.europa.eu/media-press/newsroom/news/european-police-chiefs-call-for-industry-and-governments-to-take-action-against-end-to-end-encryption-roll-out" rel="nofollow">Meta rolling out exactly such
encryption</a>.
The echoes from the Chat Control debate are clear. But it is also an
echo of an older battle.</p> <p data-svelte-h="svelte-fchcvw">The Going Dark initiative is really just an extension of the so-called
crypto war (the war against encryption) that US authorities have been
involved in since the internet began. As Signal’s CEO <a href="https://signal.org/blog/pdfs/ndss-keynote.pdf" rel="nofollow">Meredith
Whittaker said in a keynote
speech</a>:</p> <p data-svelte-h="svelte-1wyi685">“Encryption was essential for the commercial internet. But law
enforcement and security services saw any network resistant to
government surveillance as a threat and a problem.”</p> <p data-svelte-h="svelte-1mez9xt">The US authorities have already tested the backdoors that the European
Going Dark initiative is now seeking. Edward Snowden revealed that <a href="https://archive.nytimes.com/www.nytimes.com/interactive/2013/09/05/us/documents-reveal-nsa-campaign-against-encryption.html" rel="nofollow">the
NSA spent $250 million a
year</a>
getting tech companies to install backdoors in their services, which
also exposed the risks of backdoors. In 2010, Chinese hackers managed
<a href="https://edition.cnn.com/2010/OPINION/01/23/schneier.google.hacking/index.html" rel="nofollow">to use a Google
backdoor</a>
to get into Gmail. The same thing happened in 2005, <a href="https://spectrum.ieee.org/the-athens-affair" rel="nofollow">when state
surveillance of Vodafone was
exploited</a> by outside
actors to bug the Greek Prime Minister, his Foreign Minister, Justice
Minister, and a hundred other government officials.</p> <p data-svelte-h="svelte-5waapy">And when it comes to client-side scanning, it’s also doomed to fail.
Apple, one of the world’s most technologically advanced and wealthy
companies, has poured incredible resources into figuring out if it can
be done in a secure and private way. But when Apple made its effort
public, it took hackers just two weeks to break in. Apple abandoned the
attempt and continue to say no to anyone who asks them to try this again
– <a href="https://blog.cryptographyengineering.com/2023/03/23/remarks-on-chat-control/" rel="nofollow">simply because it’s too easy to hack systems where client-side
scanning is
involved</a>.</p> <p data-svelte-h="svelte-186vxxe">Snowden’s revelations changed the internet in many respects. Encrypted
websites (https) became standard. End-to-end encrypted messaging
services like Signal saw a widespread increase in popularity. Apple
started using strong encryption in its operating systems. From having
virtually free access to people’s internet traffic (if they didn’t use a
trustworthy VPN, that is) and from having been able to read people’s
messages in plain text, the internet now became more difficult for US
authorities to mass monitor.</p> <p data-svelte-h="svelte-4s3l6u">In her <a href="https://signal.org/blog/pdfs/ndss-keynote.pdf" rel="nofollow">lecture</a>,
Meredith Whittaker points to an important point: “Strong encryption was
an important win. But the result of this win was not privacy. Indeed,
the legacy of the crypto wars was to trade privacy for encryption – and
to usher in an age of mass corporate surveillance. Because the power to
enable – or violate – privacy was left in the hands of companies, not
those who relied on their services. Companies that were incentivized to
implement surveillance in service of advertising and commerce.”</p> <p data-svelte-h="svelte-q6flu7">For more than twenty years, so-called commercial mass surveillance has
created some of the richest companies in the world. The fact that Meta
is rolling out end-to-end encryption doesn’t mean they have abandoned
their business model. But it was still sufficient for the European
police chiefs, cheered on by the US authorities, to make a joint
declaration demanding legal access to the content in secure and private
communications. Meredith Whittaker again:</p> <p data-svelte-h="svelte-wzc8j2">“In my view, the ferocity of the current attack on end-to-end
encryption, and other privacy-preserving technologies, is very much
related to a desire by some in government to return to the less fettered
access to surveillance that they see as having lost post-Snowden.”</p> <p data-svelte-h="svelte-3ttgl1">We can see the attack coming in Europe right now. But the movement is
based in the United States. Back in 2014, just a year after Snowden’s
revelations, <a href="https://www.fbi.gov/news/speeches/going-dark-are-technology-privacy-and-public-safety-on-a-collision-course?ref=tailored-access.com" rel="nofollow">FBI Director James Comey
spoke</a>
of how “the challenges of real-time interception threaten to leave us in
the dark, encryption threatens to lead all of us to a very dark place.”</p> <p data-svelte-h="svelte-bca7cd">The US authorities, which in 2014 had recently been caught spying on the
entire world, used a particular expression when they began lobbying to
regain access to easily controlling everything and everyone. FBI
Director Comey talked about “Going Dark.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NoTunes is a macOS application that will prevent Apple Music from launching (289 pts)]]></title>
            <link>https://github.com/tombonez/noTunes</link>
            <guid>40426621</guid>
            <pubDate>Tue, 21 May 2024 10:42:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tombonez/noTunes">https://github.com/tombonez/noTunes</a>, See on <a href="https://news.ycombinator.com/item?id=40426621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/app-icon.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/app-icon.png" alt="noTunes Logo"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/91118a2cb5f7de8f17625b0df69be17c1e0b64456ba02635542eb86538dceb5e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f746f6d626f6e657a2f6e6f74756e6573"><img src="https://camo.githubusercontent.com/91118a2cb5f7de8f17625b0df69be17c1e0b64456ba02635542eb86538dceb5e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f746f6d626f6e657a2f6e6f74756e6573" alt="GitHub release (latest by date)" data-canonical-src="https://img.shields.io/github/v/release/tombonez/notunes"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e91deddbc62cfaedccdad7770a4a0bcbfd1f15f9453d143436059ae697f6fde1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f746f6d626f6e657a2f6e6f74756e65732f746f74616c"><img src="https://camo.githubusercontent.com/e91deddbc62cfaedccdad7770a4a0bcbfd1f15f9453d143436059ae697f6fde1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f746f6d626f6e657a2f6e6f74756e65732f746f74616c" alt="GitHub all releases" data-canonical-src="https://img.shields.io/github/downloads/tombonez/notunes/total"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/73b4795407952c1c447d0262594b54393ea2da3aeecfe0c73f0c7f618fff1047/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746f6d626f6e657a2f6e6f74756e6573"><img src="https://camo.githubusercontent.com/73b4795407952c1c447d0262594b54393ea2da3aeecfe0c73f0c7f618fff1047/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f746f6d626f6e657a2f6e6f74756e6573" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/tombonez/notunes"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notice</h2><a id="user-content-notice" aria-label="Permalink: Notice" href="#notice"></a></p>
<p dir="auto">The certificate used in noTunes prior to version 3.2 is set to expire on the 14th January 2022.</p>
<p dir="auto">To continue using noTunes please update to version 3.2 or greater.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">noTunes</h2><a id="user-content-notunes" aria-label="Permalink: noTunes" href="#notunes"></a></p>
<p dir="auto">noTunes is a macOS application that will prevent iTunes <em>or</em> Apple Music from launching.</p>
<p dir="auto">Simply launch the noTunes app and iTunes/Music will no longer be able to launch. For example, when bluetooth headphones reconnect.</p>
<p dir="auto">You can toggle the apps functionality via the menu bar icon with a simple left click.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install --cask notunes"><pre>brew install --cask notunes</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Direct Download</h3><a id="user-content-direct-download" aria-label="Permalink: Direct Download" href="#direct-download"></a></p>
<p dir="auto"><a href="https://github.com/tombonez/noTunes/releases/download/v3.4/noTunes-3.4.zip">noTunes-3.4.zip</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Set noTunes to launch at startup</h3><a id="user-content-set-notunes-to-launch-at-startup" aria-label="Permalink: Set noTunes to launch at startup" href="#set-notunes-to-launch-at-startup"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Before Ventura:</h4><a id="user-content-before-ventura" aria-label="Permalink: Before Ventura:" href="#before-ventura"></a></p>
<p dir="auto">Navigate to System Preferences -&gt; Users &amp; Groups. Under your user, select "Login Items", click the lock on the bottom left and enter your login password to make changes. Click the plus sign (+) in the main panel and search for noTunes. Select it and click "Add".</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Ventura and later:</h4><a id="user-content-ventura-and-later" aria-label="Permalink: Ventura and later:" href="#ventura-and-later"></a></p>
<ol dir="auto">
<li>Navigate to System Settings</li>
<li>Select General</li>
<li>Select Login Items</li>
<li>Click the + under Open at Login and select noTunes</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Toggle noTunes Functionality</h3><a id="user-content-toggle-notunes-functionality" aria-label="Permalink: Toggle noTunes Functionality" href="#toggle-notunes-functionality"></a></p>
<p dir="auto">Left click the menu bar icon to toggle between its active states.</p>
<p dir="auto"><strong>Enabled (prevents iTunes/Music from opening)</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/menubar-enabled.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/menubar-enabled.png" alt="noTunes Enabled"></a></p>
<p dir="auto"><strong>Disabled (allows iTunes/Music to open)</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/tombonez/noTunes/blob/master/screenshots/menubar-disabled.png"><img src="https://github.com/tombonez/noTunes/raw/master/screenshots/menubar-disabled.png" alt="noTunes Disabled"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hide Menu Bar Icon</h3><a id="user-content-hide-menu-bar-icon" aria-label="Permalink: Hide Menu Bar Icon" href="#hide-menu-bar-icon"></a></p>
<p dir="auto">Right click the menu bar icon and click <code>Hide Icon</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Restore Menu Bar Icon</h3><a id="user-content-restore-menu-bar-icon" aria-label="Permalink: Restore Menu Bar Icon" href="#restore-menu-bar-icon"></a></p>
<p dir="auto"><a href="#quit-notunes">Quit noTunes</a>, run the following command in Terminal and re-open the app:</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults delete digital.twisted.noTunes"><pre>defaults delete digital.twisted.noTunes</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quit noTunes</h3><a id="user-content-quit-notunes" aria-label="Permalink: Quit noTunes" href="#quit-notunes"></a></p>
<p dir="auto">To quit the app either:</p>
<p dir="auto"><strong>With menu bar icon visible</strong></p>
<p dir="auto">Right click the menu bar icon and click quit.</p>
<p dir="auto"><strong>With menu bar icon hidden</strong></p>
<p dir="auto">Quit the app via Activity Monitor or run the following command in Terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="osascript -e 'quit app &quot;noTunes&quot;'"><pre>osascript -e <span><span>'</span>quit app "noTunes"<span>'</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Set replacement for iTunes / Apple Music</h3><a id="user-content-set-replacement-for-itunes--apple-music" aria-label="Permalink: Set replacement for iTunes / Apple Music" href="#set-replacement-for-itunes--apple-music"></a></p>
<p dir="auto">Replace <code>YOUR_MUSIC_APP</code> with the name of your music app in the following command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults write digital.twisted.noTunes replacement /Applications/YOUR_MUSIC_APP.app"><pre>defaults write digital.twisted.noTunes replacement /Applications/YOUR_MUSIC_APP.app</pre></div>
<p dir="auto">Then <code>/Applications/YOUR_MUSIC_APP.app</code> will launch when iTunes/Music attempts to launch.</p>
<p dir="auto">This can be used to open a website too, for example, YouTube Music.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults write digital.twisted.noTunes replacement https://music.youtube.com/"><pre>defaults write digital.twisted.noTunes replacement https://music.youtube.com/</pre></div>
<p dir="auto">The following command will disable the replacement.</p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults delete digital.twisted.noTunes replacement"><pre>defaults delete digital.twisted.noTunes replacement</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code is available under the <a href="https://github.com/tombonez/notunes/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gifski: Optimized GIF Encoder (195 pts)]]></title>
            <link>https://github.com/ImageOptim/gifski</link>
            <guid>40426442</guid>
            <pubDate>Tue, 21 May 2024 10:16:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ImageOptim/gifski">https://github.com/ImageOptim/gifski</a>, See on <a href="https://news.ycombinator.com/item?id=40426442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://gif.ski/" rel="nofollow"><img width="100%" src="https://camo.githubusercontent.com/59e069ea3016455bd8fc71796883f06336a7b6c96c3678e416eca610c7340c3b/68747470733a2f2f6769662e736b692f676966736b692e737667" alt="gif.ski" data-canonical-src="https://gif.ski/gifski.svg"></a></h2><a id="" aria-label="Permalink: " href="#"></a></div>
<p dir="auto">Highest-quality GIF encoder based on <a href="https://pngquant.org/" rel="nofollow">pngquant</a>.</p>
<p dir="auto"><strong><a href="https://gif.ski/" rel="nofollow">gifski</a></strong> converts video frames to GIF animations using pngquant's fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6ddf6a1bcae1beaf5ed5dc074eef0d605b0b704350fd9f28353c7f1521e22cf5/68747470733a2f2f6769662e736b692f64656d6f2e676966"><img src="https://camo.githubusercontent.com/6ddf6a1bcae1beaf5ed5dc074eef0d605b0b704350fd9f28353c7f1521e22cf5/68747470733a2f2f6769662e736b692f64656d6f2e676966" alt="(CC) Blender Foundation | gooseberry.blender.org" data-animated-image="" data-canonical-src="https://gif.ski/demo.gif"></a></p>
<p dir="auto">It's a CLI tool, but it can also be compiled <a href="https://docs.rs/gifski" rel="nofollow">as a C library</a> for seamless use in other apps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download and install</h2><a id="user-content-download-and-install" aria-label="Permalink: Download and install" href="#download-and-install"></a></p>
<p dir="auto">See <a href="https://github.com/ImageOptim/gifski/releases">releases</a> page for executables.</p>
<p dir="auto">If you have <a href="https://brew.sh/" rel="nofollow">Homebrew</a>, you can also get it with <code>brew install gifski</code>.</p>
<p dir="auto">If you have <a href="https://www.rust-lang.org/install.html" rel="nofollow">Rust from rustup</a> (1.63+), you can also build it from source with <a href="https://lib.rs/crates/gifski" rel="nofollow"><code>cargo install gifski</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">gifski is a command-line tool. There is no GUI for Windows or Linux (there is one for <a href="https://sindresorhus.com/gifski" rel="nofollow">macOS</a>).</p>
<p dir="auto">The recommended way is to first export video as PNG frames. If you have <code>ffmpeg</code> installed, you can run in terminal:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ffmpeg -i video.webm frame%04d.png"><pre>ffmpeg -i video.webm frame%04d.png</pre></div>
<p dir="auto">and then make the GIF from the frames:</p>
<div dir="auto" data-snippet-clipboard-copy-content="gifski -o anim.gif frame*.png"><pre>gifski -o anim.gif frame<span>*</span>.png</pre></div>
<p dir="auto">You can also resize frames (with <code>-W &lt;width in pixels&gt;</code> option). If the input was ever encoded using a lossy video codec it's recommended to at least halve size of the frames to hide compression artefacts and counter chroma subsampling that was done by the video codec.</p>
<p dir="auto">See <code>gifski -h</code> for more options.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tips for smaller GIF files</h3><a id="user-content-tips-for-smaller-gif-files" aria-label="Permalink: Tips for smaller GIF files" href="#tips-for-smaller-gif-files"></a></p>
<p dir="auto">Expect to lose a lot of quality for little gain. GIF just isn't that good at compressing, no matter how much you compromise.</p>
<ul dir="auto">
<li>Use <code>--width</code> and <code>--height</code> to make the animation smaller. This makes the biggest difference.</li>
<li>Add <code>--quality=80</code> (or a lower number) to lower overall quality. You can fine-tune the quality with:
<ul dir="auto">
<li><code>--lossy-quality=60</code> lower values make animations noisier/grainy, but reduce file sizes.</li>
<li><code>--motion-quality=60</code> lower values cause smearing or banding in frames with motion, but reduce file sizes.</li>
</ul>
</li>
</ul>
<p dir="auto">If you need to make a GIF that fits a predefined file size, you have to experiment with different sizes and quality settings. The command line tool will display estimated total file size during compression, but keep in mind that the estimate is very imprecise.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<ol dir="auto">
<li><a href="https://www.rust-lang.org/en-US/install.html" rel="nofollow">Install Rust via rustup</a> or run <code>rustup update</code>. This project only supports up-to-date versions of Rust. You may get compile errors, warnings about "unstable edition", etc. if you don't run <code>rustup update</code> regularly.</li>
<li>Clone the repository: <code>git clone https://github.com/ImageOptim/gifski</code></li>
<li>In the cloned directory, run: <code>cargo build --release</code></li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using from C</h3><a id="user-content-using-from-c" aria-label="Permalink: Using from C" href="#using-from-c"></a></p>
<p dir="auto"><a href="https://github.com/ImageOptim/gifski/blob/main/gifski.h">See <code>gifski.h</code></a> for <a href="https://docs.rs/gifski/latest/gifski/c_api/#functions" rel="nofollow">the C API</a>. To build the library, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo build --release"><pre>rustup update
cargo build --release</pre></div>
<p dir="auto">and link with <code>target/release/libgifski.a</code>. Please observe the <a href="https://github.com/ImageOptim/gifski/blob/main/LICENSE">LICENSE</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">C dynamic library for package maintainers</h3><a id="user-content-c-dynamic-library-for-package-maintainers" aria-label="Permalink: C dynamic library for package maintainers" href="#c-dynamic-library-for-package-maintainers"></a></p>
<p dir="auto">The build process uses <a href="https://lib.rs/cargo-c" rel="nofollow"><code>cargo-c</code></a> for building the dynamic library correctly and generating the pkg-config file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo install cargo-c
# build
cargo cbuild --prefix=/usr --release
# install
cargo cinstall --prefix=/usr --release --destdir=pkgroot"><pre>rustup update
cargo install cargo-c
<span><span>#</span> build</span>
cargo cbuild --prefix=/usr --release
<span><span>#</span> install</span>
cargo cinstall --prefix=/usr --release --destdir=pkgroot</pre></div>
<p dir="auto">The <code>cbuild</code> command can be omitted, since <code>cinstall</code> will trigger a build if it hasn't been done already.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">AGPL 3 or later. I can offer alternative licensing options, including <a href="https://supso.org/projects/pngquant" rel="nofollow">commercial licenses</a>. Let <a href="https://kornel.ski/contact" rel="nofollow">me</a> know if you'd like to use it in a product incompatible with this license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">With built-in video support</h2><a id="user-content-with-built-in-video-support" aria-label="Permalink: With built-in video support" href="#with-built-in-video-support"></a></p>
<p dir="auto">The tool optionally supports decoding video directly, but unfortunately it relies on ffmpeg 4.x, which may be <em>very hard</em> to get working, so it's not enabled by default.</p>
<p dir="auto">You must have <code>ffmpeg</code> and <code>libclang</code> installed, both with their C headers installed in default system include paths. Details depend on the platform and version, but you usually need to install packages such as <code>libavformat-dev</code>, <code>libavfilter-dev</code>, <code>libavdevice-dev</code>, <code>libclang-dev</code>, <code>clang</code>. Please note that installation of these dependencies may be quite difficult. Especially on macOS and Windows it takes <em>expert knowledge</em> to just get them installed without wasting several hours on endless stupid installation and compilation errors, which I can't help with. If you're cross-compiling, try uncommenting <code>[patch.crates-io]</code> section at the end of <code>Cargo.toml</code>, which includes some experimental fixes for ffmpeg.</p>
<p dir="auto">Once you have dependencies installed, compile with <code>cargo build --release --features=video</code> or <code>cargo build --release --features=video-static</code>.</p>
<p dir="auto">When compiled with video support <a href="https://www.ffmpeg.org/legal.html" rel="nofollow">ffmpeg licenses</a> apply. You may need to have a patent license to use H.264/H.265 video (I recommend using VP9/WebM instead).</p>
<div dir="auto" data-snippet-clipboard-copy-content="gifski -o out.gif video.mp4"><pre>gifski -o out.gif video.mp4</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cross-compilation for iOS</h2><a id="user-content-cross-compilation-for-ios" aria-label="Permalink: Cross-compilation for iOS" href="#cross-compilation-for-ios"></a></p>
<p dir="auto">The easy option is to use the included <code>gifski.xcodeproj</code> file to build the library automatically for all Apple platforms. Add it as a <a href="https://lib.rs/crates/cargo-xcode" rel="nofollow">subproject</a> to your Xcode project, and link with <code>gifski-staticlib</code> Xcode target. See <a href="https://github.com/sindresorhus/Gifski">the GUI app</a> for an example how to integrate the library.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cross-compilation for iOS manually</h3><a id="user-content-cross-compilation-for-ios-manually" aria-label="Permalink: Cross-compilation for iOS manually" href="#cross-compilation-for-ios-manually"></a></p>
<p dir="auto">Make sure you have Rust installed via <a href="https://rustup.rs/" rel="nofollow">rustup</a>. Run once:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup target add aarch64-apple-ios"><pre>rustup target add aarch64-apple-ios</pre></div>
<p dir="auto">and then to build the library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rustup update
cargo build --lib --release --target=aarch64-apple-ios"><pre>rustup update
cargo build --lib --release --target=aarch64-apple-ios</pre></div>
<p dir="auto">The build will print "dropping unsupported crate type <code>cdylib</code>" warning. This is normal and expected when building for iOS (the cdylib option exists for other platforms).</p>
<p dir="auto">This will create a static library in <code>./target/aarch64-apple-ios/release/libgifski.a</code>. You can add this library to your Xcode project. See <a href="https://github.com/sindresorhus/Gifski">gifski.app</a> for an example how to use libgifski from Swift.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an AI game studio: what we've learned so far (207 pts)]]></title>
            <link>https://braindump.me/blog-posts/building-an-ai-game-studio</link>
            <guid>40426382</guid>
            <pubDate>Tue, 21 May 2024 10:05:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://braindump.me/blog-posts/building-an-ai-game-studio">https://braindump.me/blog-posts/building-an-ai-game-studio</a>, See on <a href="https://news.ycombinator.com/item?id=40426382">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Post" data-framer-component-type="RichTextContainer" name="Post"><p>Hi!</p><p>Braindump is our attempt to imagine what game creation could be like in the brave new world of LLMs and generative AI. We want to give you an entire AI game studio, complete with coders, artists, and so on, to help you create your dream game.</p><p>With Braindump, you build top-down/2.5D games or interactive worlds by simply typing prompts. For example, typing “Create a Starfighter that can shoot lasers and drop BB-8 bombs” will generate 3D models, game data, and scripts that make your prompt come to life. You can then instantly play your game, and even invite a friend to play with you.</p><video autoplay="" data-framer-asset="data:framer/asset-reference,BdVWQwjP0BwghSdVMelEWugRfw.mp4" loop="" muted="" playsinline="" src="https://framerusercontent.com/assets/BdVWQwjP0BwghSdVMelEWugRfw.mp4"></video><p>As we’ve worked on Braindump for a while now, we figured it’s time to do an update, share some of our learnings, and see if we can get some feedback. You can also <a href="https://braindump.me/signup" target="_blank" rel="noopener">sign up for the alpha here</a>, if you’d be interested in trying out the product yourself and give direct feedback on it. Everyone is more than welcome to join our <a href="https://discord.gg/braindump" target="_blank" rel="noopener">Discord</a> and chat directly with us there. And there are some <a href="https://www.tiktok.com/@braindump.me" target="_blank" rel="noopener">more videos on our TikTok</a> account.</p><p>Anyway, let’s jump into it!</p><h2>From First Experiments to Today</h2><p>Braindump started around six months ago with some humble prototypes, using SVGs to represent game units.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,ejKVi4hST2rOJl6eNwTyvBg1ofQ.png" data-framer-height="863" data-framer-width="1657" height="431" src="https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png" srcset="https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ejKVi4hST2rOJl6eNwTyvBg1ofQ.png 1657w" width="828" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>Since then, we’ve added 3D model generation, multiplayer, and we’ve reworked the UX countless times. Most of all, we’ve iterated a lot on how the prompting works. Here’s what it looks like as of now:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,35DYfRVGfKt0rFCJJrSQtjgcE.png" data-framer-height="1329" data-framer-width="2563" height="664" src="https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png" srcset="https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=512 512w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/35DYfRVGfKt0rFCJJrSQtjgcE.png 2563w" width="1281" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>There are currently a couple of core operations in Braindump:</p><ul><li data-preset-tag="p"><p>You can define units, abilities, and attributes, such as “an Orc with 50 HP that can place a magic wand.”</p></li><li data-preset-tag="p"><p>You can populate the game map with objects, for example, “place twenty Orcs in a circle.”</p></li><li data-preset-tag="p"><p>You can create game rules and logic, i.e., “When Orcs reach 0 health, create a ghost Orc in their place.”</p></li><li data-preset-tag="p"><p>You can create new 3D models, i.e., “I want my Orc to be pink and have fluffy ears.” We’re using <a href="https://www.meshy.ai/" target="_blank" rel="noopener">Meshy</a> to generate the 3D models.</p></li></ul><p>All of this is accessible through a unified natural language prompting interface. Prompts can be started from anywhere and include all kinds of context, including positions, objects, and more. Your prompt is translated into code by a LLM and is executed by the runtime to update the game state. Building and playing is online, multiplayer and instant. Jump in, send a link to a friend, and have fun building together!</p><h2>Challenge 1: Designing UX for Prompting</h2><p>There are two big problems with using LLMs to help you build things: 1) how do you get the LLM to consistently do what you want and 2) what is the best UX for interacting with the LLMs? Let’s talk about the latter problem first.</p><p>We experimented with many UX paradigms before settling on our current iteration. One of the first ones we tried was “generate a game from a description,” but it quickly broke down. Imagine that you’ve hired a game studio to build a game for you, but you only get <em>one</em> chance at <em>perfectly</em> specifying your game to them. There are zero feedback rounds; they will build their best interpretation of what you’ve specified, which is unlikely to be what you <em>imagined</em>. LLMs have the same problem.</p><p>In fact, a lot of problems with LLMs are strikingly familiar if you just replace the word “LLM” with “developer” or “game studio.” In many cases, we’ve found that the reason it can’t do what we want it to do is simply because we haven’t given it the right information or context to accomplish the task.</p><p>This also goes for users. Working with an LLM is a bit like being a boss; you need to clearly articulate what you want, as it can’t read your mind. The difference from a human employee, though, is that the LLM will carry on regardless of the input and dream up whatever response it can so that it can output <em>something</em>. We’re looking into how we can get the LLM to ask for clarification, but it’s a balancing act as you <em>do</em> want it to guess a lot of the time. If I type “Create an Orc,” then I don’t want to be asked 20 follow-up questions to exactly specify the nature of the Orc; I just want it to take a guess. Figuring out exactly when a user expects it to ask for clarification and when not is one of our bigger challenges.</p><p>After experimenting with taking a whole description and generating a game from it, we switched to a more iterative approach. By building up the game over many prompts, you have a chance to go into more detail or iterate on things. For example, I may ask it to “Create an Orc,” and once it’s done that, I may start adding details to that Orc: “Make it afraid of ducks” or “Give it sunglasses.”</p><p>We also quickly discovered that we wanted to be able to click on things to “talk” about them because, without that, it’s quite hard to describe what you’re talking about. “That tree in the middle of the forest, next to the big rock, and two meters down from the lake” gets tedious quickly compared to just clicking the tree and saying “change <em>this</em> tree.” We’re still trying to find the right balance between prompting and traditional controls, and are actively experimenting in both directions.</p><h2>Challenge 2: Designing a Game API for LLMs</h2><p>Alright, we’ve covered how to interact with the LLM from a UX perspective. Let’s talk about the code it writes for us now.</p><p>Initially, we tried to get GPT to generate code for an existing game engine (we tried Three.js, a couple of JavaScript game engines, and working with the DOM directly), but we realized that, even though it’s quite good at generating code snippets, it struggles with bigger pieces of software and building and maintaining software architecture.</p><p>Instead, we built a much more streamlined “game API” in TypeScript, which provides as much structure as possible, so that the LLM can focus on filling in code and data.</p><p>As an example, here’s the code generated for the prompt "Create a blue car that drives around randomly”. We don’t permit it to write classes, functions or any other “code structure”. Instead, we force it to create “blueprints” and output “rules”, which obey a strict structure. These are also shown to the user in the UI so you know what it’s done:</p><div><pre translate="no"><code><span>const</span> <span>carModelId</span><span>:</span> ModelId = <span>lookupModelId</span><span>(</span><span>'a blue car model'</span><span>)</span><span>;</span>
<span>const</span> <span>carBlueprintId</span><span>:</span> BlueprintId = <span>createUnitBlueprint</span><span>(</span><span>{</span>
    <span>name</span><span>:</span> <span>'Blue Car'</span><span>,</span>
    <span>tags</span><span>:</span> <span>'car,blue'</span><span>,</span>
    <span>modelId</span><span>:</span> <span>carModelId</span><span>,</span>
    <span>blocking</span><span>:</span> <span>true</span>
<span>}</span><span>)</span><span>;</span>
<span>const</span> <span>spawnPosition</span><span>:</span> Vec2 = <span>{</span> <span>x</span><span>:</span> <span>0.4236507</span><span>,</span> <span>y</span><span>:</span> <span>1.8517406</span> <span>}</span><span>;</span>
<span>const</span> <span>carUnitId</span><span>:</span> UnitId = <span>spawnUnitFromBlueprint</span><span>(</span><span>carBlueprintId</span><span>,</span> <span>spawnPosition</span><span>,</span> <span>0</span><span>)</span><span>;</span>

<span>createOrUpdateRule</span><span>(</span><span>{</span>
    <span>type</span><span>:</span> <span>'periodic'</span><span>,</span>
    <span>id</span><span>:</span> <span>'rules/move_randomly'</span><span>,</span>
    <span>name</span><span>:</span> <span>'Blue Car moves randomly'</span><span>,</span>
    <span>period</span><span>:</span> <span>1</span><span>,</span>
    <span>callback</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
        <span>const</span> <span>carPosition</span><span>:</span> Vec2 = <span>getUnitPosition</span><span>(</span><span>carUnitId</span><span>)</span><span>;</span>
        <span>const</span> <span>newPosition</span><span>:</span> Vec2 = <span>vec2Add</span><span>(</span><span>carPosition</span><span>,</span> <span>vec2MulFactor</span><span>(</span><span>vec2FromAngle</span><span>(</span><span>getRandomNumber</span><span>(</span><span>)</span> * <span>Math</span>.<span>PI</span> * <span>2</span><span>)</span><span>,</span> <span>5</span><span>)</span><span>)</span><span>;</span>
        <span>orderUnitMove</span><span>(</span><span>carUnitId</span><span>,</span> <span>newPosition</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span><span>)</span><span>;</span></code></pre></div><video autoplay="" data-framer-asset="data:framer/asset-reference,xzcHoMcldQ2M8x24oV5i62ig.mp4" loop="" muted="" playsinline="" src="https://framerusercontent.com/assets/xzcHoMcldQ2M8x24oV5i62ig.mp4"></video><p>From our Game API we generate type definitions (.d.ts) which are fed to GPT in the system prompt. GPT seems to get the cue; it consistently uses our API and mostly does it correctly on the first try.</p><p>The type checking also has provided us with a surprise benefit; GPT will actually try to self-correct when it sees that it has made a type error. All we had to do was give it the errors, and as soon as we did, it automatically started with this behavior. For the car prompt above, the LLM made an error in its first attempt, and then self-corrected with its second attempt:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png" data-framer-height="240" data-framer-width="864" height="120" src="https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png" srcset="https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png?scale-down-to=512 512w,https://framerusercontent.com/images/U0HxXz1X1j1rC5a2CLOnnqXkc2Q.png 864w" width="432" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"><img alt="" data-framer-asset="data:framer/asset-reference,yHE1ZFWM6hHll3v4X5WeLGMHomc.png" data-framer-height="1162" data-framer-width="1322" height="581" src="https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png" srcset="https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png?scale-down-to=512 512w,https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/yHE1ZFWM6hHll3v4X5WeLGMHomc.png 1322w" width="661" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><h2>Generated Macros</h2><p>The game API has also opened up another interesting UX flow: generated macros.</p><p>Normally, a macro in an application is a small program that lets you automate some task. For example, converting every second row in a spreadsheet from one currency to another.</p><p>In our system, however, <em>all</em> prompts generate code, and that code can automate pretty much anything covered by the game API.</p><p>As an example, I can type prompts such as “Place a tent next to each campfire,” and GPT will happily do so:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,bj3BJS8IuBjDFphytGtjJRro4.png" data-framer-height="1302" data-framer-width="1280" height="651" src="https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png" srcset="https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png?scale-down-to=1024 1006w,https://framerusercontent.com/images/bj3BJS8IuBjDFphytGtjJRro4.png 1280w" width="640" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>Or I can have it automate tedious tasks, such as “Create five different cats with different stats.”</p><p><img alt="" data-framer-asset="data:framer/asset-reference,Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png" data-framer-height="402" data-framer-width="1894" height="201" src="https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png" srcset="https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png?scale-down-to=512 512w,https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/Zlxsdn6phRIGvBA2k4RW2z7H2XQ.png 1894w" width="947" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>I can even ask it questions that require computation to answer, such as “How many tents are there that are facing north?” GPT will generate some code that counts the tents based on their rotation:</p><div><pre translate="no"><code><span>const</span> <span>tentsFacingNorth</span> = <span>getAllUnitIds</span><span>(</span><span>)</span>
    .<span>filter</span><span>(</span><span>unitId</span> <span>=&gt;</span> <span>getUnitString</span><span>(</span><span>unitId</span><span>,</span> <span>"unitStrings/tags"</span><span>)</span>?.<span>includes</span><span>(</span><span>"tent"</span><span>)</span><span>)</span>
    .<span>filter</span><span>(</span><span>unitId</span> <span>=&gt;</span> <span>Math</span>.<span>abs</span><span>(</span><span>getUnitRotation</span><span>(</span><span>unitId</span><span>)</span> % <span>(</span><span>2</span> * <span>Math</span>.<span>PI</span><span>)</span><span>)</span> &lt; <span>0.01</span><span>)</span>.<span>length</span><span>;</span>

<span>// Return the count of tents facing north.</span>
<span>tentsFacingNorth</span><span>;</span></code></pre></div><p>This opens up a whole new way of working. It’s a bit strange at first, as we’re not really used to being able to do these things with programs. Once you get used to it, however, you can find creative ways to complete what would otherwise be very tedious tasks in a matter of seconds.</p><h2>Collaborative Editing with AI</h2><p>We wanted everything in Braindump to be multiplayer: both the creation of games and the playing of games. We’ve supported multiplayer editing from the very first version. At the beginning, we just had one big chat, which everyone could contribute to. This proved chaotic, though, even with just two people. The main problem is that you’re often working on two different things, which may not be relevant to each other. This confused both users and GPT!</p><p>After trying several solutions, we converged on something we call “threads.” This lets you initiate a prompt from anywhere in the world, like this:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,YzBLvgAuGIYy0XUDTOsPFSZINic.png" data-framer-height="472" data-framer-width="1050" height="236" src="https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png" srcset="https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png?scale-down-to=512 512w,https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/YzBLvgAuGIYy0XUDTOsPFSZINic.png 1050w" width="525" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>But you can also refine or add to that prompt when needed, like this:</p><p><img alt="" data-framer-asset="data:framer/asset-reference,tdyU9ohU72jARA3t4En23yZI.png" data-framer-height="400" data-framer-width="944" height="200" src="https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png" srcset="https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png?scale-down-to=512 512w,https://framerusercontent.com/images/tdyU9ohU72jARA3t4En23yZI.png 944w" width="472" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p><p>The thread is given the latest game state to begin with, but it doesn’t have the full history of the project. Multiple threads can be “running” at the same time, but only one prompt can be running per thread at a time.</p><p>So far, this has worked pretty well. In our testing, we’ve had five people working in the same world simultaneously, which was definitely a bit chaotic but still functional. We’re actively exploring ways to help users coordinate effectively.</p><h2>Benchmarking and Testing</h2><p>To evaluate the performance of our prompt engine, we’ve developed a benchmarking tool. The tool runs dozens of scenarios we’ve specified, each with their own prompts, and then uses GPT to evaluate the success of those prompts.</p><p>Here’s an example of a benchmark:</p><div><pre translate="no"><code><span>{</span>
    <span>"user"</span><span>:</span> <span>[</span><span>"Give the player an ability to drop a stone"</span><span>]</span><span>,</span>
    <span>"expected"</span><span>:</span> <span>[</span>
        <span>"A new ability should have been created, and it should be called something like 'drop stone'."</span><span>,</span>
        <span>"The player blueprint should have a 'drop stone' ability assigned to it, at slot 0."</span><span>,</span>
        <span>"When the ability is invoked, the stone should be placed at the feet of the player."</span>
    <span>]</span>
<span>}</span></code></pre></div><p>As you can see, we simply specify the “expected” conditions using natural language. A second GPT, “the evaluator” (with its own system prompt), is provided these conditions, the state of the simulation at completion, and any errors encountered, and is asked to judge whether or not the test was successful, including a critique of what went wrong.</p><p>Our test suite is in its early days still, but we’re constantly adding more and more tests as we discover new prompting styles and failure cases.</p><p>(Just as we were finishing up this blog post, GPT-4o was released and our test suite went from 80% successful to 91% successful)</p><h2>Why We’re Building Braindump</h2><p>Personally, I’ve always loved games and creativity. I learned to code so that I could create games. To me, generative AI is naturally the next step in productivity enhancement; with it, you can simply do more. As big studios are getting more and more conservative with the games they build, I’m excited to empower small groups or even individuals to build their dream games. I want to see what crazy ideas people come up with and realize when they have an entire AI game studio at their fingertips.</p><h2>Next Up</h2><p>Braindump is just getting started. Although it’s pretty good at executing “commands” right now (“Create a cat”), we know that we can expand that to handle much more vague or “big” tasks as well. Some of the things we’re looking into are:</p><ul><li data-preset-tag="p"><p>Supporting “bigger” prompts through planning</p></li><li data-preset-tag="p"><p>Getting GPT to stop guessing and instead ask the user for clarification</p></li><li data-preset-tag="p"><p>Improving code quality by making GPT critique its own work</p></li><li data-preset-tag="p"><p>Improving discoverability and inspiration (“what can I build with this?”)</p></li><li data-preset-tag="p"><p>Improving game engine capabilities in an LLM-amenable way</p></li></ul><h2>Wrapping up</h2><p>If you’ve stuck with me all the way down here, then thank you for reading this, and I hope it was interesting!</p><p>If you would like to try Braindump yourself, then make sure to <a href="https://braindump.me/signup" rel="noopener">sign up for alpha testing</a>. We’re letting in the first few people in the coming weeks. You can also follow us on <a href="https://discord.gg/braindump" target="_blank" rel="noopener">Discord</a>, <a href="https://x.com/braindumpme" target="_blank" rel="noopener">Twitter</a>, <a href="https://www.tiktok.com/@braindump.me" target="_blank" rel="noopener">TikTok</a>, and <a href="https://www.youtube.com/@braindumpincorporated" target="_blank" rel="noopener">YouTube</a>, where we’ll be posting updates.</p><p>Hope you’ve enjoyed it, and until next time!</p><p>/Fredrik Norén, CPTO Braindump</p><h3>About Us</h3><p>We’re a small company based in Stockholm, Sweden. There are currently six of us with many years of experience from everything from Spotify and Snapchat to AAA game studios. You can find us on <a href="https://www.linkedin.com/company/braindump" target="_blank" rel="noopener">LinkedIn</a>.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,2sJeT9jfDrpvmworc3n9PO2HjJc.png" data-framer-height="732" data-framer-width="1261" height="366" src="https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png" srcset="https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png?scale-down-to=512 512w,https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/2sJeT9jfDrpvmworc3n9PO2HjJc.png 1261w" width="630" data-framer-original-sizes="" sizes="(min-width: 1200px) 100vw, (min-width: 810px) and (max-width: 1199px) 100vw, (max-width: 809px) 100vw"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reversing Choplifter (127 pts)]]></title>
            <link>https://blondihacks.com/reversing-choplifter/</link>
            <guid>40425905</guid>
            <pubDate>Tue, 21 May 2024 09:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blondihacks.com/reversing-choplifter/">https://blondihacks.com/reversing-choplifter/</a>, See on <a href="https://news.ycombinator.com/item?id=40425905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<h4>Because it seemed like a good idea at the time.</h4>
<p>The Apple II line of computers had an amazing run, from 1977 to 1993. In that time, hundreds of thousands of pieces of software were written for it, including many tens of thousands of games. Like any platform, however, the number of truly&nbsp;great games within that range is much smaller. If you ask any former (or current) Apple II user what the best five games on the platform are, there would be variation of course, but one game would be on everyone’s list: Choplifter.</p>
<p>Choplifter was written by Dan Gorlin in 1982, and published by Brøderbund. That date, 1982, is especially noteworthy. The game came out just a few years after the original Apple II did, and it remained one of the best games on the platform for sixteen years. On most platforms, the games get better over time as programmers learn the hardware and get better at squeezing more out of it. Certainly lots of that happened on the Apple II as well. Towards the end of the run, some truly astonishing games like Knights of Legend and Space Rogue came out. These are games that people would not have thought technically possible on the machine in 1977. However in terms of pure <em>fun</em>, Choplifter remained hard to beat for sixteen years, and it was developed so early in the life of the Apple II that good development tools didn’t even exist yet. It’s an amazing piece of work by Dan Gorlin, and it was an honour and a pleasure to pick it apart to see how it works.</p>
<p>What really impressed me is that you can see how much effort he put into tuning the gameplay. The chopper feels really good when you fly for good reason. There is a lot of code doing a&nbsp;lot of little fudging of the physics and it’s clear it was all to make things feel better. As a game developer, I know what smoothness fudging looks like, and the chopper control/physics code is full of it. Furthermore, there are a lot of dynamic tuning mechanisms built in, as you’ll see below. This speaks to Dan having spent a lot of time massaging numbers to make things fun. Dan was not only a great programmer, he was a great game designer. This is why Choplifter is the masterpiece that it is.</p>
<p>The full source is available at&nbsp;<a href="https://github.com/blondie7575/ChoplifterReverse">https://github.com/blondie7575/ChoplifterReverse</a> . I won’t be going through it line by line here, because it’s been (I think) very thoroughly commented in the code itself. What I’ll do here is talk about the broad strokes of what was interesting, and my process for doing this. The source includes a makefile which will build and run into an identical binary of the original Choplifter. Also note that the source includes a custom ProDOS loader that I wrote to replicate the behaviour of the original in a more modern environment. I did not reverse engineer Dan’s custom floppy format or copy-protected loader. More on this in the Caveats section below.</p>
<p><a href="https://blondihacks.com/wp-content/uploads/2024/05/Title.jpg"><img src="https://blondihacks.com/wp-content/uploads/2024/05/Title.jpg" alt="" width="560" height="384" srcset="https://blondihacks.com/wp-content/uploads/2024/05/Title.jpg 560w, https://blondihacks.com/wp-content/uploads/2024/05/Title-300x206.jpg 300w" sizes="(max-width: 560px) 100vw, 560px"></a>￼</p>
<h2>Why Reverse Engineer It?</h2>
<p>The Apple II is still alive and well today, with a large and active retro-enthusiast community around it. Lots of new games are being written for it, and lots of programmers are still interested in it. If you’re a programmer new to the platform who wants to write games for it, however, resources are fewer. There are not a lot of full games with source code online for you to learn from. Looking at the structure and techniques in an existing successful game is one of the best ways to learn to write your own games. I myself have written a couple of Apple II things, but really wanted to see how the pros did it back in the day, so this is for my learning as much as anyone else.</p>
<p><em>Side Note</em>:&nbsp;I can’t continue without mentioning the <a href="https://github.com/RobertBaruch/lode_runner_reveng">big Lode Runner reverse engineering</a> that was done fairly recently also. Amazingly I did not know about this until after I had done mine, but I love the way he did his (as a “literate programming document”). Lode Runner is probably the other game that will be on every single Apple II top five list, so in some ways it’s luck that I chose a different one than he did, not knowing about his project.</p>


<p>Aside from it being a Very Important™ game on the platform, it’s an excellent candidate for reverse engineering for a number of other reasons:</p>
<ol>
<li>It’s a single-loading game. Once the game is booted, it never touches the disk again. This was typical of early Apple II games, when they were small enough to fit entirely in RAM. However the Apple II disk drive is very fast, so unlike the Commodore 64, single-load games went away quickly. C64 games did a lot of work to stay single-load since the C64 drive is so slow, but on the Apple II, running back to the disk to load your title screen, menu system, or new levels was no big deal. Reverse engineering a single-load game is vastly easier, though, because it means I can dump the contents of RAM and know I have the full working game. If can I generate source code that compiles to that exact RAM image, I will know it is correct. As you’ll see, that’s precisely what I did. .<br>
.</li>
<li>It’s a 48k game. Choplifter is so early that it was built to run on the early 48k Apple II and Apple II+ machines. This means it doesn’t use auxiliary memory or language card memory, and the graphics are the much simpler High Res mode (as opposed to the nightmarish Double Hi-Res used in later games). Being a 48k game also means it’s, well, smaller. The smaller the better when you’re faced with deducing the exact purpose of every single byte. .<br>
.</li>
<li>I’ve never done this before. I have zero experience reverse-engineering Apple II games, so I wanted to give myself the best possible chance for success. An early single-loading hi-res game fits the bill. Choplifter is also a small game in terms of gameplay. There’s only one level, only three enemies, and you can play it to completion in a few minutes (if you’re very good at it, which you won’t be at first). I should say that while I have no experience reverse-engineering Apple II games, I do have decades of experience writing software for the Apple II. Without that, I doubt I could have done this. You need to know the platform backwards and forwards to do this. Or at least that helps a lot. .<br>
.</li>
<li>It’s one of my favourite games. This matters because that means I know every inch of the gameplay. That turns out to be hugely helpful when reverse-engineering it, because you know what you’re looking for in the code. You know the game will have a routine to animate a waving man because if you’ve played the game a lot, you know that sometimes the little men wave at you and you know roughly how often they do it. That kind of domain-knowledge made all the difference in the world. It would be enormously difficult to reverse engineer a game that you didn’t know anything about gameplay-wise. I should also say that I was a professional game developer for almost thirty years. That was also a huge help because all games have the same basic structure and need the same things, in broad terms. More on this later.</li>
</ol>
<h2>The Tools</h2>
<p>While it would be technically possible to do a reverse engineer like this in 1982, I’m really glad I did it in 2024. Here in the Crazy Science Fiction Future, we have unbelievably powerful tools. Because the Apple II is so small compared to modern machines, we can pick it apart with incredibly deluxe tools that virtually eliminate all repetitive tasks, allow you to test nearly any hypothesis with a couple of clicks, and you never have to reboot or repair a corrupted floppy disk. Here are the main tools I used for this job:</p>
<ol>
<li><a href="https://www.virtualii.com/">Virtual II</a>. This is (in my opinion) hands-down the best Apple II emulator. Not only is it seemingly cycle-perfect in every way as an emulator, it also has a whole range of powerful development tools built in. You can view memory, edit memory, dump memory to files, set breakpoints, set watchpoints on memory locations, single step through code, step into and out of subroutines, disassemble code… the list goes on. This is stuff that a developer in 1982 would have <em>killed</em> for. Literally, there would be bodies and all Apple II games would be written from federal prisons. That’s how much better modern tools are. I used just about every debugging/development feature Virtual II has at various times during this effort, and I can honestly say I could not have done this without it. Or at least, I would have given up in frustration without it. ￼<br>
<a href="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM.png"><img src="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-600x380.png" alt="" width="600" height="380" srcset="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-600x380.png 600w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-300x190.png 300w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-768x487.png 768w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-1536x973.png 1536w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.47.03-AM-2048x1298.png 2048w" sizes="(max-width: 600px) 100vw, 600px"></a><br>
.</li>
<li><a href="https://hexfiend.com/">HexFiend</a>. One thing you need when dealing with old computers is a good hex editor. They aren’t common anymore because modern development rarely requires it. Lucky for me, Mac OS has a really great one. HexFiend not only lets you open and edit files in hex with all kinds of helpful formatting options, it also allows you to <em>compare</em> binary files (like diff-ing source code, but binary). This made all the difference at the end, as you’ll see. ￼<br>
<a href="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM.png"><img src="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-526x1024.png" alt="" width="316" height="615" srcset="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-526x1024.png 526w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-154x300.png 154w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-768x1496.png 768w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-789x1536.png 789w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM-600x1168.png 600w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.48.03-AM.png 952w" sizes="(max-width: 316px) 100vw, 316px"></a><br>
.</li>
<li><a href="https://cc65.github.io/doc/da65.html">da65</a>. Of course, I needed a 6502 disassembler. There are lots out there, but I am partial to the cc65/ca65 toolchain for modern 6502 cross-development. These tools are a little long in the tooth, but I still like them. The disassembler in this package is da65, and it worked great for me. It’s not fancy- it just takes a file and disassembles it at the origin address of your choice. However, it’s quite good at making labels for you (both internal and external) and I found it to be pretty robust at handling tracking problems. One of the challenges with 6502 disassembly is that if your disassembler gets off by one byte, all of a sudden the code is gibberish but it can still look valid (leaving you totally confused). In a reverse-engineer, this is a real problem. How can it get off by one byte? Well, one way is inline parameters for functions. This is where you put data byte constants inline with your 6502 instructions, to be used by subroutines. This was not a super common technique on the Apple II (although ProDOS did it with its Machine Language Interface), but guess what- Choplifter did a lot of it. I still had to help da65 along with this, but it handled this much better than, say Virtual II’s built in disassembler does. da65 is actually pretty good at figuring out that a string of two or three bytes are not real code, whereas Virtual II will steadfastly insist every byte is code. ￼ I should say that this is really helped along by Choplifter being an Apple II+ game. That means it uses only the NMOS 6502 instruction set, which has a lot more unused opcodes than the later CMOS 65C02 does. Setting da65 to look only for the earlier opcode set really helped it sort out the real code from the junk.<br>
<a href="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM.png"><img src="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM-600x705.png" alt="" width="600" height="705" srcset="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM-600x705.png 600w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM-255x300.png 255w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM-768x902.png 768w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.50.53-AM.png 964w" sizes="(max-width: 600px) 100vw, 600px"></a><br>
.</li>
<li><a href="http://gimp.org/">GIMP</a>. This might seem like an unexpected choice, but having an image editor that can view images with no interpolation turns out to be very helpful. When you get into reverse engineering rendering code, you can take screenshots and see what pixel various game elements are sitting on. Then when you find code rendering on that pixel, you know what’s being rendered. I spent a lot of time in GIMP, zoomed way in on screenshots from Virtual II, picking apart the pixels. ￼<br>
<a href="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM.png"><img src="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-600x388.png" alt="" width="600" height="388" srcset="https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-600x388.png 600w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-300x194.png 300w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-768x497.png 768w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-1536x994.png 1536w, https://blondihacks.com/wp-content/uploads/2024/05/Screenshot-2024-05-13-at-10.52.50-AM-2048x1326.png 2048w" sizes="(max-width: 600px) 100vw, 600px"></a></li>
</ol>
<h2>The Disk Image</h2>
<p>The first decision I had to make was, which disk image should I start with? Like any popular game, there are a lot of disk images of Choplifter out there. Most of them are terrible. As on all computers back then, every single Apple II game was cracked and the popular ones were cracked a lot by a lot of people. Many of these cracks are terrible. The crackers would remove parts of the game to put in splash screens containing their bragging, BBS phone numbers, and crudely drawn pixelated breasts. All sorts of other terrible things would be done to games if it made cracking them quicker, or the final game smaller for easier BBS uploading and distribution. Cracking games was about quantity and speed, not quality.</p>
<p>I wanted to reverse engineer the “purest” form of the game I could, but I also didn’t want to deal with a copy-protected binary that was trying to deceive me. Luckily, local hero and personal friend <a href="https://archive.org/details/apple_ii_library_4am">4am</a> has already solved this problem. He’s what you might call an “ethical cracker”. He re-cracks all these original games (and everything else) in a way that is transparent and preserves the original as intact as possible for archival and historical value. His services are also vital in other ways. In the case of Choplifter, remember it was an early 48k game. When the 64k Apple IIe came out, Choplifter wouldn’t boot on it. The copy protection had silly checks in it designed to foil crackers (specifically looking for cracking EEPROM tools) that made it incompatible with the Apple IIe and later machines. Brøderbund did patch this problem in a later release, but none of the disk images you’ll find in the wild are this patched version. This is just one of many issues that 4am fixes in his cracks. Thus, for me, the decision was easy- I started with <a href="https://archive.org/details/Choplifter4amCrack">The 4am Crack of Choplifter</a>.</p>

<h2>The Process</h2>
<p>Where do you even start with this? Honestly, I didn’t know either. There’s a few different approaches a person might use, but all have flaws:</p>
<ol>
<li><em>Top Down</em>. Disassemble the entire binary all at once, and try to make sense of the result. You’ll quickly find this gets you nowhere, because most of the binary is not code. There is graphics data, huge data tables, unused space, etc. With the aforementioned weaknesses of disassemblers, you won’t get far with this at all.<br>
.</li>
<li><em>Bottom Up</em>. Set a break point where the floppy drive starts reading sector 0, and start stepping through code, figuring out what it does as you go along. This is called Boot Tracing and is how crackers did their thing. If you need to know how copy protection works and how the game brings itself up, this is really the only way. This might also work for some folks for reverse engineering the entire game (after you crack it, just keep going!) but I knew it wouldn’t work for me. It was too much to try and deduce at once. Too many unknowns to start with. At the end of the day, reverse engineering is a massive deduction puzzle. It’s like doing a million sudokus. You need to deduce what every single byte in the binary does, which you figure out by eliminating all the things that byte is not doing. It’s about eliminating unknowns. Thus while boot tracing your way to the entire game is certainly possible, you’re starting with nearly infinite unknowns. I wanted to eliminate as many unknowns upfront as I could. To that end, I landed on:<br>
.</li>
<li><em>Middle Out</em>. Remember what I said about knowing the machine? Like many early computers, the Apple II leans heavily on its ROM. There is a fixed set of ROM addresses that contain utility routines used by software. Furthermore, the 6502 is a memory-mapped I/O chip, so there’s also a large list of magic memory addresses for talking to the hardware. Remember there is no operating system, APIs, or libraries here. These games sit directly on the hardware. In effect, the ROM and memory map combine to form a fixed API that we can exploit. This means, for example, you can find the routine that reads the keyboard simply by looking for code that touches the keyboard buffer in the memory map. This is in fact exactly how I started.</li>
</ol>

<h2>The Beginning</h2>
<p>The very first piece of code that I reversed was the keyboard handler. Choplifter doesn’t have a lot of keyboard use (it requires a joystick for gameplay) but it allows you to hit a key to start the game, and has a few options for toggling sound, etc. That means the game has a keyboard handler, and that means the game is touching memory address <strong>$C000</strong> which is the keyboard strobe in the Apple II. To check for and read for any key being down, you must <strong>LDA $C000</strong>. That means somewhere in Choplifter is the binary sequence <strong>AD 00 C0</strong>. That binary sequence will be in the keyboard handler. That’s our in. That’s our foot in the door. That’s our picking of the lock. That’s our… okay I’m running out of metaphors here, but you get it.</p>
<p>Of course it’s possible there is more than one routine reading the keyboard strobe and there may be more than one input handler, but you have to start somewhere and start with some assumptions. I assumed the first&nbsp;<strong>AD 00 C0</strong>&nbsp;sequence I found was the main keyboard handler and went from there.</p>
<p>From this point on, it was routine-by-routine. The other big thing you have working in your favour in an effort like this is that this code was written by a human. A fellow programmer. A comrade in arms. Outside of the copy protection code, they are not trying to trick you or mislead you in any way. They have written code that is logical to them and will be reasonably well organized. In particular, it will be broken down into subroutines. That’s the basic structure that drove me. Once I had my foot in with the keyboard strobe, I expanded my disassembly above and below that until I had what looked like a single subroutine. Finding the end is easy because there will be an&nbsp;<strong>RTS</strong>. Finding the start is trickier, so I assumed for now that the&nbsp;<strong>LDA $C000</strong> would be the first thing in the routine. Sometimes you can verify this by looking upwards, because the end of the previous routine ought to be right above this one, so there will be an <strong>RTS</strong> there as well. However with Dan that was a dangerous assumption. Dan very much liked to use space between his routines to store local variables and such for the routines above. Thus the bytes above you may or may not be code. However assuming the <strong>LDA $C000</strong> was the first line turned out to be correct anyway and I had my very first routine.</p>
<pre>; Checks for any form of button input (keyboard or joystick buttons).
; Returns carry set if something was detected. For keyboard, we do some
; processing of that input as well. Cheat keys will be checked, etc
checkButtonInput: ; $0d92
    lda $C000 ; Check any key
    cmp #$80
    bcs keyPushed
    lda $C061 ; Button 0
    cmp #$80
    bcs joystickPushed
    lda $C062 ; Button 1
    cmp #$80
    bcs joystickPushed
    clc ; Nothing pushed - clear carry and return
    rts</pre>
<p>A few things to note here. First is that I have noted the memory address where I found this routine in RAM. It was immensely helpful to keep track of this, because it means I can replace calls to this routine with my labels in the source code as I find them. Furthermore, it allows me to know when I have holes between routines, or when routines jump into the middle of each other (which happens in Apple II code more than you’d like). It also means that in my final compiled binary, I can verify everything is in the right place. I could often find bugs simply by comparing my memory address notes to where the assembler put the final code in the list output of the build process. If they didn’t line up, then I was missing bytes somewhere, I had missed a global variable somewhere, or I had a routine that wasn’t disassembled correctly.</p>
<p>Now comes the “middle out” part of the process. I start by going down from here. If there are any&nbsp;<strong>JSR</strong>s in this code, I would go to those areas of memory and disassemble them. From here on, knowing the boundaries of each routine is easy because I know the entry point (from the <strong>JSR</strong>) and I know the end point (the next&nbsp;<strong>RTS</strong>&nbsp;I find). Of course, some routines have more than one&nbsp;<strong>RTS</strong>, but for Dan’s programming style that was thankfully not very common and I could work those out as I went. He mostly shared my belief in avoiding early-out conditions in subroutines.</p>
<p>Once I had found all the routines working downward from the current point, then I would simply advance to the next routine after my current “top” and disassemble that one. This might sound like boot tracing from here on and it kinda was, which is why I switched gears a bit. After reversing a few routines linearly following input in memory, I was piling up unknowns because those new routines lack the context of the code calling them. As you disassemble code, there will be a lot of things you can’t know at first. In an Apple II game, there are two big ones. Global variables, and the zero page.</p>

<h2>The Big Unknowns</h2>
<p>At the end of the day, all the code you encounter is either reading memory, writing memory, or branching (with occasional light math). As established above, the branching is actually pretty easy to unwind. As you go along, it gets easier because you find jumps and <strong>JSR</strong>s to addresses for which you have already disassembled that routine. The memory thing is much more difficult.</p>
<p>The first type of memory to sort out is global data. As is typical of Apple II software, various global data is stashed wherever was handy. Fortunately Dan was pretty organized about this. Local variables needed by a particular function were stored at the bottom of that function. This would be things like loop counters, animation frame counters, caches to save and restore values, etc. For truly global data like game state, Dan had an area of memory where he put all that stuff together. Interestingly, he also used global data for all his gameplay tuning. These are things like the size of the world, the maximum number of enemies, the positions of the buildings, etc. None of this ever changes (and this data is all initialized by the loader) but it isn’t hardcoded. This likely made development easier for Dan, as he could tweak things easily for making the game better, and could move things around for debugging as needed.</p>
<p>The second big category is zero page. If you’re not familiar with the 6502, it’s essentially a RISC architecture. It has a small instruction set, operating on a very small number of registers. However, the 6502 has something special called the Zero Page. This is literally page <strong>$0000</strong> in memory and it gives you 256 scratch values that you can use like registers. They are not quite as fast as registers, but much faster than normal memory. The design intent was to use these zero page locations as 256 registers. In normal Apple II development, you don’t actually get to use them much. The majority of the zero page is reserved for use by ProDOS (or DOS 3.3) and AppleSoft BASIC. However for a game booted directly from floppy, neither of those exist. The entire zero page is yours for the taking. Dan takes full advantage of this, and Choplifter makes heavy use of the zero page. Nearly every byte in it is used. Thus, figuring out the purpose of all 256 zero page locations was critical to this project.<br>
All of these memory deductions were handled the same way. The first time they are encountered, I simply made a note of the memory location and gave it a generic name, like <strong>STATE_701F</strong> for globals or <strong>ZP_A5</strong> for zero page. Later, when I figured out what it was, I had an easy unique identifier to search-and-replace with a better name. At first glance then, a lot of code makes no sense at all because it’s just loading, storing, adding, and otherwise shuffling zero page and global memory locations of unknown purpose. However, eventually you come across code that is unambiguous.</p>
<p>The first example of this that I found was the sound code. Much like the keyboard strobe, the speaker in the Apple II is a magic memory location (<strong>$C030</strong>) so any code that touches that is a sound playing routine. I found a handful of sound playing routines and every one of them had the same bit of logic at the top:</p>
<pre>    bit $72f6
    bpl routineEnd
...
routineEnd:
    rts</pre>
<p>In other words, if the high bit of&nbsp;<strong>$72f6</strong> is not set, skip this entire routine (which again, plays sound). What does that look like to you, then? Probably a preference for whether sound is enabled, right? It could also be a check to make sure only one sound at a time is playing, but sound preference is a good guess. This was further reinforced when I found a preference for inverting each joystick axis right below that in memory. Again, this is written by a human who is trying to be organized, and it’s likely Dan grouped the player preferences together in memory.</p>
<p>This example gives you the gist of how it all goes. I would find a little detail, form a hypothesis about what that was, then gradually reinforce or disprove that hypothesis as I went. Mostly the hypotheses get confirmed (as did the sound preference when I found a keyboard command that sets it). Occasionally I did have some hypotheses that were wrong as well, which tends to happen later in the process. This is much more mind-blowing than the confirmations, because you can operate for a long time with an incorrect hypothesis.</p>
<p>For example, I had found a series of memory locations that appeared to be tracking state for horizontal shifting of bitmaps. There is code in the game for shifting bitmaps vertically, because this effect is used when the chopper sinks into the ground during a crash, and for the animated titles. It looked to me like there was code for horizontal shifting that never got used, as though Dan had intended for the shifting code to be more general. I operated for weeks on that assumption, naming various related memory locations for this “unused horizontal shifting feature”. Eventually though, you get to a place where the assumptions just don’t add up. I started to see more and more functions using this “horizontal shift” state in ways that didn’t make any sense. The vague unease that your hypothesis is wrong builds and builds until you can’t deny it anymore. However until you have a better hypothesis, what can you do? You hang on to the old one until a better one comes along. Then all of a sudden, quite near the end of this entire reverse-engineer, I had accumulated enough little counter-clues that my lizard brain snapped it into focus. My “horizontal shift” code was <em>clipping code</em>. It was clipping sprites and images to the edges of the screen during scrolling (which looks a lot like horizontal shifting, algorithmically). A big disconfirmation like this is always mind blowing because all of a sudden, fifty other things in the code that you were confused about all snap into place.</p>
<p>The whole process was like this. Little victories like the sound preference would come daily, and occasionally huge victories like the clipping system would come. It was an addictive process overall. I couldn’t stop until it was done and I always ended a session feeling like I had accomplished something.</p>

<h2>The Main Loop</h2>
<p>I said above that I reversed methods mostly linearly starting from the keyboard input routine, but that became difficult pretty quickly. I switched gears then and tried something else that resulted in much more progress, but still in a “middle out” way. As a game developer, you know that every game has a main loop. That loop will alternate between updating things and rendering things. It will also track time, play sounds, simulate physics, etc. The contents will vary somewhat, but the broad strokes will be there and such a main loop will always exist. It finally occurred to me that because of the power of Virtual II, the main loop is easy to find. With the game running, I simply hit Break in the debugger. Presto, I’m either in the main loop, or something called by the main loop. I then Stepped Out climbing back up the call stack until I was at the top and that’s almost certainly the main loop. I did indeed find a big loop and proceeded to disassemble it from there. I now had a new “top” and could resume the drilling down approach that I started with above.</p>

<h2>The Hard Parts</h2>
<p>There were a couple of sections that were especially difficult to unravel. The first was what I call the Entity Table. This is a linked list stored in an array that holds the state for every game object.<br>
This is where experience as a game developer really comes into play. As a game developer, you know something like this must be in the game. All games have game objects, and all games have some sort of management and allocation scheme for them. It could be a table, a list, a tree, a database, striding arrays, or various other things. But there absolutely will be a game object structure. This structure typically contains position, velocity, animation frame, heading, etc for each object. A game object will be the player, but also every bullet, every enemy, every hostage on the ground, and so on.<br>
It was probably mid-way through the process before I started to see signs of what the game object system in Choplifter might be. I was seeing a lot of usage of zero page location <strong>$7A</strong>&nbsp;throughout the game logic code, so I knew that location was important and probably related to game object management. The breakthrough came when I found a method called off the main loop every frame that was iterating through something using <strong>$7A</strong> as an index into a big mysterious table, then calling a method from a function pointer table based on a field in that big table. As a game developer, this looks an awful lot like an entity (game object) update loop. The “update” function pointers are a form of subclassing in modern terms. It meant <strong>$7A</strong> was a “current entity” marker and it was one of those “blow it wide open” moments. Because <strong>$7A</strong> was used in so many places, I now knew a lot more about a huge number of routines. In a flurry of activity, dozens of routines snapped into place. Just knowing what <strong>$7A</strong> was unlocked collision detection, physics routines, parallax scrolling, animation, game object memory management, and more. It was huge.</p>
<p>Another really challenging section to figure was the sprite geometry and layering system. Old 2D games all handle rendering a bit differently. Because these machines are too slow to simply redraw the whole screen every frame, you have to have clever ways to figure out exactly what and how to redraw things. Every game does this differently because the ideal solution is often game-specific. What Choplifter does is keep a big table of everything that moves and it erases those things each frame. As new objects that move are created and deleted, they are added to this table. The game has a separate “sprite geometry” table that knows how big each object is at worst-case rotation (because some sprites in Choplifter can rotate). The “erase sprite” code is a clever bit of 6502 math to figure out how much of the sprite has sky behind it and how much has ground behind it. It then renders black (sky) and/or pink (ground) rectangles to erase the sprite.</p>
<p>There is no saving or restoring of sprite backgrounds in Choplifter. Sprites are always erased with solid colour(s). The stars are redrawn every frame, so if a star is erased it will get redrawn anyway. Same for the moon, which by the way is a compiled sprite. It’s a static object that never moves or changes, so it’s a rare perfect case for sprite compiling on the 8-bit Apple II. Sprite overlaps are handled with the classic Painter’s Algorithm. The game objects all have a depth value assigned to them, and the game keeps the game object list sorted by depth. Sprites are then drawn back to front after being erased. Simple, classic, and effective.</p>

<h2>The Techniques</h2>
<p>In addition to basic deduction from context as given in examples above, a number of other techniques were used to determine what a piece of code is doing. Here are the most useful methods that I used:</p>
<ol>
<li>Stubbing out a routine and seeing what breaks. This simply means replacing the first byte of a subroutine with <strong>$60</strong> (<strong>RTS</strong>). This is done in the running game within Virtual II. Then sit back and see what changes! For example, I found the main scrolling routine this way because I suspected where it might be. I stubbed that out, and the game played normally except the map didn’t scroll. Of course, oftentimes the game just crashes or blows up in crazy ways that aren’t informative, but it’s often very helpful. Another good example is rendering- if you find a function that is rendering stuff, but don’t know what, simply stub it out and see what disappears! I confirmed most of the rendering code this way.<br>
.</li>
<li>Changing memory contents and see what happens. This was especially useful for figuring out what global variables and zero page locations are for. For example, I found the globals that hold the position of the security fence by changing those values and seeing the fence move on the next frame! This is also great for finding boundaries in collision detection, various physics and control parameters, etc. Of course this does not work on data that changes frequently (like more than once every dozen frames or so) because you won’t have enough time to see the effects before the game overwrites it again. However it’s still really powerful.<br>
.</li>
<li>Making code do something else. A good example of this was identifying all the sprites in the game. The first rendering function that I found was for the Brøderbund logo on the title screen. That code had what appeared to be a pointer to a sprite in it. Near that pointer’s destination in memory, I found what looked like many tables of very similar looking pointers. So with the game running in Virtual II, I changed the Brøderbund logo pointer to all those other pointers one by one. I identified every single sprite and image in the game this way (and made some very silly looking scrapbooked title screens in the process). Another good example of this is moving branches around. I suspected, for example, a block of code was doing collision detection with the helicopter, so I added a <strong>JMP</strong> within the chopper update routine to skip over that code. Again, this is all done in Virtual II’s memory editor while the game is running. After doing that, the chopper could now fly through everything! This sort of hypothesis-testing then sets off another explosion of new information. Once I know for sure that that a block is doing collision detection, then I can deduce which memory areas are storing bounding boxes, velocities, etc. Every new piece of information snowballs.<br>
.</li>
<li>Breakpoints. Good old fashioned breakpoints! I learned a lot about many routines simply by stepping through them and seeing what’s in memory and what they are changing. This was also helpful towards the end for debugging areas where my reverse engineer was incorrect.<br>
.</li>
<li>Watchpoints. This is a very powerful feature of Virtual II, and one of those tools that I probably could not have lived without. Particularly for figuring out zero page locations, it was immensely helpful to be able to set a watch and see when the data changes (and who changed it). For example, if I suspected a particular value was storing chopper Y position, I could put the chopper on the ground, set a watchpoint, then take off. If I’m right, then the watchpoint trips the moment the chopper begins to lift off the ground. Furthermore, I now likely know which routine is handing flight dynamics because that’s the code that will be modifying it!</li>
</ol>
<h2>The Flaws</h2>
<p>I hate to call them flaws, because this is a great game that does everything it needs to perfectly. I also hate to criticize another programmer’s work because I certainly could not have written this game, and Dan was working on a new platform that very little was known about at the time. That said, there are definitely substantial technical improvements that can be made to this game, with the benefit of hindsight.</p>
<p>In the rendering system, Choplifter does a lot more erasing and redrawing than it needs to. Most of the objects move in consistent horizontal ways across solid backgrounds. For example, the tanks rumble along horizontally on the pink ground at a slow speed. There’s no need to ever erase these. Simply redraw them over top of themselves with a small pink border on either side. Since they move slowly, this will erase as they go. Same for the hostages and aliens.</p>
<p>Furthermore, the rendering is almost all pixel accurate. If you’re an Apple II programmer, you know how extraordinary this is. It’s out of scope here to explain the intricacies of the crazy Apple II video memory layout, but the gist is that it has seven pixels per byte (phase-shifted in every second byte because fuck you that’s why) and thus involves a lot of dividing by seven if you want to place specific pixels in specific places. Game programmers typically work around this by byte-aligning artwork whenever possible, and having seven copies of every sprite pre-aligned with each of the seven pixels in a byte. Choplifter does none of this. Even the animated title cards, which are static rectangles, are not byte-aligned. They are rendered pixel-aligned with arbitrary-width image data. All sprites are meticulously rendered pixel-accurate from a single image, at render time. No pre-calculating of shifts or even table-looking-up of shifted patterns is done. He “does the math” on every row of sprite pixels, every time.</p>
<p>Again, lest this sound like criticism of Dan, bear in mind that he then went on to write <a href="https://en.wikipedia.org/wiki/Airheart">Airheart</a>, generally regarded as the fastest and most sophisticated rendering engine ever built on the Apple II. Suffice it to say, he took what he learned writing Choplifter and got very very good at this stuff. I also want to emphasize again that Choplifter renders beautifully and does not need to be faster than it is. His rendering code does the job with no problem, so it is by definition correct.</p>

<h2>The Jump Tables</h2>
<p>Choplifter really, really likes jump tables. Nearly every single subroutine is piped through a jump table. There are a couple of huge ones, and some smaller ones scattered throughout. Jump tables have many excellent uses, most notably because they give you a layer of vectored indirection so you can change what code is doing on the fly. What’s interesting about Choplifter’s jump tables is that they are actually costing performance for no runtime benefit. Dan never modifies any of those vectors. However the indirection is always there– nearly every subroutine call in the entire game is paying an extra long jump cost every time. I believe this was a development tool for Dan, and I think it tells us something about his development process. Because this game is so early in the life of the Apple II, really good development tools didn’t exist yet. Dan was probably working from a very basic assembler, a homemade linker (or none at all) and none of the code was relocatable even at assembly time. In this kind of environment, jump tables are a very useful development tool because they give you a handle to call things while still making it easy to move code around as functions grow and shift. If you don’t have a multi-pass macro assembler managing all your labels and a linker shuffling all your code into place for you, then jump tables are a way to do all this by hand without losing your mind. It’s also telling that all the jump tables are at round numbers in memory. There’s one at <strong>$1000</strong>, one at <strong>$6000</strong>, one at <strong>$7000</strong>, one at <strong>$8000</strong>, and one at <strong>$9000</strong>. No linker or assembler labeling system would do that. A human did that. A human named Dan.</p>
<p>One of the neatest things about this process is how much I feel I learned about Dan while doing it. Or at least, Dan in 1982. For example, I believe he must be a classic math and computer graphics guy. A really notable thing about Choplifter is that the coordinate space is left-handed. The Y axis is zero at the bottom of the screen and 191 at the top. This the opposite of basically all 2D games (and the opposite of the hardware). This is just the kind of thing that classically trained math people do, because in math Y is zero at the bottom. I had this argument back in the day with many a math person who were new to computers. They all thought it was dumb that Y was 0 at the top of the screen, whereas for us hacks who came at this from computing principles, it made perfect sense. The CRT scans top to bottom, and video memory is thus sampled top to bottom, so of course Y=0 at the top because the memory addresses are lowest there. Dan reverses this with a look up table for video memory rows (which is actually a free flip because the crazy Apple II video system requires such a lookup table to linearize video memory anyway). The other reason I believe he is a classically trained math and/or computer graphics guy is because of how he approached sprite rendering. He tackled every problem the way a computer graphics person does- pixel accurate rendering, texture mapped scanline conversion for rotation, realtime flipping of sprites, realtime division-by-seven for shifting, etc. A pure game programmer, on the other hand, looks at every single one of those problems and says “table”. You’d have a table for all the math, and a pre-calculated sprite for all the transforms. Choplifter does very little of the latter and a metric crap-ton of the former. This really surprised me, since I wouldn’t have thought you could get a game to be as fast and smooth as Choplifter is while doing everything the expensive way.</p>
<p>Back to the jump tables for a moment, one amusing (and very human thing) is that they are used inconsistently. There are a number of routines that are in the jump tables, but only called through the jump table about half the time. Sometimes they are called directly. Some of them are always called directly despite being in the jump table. Perhaps they were added to the tables later and Dan didn’t back-port the changes, or perhaps he got lazy about calling through the tables late in development when he knew they weren’t going to move anymore. I can’t say for sure, but it’s a very human and very programmer thing to do.</p>

<h2>The Inline Parameters</h2>
<p>Something we take for granted in modern computing is passing parameters to functions. When compilers came along, smart people agreed that the way to handle this is stack frames. All the parameters and local variables for your function all live on the stack in a standard format, and the compiler handles this magically for you. Modern CPUs even have hardware support for this concept. The 68000, for example, has special registers and opcodes devoted entirely to managing stack frames. Early assembly-language development, however, has none of this. In fact, there is no obvious way to pass parameters to your functions at all. There aren’t enough registers to use those, except for trivial cases. Pushing everything on to the stack is clumsy and crash-prone with a stack as small as the 6502’s. You can use global data, and many games do. You can use zero page, and many games do this as well. However, there is one other way that is equal parts sneaky, clever, hacky, and maddening- inline data. Choplifter mostly uses zero page for passing data between routines, but it does also use quite a bit of inline data. Here’s an example:</p>
<pre>    jsr jumpSetSpriteAnimPtr ; $0880
        .word $a0f0          ; Set animation graphic pointer to Choplifter logo
</pre>
<p>If you’re an assembly programmer, that should look very very strange to you. In fact, you may be wondering how that doesn’t crash. The CPU will execute that&nbsp;<strong>JSR</strong>, then return to… two garbage bytes that are a pointer sitting right in the code stream. Maybe those two pointer bytes will form valid opcodes, but probably not. Either way, you don’t want them <em>executed as code&nbsp;</em>which is what should happen. So why doesn’t it?</p>
<p>Inside that routine <strong>jumpSetSpriteAnimPtr</strong> is a little helper routine that does a lot of stack jiggery-pokery. Once inside the routine, our return address is sitting on the stack because the CPU put it there. The routine we are in can pull that address&nbsp;<em>off</em> the stack, then use it to find that spot in memory that we jumped <em>from</em> where our pointer parameter is sitting. We can now read these parameters, store them somewhere, then&nbsp;<em>modify </em>the return address to land&nbsp;<em>after</em> those inline data bytes, put that modified return address back on the stack, and the CPU will magically skip over those data bytes none the wiser. It’s a tricky system to implement, but it makes for quite clean code. You can take this one step further and use self-modifying code to alter those inline bytes before the <strong>JSR</strong> so those parameters don’t have to be assembly-time constants. Choplifter does some of this as well. Overall it’s about as clean a parameter-passing technique as you can get on an 8-bit CPU with a tiny stack. However, it makes disassembling that code very tricky because you have these subroutines all over the place that have garbage bytes inline with them. That really screws with disassemblers and until you figure out what the format of those bytes is and how many there are for every routine that does this, it can be a real mess. This was probably the biggest pure hassle I had with this project. Constantly dealing with the disassembly mess created by these parameters was a pain in the ass.</p>

<h2>The Bugs</h2>
<p>Every single piece of software in history has shipped with bugs. I fully expected to find some in this process. What’s amazing is, perhaps, how few I found. In fact, I believe there is only one. The full details are documented in the source code, but I found a bug in the linked list initialization routine for the master entity table. There’s an off-by-one error in the ID counter, combined with a mis-write of the list terminator. Instead of null-terminating the end of the list, it null-terminates about 2/3rds of the way through. Right in the middle of a record, in fact. The entity table has a lot of extra space in it, so this never became an issue. However, I believe Dan was looking for this bug and never found it. There are multiple “assertions” throughout the entity management code. This takes the form of “if an entity ID or count is weird, beep a bunch of times and crash”. You’ll see this documented by me in the code. Furthermore, there is a count limit of four on each enemy type. That in itself isn’t unusual. However, in several totally unnecessary places during the enemy spawning logic, the code checks for enemy count and bails out early if it is reached. This only needs to be done once, but Dan does it in many many places. As a software engineer, this smells very much to me like desperate safeguards. I think he had bugs where he was somehow getting five jets spawned (the limit is four) and he couldn’t figure out how. I believe he was chasing that linked list initialization bug, never quite found it, so he put in all the safeguards so he could ship the game. This is all speculation of course, but I believe the evidence in the code supports this hypothesis. Again, all the details are in the code itself, so take a look at see what you think.</p>

<h2>The Dead Code</h2>
<p>Every piece of software also ships with dead code in it. There are always routines that you thought you’d need and didn’t, ideas that didn’t work out, tools used for debugging, etc. Apple II games don’t tend to have much of this stuff because space is too precious, but there is some in Choplifter.</p>
<p>Most notably is a complex sprite rotation routine. Choplifter does a lot of “tilting” of sprites. That is to say, Dan simulates rotation by shifting pixels around. This is used, for example, when flying sideways with the helicopter facing the camera. The chopper “leans” into the direction of travel. You might think there would be multiple sprites to render each angle, but no, Dan does this procedurally. What’s interesting is that it looks like he tried to take this even farther. There is a very complex sprite rotation routine that basically boils down to texture mapping. He’s written what looks to me like a scan-line converter for a software polygon rasterizer in a 3D engine. I know because I have written my share of those. It looks to me like Dan had intended to use this for rotating the helicopter when flying sideways. In the end, he did what you would expect- there is a separate sprite for each sideways rotation angle of the helicopter. His attempt at a subset-of-texture-mapping is impressive though, and would have allowed all the helicopter rendering to be done with a single sprite.</p>
<p>There are a couple of other little routines here and there that went unused. I have marked them all “dead code” in the source.</p>

<h2>The Secret Features</h2>
<p>It’s not uncommon to find shipped Apple II games with debugging tools or cheats used by the developer still in them. Choplifter has lots of those. The best one is Ctrl-L followed by a number (0-7). This sets the difficulty of the enemies. After each trip back to base, the game increases your “level” making enemies come faster and with more types. If the game gets too hard, you can set the level back down! This is no doubt a debugging tool so that Dan could test the harder levels without having to play through the entire game over and over.</p>
<p>The most striking thing, though, was that I found strong evidence that the game had vertical scrolling at some point. Choplifter is famously a horizontal scrolling game, which was very cool at the time. It’s not super common on the Apple II because the machine doesn’t have the horsepower to move enough pixels to scroll. However Choplifter limits the design and the visuals just enough to create convincing parallax scrolling with minimal horsepower. What was amazing though is that all throughout all the code that handles scrolling and clipping, vertical parameters are supported. There is even tuning for the vertical scrolling boundaries of the screen, and the worldspace/screenspace coordinate conversion all handles vertical scrolling. It’s well worth experimenting with this to see if this feature can be reactivated, but as of this writing I have not yet tried this.</p>

<h2>The Debugging</h2>
<p>When this reverse engineer was done, I had to make it run and make it work. This was, amazingly, quite easy. It only took a couple of days to get it fully debugged. Why? Because I had a working binary to compare to. This was like programming with a time machine. I had a version of the game from the future that showed me what my broken one was supposed to look like. My debugging was done almost entirely by doing binary compares of my compiled image and the image I had started with. Anywhere they were different was probably a bug in mine. From there all I had to do was compare my source to the disassembly in Virtual II to see why mine was wrong. 90% of the bugs were copy-paste errors, code being misaligned because I’d gotten a jump address wrong, that sort of thing. Of course when comparing binaries, I had to skip over global variables (who’s value would depend on the exact moment the dump was captured) and self-modifying code (of which there is some, but not a lot in Choplifter). Otherwise all the static areas of the code should be binary-identical. If not, mine is wrong.</p>
<p>There was one other class of bugs, which goes back to that global tuning data that I talked about. Dan’s custom sector 0 floppy loader initialized all those tuning values to various constants. They don’t change during gameplay, but they have to be set correctly. Debugging these was a matter of seeing which variables affect whatever in the game was misbehaving, then pausing the working game and seeing what value was there in memory. Since these never change, it doesn’t matter when you break into the game to check them. Rather than initialize these in my replacement loader (see below on that point), I have hard-coded the initial values of those memory areas in my source code.</p>

<h2>The Caveats</h2>
<p>As alluded to above, I did not reverse engineer Dan’s loader. It was much easier for me to work with only what the game puts in RAM. That is the entire game minus the loader. I also didn’t want to deal with custom disk images and the copy protection. For that reason, I wrote my own loader based on ProDOS that loads the game into the same places in memory as Dan’s does. You’ll notice two primary artifacts from this change:</p>
<ol>
<li>The game does not show the title screen while loading the way Dan does. Dan’s loader throws up a splash screen that is a screenshot taken from the self-playing demo. This screenshot is somewhere in his loader and I didn’t pull it out to replicate the behaviour in my loader.<br>
.</li>
<li>On first launch, during the self-playing demo, the score HUD at the top is blank. This is because, amusingly, Dan does not render those scores until you play the first game. They are only there because they were part of the screenshot rendered by the loader. This is perhaps technically a bug and maybe Dan never noticed because there’s no way to tell, but it doesn’t affect anything anyway. The HUD is (by design) not updated during the self-playing demo so the numbers don’t change regardless.</li>
</ol>

<h2>The Conclusion</h2>
<p>That’s it! This was an amazing journey for me and I had a ton of fun doing this. I banged it out in about eight weeks of nights and some weekends. That’s much more intense than I intended to go on this, but the process was frankly addictive. Every time I deduced a new piece from other pieces, it was a shot of adrenalin and endorphin. It was non-stop “a-ha” moments and the feeling of cracking a giant code was amazing. However I am also now deeply, cellularly exhausted and never want to do this again. Please enjoy!</p>
		
				
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The OpenAI board was right – No (by a voice actres named Scarlet) means No (283 pts)]]></title>
            <link>https://garymarcus.substack.com/p/the-openai-board-was-right</link>
            <guid>40425403</guid>
            <pubDate>Tue, 21 May 2024 08:06:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/the-openai-board-was-right">https://garymarcus.substack.com/p/the-openai-board-was-right</a>, See on <a href="https://news.ycombinator.com/item?id=40425403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A week ago, OpenAI released an exciting new demo, featuring a voice character with a sexy breathy voice that was supposed to remind you of Scarlett Johansson’s AI agent character in the fabulous film </span><em>Her</em><span>. Lots of people gushed over it. (Some worried about the sexism, as well they should, but that’s a story for another day. And of course I daresay the demo was </span><em>just </em><span>a demo, one that will never work robustly as advertised, but that too is a story for another day.)</span></p><p><span>Fast forward to today; there’s been a backlash.  Too many people noticed the coincidence and not everyone was happy. Some wondered whether ScarJo had been compensated. Today, under pressure, OpenAI pulled the ScarJo-like voice, </span><a href="https://www.tomsguide.com/ai/chatgpt/openai-says-sky-voice-in-chatgpt-will-be-paused-after-concerns-it-sounds-too-much-like-scarlett-johansson" rel="">alleging that the resemblance was purely a coincidence.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg" width="1282" height="309" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:309,&quot;width&quot;:1282,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:180593,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f8cd7d5-7a51-474b-a959-3fa58c0aa4c3_1282x309.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I probably don’t have to tell you, but that’s complete bullshit. And stupid, obviously refuted bullshit at that. </p><p>For one thing, Sam himself had proudly posted a reference to the film “Her” within hours of the demo:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg" width="636" height="360" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:360,&quot;width&quot;:636,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29582,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27faac92-2ede-4204-9786-cfd1888aff9a_636x360.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>I can’t tell you what happened to Sam’s caps lock, but obviously the claim that the resemblance to ScarJo was a “coincidence” was a lie. Sam knew perfectly well what the character sounded like.</p><p>§</p><p>A couple hours later, ScarJo herself (via her publicist) sent a statement, even more damning, to Bobby Allyn, a journalist (that I happen to know) at NPR, telling the real story:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png" width="1283" height="1772" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1772,&quot;width&quot;:1283,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:940148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ccb6fa2-c548-4d6e-b5c0-c6a8c907ee85_1283x1772.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Coincidence, my eye. </p><p>§</p><p>ScarJo’s contention that this goes back to September checks out:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png" width="1301" height="499" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:499,&quot;width&quot;:1301,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106318,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b288107-e259-4b14-93fb-998ea87bd157_1301x499.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>They said it wasn’t intentional, but of course it was. Sam may not be wanting to delete his “her” tweet, but 6 million people saw it. And pur concidence line is a sign of consciousness of guilt.</p><p>§</p><p><span>All of this is really about </span><em><strong>consent</strong></em><span>. Artists and writer and actors don’t want their work to be used without their permission; if you want to use their stuff, you should compensate them, and get their permission. </span></p><p>If they say “no”, no means no.</p><p>Scarlett said “no”.</p><p>§</p><p>That didn’t stop Sam.</p><p>§</p><p>As the film maker Toni Thai put it:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png" width="1337" height="436" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:436,&quot;width&quot;:1337,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:99683,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd5c01d5c-17f3-44f8-8c54-5ce8b8f05b9a_1337x436.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Sam got away with a lot for a long time, but people are starting to see through the ruse. Here’s an (admittedly unscientific) poll I ran earlier today:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg" width="1355" height="573" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:573,&quot;width&quot;:1355,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:168923,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff3ad789f-adf8-4d40-b9d5-619f045f5dce_1355x573.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Count me with the majority. Spin is a way of life at OpenAI; telling the truth is not.</p><p>§</p><p>Casey Newton noticed, too:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png" width="1456" height="1300" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1300,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1496725,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7c08d76-57ba-4f3b-95f1-0b05e93c78ed_1511x1349.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So did Canadian MP Michelle Rempel Garner:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png" width="1341" height="433" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:433,&quot;width&quot;:1341,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:105941,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F172c62fa-bd31-469b-8ef2-980ec0d2e41f_1341x433.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>The (old, now-replaced) Board said in November they fired Sam because he was not consistently candid. I saw that with his fudges to the Senate about OpenAI equity (he has indirect equity, which he failed to mention, and owns an OpenAI VC firm that trades on the company name that he failed to mention), the board saw it with his lies about Helen Toner, and now we all see it with his embarrassing lies about Scarlett Johansson. </p><p>It’s a pattern.  </p><p><em><strong>Gary Marcus</strong><span> has seen enough, and hopes that the new OpenAI board recognizes that Sam’s behavior is not consistent with what one would expect of a nonprofit that has pledged to help humanity. </span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft's AI chatbot will 'recall' everything you do on its new PCs (102 pts)]]></title>
            <link>https://www.theguardian.com/technology/article/2024/may/20/microsoft-chatbot-assistant-pc</link>
            <guid>40425306</guid>
            <pubDate>Tue, 21 May 2024 07:53:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/technology/article/2024/may/20/microsoft-chatbot-assistant-pc">https://www.theguardian.com/technology/article/2024/may/20/microsoft-chatbot-assistant-pc</a>, See on <a href="https://news.ycombinator.com/item?id=40425306">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Microsoft wants laptop users to get so comfortable with its artificial intelligence chatbot that it will remember everything you’re doing on your computer and help figure out what you want to do next.</p><figure id="3e68afce-a7b5-4d6b-9a91-7c8c06a042c2" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:1,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Google rolls out AI-generated, summarized search results in US&quot;,&quot;elementId&quot;:&quot;3e68afce-a7b5-4d6b-9a91-7c8c06a042c2&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/technology/article/2024/may/14/google-ai-search-results&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false,&quot;inAdvertisingPartnerABTest&quot;:false,&quot;assetOrigin&quot;:&quot;https://assets.guim.co.uk/&quot;}"></gu-island></figure><p>The software giant on Monday revealed an upgraded version of Copilot, its AI assistant, as it confronts heightened competition from big tech rivals in pitching generative AI technology that can compose documents, make images and serve as a lifelike personal assistant at work or home.</p><p>The announcements ahead of Microsoft’s annual Build developer conference in Seattle centered on imbuing AI features into a product where Microsoft already has the eyes of millions of consumers: the Windows operating system for personal computers.</p><p>The new features will include Windows Recall, enabling the AI assistant to “access virtually what you have seen or done on your PC in a way that feels like having photographic memory”. Microsoft promises to protect users’ privacy by giving them the option to filter out what they don’t want tracked.</p><p>The conference follows big AI announcements last week from rival Google, as well as Microsoft’s close business partner OpenAI, which built the AI large language models on which Microsoft’s Copilot is based.</p><p>Google rolled out a retooled search engine that periodically puts AI-generated summaries over website links at the top of the results page; while also showing off a still-in-development AI assistant Astra that will be able to “see” and converse about things shown through a smartphone’s camera lens.</p><p>ChatGPT-maker OpenAI unveiled a new version of its chatbot last week, demonstrating an AI voice assistant with human characteristics that can banter about what someone’s wearing and even attempt to assess a person’s emotions. The voice sounded so much like Scarlett Johansson playing an AI character in the sci-fi movie “Her” that OpenAI dropped the voice from its collection Monday.</p><figure id="d76c7566-8669-463e-ad05-2189a7451de0" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:8,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;New GPT-4o AI model is faster and free for all users, OpenAI announces&quot;,&quot;elementId&quot;:&quot;d76c7566-8669-463e-ad05-2189a7451de0&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/technology/article/2024/may/13/openai-new-chatgpt-free&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false,&quot;inAdvertisingPartnerABTest&quot;:false,&quot;assetOrigin&quot;:&quot;https://assets.guim.co.uk/&quot;}"></gu-island></figure><p>Though Microsoft has invested billions in OpenAI, the startup also rolled out a new desktop version of ChatGPT designed for Apple’s Mac computers.</p><p>Next up is Apple’s own annual developers conference in June. The Apple CEO Tim Cook signaled at the company’s annual shareholder meeting in February that it has been making big investments in generative AI.</p><p>Some of Microsoft’s announcements Monday appeared designed to blunt whatever Apple has in store. The newly AI-enhanced Windows PCs will start rolling out on 18 June on computers made by Microsoft partners Acer, ASUS, Dell, HP, Lenovo and Samsung, as well as on Microsoft’s Surface line of devices. But they’ll be reserved for premium models starting at $999.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erlang/OTP 27 Highlights (202 pts)]]></title>
            <link>https://www.erlang.org/blog/highlights-otp-27/</link>
            <guid>40424982</guid>
            <pubDate>Tue, 21 May 2024 06:52:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.erlang.org/blog/highlights-otp-27/">https://www.erlang.org/blog/highlights-otp-27/</a>, See on <a href="https://news.ycombinator.com/item?id=40424982">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
    

    <div>
        
        
        
        
        <p>Erlang/OTP 27 is finally here. This blog post will introduce the new
features that we are most excited about.</p>

<p>A list of all changes is found in <a href="https://erlang.org/patches/OTP-27.0">Erlang/OTP 27 Readme</a>.
Or, as always, look at the release notes of the application you are interested in.
For instance:
<a href="https://www.erlang.org/doc/apps/erts/notes#erts-15.0">Erlang/OTP 27 - Erts Release Notes - Version 15.0</a>.</p>

<p>This year’s highlights mentioned in this blog post are:</p>

<ul>
  <li><a href="#overhauled-documentation-system">Overhauled documentation system</a></li>
  <li><a href="#triple-quoted-strings">Triple-Quoted strings</a></li>
  <li><a href="#sigils">Sigils</a></li>
  <li><a href="#no-need-to-enable-feature-maybe">No need to enable feature <code>maybe</code></a></li>
  <li><a href="#the-new-json-module">The new <code>json</code> module</a></li>
  <li><a href="#process-labels">Process labels</a></li>
  <li><a href="#new-functionality-in-stdlib">New functionality in STDLIB</a></li>
  <li><a href="#new-ssl-client-side-stapling-support">New SSL client-side stapling support</a></li>
  <li><a href="#tprof-yet-another-profiling-tool"><code>tprof</code>: Yet another profiling tool</a></li>
  <li><a href="#multiple-trace-sessions">Multiple trace sessions</a></li>
  <li><a href="#native-coverage-support">Native coverage support</a></li>
  <li><a href="#deprecating-archives">Deprecating archives</a></li>
</ul>
      <h2 id="overhauled-documentation-system">
        
        
          Overhauled documentation system <a href="#overhauled-documentation-system">#</a>
        
        
      </h2>
    

<p>The Erlang/OTP documentation before Erlang/OTP 27 was authored in
<a href="https://en.wikipedia.org/wiki/XML">XML</a>, from which the
<a href="https://www.erlang.org/docs/26/apps/erl_docgen/">Erl_Docgen</a>
application could generate HTML web pages, PDFs, or Unix man pages.
The reason for generating PDFs is that the documentation used to be
printed as
<a href="https://erlangforums.com/t/old-printed-otp-documentation-cover/1989/2">actual paper books</a>.
The last time the books were printed were for Erlang/OTP R7 released in 2000.</p>

<p>As an example, here is the XML code for
<a href="https://www.erlang.org/docs/26/man/lists#duplicate-2"><code>lists:duplicate/2</code></a>
from Erlang/OTP 26:</p>

<pre><code>    &lt;func&gt;
      &lt;name name="duplicate" arity="2" since=""/&gt;
      &lt;fsummary&gt;Make &lt;c&gt;N&lt;/c&gt; copies of element.&lt;/fsummary&gt;
      &lt;desc&gt;
        &lt;p&gt;Returns a list containing &lt;c&gt;&lt;anno&gt;N&lt;/anno&gt;&lt;/c&gt; copies of term
          &lt;c&gt;&lt;anno&gt;Elem&lt;/anno&gt;&lt;/c&gt;.&lt;/p&gt;
        &lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;/p&gt;
        &lt;pre&gt;
&gt; &lt;input&gt;lists:duplicate(5, xx).&lt;/input&gt;
[xx,xx,xx,xx,xx]&lt;/pre&gt;
      &lt;/desc&gt;
    &lt;/func&gt;
</code></pre>

<p>The XML code was stored in separate files, not in the source
code. When building the documentation, the function specs from the
source code would be combined with the text from the documentation
file. It was the responsibility of the writer to ensure that variables
mentioned in the documentation body matched the names in the function
spec.</p>

<p>One thing never said about Erl_Docgen and the old documentation system
was that it made writing documentation enjoyable and effortless. That
was one thing we wanted to change with the new documentation system.
We wanted to make it fun to write documentation, or at least to
require less attention to tedious details such as using XML tags
correctly.</p>

<p>In Erlang/OTP 27, the documentation is written in
<a href="https://en.wikipedia.org/wiki/Markdown">Markdown</a> and is placed in
the source code before the function spec and implementation. Here is
the documentation and implementation of
<a href="https://www.erlang.org/doc/man/lists#duplicate/2"><code>lists:duplicate/2</code></a>
in Erlang/OTP 27:</p>

<pre><code>-doc """
Returns a list containing `N` copies of term `Elem`.

_Example:_

```erlang
&gt; lists:duplicate(5, xx).
[xx,xx,xx,xx,xx]
```
""".

-spec duplicate(N, Elem) -&gt; List when
      N :: non_neg_integer(),
      Elem :: T,
      List :: [T],
      T :: term().

duplicate(N, X) when is_integer(N), N &gt;= 0 -&gt; duplicate(N, X, []).

duplicate(0, _, L) -&gt; L;
duplicate(N, X, L) -&gt; duplicate(N-1, X, [X|L]).
```
</code></pre>

<p>The documentation is placed in a
<a href="#triple-quoted-strings">triple-quoted string</a>
following
the <a href="https://www.erlang.org/eeps/eep-0059"><code>-doc</code> attribute</a>.</p>

<p>Having the documentation near the spec makes its easy to ensure that
the text refers to variables defined in the function spec.</p>

<p>Another goal we had was to replace Erl_Docgen with a tool more widely
used so that we wouldn’t have to carry the entire burden for
maintaining it. We did that by using
<a href="https://hexdocs.pm/ex_doc/readme.html">ExDoc</a>, which is also used by
the <a href="https://elixir-lang.org/">Elixir</a> language and most, if not all,
Elixir projects.</p>

<p>An issue that arose is whether it’s advisable to include user
documentation within the source code. Wouldn’t this make it much harder
to maintain the code?</p>

<p>I don’t claim to have a universal response to that concern, but in the
case of Erlang/OTP, most actively developed code exists within modules
lacking documentation. Typically, OTP applications consist of one or
a few modules containing the documented API, while the bulk of the
implementation is found in other modules.</p>

<p>For example, the interface to the Erlang compiler is found in the
<a href="https://www.erlang.org/doc/man/compile">compile</a> module, while most
of the code being executed resides in one of the other 59 modules
of the Compiler application. Similarly, the <a href="https://www.erlang.org/doc/apps/ssl">SSL
application</a> comprises 76 modules,
of which merely four contain documentation.</p>

<p>Another application that is frequently updated is
<a href="https://www.erlang.org/doc/apps/erts">ERTS</a>. However, most of ERTS is
implemented in C (and some C++), while much of the actual
Erlang code within ERTS is located in modules without documentation.</p>

<p>There are, of course, some exceptions to how applications are
structured, for example the STDLIB application, where most modules are
documented. However, STDLIB is a mature application that is updated
relatively infrequently.</p>
      <h2 id="triple-quoted-strings">
        
        
          Triple-Quoted strings <a href="#triple-quoted-strings">#</a>
        
        
      </h2>
    

<p>To facilitate writing documentation attributes containing many lines
of text, triple-quoted strings as described in <a href="https://www.erlang.org/eeps/eep-0064">EEP
64</a> have been
implemented. Triple-quoted strings come in handy whenever one needs
to include multiple line of text in Erlang source code. For example,
assume that we want to define a function that outputs some
quotations:</p>

<pre><code>1&gt; t:quotes().
"I always have a quotation for everything -
it saves original thinking." - Dorothy L. Sayers

"Real stupidity beats artificial intelligence every time."
- Terry Pratchett
ok
</code></pre>

<p>In Erlang/OTP 26, there are several different ways to do that, but of none
of them are particularly satisfying. For example, the text can be put into a
single string:</p>

<pre><code>quotes() -&gt;
    S = "\"I always have a quotation for everything -
it saves original thinking.\" - Dorothy L. Sayers

\"Real stupidity beats artificial intelligence every time.\"
- Terry Pratchett\n",
    io:put_chars(S).
</code></pre>

<p>This works, but is ugly. We must also remember to escape every quote
character.</p>

<p>A cleaner way is to use multiple strings, one for each line, letting
the compiler combine them:</p>

<pre><code>quotes() -&gt;
    S = "\"I always have a quotation for everything -\n"
        "it saves original thinking.\" - Dorothy L. Sayers\n"
        "\n"
        "\"Real stupidity beats artificial intelligence every time.\"\n"
        "- Terry Pratchett\n",
    io:put_chars(S).
</code></pre>

<p>That is a little bit nicer, but we’ll need to type more quote characters
and we must not forget to add <code>\n</code> at the end of each string. To
make sure that we don’t forget to insert the newlines, we could delegate
that mundane chore to the computer:</p>

<pre><code>quotes() -&gt;
    S = ["\"I always have a quotation for everything -",
         "it saves original thinking.\" - Dorothy L. Sayers",
         "",
         "\"Real stupidity beats artificial intelligence every time.\"",
         "- Terry Pratchett"],
    io:put_chars(lists:join("\n", S)),
    io:nl().
</code></pre>

<p>In Erlang/OTP 27, we can use a triple-quoted string:</p>

<pre><code>quotes() -&gt;
    S = """
        "I always have a quotation for everything -
        it saves original thinking." - Dorothy L. Sayers

        "Real stupidity beats artificial intelligence every time."
        - Terry Pratchett
        """,
    io:put_chars(S),
    io:nl().
</code></pre>

<p>The ending <code>"""</code> determines how much each line in the string should be
indented. The same characters that precede <code>"""</code> are deleted from all
lines between the beginning and terminating delimiters. For this
particular example, all space characters are removed since all have
the same indentation as the terminating <code>"""</code>.  Neither quote
characters nor backslashes are special in the lines enclosed by the
triple-quotes, so there is no need to escape anything.</p>

<p>Here is another example to show the versatility of triple-quoted
strings:</p>

<pre><code>effect_warning() -&gt;
    """
    f() -&gt;
        %% Test that the compiler warns for useless tuple building.
        {a,b,c},
        ok.
    """.
</code></pre>

<p>The function returns a string containing a short Erlang function.</p>

<p>Assuming that <code>effect_warning/0</code> is defined in module <code>t</code>, it can be
called like so:</p>

<pre><code>1&gt; io:format("~ts\n", [t:effect_warning()]).
f() -&gt;
    %% Test that the compiler warns for useless tuple building.
    {a,b,c},
    ok.
</code></pre>

<p>Note that indentation of the Erlang code for function <code>f/0</code> is retained.</p>

<p>For more information, see section <a href="https://www.erlang.org/doc/reference_manual/data_types#string">String</a>
in the Reference Manual.</p>
      <h2 id="sigils">
        
        
          Sigils <a href="#sigils">#</a>
        
        
      </h2>
    

<p>Sigils for string literals as described in <a href="https://www.erlang.org/eeps/eep-0066">EEP 66</a>
have been implemented.</p>

<p>Continuing with the theme of quotes, let’s explore why sigils were
introduced into Erlang, drawing inspiration from the wisdom of ancient
Greek philosophers:</p>

<pre><code>1&gt; t:greek_quote().
"Know thyself" (Greek: Γνῶθι σαυτόν)
ok
</code></pre>

<p>In Erlang/OTP 26, this can be implemented as follows:</p>

<pre><code>greek_quote() -&gt;
    S = "\"Know thyself\" (Greek: Γνῶθι σαυτόν)",
    io:format("~ts\n", [S]).
</code></pre>

<p>At this point, we get some customer feedback indicating that the
modules containing all the quotes are consuming an excessive amount of
memory. Each character in a string consumes 16 bytes of memory (on a
64-bit computer). That could be reduced to one byte for each character
if a binary were to be used instead of a string.  (Actually, one byte
for each US ASCII character and two bytes for each Greek letter.)</p>

<p>That change should be really easy. Let’s try:</p>

<pre><code>greek_quote() -&gt;
    S = &lt;&lt;"\"Know thyself\" (Greek: Γνῶθι σαυτόν)"&gt;&gt;,
    io:format("~ts\n", [S]).
</code></pre>

<p>That works for the English text, but not for the Greek characters:</p>

<pre><code>2&gt; t:greek_quote().
"Know thyself" (Greek: ½ö¸¹ Ã±ÅÄÌ½)
</code></pre>

<p>What’s wrong?</p>

<p>Strings in binary expression are by default assumed to be a sequence
of byte-size characters. Therefore, this expression:</p>

<pre><code>1&gt; &lt;&lt;"Γνῶθι"&gt;&gt;.
&lt;&lt;147,189,246,184,185&gt;&gt;
</code></pre>

<p>is <a href="https://en.wikipedia.org/wiki/Syntactic_sugar">syntactic sugar</a> for:</p>

<pre><code>2&gt; &lt;&lt;$Γ:8, $ν:8, $ῶ:8, $θ:8, $ι:8&gt;&gt;.
&lt;&lt;147,189,246,184,185&gt;&gt;
</code></pre>

<p>It is necessary to specify that the characters are to be encoded as
<a href="https://en.wikipedia.org/wiki/UTF-8">UTF-8</a>
encoded characters by appending an <code>/utf8</code> suffix:</p>

<pre><code>greek_quote() -&gt;
    S = &lt;&lt;"\"Know thyself\" (Greek: Γνῶθι σαυτόν)"/utf8&gt;&gt;,
    io:format("~ts\n", [S]).
</code></pre>

<p>That works because <code>&lt;&lt;"Γνῶθι"/utf8&gt;&gt;</code> is syntactic sugar for
<code>&lt;&lt;$Γ/utf8, $ν/utf8, $ῶ/utf8, $θ/utf8, $ι/utf8&gt;&gt;</code>.</p>

<p>Enter sigils.</p>

<pre><code>greek_quote() -&gt;
    S = ~B["Know thyself" (Greek: Γνῶθι σαυτόν)],
    io:format("~ts\n", [S]).
</code></pre>

<p>The <code>~</code> character begins a sigil. It is usually followed by a letter that
indicates how the characters in the string should be interpreted or encoded.</p>

<p>In this case the character <code>B</code> means that the characters should be put into a binary in UTF-8 encoding,
and also that that no escape characters are allowed.</p>

<p>After <code>B</code> follows the start delimiter, in this case <code>[</code>.  Since no escape characters
are allowed, it is necessary to choose delimiters that don’t occur in the string
contents. After the contents follows the end delimiter, in this case <code>]</code>.</p>

<p>The <code>B</code> sigil is the default sigil that is used if the letter following
<code>~</code> is omitted. Thus we get the same binary and the same output if we omit the <code>B</code>:</p>

<pre><code>greek_quote() -&gt;
    S = ~["Know thyself" (Greek: Γνῶθι σαυτόν)],
    io:format("~ts\n", [S]).
</code></pre>

<p>Sigils can also be used to begin a triple-quoted string. Returning to
the quotations example from the previous section, a binary literal can
be created by inserting <code>~</code> before the leading <code>"""</code>:</p>

<pre><code>quotes() -&gt;
    S = ~"""
         "I always have a quotation for everything -
         it saves original thinking." - Dorothy L. Sayers

         "Real stupidity beats artificial intelligence every time."
         - Terry Pratchett
         """,
    io:put_chars(S),
    io:nl().
</code></pre>

<p>Here follows a few quick examples to show the other sigils.</p>

<p><code>~b</code> creates a binary in the same way as <code>~B</code>, except that backslashes
will be interpreted as an escape character. This can be useful if one
want to insert control characters such as TAB (<code>\t</code>) into a string:</p>

<pre><code>1&gt; ~b"abc\txyz".
&lt;&lt;"abc\txyz"&gt;&gt;
</code></pre>

<p>Here we used the <code>"</code> character as delimiters as it is not used within
the string.</p>

<p><code>~s</code> creates a string in the usual way. The only useful way it differs
from a plain quoted string is that the delimiters can be switched. That
way, one can avoid the hassle of escaping quote characters and still
get to use control characters such as TAB:</p>

<pre><code>2&gt; ~s{"abc\txyz"}.
"\"abc\txyz\""
</code></pre>

<p><code>~S</code> creates a string, but does not support escaping of characters
within the string, similar to <code>~B</code>.</p>

<p>For more information, see section <a href="https://www.erlang.org/doc/reference_manual/data_types#sigil">Sigil</a>
in the Reference Manual.</p>
      <h2 id="no-need-to-enable-feature-maybe">
        
        
          No need to enable feature <code>maybe</code> <a href="#no-need-to-enable-feature-maybe">#</a>
        
        
      </h2>
    

<p>The <a href="https://www.erlang.org/doc/reference_manual/expressions#maybe">maybe expression</a>
was introduced as a <a href="https://www.erlang.org/doc/reference_manual/features.html">feature</a>
in Erlang/OTP 25. In that release, it was necessary to enable it both in
the compiler and the runtime system.</p>

<p>Erlang/OTP 26 lifted the necessity to enable <code>maybe</code> in the runtime system.</p>

<p>Now in Erlang/OTP 27, <code>maybe</code> is enabled by default in the compiler.
In the example from
<a href="https://www.erlang.org/blog/otp-26-highlights/#no-need-to-enable-feature-maybe-in-the-runtime-system">last year’s blog post</a>,
the line <code>-feature(maybe_expr, enable).</code> can now be removed:</p>

<pre><code>$ cat t.erl
-module(t).
-export([listen_port/2]).
listen_port(Port, Options) -&gt;
    maybe
        {ok, ListenSocket} ?= inet_tcp:listen(Port, Options),
        {ok, Address} ?= inet:sockname(ListenSocket),
        {ok, {ListenSocket, Address}}
    end.
$ erlc t.erl
$ erl
Erlang/OTP 27 . . .

Eshell V15.0  (abort with ^G)
1&gt; t:listen_port(50000, []).
{ok,{#Port&lt;0.5&gt;,{{0,0,0,0},50000}}}
</code></pre>

<p>When <code>maybe</code> is used as an atom, it need to be quoted. For example:</p>

<pre><code>will_succeed(. . .) -&gt; yes;
will_succeed(. . .) -&gt; no;
   .
   .
   .
will_succeed(_) -&gt; 'maybe'.
</code></pre>

<p>Alternatively, it is still possible to disable the <code>maybe_expr</code> feature. With
the feature disabled, <code>maybe</code> can be used as an atom without quotes.</p>

<p>One way to disable <code>maybe</code> is to use the <code>-disable-feature</code> option when compiling.
For example:</p>

<pre><code>erlc -disable-feature maybe_expr *.erl
</code></pre>

<p>Another way to disable <code>maybe</code> is to add the following directive to
the source code:</p>

<pre><code>-feature(maybe_expr, disable).
</code></pre>
      <h2 id="the-new-json-module">
        
        
          The new <code>json</code> module <a href="#the-new-json-module">#</a>
        
        
      </h2>
    

<p>There is a new module <a href="https://www.erlang.org/doc/man/json"><code>json</code></a> in
STDLIB for generating and parsing
<a href="https://en.wikipedia.org/wiki/JSON">JSON (JavaScript Object Notation)</a>.</p>

<p>It is implemented by <a href="https://github.com/michalmuskala">Michał
Muskała</a> who has also implemented
the <a href="https://github.com/michalmuskala/jason"><code>Jason</code></a> library for
Elixir. <code>Jason</code> is known for being faster than other pure Erlang or
Elixir JSON libraries. The <code>json</code> module is not a pure translation of
the Elixir code for Jason, but a re-implementation with even better
performance than <code>Jason</code>.</p>

<p>As an example, imagine that we have this file <code>quotes.json</code> with
quotes from the film <a href="https://en.wikipedia.org/wiki/Jason_and_the_Argonauts_(1963_film)">Jason and the
Argonauts</a>:</p>

<pre><code>[
    {"quote": "The gods are best served by those who need their help the least.",
     "attribution": "Zeus",
     "verified": true},
    {"quote": "Now the voyage is over, I don't want any trouble to begin.",
     "attribution": "Jason",
     "verified": true}
]
</code></pre>

<p>The JSON contents of the file can be be decoded by calling
<a href="https://www.erlang.org/doc/man/json#decode/1">json:decode/1</a>:</p>

<pre><code>1&gt; {ok,JSON} = file:read_file("quotes.json").
{ok,&lt;&lt;"[\n   {\"quote\": \"The gods are best served by those who need their help the least.\",\n    \"attribution\": \"Zeus\""...&gt;&gt;}
2&gt; json:decode(JSON).
[#{&lt;&lt;"attribution"&gt;&gt; =&gt; &lt;&lt;"Zeus"&gt;&gt;,
   &lt;&lt;"quote"&gt;&gt; =&gt;
       &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
   &lt;&lt;"verified"&gt;&gt; =&gt; true},
 #{&lt;&lt;"attribution"&gt;&gt; =&gt; &lt;&lt;"Jason"&gt;&gt;,
   &lt;&lt;"quote"&gt;&gt; =&gt;
       &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
   &lt;&lt;"verified"&gt;&gt; =&gt; true}]
</code></pre>

<p>By default, for safety, the keys for objects are translated to binaries. Using atoms
could open up for
<a href="https://en.wikipedia.org/wiki/Denial-of-service_attack">denial-of-service attacks</a>
if a malicious JSON object would define millions of unique keys.</p>

<p>For convenience, it is still possible to convert keys to atoms in
a safe way by using a <em>decoder callback</em>. Here is an example:</p>

<pre><code>1&gt; Push = fun(Key, Value, Acc) -&gt; [{binary_to_existing_atom(Key), Value} | Acc] end.
#Fun&lt;erl_eval.40.39164016&gt;
</code></pre>

<p>This fun converts the key for a JSON object to an <strong>existing</strong> atom,
or raises an exception if no such atom exists.</p>

<p>Since this example is run from the shell, we’ll need to make sure that all possible keys
are known atoms:</p>

<pre><code>2&gt; {quote,attribution,verified}.
{quote,attribution,verified}
</code></pre>

<p>This would normally not be necessary when JSON decoding is done in an Erlang module,
because the atoms to be used as keys would presumably be defined naturally by being used
when processing the decoded JSON objects.</p>

<p>With this preparation done, the JSON decoder can be called using the <code>Push</code> fun
as an <code>object_push</code> decoder callback:</p>

<pre><code>3&gt; {Qs,_,&lt;&lt;&gt;&gt;} = json:decode(JSON, [], #{object_push =&gt; Push}), Qs.
[#{quote =&gt;
       &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
   attribution =&gt; &lt;&lt;"Zeus"&gt;&gt;,verified =&gt; true},
 #{quote =&gt;
       &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
   attribution =&gt; &lt;&lt;"Jason"&gt;&gt;,verified =&gt; true}]
</code></pre>

<p>The <a href="https://www.erlang.org/doc/man/json#encode/1">json:encode/1</a> function encodes
an Erlang term to JSON:</p>

<pre><code>4&gt; io:format("~ts\n", [json:encode(Qs)]).
[{"quote":"The gods are best served by those who need their help the least.","attribution":"Zeus","verified":true},{"quote":"Now the voyage is over, I don't want any trouble to begin.","attribution":"Jason","verified":true}]
ok
</code></pre>

<p>The encoder accepts binaries, atoms, and integer as keys for objects,
so there is no need to customize encoding for this particular example.</p>

<p>However, when necessary, it is possible to customize the encoding. For
example, assume that we want to store each quotation in a three-tuple
instead of in a map:</p>

<pre><code>1&gt; Q = [{~"The gods are best served by those who need their help the least.",
~"Zeus",true},
{~"Now the voyage is over, I don't want any trouble to begin.",
~"Jason",true}].
[{&lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
  &lt;&lt;"Zeus"&gt;&gt;,true},
 {&lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
  &lt;&lt;"Jason"&gt;&gt;,true}]
</code></pre>

<p>The <code>json:encode/1</code> function does not handle that format by default, but it can be
handled by defining an <em>encoder function</em>:</p>

<pre><code>quote_encoder({Q, A, V}, Encode)
  when is_binary(Q), is_binary(A), is_boolean(V) -&gt;
    json:encode_map(#{quote =&gt; Q,
                      attribution =&gt; A,
                      verified =&gt; V},
                    Encode);
quote_encoder(Other, Encode) -&gt;
    json:encode_value(Other, Encode).
</code></pre>

<p>The first clause matches a tuple of size three that looks like a
quotation. If it matches, it is converted to the map representation
for a JSON object, which is then converted by the utility function
<a href="https://www.erlang.org/doc/man/json#encode_map/2">json:encode_map/1</a>
to JSON.</p>

<p>The second clause handles all other Erlang terms by calling the
default encoding function
<a href="https://www.erlang.org/doc/man/json#encode_value/2">json:encode_value/2</a>
for converting a term to JSON.</p>

<p>Assuming that this function is defined in module <code>t</code>, the conversion to JSON
is invoked as follows:</p>

<pre><code>2&gt; io:format("~ts\n", [json:encode(Q, fun t:quote_encoder/2)]).
[{"quote":"The gods are best served by those who need their help the least.","attribution":"Zeus","verified":true},{"quote":"Now the voyage is over, I don't want any trouble to begin.","attribution":"Jason","verified":true}]
</code></pre>

<p>The JSON encoder will call the callback recursively for given term. That can
be clearly seen if we modify the second clause of <code>quote_encoder/2</code> to also
print the value of <code>Other</code>:</p>

<pre><code>3&gt; json:encode(Q, fun t:quote_encoder/2), ok.
-- [{&lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;,
     &lt;&lt;"Zeus"&gt;&gt;,true},
    {&lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;,
     &lt;&lt;"Jason"&gt;&gt;,true}]
-- &lt;&lt;"quote"&gt;&gt;
-- &lt;&lt;"The gods are best served by those who need their help the least."&gt;&gt;
-- &lt;&lt;"attribution"&gt;&gt;
-- &lt;&lt;"Zeus"&gt;&gt;
-- &lt;&lt;"verified"&gt;&gt;
-- true
-- &lt;&lt;"quote"&gt;&gt;
-- &lt;&lt;"Now the voyage is over, I don't want any trouble to begin."&gt;&gt;
-- &lt;&lt;"attribution"&gt;&gt;
-- &lt;&lt;"Jason"&gt;&gt;
-- &lt;&lt;"verified"&gt;&gt;
-- true
</code></pre>
      <h2 id="process-labels">
        
        
          Process labels <a href="#process-labels">#</a>
        
        
      </h2>
    

<p>As an help for debugging or observing in general, labels can be now
set on non-registered processes using
<a href="https://www.erlang.org/doc/man/proc_lib#set_label/1"><code>proc_lib:set_label/1</code></a>.</p>

<p>The label is an arbitrary term. The label is shown by the the shell
command <code>i/0</code> and by <a href="https://www.erlang.org/doc/man/observer"><code>observer</code></a>.
They can also be found in the dictionary section of a
<a href="https://www.erlang.org/doc/man/crashdump_viewer">crash dump</a>.</p>

<p>Here is an example where five labeled quote-handler processes are started and
inspected:</p>

<pre><code>1&gt; F = fun(I) -&gt;
   spawn_link(fun() -&gt;
     proc_lib:set_label({quote_handler, I}),
     receive _ -&gt; ok end
   end)
   end.
#Fun&lt;erl_eval.42.39164016&gt;
2&gt; Ps = [F(I) || I &lt;- lists:seq(1, 5)].
[&lt;0.91.0&gt;,&lt;0.92.0&gt;,&lt;0.93.0&gt;,&lt;0.94.0&gt;,&lt;0.95.0&gt;]
3&gt; proc_lib:get_label(hd(Ps)).
{quote_handler,1}
4&gt; i().
Pid                   Initial Call                          Heap     Reds Msgs
Registered            Current Function                     Stack
&lt;0.0.0&gt;               erl_init:start/2                       987     5347    0
init                  init:loop/1                              2
   .
   .
   .
{quote_handler,1}     prim_eval:'receive'/2                    9
&lt;0.92.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,2}     prim_eval:'receive'/2                    9
&lt;0.93.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,3}     prim_eval:'receive'/2                    9
&lt;0.94.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,4}     prim_eval:'receive'/2                    9
&lt;0.95.0&gt;              erlang:apply/2                         233     4006    0
{quote_handler,5}     prim_eval:'receive'/2                    9
Total                                                     642876  1156835    0
                                                             438
ok
</code></pre>

<p>The SSH and and SSL applications have been updated to label the processes they
create.</p>
      <h2 id="new-functionality-in-stdlib">
        
        
          New functionality in STDLIB <a href="#new-functionality-in-stdlib">#</a>
        
        
      </h2>
    
      <h2 id="new-utility-functions-for-set-modules">
        
        
          New utility functions for set modules <a href="#new-utility-functions-for-set-modules">#</a>
        
        
      </h2>
    

<p>The three sets modules in STDLIB —
<a href="https://www.erlang.org/doc/man/sets"><code>sets</code></a>,
<a href="https://www.erlang.org/doc/man/gb_sets"><code>gb_sets</code></a>, and
<a href="https://www.erlang.org/doc/man/ordsets"><code>ordsets</code></a> —
have new functions <code>is_equal/2</code>, <code>map/2</code>, and <code>filtermap/2</code>.</p>

<p>The <code>is_equal/2</code> function is useful when one needs to find out whether two
sets contain the same elements. Comparing with <code>==</code> or <code>=:=</code> is not always
reliable. For example:</p>

<pre><code>1&gt; Seq = lists:seq(1, 20, 2).
[1,3,5,7,9,11,13,15,17,19]
2&gt; gb_sets:from_list(Seq) == gb_sets:delete(10, gb_sets:from_list([10|Seq])).
false
3&gt; gb_sets:is_equal(gb_sets:from_list(Seq), gb_sets:delete(10, gb_sets:from_list([10|Seq]))).
true
</code></pre>

<p>The <code>map/2</code> maps the element of a set, producing a new set:</p>

<pre><code>4&gt; Seq = lists:seq(1, 20, 2).
[1,3,5,7,9,11,13,15,17,19]
#Fun&lt;erl_eval.42.39164016&gt;
5&gt; ordsets:to_list(ordsets:map(fun(N) -&gt; N div 4 end, ordsets:from_list(Seq))).
[0,1,2,3,4]
</code></pre>

<p>The <code>filtermap/2</code> function can map and filter at the same time. Here is an example
showing how to multiply each integer in a set by 100 and remove non-integers:</p>

<pre><code>1&gt; Mixed = [1,2,3,a,b,c].
[1,2,3,a,b,c]
2&gt; F = fun(N) when is_integer(N) -&gt; {true,N * 100};
   (_) -&gt; false
   end.
#Fun&lt;erl_eval.42.39164016&gt;
3&gt; sets:to_list(sets:filtermap(F, sets:from_list(Mixed))).
[300,200,100]
</code></pre>
      <h2 id="new-timer-convenience-functions-that-take-funs">
        
        
          New <code>timer</code> convenience functions that take funs <a href="#new-timer-convenience-functions-that-take-funs">#</a>
        
        
      </h2>
    

<p>In Erlang/OTP 26, the functions in the
<a href="https://www.erlang.org/doc/man/timer"><code>timer</code></a> module don’t accept funs.
It is certainly possibly to pass in a fun in the argument for
<a href="https://www.erlang.org/doc/man/erlang#apply/2"><code>erlang:apply/2</code></a>,
but if one makes a mistake it will be only be noticed when the
timer expires:</p>

<pre><code>1&gt; timer:apply_after(10, erlang, apply, [fun() -&gt; io:put_chars("now!\n") end]).
{ok,{once,#Ref&lt;0.2380540714.1485570051.86513&gt;}}
=ERROR REPORT==== 10-Apr-2024::05:56:43.894073 ===
Error in process &lt;0.109.0&gt; with exit value:
{undef,[{erlang,apply,[#Fun&lt;erl_eval.43.105768164&gt;],[]}]}
</code></pre>

<p>Here the empty argument list for the fun was forgotten. It should have been:</p>

<pre><code>2&gt; timer:apply_after(10, erlang, apply, [fun() -&gt; io:put_chars("now!\n") end, []]).
{ok,{once,#Ref&lt;0.2380540714.1485570051.86522&gt;}}
now!
</code></pre>

<p>In Erlang/OTP 27, using a fun is much easier:</p>

<pre><code>1&gt; timer:apply_after(10, fun() -&gt; io:put_chars("now!\n") end).
{ok,{once,#Ref&lt;0.3845681669.1215561736.51634&gt;}}
now!
</code></pre>

<p>In systems that use hot code updating, using a local fun for a long-running
timer is not ideal. The code that defines the fun could have been replaced,
and when the timer finally expires the call will fail. Therefore, it is also
possible to pass a fun as well as its arguments, making it possible to use
use a remote fun that will survive hot code updating:</p>

<pre><code>2&gt; timer:apply_after(10, fun io:put_chars/1, ["now\n"]).
{ok,{once,#Ref&lt;0.3845681669.1215561736.51650&gt;}}
now
</code></pre>

<p>The <code>apply_interval/*</code> and <code>apply_repeatedly/*</code> functions now also accept
funs.</p>
      <h2 id="new-ets-functions">
        
        
          New <code>ets</code> functions <a href="#new-ets-functions">#</a>
        
        
      </h2>
    

<p>The new functions
<a href="https://www.erlang.org/doc/man/ets#first_lookup/1"><code>ets:first_lookup/1</code></a>
and
<a href="https://www.erlang.org/doc/man/ets#next_lookup/2"><code>ets:next_lookup/2</code></a>
simplifies and speeds up traversing an ETS table:</p>

<pre><code>1&gt; T = ets:new(example, [ordered_set]).
#Ref&lt;0.1968915180.2077884419.247786&gt;
2&gt; ets:insert(T, [{I,I*I} || I &lt;- lists:seq(1, 10)]).
true
3&gt; {K1,_} = ets:first_lookup(T).
{1,[{1,1}]}
4&gt; {K2,_} = ets:next_lookup(T, K1).
{2,[{2,4}]}
5&gt; {K3,_} = ets:next_lookup(T, K2).
{3,[{3,9}]}
6&gt; {K4,_} = ets:next_lookup(T, K3).
{4,[{4,16}]}
</code></pre>

<p>Similarly,
<a href="https://www.erlang.org/doc/man/ets#last_lookup/1"><code>ets:last_lookup/1</code></a>
and
<a href="https://www.erlang.org/doc/man/ets#prev_lookup/2"><code>ets:prev_lookup/2</code></a>
can be used to traverse a table in reverse order.</p>

<p>The new function
<a href="https://www.erlang.org/doc/man/ets#update_element/4"><code>ets:update_element/4</code></a>
is similar to
<a href="https://www.erlang.org/doc/man/ets#update_element/3"><code>ets:update_element/3</code></a>,
but makes it possible to supply a default object when there is no existing
object with the given key:</p>

<pre><code>1&gt; T = ets:new(example, []).
#Ref&lt;0.878413430.1983512583.205850&gt;
2&gt; ets:update_element(T, a, {2, true}, {a, true}).
true
3&gt; ets:lookup(T, a).
[{a,true}]
</code></pre>
      <h2 id="new-ssl-client-side-stapling-support">
        
        
          New SSL client-side stapling support <a href="#new-ssl-client-side-stapling-support">#</a>
        
        
      </h2>
    

<p>A new feature in the SSL client in Erlang/OTP 27 is support for <a href="https://en.wikipedia.org/wiki/OCSP_stapling">OCSP
stapling</a> for easier and
faster verification of the revocation status of server
certificates.</p>

<p>With OCSP stapling, the SSL client can streamline the validation of
revocation status. Normally the client would have to query the
<a href="https://en.wikipedia.org/wiki/Certificate_authority">CA (Certificate
Authority)</a> using
<a href="https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol">OCSP (Online Certificate Status
Protocol)</a>
to ensure that the server’s certificate has not been
<a href="https://en.wikipedia.org/wiki/Certificate_revocation">revoked</a>.</p>

<p>The basic idea behind OCSP stapling is that the server itself will
proactively query the CA regarding the revocation status for its own
certificate and “staple” the time-stamped OCSP response from the CA to
the certificate. When a client connects, the server passes along
its OCSP-stapled certificate to the client. To verify the revocation
status, the client only needs to check that the OCSP response was
signed by the CA.</p>

<p>Here follows an example showing how OCSP stapling can be enabled in the
SSL client:</p>

<pre><code>1&gt; ssl:start().
ok
2&gt; {ok, Socket} = ssl:connect("duckduckgo.com", 443,
                              [{cacerts, public_key:cacerts_get()},
                               {stapling, staple}]).
{ok,{sslsocket,{gen_tcp,#Port&lt;0.5&gt;,tls_connection,undefined},
               [&lt;0.122.0&gt;,&lt;0.121.0&gt;]}}
</code></pre>
      
    

<p>In Erlang/OTP 27, the new profiling tool
<a href="https://www.erlang.org/doc/man/tprof"><code>tprof</code></a>
joins the existing profiling tools
<a href="https://www.erlang.org/doc/man/cprof"><code>cprof</code></a>,
<a href="https://www.erlang.org/doc/man/eprof"><code>eprof</code></a>,
and <a href="https://www.erlang.org/doc/man/fprof"><code>fprof</code></a>.</p>

<p>Why introduce a new profiling tool?</p>

<p>One reason is that <code>cprof</code> and <code>eprof</code> perform similar profiling
tasks, but the naming of the API functions are different. It is quite
easy to mix up the names when running one tool after the other, and
running them after each other is not uncommon.  For example, when
trying to find a
<a href="https://en.wikipedia.org/wiki/Bottleneck_(software)">bottleneck</a> in a
complex running Erlang system, one approcach is to first use
<code>cprof</code> to get a rough idea of the general part of the system where a
bottleneck could be located. After that, <code>eprof</code> is run on a limited
part of the system trying to narrow it down. Directly running <code>eprof</code>
on a large Erlang application could overload it and bring it down.</p>

<p>Using <code>tprof</code>, the same function is used for both counting calls and
measuring the time for each call. Here is how to count calls when
<code>lists:seq(1, 1000)</code> is called:</p>

<pre><code>1&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_count}).
FUNCTION          CALLS  [    %]
lists:seq/2           1  [ 0.40]
lists:seq_loop/3    251  [99.60]
                         [100.0]
ok
</code></pre>

<p>Note that call counting is always done for all processes.</p>

<p>The bulk of the work for <code>lists:seq/2</code> is done in <code>lists:seq_loop/3</code>,
which was called 251 times. Since we asked for 1000 integers, we
reach the conclusion that each tail-recursive call to <code>seq_loop/3</code>
creates four list elements at once. That can be confirmed by
looking at the
<a href="https://github.com/erlang/otp/blob/ca50a5d73703f74e2eae1ca40bbe6c4f027f9f98/lib/stdlib/src/lists.erl#L409-L416">source code</a>.</p>

<p>To measure the time for each call, we only need to replace
<code>call_count</code> with <code>call_time</code>:</p>

<pre><code>2&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_time}).

****** Process &lt;0.94.0&gt;  --  100.00% of total ***
FUNCTION          CALLS  TIME (μs)  PER CALL  [     %]
lists:seq/2           1          0      0.00  [  0.00]
lists:seq_loop/3    251         50      0.20  [100.00]
                                50            [ 100.0]
ok
</code></pre>

<p>Call time is only measured the process that called
<a href="https://erlang.org/doc/man/tprof#profile/4"><code>tprof:profile/4</code></a>
and any process spawned by that process.</p>

<p>By replacing <code>call_time</code> with <code>call_memory</code> the amount of memory consumed
by each call will be measured:</p>

<pre><code>3&gt; tprof:profile(lists, seq, [1, 1000], #{type =&gt; call_memory}).

****** Process &lt;0.97.0&gt;  --  100.00% of total ***
FUNCTION          CALLS  WORDS  PER CALL  [     %]
lists:seq_loop/3    251   2000      7.97  [100.00]
                          2000            [ 100.0]
ok
</code></pre>

<p>The total number of words created is 2000, which make sense since each
list element needs 2 words. The number of words consumed per call is
<code>2000 / 251</code>, which is approximately 7.97 or almost 8. That also makes
sense since each tail-recursive call creates 4 list elements, or 8
words, and there are 250 such calls. The remaining call creates the final
empty list (<code>[]</code>).</p>

<p><code>call_memory</code> tracing was introduced in the runtime system in
Erlang/OTP 26, but was not exposed in any existing profiling tool
because it didn’t really fit in any of them. It made more sense to enable
support for it in a new tool.</p>
      <h2 id="multiple-trace-sessions">
        
        
          Multiple trace sessions <a href="#multiple-trace-sessions">#</a>
        
        
      </h2>
    

<p>Tracing makes it possible to observe, debug, analyse, and measure the
performance of a running Erlang system. Over the year, numerous tools
using tracing has been developed. In Erlang/OTP alone, several tools
leverage tracing for different purposes:</p>

<ul>
  <li>
    <p><a href="https://www.erlang.org/doc/man/dbg"><code>dbg</code></a>, <a href="https://www.erlang.org/doc/man/ttb"><code>ttb</code></a> -
general tracing tools</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/etop"><code>etop</code></a> - similar to <code>top</code> in Unix</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/eprof"><code>eprof</code></a>,
<a href="https://www.erlang.org/doc/man/cprof"><code>cprof</code></a>,
<a href="https://www.erlang.org/doc/man/fprof"><code>fprof</code></a>,
<a href="https://www.erlang.org/doc/man/tprof"><code>tprof</code></a> - profiling tools</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/apps/et"><code>et</code></a> - event tracer</p>
  </li>
  <li>
    <p><a href="https://www.erlang.org/doc/man/debugger"><code>debugger</code></a> - uses tracing
internally when evaluating <code>receive</code> expressions</p>
  </li>
</ul>

<p>In Erlang/OTP 26 and earlier tracing had some limitations:</p>

<ul>
  <li>
    <p>There could only be a single tracer per traced process.</p>
  </li>
  <li>
    <p>The configuration for which processes and functions to trace were
global within the runtime system.</p>
  </li>
</ul>

<p>Those limitations meant that different tracing tools could easily step
on each other’s toes. The treacherous part was that using multiple tracing
tools at the same time would seem to work for a while… until it didn’t.</p>

<p>In Erlang/OTP 27, multiple trace sessions can be created. Each trace
session has its own tracer process and configuration for which
processes and functions to trace.</p>

<p>To create a trace session and set up tracing, there is the new
<a href="https://www.erlang.org/doc/man/trace"><code>trace</code></a> module in the Kernel
application. Tools that set up tracing using that module will no longer
interfere with each other. Tools that use the
<a href="https://www.erlang.org/doc/man/erlang#trace/3">old API</a>
will share a single global trace session.</p>

<p>In the initial Erlang/OTP 27 release, some of the tools using tracing
have been updated to use trace sessions. Other tools will be updated in
upcoming maintenance releases.</p>

<p>We have tried to design the new API in a way to make it relatively
easy for maintainers of external tools to migrate their code.  Apart
from the names of the functions and the first argument (the session
argument), the other arguments and their semantics are almost entirely
identical to the old API.</p>
      <h2 id="quick-trace-session-example">
        
        
          Quick trace session example <a href="#quick-trace-session-example">#</a>
        
        
      </h2>
    

<p>Here is an example to show how the new API is used. First we’ll need
a tracer process that prints all trace messages it receives:</p>

<pre><code>1&gt; Tracer = spawn(fun F() -&gt; receive M -&gt; io:format("== ~p ==\n", [M]), F() end end).
&lt;0.90.0&gt;
</code></pre>

<p>Having a tracer process, we can create a trace session:</p>

<pre><code>2&gt; Session = trace:session_create(my_session, Tracer, []).
{#Ref&lt;0.179442114.3923902468.103849&gt;,{my_session,0}}
</code></pre>

<p>Next we turn on call tracing on the current process:</p>

<pre><code>3&gt; trace:process(Session, self(), true, [call]).
1
</code></pre>

<p>Make sure that module <code>array</code> is loaded and trace all calls in it:</p>

<pre><code>4&gt; l(array).
{module,array}
5&gt; trace:function(Session, {array,'_','_'}, [], [local]).
89
</code></pre>

<p>Next create a new array:</p>

<pre><code>6&gt; array:new(10).
== {trace,&lt;0.88.0&gt;,call,{array,new,"\n"}} ==
{array,10,0,undefined,10}
== {trace,&lt;0.88.0&gt;,call,{array,new_0,[10,0,false]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new_1,["\n",0,false,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new_1,[[],10,true,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,new,[10,true,undefined]}} ==
== {trace,&lt;0.88.0&gt;,call,{array,find_max,"\t\n"}} ==
</code></pre>

<p>Note that trace messages are randomly intermingled with the return value
of the call.</p>

<p>When we are done, we can destroy the session:</p>

<pre><code>7&gt; trace:session_destroy(Session).
</code></pre>

<p>If we don’t destroy the session, it will be automatically destroyed when
the last reference to it goes away.</p>
      <h2 id="native-coverage-support">
        
        
          Native coverage support <a href="#native-coverage-support">#</a>
        
        
      </h2>
    

<p>The <a href="https://www.erlang.org/doc/man/cover">Cover</a> tool for determining
<a href="https://en.wikipedia.org/wiki/Code_coverage">code coverage</a> has long been
part of Erlang/OTP.</p>

<p>Traditionally, Cover collected its coverage metrics without the
help of any specialized functionality in the runtime system. To count how
many times each line in a module was executed, Cover
<a href="https://en.wikipedia.org/wiki/Instrumentation_(computer_programming)">instrumented</a>
abstract code for the module by inserting calls to
<a href="https://www.erlang.org/doc/man/ets#update_counter/3"><code>ets:update_counter/3</code></a>
on each executable line.</p>

<p>That worked, but the cover-instrumented Erlang code would always run
slower. How much slower depended on the nature of the code being
tested.</p>

<p>In Erlang/OTP 27, runtime systems supporting the
<a href="https://www.erlang.org/blog/a-first-look-at-the-jit/">JIT (just-in-time compiler)</a>
can now collect coverage metrics in the runtime system with minimal
performance overhead.</p>

<p>The Cover tool has been updated to automatically take advantage of
native coverage support if supported by the runtime system. When
running the test suites for most OTP applications, there is no
noticeable difference in execution time running with and without
Cover.</p>

<p>The native coverage support can also be used directly for performing
measurements that Cover cannot accomplish, such as collecting metrics
for code that is executed while the Erlang runtime system is starting.</p>

<p>Here is a quick example showing how we can collect coverage metrics
for <code>init</code>, which is the first module executed when starting up the
runtime system. First we need to instruct the runtime system to
instrument all functions in all modules with extra code to count the
number of times each function is called:</p>

<pre><code>$ bin/erl +JPcover function_counters
</code></pre>

<p>The runtime system starts normally. We can now read out the counters
for the <code>init</code> module:</p>

<pre><code>1&gt; lists:reverse(lists:keysort(2, code:get_coverage(function, init))).
[{{archive_extension,0},392},
 {{get_argument1,2},198},
 {{objfile_extension,0},101},
 {{boot_loop,2},64},
 {{request,1},55},
 {{to_strings,1},44},
 {{do_handle_msg,2},38},
 {{handle_msg,2},38},
 {{b2s,1},38},
 {{get_argument,2},33},
 {{get_argument,1},31},
 {{'-load_modules/2-lc$^0/1-0-',1},30},
 {{'-load_modules/2-lc$^1/1-2-',1},30},
 {{'-load_modules/2-lc$^2/1-3-',1},30},
 {{'-load_modules/2-lc$^3/1-4-',1},30},
 {{extract_var,2},30},
 {{'-prepare_loading_fun/0-fun-0-',3},29},
 {{eval_script,2},23},
 {{append,1},18},
 {{get_arguments,1},18},
 {{reverse,1},17},
 {{check,2},17},
 {{ensure_loaded,2},16},
 {{ensure_loaded,1},16},
 {{do_load_module,2},14},
 {{do_ensure_loaded,2},14},
 {{get_flag_args,...},12},
 {{...},...},
 {...}|...]
</code></pre>

<p>The returned list of counter values for each function is sorted in
descending order on the number of time each function was executed.</p>

<p>For more information, see
<a href="https://www.erlang.org/doc/man/code#module-native-coverage-support">Native Coverage Support</a>
in the documentation for the <code>code</code> module.</p>
      <h2 id="deprecating-archives">
        
        
          Deprecating archives <a href="#deprecating-archives">#</a>
        
        
      </h2>
    

<p><a href="https://www.erlang.org/doc/man/code#module-loading-of-code-from-archive-files">Archives</a>
is experimental functionality that has existed in Erlang/OTP for a
long time. Part of the support for archives is deprecated in Erlang/OTP 27.</p>

<p>The reason is that the performance of code loading from archives has
never been great. Even worse is that the very existence of the archive
functionality degrades the performance of code loading even when no
archives are used, and complicates or prevents optimizations aimed at
reducing startup time.</p>

<p>In Erlang/OTP 27, the following functionality is deprecated:</p>

<ul>
  <li>
    <p>Using archives for packaging a single application or parts of a single application
into an archive file that is included in the code path. This functionality will
likely be removed in Erlang/OTP 28.</p>
  </li>
  <li>
    <p>The <a href="https://www.erlang.org/doc/man/code#lib_dir/2"><code>code:lib_dir/2</code></a>
function. This function was introduced to allow reading files
inside archives. In Erlang/OTP 28, the function itself will not be
removed, but it will most likely no longer support looking into
archives.</p>
  </li>
  <li>
    <p>All functionality to handle archives in module
<a href="https://www.erlang.org/doc/man/erl_prim_loader"><code>erl_prim_loader</code></a>.
That same functionality is likely to be removed in Erlang/OTP 28.</p>
  </li>
  <li>
    <p>The <code>-code_path_choice</code> flag for <code>erl</code>. In Erlang/OTP 27, the default
has changed from <code>relaxed</code> to <code>strict</code>. This flag is likely to be removed
in Erlang/OTP 28.</p>
  </li>
</ul>

<p>In order to use archives in Erlang/OTP 27, it is necessary to use the flag
<code>-code_path_choice relaxed</code>.</p>
      <h2 id="using-a-single-archive-in-an-escript-is-not-deprecated">
        
        
          Using a single archive in an Escript is <strong>not</strong> deprecated <a href="#using-a-single-archive-in-an-escript-is-not-deprecated">#</a>
        
        
      </h2>
    

<p>An archive can still be used to hold all files needed by an
<a href="https://www.erlang.org/doc/apps/erts/escript_cmd">Escript</a>.
However, to access files in the archive (for example, to read templates or other
data files), the only supported way guaranteed to work in future
releases is to use the
<a href="https://www.erlang.org/doc/man/escript#extract/2"><code>escript:extract/2</code></a>
function.</p>

        
    </div>
</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Taking Risk (117 pts)]]></title>
            <link>https://tomblomfield.com/post/750852175114174464/taking-risk</link>
            <guid>40424938</guid>
            <pubDate>Tue, 21 May 2024 06:46:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomblomfield.com/post/750852175114174464/taking-risk">https://tomblomfield.com/post/750852175114174464/taking-risk</a>, See on <a href="https://news.ycombinator.com/item?id=40424938">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How Might We Learn? (164 pts)]]></title>
            <link>https://andymatuschak.org/hmwl/</link>
            <guid>40424536</guid>
            <pubDate>Tue, 21 May 2024 05:33:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andymatuschak.org/hmwl/">https://andymatuschak.org/hmwl/</a>, See on <a href="https://news.ycombinator.com/item?id=40424536">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="Transcript">
        <h2 id="ideal-learning-environment">Your ideal learning environment <a href="#ideal-learning-environment">#</a></h2>
<p data-timestamp="245">Talks about learning technology often center on technology. Instead, I want to begin by asking: <em>what do you want learning to be like—for yourself?</em> If you could snap your fingers and drop yourself into a perfect learning environment, what’s your ideal?</p>
<p data-timestamp="268">One way to start thinking about this question is to ask: what were the most rewarding high-growth periods of your life?</p>
<p data-timestamp="278">I’ve noticed two patterns in answers to this question: first, people will tell me about a time when they learned a lot, but <em>learning wasn’t the point</em>. Instead, they were immersed in a situation with real personal meaning—like a startup, a research project, an artistic urge, or just a fiery curiosity. They dove in, got their hands dirty, and learned whatever was important along the way. And secondly: in these stories, <em>learning really worked</em>. People emerged feeling transformed, newly capable, filled with insight and understanding that has stayed with them years later.</p>
<p data-timestamp="323">These stories are so vivid because learning isn’t usually like this. People are often telling me somewhat wistfully about experiences that happened years or decades earlier. Learning rarely feels so subordinated to an authentic pursuit. Often if we try to “just dive in”, we hit a brick wall, or find ourselves uneasily cargo culting others, with no real understanding.</p>
<p data-timestamp="348">Why can’t we “just dive in” all the time?</p>
<p data-timestamp="351">Instead it often feels like we have to put our aims on hold while we go do some homework—learn properly. Worse, learning so often just doesn’t really work! We take the class, we read the book… but when we try to put that knowledge into practice, it’s fragile; it doesn’t transfer well. Then we’ll often find that we’ve forgotten half of it by the time we try to use it.</p>
<p data-timestamp="377">Why does learning so often fail to actually work?</p>
<img src="https://andymatuschak.org/hmwl/img/implicit-guided.png" alt=""><p data-timestamp="380"> These questions connect to an age-old conflict among educators and learning scientists—between implicit learning (also called discovery learning, inquiry learning, or situated learning), and guided learning (often represented by cognitive psychologists). Advocates of implicit learning methods argue that we should prioritize discovery, motivation, authentic involvement, and being situated in a community of practice. In the opposing camp, cognitive psychologists argue that you really do need to pay attention to architecture of cognition, long-term memory, procedural fluency, and to scaffold appropriately for cognitive load.</p>
<p data-timestamp="419">In my view, each of these points of view contains a lot of truth. And they each wrongly deny the other’s position, to their mutual detriment. Implicit learning aptly recognizes meaning and emotion, but ignores the often decisive constraints of cognition—what we usually need to make “learning actually work”. Guided learning advocates are focused on making learning work, and they sometimes succeed, but usually by sacrificing the purposeful sense of immersion we love about those rewarding high-growth periods.</p>
<img src="https://andymatuschak.org/hmwl/img/middle.png" alt=""><p data-timestamp="452"> One obvious approach is to try to compromise. Project-based learning is a good representation of that. By creating a scaffolded sequence of projects, the suggestion is that we can get some of the benefits of implicit learning—authenticity, motivation, transfer—while also exerting some of the instructional control and cognitive awareness typical of traditional courses. But so often it ends up getting the worst of both worlds—neither motivation and meaning, nor adequate guidance, explanation, and cognitive support.</p>
<p data-timestamp="482">In university, I was interested in 3D game programming, so I took a project-based course on computer graphics. The trouble was that <em>those projects weren’t my projects</em>. So a few weeks in, I found myself implementing a ray marching shader for more efficient bump mapping. Worse, because this course was trying to take project-based learning seriously, there weren't long textbook readings or problem sets. I found myself just translating math I’d been given into code. What I ended up with was a project I didn't care about, implementing math I didn't understand.</p>
<p data-timestamp="513">Instead, I suggest: we should take both views seriously, and find a way to synthesize the two. You really do want to make doing-the-thing the primary activity. But the realities of cognitive psychology mean that in many cases, you really do need explicit guidance, scaffolding, practice, and attention to memory support.</p>
<p data-timestamp="534">Learning by immersion works naturalistically when the material has a low enough complexity relative to your prior knowledge that you can successfully process it on the fly, and when natural participation routinely reinforces everything important, so that you build fluency. When those conditions aren’t satisfied—which is most of the time—you’ll need some support.</p>
<p data-timestamp="552">You want to just dive in, and you want learning to actually work. To make that happen, we need to infuse your authentic projects with guided support, where necessary, inspired by the best ideas from cognitive psychology. And if there’s something that requires more focused, explicit learning experiences, you want those experiences to be utterly in service to your actual aims.</p>
<p data-timestamp="576">I’ve been thinking about this synthesis for many years, and honestly: I’ve mostly been pretty stuck! Recently, though, I’ve been thinking a lot about AI. I know: eye-roll! Pretty much every mention of AI in learning technologies gets an eye-roll from me. But I confess: the possibility of AI has helped me finally get what feels like some traction on this problem. I’d like to share some of those early concepts today.</p>
<h2 id="tractable-immersion">Demo, part 1: Tractable immersion <a href="#tractable-immersion">#</a></h2>
<p data-timestamp="604">We’ll explore this possible synthesis through a story in six parts.</p>
<img src="https://andymatuschak.org/hmwl/img/sam.png" alt=""><p data-timestamp="608"> Meet Sam. Sam studied computer science in university, and they’re now working as a software engineer at a big tech company. But Sam’s somewhat bored at their day job. Not everything is boring, though: every time Sam sees a tweet announcing a new result in brain-computer interfaces, they’re absolutely captivated. These projects seem so much more interesting. Sam pull up the papers, looking for some way to contribute, but they hit a brick wall—so many unfamiliar topics, all at once.</p>
<img src="https://andymatuschak.org/hmwl/img/1-start.png" alt=""><p data-timestamp="639"> What if Sam could ask for help finding some meaningful way to start participating? With Sam’s permission, our AI—and let’s assume it’s a local AI—can build up a huge amount of context about their background. From old documents on Sam’s hard drive, our AI knows all about their university coursework. It can see their current skills through work projects. It knows something about Sam’s interests through their browsing history.</p>
<p data-timestamp="666">Sam’s excited about the idea of reproducing the paper’s data analysis. It seems to play to their strengths. They notice that the authors used a custom Python package to do their analysis, but that code was never published. That seems intriguing: Sam’s built open-source tools before. Maybe they could contribute here by building an open source version of this signal processing pipeline.</p>
<h2 id="guidance-in-action">Demo, part 2: Guidance in action <a href="#guidance-in-action">#</a></h2>
<p data-timestamp="689">So—Sam dives in. They’ve found an open-access dataset, and they’ve taken the first steps to start working with it. Tools like Copilot help Sam get started, but to follow some of these signal processing steps, what Sam really needs here is something like Copilot, but with awareness of the paper in addition to the code, and with context about what Sam’s trying to do.</p>
<img src="https://andymatuschak.org/hmwl/img/2-view-sample.png" alt=""><p data-timestamp="714"> This AI system isn’t trapped in its own chatbox, or in the sidebar of one application. It can see what’s going on across multiple applications, and it can propose <em>actions</em> across multiple applications. Sam can click that button to view a changeset with the potential implementation. Then they can continue the conversation, smoothly switching into the context of the code editor.</p>
<p data-timestamp="740">Like, what’s this “axis=1” parameter? The explanation depends on context from the code editor, the paper being implemented, and also documentation that came with the dataset Sam’s working with. The AI underlines assumptions made based on specific information, turning them into links.</p>
<img src="https://andymatuschak.org/hmwl/img/2-readme.png" alt=""><p data-timestamp="761"> Here Sam clicks on that “In this dataset” link, and our AI opens the README to the relevant line.</p>
<p data-timestamp="769">All this is to support our central aim—that Sam can immerse themselves, as much as possible, in what they’re actually trying to do, but get the support they need to understand what they’re doing.</p>

<p data-timestamp="783">That support doesn’t have to just mean text.</p>
<img src="https://andymatuschak.org/hmwl/img/3-dynamic-media.png" alt=""><p data-timestamp="786"> Sam next needs to implement a downsampling stage. This time, guidance includes synthesized dynamic media, so that Sam can understand what downsampling does through scaffolded immersion.</p>
<p data-timestamp="799">Sam doesn’t need to read an abstract explanation and try to imagine what that would do to different signals: instead, as they try different sampling rates, realtime feedback can help them internalize the effect on different signals. By playing with the dynamic media, Sam notices that some of the peaks are lost when the signal is downsampled.</p>
<p data-timestamp="819">These dynamic media aren’t trapped in the chatbox. They’re using the same input data and libraries Sam’s using in their notebook. At any time, Sam can just “view source” to tinker with this figure, or to use some of its code in their own notebook.</p>
<h2 id="contextualized-study">Demo, part 4: Contextualized study <a href="#contextualized-study">#</a></h2>
<p data-timestamp="838">Now Sam presses on but as they dig into band-pass filters, the high-level explanations they can get from these short chat interactions really just don’t feel like enough. What’s a frequency domain? What’s a Nyquist rate? Sam can copy and paste some AI-generated code, but they don’t understand what’s going on at all. A chat interface is just not a great medium for long-from conceptual explanation. It’s time for something deeper.</p>
<img src="https://andymatuschak.org/hmwl/img/4-toc.png" alt=""><p data-timestamp="872"> The AI knows Sam’s background and aims here, so it suggests an undergraduate text with a practical focus. More importantly, the AI reassures Sam that they don’t necessarily need to read this entire thousand-page book right now. It focuses on Sam’s goal here and suggests a range of accessible paths that Sam can choose according to how deeply they’d like to understand these filters. It's made a personal map in the book’s table of contents.</p>
<p data-timestamp="902">So that, for instance, if Sam just wants to understand what these filters are doing and why, there's a 25-page path for that. But if they want to know the mathematical background—how these filters work—there's a deeper path. And if they want to be able to implement these filters themselves, there's an even deeper path. Sam can choose their journey here.</p>
<img src="https://andymatuschak.org/hmwl/img/4-note.png" alt=""><p data-timestamp="912"> When Sam digs into the book, they'll find notes from the AI at the start of each section, and scattered throughout, which ground the material in Sam’s context, Sam’s project, Sam’s purpose. “This section will help you understand how to think of signals in terms of frequency spectra. That’s what low-pass filters manipulate.” Sam’s spending some time away from their project, in a more traditionally instructional setting, but that doesn’t mean the experience has to lose its connection to their authentic practice.</p>
<p data-timestamp="952">Incidentally, I’ve heard some technologists suggest that we should use AI to synthesize the whole book, bespoke for each person. But I think there’s a huge amount of value in shared canonical artifacts—in any given field, there are key texts that everyone can refer to, and they form common ground for the culture. I think we can preserve those by layering personalized context as a lens on top of texts like this.</p>
<p data-timestamp="978">In my ideal future, of course, our canonical shared artifacts are dynamic media, not digital representations of dead trees. But until all of our canonical works are rewritten, as a transitional measure, we can at least wave our hands and imagine that our AI could synthesize dynamic media versions of figures like this one.</p>
<p data-timestamp="998">Now, as Sam reads through the book, they can continue to engage with the text by asking questions, and our AI’s responses will continue to be grounded in their project.</p>
<p data-timestamp="1008">As Sam highlights the text or makes comments about details which seem particularly important or surprising, those annotations won’t end up trapped in this PDF: they’ll feed into future discussion and practice, as we’ll see later.</p>
<img src="https://andymatuschak.org/hmwl/img/4-q-from-ai.png" alt=""><p data-timestamp="1022"> In addition to Sam asking questions of the AI, the AI can insert questions for Sam to consider—again, grounded in their project—to promote deeper processing of the material.</p>
<img src="https://andymatuschak.org/hmwl/img/4-exercise.png" alt=""><p data-timestamp="1032"> And just as our AI guided Sam to the right sections of this thousand page book, it could point out which exercises might be most valuable, considering both Sam’s background and their aims. And it can connect the exercises to Sam’s aims so that, aspirationally, doing those problems feels continuous with Sam’s authentic practice. Even if the exercises do still feel somewhat decontextualized, Sam can at least feel more confident that the work is going to help them do what they want to do.</p>
<h2 id="practice-and-memory">Interlude: Practice and memory <a href="#practice-and-memory">#</a></h2>
<p data-timestamp="1068">Sam ends the day with some rewarding progress on their project, and a newfound understanding of quite a few topics. But this isn’t yet robust knowledge. Sam has very little fluency—if they try to use this material seriously, they’ll probably feel like they’re standing on shaky ground. And more prosaically, <em>they’re likely to forget much of what they just learned</em>.</p>
<p data-timestamp="1092">I’d like to focus on memory for a moment. It’s worth asking: why do we sometimes remember conceptual material, and sometimes not? Often we take a class, or read a book, or even just look something up, and find that a short time later, we’ve retained almost nothing. But sometimes things seem to stick. Why is that?</p>
<p data-timestamp="1115">There are some easier cases. If you’re learning something new in a domain you know well, each new fact connects to lots of prior knowledge. That creates more cues for recall and more opportunities for reinforcement. And if you’re in some setting where you need that knowledge every single day, you’ll find that your memory becomes reliable pretty quickly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-works.png" alt=""><p data-timestamp="1134"> Conceptual material like what Sam’s learning doesn’t usually get reinforced every day like that. But sometimes the world conspires to give those memories the reinforcement they need. Sometimes you read about a topic, then later that evening, that topic comes up in conversation with a collaborator. You have to retrieve what you learned, and that retrieval reinforces the memory. Then, maybe, two days later you need to recall that knowledge again for a project. Each time you reinforce the memory this way, you forget it more slowly. Now perhaps a week can go by, and you’re still likely to remember. Then maybe a few weeks, and a few months, and so on. With a surprisingly small number of retrievals, if they’re placed close enough to each other to avoid forgetting, you can retain that knowledge for months or years.</p>
<img src="https://andymatuschak.org/hmwl/img/5-fails.png" alt=""><p data-timestamp="1185"> By contrast, sometimes when you learn something, it doesn’t come up again until the next week. Then you try to retrieve that knowledge, but maybe it’s already been forgotten. So you have to look it up. That doesn’t reinforce your memory very much. And then if it doesn’t come up again for a while longer, you may still not remember next time. So you have to look it up yet again. And so on. The key insight here is that it’s possible to <em>arrange</em> the top timeline for yourself.</p>
<p data-timestamp="1216">Courses sometimes do, when each problem set consistently interleaves knowledge from all the previous ones. But immersive learning—and for that matter most learning—usually doesn’t arrange this properly, so you usually forget a lot. What if this kind of reinforcement were woven into the grain of the learning medium?</p>
<p data-timestamp="1239">My collaborator Michael Nielsen and I created a quantum computing primer, <a href="https://quantum.country/">Quantum Country</a>, to explore this idea. It’s available for free online. If you head to <a href="https://quantum.country/">quantum.country</a>, you’ll see what looks at first like a normal book.</p>
<img src="https://andymatuschak.org/hmwl/img/5-qc.png" alt=""><p data-timestamp="1255"> After a few minutes of reading, the text is interrupted with a small set of review questions. They’re designed to take just a few seconds each: think the answer to yourself, then mark whether or not you were able to answer correctly. So far, these look like simple flashcards. But as we’ve discussed, even if you can answer these questions now, that doesn’t mean you’ll be able to in a few weeks, or even in a few days.</p>
<p data-timestamp="1285">Notice these markings along the bottom of each question. These represent intervals. So you practice the questions while you’re reading the text, then, one week later, you’ll get an email that says: “Hey, you’ve probably started to forget some of what you read. Do you want to take five minutes to quickly review that material again?” Each time you answer successfully, the interval increases—to a few weeks, then a few months, and so on. If you begin to forget, then the intervals tighten up to provide more reinforcement.</p>
<img src="https://andymatuschak.org/hmwl/img/5-conceptual.png" alt=""><p data-timestamp="1314"> You may have seen systems like this before. Language learners and medical students often use tools called spaced repetition memory systems to remember vocabulary and basic facts. But the same cognitive mechanisms should work for more complex conceptual knowledge as well. There are 112 of these questions scattered through the first chapter of the book on that basis.</p>
<p data-timestamp="1337">Quantum Country is a new medium—a mnemonic medium—integrating a spaced repetition memory system with an explanatory text to make it easier for people to absorb complex material reliably.</p>
<img src="https://andymatuschak.org/hmwl/img/5-retention.png" alt=""><p data-timestamp="1350"> We now have millions of practice data points, so we can start to see how well it’s working. This plot shows the amount of time spent practicing, on the x axis, versus a reader’s demonstrated retention on the y axis— that is, how long a reader was able to go without practicing, and still answer at least 90% of questions correctly. These five dots represent the median user’s first five repetitions, for the first chapter. Notice that the y axis is logarithmic, so we’re seeing a nice exponential growth here. Each extra repetition—constant extra input—yields increasing output—i.e. retention.</p>
<p data-timestamp="1393">In exchange for about an hour and a half of total practice, the median reader was able to correctly answer over a hundred detailed questions about the first chapter, after more than two months without practice. Now, the first chapter takes most readers about four hours to read the first time, so this plot implies that an extra overhead of less than 50% in time commitment can yield months or years of detailed retention.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-1.png" alt=""><p data-timestamp="1421"> It’s also interesting to explore the counterfactual: how much would people forget without the extra reinforcement? As an experiment, we removed nine questions from the first chapter for some readers, then covertly reinserted the questions into those readers’ practice sessions a month later. This graph shows what happened.</p>
<p data-timestamp="1442">These nine points represent those nine questions. The y axis shows the percentage of readers who were able to answer that question correctly after one month, with no support at all. You can see that some questions are harder than others. One month later, the majority of readers missed the hardest three questions, on the left, about 30% missed the middle three, and about 15% missed the easiest three.</p>
<p data-timestamp="1467">Another group of users got practice while reading the essay, like we saw in the video a moment ago—and for any questions they missed, a bonus round of practice the next day. Then these questions disappeared for a month, at which point they were tested. These readers perform noticeably better, though a big chunk of them are still missing some of these questions.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-3.png" alt=""><p data-timestamp="1488"> Here’s one last group, like the previous one, except they got one extra round of practice a week after reading the book. Then we tested them again at the one month mark, and that’s what you’re seeing here. Each question takes six seconds on average to answer, so this is less than a minute of extra practice in total for these nine questions. But now for all of these questions, at least 90% of readers were able to answer correctly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-forgetting-q1.png" alt=""><p data-timestamp="1512"> Of course, some readers have a much easier time than others. This left plot focuses on the bottom quartile of users—the readers who missed the most questions while they were first reading the essay. Notice I’ve had to lengthen the y axis downwards. We can see that without any practice, most of them forgot two thirds of these held-out questions. In-essay practice alone still left roughly half of them forgetting roughly half of the questions. But with one extra round of practice, even this bottom quartile of readers performs quite well.</p>
<p data-timestamp="1557">Systems like Quantum Country are useful for more than just quantum computing. In my personal practice, I’ve accumulated thousands and thousands of questions. I write questions about scientific papers, about conversations, about lectures, about memorable meals. All this makes my daily life more rewarding, because I know that if I invest my attention in something, I will internalize it indefinitely.</p>
<p data-timestamp="1596">Central to this is the idea of a daily ritual, a vessel for practice. Like meditation and exercise, I spend about ten minutes a day using my memory system. Because these exponential schedules are very efficient, those ten minutes are enough to maintain my memory for thousands of questions, and to allow me to add up to about forty new questions each day.</p>
<p data-timestamp="1620">But I want to mention a few problems with these memory systems.</p>
<p data-timestamp="1627">One is pattern matching: once a question comes up a few times, I may recognize the text of the question without really thinking about it. This creates the unpleasant feeling of parroting, but I suspect it often leaves my memory brittle: I’ll remember the answer, but only when cued exactly as I’ve practiced. I wish the questions had more variability.</p>
<p data-timestamp="1649">Likewise, the questions are necessarily somewhat abstract. When I face a real problem in that domain, I won’t always recognize what knowledge I should use, or how to adapt it to the situation. A cognitive scientist would say I need to acquire schemas.</p>
<p data-timestamp="1667">Unless I intervene, questions stay the same over years. They’re maintaining memory—but ideally, they would push for further processing—more depth over time.</p>
<p data-timestamp="1679">Finally, returning to this talk’s thesis: memory systems are often too disconnected from my authentic practice. Say I’m studying a topic in signal processing for a creative project. Unless I’m careful, those questions probably won’t feel very connected to my project—they’ll feel like generic textbook questions about signal processing.</p>
<h2 id="dynamic-practice">Demo, part 5: Dynamic practice <a href="#dynamic-practice">#</a></h2>
<p data-timestamp="1704">Let’s return to Sam now, and see if we can apply some of these ideas about practice and memory. Sam did the work to study that signal processing material, so they want to make sure it actually sticks.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-1.png" alt=""><p data-timestamp="1716"> They install a home screen widget, which ambiently exposes them to practice prompts drawn from highlights, questions asked, and other activity the AI can access. Sam can flip through these questions while waiting in line or on the bus. Notice that this isn’t a generic signal processing question: it’s grounded in the details of Sam’s BCI project, so that, aspirationally, practice feels more continuous with authentic doing.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-3.png" alt=""><p data-timestamp="1750"> These synthesized prompts can vary each time they’re asked, so that Sam gets practice accessing the same idea from different angles. The prompts get deeper and more complex over time, as Sam gets more confident with the material. Notice also that this question isn’t so abstract: it’s really about applying what Sam’s learned, in a bite-sized form factor.</p>
<img src="https://andymatuschak.org/hmwl/img/5-widget-discuss.png" alt=""><p data-timestamp="1777"> The widget can also include open-ended discussion questions. Here Sam gets elaborative feedback—an extra detail to consider in their answer.</p>
<p data-timestamp="1804">When questions are synthesized like this, it’s important that Sam can steer them with feedback. Future questions will be synthesized accordingly.</p>
<img src="https://andymatuschak.org/hmwl/img/5-desktop-task.png" alt=""><p data-timestamp="1813"> So far, we’ve been looking at bite-sized questions Sam can answer while they’re out and about, but if they make time for a longer dedicated session, we can suggest meatier tasks, like this one. What’s more, we can move that work out of fake-practice-land and into Sam’s real context—the Jupyter notebook. Notice that the task is still framed in terms of Sam’s specific aims, rather than some generic signal processing problem.</p>

<p data-timestamp="1849">Now, Sam got into this project not as a “learning exercise”, but <em>as a way to start legitimately participating</em>. To start working with BCIs while playing to existing strengths.</p>
<img src="https://andymatuschak.org/hmwl/img/6-suggestion.png" alt=""><p data-timestamp="1861"> Just as our AI can help Sam find a tractable way into this space, it can also facilitate connections to communities of practice—here suggesting a local neurotech meetup. So Sam goes to the neurotech event, meets a local scientist, and sets up a coffee date. With permission, Sam records the meeting, knowing the notes will probably be helpful later.</p>
<img src="https://andymatuschak.org/hmwl/img/6-task.png" alt=""><p data-timestamp="1881"> And of course, Sam ends up surprised and intrigued quite a lot during this conversation. Our AI can notice these moments and help Sam metabolize them. Here that insight turns into a reflective practice prompt.</p>
<h2 id="design-principles">Design principles <a href="#design-principles">#</a></h2>
<p data-timestamp="1900">Four big design principles are threaded through Sam’s story. I’d like to review them now, and for each, point out the ways I think AI can help.</p>
<p data-timestamp="1911">First, we <em>bring guided learning to authentic contexts</em>, rather than thinking of it as a separate activity. We’re able to make that happen by imagining an AI which can perceive and act across applications on Sam’s computer. And as the audio transcript at the end demonstrated, that can extend to activities outside the computer too. This AI can give appropriate guidance in part because—with permission and executing locally—it can learn from every piece of text that’s ever crossed Sam’s screen, every action they’ve taken on the computer. It can synthesize scaffolded dynamic media, so that Sam can learn by doing, but with guidance.</p>
<p data-timestamp="1957">Then, when explicit learning activities are necessary, we <em>suffuse them with authentic context</em>. The AI grounds all the reading and practice Sam’s doing in their actual aims. It helps Sam match the learning activities to their depth of interest. It draws on important moments that happen when Sam is doing, like insights from that coffee meeting at the end, or questions asked while implementing parts of the project, and brings those moments into study activities</p>
<p data-timestamp="1989">Besides connecting these two domains, we can also strengthen each of them. Our AI suggest tractable ways for Sam to “just dive in” to a new interest, and helped Sam build connections with a community of practice.</p>
<p data-timestamp="2007">Finally, when we’re spending time in explicit learning activities, let’s make sure that learning actually works. Our AI creates a dynamic vessel for ongoing reinforcement it varies over time so that the knowledge transfers well to real situations. And it doesn’t just maintain memory—but increases depth of understanding over time.</p>
<h2 id="chatbot-tutors">Two cheers for chatbot tutors <a href="#chatbot-tutors">#</a></h2>
<p data-timestamp="2035">Most discussion of AI and education at the moment revolves around the framing of chatbot tutors.</p>
<p data-timestamp="2040">I think this framing correctly identifies something really wonderful about language models: they’re great at answering long-tail questions… if the user can articulate the question clearly enough. And if the user’s trying to perform a routine task, chatbot tutors can often diagnose problems and find ways to get the user unstuck. That’s great.</p>
<p data-timestamp="2061">But when I look at others’ visions of chatbot tutors through the much broader framing we’ve been discussing—they’re clearly missing a lot of what I want. I think these visions also often fail to take seriously just how much a real tutor can do. In large part, I think that’s because the authors of these visions are usually thinking about <em>educating</em> (something they want to do to others) rather than <em>learning</em> (something they want for themselves).</p>
<p data-timestamp="2087">Now, a sad truth about the world is that postdocs and graduate students are incredibly underpaid, so it’s surprisingly affordable to get an expert tutor for a technical topic I care about.</p>
<p data-timestamp="2096">But if I hire a real tutor, as an adult, to learn about signal processing, I’ll tell them about my interest in brain-computer interfaces, and I’ll expect them to ground every conversation in that purpose. My goal isn’t to “learn signal processing”, it’s to “participate in the creation of BCIs”. Chatbot tutors aren’t interested in what I’m trying to do; there’s a set of things they think I should know or should be able to do, and they view me as defective until I say the right things.</p>
<p data-timestamp="2128">If I hire a real tutor, I might ask them to sit beside me as I try to actually do something involving the material. They can see everything I’m doing, see what I’m pointing at. If it’s appropriate, I can scoot over, and they can drive for a minute. By comparison, the typical conception of a chatbot tutor lives in a windowless box, can only see whatever’s provided on scraps of paper passed under the door, and can have no effect on the outside world. My goal is to dive in, to immerse myself, to start doing the thing. But these chatbot tutors can’t join me where the real action is. So interactions with them create distance, pull me away from immersion.</p>
<p data-timestamp="2169">If I hire a real tutor, we’ll build a relationship. With every session, they’ll learn more about me—my interests, my strengths, my confusions. Chatbot tutors, as typically conceived, are transactional, amnesic. Now, we could fix that as context windows get longer. But that relationship is also important to my emotional engagement. If I view conversation with my tutor as a kind of peripheral participation in the community I’m hoping to enter—an interaction between novice in the discipline and mentor in the discipline—then tutoring will become just part of doing the thing. But if my interaction with my tutor is transactional, that will tend to make my tutoring sessions feel like “learning time”, separate from doing the thing.</p>
<p data-timestamp="2217">Finally, people talk about how Aristotle was a tutor for Alexander the Great. But what’s most valuable about having Aristotle as a tutor isn’t “diagnosing misconceptions”, but rather that he’s modeling the practices and values of an earnest, intellectually engaged adult. He’s demonstrating how and why he thinks about problems. His taste in the discipline. The high-growth periods we love transform the way we see the world. They reshape our identity.</p>
<p data-timestamp="2247">In my demo earlier, I showed a chatbot, but it didn’t really work like most “chatbot tutors” I see described. It focused all its actions on the user’s interest, rather than bringing its own agenda. It wasn’t trapped in a little text box—it could see and take action in the context of authentic use; it could communicate through dynamic media. It had a deep memory, drawing on everything you’ve ever written or seen.</p>
<p data-timestamp="2271">So in some ways, the system I’ve shown is more like a real tutor. But in my ideal world, I don’t want a tutor; I want to legitimately participate in some new discipline, and to learn what I need as much as possible from interaction with real practitioners.</p>
<p data-timestamp="2288">I view the role of the augmented learning system as helping me act on my creative interests, ideally by letting me just dive in and start doing, as much as possible. That will often mean scaffolding connections to and interactions with communities of practice.</p>
<h2 id="ethics">A note on ethics <a href="#ethics">#</a></h2>
<p data-timestamp="2305">One theme for this Design@Large series is the ethics of AI and its likely enormous social impacts. Let me say: <a href="https://andymatuschak.org/personal-ai-ethics/">I’m tremendously worried about those impacts</a>, in the general case. I’m worried about despots locking in their power, about lowering the bar to bioweapons, about economic chaos. I wouldn’t feel comfortable ethically with researching more powerful frontier models.</p>
<p data-timestamp="2327">But within the narrower domain of learning, my main moral concern is that we’ll end up trapped on a sad, narrow path. A condescending, authoritarian frame dominates the narrative in the future of learning. I’ll caricature it to make the point: with AI, we can take all these defective kids that don’t know the stuff they’re supposed to know, and_ finally get them to know it_! You know: personalized learning! The AI will let us precisely identify where the kids are wrong, or ignorant, and <em>fix them</em>. Then we can fill their heads to the brim with what’s good for them.</p>
<p data-timestamp="2368">The famous “bicycle for the mind” metaphor is better because it has no agenda other than the one you bring. It just lets you reach a wider range of destinations than you could on foot. And it makes the journey fun too, particularly if you’re biking along with some friends. The bicycle asks: where do you want to go?</p>
<p data-timestamp="2385">Of course, that question assumes your destination is well-known and clearly charted on some map. But those most rewarding high-growth experiences are often centered on a creative project. You’re trying to get somewhere no one’s ever gone before—to reach the frontier, then start charting links into the unknown. Learning in service of creation. It’s a dynamic, context-laden kind of learning. It’s about more than just efficiency and correctness. More than just faster gears on a bike. That’s the kind of learning I feel an almost moral imperative to help create.</p>
        
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shipbreaking (303 pts)]]></title>
            <link>https://www.edwardburtynsky.com/projects/photographs/shipbreaking</link>
            <guid>40424304</guid>
            <pubDate>Tue, 21 May 2024 04:56:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.edwardburtynsky.com/projects/photographs/shipbreaking">https://www.edwardburtynsky.com/projects/photographs/shipbreaking</a>, See on <a href="https://news.ycombinator.com/item?id=40424304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-type="page" data-updated-on="1502822602071" id="canvasWrapper" role="main" data-content-field="main-content"><div data-block-type="2" id="block-23bb69a3997e8cab3a7c">
<h2>SHIPBREAKING</h2><p><strong><em>Artist's Statement</em></strong></p><p>“The original idea for the shipbreaking started a long time ago. About four years after the Exxon Valdez oil spill I heard a radio program where they were talking about the danger of single-hulled ships. The insurance companies were refusing to cover them after 2004, which would force all these ships to be decommissioned. Only double-hulled ships would be allowed on the open sea to prevent that kind of catastrophe from happening again.</p><p>What went off in my mind was, wouldn’t it be interesting to see where these massive vessels will be taken apart. It would be a study of humanity and the skill it takes to dismantle these things. I looked upon the shipbreaking as the ultimate in recycling, in this case of the largest vessels ever made. It turned out that most of the dismantling was happening in India and Bangladesh so that's where I went.” — Edward Burtynsky</p>
</div><div data-block-type="2" id="block-yui_3_17_2_68_1502815486300_8023">
<p>Burtynsky's <em>Shipbreaking</em>&nbsp;photographs, like all his works, appear to us as images of the end of time. The abandoned mines and quarries, the piles of discarded tires, the endless fields of oil derricks, and the huge monoliths of retired tankers show how our attempts at industrial "progress" often leave a residue of destruction. Nevertheless there is something uncannily beautiful and breathtaking in the very expansiveness of these images―it is as if the vastness of their perspective somehow opens onto the longer view of things. For Burtynsky, nature itself, over time, can reclaim even the most ambitious of human incursions into the land. As long as human needs and desires change, so too will the landscape.</p>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modos Paper Monitor Pre-Launch on Crowd Supply (147 pts)]]></title>
            <link>https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply</link>
            <guid>40423746</guid>
            <pubDate>Tue, 21 May 2024 03:23:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply">https://www.modos.tech/blog/modos-paper-monitor-pre-launch-on-crowd-supply</a>, See on <a href="https://news.ycombinator.com/item?id=40423746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-w-id="19c00176-298a-f149-969d-4f67928c4aa4"><p>Today, I'm excited to announce the <a href="https://www.crowdsupply.com/modos-tech/modos-paper-monitor">pre-launch page of the Modos Paper Monitor on Crowd Supply</a>. The <strong>Modos Paper Monitor</strong> is an open-hardware 13.3-inch, 1600 x 1200 monochrome or color e-ink monitor with a fast 60 Hz refresh rate, low latency, multiple image modes and dithering options, and flexible screen update control. It can be connected using HDMI and USB-C and works on Linux, macOS, and Windows.&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/61a133075b8ce58a48f1bed1/664c0987c07254343e6bf379_modos-paper-monitor-mono-prototype-01_jpg_gallery-lg.jpg" loading="lazy" alt=""></p><figcaption>Modos Paper Monitor</figcaption></figure><p>Aimed at developers and makers, the <strong>Modos Development Kit</strong> uses the same display controller that powers our Paper Monitor with your choice of e-ink screen. It is available with 6-inch and 13.3-inch monochrome or color panels.</p><figure><p><img src="https://assets-global.website-files.com/61a133075b8ce58a48f1bed1/664c09a85140827f96dce3f9_modos-paper-monitor-mainboard-13in-screen-01_jpg_gallery-lg.jpg" loading="lazy" alt=""></p><figcaption>Modos Development Kit</figcaption></figure><p>‍</p><p><iframe width="100%" height="515" src="https://www.youtube.com/embed/pXn-bAwzNv4?si=u3xlX3lbrojRJbEy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p><p>‍</p><h3>The Beginning of the Journey</h3><p>This product is the result of a journey that began in February 2021. Like many others, I&nbsp;found myself spending&nbsp;over 12 hours a day in front of a computer or digital screen. My days&nbsp;were filled&nbsp;with&nbsp;supporting students, attending meetings, reading documentation, learning, and programming. By the end of the day, my eyes felt strained, and I was tired and distracted.</p><p>I realized that the digital tools we depend on for our day-to-day activities could be improved. They should be more beneficial to our health, less demanding of our attention, and better suited to our needs. I wanted to create a space free from distractions and eyestrain where I could focus on reading, writing, and thinking—a productive environment conducive to my health. This realization led me to reimagine personal computing and build calm, intentional technology.</p><h3>Shared Challenges</h3><p>As I shared my vision with others, I found that many people faced similar challenges: writers, authors, journalists, programmers, students, teachers, doctors, lawyers, engineers, makers, digital minimalists, people with visual impairments, those with health issues and many more.</p><p>We all struggled with the same problems—headaches, eyestrain, and distractions caused by our reliance on digital screens. This shared experience fueled my desire to create technology that respects our time, attention, and well-being.</p><h3>Open-Hardware and Open-Source</h3><p>As an open-hardware and open-source company, we strive to create an environment where experimentation, learning, and creativity can flourish. For those who wish to delve deeper, <a href="https://github.com/Modos-Labs/Glider">our README on GitHub</a> provides a comprehensive primer on e-ink technology. </p><p>We believe that open-source and open-hardware are crucial to envisioning and building a different reality—one where technology supports our well-being and fosters a healthier relationship with our digital lives.</p><p>We invite you to <a href="https://www.crowdsupply.com/modos-tech/modos-paper-monitor">sign up on Crowd Supply</a> to stay informed about the latest news and updates on our project.<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chameleon: Meta's New Multi-Modal LLM (277 pts)]]></title>
            <link>https://arxiv.org/abs/2405.09818</link>
            <guid>40423082</guid>
            <pubDate>Tue, 21 May 2024 01:37:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.09818">https://arxiv.org/abs/2405.09818</a>, See on <a href="https://news.ycombinator.com/item?id=40423082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.09818">View PDF</a>
    <a href="https://arxiv.org/html/2405.09818v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Armen Aghajanyan [<a href="https://arxiv.org/show-email/4972e8c8/2405.09818">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 16 May 2024 05:23:41 UTC (26,721 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stripe increasing "instant payout" fees by 50% (147 pts)]]></title>
            <link>https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states</link>
            <guid>40423035</guid>
            <pubDate>Tue, 21 May 2024 01:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states">https://support.stripe.com/questions/june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states</a>, See on <a href="https://news.ycombinator.com/item?id=40423035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-prop-key="answer" data-react-class="QuestionPage" data-react-props="{&quot;showL10nStaleNotice&quot;:false,&quot;showL10nPendingNotice&quot;:false,&quot;voted&quot;:false,&quot;slug&quot;:&quot;june-2024-pricing-update-for-instant-payouts-for-businesses-in-the-united-states&quot;,&quot;tags&quot;:[],&quot;relatedArticles&quot;:[{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/june-2024-pricing-update-for-european-bank-based-payment-methods-for-uk-businesses&quot;,&quot;title&quot;:&quot;June 2024 pricing update for European bank-based payment methods for UK businesses&quot;,&quot;description&quot;:&quot;Starting June 1, 2024, UK businesses will be charged a 1.5% international transaction fee when customers pay with a European bank-based payment method…&quot;,&quot;id&quot;:&quot;47a93692-07c1-4314-bac6-87b9c6e2dccf&quot;,&quot;tags&quot;:[]},{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/june-2024-pricing-updates-for-bacs-direct-debit&quot;,&quot;title&quot;:&quot;June 2024 pricing updates for Bacs Direct Debit&quot;,&quot;description&quot;:&quot;Starting June 1, 2024, Stripe is making pricing changes for Bacs Direct Debit transactions. What’s changing? Fee cap: When your customers pay with…&quot;,&quot;id&quot;:&quot;5a92bc1c-9ea8-4321-b0d2-8e3fa0742c5b&quot;,&quot;tags&quot;:[]},{&quot;type&quot;:&quot;question&quot;,&quot;url&quot;:&quot;/questions/april-2024-pricing-update-for-businesses-on-standard-pricing-in-australia&quot;,&quot;title&quot;:&quot;April 2024 pricing update for businesses on standard pricing in Australia&quot;,&quot;description&quot;:&quot;Starting April 1, 2024, we will decrease our domestic card processing fees for Australian businesses on standard pricing from 1.75% + A$0.30 to 1.70…&quot;,&quot;id&quot;:&quot;93a72c50-1b86-4903-8fb6-cc1197a74095&quot;,&quot;tags&quot;:[]}]}" data-react-props-from-content="" data-locale="en-US" data-article-id="aee91921-d4e8-495e-8ff3-d91433a78e34">
    

<p>With Instant Payouts, Stripe lets you access your funds and pay them out to your bank account or debit card immediately. Starting June 1, 2024, the fee for <a href="https://stripe.com/docs/payouts/instant-payouts" target="_blank" rel="noopener noreferrer"><u>Instant Payouts</u></a> in the US will increase from 1% to 1.5% per payout. You can always pay out your funds <a href="https://dashboard.stripe.com/account/payouts" target="_blank" rel="noopener noreferrer"><u>using our standard schedule</u></a> (2 business days) for free. This applies to all direct Stripe accounts, as well as <a href="https://stripe.com/docs/connect/explore-connect-guide#account-types" target="_blank" rel="noopener noreferrer"><u>standard connected accounts</u></a>. </p>

<h2>FAQs</h2>
<h3>How can I calculate the impact of these fee changes to my business?</h3>
<p>The best way to calculate the impact these changes might have to your business is to use the <a href="https://dashboard.stripe.com/" target="_blank" rel="noopener noreferrer"><u>Stripe Dashboard.</u></a></p>
<p>To understand the impact of changes:</p>
<ol start="1">
<li>Go to the <a href="https://dashboard.stripe.com/balance/overview" target="_blank" rel="noopener noreferrer"><u>Balances section</u></a> of your Dashboard</li>
<li>Navigate to the <a href="https://dashboard.stripe.com/payouts" target="_blank" rel="noopener noreferrer"><u>Payouts</u></a> section of your Balances page</li>
<li>Filter by “method” and ensure that “Instant” is selected and press the apply button</li>
<li>You can then export all Instant Payouts to calculate impact</li>
</ol>
<h3>What can help me offset these costs?</h3>
<p>You can always pay out your funds using our standard schedule (2 business days) for free.</p>

<h3>I’m a Connect platform. How does this affect my connected accounts?</h3>
<p>This applies to all direct Stripe accounts, as well as <a href="https://stripe.com/docs/connect/explore-connect-guide#account-types" target="_blank" rel="noopener noreferrer"><u>Standard connected accounts</u></a>. Stripe will automatically notify your Standard connected accounts about the change. They will also be able to see the new pricing when they opt for an Instant Payout. </p>
<p>If you use Connect with Custom accounts, there are no changes to your fees.</p>

<h3>How can I understand which of my connected accounts are impacted?</h3>
<p>To understand which of your connected accounts are impacted, please reach out to the <a href="https://support.stripe.com/" rel="noopener noreferrer"><u>Stripe team</u></a>. &nbsp;</p>

<h3>Why did I get this notice?</h3>
<p>This support page provides you with more details on the notice we sent you describing changes to certain fees. That notice is a legal notice sent to Stripe users, even those who have unsubscribed from optional marketing notices. You cannot unsubscribe from legal notices, but if you’d prefer not to receive any further legal notices from Stripe, you can close your account by following <a href="https://support.stripe.com/questions/close-a-stripe-account#:~:text=Close%20a%20Stripe%20account%20%3A%20Stripe%3A%20Help%20%26%20Support&amp;text=The%20owner%20of%20the%20account,Close%20account...%27" rel="noopener noreferrer"><u>these steps</u></a>. </p>
<p>Your continued use of Stripe's services after June 1, 2024 is subject to these fee changes. Any termination rights you have under your agreement with us are unaffected by this change.</p>

<h3>Do I need to take action? </h3>
<p>There’s no action necessary for you at this time and these changes will automatically apply to your business and / or your connected accounts on June 1, 2024. </p>

<h3>Contact support</h3>
<p>If you have questions beyond what's detailed above, please reach out to the <a href="https://support.stripe.com/" rel="noopener noreferrer"><u>Stripe team</u></a>. </p>




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Regular expression matching can be simple and fast (2007) (117 pts)]]></title>
            <link>https://swtch.com/%7Ersc/regexp/regexp1.html</link>
            <guid>40422997</guid>
            <pubDate>Tue, 21 May 2024 01:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swtch.com/%7Ersc/regexp/regexp1.html">https://swtch.com/%7Ersc/regexp/regexp1.html</a>, See on <a href="https://news.ycombinator.com/item?id=40422997">Hacker News</a></p>
<div id="readability-page-1" class="page">

<h2>
Regular Expression Matching Can Be Simple And Fast
<br>
(but is slow in Java, Perl, PHP, Python, Ruby, ...)
</h2>
<h2>
<a href="https://swtch.com/~rsc/">Russ Cox</a>
<br>
<i>rsc@swtch.com</i>
<br>
January 2007
</h2>


<h2>Introduction</h2>

<p>
This is a tale of two approaches to regular expression matching.
One of them is in widespread use in the
standard interpreters for many languages, including Perl.
The other is used only in a few places, notably most implementations
of awk and grep.
The two approaches have wildly different
performance characteristics:
</p>



<p>
Let's use superscripts to denote string repetition,
so that 
<code>a?<sup>3</sup>a<sup>3</sup></code>
is shorthand for
<code>a?a?a?aaa</code>.
The two graphs plot the time required by each approach
to match the regular expression 
<code>a?</code><sup><i>n</i></sup><code>a</code><sup><i>n</i></sup>
against the string <code>a</code><sup><i>n</i></sup>.
</p>

<p>
Notice that Perl requires over sixty seconds to match
a 29-character string.
The other approach, labeled Thompson NFA for
reasons that will be explained later,
requires twenty <i>microseconds</i> to match the string.
That's not a typo.  The Perl graph plots time in seconds,
while the Thompson NFA graph plots time in microseconds:
the Thompson NFA implementation
is a million times faster than Perl
when running on a miniscule 29-character string.
The trends shown in the graph continue: the
Thompson NFA handles a 100-character string in under 200 microseconds,
while Perl would require over 10<sup>15</sup> years.
(Perl is only the most conspicuous example of a large
number of popular programs that use the same algorithm;
the above graph could have been Python, or PHP, or Ruby,
or many other languages.  A more detailed
graph later in this article presents data for other implementations.)
</p>

<p>
It may be hard to believe the graphs: perhaps you've used Perl,
and it never seemed like regular expression matching was
particularly slow.
Most of the time, in fact, regular expression matching in Perl
is fast enough.  
As the graph shows, though, it is possible
to write so-called “pathological” regular expressions that
Perl matches very <i>very</i> slowly.
In contrast, there are no regular expressions that are 
pathological for the Thompson NFA implementation.
Seeing the two graphs side by side prompts the question, 
“why doesn't Perl use the Thompson NFA approach?”
It can, it should, and that's what the rest of this article is about.
</p>

<p>
Historically, regular expressions are one of computer science's
shining examples of how using good theory leads to good programs.
They were originally developed by theorists as a
simple computational model,
but Ken Thompson introduced them to
programmers in his implementation of the text editor QED
for CTSS.
Dennis Ritchie followed suit in his own implementation
of QED, for GE-TSS.
Thompson and Ritchie would go on to create Unix,
and they brought regular expressions with them.
By the late 1970s, regular expressions were a key
feature of the Unix landscape, in tools such as
ed, sed, grep, egrep, awk, and lex.
</p>

<p>
Today, regular expressions have also become a shining
example of how ignoring good theory leads to bad programs.
The regular expression implementations used by
today's popular tools are significantly slower
than the ones used in many of those thirty-year-old Unix tools.
</p>

<p>
This article reviews the good theory: 
regular expressions, finite automata, 
and a regular expression search algorithm
invented by Ken Thompson in the mid-1960s.
It also puts the theory into practice, describing 
a simple implementation of Thompson's algorithm.
That implementation, less than 400 lines of C,
is the one that went head to head with Perl above.
It outperforms the more complex real-world
implementations used by
Perl, Python, PCRE, and others.
The article concludes with a discussion of how 
theory might yet be converted into practice
in the real-world implementations.
</p>

<h2>
Regular Expressions
</h2>


<p>
Regular expressions are a notation for
describing sets of character strings.
When a particular string is in the set
described by a regular expression,
we often say that the regular expression
<i>matches</i>
the string.
</p>

<p>
The simplest regular expression is a single literal character.
Except for the special metacharacters 
<code>*+?()|</code>,
characters match themselves.
To match a metacharacter, escape it with
a backslash:
<code>\+</code>
matches a literal plus character.
</p>

<p>
Two regular expressions can be alternated or concatenated to form a new
regular expression:
if <i>e</i><sub>1</sub> matches
<i>s</i>
and <i>e</i><sub>2</sub> matches
<i>t</i>,
then <i>e</i><sub>1</sub><code>|</code><i>e</i><sub>2</sub> matches
<i>s</i>
or
<i>t</i>,
and
<i>e</i><sub>1</sub><i>e</i><sub>2</sub>
matches 
<i>st</i>.
</p>

<p>
The metacharacters
<code>*</code>,
<code>+</code>,
and
<code>?</code>
are repetition operators:
<i>e</i><sub>1</sub><code>*</code>
matches a sequence of zero or more (possibly different)
strings, each of which match <i>e</i><sub>1</sub>;
<i>e</i><sub>1</sub><code>+</code>
matches one or more; 
<i>e</i><sub>1</sub><code>?</code>
matches zero or one.
</p>

<p>
The operator precedence, from weakest to strongest binding, is
first alternation, then concatenation, and finally the
repetition operators.
Explicit parentheses can be used to force different meanings,
just as in arithmetic expressions.
Some examples:
<code>ab|cd</code>
is equivalent to
<code>(ab)|(cd)</code>;
<code>ab*</code>
is equivalent to
<code>a(b*)</code>.
</p>

<p>
The syntax described so far is a subset of the traditional Unix
egrep
regular expression syntax.
This subset suffices to describe all regular
languages: loosely speaking, a regular language is a set
of strings that can be matched in a single pass through
the text using only a fixed amount of memory.
Newer regular expression facilities (notably Perl and
those that have copied it) have added 
<a href="http://www.perl.com/doc/manual/html/pod/perlre.html">many new operators
and escape sequences</a>.  These additions make the regular
expressions more concise, and sometimes more cryptic, but usually
not more powerful:
these fancy new regular expressions almost always have longer
equivalents using the traditional syntax.
</p>

<p>
One common regular expression extension that 
does provide additional power is called
<i>backreferences</i>.
A backreference like
<code>\1</code>
or
<code>\2</code>
matches the string matched
by a previous parenthesized expression, and only that string:
<code>(cat|dog)\1</code>
matches
<code>catcat</code>
and
<code>dogdog</code>
but not
<code>catdog</code>
nor
<code>dogcat</code>.
As far as the theoretical term is concerned,
regular expressions with backreferences
are not regular expressions.
The power that backreferences add comes at great cost:
in the worst case, the best known implementations require
exponential search algorithms,
like the one Perl uses.
Perl (and the other languages)
could not now remove backreference support,
of course, but they could employ much faster algorithms
when presented with regular expressions that don't have
backreferences, like the ones considered above.
This article is about those faster algorithms.
</p>

<h2>
Finite Automata
</h2>



<p>
Another way to describe sets of character strings is with
finite automata.
Finite automata are also known as state machines,
and we will use “automaton” and “machine” interchangeably.
</p>

<p>
As a simple example, here is a machine recognizing
the set of strings matched by the regular expression
<code>a(bb)+a</code>:
</p>

<p><img src="https://swtch.com/%7Ersc/regexp/fig0.png" alt="DFA for a(bb)+a" width="278" height="54"></p>

<p>
A finite automaton is always in one of its states,
represented in the diagram by circles.
(The numbers inside the circles are labels to make this
discussion easier; they are not part of the machine's operation.)
As it reads the string, it switches from state to state.
This machine has two special states: the start state <i>s</i><sub>0</sub>
and the matching state <i>s</i><sub>4</sub>.
Start states are depicted with lone arrowheads pointing at them,
and matching states are drawn as a double circle.
</p>

<p>
The machine reads an input string one character at a time,
following arrows corresponding to the input to move from
state to state.
Suppose the input string is
<code>abbbba</code>.
When the machine reads the first letter of the string, the
<code>a</code>,
it is in the start state <i>s</i><sub>0</sub>.  It follows the
<code>a</code>
arrow to state <i>s</i><sub>1</sub>.
This process repeats as the machine reads the rest of the string:
<code>b</code>
to
<code><i>s</i><sub>2</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>3</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>2</sub></code>,
<code>b</code>
to
<code><i>s</i><sub>3</sub></code>,
and finally
<code>a</code>
to
<code><i>s</i><sub>4</sub></code>.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig1.png" alt="DFA execution on abbbba" width="357" height="426"></p>
<p>
The machine ends in <i>s</i><sub>4</sub>, a matching state, so it
matches the string.
If the machine ends in a non-matching state, it does not 
match the string.
If, at any point during the machine's execution, there is no
arrow for it to follow corresponding to the current
input character, the machine stops executing early.
</p>

<p>
The machine we have been considering is called a
<i>deterministic</i>
finite automaton (DFA),
because in any state, each possible input letter
leads to at most one new state.
We can also create machines
that must choose between multiple possible next states.
For example, this machine is equivalent to the previous
one but is not deterministic:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig2.png" alt="NFA for a(bb)+a" width="278" height="54"></p>
<p>
The machine is not deterministic because if it reads a
<code>b</code>
in state <i>s</i><sub>2</sub>, it has multiple choices for the next state:
it can go back to <i>s</i><sub>1</sub> in hopes of seeing another
<code>bb</code>,
or it can go on to <i>s</i><sub>3</sub> in hopes of seeing the final
<code>a</code>.
Since the machine cannot peek ahead to see the rest of
the string, it has no way to know which is the correct decision.
In this situation, it turns out to be interesting to
let the machine
<i>always guess correctly</i>.
Such machines are called non-deterministic finite automata
(NFAs or NDFAs).
An NFA matches an input string if there is some way 
it can read the string and follow arrows to a matching state.
</p>

<p>
Sometimes it is convenient to let NFAs have arrows with no
corresponding input character.  We will leave these arrows unlabeled.
An NFA can, at any time, choose to follow an unlabeled arrow
without reading any input.
This NFA is equivalent to the previous two, but the unlabeled arrow
makes the correspondence with
<code>a(bb)+a</code>
clearest:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig3.png" alt="Another NFA for a(bb)+a" width="278" height="39"></p>

<h2>
Converting Regular Expressions to NFAs
</h2>

<p>
Regular expressions and NFAs turn out to be exactly
equivalent in power: every regular expression has an
equivalent NFA (they match the same strings) and vice versa.
(It turns out that DFAs are also equivalent in power 
to NFAs and regular expressions; we will see this later.)
There are multiple ways to translate regular expressions into NFAs.
The method described here was first described by Thompson
in his 1968 CACM paper.
</p>

<p>
The NFA for a regular expression is built up from partial NFAs
for each subexpression, with a different construction for
each operator.  The partial NFAs have
no matching states: instead they have one or more dangling arrows,
pointing to nothing.  The construction process will finish by
connecting these arrows to a matching state.
</p>

<p>
The NFAs for matching single characters look like:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig4.png" alt="Single-character NFA" width="113" height="21"></p>
<p>
The NFA for the concatenation <i>e</i><sub>1</sub><i>e</i><sub>2</sub>
connects the final arrow of the <i>e</i><sub>1</sub> 
machine to the start of the <i>e</i><sub>2</sub> machine:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig5.png" alt="Concatenation NFA" width="242" height="20"></p>
<p>
The NFA for the alternation <i>e</i><sub>1</sub><code>|</code><i>e</i><sub>2</sub>
adds a new start state with a choice of either the
<i>e</i><sub>1</sub> machine or the <i>e</i><sub>2</sub> machine.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig6.png" alt="Alternation NFA" width="202" height="62"></p>
<p>
The NFA for <i>e</i><code>?</code> alternates the <i>e</i> machine with an empty path:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig7.png" alt="Zero or one NFA" width="184" height="56"></p>
<p>
The NFA for <i>e</i><code>*</code> uses the same alternation but loops a 
matching <i>e</i> machine back to the start:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig8.png" alt="Zero or more NFA" width="184" height="56"></p>
<p>
The NFA for <i>e</i><code>+</code> also creates a loop, but one that
requires passing through <i>e</i> at least once:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig9.png" alt="One or more NFA" width="190" height="41"></p>

<p>
Counting the new states in the diagrams above, we can see
that this technique creates exactly one state per character
or metacharacter in the regular expression,
excluding parentheses.
Therefore the number of states in the final NFA is at most
equal to the length of the original regular expression.
</p>

<p>
Just as with the example NFA discussed earlier, it is always possible
to remove the unlabeled arrows, and it is also always possible to generate
the NFA without the unlabeled arrows in the first place.
Having the unlabeled arrows makes the NFA easier for us to read
and understand, and they also make the C representation
simpler, so we will keep them.
</p>

<h2>
Regular Expression Search Algorithms
</h2>

<p>
Now we have a way to test whether a regular expression
matches a string: convert the regular expression to an NFA
and then run the NFA using the string as input.
Remember that NFAs are endowed with the ability to guess
perfectly when faced with a choice of next state:
to run the NFA using an ordinary computer, we must find
a way to simulate this guessing.
</p>

<p>
One way to simulate perfect guessing is to guess
one option, and if that doesn't work, try the other.
For example, consider the NFA for
<code>abab|abbb</code>
run on the string
<code>abbb</code>:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig10.png" alt="NFA for abab|abbb" width="364" height="79"></p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig11.png" alt="Backtracking execution on abbb" width="729" height="619"></p>
<p>
At step 0, the NFA must make a choice: try to match
<code>abab</code>
or
try to match
<code>abbb</code>?
In the diagram, the NFA tries
<code>abab</code>,
but that fails after step 3.
The NFA then tries the other choice, leading to step 4 and eventually a match.
This backtracking approach
has a simple recursive implementation
but can read the input string many times
before succeeding.
If the string does not match,
the machine must try
<i>all</i>
possible execution paths before
giving up.
The NFA tried only two different paths in the example,
but in the worst case, there can be exponentially
many possible execution paths, leading to very slow run times.
</p>

<p>
A more efficient but more complicated way to simulate perfect
guessing is to guess both options simultaneously. 
In this approach, the simulation allows the machine
to be in multiple states at once.  To process each letter,
it advances all the states along all the arrows that
match the letter.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig12.png" alt="Parallel execution on abbb" width="329" height="511"></p>
<p>
The machine starts in the start state and all the states
reachable from the start state by unlabeled arrows.
In steps 1 and 2, the NFA is in two states simultaneously.
Only at step 3 does the state set narrow down to a single state.
This multi-state approach tries both paths at the same time,
reading the input only once.
In the worst case, the NFA might be in
<i>every</i>
state at each step, but this results in at worst a constant amount
of work independent of the length of the string,
so arbitrarily
large input strings can be processed in linear time.
This is a dramatic improvement over the exponential time
required by the backtracking approach.
The efficiency comes from tracking the set of reachable
states but
<i>not</i>
which paths were used to reach them.
In an NFA with 
<i>n</i>
nodes, there can only be 
<i>n</i>
reachable states at any step, but there might be
2<sup><i>n</i></sup> paths through the NFA.
</p>

<h2>
Implementation
</h2>

<p>
Thompson introduced the multiple-state simulation approach
in his 1968 paper.
In his formulation, the states of the NFA were represented
by small machine-code sequences, and the list of possible states
was just a sequence of function call instructions.
In essence, Thompson compiled the regular expression into clever
machine code.
Forty years later, computers are much faster and the 
machine code approach is not as necessary.
The following sections
present an implementation written in portable ANSI C.
The full source code (under 400 lines)
and the benchmarking scripts are 
<a href="https://swtch.com/~rsc/regexp/">available online</a>.
(Readers who are unfamiliar or uncomfortable with C or pointers should
feel free to read the descriptions and skip over the actual code.)
</p>

<h2 id="compiling">
Implementation: Compiling to NFA
</h2>

<p>
The first step is to compile the regular expression
into an equivalent NFA.
In our C program, we will represent an NFA as a
linked collection of 
<code>State</code>
structures:
</p>
<pre>struct State
{
	int c;
	State *out;
	State *out1;
	int lastlist;
};
</pre><p>
Each
<code>State</code>
represents one of the following three NFA fragments,
depending on the value of
<code>c</code>.
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig13.png" alt="Possible per-State NFA fragments" width="340" height="109"></p>
<p>
(<code>Lastlist</code>
is used during execution and is explained in the next section.)
</p>

<p>
Following Thompson's paper,
the compiler builds an NFA from a regular expression in
<i>postfix</i>
notation with dot
(<code>.</code>) added
as an explicit concatenation operator.
A separate function
<code>re2post</code>
rewrites infix regular expressions like
“<code>a(bb)+a</code>”
into equivalent postfix expressions like
“<code>abb.+.a.</code>”.
(A “real” implementation would certainly
need to use dot as the “any character” metacharacter
rather than as a concatenation operator.
A real implementation would also probably build the 
NFA during parsing rather than build an explicit postfix expression.
However, the postfix version is convenient and follows 
Thompson's paper more closely.)
</p>

<p>
As the compiler scans the postfix expression, it maintains
a stack of computed NFA fragments.
Literals push new NFA fragments onto the stack, while
operators pop fragments off the stack and then
push a new fragment.
For example, 
after compiling the
<code>abb</code> in <code>abb.+.a.</code>,
the stack contains NFA fragments for
<code>a</code>,
<code>b</code>,
and
<code>b</code>.
The compilation of the
<code>.</code>
that follows pops the two
<code>b</code>
NFA fragment from the stack and pushes an NFA fragment for the
concatenation
<code>bb.</code>.
Each NFA fragment is defined by its start state and its
outgoing arrows:
</p><pre>struct Frag
{
	State *start;
	Ptrlist *out;
};
</pre><p>
<code>Start</code>
points at the start state for the fragment,
and
<code>out</code>
is a list of pointers to 
<code>State*</code>
pointers that are not yet connected to anything.
These are the dangling arrows in the NFA fragment.
</p>

<p>
Some helper functions manipulate pointer lists:
</p><pre>Ptrlist *list1(State **outp);
Ptrlist *append(Ptrlist *l1, Ptrlist *l2);

void patch(Ptrlist *l, State *s);
</pre><p>
<code>List1</code>
creates a new pointer list containing the single pointer
<code>outp</code>.
<code>Append</code>
concatenates two pointer lists, returning the result.
<code>Patch</code>
connects the dangling arrows in the pointer list
<code>l</code>
to the state
<code>s</code>:
it sets
<code>*outp</code>
<code>=</code>
<code>s</code>
for each pointer
<code>outp</code>
in
<code>l</code>.
</p>

<p>
Given these primitives and a fragment stack,
the compiler is a simple loop over the postfix expression.
At the end, there is a single fragment left:
patching in a matching state completes the NFA.
</p><pre>State*
post2nfa(char *postfix)
{
	char *p;
	Frag stack[1000], *stackp, e1, e2, e;
	State *s;

	#define push(s) *stackp++ = s
	#define pop()   *--stackp

	stackp = stack;
	for(p=postfix; *p; p++){
		switch(*p){
		/* <i>compilation cases, described below</i> */
		}
	}
	
	e = pop();
	patch(e.out, matchstate);
	return e.start;
}
</pre><p><a id="compile"></a>
The specific compilation cases mimic the translation 
steps described earlier.
</p>

<table>
<tbody><tr><td><p>
Literal characters:
</p><pre>default:
	s = state(*p, NULL, NULL);
	push(frag(s, list1(&amp;s-&gt;out));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig14.png" alt="" width="61" height="24">

</td></tr><tr><td><p>
Catenation:
</p><pre>case '.':
	e2 = pop();
	e1 = pop();
	patch(e1.out, e2.start);
	push(frag(e1.start, e2.out));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig15.png" alt="" width="182" height="20">

</td></tr><tr><td><p>
Alternation:
</p><pre>case '|':
	e2 = pop();
	e1 = pop();
	s = state(Split, e1.start, e2.start);
	push(frag(s, append(e1.out, e2.out)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig16.png" alt="" width="140" height="62">

</td></tr><tr><td><p>
Zero or one:
</p><pre>case '?':
	e = pop();
	s = state(Split, e.start, NULL);
	push(frag(s, append(e.out, list1(&amp;s-&gt;out1))));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig17.png" alt="" width="140" height="68">

</td></tr><tr><td><p>
Zero or more:
</p><pre>case '*':
	e = pop();
	s = state(Split, e.start, NULL);
	patch(e.out, s);
	push(frag(s, list1(&amp;s-&gt;out1)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig18.png" alt="" width="131" height="68">

</td></tr><tr><td><p>
One or more:
</p><pre>case '+':
	e = pop();
	s = state(Split, e.start, NULL);
	patch(e.out, s);
	push(frag(e.start, list1(&amp;s-&gt;out1)));
	break;
</pre>
</td><td><img src="https://swtch.com/%7Ersc/regexp/fig19.png" alt="" width="140" height="53">
</td></tr></tbody></table>

<h2>
Implementation: Simulating the NFA
</h2>

<p>
Now that the NFA has been built, we need to simulate it.
The simulation requires tracking 
<code>State</code>
sets, which are stored as a simple array list:
</p><pre>struct List
{
	State **s;
	int n;
};
</pre><p>
The simulation uses two lists:
<code>clist</code>
is the current set of states that the NFA is in,
and
<code>nlist</code>
is the next set of states that the NFA will be in,
after processing the current character.
The execution loop initializes
<code>clist</code>
to contain just the start state and then
runs the machine one step at a time.
</p><pre>int
match(State *start, char *s)
{
	List *clist, *nlist, *t;

	/* l1 and l2 are preallocated globals */
	clist = startlist(start, &amp;l1);
	nlist = &amp;l2;
	for(; *s; s++){
		step(clist, *s, nlist);
		t = clist; clist = nlist; nlist = t;	/* swap clist, nlist */
	}
	return ismatch(clist);
}
</pre><p>
To avoid allocating on every iteration of the loop,
<code>match</code>
uses two preallocated lists
<code>l1</code>
and
<code>l2</code>
as
<code>clist</code>
and
<code>nlist</code>,
swapping the two after each step.
</p>

<p>
If the final state list contains the matching state,
then the string matches.
</p><pre>int
ismatch(List *l)
{
	int i;

	for(i=0; i&lt;l-&gt;n; i++)
		if(l-&gt;s[i] == matchstate)
			return 1;
	return 0;
}
</pre>

<p>
<code>Addstate</code>
adds a state to the list,
but not if it is already on the list.
Scanning the entire list for each add would be inefficient;
instead the variable
<code>listid</code>
acts as a list generation number.
When
<code>addstate</code>
adds
<code>s</code>
to a list,
it records
<code>listid</code>
in
<code>s-&gt;lastlist</code>.
If the two are already equal,
then 
<code>s</code>
is already on the list being built.
<code>Addstate</code>
also follows unlabeled arrows:
if 
<code>s</code>
is a
<code>Split</code>
state with two unlabeled arrows to new states,
<code>addstate</code>
adds those states to the list instead of
<code>s</code>.
</p><pre>void
addstate(List *l, State *s)
{
	if(s == NULL || s-&gt;lastlist == listid)
		return;
	s-&gt;lastlist = listid;
	if(s-&gt;c == Split){
		/* follow unlabeled arrows */
		addstate(l, s-&gt;out);
		addstate(l, s-&gt;out1);
		return;
	}
	l-&gt;s[l-&gt;n++] = s;
}
</pre>

<p>
<code>Startlist</code>
creates an initial state list by adding just the start state:
</p><pre>List*
startlist(State *s, List *l)
{
	listid++;
	l-&gt;n = 0;
	addstate(l, s);
	return l;
}
</pre>

<p>
Finally,
<code>step</code>
advances the NFA past a single character, using
the current list
<code>clist</code>
to compute the next list
<code>nlist</code>.
</p><pre>void
step(List *clist, int c, List *nlist)
{
	int i;
	State *s;

	listid++;
	nlist-&gt;n = 0;
	for(i=0; i&lt;clist-&gt;n; i++){
		s = clist-&gt;s[i];
		if(s-&gt;c == c)
			addstate(nlist, s-&gt;out);
	}
}
</pre>

<h2>
Performance
</h2>

<p>
The C implementation just described was not written with performance in mind.
Even so, a slow implementation of a linear-time algorithm
can easily outperform a fast implementation of an 
exponential-time algorithm once the exponent is large enough.
Testing a variety of popular regular expression engines on 
a so-called pathological regular expression demonstrates this nicely.
</p>

<p>
Consider the regular expression
<code>a?<sup><i>n</i></sup>a<sup><i>n</i></sup></code>.
It matches the string
<code>a<sup><i>n</i></sup></code>
when the
<code>a?</code>
are chosen not to match any letters,
leaving the entire string to be matched by the
<code>a<sup><i>n</i></sup></code>.
Backtracking regular expression implementations
implement the zero-or-one
<code>?</code>
by first trying one and then zero.
There are
<i>n</i>
such choices to make, a total of
2<sup><i>n</i></sup> possibilities.
Only the very last
possibility—choosing zero for all the <code>?</code>—will lead to a match.
The backtracking approach thus requires
<i>O</i>(2<sup><i>n</i></sup>) time, so it will not scale much beyond <i>n</i>=25.
</p>

<p>
In contrast, Thompson's algorithm maintains state lists of length
approximately <i>n</i> and processes the string, also of length <i>n</i>,
for a total of <i>O</i>(<i>n</i><sup>2</sup>) time.
(The run time is superlinear,
because we are not keeping the regular expression constant
as the input grows.
For a regular expression of length <i>m</i> run on text of length <i>n</i>,
the Thompson NFA requires <i>O</i>(<i>mn</i>) time.)
</p>

<p>
The following graph plots time required to check whether
<code>a?<sup><i>n</i></sup>a<sup><i>n</i></sup></code>
matches
<code>a<sup><i>n</i></sup></code>:
</p>

<div>
<center>
<div>
<center>
<img src="https://swtch.com/%7Ersc/regexp/grep1p.png" alt="Performance graph" width="779" height="388">
<br>
regular expression and text size <i>n</i>
<br>
<code>a?</code><sup><i>n</i></sup><code>a</code><sup><i>n</i></sup>
matching 
<code>a</code><sup><i>n</i></sup>
</center>
</div>
</center>
</div>

<p>
Notice that the graph's <i>y</i>-axis has a logarithmic scale,
in order to be able to see a wide variety of times on a single graph.
</p>

<p>
From the graph it is clear that Perl, PCRE, Python, and Ruby are
all using recursive backtracking.
PCRE stops getting the right answer at 
<i>n</i>=23,
because it aborts the recursive backtracking after a maximum number
of steps.
As of Perl 5.6, Perl's regular expression engine is
<a href="http://perlmonks.org/index.pl?node_id=502408">said to memoize</a>
the recursive backtracking search, which should, at some memory cost,
keep the search from taking exponential amounts of time 
unless backreferences are being used.
As the performance graph shows, the memoization is not complete:
Perl's run time grows exponentially even though there
are no backreferences
in the expression.
Although not benchmarked here, Java uses a backtracking
implementation too.
In fact, the
<code>java.util.regex</code>
interface requires a backtracking
implementation, because arbitrary Java code
can be substituted into the matching path.
PHP uses the PCRE library.
</p>

<p>
The thick blue line is the C implementation of Thompson's algorithm given above.
Awk, Tcl, GNU grep, and GNU awk 
build DFAs, either precomputing them or using the on-the-fly
construction described in the next section.
</p>

<p>
Some might argue that this test is unfair to
the backtracking implementations, since it focuses on an
uncommon corner case.
This argument misses the point:
given a choice between an implementation
with a predictable, consistent, fast running time on all inputs
or one that usually runs quickly but can take
years of CPU time (or more) on some inputs,
the decision should be easy.
Also, while examples as dramatic as this one
rarely occur in practice, less dramatic ones do occur.
Examples include using
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
<code>(.*)</code>
to split five space-separated fields, or using
alternations where the common cases
are not listed first.
As a result, programmers often learn which constructs are
expensive and avoid them, or they turn to so-called
<a href="http://search.cpan.org/~dankogai/Regexp-Optimizer-0.15/lib/Regexp/Optimizer.pm">optimizers</a>.
Using Thompson's NFA simulation does not require such adaptation:
there are no expensive regular expressions.
</p>

<h2>
Caching the NFA to build a DFA
</h2>

<p>
Recall that DFAs are more efficient to execute than NFAs,
because DFAs are only ever in one state at a time: they never
have a choice of multiple next states.
Any NFA can be converted into an equivalent DFA
in which each DFA state corresponds to a
list of NFA states.
</p>

<p>
For example, here is the NFA we used earlier for
<code>abab|abbb</code>,
with state numbers added:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig20.png" alt="NFA for abab|abbb" width="424" height="91"></p>
<p>
The equivalent DFA would be:
</p>
<p><img src="https://swtch.com/%7Ersc/regexp/fig21.png" alt="DFA for abab|abbb" width="496" height="170"></p>
<p>
Each state in the DFA corresponds to a list of 
states from the NFA.
</p>

<p>
In a sense, Thompson's NFA simulation is
executing the equivalent DFA: each
<code>List</code>
corresponds to some DFA state,
and the 
<code>step</code>
function is computing, given a list and a next character,
the next DFA state to enter.
Thompson's algorithm simulates the DFA by 
reconstructing each DFA state as it is needed.
Rather than throw away this work after each step,
we could cache the
<code>Lists</code>
in spare memory, avoiding the cost of repeating the computation
in the future
and essentially computing the equivalent DFA as it is needed.
This section presents the implementation of such an approach.
Starting with the NFA implementation from the previous section,
we need to add less than 100 lines to build a DFA implementation.
</p>

<p>
To implement the cache, we first introduce a new data type
that represents a DFA state:
</p><pre>struct DState
{
	List l;
	DState *next[256];
	DState *left;
	DState *right;
};
</pre><p>
A
<code>DState</code>
is the cached copy of the list
<code>l</code>.
The array
<code>next</code>
contains pointers to the next state for each
possible input character:
if the current state is
<code>d</code>
and the next input character is
<code>c</code>,
then
<code>d-&gt;next[c]</code>
is the next state.
If
<code>d-&gt;next[c]</code>
is null, then the next state has not been computed yet.
<code>Nextstate</code>
computes, records, and returns the next state
for a given state and character.
</p>

<p>
The regular expression match follows
<code>d-&gt;next[c]</code>
repeatedly, calling
<code>nextstate</code>
to compute new states as needed.
</p><pre>int
match(DState *start, char *s)
{
	int c;
	DState *d, *next;
	
	d = start;
	for(; *s; s++){
		c = *s &amp; 0xFF;
		if((next = d-&gt;next[c]) == NULL)
			next = nextstate(d, c);
		d = next;
	}
	return ismatch(&amp;d-&gt;l);
}
</pre>

<p>
All the
<code>DStates</code>
that have been computed need to be saved in a 
structure that lets us look up a
<code>DState</code>
by its
<code>List</code>.
To do this, we arrange them 
in a binary tree
using the sorted
<code>List</code>
as the key.
The
<code>dstate</code>
function returns the
<code>DState</code>
for a given
<code>List</code>,
allocating one if necessary:
</p><pre>DState*
dstate(List *l)
{
	int i;
	DState **dp, *d;
	static DState *alldstates;

	qsort(l-&gt;s, l-&gt;n, sizeof l-&gt;s[0], ptrcmp);

	/* look in tree for existing DState */
	dp = &amp;alldstates;
	while((d = *dp) != NULL){
		i = listcmp(l, &amp;d-&gt;l);
		if(i &lt; 0)
			dp = &amp;d-&gt;left;
		else if(i &gt; 0)
			dp = &amp;d-&gt;right;
		else
			return d;
	}
	
	/* allocate, initialize new DState */
	d = malloc(sizeof *d + l-&gt;n*sizeof l-&gt;s[0]);
	memset(d, 0, sizeof *d);
	d-&gt;l.s = (State**)(d+1);
	memmove(d-&gt;l.s, l-&gt;s, l-&gt;n*sizeof l-&gt;s[0]);
	d-&gt;l.n = l-&gt;n;

	/* insert in tree */
	*dp = d;
	return d;
}
</pre><p>
Nextstate runs the NFA
<code>step</code>
and returns the corresponding
<code>DState</code>:
</p><pre>DState*
nextstate(DState *d, int c)
{
	step(&amp;d-&gt;l, c, &amp;l1);
	return d-&gt;next[c] = dstate(&amp;l1);
}
</pre><p>
Finally, the DFA's start state is the
<code>DState</code>
corresponding to the NFA's start list:
</p><pre>DState*
startdstate(State *start)
{
	return dstate(startlist(start, &amp;l1));
}
</pre><p>
(As in the NFA simulation,
<code>l1</code>
is a preallocated
<code>List</code>.)
</p>

<p>
The
<code>DStates</code>
correspond to DFA states, but the DFA is only built as needed:
if a DFA state has not been encountered during the search,
it does not yet exist in the cache.
An alternative would be to compute the entire DFA at once.
Doing so would make
<code>match</code>
a little faster by removing the conditional branch,
but at the cost of increased startup time and
memory use.
</p>

<p>
One might also worry about bounding the amount of
memory used by the on-the-fly DFA construction.
Since the
<code>DStates</code>
are only a cache of the 
<code>step</code>
function, the implementation of
<code>dstate</code>
could choose to throw away the entire DFA so far
if the cache grew too large.
This cache replacement policy 
only requires a few extra lines of code in 
<code>dstate</code>
and in
<code>nextstate</code>,
plus around 50 lines of code for memory management.
An implementation is
<a href="https://swtch.com/~rsc/regexp/">available online</a>.
(<a href="http://cm.bell-labs.com/cm/cs/awkbook/">Awk</a>
uses a similar limited-size cache strategy,
with a fixed limit of 32 cached states; this explains the discontinuity
in its performance at <i>n</i>=28 in the graph above.)
</p>

<p>
NFAs derived from regular expressions
tend to exhibit good locality: they visit the same states
and follow the same transition arrows over and over
when run on most texts.
This makes the caching worthwhile: the first time an arrow
is followed, the next state must be computed as in the NFA
simulation, but future traversals of the arrow are just
a single memory access.
Real DFA-based implementations can make use
of additional optimizations to run even faster.
A companion article (not yet written) will explore
DFA-based regular expression implementations in more detail.
</p>


<h2>
Real world regular expressions
</h2>

<p>
Regular expression usage in real programs
is somewhat more complicated than what the regular expression
implementations described above can handle.
This section briefly describes the common complications;
full treatment of any of these is beyond the scope of this
introductory article.
</p>

<p>
<i>Character classes</i>.
A character class, whether 
<code>[0-9]</code>
or
<code>\w</code>
or
<code>.</code> (dot),
is just a concise representation of an alternation.
Character classes can be expanded into alternations
during compilation, though it is more efficient to add
a new kind of NFA node to represent them explicitly.
<a href="http://www.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap09.html">POSIX</a>
defines special character classes
like <code>[[:upper:]]</code> that change meaning
depending on the current locale, but the hard part of
accommodating these is determining their meaning,
not encoding that meaning into an NFA.
</p>

<p>
<i>Escape sequences</i>.
Real regular expression syntaxes need to handle
escape sequences, both as a way to match metacharacters
(<code>\(</code>,
<code>\)</code>,
<code>\\</code>,
etc.)
and to specify otherwise difficult-to-type characters such as
<code>\n</code>.
</p>

<p>
<i>Counted repetition</i>.
Many regular expression implementations provide a counted
repetition operator
<code>{<i>n</i>}</code>
to match exactly 
<i>n</i>
strings matching a pattern;
<code>{</code><i>n</i><code>,</code><i>m</i><code>}</code>
to match at least 
<i>n</i>
but no more than
<i>m</i>;
and
<code>{</code><i>n</i><code>,}</code>
to match
<i>n</i>
or more.
A recursive backtracking implementation can implement
counted repetition using a loop; an NFA or DFA-based
implementation must expand the repetition:
<i>e</i><code>{3}</code>
expands to
<i>eee</i>;
<i>e</i><code>{3,5}</code>
expands to
<i>eeee</i><code>?</code><i>e</i><code>?</code>,
and
<i>e</i><code>{3,}</code>
expands to
<i>eee</i><code>+</code>.
</p>

<p>
<i>Submatch extraction</i>.
When regular expressions are used for splitting or parsing strings,
it is useful to be able to find out which sections of the input string
were matched by each subexpression.
After a regular expression like
<code>([0-9]+-[0-9]+-[0-9]+)</code>
<code>([0-9]+:[0-9]+)</code>
matches a string (say a date and time),
many regular expression engines make the
text matched by each parenthesized expression
available.
For example, one might write in Perl:
</p><pre>if(/([0-9]+-[0-9]+-[0-9]+) ([0-9]+:[0-9]+)/){
	print "date: $1, time: $2\n";
}
</pre><p>
The extraction of submatch boundaries has been mostly ignored
by computer science theorists, and it is perhaps the most
compelling argument for using recursive backtracking.
However, Thompson-style algorithms can be adapted to
track submatch boundaries without giving up efficient performance.
The Eighth Edition Unix
<i>regexp</i>(3)
library implemented such an algorithm as early as 1985,
though as explained below,
it was not very widely used or even noticed.
</p>

<p>
<i>Unanchored matches</i>.
This article has assumed that regular expressions
are matched against an entire input string.
In practice, one often wishes to find a substring
of the input that matches the regular expression.
Unix tools traditionally return the longest matching substring
that starts at the leftmost possible point in the input.
An unanchored search for 
<i>e</i>
is a special case
of submatch extraction: it is like searching for
<code>.*(<i>e</i>).*</code>
where the first
<code>.*</code>
is constrained to match as short a string as possible.
</p>

<p>
<i>Non-greedy operators</i>.
In traditional Unix regular expressions, the repetition operators
<code>?</code>,
<code>*</code>,
and
<code>+</code>
are defined to match as much of the string as possible while
still allowing the entire regular expression to match:
when matching
<code>(.+)(.+)</code>
against
<code>abcd</code>,
the first
<code>(.+)</code>
will match
<code>abc</code>,
and the second
will match
<code>d</code>.
These operators are now called
<i>greedy</i>.
Perl introduced
<code>??</code>,
<code>*?</code>,
and
<code>+?</code>
as non-greedy versions, which match as little of the string
as possible while preserving the overall match:
when matching
<code>(.+?)(.+?)</code>
against
<code>abcd</code>,
the first
<code>(.+?)</code>
will match only
<code>a</code>,
and the second
will match
<code>bcd.</code>
By definition, whether an operator is greedy
cannot affect whether a regular expression matches a
particular string as a whole; it only affects the
choice of submatch boundaries.
The backtracking algorithm admits a simple implementation
of non-greedy operators:
try the shorter match before the longer one.
For example, in a standard backtracking implementation,
<code><i>e</i>?</code>
first tries using
<i>e</i>
and then tries not using it;
<code><i>e</i>??</code>
uses the other order.
The submatch-tracking variants of Thompson's algorithm
can be adapted to accommodate non-greedy operators.
</p>

<p>
<i>Assertions</i>.
The traditional regular expression metacharacters
<code>^</code>
and
<code>$</code>
can be viewed as
<i>assertions</i>
about the text around them:
<code>^</code>
asserts that the previous character
is a newline (or the beginning of the string),
while
<code>$</code>
asserts that the next character is a newline
(or the end of the string).
Perl added more assertions, like
the word boundary
<code>\b</code>,
which asserts that 
the previous character is alphanumeric but the next
is not, or vice versa.
Perl also generalized the idea to arbitrary
conditions called lookahead assertions:
<code>(?=</code><i>re</i><code>)</code>
asserts that the text after the current input position matches
<i>re</i>,
but does not actually advance the input position;
<code>(?!</code><i>re</i><code>)</code>
is similar but 
asserts that the text does not match
<i>re</i>.
The lookbehind assertions
<code>(?&lt;=</code><i>re</i><code>)</code>
and
<code>(?&lt;!</code><i>re</i><code>)</code>
are similar but make assertions about the text
before the current input position.
Simple assertions like
<code>^</code>,
<code>$</code>,
and
<code>\b</code>
are easy to accommodate in an NFA,
delaying the match one byte for forward assertions.
The generalized assertions
are harder to accommodate but in principle could
be encoded in the NFA.
</p>

<p>
<i>Backreferences</i>.
As mentioned earlier, no one knows how to 
implement regular expressions with backreferences efficiently,
though no one can prove that it's impossible either.
(Specifically, the 
<a href="http://perl.plover.com/NPC/NPC-3SAT.html">problem is NP-complete</a>, meaning that if
someone did find an efficient implementation, that would
be <i>major</i> news to computer scientists and would
win a <a href="http://www.claymath.org/Popular_Lectures/Minesweeper/">million dollar prize</a>.)
The simplest, most effective strategy for backreferences,
taken by the original awk and egrep, is not to implement them.
This strategy is no longer practical: users have come to
rely on backreferences for at least occasional use,
and backreferences are part of
the
<a href="http://www.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap09.html">POSIX standard for regular expressions</a>.
Even so, it would be reasonable to use Thompson's NFA simulation
for most regular expressions, and only bring out
backtracking when it is needed.
A particularly clever implementation could combine the two,
resorting to backtracking only to accommodate the backreferences.
</p>

<p>
<i>Backtracking with memoization</i>.
Perl's approach of using memoization to avoid exponential blowup
during backtracking
when possible is a good one.  At least in theory, it should make
Perl's regular expressions behave more like an NFA and
less like backtracking.  
Memoization does not completely solve the problem, though:
the memoization itself requires a memory footprint roughly 
equal to the size of the text times the size of the regular expression.
Memoization also does not address the issue of the stack space used
by backtracking, which is linear in the size of the text:
matching long strings typically causes a backtracking
implementation to run out of stack space:
</p><pre>$ perl -e '("a" x 100000) =~ /^(ab?)*$/;'
Segmentation fault (core dumped)
$
</pre>

<p>
<i>Character sets</i>.
Modern regular expression implementations must deal with 
large non-ASCII character sets such as Unicode.
The 
<a href="https://swtch.com/plan9port/unix/">Plan 9 regular expression library</a>
incorporates Unicode by running an NFA with a
single Unicode character as the input character for each step.
That library separates the running of the NFA from decoding
the input, so that the same regular expression matching code
is used for both 
<a href="http://plan9.bell-labs.com/sys/doc/utf.html">UTF-8</a>
and wide-character inputs.
</p>

<h2 id="History">
History and References
</h2>


<p>
<a name="rabin-scott-b"></a>Michael Rabin and Dana Scott
introduced non-deterministic finite automata
and the concept of non-determinism in 1959
[<a href="#rabin-scott">7</a>],
showing that NFAs can be simulated by
(potentially much larger) DFAs in which 
each DFA state corresponds to a set of NFA states.
(They won the Turing Award in 1976 for the introduction
of the concept of non-determinism in that paper.)
</p>

<p>
<a name="mcnaughton-yamada-b">R. McNaughton and H. Yamada
</a>[<a href="#mcnaughton-yamada">4</a>]
and 
<a name="thompson-b"></a>Ken Thompson
[<a href="#thompson">9</a>]
are commonly credited with giving the first constructions
to convert regular expressions into NFAs,
even though neither paper mentions the
then-nascent concept of an NFA.
McNaughton and Yamada's construction
creates a DFA,
and Thompson's construction creates IBM 7094 machine code,
but reading between the lines one can
see latent NFA constructions underlying both.
Regular expression to NFA constructions differ only in how they encode 
the choices that the NFA must make.
The approach used above, mimicking Thompson,
encodes the choices with explicit choice
nodes
(the
<code>Split</code>
nodes above)
and unlabeled arrows.
An alternative approach,
the one most commonly credited to McNaughton and Yamada,
is to avoid unlabeled arrows, instead allowing NFA states to
have multiple outgoing arrows with the same label.
<a name="mcilroy-b"></a>McIlroy
[<a href="#mcilroy">3</a>]
gives a particularly elegant implementation of this approach
in Haskell.
</p>

<p>
<a name="vanvleck-b"></a>Thompson's regular expression implementation
was for his QED editor running on the CTSS 
[<a href="#vanvleck">10</a>]
operating
system on the IBM 7094.
<a name="pierce-b"></a>A copy of the editor can be found in archived CTSS sources
[<a href="#pierce">5</a>].
<a name="deutsch-lampson-b"></a>L. Peter Deutsch and Butler Lampson
[<a href="#deutsch-lampson">1</a>]
developed the first QED, but
Thompson's reimplementation was the first to use
regular expressions.
<a name="ritchie-b"></a>Dennis Ritchie, author of yet another QED implementation,
has documented the early history of the QED editor
[<a href="#ritchie">8</a>]
(Thompson, Ritchie, and Lampson later won
Turing awards for work unrelated to QED or finite automata.)
</p>

<p>
Thompson's paper marked the 
beginning of a long line of regular expression implementations.
Thompson chose not to use his algorithm when 
implementing the text editor ed, which appeared in 
First Edition Unix (1971), or in its descendant grep,
which first appeared in the Fourth Edition (1973).
Instead, these venerable Unix tools used
recursive backtracking!
Backtracking was justifiable because the
regular expression syntax was quite limited:
it omitted grouping parentheses and the
<code>|</code>,
<code>?</code>,
and
<code>+</code>
operators.
Al Aho's egrep,
which first appeared in the Seventh Edition (1979),
was the first Unix tool to provide
the full regular expression syntax, using a
precomputed DFA.
By the Eighth Edition (1985), egrep computed the DFA on the fly,
like the implementation given above.
</p>

<p>
<a name="pike-b"></a>While writing the text editor sam 
[<a href="#pike">6</a>]
in the early 1980s,
Rob Pike wrote a new regular expression implementation,
which Dave Presotto extracted into a library that 
appeared in the Eighth Edition.
Pike's implementation
incorporated submatch tracking into an efficient NFA simulation
but, like the rest of the Eighth Edition source, was not widely
distributed.
Pike himself did not realize that his technique was anything new.
Henry Spencer reimplemented the Eighth Edition library
interface from scratch, but using backtracking,
and
<a href="http://arglist.com/regex/">released his implementation</a>
into the public domain.
It became very widely used, eventually serving as the basis
for the slow regular expression implementations
mentioned earlier: Perl, PCRE, Python, and so on.
(In his defense,
Spencer knew the routines could be slow,
and he didn't know that a more efficient algorithm existed.
He even warned in the documentation,
“Many users have found the speed perfectly adequate,
although replacing the insides of egrep with this code
would be a mistake.”)
Pike's regular expression implementation, extended to
support Unicode, was made freely available
with sam in 
<a href="http://groups.google.com/group/comp.os.research/msg/f1783504a2d18051">late 1992</a>,
but the particularly efficient
regular expression search algorithm went unnoticed.
The code is now available in many forms: as 
<a href="http://plan9.bell-labs.com/sources/plan9/sys/src/cmd/sam/">part of sam</a>,
as 
<a href="http://plan9.bell-labs.com/sources/plan9/sys/src/libregexp/">Plan&nbsp;9's regular expression library</a>,
or
<a href="https://swtch.com/plan9port/unix/">packaged separately for Unix</a>.
<a name="laurikari-b"></a>Ville Laurikari independently discovered Pike's algorithm
in 1999, developing a theoretical foundation as well
[<a href="#laurikari">2</a>].
</p>


<p>
Finally, any discussion of regular expressions
would be incomplete without mentioning 
Jeffrey Friedl's book
<i>Mastering Regular Expressions</i>,
perhaps the most popular reference among today's programmers.
Friedl's book teaches programmers how best to use today's
regular expression implementations, but not how best to implement them.
What little text it devotes to implementation
issues perpetuates the widespread belief that recursive backtracking
is the only way to simulate an NFA.
Friedl makes it clear that he 
<a href="http://regex.info/blog/2006-09-15/248">neither understands nor respects</a>
the underlying theory.
</p>

<h2>
Summary
</h2>

<p>
Regular expression matching can be simple and fast, using
finite automata-based techniques that have been known for decades.
In contrast, Perl, PCRE, Python, Ruby, Java,
and many other languages
have regular expression implementations based on 
recursive backtracking that are simple but can be
excruciatingly slow.
With the exception of backreferences, the features
provided by the slow backtracking implementations
can be provided by the automata-based implementations
at dramatically faster, more consistent speeds.
</p>

<p>
The next article in this series,
“<a href="https://swtch.com/%7Ersc/regexp/regexp2.html">Regular Expression Matching: the Virtual Machine Approach</a>,” discusses NFA-based submatch extraction.
The third article, “<a href="https://swtch.com/%7Ersc/regexp/regexp3.html">Regular Expression Matching in the Wild</a>,” examines a production implementation.
The fourth article, “<a href="https://swtch.com/%7Ersc/regexp/regexp4.html">Regular Expression Matching with a Trigram Index</a>,” explains how Google Code Search was implemented.
</p>

<h2>
Acknowledgements
</h2>

<p>
Lee Feigenbaum,
James Grimmelmann,
Alex Healy,
William Josephson,
and
Arnold Robbins
read drafts of this article and made many helpful suggestions.
Rob Pike clarified some of the history surrounding his
regular expression implementation.
Thanks to all.
</p>

<h2>
References
</h2>

<p>
<a name="deutsch-lampson"></a>
[<a href="#deutsch-lampson-b">1</a>]
L. Peter Deutsch and Butler Lampson,
“An online editor,”
Communications of the ACM 10(12) (December 1967), pp.&nbsp;793–799.
<a href="http://doi.acm.org/10.1145/363848.363863"><i>http://doi.acm.org/10.1145/363848.363863</i></a>
</p><p>
<a name="laurikari"></a>
[<a href="#laurikari-b">2</a>]
Ville Laurikari,
“NFAs with Tagged Transitions,
their Conversion to Deterministic Automata
and
Application to Regular Expressions,”
in Proceedings of the Symposium on String Processing and
Information Retrieval, September 2000.
<a href="http://laurikari.net/ville/spire2000-tnfa.ps"><i>http://laurikari.net/ville/spire2000-tnfa.ps</i></a>
</p><p>
<a name="mcilroy"></a>
[<a href="#mcilroy-b">3</a>]
M. Douglas McIlroy,
“Enumerating the strings of regular languages,”
Journal of Functional Programming 14 (2004), pp.&nbsp;503–518.
<a href="http://www.cs.dartmouth.edu/~doug/nfa.ps.gz"><i>http://www.cs.dartmouth.edu/~doug/nfa.ps.gz</i></a> (preprint)
</p><p>
<a name="mcnaughton-yamada"></a>
[<a href="#mcnaughton-yamada-b">4</a>]
R. McNaughton and H. Yamada,
“Regular expressions and state graphs for automata,”
IRE Transactions on Electronic Computers EC-9(1) (March 1960), pp.&nbsp;39–47.
</p><p>
<a name="pierce"></a>
[<a href="#pierce-b">5</a>]
Paul Pierce,
“CTSS source listings.”
<a href="http://www.piercefuller.com/library/ctss.html"><i>http://www.piercefuller.com/library/ctss.html</i></a> 
(Thompson's QED is in the file
<code>com5</code>
in the source listings archive and is marked as
<code>0QED</code>)
</p><p>
<a name="pike"></a>
[<a href="#pike-b">6</a>]
Rob Pike,
“The text editor sam,”
Software—Practice &amp; Experience 17(11) (November 1987), pp.&nbsp;813–845.
<a href="http://plan9.bell-labs.com/sys/doc/sam/sam.html"><i>http://plan9.bell-labs.com/sys/doc/sam/sam.html</i></a>
</p><p>
<a name="rabin-scott"></a>
[<a href="#rabin-scott-b">7</a>]
Michael Rabin and Dana Scott,
“Finite automata and their decision problems,”
IBM Journal of Research and Development 3 (1959), pp.&nbsp;114–125.
<a href="http://www.research.ibm.com/journal/rd/032/ibmrd0302C.pdf"><i>http://www.research.ibm.com/journal/rd/032/ibmrd0302C.pdf</i></a>
</p><p>
<a name="ritchie"></a>
[<a href="#ritchie-b">8</a>]
Dennis Ritchie,
“An incomplete history of the QED text editor.”
<a href="http://plan9.bell-labs.com/~dmr/qed.html"><i>http://plan9.bell-labs.com/~dmr/qed.html</i></a>
</p><p>
<a name="thompson"></a>
[<a href="#thompson-b">9</a>]
Ken Thompson,
“Regular expression search algorithm,”
Communications of the ACM 11(6) (June 1968), pp.&nbsp;419–422.
<a href="http://doi.acm.org/10.1145/363347.363387"><i>http://doi.acm.org/10.1145/363347.363387</i></a>
(<span size="-1"><a href="http://www.cs.chalmers.se/~coquand/AUTOMATA/thompson.pdf">PDF</a></span>)
</p><p>
<a name="vanvleck"></a>
[<a href="#vanvleck-b">10</a>]
Tom Van Vleck,
“The IBM 7094 and CTSS.”
<a href="http://www.multicians.org/thvv/7094.html"><i>http://www.multicians.org/thvv/7094.html</i></a>
</p>


<p>
Discussion on <a href="http://programming.reddit.com/info/10c60/comments">reddit</a> and <a href="http://perlmonks.org/?node_id=597262">perlmonks</a> and
<a href="http://lambda-the-ultimate.org/node/2064">LtU</a>
</p>

<center>
<p>
Copyright © 2007 Russ Cox.  All Rights Reserved.
<br>
<a href="https://swtch.com/~rsc/regexp/">https://swtch.com/~rsc/regexp/</a>
</p>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bento3D (203 pts)]]></title>
            <link>https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63</link>
            <guid>40422940</guid>
            <pubDate>Tue, 21 May 2024 01:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63">https://polar-tadpole-97b.notion.site/Bento3D-e40483712b304d389d7c2da26196e113#d7f452d3ffd94cd1a0a9e70e361efb63</a>, See on <a href="https://news.ycombinator.com/item?id=40422940">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>