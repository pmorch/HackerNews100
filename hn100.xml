<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 22 May 2025 19:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Claude 4 (883 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-4</link>
            <guid>44063703</guid>
            <pubDate>Thu, 22 May 2025 16:34:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-4">https://www.anthropic.com/news/claude-4</a>, See on <a href="https://news.ycombinator.com/item?id=44063703">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we’re introducing the next generation of Claude models: <strong>Claude Opus 4</strong> and <strong>Claude Sonnet 4</strong>, setting new standards for coding, advanced reasoning, and AI agents. </p><p>Claude Opus 4 is the world’s best coding model, with sustained performance on complex, long-running tasks and agent workflows. Claude Sonnet 4 is a significant upgrade to Claude Sonnet 3.7, delivering superior coding and reasoning while responding more precisely to your instructions.</p><p>Alongside the models, we're also announcing:</p><ul><li><strong>Extended thinking with tool use (beta)</strong>: Both models can use tools—like <a href="https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool">web search</a>—during extended thinking, allowing Claude to alternate between reasoning and tool use to improve responses.</li><li><strong>New model capabilities</strong>: Both models can use tools in parallel, follow instructions more precisely, and—when given access to local files by developers—demonstrate significantly improved memory capabilities, extracting and saving key facts to maintain continuity and build tacit knowledge over time.</li><li><strong>Claude Code is now generally available</strong>: After receiving extensive positive feedback during our research preview, we’re expanding how developers can collaborate with Claude. Claude Code now supports background tasks via GitHub Actions and native integrations with VS Code and JetBrains, displaying edits directly in your files for seamless pair programming.</li><li><strong>New API capabilities:</strong> We’re releasing <a href="https://www.anthropic.com/news/agent-capabilities-api">four new capabilities</a> on the Anthropic API that enable developers to build more powerful AI agents: the code execution tool, MCP connector, Files API, and the ability to cache prompts for up to one hour.</li></ul><p>Claude Opus 4 and Sonnet 4 are hybrid models offering two modes: near-instant responses and extended thinking for deeper reasoning. The Pro, Max, Team, and Enterprise Claude plans include both models and extended thinking, with Sonnet 4 also available to free users. Both models are available on the Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing remains consistent with previous Opus and Sonnet models: Opus 4 at $15/$75 per million tokens (input/output) and Sonnet 4 at $3/$15.</p><h2 id="claude-4">Claude 4</h2><p>Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.</p><p>Claude Opus 4 excels at coding and complex problem-solving, powering frontier agent products. <strong>Cursor</strong> calls it state-of-the-art for coding and a leap forward in complex codebase understanding. <strong>Replit</strong> reports improved precision and dramatic advancements for complex changes across multiple files. <strong>Block</strong> calls it the first model to boost code quality during editing and debugging in its agent, <em>codename goose</em>, while maintaining full performance and reliability. <strong>Rakuten</strong> validated its capabilities with a demanding open-source refactor running independently for 7 hours with sustained performance. <strong>Cognition</strong> notes Opus 4 excels at solving complex challenges that other models can't, successfully handling critical actions that previous models have missed.</p><p>Claude Sonnet 4 significantly improves on Sonnet 3.7's industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.</p><p><strong>GitHub</strong> says Claude Sonnet 4 soars in agentic scenarios and will introduce it as the base model for the new coding agent in GitHub Copilot. <strong>Manus</strong> highlights its improvements in following complex instructions, clear reasoning, and aesthetic outputs. <strong>iGent</strong> reports Sonnet 4 excels at autonomous multi-feature app development, as well as substantially improved problem-solving and codebase navigation—reducing navigation errors from 20% to near zero. <strong>Sourcegraph</strong> says the model shows promise as a substantial leap in software development—staying on track longer, understanding problems more deeply, and providing more elegant code quality. <strong>Augment Code</strong> reports higher success rates, more surgical code edits, and more careful work through complex tasks, making it the top choice for their primary model.</p><p>These models advance our customers' AI strategies across the board: Opus 4 pushes boundaries in coding, research, writing, and scientific discovery, while Sonnet 4 brings frontier performance to everyday use cases as an instant upgrade from Sonnet 3.7.</p><div><figure><img alt="Bar chart comparison between Claude and other LLMs on software engineering tasks" loading="lazy" width="3840" height="2304" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a6d5aa47c25cb2037efff9f486da4918f77708-3840x2304.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a6d5aa47c25cb2037efff9f486da4918f77708-3840x2304.png&amp;w=3840&amp;q=75"><figcaption>Claude 4 models lead on SWE-bench Verified, a benchmark for performance on real software engineering tasks. See appendix for more on methodology.</figcaption></figure></div><div><figure><img alt="Benchmark table comparing Opus 4 and Sonnet 4 to other LLM" loading="lazy" width="2600" height="2118" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6246b412f30444ce8e1e5746e226c56a743bd99f-2600x2118.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F6246b412f30444ce8e1e5746e226c56a743bd99f-2600x2118.png&amp;w=3840&amp;q=75"><figcaption>Claude 4 models deliver strong performance across coding, reasoning, multimodal capabilities, and agentic tasks. See appendix for more on methodology.</figcaption></figure></div><h2 id="model-improvements">Model improvements</h2><p>In addition to extended thinking with tool use, parallel tool execution, and memory improvements, we’ve significantly reduced behavior where the models use shortcuts or loopholes to complete tasks. Both models are 65% less likely to engage in this behavior than Sonnet 3.7 on agentic tasks that are particularly susceptible to shortcuts and loopholes.</p><p>Claude Opus 4 also dramatically outperforms all previous models on memory capabilities. When developers build applications that provide Claude local file access, Opus 4 becomes skilled at creating and maintaining 'memory files' to store key information. This unlocks better long-term task awareness, coherence, and performance on agent tasks—like Opus 4 creating a 'Navigation Guide' while playing Pokémon.</p><div><figure><img alt="A visual note in Claude's memories that depicts a navigation guide for the game Pokemon Red." loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fe51564bb5ce9597dbfc59bbab13a0efbe25a7d66-1920x1080.gif&amp;w=3840&amp;q=75"><figcaption>Memory: When given access to local files, Claude Opus 4 records key information to help improve its game play. The notes depicted above are real notes taken by Opus 4 while playing Pokémon.</figcaption></figure></div><p>Finally, we've introduced thinking summaries for Claude 4 models that use a smaller model to condense lengthy thought processes. This summarization is only needed about 5% of the time—most thought processes are short enough to display in full. Users requiring raw chains of thought for advanced prompt engineering can <a href="https://www.anthropic.com/contact-sales">contact sales</a> about our new Developer Mode to retain full access.</p><h2 id="claude-code">Claude Code</h2><p>Claude Code, now generally available, brings the power of Claude to more of your development workflow—in the terminal, your favorite IDEs, and running in the background with the Claude Code SDK.</p><p>New beta extensions for VS Code and JetBrains integrate Claude Code directly into your IDE. Claude’s proposed edits appear inline in your files, streamlining review and tracking within the familiar editor interface. Simply run Claude Code in your IDE terminal to install.</p><p>Beyond the IDE, we're releasing an extensible Claude Code SDK, so you can build your own agents and applications using the same core agent as Claude Code. We're also releasing an example of what's possible with the SDK: Claude Code on GitHub, now in beta. Tag Claude Code on PRs to respond to reviewer feedback, fix CI errors, or modify code. To install, run /install-github-app from within Claude Code.</p><h2 id="getting-started">Getting started</h2><p>These models are a large step toward the virtual collaborator—maintaining full context, sustaining focus on longer projects, and driving transformational impact. They come with extensive testing and evaluation to minimize risk and maximize safety, including <a href="https://www.anthropic.com/news/activating-asl3-protections">implementing measures</a> for higher AI Safety Levels like ASL-3.</p><p>We're excited to see what you'll create. Get started today on <a href="https://claude.ai/">Claude</a>, <a href="https://www.anthropic.com/claude-code">Claude Code</a>, or the platform of your choice.</p><p><em>As always, your <a href="mailto: feedback@anthropic.com">feedback</a> helps us improve.</em></p></div></article><div><h4>Appendix</h4><h4>Performance benchmark data sources</h4><ul><li>Open AI: <a href="https://openai.com/index/introducing-o3-and-o4-mini/">o3 launch post</a>, <a href="https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf">o3 system card</a>, <a href="https://openai.com/index/gpt-4-1/">GPT-4.1 launch post</a>, <a href="https://github.com/openai/simple-evals/blob/main/multilingual_mmlu_benchmark_results.md">GPT-4.1 hosted evals</a></li><li>Gemini: <a href="https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf">Gemini 2.5 Pro Preview model card</a></li><li>Claude: <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet launch post</a></li></ul><h4>Performance benchmark reporting</h4><p>Claude Opus 4 and Sonnet 4 are hybrid reasoning models. The benchmarks reported in this blog post show the highest scores achieved with or without extended thinking. We’ve noted below for each result whether extended thinking was used:</p><ul><li>No extended thinking: SWE-bench Verified, Terminal-bench</li><li>Extended thinking (up to 64K tokens):<ul><li>TAU-bench (no results w/o extended thinking reported)</li><li>GPQA Diamond (w/o extended thinking: Opus 4 scores 74.9% and Sonnet 4 is 70.0%)</li><li>MMMLU (w/o extended thinking: Opus 4 scores 87.4% and Sonnet 4 is 85.4%)</li><li>MMMU (w/o extended thinking: Opus 4 scores 73.7% and Sonnet 4 is 72.6%)</li><li>AIME (w/o extended thinking: Opus 4 scores 33.9% and Sonnet 4 is 33.1%)</li></ul></li></ul><h4>TAU-bench methodology</h4><p>Scores were achieved with a prompt addendum to both the Airline and Retail Agent Policy instructing Claude to better leverage its reasoning abilities while using extended thinking with tool use. The model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).</p><h3>SWE-bench methodology</h3><p>For the Claude 4 family of models, we continue to use the same simple scaffold that equips the model with solely the two tools described in our prior releases <a href="https://www.anthropic.com/engineering/swe-bench-sonnet">here</a>—a bash tool, and a file editing tool that operates via string replacements. We no longer include the <a href="https://www.anthropic.com/engineering/claude-think-tool">third ‘planning tool’</a> used by Claude 3.7 Sonnet. On all Claude 4 models, we report scores out of the full 500 problems. Scores for OpenAI models are reported out of a <a href="https://openai.com/index/gpt-4-1/">477 problem subset</a>.</p><p>For our “high compute” numbers we adopt additional complexity and parallel test-time compute as follows:</p><ul><li>We sample multiple parallel attempts.</li><li>We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by <a href="https://arxiv.org/abs/2407.01489">Agentless (Xia et al. 2024)</a>; note no hidden test information is used.</li><li>We then use an internal scoring model to select the best candidate from the remaining attempts.</li></ul><p>This results in a score of 79.4% and 80.2% for Opus 4 and Sonnet 4 respectively.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla to shut down Pocket on July 8 (373 pts)]]></title>
            <link>https://support.mozilla.org/en-US/kb/future-of-pocket</link>
            <guid>44063662</guid>
            <pubDate>Thu, 22 May 2025 16:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.mozilla.org/en-US/kb/future-of-pocket">https://support.mozilla.org/en-US/kb/future-of-pocket</a>, See on <a href="https://news.ycombinator.com/item?id=44063662">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="doc-content">
    
      <p>We’ve made the difficult decision to shut down Pocket on July 8, 2025. Thank you for being part of our journey over the years—we're proud of the impact Pocket has had for our users and communities.  
</p><p>This article explains everything you need to know, including how to save your content, get a refund (if you're a Premium user), and what to expect next.
</p>
<div id="toc"><h2>Table of Contents</h2><ul><li><a href="#w_when-is-pocket-shutting-down"><span>1</span> <span>When is Pocket shutting down?</span></a></li><li><a href="#w_why-is-pocket-shutting-down"><span>2</span> <span>Why is Pocket shutting down?</span></a></li><li><a href="#w_a-look-back-the-legacy-of-pocket"><span>3</span> <span>A Look Back: The Legacy of Pocket</span></a></li><li><a href="#w_account-saves"><span>4</span> <span>Account &amp; Saves</span></a><ul><li><a href="#w_how-to-export-your-saved-articles"><span>4.1</span> <span>How to export your saved articles</span></a></li><li><a href="#w_do-i-need-to-delete-my-account-to-protect-my-data-what-happens-if-i-dont-delete-it"><span>4.2</span> <span>Do I need to delete my account to protect my data? What happens if I don’t delete it?</span></a></li></ul></li><li><a href="#w_refunds-for-pocket-premium-subscribers"><span>5</span> <span>Refunds for Pocket Premium Subscribers</span></a><ul><li><a href="#w_how-will-refunds-to-premium-annual-subscribers-be-calculated"><span>5.1</span> <span>How will refunds to Premium annual subscribers be calculated?</span></a></li></ul></li><li><a href="#w_pocket-apps-extensions"><span>6</span> <span>Pocket Apps &amp; Extensions</span></a><ul><li><a href="#w_what-will-happen-to-the-pocket-browser-extensions"><span>6.1</span> <span>What will happen to the Pocket browser extensions?</span></a></li><li><a href="#w_when-will-the-app-no-longer-be-available-on-app-stores"><span>6.2</span> <span>When will the app no longer be available on app stores?</span></a></li></ul></li><li><a href="#w_pocket-api-use"><span>7</span> <span>Pocket API Use</span></a><ul><li><a href="#w_how-are-api-users-going-to-be-impacted"><span>7.1</span> <span>How are API users going to be impacted?</span></a></li><li><a href="#w_will-api-users-be-able-to-continue-using-pocket-after-it-shuts-down"><span>7.2</span> <span>Will API users be able to continue using Pocket after it shuts down?</span></a></li></ul></li><li><a href="#w_pocket-hits"><span>8</span> <span>Pocket Hits</span></a><ul><li><a href="#w_whats-happening-to-pocket-hits"><span>8.1</span> <span>What’s happening to Pocket Hits?</span></a></li></ul></li><li><a href="#w_key-dates"><span>9</span> <span>Key dates</span></a></li><li><a href="#w_need-help"><span>10</span> <span>Need help?</span></a></li></ul></div>
<h2 id="w_when-is-pocket-shutting-down">When is Pocket shutting down?</h2>
<p>Pocket will no longer be available after July 8, 2025.
</p><p>You can continue using the app and browser extensions until this date. After July 8, Pocket will move into export-only mode. Users can export saves anytime until October 8, 2025, after which user data will be permanently deleted.
</p>
<h2 id="w_why-is-pocket-shutting-down">Why is Pocket shutting down?</h2>
<p>Pocket has helped millions save articles and discover stories worth reading. But the way people use the web has evolved, so we’re channeling our resources into projects that better match their browsing habits and online needs. 
</p><p>Read more about the decision <a href="https://blog.mozilla.org/en/mozilla/building-whats-next/">here</a>.
</p>
<h2 id="w_a-look-back-the-legacy-of-pocket">A Look Back: The Legacy of Pocket</h2>
<p>What began as a read-it-later app evolved into something much bigger. After <a href="https://blog.mozilla.org/en/mozilla/news/mozilla-acquires-pocket/">Mozilla acquired Pocket in 2017</a>, we invested in building our content curation and recommendation capabilities so people everywhere can discover and access high quality web content. While Pocket is shutting down, we will continue to invest in this promise—through the New Tab experience, our email newsletter, and more.
</p><p>Over the past eight years, we’ve:
</p>
<ul><li>Expanded high-quality content recommendations to more than a dozen countries and five languages.
</li><li>Connected tens of millions of people across the world with stories worth their time and attention.
</li><li>Earned recognition including a Webby Award for “Best of Pocket: 2020” and an Anthem Award in 2023 for supporting local journalism.
</li><li>Published hundreds of curated collections on topics from fighting algorithmic bias to rethinking happiness.
</li></ul>
<p>While this chapter is ending, we're deeply proud of Pocket’s impact and grateful to every reader, saver, and explorer who made it special.
</p>
<h2 id="w_account-saves">Account &amp; Saves</h2>
<h2 id="w_how-to-export-your-saved-articles">How to export your saved articles</h2>
<p>You will be able to export your saved articles, including items in your list, archive, favorites, notes, and highlights, until October 8, 2025. After this date, all user accounts and data will be permanently deleted.
</p><p>You can learn more about exporting your saved content <a href="https://support.mozilla.org/en-US/kb/exporting-your-pocket-list">here</a>.
</p>
<h2 id="w_do-i-need-to-delete-my-account-to-protect-my-data-what-happens-if-i-dont-delete-it">Do I need to delete my account to protect my data? What happens if I don’t delete it?</h2>
<p>You don’t need to delete your account. All Pocket user data will automatically be deleted on October 8, 2025. You can export your saves anytime before then from the Pocket export page.  
</p>
<h2 id="w_refunds-for-pocket-premium-subscribers">Refunds for Pocket Premium Subscribers</h2>
<h2 id="w_how-will-refunds-to-premium-annual-subscribers-be-calculated">How will refunds to Premium annual subscribers be calculated?</h2>
<p>Pocket Premium refunds will be made on a prorated basis. This means the refund will be based on how much time was left in a subscription after July 8, 2025. We’ve arranged refunds to your original payment method based on your subscription type:
</p>
<ol><li><strong>Monthly subscribers</strong>
<ul><li> We will begin disabling automatic renewal of monthly subscriptions immediately.
</li><li> You can continue to enjoy the benefits of Pocket Premium until the end of the monthly subscription period.
</li><li> You will not be charged again, so no refund will be necessary.
</li><li> No action is required from you.
</li></ul>
</li><li><strong>Annual subscribers</strong>
<ul><li> On July 8, 2025, Annual subscriptions will be cancelled and Annual users will receive a prorated refund automatically to the original payment method.
</li><li> No action is needed from you.
</li></ul>
</li></ol>
<h2 id="w_pocket-apps-extensions">Pocket Apps &amp; Extensions</h2>
<h2 id="w_what-will-happen-to-the-pocket-browser-extensions">What will happen to the Pocket browser extensions?</h2>
<p>The Pocket web extensions will no longer be available to install from May 22, 2025. Anyone attempting to use the Pocket extensions from this date will be taken to the Pocket export page. 
</p><p>Pocket browser add ons will remain on users' browsers after Pocket shuts down on July 8, 2025. Users have to manually remove the Pocket add on from their browsers. 
</p><p>For more information on how to remove an add on from their browser, users should visit their browser’s support pages. 
</p><p>More information for Firefox users on how to remove the Pocket add on is available <a href="https://support.mozilla.org/et/kb/disable-or-re-enable-pocket-for-firefox">here</a>.
</p>
<h2 id="w_when-will-the-app-no-longer-be-available-on-app-stores">When will the app no longer be available on app stores?</h2>
<ul><li>Users who have never installed the Pocket app will not be able to install it after May 22, 2025. 
</li><li>Users who already have the app installed will be able to re-install it up to October 8, 2025. 
</li><li>Users who still have the app installed on their device will need to manually delete it. 
</li></ul>
<h2 id="w_pocket-api-use">Pocket API Use</h2>
<h2 id="w_how-are-api-users-going-to-be-impacted">How are API users going to be impacted?</h2>
<p>Any product that leverages Pocket’s API will no longer be able to load users’ lists, or save, tag, or delete articles. 
</p>
<h2 id="w_will-api-users-be-able-to-continue-using-pocket-after-it-shuts-down">Will API users be able to continue using Pocket after it shuts down?</h2>
<p>API users will no longer be able to transact data (read or write) over Pocket’s API from October 8, 2025 and will need to export their data before this date.
</p>
<h2 id="w_pocket-hits">Pocket Hits</h2>
<h2 id="w_whats-happening-to-pocket-hits">What’s happening to Pocket Hits?</h2>
<p>Our email newsletter, currently known as Pocket Hits, will soon be renamed “Ten Tabs”. While the name is changing, the heart of the newsletter remains the same—expertly curated, trustworthy content delivered by the same editorial team you know and enjoy, now powered by Firefox.
</p><p>All readers will be notified about the update through the newsletter itself, and no action is needed to continue receiving it.
</p><p>Once the transition is complete, Ten Tabs will be sent Monday through Friday. The weekend editions will be discontinued, and the once-a-week delivery option will no longer be offered. Readers on the weekly schedule will only be moved to daily (weekday) delivery if they opt in. As always, you can unsubscribe anytime using the link at the bottom of each email.
</p>
<h2 id="w_key-dates">Key dates</h2>
<p>Thank you for your support, feedback, and enthusiasm over the years. We’re grateful to have been part of your reading and discovery experience online.
</p>
<table>
<tbody><tr>
<th>Date
</th><th>What happens:
</th></tr>
<tr>
<td>May 22, 2025
</td><td>Pocket removed from app stores
<p><br>
Monthly subscription renewals disabled
</p><p>New Pocket account sign-ups disabled
</p>
</td></tr>
<tr>
<td>July 8, 2025
</td><td>Pocket shuts down
<p><br>
Annual subscriber refunds occur – prorated and automatic	
</p><p>Export-only mode on Pocket Web begins
</p>
</td></tr>
<tr>
<td>October 8, 2025
</td><td>Final date to export data
<p><br>
All accounts and data deleted
</p>
</td></tr></tbody></table>
<h2 id="w_need-help">Need help?</h2>
<p>Need help or have questions? Our support team is here for you. <a href="https://support.mozilla.org/en-US/questions/new/pocket/form">Ask a question</a> for help with exporting data, account info, or anything else.
</p>
    
  </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[That fractal that's been up on my wall for 12 years (166 pts)]]></title>
            <link>https://chriskw.xyz/2025/05/21/Fractal/</link>
            <guid>44063248</guid>
            <pubDate>Thu, 22 May 2025 15:50:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chriskw.xyz/2025/05/21/Fractal/">https://chriskw.xyz/2025/05/21/Fractal/</a>, See on <a href="https://news.ycombinator.com/item?id=44063248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><strong>Warning:</strong> Math, Handwaving</p>

<p>I spent a lot of time doodling in middle school in lieu of whatever it is middle schoolers are
supposed to be doing. Somewhere between the <a href="https://en.wikipedia.org/wiki/Cool_S">Cool S</a>’s
and <a href="https://en.wikipedia.org/wiki/Penrose_triangle">Penrose triangles</a> I stumbled upon a neat
way to fill up graph paper by repeatedly combining and copying squares. I suspected there was
more to the doodle but wasn’t quite sure how to analyze it. Deciding to delegate to a future version of me that
knows more math, I put it up on the wall behind my desk where it has followed me from high
school to college to the present day.</p>

<!--more-->

<p><img src="https://chriskw.xyz/images/fractal/iter0to4.jpg" alt="Static image of iterations 0 to 4"></p>

<p>Anyway, after a series of accidents I am now the prophesized future version of me that knows a bit more math.
Due to its petal-like blooming structure and timeless presence scotch taped to my wall I’ll be referring to the
fractal affectionately as “the wallflower,” although further down we’ll see it’s closely related
to some well-known fractals. To start investigating it might help to run through
the steps of how middle school me originally drew it:</p>

<ol>
  <li>Start with a single square.</li>
  <li>Tile four copies of the current state to the left, right, top, and bottom of the current state.</li>
  <li>Tile four copies of the current state slightly angled (about 27 degrees clockwise) from the left, right, top, and bottom of the current state.</li>
  <li>Alternate between steps 2 and 3 until you run out of graph paper.</li>
</ol>

<p>In animated form:</p>

<p><img src="https://chriskw.xyz/images/fractal/wallflower.gif" alt="Gif of the first 7 iterations of constructing the fractal"></p>

<p><em>Shoutout to <a href="https://www.manim.community/">manim</a> and <a href="https://www.3blue1brown.com/">3Blue1Brown</a> for making this and many other visualizations to come possible!</em></p>

<p>Similar to the <a href="https://en.wikipedia.org/wiki/Gosper_curve">Gosper Curve</a>, the steps can
be run repeatedly to eventually cover any part of the plane, and each intermediate state can tile the plane.
If you have graph paper and free time you can try out the steps for yourself – it’s fun to
translate and trace the contour of the previous state and watch things lock into place like a puzzle.
Alternatively, a bit over a year ago I realized you could
generate the contour using an <a href="https://en.wikipedia.org/wiki/L-system">L-System</a>.
The rules are simple and consist of only 90 degree right (\(R\)) and left (\(L\)) turns:</p>

<ol>
  <li>Start with 4 right turns: \(RRRR\)</li>
  <li>Each iteration run the following substitutions: \(R \rightarrow RLR, L \rightarrow RLL\)</li>
</ol>

<p>For example, after applying the first iteration of substitutions you should have \(RLRRLRRLRLR\).
The following images demonstrate the first few applications of the rule:</p>

<p><img src="https://chriskw.xyz/images/fractal/contouriter0to4.jpg" alt="Static image of iterations 0 to 4"></p>

<p><em>Yellow=next turn (going clockwise) will be to the left, Blue=to the right</em></p>

<p>And in animated form:</p>

<p><img src="https://chriskw.xyz/images/fractal/contour.gif" alt="Gif of the first 4 iterations of constructing the boundary"></p>

<p>Both methods end up generating equivale– hold up! When I first tried
the L-System method a year ago I <em>thought</em> it generated the same contour as the wallflower.
In other words, I tried drawing the fourth iteration and its many right angles free
hand and gave up partway thinking “well it worked for the first 3 iterations, therefore it works in general \(\text{Q.E.D.}\)”
It was only when I started making the animations for this post that I realized
the two don’t quite match up. Comparing the 4th iteration from each method:</p>

<p><img src="https://chriskw.xyz/images/fractal/woops.jpg" alt="Annotated comparison between two different versions of fourth iteration"></p>

<p>Side by side, the main difference between the two is how the “copies”
of the 3rd iteration are placed around the original in the center. The first method (let’s call it
“drag and drop”) places the copies directly above, below, etc… around the center, while
the L-System method places them diagonally. The contour produced by the L-System
approach is already documented in a few places:</p>
<ul>
  <li>Wikipedia article for <a href="https://en.wikipedia.org/wiki/List_of_fractals_by_Hausdorff_dimension">List of fractals</a> (listed as “Quadratic von Koch island”)</li>
  <li>Wikipedia article for <a href="https://en.wikipedia.org/wiki/Koch_snowflake">Koch snowflake</a> (listed as “Quadratic Flake”, with file name “<a href="https://imagej.net/ij/plugins/fractal-generator/index.html">Karperien</a> Flake”)</li>
  <li>Wikipedia article for <a href="https://en.wikipedia.org/wiki/Minkowski_sausage">Minkowski Sausage</a></li>
  <li>Jeffrey Ventrella’s generator for <a href="http://www.fractalcurves.com/Root5.html">Mandelbrot’s Quartet</a></li>
</ul>

<p>Meanwhile, the variation made from the drag and drop method doesn’t appear anywhere I can find
via Google image search and Wikipedia surfing. Why would one way be so much more common than the other?
With some fiddling around I found rules to generate my wall’s version of the fractal (\(L \rightarrow RLR, R \rightarrow LLR\)), however they have a strange
effect that seems to “flip” the direction you draw the contour at each step, e.g. the first step
goes from \(RRRR\) (majority right turns) to \(LLRLLRLLRLLR\) (majority left turns).
Another natural question is if the original L-System doesn’t place copies aligned with
the axes, what angle is it placing them at? It turns out the “flipping” behavior,
the L-System’s angles, and the seemingly arbitrary “about 27 degrees” from the beginning
are connected in a surprising way. But before we get to that, lets take a detour to review an important topic:</p>

<h2 id="how-to-count">How to count</h2>

<p>Procrastinating for over a decade has given me plenty of opportunities to look at the fractal
with fresh eyes as I was introduced to new branches of math. During freshman year of college I learned how to show the cardinality
of the natural numbers \(\mathbb{N}\) is equal to the cardinality of pairs of natural
numbers \(\mathbb{N}^2\) using the <a href="https://en.wikipedia.org/wiki/Pairing_function#Cantor_pairing_function">Cantor pairing function</a>
to “dovetail” across the Cartesian plane. Similarly, I learned you could map the natural
numbers onto a spiral to show \(\mathbb{N}\) has the same cardinality as pairs of integers \(\mathbb{Z}^2\).</p>

<p><img src="https://chriskw.xyz/images/fractal/cantorpairing.png" alt="Cantor pairing function">
<img src="https://chriskw.xyz/images/fractal/z2spiral.jpg" alt="Z^2 spiral"></p>

<p><em>Credit to <a href="https://en.m.wikipedia.org/wiki/Pairing_function#/media/File%3ACantor's_Pairing_Function.svg">Wikipedia</a> for the Cantor pairing function image, <a href="http://stanford.edu/~dntse/classes/cs70_fall09/n20_fall09.pdf">UCB CS70</a> for the spiral</em></p>

<p>Both of these reminded me of how the wallflower fills space in the Cartesian
plane by building outwards from the origin. To use the wallflower’s structure as a pairing
function we would need to find a way to assign an “order” when we place each
square, preferably in a way that complements the recursive nature of its construction.
A natural starting point would be to use the center of the fractal as 0. From there we can number
the surrounding four squares added in the first iteration as 1, 2, 3 and 4
in clockwise order:</p>

<p><img src="https://chriskw.xyz/images/fractal/numbering1.jpg" alt="Numbering 0 to 4"></p>

<p>Now we’re faced with the question of how to label the squares from the next iteration. One
way would be to number them in the order they appear scanning from top down, left to right:</p>

<p><img src="https://chriskw.xyz/images/fractal/numberingjank.jpg" alt="Jank numbering 0 to 24"></p>

<p>For lack of better words, this doesn’t feel very fractally – the order here seems unrelated to recursive
structure of fractal. What if instead we tried to reuse the “middle out” approach used for
0 to 4? After all, each “petal” is a copy of the first iteration we’ve
already given an ordering to. Reusing the clockwise scheme from 0 to 4 within each blue
petal, and <em>across</em> each blue petal (the dashed lines):</p>

<p><img src="https://chriskw.xyz/images/fractal/numbering2.jpg" alt="Numbering 0 to 24"></p>

<p>And extending to the next set of petals:</p>

<p><img src="https://chriskw.xyz/images/fractal/numbering3.jpg" alt="Numbering 0 to 124"></p>

<p>As chaotic as it may look at first glance, a few interesting properties emerge
if we look at the positions of certain numbers. If we isolate our view to just multiples of 5,
a scaled up grid tilted about 27 degrees clockwise is formed:</p>

<p><img src="https://chriskw.xyz/images/fractal/just5s.jpg" alt="Just multiples of 5"></p>

<p>If we only look at the numbers of the form \(5n + 1\),
we get the previous grid but scooted up by 1 square:</p>

<p><img src="https://chriskw.xyz/images/fractal/just5plus1s.jpg" alt="Just 5n + 1"></p>

<p>And if we look at just multiples of 25 we get another grid, scaled up even further:</p>

<p><img src="https://chriskw.xyz/images/fractal/just25s.jpg" alt="Just multiples of 25"></p>

<p>The number 5 appears to have a special relationship with the fractal. The reason becomes
apparent if you look at the number of squares in each iteration. The 0th iteration
is a single square, the first iteration has 5, the second has 25, the third has 125, etc… Since
each iteration is constructed by taking the previous state and adding 4 additional copies,
it scales up by a factor of 5 in each step. Given the special relationship
with multiples of 5 and powers and 5, it’s tempting to redo the labeling while counting
in base 5 instead of base 10. Doing so we get this:</p>

<p><img src="https://chriskw.xyz/images/fractal/numberingbase5.jpg" alt="Numbering in base 5"></p>

<p>This is a ton of information, but if we focus in on just the iteration 3 pattern and its copy to the right you might
notice something interesting:</p>

<h2 id="how-to-add">How to add</h2>

<p><img src="https://chriskw.xyz/images/fractal/numberingsidebyside.jpg" alt="Iteration 3 + copy to the right"></p>

<p>If you take any number on the left and check 5 spaces to the right, you’ll find
its copy on the teal petal. Comparing the numbers between cells, they always seem to be
the original value plus 200. For example, 5 spaces to the right of
44 you’ll find 244, and 5 spaces to the right of 3 is 203. In some sense, adding 200
seems to encode “shifting” 5 spaces to the right. This “shifting” property isn’t unique to 200
either: consider the positions of 0, 1, 2, 3, and 4 relative to 30, 31, 32, 33, and 34.</p>

<p>If you stare for long enough you might notice by expanding any number, say 231 = 200 + 30 + 1,
we can use each number in the expanded form to find its location on the grid. In this case, we
can find vectors corresponding to 200, 30, and 1 on the grid and add them
together to get the location of 231:</p>

<p><img src="https://chriskw.xyz/images/fractal/vector231.gif" alt="231 decomposition vectors"></p>

<p>You can test out other numbers using the same strategy of breaking down the number into
its 1s, 10s, 100s, etc… places and adding vectors. Assuming
this works in general, all we need to find the position of any number on the grid is to know the position of each number
in its expanded form, and then add them together. Let’s <del>abuse</del> introduce some notation, defining \(\vec{n}\) as the vector
pointing to where the number \(n\) sits on the grid. Using our 1 digit values as an example:</p>

<p>\(\begin{align}
    \overrightarrow{0} &amp;= \begin{bmatrix}
           0 \\
           0 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{1} &amp;= \begin{bmatrix}
           1 \\
           0 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{2} &amp;= \begin{bmatrix}
           0 \\
           1 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{3} &amp;= \begin{bmatrix}
           -1 \\
           0 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{4} &amp;= \begin{bmatrix}
           0 \\
           -1 \\
         \end{bmatrix}
  \end{align}\)</p>

<p>With this new notation, we can stare at how “powers of 10” seem to behave:</p>

<p>\(\begin{align}
    \overrightarrow{1} &amp;= \begin{bmatrix}
           0 \\
           1 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{10} &amp;= \begin{bmatrix}
           1 \\
           2 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{100} &amp;= \begin{bmatrix}
           0 \\
           5 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{1000} &amp;= \begin{bmatrix}
           5 \\
           10 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{10000} &amp;= \begin{bmatrix}
           0 \\
           25 \\
         \end{bmatrix}
  \end{align},\)\(\begin{align}
    \overrightarrow{100000} &amp;= \begin{bmatrix}
           25 \\
           50 \\
         \end{bmatrix}
  \end{align}\)</p>

<p>Looking closely you might pick up on the pattern:</p><p>

\[\begin{equation}
\overrightarrow{(10^n)} =  \begin{cases}
5^n \begin{bmatrix}
           0 \\
           1 \\
         \end{bmatrix} &amp; \text{if } n \text{ is even} \\
5^{(n-1)} \begin{bmatrix}
           1 \\
           2 \\
         \end{bmatrix} &amp; \text{if } n \text{ is odd}
\end{cases}
\end{equation}\]

</p><p>This conditional formula was
how I initally wrote the visualization code. However it begged the question:
is there a more elegant way to compute the values without needing a conditional?
Ideally we want a transformation we could repeatedly apply to scale and rotate a vector
based on the value of \(n\), i.e. a matrix raised to the power of \(n\). With some fiddling around, we can find the matrix \(M\) and its powers:</p>

<p>\(M = \begin{bmatrix}
           -2 &amp; 1 \\
           1 &amp; 2\\
\end{bmatrix},\)\(M^2 = \begin{bmatrix}
           5 &amp; 0 \\
           0 &amp; 5\\
\end{bmatrix},\)\(M^3 = \begin{bmatrix}
           -10 &amp; 5 \\
           5 &amp; 10\\
\end{bmatrix},\)\(M^4 = \begin{bmatrix}
           25 &amp; 0 \\
           0 &amp; 25\\
\end{bmatrix}, \text{etc...}\)</p>

<p>Which conveniently matches the values we’re expecting <em>without</em> needing a conditional in the equation:</p><p>

\[\overrightarrow{(10^n)}  = M^n\overrightarrow{1} = \begin{bmatrix}
           -2 &amp; 1 \\
           1 &amp; 2\\
         \end{bmatrix}^n\begin{bmatrix}
           0 \\
           1 \\
         \end{bmatrix}\]

</p><p>Similarly we can set up equations for the other possible digits:</p><p>

\[\begin{align}
\overrightarrow{(2 \cdot 10^n)} &amp;= M^n\overrightarrow{2} = \begin{bmatrix}
           -2 &amp; 1 \\
           1 &amp; 2\\
         \end{bmatrix}^n\begin{bmatrix}
           1 \\
           0 \\
         \end{bmatrix}
\newline
\overrightarrow{(3 \cdot 10^n)} &amp;= M^n\overrightarrow{3} = \begin{bmatrix}
           -2 &amp; 1 \\
           1 &amp; 2\\
         \end{bmatrix}^n\begin{bmatrix}
           0 \\
           -1 \\
         \end{bmatrix}
\newline
\overrightarrow{(4 \cdot 10^n)} &amp;= M^n\overrightarrow{4} = \begin{bmatrix}
           -2 &amp; 1 \\
           1 &amp; 2\\
         \end{bmatrix}^n\begin{bmatrix}
           -1 \\
           0 \\
         \end{bmatrix}
\end{align}\]

</p><p>If you blur your eyes with the tears shed over the cursed notation, you might
make out a connection to number systems such as base 5 or base 10.
Similar to how in base 10 the number 1234 would expand to:</p><p>

\[\begin{aligned}
1234 &amp;= 10^3 \cdot 1 + 10^2 \cdot 2 + 10^1 \cdot 3 + 10^0 \cdot 4
\newline
1234 &amp;= 1000 + 200 + 30 + 4
\end{aligned}\]

</p><p>And in base 5:</p><p>

\[\begin{aligned}
1234_5 &amp;= 5^3 \cdot 1 + 5^2 \cdot 2 + 5^1 \cdot 3 + 5^0 \cdot 4
\newline
1234_5 &amp;= 1000_5 + 200_5 + 30_5 + 4_5
\end{aligned}\]

</p><p>We can use the matrix \(M\) as our base, and vectors as our digits to encode positions
in the fractal:</p><p>

\[\begin{aligned}
\overrightarrow{1234} &amp;= M^3\overrightarrow{1} + M^2\overrightarrow{2} + M^1\overrightarrow{3} + M^0\overrightarrow{4}
\newline
\overrightarrow{1234} &amp;= \overrightarrow{1000} + \overrightarrow{200} + \overrightarrow{30} + \overrightarrow{4}
\end{aligned}\]

</p><p>We’ve stumbled upon a number system with a matrix base and vector digits, rather than scalars!
Counting up from 0 we can get a feel for the how the number system connects to the structure
of the fractal:</p>

<p><img src="https://chriskw.xyz/images/fractal/makeitcount.gif" alt="Gif with vectors + formulas"></p>

<h2 id="determinants">Determinants</h2>

<p>You may have noticed \(20\) and \(40\)
have switched positions compared to our original numbering. This is because our choice
of \(M\) has a negative determinant \(\det(M) = -5\), which means it has the side effect of “flipping” the
orientation of space each iteration. Now that we know our fractal is linked to linear algebra,
we can visualize the connection by overlaying scaled grids representing how powers of \(M\) act on our “digit vectors”
\(\vec{1}\), \(\vec{2}\), \(\vec{3}\) and \(\vec{4}\) a la <a href="https://www.3blue1brown.com/lessons/matrix-multiplication">3Blue1Brown</a>.</p>

<p><img src="https://chriskw.xyz/images/fractal/gridoverlay.gif" alt="Overlayed grid"></p>

<p>To avoid the flipping behavior we would need to pick a matrix with a positive determinant,
for example:</p><p>

\[M^\prime = \begin{bmatrix}
           2 &amp; 1 \\
           -1 &amp; 2\\
\end{bmatrix}\]

\[\det(M^\prime) = 5\]

</p><p>Visualizing this new choice of matrix:</p>

<p><img src="https://chriskw.xyz/images/fractal/gridoverlayprime.gif" alt="Overlayed grid for M'"></p>

<p>Instead of “flipping” and realigning with the axes every other iteration, this choice of
base continually rotates our “digit vectors” clockwise.
Wait, didn’t we see a version of the fractal like this way back at the start with the L-System?
Sure enough, using \(M^\prime\) as our base reproduces the L-System version.</p>

<p><img src="https://chriskw.xyz/images/fractal/unwoops.jpg" alt="Unwoops"></p>

<p>Mystery solved! The two fractals are <em>almost</em> the same, but the one on my wall is generated using
\(M\) where \(\det(M) = -5\), while the more common one is generated from \(M^\prime\)
where \(\det(M^\prime) = 5\). The choice of matrix also sheds some light on where the seemingly
arbitrary “about 27 degrees” comes from. You may have noticed the absolute value of both
determinants is \(5\), which conveniently matches the way the fractal increases in size by a factor
of 5 each iteration. If we picked a matrix with a larger determinant, our “digit vectors” would
grow too quickly and leave behind “empty space” each iteration. For example adjusting \(M\) to give
it a determinant of \(-6\) results in:</p>

<p><img src="https://chriskw.xyz/images/fractal/det6.jpg" alt="Determinant 6"></p>

<p>Meanwhile if we pick a matrix with a smaller determinant, our “digit vectors” would grow too
slowly and iterations would overlap. Adjusting \(M\) to give it a determinant of \(-4\):</p>

<p><img src="https://chriskw.xyz/images/fractal/det4.jpg" alt="Determinant 4"></p>

<p>So ostensibly we want a matrix base with determinant \(\pm 5\). Additionally, we want our
matrix to have integer entries to ensure it always maps our digit vectors onto
whole number coordinates. It just so happens the vector \(\left\langle 1, 2 \right\rangle\)
has integer entries and magnitude \(\sqrt{5}\), meaning we can use it and one of
its 90 degree rotations as the columns of our matrix to satisfy the determinant
and whole number constraint. Computing the angle of this vector we get \(\arctan{\frac{2}{1}} \approx 63.43^\circ\),
i.e. “about 27 degrees” away from the y-axis.</p>

<h2 id="how-to-add-part-2">How to add part 2</h2>

<p>You may have noticed vector addition fails horribly in most cases other than
the expanded form ones, e.g. \(\vec{2} + \vec{2} \neq \vec{4}\). This is expected, since the choice of \(2\) and \(4\)
were to help make the connection to base 5 number systems but unrelated to the
actual direction the vectors point. It might be more appropriate to refer to 1 through 4
as up (\(\vec{1}\)), right (\(\vec{2}\)), down (\(\vec{3}\)), and left (\(\vec{4}\)). As you
might expect, opposite directions cancel eachother out, i.e. \(\vec{1} + \vec{3} = \vec{2} + \vec{4} = \vec{0}\).
But what about other combinations of additions? By looking closely at the base 5 numbering and following where
combinations of unit vectors point:</p>

<p><img src="https://chriskw.xyz/images/fractal/base5center.jpg" alt="Base 5 numbering"></p>

<p>We can build up a table based on where it appears the sum of unit vectors would fall:</p>



<table>
  <thead>
    <tr>
      <th>\(+\)</th>
      <th>\(\overrightarrow{0}\hspace{5pt}\)</th>
      <th>\(\overrightarrow{1}\hspace{5pt}\)</th>
      <th>\(\overrightarrow{2}\hspace{5pt}\)</th>
      <th>\(\overrightarrow{3}\hspace{5pt}\)</th>
      <th>\(\overrightarrow{4}\hspace{5pt}\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(\overrightarrow{0}\hspace{5pt}\)</td>
      <td>\(\overrightarrow{0}\)</td>
      <td>\(\overrightarrow{1}\)</td>
      <td>\(\overrightarrow{2}\)</td>
      <td>\(\overrightarrow{3}\)</td>
      <td>\(\overrightarrow{4}\)</td>
    </tr>
    <tr>
      <td>\(\overrightarrow{1}\hspace{5pt}\)</td>
      <td>\(\overrightarrow{1}\)</td>
      <td>\(\overrightarrow{14}\)</td>
      <td>\(\overrightarrow{13}\)</td>
      <td>\(\overrightarrow{0}\)</td>
      <td>\(\overrightarrow{22}\)</td>
    </tr>
    <tr>
      <td>\(\overrightarrow{2}\hspace{5pt}\)</td>
      <td>\(\overrightarrow{2}\)</td>
      <td>\(\overrightarrow{13}\)</td>
      <td>\(\overrightarrow{41}\)</td>
      <td>\(\overrightarrow{44}\)</td>
      <td>\(\overrightarrow{0}\)</td>
    </tr>
    <tr>
      <td>\(\overrightarrow{3}\hspace{5pt}\)</td>
      <td>\(\overrightarrow{3}\)</td>
      <td>\(\overrightarrow{0}\)</td>
      <td>\(\overrightarrow{44}\)</td>
      <td>\(\overrightarrow{32}\)</td>
      <td>\(\overrightarrow{31}\)</td>
    </tr>
    <tr>
      <td>\(\overrightarrow{4}\hspace{5pt}\)</td>
      <td>\(\overrightarrow{4}\)</td>
      <td>\(\overrightarrow{22}\)</td>
      <td>\(\overrightarrow{0}\)</td>
      <td>\(\overrightarrow{31}\)</td>
      <td>\(\overrightarrow{23}\)</td>
    </tr>
  </tbody>
</table>

<p>The table is symmetric across the diagonal meaning addition is commutative, as expected
for vector addition. More importantly, several additions result in 2 digit values. While it might
not seem like a big deal, it means when doing larger additions we have to worry about
carrying over values to the next digit. For example, if we want to find the number directly
above 22, we can compute \(\overrightarrow{22} + \overrightarrow{1}\). Using
traditional long addition and carrying notation:</p><p>

\[\begin{array}{cccc}
	    &amp; \overset{1}{\vphantom{0}} &amp; \overset{1}{2} &amp; 2 \\
	  + &amp;   &amp;   &amp; 1 \\
        \hline
	    &amp; 1 &amp; 3 &amp; 3 \\
	\end{array}\]

</p><p>If you’re a bit confused, remember \(\overrightarrow{2} + \overrightarrow{1} = \overrightarrow{13}\).
And if you’re suprised this works at all, join the club! Since this is a blog post and not a proof,
I leave sanity checking this addition scheme works in general to the reader.</p>



<p>The concept of using things outside of \(\mathbb{N}\) in a number system wasn’t totally unfamiliar thanks to <a href="https://en.wikipedia.org/wiki/Balanced_ternary">Balanced Ternary</a>
which uses \(-1\), \(0\), and \(1\) as digits, and a base of \(3\). If you imagine balanced
ternary as being 1 dimensional along the x-axis, the wallflower can be seen as its 2D analog
by adding two new digits to account for the positive and negative directions of the y-axis.
An alternative scheme for <a href="https://en.wikipedia.org/wiki/Generalized_balanced_ternary">generalized balanced ternary</a>
already exists, and generalizes to any number of dimensions using lattices of <a href="https://en.wikipedia.org/wiki/Permutohedron">permutahedrons</a>
(at least from what I’ve been able to dig up). In 2 dimensions, this ends up being the hexagonal lattice:</p>

<p><img src="https://chriskw.xyz/images/fractal/hexagon.jpg" alt="2D generalized balanced ternary with hexagons"></p>

<p><em>Visualization of generalized balanced ternary in 2D using hexagons from <a href="https://en.wikipedia.org/wiki/Generalized_balanced_ternary#/media/File:Visualization_of_three-digit_2D_generalized_balanced_ternary_numbers.pngkipedia.org/wiki/Generalized_balanced_ternary#/media/File:Visualization_of_three-digit_2D_generalized_balanced_ternary_numbers.png">Wikipedia</a>. See also <a href="https://en.wikipedia.org/wiki/Gosper_curve">Gosper Curve</a>,
which is like a hexagonal wallflower.</em></p>

<p>Another exotic number system is <a href="https://en.wikipedia.org/wiki/Quater-imaginary_base">Quater-imaginary Base</a>,
which uses the imaginary value \(2i\) as its base, and \(0\), \(1\), \(2\) and \(3\) as digits.
If you imagine complex numbers as vectors, and imagine \(2i\) as a matrix that performs a scale and rotation,
we can convert this number system into our cursed notation:</p>

<p>\(M_{2i} = \begin{bmatrix}
           0 &amp; -2 \\
           2 &amp;  0\\
         \end{bmatrix},\)\(\overrightarrow{0} = \begin{bmatrix}
           0 \\
           0 \\
         \end{bmatrix},\)\(\overrightarrow{1} = \begin{bmatrix}
           1 \\
           0 \\
         \end{bmatrix},\)\(\overrightarrow{2} = \begin{bmatrix}
           2 \\
           0 \\
         \end{bmatrix},\)\(\overrightarrow{3} = \begin{bmatrix}
           3 \\
           0 \\
         \end{bmatrix}\)</p>

<p>Alternatively, we can convert the positive determinant matrix \(M^\prime\) from earlier into its
complex number equivalent to get a base \(2+i\) number system. <a href="https://ideophilus.wordpress.com/2016/10/17/balanced-base-2i-and-some-gratuitous-fractals/">Balanced base 2+i (and some gratuitous fractals)</a>
by Timothy James McKenzie Makarios explores this concept. I ran into this while looking for visualizations of
quater-imaginary base on Google images for this section, only to realize this connection had been
made back in 2016. Somewhat embarassingly I found this after I made the animation at the
start, only to find the author had already beaten me to that as well, although
using the \(M^\prime\) version of the fractal instead of \(M\) (as far as I know \(M\)
cannot be encoded as a complex number).</p>

<p>When I realized the matrix base number system worked at all I started
searching around to see if anyone else had made the connection between fractals, tesselations, linear algebra
and number systems. Digging around:</p>
<ul>
  <li><a href="https://math.stackexchange.com/questions/43054/is-there-a-number-system-with-matrix-base">Project BinSys</a> led by Attila Kovács
is focused on finding matrix bases specifically where the determinant is 2, for a generalized form of binary.</li>
  <li><a href="https://people.clas.ufl.edu/avince/files/SIDMARepTess.pdf">Replicating Tesselations</a>
by Andrew Vince does all the rigorous math stuff to formalize what I’ve been trying to
describe through vigorous handwaving, and generalizes to any lattice rather than just \(\mathbb{Z}^2\).
    <ul>
      <li>Here you can also find proofs for the alternative way to generalize balanced ternary
  into higher dimensions.</li>
    </ul>
  </li>
</ul>

<p>Speaking of handwaving: the next section is now fully in the territory of “things I
started to think about a week ago when I started writing this,” so any semblance of rigor
that came from having many years to think about the problem is out the window. Instead
I’ll be relying on my mental model of linear algebra, which is “if it sounds like it’s true
and looks like it’s true, it’s probably true.”</p>

<h2 id="to-go-even-further-beyond">To go even further beyond</h2>

<p>Middle school me was really into Minecraft, so naturally I always wondered: would the fractal work with
cubes? Specifically, is there a way to create a 3D version of the fractal by
starting with cube and copying outwards in groups of six to form a “3D plus”? This mostly
comes down to what properties we want to include when we generalize to a 3x3 matrix. The
ones that beckoned to me from the ether are:</p>
<ul>
  <li>All of the entries in the matrix must be integers. This is needed so when we apply
the matrix base to our vector digits (6 unit vectors sitting on the axes) they still
have integer values for each component. In the Vince paper this is formalized as “endomorphism of \(\Lambda\)”,
i.e. a mapping from the lattice \(\Lambda = \mathbb{Z}^3\) back onto itself.</li>
  <li>Each column vector in the matrix should have a Hamming distance of 3 from the origin.
This constraint ensures the 6 copies of the “3D plus” we create on the second
iteration don’t overlap with the original centered on the origin, but are near enough to still
be adjacent to it.</li>
  <li>We want a matrix with determinant \(\pm\)7. Since each iteration of the fractal adds 6 new copies,
we increase the size of the fractal by 7 times each step. If we want to “pack” together these
copies efficiently we need to make sure the matrix we apply scales up
inputs by a factor of 7 as well.</li>
</ul>

<p>Brute force searching through triplets of vectors with Hamming distance of 3, we get this nifty 3x3
matrix that checks all of the boxes:</p><p>

\[\begin{bmatrix}
            2 &amp; -1 &amp; 0 \\
           -1 &amp;  0 &amp; 2 \\
            0 &amp; -2 &amp; 1 \\
         \end{bmatrix}\]

</p><p>Visualizing gives us:</p>

<p><img src="https://chriskw.xyz/images/fractal/3d.gif" alt="3D iterations 0 through 3"></p>

<p>Wow, it looks terrible! One problem that stands out immediately is later iterations seem to be “smooshed”
resulting in spots where previous iterations are exposed.
Looking closely at iteration 2, you might notice we can comb over the bald spots
by adding two more “3D pluses” centered on \(\left\langle 1, 1, 1\right\rangle\) and \(\left\langle-1, -1, -1\right\rangle\).</p>

<p><img src="https://chriskw.xyz/images/fractal/bandaid.gif" alt="Iteration 2 with bandaid"></p>

<p>Here we added two new pluses marked in yellow, and visualize the location of the centers
of all 8 pluses. For a bandaid fix it feels like it works a little too well. Looking at just
the center points of our 8 new pluses they are arranged like the vertices of a warped cube.
I’m still not entirely sure what to make of this, although it feels like it’s related to
the fact that cubes (8 vertices, 6 faces) are the dual solid to octahedrons (6 vertices, 8 faces).
It’s obviously tempting to try to take this new cube and make 6 copies of it, but to avoid
derailing this post too much let’s save that for another day.</p>

<p>Visually, the problem with iteration 2 seems to be that the 3D pluses aren’t placed symmetrically
around the center, causing the fractal to expand in a non-uniform way. Manually trying to find a more symmetrical
arrangement of 6 pluses is tricky, feeling a bit like trying to <a href="https://en.wikipedia.org/wiki/Hairy_ball_theorem">comb a hairy ball</a>.
A sufficient (and possibly necessary) condition to prevent the smooshing behavior is
to pick a matrix where each column is mutually orthogonal and has the same magnitude. I suspect if
we include this with the prior constaints it’s impossible to satisfy in 3D. Each integer valued
column vector would need a magnitude of \(\sqrt[3]{7}\), so given that \(x, y\) and \(z\) are integers,
we must solve:</p><p>

\[\sqrt{x^2+y^2+z^2} = \sqrt[3]{7}\]

\[x^2 + y^2 + z^2 = 7^{2/3}\]

</p><p>This can never be true since the left hand side is always an integer, while the right hand
side is irrational. Luckily, if we go up to four dimensions the math works out in our favor. In 4 dimensions
the components of each vector must satisfy:</p><p>

\[\sqrt{x^2 + y^2 + z^2 + w^2} = \sqrt[4]{9}\]

\[x^2 + y^2 + z^2 + w^2 = 3\]

</p><p>Which works if we let 3 out of 4 of the entries to be \(\pm 1\),
and set the last to \(0\). It turns out there’s plenty of space in hyperspace, giving us many
possible matrices with columns of this form that satisfy all previous conditions. The
following matrix has one extra property that we’ll cover briefly near the end:</p><p>

\[\begin{bmatrix}
0 &amp; -1 &amp; -1 &amp; -1 \\
1 &amp;  0 &amp; -1 &amp;  1 \\
1 &amp;  1 &amp;  0 &amp; -1 \\
1 &amp; -1 &amp;  1 &amp;  0 \\
\end{bmatrix}\]

</p><p>Now that we have a suitable matrix, we can attempt to visualize our 4D fractal.
One way to do this is to take 3D “slices” of 4D space by fixing the
value of \(w\):</p>

<p><img src="https://chriskw.xyz/images/fractal/4d_iter0.gif" alt="4D iteration 0"></p>

<p>It turns out the base case is boring regardless of how many dimensions we give it. Moving
on to iteration 1:</p>

<p><img src="https://chriskw.xyz/images/fractal/4d_iter1.gif" alt="4D iteration 1"></p>

<p>From here you can get a better feel for how the visualization works. The purple cubes on the
left and right that appear to be sitting at the origin are really at the \((x,y,z,w)\)
coordinates of \((0,0,0,-1)\) and \((0,0,0,1)\) respectively.</p>

<p><img src="https://chriskw.xyz/images/fractal/4d_iter2.gif" alt="4D iteration 2"></p>

<p>We can see parts of iteration 1 still poking through in iteration 2, which is concerning given this
was a symptom of the squishing behavior we saw in 3D. However looking closely, the squishing
behavior doesn’t seem as bad yet. If you really twist your mind around the concept of 4
dimensions, you might be able to see that while the two pluses in \(w=0\) don’t
seem to be balanced, there are 3 pluses sitting to the left (\(w=-1\)) and 3 more to the
right (\(w=1\)) that you could imagine as being orthogonal to the \(w=0\) ones. The trios
of cubes sitting at each side (\(w=\pm 2\)) are part of the 4D pluses that poke through
the 4th dimension into the slices on the ends.</p>

<p><img src="https://chriskw.xyz/images/fractal/4d_iter3.gif" alt="4D iteration 3"></p>

<p>My main takeaway from this is that while iteration 2
can still be seen, iteration 1 has now been completely covered.
Technically iteration 3 extends farther out in the \(w\) dimension, but at this point it’s
difficult to comprehend the fractal anyway. I imagine this is the sort of thing my higher dimensional analog would
put up on the 4D hypersurface of their room’s 5D walls to admire in its full glory, so I’m dubbing it “the orthotopeflower.”
Unfortunately my apartment is measured in square feet and not hypercubic feet, so
it’s difficult for me to fully appreciate it. There are a couple of issues with the “3D slices”
approach of visualizing in 4D:</p>
<ul>
  <li>It extends way too quickly in the horizontal direction, wasting precious screen
(or wall) real estate. Notably in iteration 3 we
need to add 2 more slices to each side to visualize the whole thing.</li>
  <li>My inferior 3 dimensional eyes can’t see the interior of 3D objects (unlike how I’m
able to see the interior of colored in 2D squares on a piece of paper), making it
hard to fully appreciate the structure.</li>
</ul>

<p>To get around both of these limitations we can use the following 7x7 grid of 7x7 grids to visualize it
a la the very first animation of this post:</p>

<p><img src="https://chriskw.xyz/images/fractal/grow4d.gif" alt="Flat 4D animation"></p>

<p><em><a href="https://chriskw.xyz/images/fractal/static4d.jpg">Here’s a static image</a> of the last iteration if you want to admire it. If this isn’t nice, what is?</em></p>

<p>Each smaller grid shows a slice of -3 to 3 on the x and y axes. The “grid of grids” represent
-3 to 3 along the z and w axes. Whenever a square crosses from one small grid
to another it’s really being translated through the z or w axis. Squares that appear to
fade in or fade out are moving in or out of the 7x7x7x7 “viewing window” in 4 dimensions.
Compared to the line of 3D slices, this approach uses screen real estate a bit better
(growing as a square instead of in a line), and lets you see the interior of the entire
fractal without obscuring previous iterations. Plus this design would go nicely with
the existing fractal on my wall.</p>

<p>If we’re willing to discard all of the flourishes from the animation and assign 1 pixel
per tile in the fractal we can increase our viewing window up to 31x31x31x31:</p>

<p><img src="https://chriskw.xyz/images/fractal/31x31x31x31.png" alt="31x31x31x31 visualization"></p>

<p>It’s a lot to take in, but at the very least seems to confirm that the fractal consistently
expands outwards without excessive smooshing like we saw in 3D. And personally, I think
it would make a lovely quilt or picnic blanket. In particular these slices near the very
center are remarkably pleasant:</p>

<p><img src="https://chriskw.xyz/images/fractal/quilt.png" alt="Aesthetically pleasing slices 2D slices of the fractal"></p>

<p>A natural question is if we can continue into higher dimensions. Sadly I suspect the
answer is no. Applying all our constraints from earlier we need a dimension \(d\) such that:</p>

<p>\(\sqrt[d]{2d+1} \in\)\(\{\sqrt{3^2}, \sqrt{1^2 + 2^2}, \sqrt{1^2+1^2+1^2}\}\)</p>

<p>The left hand side is the magnitude each column vector needs to be for a matrix with \(d\) mutually
orthogonal columns to have determinant \(2d+1\) so that it scales inputs enough to fit the \(2d\) copies added each iteration.
The right hand side is the set of possible magnitudes of integer valued vectors that have a Hamming distance of 3 away from the origin, so
the copies created in the second iteration touch but don’t intersect with the first iteration.
Graphing this, it looks like the only dimensions where these can be satisifed are 1 (balanced
ternary), 2 (the wallflower/quadratic flake), and 4 (the orthotopeflower).</p>

<p><img src="https://chriskw.xyz/images/fractal/desmos.jpg" alt="Demos graph of viable dimensions"></p>

<p>Finally, as mentioned earlier, the choice of matrix to use as the base of our 4D number system was special. In particular
it encodes the quaternion \(i+j+k\), meaning similar to quater imaginary base we now
have a way to encode quaternions in “balanced nonary quaternion base”. In this scheme
the 9 possible digits are \(0\), \(\pm1\), \(\pm i\), \(\pm j\) and \(\pm k\), with base \(i+j+k\). Since I’m not too familiar
with quaternions and still not entirely sure if this actually works, I’ll be delegating
this problem to a future version of me who knows more math. This strategy seemed to work
pretty well last time.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>I once had a dream about a post-scarcity society that had to work around the problem of
people not having any motivation to do anything, since all their basic needs were met.
To give people something to do, every few years everyone over a certain age was expected
to report on something beautiful they learned or discovered about the world, otherwise
they’d be turned into soylent or something (Note: post-scarcity \(\neq\) utopia). Anyway, I’d like to
imagine the discovery of a strange flower with jigsaw-like petals blooming endlessly across 4
dimensional space would be enough to keep me off the chopping block for awhile.</p>

<p>I finally got around to this in an attempt attempt to reignite some passion for math and programming
after a period of burnout. It turns out my younger
self left behind the perfect gift in the form of a scavenger hunt across the domains of fractals, number systems,
linear algebra and higher dimensions. It makes me curious about how many other interesting
ideas other people have lying around in plain sight as sketches or To Do’s.</p>

<p>If this post comes off as a bit rambly from the many twists and turns between domains it’s
because I tried to follow the path I took while haphazardly picking up and forgetting
about the problem over the years. There are a handful of spots where you might ask “how did you know
doing this specific thing would lead to that?” The simple answer is I had no clue if any of this
would lead anywhere, I just tried different things on a whim and this post outlines the parts that stuck.
Hopefully by writing and visualizing my lines of reasoning I’m able to make the connections a bit more accessible
for future fractal fiddlers finding themselves falling face first down the same rabbit hole.</p>

<p>As a final twist, an astute reader may have
noticed none of the visualizations made for this post
matched the fractal on my wall in the post’s thumbnail. The 4th iteration (green) is copied about 27
degrees in the <em>wrong</em> direction:</p>

<p><img src="https://chriskw.xyz/images/fractal/closingwoops.jpg" alt="Closing woops"></p>

<p>In a fun case of “why do I remember this, but not what I ate for dinner 2 days ago,” I
remember my exact reasoning for doing this over a decade ago: I thought the fractal
would unalign itself from the cardinal directions if I always tilted it off the axes in the
same direction. As we saw earlier with \(M\) vs \(M^\prime\) my intuition to compensate for tilt almost made sense, other
than the fact it already corrects itself in alternating steps. I was a bit
embarassed about this until I remembered even Donald Knuth
<a href="https://www.youtube.com/watch?v=v678Em6qyzk">made a wrong turn</a> when putting up a fractal on his wall.
Fools rarely differ!</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MCP explained without hype or fluff (102 pts)]]></title>
            <link>https://blog.nilenso.com/blog/2025/05/12/mcp-explained-without-hype-or-fluff/</link>
            <guid>44063141</guid>
            <pubDate>Thu, 22 May 2025 15:39:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.nilenso.com/blog/2025/05/12/mcp-explained-without-hype-or-fluff/">https://blog.nilenso.com/blog/2025/05/12/mcp-explained-without-hype-or-fluff/</a>, See on <a href="https://news.ycombinator.com/item?id=44063141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Model Context Protocol, like most protocols, solves the M ⨯ N integration problem by turning it into an M + N integration problem.</p>

<p>An AI client application that speaks this protocol does not have to figure out how to fetch data or take actions specific to a platform.</p>

<p><img src="https://blog.nilenso.com/images/blog/mcp.jpg" alt=""></p>

<p>MCP may or may not make your AI smarter, or improve your product, but it will reduce the friction to integrate against other applications that already support MCP. This may or may not be important to you.</p>

<p>The protocol specifies MCP Servers, that generally connect to data sources and expose tools specific to it. Then there are MCP clients, which are a part of AI applications. They can connect to any MCP Server, typically through a configuration that specifies how to connect to or run the server.</p>

<p>The servers, more commonly implemented than clients, may expose:</p>

<ul>
  <li><strong>Tools</strong> that the LLM can call, eg, <code>fetch_file</code> for a filesystem or <code>send_mail</code> for a mail client integration.</li>
  <li><strong>Prompts</strong>, which are reusable templates of instructions or multi-step conversations for the LLM, that are intended to be user-controlled.</li>
  <li><strong>Resources</strong> that are exposed via URIs; it’s up to the client application’s design to decide how these are fetched or used.</li>
  <li><strong>Sampling</strong>, which allows servers to request LLM completions on the client application, which is useful for agentic patterns and running context-aware inference without needing to receive all the contextual data from the client.</li>
</ul>

<p>There are a few more functions and nuances to servers, but these are what broadly stood out to me. Most servers that I have seen or used mostly just expose tool calls.</p>

<h2 id="a-tiny-concrete-example-an-mcp-server-for-open-data-access">A tiny concrete example: an MCP server for Open Data access</h2>

<p>I wrote a tiny MCP server to expose actions to take on CKAN, an open source data management system that’s used by Governments and other organisations to publish open datasets. CKAN has a web interface that links to these tagged datasets, which are usually semi-structured (CSVs, XLS) or totally unstructured (PDF reports and papers).</p>

<p><img src="https://blog.nilenso.com/images/blog/screenshot-2025-05-14-at-15.48.51.png" alt="A view of the CKAN interface"></p>

<p>This is not particularly conducive to discovery and drilling through data. It’s also significant friction to connect dots across datasets. I thought it would be nice to have an AI application that can access all the datasets on CKAN and make sense of it. Open Data is as useful as the insights that can be extracted from it.</p>

<p>One way for me to have approached this is to write an AI application from scratch encoded with knowledge about all the CKAN REST APIs. Unfortunately, this would have “locked in” AI use of CKAN open data sets to just my application. <a href="https://en.wikipedia.org/wiki/Information_wants_to_be_free">And data, especially Open Data wants to be free</a>.</p>

<p>What I really wanted is a well-known “doorknob” that a lot of AI applications and agents in the world would know how to open. This is what MCP servers do. I wrote one in a couple of hours.</p>

<p>I used the official MCP Python SDK and defined some tools. Here’s an excerpt of what that looks like:</p>

<div><pre><code><span>@mcp.tool</span><span>()</span>
<span>async</span> <span>def</span> <span>list_tags</span><span>(</span><span>query</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span> <span>limit</span><span>:</span> <span>int</span> <span>=</span> <span>50</span><span>,</span> <span>ctx</span><span>:</span> <span>Context</span> <span>=</span> <span>None</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
    <span>"""</span><span>List available tags in CKAN.

    Args:
        query: Optional search string to filter tags
        limit: Maximum number of tags to return

    Returns:
        A formatted string containing available tags.
    </span><span>"""</span>
    <span># code to list all the tags used to tag data, via the CKAN API
</span>
<span>@mcp.tool</span><span>()</span>
<span>async</span> <span>def</span> <span>search_datasets</span><span>(</span>
    <span>query</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
    <span>tags</span><span>:</span> <span>Optional</span><span>[</span><span>List</span><span>[</span><span>str</span><span>]]</span> <span>=</span> <span>None</span><span>,</span>
    <span>organization</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
    <span>format</span><span>:</span> <span>Optional</span><span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
    <span>limit</span><span>:</span> <span>int</span> <span>=</span> <span>10</span><span>,</span>
    <span>offset</span><span>:</span> <span>int</span> <span>=</span> <span>0</span><span>,</span>
    <span>ctx</span><span>:</span> <span>Context</span> <span>=</span> <span>None</span>
<span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
    <span>"""</span><span>Search for datasets with various filters.

    Args:
        query: Free text search query
        tags: Filter by tags (list of tag names)
        organization: Filter by organization name
        format: Filter by resource format (e.g., CSV, JSON)
        limit: Maximum number of datasets to return
        offset: Number of datasets to skip

    Returns:
        A formatted string containing matching datasets.
    </span><span>"""</span>
    <span># code to handle searches, using the CKAN API
</span>
<span>@mcp.tool</span><span>()</span>
<span>async</span> <span>def</span> <span>get_resource_details</span><span>(</span><span>resource_id</span><span>:</span> <span>str</span><span>,</span> <span>ctx</span><span>:</span> <span>Context</span> <span>=</span> <span>None</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
    <span>"""</span><span>Get detailed information about a specific resource (file/data).

    Args:
        resource_id: The ID of the resource

    Returns:
        A formatted string containing resource details.
    </span><span>"""</span>
    <span># code to read the details and get the link to a specific resource, using the CKAN API
</span></code></pre></div>

<p>The details of the SDK are better explained in official guides, but the gist of it is that it is an abstraction over JSON-RPC request-response messages that are defined in the protocol. The server I have implemented runs locally, launched as a subprocess by the client app and uses the stdio streams to pass these protocol messages around. Remote MCP servers are a thing as well.</p>

<p>After I wrote this server, I exposed it to the Claude desktop app, which is also an MCP client by editing <code>claude_desktop_config.json</code>. I pointed it to <a href="https://justicehub.in/">JusticeHub</a>, a CKAN instance that contains legal and justice data, created by the folks at <a href="https://civicdatalab.in/">CivicDataLabs</a>.</p>

<div><pre><code><span>{</span><span>
  </span><span>"mcpServers"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"CKAN Server"</span><span>:</span><span> </span><span>{</span><span>
      </span><span>"command"</span><span>:</span><span> </span><span>"/Users/atharva/.local/bin/uv"</span><span>,</span><span>
      </span><span>"args"</span><span>:</span><span> </span><span>[</span><span>
        </span><span>"run"</span><span>,</span><span>
        </span><span>"--with"</span><span>,</span><span>
        </span><span>"httpx"</span><span>,</span><span>
        </span><span>"--with"</span><span>,</span><span>
        </span><span>"mcp[cli]"</span><span>,</span><span>
        </span><span>"mcp"</span><span>,</span><span>
        </span><span>"run"</span><span>,</span><span>
        </span><span>"/Users/atharva/ckan-mcp-server/main.py"</span><span>
      </span><span>],</span><span>
      </span><span>"env"</span><span>:</span><span> </span><span>{</span><span>
        </span><span>"CKAN_URL"</span><span>:</span><span> </span><span>"https://justicehub.in"</span><span>
      </span><span>}</span><span>
    </span><span>}</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>

<p>This allowed me to use this data through Claude.</p>

<p><img src="https://blog.nilenso.com/images/blog/screenshot-2025-05-15-at-16.51.51.png" alt="A screenshot of a conversation with an AI assistant about exploring JusticeHub open data. The assistant lists available datasets, tags, and organizations on the JusticeHub platform. The conversation shows multiple function calls like list_datasets, list_tags, list_organizations, and search_datasets to explore different aspects of the data."></p>

<p><img src="https://blog.nilenso.com/images/blog/screenshot-2025-05-15-at-16.52.08.png" alt=""></p>

<p>Claude discovered my MCP server and gave me a summary of what kind of data was available in JusticeHub.</p>

<p><img src="https://blog.nilenso.com/images/blog/screenshot-2025-05-14-at-14.58.41.png" alt="A continuation of the conversation showing the assistant's comprehensive overview of the JusticeHub open data platform. It describes JusticeHub as a specialized data platform focused on the Indian justice system. The overview includes key dataset categories like Judicial System Performance, Legal Budget and Financial Data, Legal Aid and Access to Justice, and Parliamentary and Legislative Data, with bullet points listing specific datasets under each category."></p>

<p>I was able to take advantage of Claude’s analysis tool to help me visualise the data in an interactive dashboard!</p>

<p><img src="https://blog.nilenso.com/images/blog/screenshot-2025-05-14-at-14.59.44.png" alt="A split-screen view showing a conversation about creating a judicial demographics dashboard on the left and the actual dashboard visualization on the right. The right side displays an &quot;Indian Judiciary Demographics Dashboard&quot; analyzing Supreme Court Justices from 1950-2019, with visualizations showing gender representation (95.6% male, 4.4% female), women's representation over time (a line graph showing gradual increase from the 1970s to 2010s), and a partial view of SC Justices appointed by decade."></p>

<p>I can envision other MCP clients in the future that could make better use of this data, beyond this basic conversational interface and tackle problems such as backlinks and provenance, while providing more structured, opinionated visualisations and analysis.</p>

<h2 id="should-i-build-an-mcp">Should I build “an MCP”?</h2>

<p>It’s worth noting that this is not a mature protocol—<a href="https://modelcontextprotocol.io/development/roadmap">it is continuously evolving</a>. But the adoption has been fantastic—I opened the first random MCP aggregating website and it lists over 4000 servers coming from various organisations and individuals. I’d estimate there’s a lot more out there.</p>

<p>Building against MCP is a clear, well-defined thing to do, something that’s rare in the volatile landscape of AI. This could explain its popularity. But it doesn’t make a good product. It’s another tool in your toolbox.</p>

<p>I (and other folks at nilenso) maintain that good products are built on a foundation that requires software engineering maturity, and this is especially true of AI products.</p>

<p>So let’s revisit what MCP brings to the table:</p>

<ul>
  <li>Turns M ⨯ N integration problem by turning it into an M + N integration problem.</li>
  <li>Decouples AI client applications from AI tools and workflows for a platform.</li>
</ul>

<p>This decoupling is not free of cost. There is extra scaffolding to make your applications talk this protocol. Your LLM performance is sensitive to prompting and tool descriptions. Adding lots of tools indiscriminately affects latencies and overall quality of your responses.</p>

<p>It makes sense for GitHub to expose repository actions for AI tools like Cursor or Windsurf to carry out. This is a valuable form of decoupling.</p>

<p>Does it make sense to have this decoupling for an internal tool, where the clients and servers are under your control, and the value comes from having well-optimised finetuned responses? Probably not.</p>

<p>Anywho, here’s some references. Happy building.</p>

<h2 id="references-for-a-deeper-dive">References, for a deeper dive</h2>

<ul>
  <li><a href="https://modelcontextprotocol.io/introduction">Official Docs</a>: If I have left out a lot of details on the specifics of MCP, it’s because the official docs are pretty solid and far likely to be up-to-date.</li>
  <li><a href="https://www.latent.space/p/why-mcp-won">Why MCP Won</a></li>
  <li><a href="https://github.com/modelcontextprotocol/python-sdk">Python SDK</a></li>
  <li><a href="https://claude.ai/share/e0ffb600-abf1-4f6f-8fd8-6269ba83d73d">Full conversation transcript with Claude using CKAN MCP</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Spy Agencies–One-Stop Shop to Buy Your Personal Data (110 pts)]]></title>
            <link>https://theintercept.com/2025/05/22/intel-agencies-buying-data-portal-privacy/</link>
            <guid>44062586</guid>
            <pubDate>Thu, 22 May 2025 14:43:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2025/05/22/intel-agencies-buying-data-portal-privacy/">https://theintercept.com/2025/05/22/intel-agencies-buying-data-portal-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=44062586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    
<p><span>The ever-growing market</span> for personal data has been a boon for American spy agencies. The U.S. intelligence community is now buying up vast volumes of sensitive information that would have previously required a court order, essentially bypassing the Fourth Amendment. But the surveillance state has encountered a problem: There’s simply too much data on sale from too many corporations and brokers.</p>



<p>So the government has a plan for a one-stop shop.</p>



<p>The Office of the Director of National Intelligence is working on a system to centralize and “streamline” the use of commercially available information, or CAI, like location data derived from mobile ads, by American spy agencies, according to <a href="https://www.documentcloud.org/documents/25949136-1-ic-data-consortium-other-transaction/">contract</a> <a href="https://www.documentcloud.org/documents/25949135-2-appendix-a-building-ic-data-consortium-interface-at-wwwicdatagov/">documents</a> reviewed by The Intercept. The data portal will include information deemed by the ODNI as highly sensitive, that which can be “misused to cause substantial harm, embarrassment, and inconvenience to U.S. persons.” The documents state spy agencies will use the web portal not just to search through reams of private data, but also run them through artificial intelligence tools for further analysis.</p>



<p>Rather than each agency purchasing CAI individually, as has been the case until now, the “Intelligence Community Data Consortium” will provide a single convenient web-based storefront for searching and accessing this data, along with a “data marketplace” for purchasing “the best data at the best price,” faster than ever before, according to the documents. It will be designed for the 18 different federal agencies and offices that make up the U.S. intelligence community, including the National Security Agency, CIA, FBI Intelligence Branch, and Homeland Security’s Office of Intelligence and Analysis — though one document suggests the portal will also be used by agencies not directly related to intelligence or defense.</p>



<p>“In practice, the Data Consortium would provide a one-stop shop for agencies to cheaply purchase access to vast amounts of Americans’ sensitive information from commercial entities, sidestepping constitutional and statutory privacy protections,” said Emile Ayoub, a lawyer with the Brennan Center’s liberty and national security program.</p>



<p>“ODNI is working to streamline a number of inefficient processes, including duplicative contracts to access existing data, and ensuring Americans civil liberties and Fourth Amendment rights are upheld,” ODNI spokesperson Olivia Coleman said in a statement to The Intercept. Coleman did not answer when asked if the new platform would sell access to data on U.S. citizens, or how it would make use of artificial intelligence.</p>



  



<p>Spy agencies and military intelligence offices have for years <a href="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/">freely purchased</a> sensitive personal information rather than obtain it by dint of a judge’s sign-off. Thanks largely to unscrupulous advertisers and <a href="https://theintercept.com/2020/04/09/coronavirus-trump-smartphone-tracking/">app-makers</a> working in a <a href="https://theintercept.com/2021/11/04/treasury-surveillance-location-data-babel-street/">regulatory vacuum</a>, it’s <strong>trivial</strong> to <a href="https://theintercept.com/2022/02/18/location-data-tracking-irs-dhs-digital-envoy/">procure </a>extremely sensitive information about virtually anyone with an online presence. Smartphones in particular leave behind <a href="https://theintercept.com/2023/10/26/cellphone-roaming-location-tracking-surveillance/">immense plumes of data</a>, including detailed records of your movement that can be bought and sold by anyone with an interest. The ODNI has previously defined “sensitive” CAI as information “not widely known about an individual that could be used to cause harm to the person’s reputation, emotional well-being, or physical safety.” Procurement documents reviewed by The Intercept make clear the project is designed to provide access to this highest “sensitive” tier of CAI.</p>



<p>The documents provide a glimpse at some of the many types of CAI available, including “information addressing economic security, supply chain, critical infrastructure protection, great power competition, agricultural data, industrial data, sentiment analysis, and video analytic services.”</p>



<p>While the proliferation of data that can reveal intimate details about virtually anyone has alarmed civil libertarians, privacy advocates, and certain members of Congress, the intelligence community sees another problem: There’s too much data to keep organized, and the disorganized process of buying it is wasting money. To address this overabundance, the ODNI is seeking private sector vendors to build and manage a new “commercial data consortium that unifies commercial data acquisition then enables IC users to access and interact with this commercial data in one place,” according to <strong>one</strong> procurement document obtained by The Intercept.</p>



<p>The ODNI says the platform, the “Intelligence Community (IC) Data Consortium (ICDC),” will help correct the currently “fragmented and decentralized” purchase of commercial data like smartphone location pings, real estate records, biometric data, and social media content. The document laments how often various spy agencies are buying the same data without realizing it. The ODNI says this new platform, which will live at <a href="http://www.icdata.gov/">www.icdata.gov</a>, will “help streamline access to CAI for the entire IC and make it available to mission users in a more cohesive, efficient, and cost-effective manner by avoiding duplicative purchases, preventing sunk costs from unused licenses, and reducing overall data storage and compute costs,” while also incorporating “civil liberties and privacy best practices.”</p>



<figure><blockquote><p>“The IC is still adhering to the ‘just grab all of it, we’ll find something to do with it’ mentality.”</p></blockquote></figure>



<p>While the project’s nod to civil liberties might come as some relief to privacy advocates, the project also represents the extent to which the use of this inherently controversial form of surveillance is here to stay. “Clearly the IC is still adhering to the ‘just grab all of it, we’ll find something to do with it’ mentality rather than being remotely thoughtful about only collecting data it needs or has a specific envisioned use for,” said Calli Schroeder, senior counsel at the Electronic Privacy Information Project.</p>



<!-- BLOCK(cta)[0](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->


<!-- END-BLOCK(cta)[0] -->



<p>Once the website is up and running, the procurement materials say the portal will eventually allow users to analyze the data using large language models, AI-based text tools prone to major factual errors and fabrications. The portal will also facilitate “sentiment analysis,” an often pseudoscientific endeavor purporting to discern one’s opinion about a given topic using implicit signals in their behavior, movement, or speech.</p>



<p>Such analysis is a “huge cause for concern” according to Schroeder. “It means the intelligence community is still, to at least some degree, buying into the false promise of a constantly and continuously debunked practice,” she said. “Let me be clear: Sentiment analysis not only does not work, it cannot work. Its only consistent success has been in perpetuating harmful discrimination (of gender, culture, race, and neurodivergence, among others).”</p>



<p>Whether for sentiment analysis or some other goal, using CAI data sets to query an AI crystal ball poses serious risks, said Ayoub. If such analysis worked as billed, “AI tools make it easier to extract, re-identify, and infer sensitive information about people’s identities, locations, ideologies, and habits — amplifying risks to Americans’ privacy and freedoms of speech and association,” he said. On top of that, “These tools are a black box with little insight into training data, metric, or reliability of outcomes. The IC’s use of these tools typically comes with high risk, questionable track records, and little accountability, especially now that AI policy safeguards were rescinded early in this administration.”</p>



<p>In 2023, the ODNI declassified a 37-page report detailing the vastly expanding use of such CAI data by the U.S. intelligence community, and the threat this poses to the millions of Americans whose lives are cataloged, packaged, and sold by a galaxy of unregulated data brokers. The report, drafted for then-director of national intelligence Avril Haines, included a dire warning to the public: “Today, in a way that far fewer Americans seem to understand, and even fewer of them can avoid, CAI includes information on nearly everyone that is of a type and level of sensitivity that historically could have been obtained, if at all, only through targeted (and predicated) collection, and that could be used to cause harm to an individual’s reputation, emotional well-being, or physical safety.”</p>



  



<p>The extent to which CAI has commodified spy powers previously attainable only by well-resourced governments cannot be overstated: In 2021, for instance, The Intercept reported the existence of Anomaly Six, a startup that <a href="https://theintercept.com/2022/05/04/surveillance-anomaly-six-phone-tracking/">buys geolocational data</a> leaked from smartphones apps. During an Anomaly Six presentation, the company demonstrated its ability to track not only the Chinese navy through the phones of its sailors, but also <a href="https://theintercept.com/2022/04/22/anomaly-six-phone-tracking-zignal-surveillance-cia-nsa/">follow CIA and NSA employees as they commuted to and from work</a>.</p>



<p>The ICDC project reflects a fundamental dissonance within the intelligence community, which acknowledges that CAI is a major threat to the public while refusing to cease buying it. “The government would never have been permitted to compel billions of people to carry location tracking devices on their persons at all times, to log and track most of their social interactions, or to keep flawless records of all their reading habits,” the ODNI wrote in its 2022 report. While conceding “unfettered access to CAI increases its power in ways that may exceed our constitutional traditions or other societal expectations,” the report says, “the IC cannot willingly blind itself to this information.”</p>



<p>In 2024, following the declassified report and the alarm it generated, the ODNI put forth a set of CAI usage rules purporting to establish guardrails against privacy violations and other abuses. The framework earned praise from some corners for requiring the intelligence community to assess the origin and sensitivity of CAI before using it, and for placing more rigorous requirements on agencies that wish to use the most intimate forms of private data. But critics were quick to point out that the ODNI’s rules, which enshrined the intelligence community’s “flexibility to experiment” with CAI, amounted to more self-regulation from a part of the government with a poor track record of self-regulating.</p>



<p>While sensitive CAI comes with more rules — like keeping records of its use, protecting its storage, and some disclosure requirements — these guidelines offer great deal latitude to the intelligence community. The rule about creating a paper trail pertaining to sensitive CAI use, for example, is mandated only “to the extent practicable and consistent with the need to protect intelligence sources and methods,” and can be ignored entirely in “exigent circumstances.” In other words, it’s not really a requirement at all.</p>



<p>Ayoub told The Intercept he worries the ICDC plan will only entrench this self-policing approach. The documents note that vendors would be tasked to some extent with determining whether the data they sell is indeed sensitive, and therefore subject to stricter privacy safeguards, rather than a third party. “Relying on private vendors to determine whether CAI is considered sensitive may increase the risk that the IC purchases known categories of sensitive information without sufficient safeguards for privacy and civil liberties or the warrant, court order, or subpoena they would otherwise need to obtain,” he said.</p>



<p>The portal idea appears to have started under the Biden administration, when it was known as the “Data Co-Op.” It now looks like it will go live during a Trump administration. Elon Musk’s so-called Department of Government Efficiency is already working on building and streamlining access to other large repositories of perilously sensitive information. In March, the Washington Post <a href="https://www.washingtonpost.com/business/2025/04/15/doge-ssa-immigration-trump-housing/">reported</a> that DOGE workers intent on breaking down “information silos” across the federal government were trying to “unify systems into one central hub aims to advance multiple Trump administration priorities, including finding and deporting undocumented immigrants.” The documents note that the portal will also be accessible to so-called “non-Title 50” agencies outside of the national defense and intelligence apparatus.</p>



<!-- BLOCK(newsletter)[1](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[1] -->



<p>Ayoub argued the intelligence community can’t provide access to its upcoming CAI portal without “raising the risk that agencies like DHS’s Homeland Security Investigations (HSI) would access the CAI database to identify and target noncitizens such as student protestors based on their search or browsing histories and location information.”</p>



<p>While the ODNI has acknowledged the importance of transparency, usernames for the portal will not include the name of the analyst’s agency, “thus obscuring any specific participation from individual participants,” according to the project documents.</p>



<p>“The irony is not lost on me that they are making efforts to protect individuals within the IC from being identified regarding their participation in this project but have no qualms about vacuuming up the personal data of Americans against their wishes and knowledge,” said Schroeder.</p>



<p>Sen. Ron Wyden, D-Ore., a longtime critic of the Fourth Amendment end run posed by CAI, expressed concern to The Intercept over how the portal will ultimately be used. “Policies are one thing, but I’m concerned about what the government is actually doing with data about Americans that it buys from data brokers,” he said in a statement. “All indications from news reports and Trump administration officials are that Americans should be extremely worried about how this administration may be using commercial data.”</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fast Allocations in Ruby 3.5 (131 pts)]]></title>
            <link>https://railsatscale.com/2025-05-21-fast-allocations-in-ruby-3-5/</link>
            <guid>44062160</guid>
            <pubDate>Thu, 22 May 2025 14:01:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2025-05-21-fast-allocations-in-ruby-3-5/">https://railsatscale.com/2025-05-21-fast-allocations-in-ruby-3-5/</a>, See on <a href="https://news.ycombinator.com/item?id=44062160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Many Ruby applications allocate objects. What if we could make allocating
objects six times faster?  We can!  Read on to learn more!</p>

<h2 id="speeding-up-allocations-in-ruby">Speeding up allocations in Ruby</h2>

<p>Object allocation in Ruby 3.5 will be much faster than previous versions of Ruby.
I want to start this article with benchmarks and graphs, but if you stick around I’ll also be explaining how we achieved this speedup.</p>

<p>For allocation benchmarks, we’ll compare types of parameters (positional and keyword) with and without YJIT enabled.
We’ll also vary the number of parameters we pass to initialize so that we can see how performance changes as the number of parameters increases.</p>

<p>The full benchmark code can be found expanded below, but it’s basically as follows:</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span># Measure performance as parameters increase</span>
  <span>def</span> <span>initialize</span><span>(</span><span>a1</span><span>,</span> <span>a2</span><span>,</span> <span>aN</span><span>)</span>
  <span>end</span>
<span>end</span>

<span>def</span> <span>test</span>
  <span>i</span> <span>=</span> <span>0</span>
  <span>while</span> <span>i</span> <span>&lt;</span> <span>5_000_000</span>
    <span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>N</span><span>)</span>
    <span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>N</span><span>)</span>
    <span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>N</span><span>)</span>
    <span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>N</span><span>)</span>
    <span>Foo</span><span>.</span><span>new</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>N</span><span>)</span>
    <span>i</span> <span>+=</span> <span>1</span>
  <span>end</span>
<span>end</span>

<span>test</span>
</code></pre></div>

<details>

  <summary>Full Benchmark Code</summary>

  <p>Positional parameters benchmark:</p>

  <div><pre><code><span>N</span> <span>=</span> <span>(</span><span>ARGV</span><span>[</span><span>0</span><span>]</span> <span>||</span> <span>0</span><span>).</span><span>to_i</span>

<span>class</span> <span>Foo</span>
  <span>class_eval</span> <span>&lt;&lt;-</span><span>eorb</span><span>
  def initialize(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
  end
</span><span>  eorb</span>
<span>end</span>

<span>eval</span> <span>&lt;&lt;-</span><span>eorb</span><span>
def test
  i = 0
  while i &lt; 5_000_000
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>_1</span><span>.</span><span>to_s</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>_1</span><span>.</span><span>to_s</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>_1</span><span>.</span><span>to_s</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>_1</span><span>.</span><span>to_s</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>_1</span><span>.</span><span>to_s</span> <span>}</span><span>.join(", ") })
    i += 1
  end
end
</span><span>eorb</span>

<span>test</span>
</code></pre></div>

  <p>Keyword parameters benchmark:</p>

  <div><pre><code><span>N</span> <span>=</span> <span>(</span><span>ARGV</span><span>[</span><span>0</span><span>]</span> <span>||</span> <span>0</span><span>).</span><span>to_i</span>

<span>class</span> <span>Foo</span>
  <span>class_eval</span> <span>&lt;&lt;-</span><span>eorb</span><span>
  def initialize(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>:"</span> <span>}</span><span>.join(", ") })
  end
</span><span>  eorb</span>
<span>end</span>

<span>eval</span> <span>&lt;&lt;-</span><span>eorb</span><span>
def test
  i = 0
  while i &lt; 5_000_000
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>: </span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>: </span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>: </span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>: </span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
    Foo.new(</span><span>#{</span><span>N</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>"a</span><span>#{</span><span>_1</span><span>}</span><span>: </span><span>#{</span><span>_1</span><span>}</span><span>"</span> <span>}</span><span>.join(", ") })
    i += 1
  end
end
</span><span>eorb</span>

<span>test</span>
</code></pre></div>
</details>

<p>We want to measure how long this script will take, but change the number and type of parameters we pass.
To emphasize the cost of object allocation while minimizing the impact of loop execution, the benchmark allocates several objects per iteration.</p>

<p>Running the benchmark code with 0 to 8 parameters, varying parameter type and whether or not YJIT is enabled will produce the following graph:</p>

<p><img src="https://railsatscale.com/2025-05-21-fast-allocations-in-ruby-3-5/graph.png" alt="Benchmark results graph"></p>

<p>The graph illustrates the speedup ratio, calculated by dividing the time spent on Ruby 3.4.2 by that spent on Ruby 3.5.
That means that any values below 1 represent a slowdown, where any values above 1 would represent a speedup.
When we compare Ruby 3.5 to Ruby 3.4.2 we either disable YJIT on both versions or enable YJIT on both versions.
In other words we compare Ruby 3.5 with Ruby 3.4.2 and Ruby 3.5+YJIT with Ruby 3.4.2+YJIT.</p>

<p>The X axis shows the number of parameters passed to initialize, and the Y axis is the speedup ratio.
The blue bars are positional parameters <em>without</em> YJIT, the green bars are positional parameters <em>with</em> YJIT.
The grey bars are keyword parameters <em>without</em> YJIT, and the yellow bars are keyword parameters <em>with</em> YJIT.</p>

<p>First, we can see that all bars are above 1, meaning that every allocation type is faster on Ruby 3.5 than on Ruby 3.4.2.
Positional parameters have a constant speedup ratio regardless of the number of parameters.</p>

<h3 id="positional-parameter-comparison">Positional Parameter Comparison</h3>

<p>For positional parameters the speedup ratio remains constant regardless of the number of parameters.
Without YJIT, Ruby 3.5 is always about 1.8x faster than Ruby 3.4.2.
When we enable YJIT, Ruby 3.5 is always about 2.3x faster.</p>

<h3 id="keyword-parameter-comparison">Keyword Parameter Comparison</h3>

<p>Keyword parameters are a little more interesting.
For both the interpreter and YJIT, as the number of keyword parameters increases, the speedup ratio also increases.
In other words, the more keyword parameters used, the more effective this change is.</p>

<p>With just 3 keyword parameters passed to initialize, Ruby 3.5 is 3x faster than Ruby 3.4.2, and if we enable YJIT it’s over 6.5x faster.</p>

<h2 id="bottlenecks-in-classnew">Bottlenecks in Class#new</h2>

<p>I’ve been interested in speeding up allocations, and thus <code>Class#new</code> for a while.
But what made it slow?</p>

<p><code>Class#new</code> is a very simple method.
All it does is allocate an instance, pass all parameters to <code>initialize</code>, and then return the instance.
If we were to implement <code>Class#new</code> in Ruby, it would look something like this:</p>

<div><pre><code><span>class</span> <span>Class</span>
  <span>def</span> <span>self</span><span>.</span><span>new</span><span>(</span><span>...</span><span>)</span>
    <span>instance</span> <span>=</span> <span>allocate</span>
    <span>instance</span><span>.</span><span>initialize</span><span>(</span><span>...</span><span>)</span>
    <span>instance</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>The implementation has two main parts.
First, it allocates a bare object with <code>allocate</code>,
and second it calls the <code>initialize</code> method, forwarding all parameters <code>new</code> received.
So to speed up this method, we can either speed up object allocation, or speed up calling out to the <code>initialize</code> method.</p>

<p>Speeding up <code>allocate</code> means speeding up the garbage collector, and while there are merits to doing that, I wanted to focus on the runtime side of the equation.
That means trying to decrease the overhead of calling out to another method.
So what makes a method call slow?</p>

<h2 id="calling-ruby-methods-from-ruby">Calling Ruby methods from Ruby</h2>

<p>Ruby’s virtual machine, YARV, uses a stack as a scratch space for processing values.
We can think of this stack as a really large heap allocated array.
Every time we process a YARV instruction, we’ll read or write to this heap allocated array.
This is also true for passing parameters between functions.</p>

<p>When we call a function in Ruby, the caller pushes parameters to the stack before the call is made to the callee.
The callee then reads its parameters from the stack, does any processing it needs, and returns.</p>

<div><pre><code><span>def</span> <span>add</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span>
  <span>a</span> <span>+</span> <span>b</span>
<span>end</span>

<span>def</span> <span>call_add</span>
  <span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
<span>end</span>
</code></pre></div>

<p>For example in the above code, the caller <code>call_add</code> will push the arguments <code>1</code> and <code>2</code> to the stack before calling the <code>add</code> function.
When the <code>add</code> function reads its parameters in order to perform the <code>+</code>, it reads <code>a</code> and <code>b</code> from the stack.
The values pushed by the caller become the parameters for the callee.
You can see this in action in our recent <a href="https://railsatscale.com/2025-05-14-merge-zjit/">post about Launching ZJIT</a>.</p>

<p>This “calling convention” is convenient because the arguments pushed to the stack don’t need to be copied anywhere when they become the parameters to the callee.
If you examine the memory addresses for where 1 and 2 are stored, you’ll see that they are the same addresses used for the values of <code>a</code> and <code>b</code>.</p>

<h2 id="calling-c-methods-from-ruby">Calling C methods from Ruby</h2>

<p>Unfortunately C functions do not use the same calling convention as Ruby functions.
That means when we call a C function from Ruby, or a Ruby function from C, we must convert method parameters to their respective calling convention.</p>

<p>In C, parameters are passed via registers or machine stack.
This means that when we call a C function from Ruby, we need to copy values from the Ruby stack into registers.
Or when we call a Ruby function from C, we must copy register values to the Ruby stack.</p>

<p>This conversion between calling conventions takes some time, so this is a place we can target for optimization.</p>

<p>When calling a C function from Ruby, positional parameters can be directly copied to registers.</p>

<div><pre><code><span>static</span> <span>VALUE</span>
<span>foo</span><span>(</span><span>VALUE</span> <span>a</span><span>,</span> <span>VALUE</span> <span>b</span><span>)</span>
<span>{</span>
  <span>return</span> <span>INT2NUM</span><span>(</span><span>NUM2INT</span><span>(</span><span>a</span><span>)</span> <span>+</span> <span>NUM2INT</span><span>(</span><span>b</span><span>));</span>
<span>}</span>
</code></pre></div>

<div><pre><code><span># calls the `foo` C function</span>
<span>foo</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span>
</code></pre></div>

<p>In the above example, on ARM64, the parameters <code>a</code> and <code>b</code> will be in the X0 and X1 registers respectively.
When we call the <code>foo</code> function from Ruby, the <a href="https://github.com/ruby/ruby/blob/e0545a02503983e8824d0fb5972c15d51093d927/vm_insnhelper.c#L7252-L7362">parameters can be copied directly to the X0 and X1 registers from the Ruby stack</a>.</p>

<p>Unfortunately the conversion isn’t so simple for keyword parameters.
Since C doesn’t support keyword parameters, we have pass the keyword parameters as a hash to the C function.
This means <a href="https://github.com/ruby/ruby/blob/e0545a02503983e8824d0fb5972c15d51093d927/vm_insnhelper.c#L2724-L2730">allocating a new hash, iterating over the parameters, and setting them in the hash</a>.</p>

<p>We can see this in action with the following program when run on Ruby 3.4.2:</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span>def</span> <span>initialize</span><span>(</span><span>a</span><span>:)</span>
  <span>end</span>
<span>end</span>

<span>def</span> <span>measure_allocations</span>
  <span>x</span> <span>=</span> <span>GC</span><span>.</span><span>stat</span><span>(</span><span>:total_allocated_objects</span><span>)</span>
  <span>yield</span>
  <span>GC</span><span>.</span><span>stat</span><span>(</span><span>:total_allocated_objects</span><span>)</span> <span>-</span> <span>x</span>
<span>end</span>

<span>def</span> <span>test</span>
  <span>measure_allocations</span> <span>{</span> <span>Foo</span><span>.</span><span>new</span><span>(</span><span>a: </span><span>1</span><span>)</span> <span>}</span>
<span>end</span>

<span># We need to warm the callsite before measurement because inline caches are Ruby</span>
<span># objects, so they will skew our results</span>
<span>test</span> <span># warmup</span>
<span>test</span> <span># warmup</span>
<span>p</span> <span>test</span>
</code></pre></div>

<p>If we run the above program with Ruby 3.4.2, we’ll see that the <code>test</code> method allocates 2 objects: and instance of <code>Foo</code>, and a hash for passing the keyword parameters to the C implementation of <code>Class#new</code>.</p>

<h2 id="achieving-an-allocation-speedup">Achieving an allocation speedup</h2>

<p>I want to start first with a little bit of history.</p>

<p>I’ve been interested in speeding up allocations for quite some time.
We know that calling a C function from Ruby incurs some overhead, and that the overhead depends on the type of parameters we pass.
So my initial inclination was to rewrite <code>Class#new</code> in Ruby.
Since <code>Class#new</code> just forwards all of its parameters to <code>initialize</code>, it seemed quite natural to use the triple-dot forwarding syntax (<code>...</code>).
You can find remnants of my initial implementation <a href="https://github.com/ruby/ruby/pull/9289">here</a>.
Unfortunately I found that using <code>...</code> was quite expensive because at the time, it was syntactic sugar for <code>*, **, &amp;</code>, and Ruby would allocate extra objects to represent these splat parameters.</p>

<p><a href="https://github.com/ruby/ruby/pull/10510">This lead me to implement an optimization for <code>...</code></a>.
The optimization for <code>...</code> allowed us to use parameter forwarding without allocating any extra objects.
I think this optimization is useful in general, but what I had in mind was using it for <code>Class#new</code>.
Fast forward some months, and I was able to <a href="https://github.com/ruby/ruby/pull/9289/files#diff-919ef5932e2ffb97a00a90eb06036b733c6d26cf69cc13014a3ac174bd351fde">implement <code>Class#new</code> in Ruby with this new optimization</a>.
The initial benchmarks were decent, it eliminated allocations and decreased the cost of passing parameters from <code>new</code> to <code>initialize</code>.
But I was somewhat worried about inline cache misses <a href="https://github.com/ruby/ruby/pull/9289/files#diff-919ef5932e2ffb97a00a90eb06036b733c6d26cf69cc13014a3ac174bd351fdeR7">at this call site</a>.</p>

<p>The <code>Class#new</code> implementation linked to above is a little complex, but if we boil it down, it’s essentially the same as the <code>Class#new</code> implementation we saw at the beginning of the post:</p>

<div><pre><code><span>class</span> <span>Class</span>
  <span>def</span> <span>self</span><span>.</span><span>new</span><span>(</span><span>...</span><span>)</span>
    <span>instance</span> <span>=</span> <span>allocate</span>
    <span>instance</span><span>.</span><span>initialize</span><span>(</span><span>...</span><span>)</span>
    <span>instance</span>
  <span>end</span>
<span>end</span>
</code></pre></div>

<p>The problem with the above code is the inline cache at the <code>initialize</code> call site.
When we make method calls, Ruby will try to cache the destination of that call.
That way we can speed up subsequent calls on the same type at that call site.</p>

<p>CRuby only has a monomorphic inline cache, meaning it can only store one inline cache at any particular call site.
The inline cache is used to help look up the method we will call, and the key to the cache is the class of the receiver (in this case, the class of the <code>instance</code> local variable).
Each time the type of the receiver changes, the cache misses, and we have to do a slow path lookup of the method.</p>

<p>It’s very rare for code to allocate exactly the same type of object many times in a row, so the class of the <code>instance</code> local variable will change quite frequently.
Meaning we could potentially have very poor cache hit rates.
Even if the call site could support multiple cache entries (a “polymorphic” inline cache), the cardinality at this particular call site would be so high that cache hit rates would still be quite poor.</p>

<p>I showed this PR to <a href="https://github.com/ko1">Koichi Sasada</a> (author of YARV), and he suggested that instead of implementing <code>Class#new</code> in Ruby, we add a new YARV instruction and “inline” the implementation of <code>Class#new</code>.
I worked with <a href="https://github.com/jhawthorn">John Hawthorn</a> to implement it and we had a prototype implementation done within a week.
Fortunately (or unfortunately) this prototype turned out to be <em>much</em> faster than a Ruby implementation of <code>Class#new</code>, so I decided to abandon that effort.</p>

<h3 id="inlining-classnew">Inlining <code>Class#new</code></h3>

<p>So what is inlining?
Inlining is pretty much just copy / pasting code from the callee to the caller.</p>



<p>Any time the compiler sees code like the above, instead of generating a simple method call to <code>new</code>, it generates the instructions that <code>new</code> <em>would have used</em> but at the call site of <code>new</code>.</p>

<p>To make this more concrete, lets look at the instructions for the above code before and after inlining.</p>

<p>Here is the bytecode for <code>Foo.new</code> before inlining:</p>

<div><pre><code>&gt; ruby -v --dump=insns -e'Foo.new'
ruby 3.4.2 (2025-02-15 revision d2930f8e7a) +PRISM [arm64-darwin24]
== disasm: #&lt;ISeq:&lt;main&gt;@-e:1 (1,0)-(1,7)&gt;
0000 opt_getconstant_path                   &lt;ic:0 Foo&gt;                (   1)[Li]
0002 opt_send_without_block                 &lt;calldata!mid:new, argc:0, ARGS_SIMPLE&gt;
0004 leave
</code></pre></div>

<p>Here is the bytecode for <code>Foo.new</code> after inlining:</p>

<div><pre><code>&gt; ./ruby -v --dump=insns -e'Foo.new'
ruby 3.5.0dev (2025-04-29T20:36:06Z master b5426826f9) +PRISM [arm64-darwin24]
== disasm: #&lt;ISeq:&lt;main&gt;@-e:1 (1,0)-(1,7)&gt;
0000 opt_getconstant_path                   &lt;ic:0 Foo&gt;                (   1)[Li]
0002 putnil
0003 swap
0004 opt_new                                &lt;calldata!mid:new, argc:0, ARGS_SIMPLE&gt;, 11
0007 opt_send_without_block                 &lt;calldata!mid:initialize, argc:0, FCALL|ARGS_SIMPLE&gt;
0009 jump                                   14
0011 opt_send_without_block                 &lt;calldata!mid:new, argc:0, ARGS_SIMPLE&gt;
0013 swap
0014 pop
0015 leave
</code></pre></div>

<p>Before inlining, the instructions look up the constant <code>Foo</code>, then call the <code>new</code> method.
After inlining, we still look up the constant <code>Foo</code>, but instead of calling the <code>new</code> method, there are a bunch of other instructions.</p>

<p>The most important of these new instructions is the <code>opt_new</code> instruction which allocates a new instance and writes that instance to the stack.
Immediately after the <code>opt_new</code> instruction we see a method call to <code>initialize</code>.
These instructions effectively allocate a new instance and call initialize on that instance, the same thing that <code>Class#new</code> <em>would</em> have done, but without actually calling <code>Class#new</code>.</p>

<p>What’s really nice about this is that any parameters pushed onto the stack <em>are left on the stack</em> for the <code>initialize</code> method to consume.
Where we had to do copies in the C implementation, there are no longer any copies!
Additionally, we no longer push and pop a stack frame for <code>Class#new</code> which further speeds up our code.</p>

<p>Finally, since every call to <code>new</code> includes another call to <code>initialize</code> we have very good cache hit rates compared to the pure Ruby implementation of <code>Class#new</code>.
Rather than <em>one</em> <code>initialize</code> call site, we have an <code>initialize</code> call site at every call to <code>new</code>.</p>

<p>Eliminating a stack frame, eliminating parameter copies, and improving inline cache hits are the major advantages of this optimization.</p>

<h3 id="downsides-to-inlining">Downsides to Inlining</h3>

<p>Of course this optimization is not without downsides.</p>

<p>First, there are more instructions, so it requires more memory usage.
However, this memory increase only grows in proportion to the number of call sites that use <code>new</code>.
We measured this in our monolith and only saw a 0.5% growth in instruction sequence size, which is an even smaller percentage of overall heap size.</p>

<p>Second, this optimization introduces a small backwards incompatibility.
Consider the following code:</p>

<div><pre><code><span>class</span> <span>Foo</span>
  <span>def</span> <span>initialize</span>
    <span>puts</span> <span>caller</span>
  <span>end</span>
<span>end</span>

<span>def</span> <span>test</span>
  <span>Foo</span><span>.</span><span>new</span>
<span>end</span>

<span>test</span>
</code></pre></div>

<p>If we run this code with Ruby 3.4, the output is like this:</p>

<div><pre><code>&gt; ruby -v test.rb
ruby 3.4.2 (2025-02-15 revision d2930f8e7a) +PRISM [arm64-darwin24]
test.rb:8:in 'Class#new'
test.rb:8:in 'Object#test'
test.rb:11:in '&lt;main&gt;'
</code></pre></div>

<p>If we run this code with Ruby 3.5, the output is like this:</p>

<div><pre><code>&gt; ./ruby -v test.rb
ruby 3.5.0dev (2025-04-29T20:36:06Z master b5426826f9) +PRISM [arm64-darwin24]
test.rb:8:in 'Object#test'
test.rb:11:in '&lt;main&gt;'
</code></pre></div>

<p>The <code>Class#new</code> frame is missing from Ruby 3.5 and that is because the frame has been eliminated.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you’ve made it this far, I hope you found the topic interesting.
I’m really excited for Ruby 3.5 to be released later this year, and I hope you are too!
I want to thank Koichi Sasada for suggesting inlining (and the <code>opt_new</code> instruction) and John Hawthorn for helping me with the implementation.</p>

<p>If you’re curious, take a look at the implementation in the <a href="https://github.com/ruby/ruby/pull/13080">pull request</a> and the discussion in the <a href="https://bugs.ruby-lang.org/issues/21254">RedMine ticket</a>.
I didn’t explain every detail of this patch (for example, what happens if you’re calling <code>new</code> on something that isn’t a class?) so if you have questions don’t hesitate to email me or ask on social media.</p>

<p>Have a good day!</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: SQLite JavaScript - extend your database with JavaScript (117 pts)]]></title>
            <link>https://github.com/sqliteai/sqlite-js</link>
            <guid>44061836</guid>
            <pubDate>Thu, 22 May 2025 13:25:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sqliteai/sqlite-js">https://github.com/sqliteai/sqlite-js</a>, See on <a href="https://news.ycombinator.com/item?id=44061836">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SQLite-JS Extension</h2><a id="user-content-sqlite-js-extension" aria-label="Permalink: SQLite-JS Extension" href="#sqlite-js-extension"></a></p>
<p dir="auto">SQLite-JS is a powerful extension that brings JavaScript capabilities to SQLite. With this extension, you can create custom SQLite functions, aggregates, window functions, and collation sequences using JavaScript code, allowing for flexible and powerful data manipulation directly within your SQLite database.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#installation">Installation</a></li>
<li><a href="#functions-overview">Functions Overview</a></li>
<li><a href="#scalar-functions">Scalar Functions</a></li>
<li><a href="#aggregate-functions">Aggregate Functions</a></li>
<li><a href="#window-functions">Window Functions</a></li>
<li><a href="#collation-sequences">Collation Sequences</a></li>
<li><a href="#syncing-across-devices">Sync JavaScript Functions Across Devices</a></li>
<li><a href="#javascript-evaluation">JavaScript Evaluation</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#update-functions">Update Functions</a></li>
<li><a href="#building-from-source">Building from Source</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-built Binaries</h3><a id="user-content-pre-built-binaries" aria-label="Permalink: Pre-built Binaries" href="#pre-built-binaries"></a></p>
<p dir="auto">Download the appropriate pre-built binary for your platform from the official <a href="https://github.com/sqliteai/sqlite-js/releases">Releases</a> page:</p>
<ul dir="auto">
<li>Linux: x86 and ARM</li>
<li>macOS: x86 and ARM</li>
<li>Windows: x86</li>
<li>Android</li>
<li>iOS</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Loading the Extension</h3><a id="user-content-loading-the-extension" aria-label="Permalink: Loading the Extension" href="#loading-the-extension"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- In SQLite CLI
.load ./js

-- In SQL
SELECT load_extension('./js');"><pre><span><span>--</span> In SQLite CLI</span>
.load .<span>/</span>js

<span><span>--</span> In SQL</span>
<span>SELECT</span> load_extension(<span><span>'</span>./js<span>'</span></span>);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Functions Overview</h2><a id="user-content-functions-overview" aria-label="Permalink: Functions Overview" href="#functions-overview"></a></p>
<p dir="auto">SQLite-JS provides several ways to extend SQLite functionality with JavaScript:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Function Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scalar Functions</td>
<td>Process individual rows and return a single value</td>
</tr>
<tr>
<td>Aggregate Functions</td>
<td>Process multiple rows and return a single aggregated result</td>
</tr>
<tr>
<td>Window Functions</td>
<td>Similar to aggregates but can access the full dataset</td>
</tr>
<tr>
<td>Collation Sequences</td>
<td>Define custom sort orders for text values</td>
</tr>
<tr>
<td>JavaScript Evaluation</td>
<td>Directly evaluate JavaScript code within SQLite</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scalar Functions</h2><a id="user-content-scalar-functions" aria-label="Permalink: Scalar Functions" href="#scalar-functions"></a></p>
<p dir="auto">Scalar functions process one row at a time and return a single value. They are useful for data transformation, calculations, text manipulation, etc.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_create_scalar('function_name', 'function_code');"><pre><span>SELECT</span> js_create_scalar(<span><span>'</span>function_name<span>'</span></span>, <span><span>'</span>function_code<span>'</span></span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters" aria-label="Permalink: Parameters" href="#parameters"></a></p>
<ul dir="auto">
<li><strong>function_name</strong>: The name of your custom function</li>
<li><strong>function_code</strong>: JavaScript code that defines your function. Must be in the form <code>function(args) { /* your code here */ }</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a custom function to calculate age from birth date
SELECT js_create_scalar('age', 'function(args) {
  const birthDate = new Date(args[0]);
  const today = new Date();
  let age = today.getFullYear() - birthDate.getFullYear();
  const m = today.getMonth() - birthDate.getMonth();
  if (m < 0 || (m === 0 &amp;&amp; today.getDate() < birthDate.getDate())) {
    age--;
  }
  return age;
}');

-- Use the function
SELECT name, age(birth_date) FROM people;"><pre><span><span>--</span> Create a custom function to calculate age from birth date</span>
<span>SELECT</span> js_create_scalar(<span><span>'</span>age<span>'</span></span>, <span><span>'</span>function(args) {</span>
<span>  const birthDate = new Date(args[0]);</span>
<span>  const today = new Date();</span>
<span>  let age = today.getFullYear() - birthDate.getFullYear();</span>
<span>  const m = today.getMonth() - birthDate.getMonth();</span>
<span>  if (m &lt; 0 || (m === 0 &amp;&amp; today.getDate() &lt; birthDate.getDate())) {</span>
<span>    age--;</span>
<span>  }</span>
<span>  return age;</span>
<span>}<span>'</span></span>);

<span><span>--</span> Use the function</span>
<span>SELECT</span> name, age(birth_date) <span>FROM</span> people;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Aggregate Functions</h2><a id="user-content-aggregate-functions" aria-label="Permalink: Aggregate Functions" href="#aggregate-functions"></a></p>
<p dir="auto">Aggregate functions process multiple rows and compute a single result. Examples include SUM, AVG, and COUNT in standard SQL.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-1" aria-label="Permalink: Usage" href="#usage-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_create_aggregate('function_name', 'init_code', 'step_code', 'final_code');"><pre><span>SELECT</span> js_create_aggregate(<span><span>'</span>function_name<span>'</span></span>, <span><span>'</span>init_code<span>'</span></span>, <span><span>'</span>step_code<span>'</span></span>, <span><span>'</span>final_code<span>'</span></span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters-1" aria-label="Permalink: Parameters" href="#parameters-1"></a></p>
<ul dir="auto">
<li><strong>function_name</strong>: The name of your custom aggregate function</li>
<li><strong>init_code</strong>: JavaScript code that initializes variables for the aggregation</li>
<li><strong>step_code</strong>: JavaScript code that processes each row. Must be in the form <code>function(args) { /* your code here */ }</code></li>
<li><strong>final_code</strong>: JavaScript code that computes the final result. Must be in the form <code>function() { /* your code here */ }</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-1" aria-label="Permalink: Example" href="#example-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a median function
SELECT js_create_aggregate('median', 
  -- Init code: initialize an array to store values
  'values = [];',
  
  -- Step code: collect values from each row
  'function(args) {
    values.push(args[0]);
  }',
  
  -- Final code: calculate the median
  'function() {
    values.sort((a, b) => a - b);
    const mid = Math.floor(values.length / 2);
    if (values.length % 2 === 0) {
      return (values[mid-1] + values[mid]) / 2;
    } else {
      return values[mid];
    }
  }'
);

-- Use the function
SELECT median(salary) FROM employees;"><pre><span><span>--</span> Create a median function</span>
<span>SELECT</span> js_create_aggregate(<span><span>'</span>median<span>'</span></span>, 
  <span><span>--</span> Init code: initialize an array to store values</span>
  <span><span>'</span>values = [];<span>'</span></span>,
  
  <span><span>--</span> Step code: collect values from each row</span>
  <span><span>'</span>function(args) {</span>
<span>    values.push(args[0]);</span>
<span>  }<span>'</span></span>,
  
  <span><span>--</span> Final code: calculate the median</span>
  <span><span>'</span>function() {</span>
<span>    values.sort((a, b) =&gt; a - b);</span>
<span>    const mid = Math.floor(values.length / 2);</span>
<span>    if (values.length % 2 === 0) {</span>
<span>      return (values[mid-1] + values[mid]) / 2;</span>
<span>    } else {</span>
<span>      return values[mid];</span>
<span>    }</span>
<span>  }<span>'</span></span>
);

<span><span>--</span> Use the function</span>
<span>SELECT</span> median(salary) <span>FROM</span> employees;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Window Functions</h2><a id="user-content-window-functions" aria-label="Permalink: Window Functions" href="#window-functions"></a></p>
<p dir="auto">Window functions, like aggregate functions, operate on a set of rows. However, they can access all rows in the current window without collapsing them into a single output row.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-2" aria-label="Permalink: Usage" href="#usage-2"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_create_window('function_name', 'init_code', 'step_code', 'final_code', 'value_code', 'inverse_code');"><pre><span>SELECT</span> js_create_window(<span><span>'</span>function_name<span>'</span></span>, <span><span>'</span>init_code<span>'</span></span>, <span><span>'</span>step_code<span>'</span></span>, <span><span>'</span>final_code<span>'</span></span>, <span><span>'</span>value_code<span>'</span></span>, <span><span>'</span>inverse_code<span>'</span></span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters-2" aria-label="Permalink: Parameters" href="#parameters-2"></a></p>
<ul dir="auto">
<li><strong>function_name</strong>: The name of your custom window function</li>
<li><strong>init_code</strong>: JavaScript code that initializes variables</li>
<li><strong>step_code</strong>: JavaScript code that processes each row. Must be in the form <code>function(args) { /* your code here */ }</code></li>
<li><strong>final_code</strong>: JavaScript code that computes the final result. Must be in the form <code>function() { /* your code here */ }</code></li>
<li><strong>value_code</strong>: JavaScript code that returns the current value. Must be in the form <code>function() { /* your code here */ }</code></li>
<li><strong>inverse_code</strong>: JavaScript code that removes a row from the current window. Must be in the form <code>function(args) { /* your code here */ }</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-2" aria-label="Permalink: Example" href="#example-2"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a moving average window function
SELECT js_create_window('moving_avg',
  -- Init code
  'sum = 0; count = 0;',
  
  -- Step code: process each row
  'function(args) {
    sum += args[0];
    count++;
  }',
  
  -- Final code: not needed for this example
  'function() { }',
  
  -- Value code: return current average
  'function() {
    return count > 0 ? sum / count : null;
  }',
  
  -- Inverse code: remove a value from the window
  'function(args) {
    sum -= args[0];
    count--;
  }'
);

-- Use the function
SELECT id, value, moving_avg(value) OVER (ORDER BY id ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) 
FROM measurements;"><pre><span><span>--</span> Create a moving average window function</span>
<span>SELECT</span> js_create_window(<span><span>'</span>moving_avg<span>'</span></span>,
  <span><span>--</span> Init code</span>
  <span><span>'</span>sum = 0; count = 0;<span>'</span></span>,
  
  <span><span>--</span> Step code: process each row</span>
  <span><span>'</span>function(args) {</span>
<span>    sum += args[0];</span>
<span>    count++;</span>
<span>  }<span>'</span></span>,
  
  <span><span>--</span> Final code: not needed for this example</span>
  <span><span>'</span>function() { }<span>'</span></span>,
  
  <span><span>--</span> Value code: return current average</span>
  <span><span>'</span>function() {</span>
<span>    return count &gt; 0 ? sum / count : null;</span>
<span>  }<span>'</span></span>,
  
  <span><span>--</span> Inverse code: remove a value from the window</span>
  <span><span>'</span>function(args) {</span>
<span>    sum -= args[0];</span>
<span>    count--;</span>
<span>  }<span>'</span></span>
);

<span><span>--</span> Use the function</span>
<span>SELECT</span> id, value, moving_avg(value) OVER (<span>ORDER BY</span> id ROWS BETWEEN <span>2</span> PRECEDING <span>AND</span> CURRENT ROW) 
<span>FROM</span> measurements;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Collation Sequences</h2><a id="user-content-collation-sequences" aria-label="Permalink: Collation Sequences" href="#collation-sequences"></a></p>
<p dir="auto">Collation sequences determine how text values are compared and sorted in SQLite. Custom collations enable advanced sorting capabilities like natural sorting, locale-specific sorting, etc.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-3" aria-label="Permalink: Usage" href="#usage-3"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_create_collation('collation_name', 'collation_function');"><pre><span>SELECT</span> js_create_collation(<span><span>'</span>collation_name<span>'</span></span>, <span><span>'</span>collation_function<span>'</span></span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters-3" aria-label="Permalink: Parameters" href="#parameters-3"></a></p>
<ul dir="auto">
<li><strong>collation_name</strong>: The name of your custom collation</li>
<li><strong>collation_function</strong>: JavaScript code that compares two strings. Must return a negative number if the first string is less than the second, zero if they are equal, or a positive number if the first string is greater than the second.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-3" aria-label="Permalink: Example" href="#example-3"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a case-insensitive natural sort collation
SELECT js_create_collation('natural_nocase', 'function(a, b) {
  // Extract numbers for natural comparison
  const splitA = a.toLowerCase().split(/(\d+)/);
  const splitB = b.toLowerCase().split(/(\d+)/);
  
  for (let i = 0; i < Math.min(splitA.length, splitB.length); i++) {
    if (splitA[i] !== splitB[i]) {
      if (!isNaN(splitA[i]) &amp;&amp; !isNaN(splitB[i])) {
        return parseInt(splitA[i]) - parseInt(splitB[i]);
      }
      return splitA[i].localeCompare(splitB[i]);
    }
  }
  return splitA.length - splitB.length;
}');

-- Use the collation
SELECT * FROM files ORDER BY name COLLATE natural_nocase;"><pre><span><span>--</span> Create a case-insensitive natural sort collation</span>
<span>SELECT</span> js_create_collation(<span><span>'</span>natural_nocase<span>'</span></span>, <span><span>'</span>function(a, b) {</span>
<span>  // Extract numbers for natural comparison</span>
<span>  const splitA = a.toLowerCase().split(/(<span>\d</span>+)/);</span>
<span>  const splitB = b.toLowerCase().split(/(<span>\d</span>+)/);</span>
<span>  </span>
<span>  for (let i = 0; i &lt; Math.min(splitA.length, splitB.length); i++) {</span>
<span>    if (splitA[i] !== splitB[i]) {</span>
<span>      if (!isNaN(splitA[i]) &amp;&amp; !isNaN(splitB[i])) {</span>
<span>        return parseInt(splitA[i]) - parseInt(splitB[i]);</span>
<span>      }</span>
<span>      return splitA[i].localeCompare(splitB[i]);</span>
<span>    }</span>
<span>  }</span>
<span>  return splitA.length - splitB.length;</span>
<span>}<span>'</span></span>);

<span><span>--</span> Use the collation</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> files <span>ORDER BY</span> name COLLATE natural_nocase;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Syncing Across Devices</h2><a id="user-content-syncing-across-devices" aria-label="Permalink: Syncing Across Devices" href="#syncing-across-devices"></a></p>
<p dir="auto">When used with <a href="https://github.com/sqliteai/sqlite-sync/">sqlite-sync</a>, user-defined functions created via sqlite-js are automatically replicated across the SQLite Cloud cluster, ensuring that all connected peers share the same logic and behavior — even offline. To enable automatic persistence and sync the special <code>js_init_table</code> function must be executed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-4" aria-label="Permalink: Usage" href="#usage-4"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_init_table();         -- Create table if needed (no loading)
SELECT js_init_table(1);        -- Create table and load all stored functions"><pre><span>SELECT</span> js_init_table();         <span><span>--</span> Create table if needed (no loading)</span>
<span>SELECT</span> js_init_table(<span>1</span>);        <span><span>--</span> Create table and load all stored functions</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">JavaScript Evaluation</h2><a id="user-content-javascript-evaluation" aria-label="Permalink: JavaScript Evaluation" href="#javascript-evaluation"></a></p>
<p dir="auto">The extension also provides a way to directly evaluate JavaScript code within SQLite queries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage</h3><a id="user-content-usage-5" aria-label="Permalink: Usage" href="#usage-5"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT js_eval('javascript_code');"><pre><span>SELECT</span> js_eval(<span><span>'</span>javascript_code<span>'</span></span>);</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Parameters</h3><a id="user-content-parameters-4" aria-label="Permalink: Parameters" href="#parameters-4"></a></p>
<ul dir="auto">
<li><strong>javascript_code</strong>: Any valid JavaScript code to evaluate</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example-4" aria-label="Permalink: Example" href="#example-4"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Perform a calculation
SELECT js_eval('Math.PI * Math.pow(5, 2)');

-- Format a date
SELECT js_eval('new Date(1629381600000).toLocaleDateString()');"><pre><span><span>--</span> Perform a calculation</span>
<span>SELECT</span> js_eval(<span><span>'</span>Math.PI * Math.pow(5, 2)<span>'</span></span>);

<span><span>--</span> Format a date</span>
<span>SELECT</span> js_eval(<span><span>'</span>new Date(1629381600000).toLocaleDateString()<span>'</span></span>);</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 1: String Manipulation</h3><a id="user-content-example-1-string-manipulation" aria-label="Permalink: Example 1: String Manipulation" href="#example-1-string-manipulation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a function to extract domain from email
SELECT js_create_scalar('get_domain', 'function(args) {
  const email = args[0];
  return email.split(&quot;@&quot;)[1] || null;
}');

-- Use it in a query
SELECT email, get_domain(email) AS domain FROM users;"><pre><span><span>--</span> Create a function to extract domain from email</span>
<span>SELECT</span> js_create_scalar(<span><span>'</span>get_domain<span>'</span></span>, <span><span>'</span>function(args) {</span>
<span>  const email = args[0];</span>
<span>  return email.split("@")[1] || null;</span>
<span>}<span>'</span></span>);

<span><span>--</span> Use it in a query</span>
<span>SELECT</span> email, get_domain(email) <span>AS</span> domain <span>FROM</span> users;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 2: Statistical Aggregation</h3><a id="user-content-example-2-statistical-aggregation" aria-label="Permalink: Example 2: Statistical Aggregation" href="#example-2-statistical-aggregation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a function to calculate standard deviation
SELECT js_create_aggregate('stddev',
  'sum = 0; sumSq = 0; count = 0;',
  
  'function(args) {
    const val = args[0];
    sum += val;
    sumSq += val * val;
    count++;
  }',
  
  'function() {
    if (count < 2) return null;
    const variance = (sumSq - (sum * sum) / count) / (count - 1);
    return Math.sqrt(variance);
  }'
);

-- Use it in a query
SELECT department, stddev(salary) FROM employees GROUP BY department;"><pre><span><span>--</span> Create a function to calculate standard deviation</span>
<span>SELECT</span> js_create_aggregate(<span><span>'</span>stddev<span>'</span></span>,
  <span><span>'</span>sum = 0; sumSq = 0; count = 0;<span>'</span></span>,
  
  <span><span>'</span>function(args) {</span>
<span>    const val = args[0];</span>
<span>    sum += val;</span>
<span>    sumSq += val * val;</span>
<span>    count++;</span>
<span>  }<span>'</span></span>,
  
  <span><span>'</span>function() {</span>
<span>    if (count &lt; 2) return null;</span>
<span>    const variance = (sumSq - (sum * sum) / count) / (count - 1);</span>
<span>    return Math.sqrt(variance);</span>
<span>  }<span>'</span></span>
);

<span><span>--</span> Use it in a query</span>
<span>SELECT</span> department, stddev(salary) <span>FROM</span> employees <span>GROUP BY</span> department;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example 3: Custom Window Function</h3><a id="user-content-example-3-custom-window-function" aria-label="Permalink: Example 3: Custom Window Function" href="#example-3-custom-window-function"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Create a window function to calculate percentile within a window
SELECT js_create_window('percentile_rank',
  'values = [];',
  
  'function(args) {
    values.push(args[0]);
  }',
  
  'function() {
    values.sort((a, b) => a - b);
  }',
  
  'function() {
    const current = values[values.length - 1];
    const rank = values.indexOf(current);
    return (rank / (values.length - 1)) * 100;
  }',
  
  'function(args) {
    const index = values.indexOf(args[0]);
    if (index !== -1) {
      values.splice(index, 1);
    }
  }'
);

-- Use it in a query
SELECT name, score, 
       percentile_rank(score) OVER (ORDER BY score) 
FROM exam_results;"><pre><span><span>--</span> Create a window function to calculate percentile within a window</span>
<span>SELECT</span> js_create_window(<span><span>'</span>percentile_rank<span>'</span></span>,
  <span><span>'</span>values = [];<span>'</span></span>,
  
  <span><span>'</span>function(args) {</span>
<span>    values.push(args[0]);</span>
<span>  }<span>'</span></span>,
  
  <span><span>'</span>function() {</span>
<span>    values.sort((a, b) =&gt; a - b);</span>
<span>  }<span>'</span></span>,
  
  <span><span>'</span>function() {</span>
<span>    const current = values[values.length - 1];</span>
<span>    const rank = values.indexOf(current);</span>
<span>    return (rank / (values.length - 1)) * 100;</span>
<span>  }<span>'</span></span>,
  
  <span><span>'</span>function(args) {</span>
<span>    const index = values.indexOf(args[0]);</span>
<span>    if (index !== -1) {</span>
<span>      values.splice(index, 1);</span>
<span>    }</span>
<span>  }<span>'</span></span>
);

<span><span>--</span> Use it in a query</span>
<span>SELECT</span> name, score, 
       percentile_rank(score) OVER (<span>ORDER BY</span> score) 
<span>FROM</span> exam_results;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Update Functions</h2><a id="user-content-update-functions" aria-label="Permalink: Update Functions" href="#update-functions"></a></p>
<p dir="auto">Due to a constraint in <a href="https://www3.sqlite.org/src/info/cabab62bc10568d4" rel="nofollow">SQLite</a>, it is not possible to update or redefine a user-defined function using the same database connection that was used to initially register it. To modify an existing JavaScript function, the update must be performed through a separate database connection.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building from Source</h2><a id="user-content-building-from-source" aria-label="Permalink: Building from Source" href="#building-from-source"></a></p>
<p dir="auto">See the included Makefile for building instructions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build for your current platform
make

# Build for a specific platform
make PLATFORM=macos
make PLATFORM=linux
make PLATFORM=windows

# Install
make install"><pre><span><span>#</span> Build for your current platform</span>
make

<span><span>#</span> Build for a specific platform</span>
make PLATFORM=macos
make PLATFORM=linux
make PLATFORM=windows

<span><span>#</span> Install</span>
make install</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the LICENSE file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making the rav1d Video Decoder 1% Faster (224 pts)]]></title>
            <link>https://ohadravid.github.io/posts/2025-05-rav1d-faster/</link>
            <guid>44061160</guid>
            <pubDate>Thu, 22 May 2025 11:59:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ohadravid.github.io/posts/2025-05-rav1d-faster/">https://ohadravid.github.io/posts/2025-05-rav1d-faster/</a>, See on <a href="https://news.ycombinator.com/item?id=44061160">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>*on macOS with an M3 chip</em><br><em>*slightly more than 1%, on a specific benchmark, without any new unsafe code</em></p><p>A while ago, <a href="https://www.memorysafety.org/blog/rav1d-perf-bounty/">memorysafety.org announced a contest</a> for improving performance of <code>rav1d</code>, a Rust port of the <code>dav1d</code> AV1 decoder.</p><p>As this literally has my name written on it, I thought it would be fun to give it a try (even though I <em>probably</em> can’t participate in the contest).</p><p>This is a write-up about two small performance improvements I found (<a href="https://github.com/memorysafety/rav1d/pull/1397">1st PR</a>, <a href="https://github.com/memorysafety/rav1d/pull/1400">2nd PR</a>) and how I found them (you can also jump to the <a href="#summary">summary in the end</a>).</p><p><img src="https://ohadravid.github.io/2025-05-rav1d-faster/drakeposting.webp" alt="drakeposting meme - working on rav1d because there’s a contest with money, working on rav1d because my last name is Ravid"></p><h2 id="background-and-approach">Background and Approach</h2><p><a href="https://github.com/memorysafety/rav1d"><code>rav1d</code></a> is a port of <a href="https://code.videolan.org/videolan/dav1d"><code>dav1d</code></a>, created by (1) running <a href="https://github.com/immunant/c2rust"><code>c2rust</code></a> on <code>dav1d</code>, (2) incorporating <code>dav1d</code>’s asm-optimized functions, and (3) changing the code to be more Rust-y and safer.</p><p>The authors also published <a href="https://www.memorysafety.org/blog/rav1d-performance-optimization/">a detailed article</a> about the process and the performance work they did.</p><p>More recently, the contest was announced, with the baseline being:</p><blockquote><p>Our Rust-based rav1d decoder is currently about 5% slower than the C-based dav1d decoder.</p></blockquote><p>Video decoders are notoriously complex pieces of software, but because we are comparing the performance of two similar deterministic binaries we might be able to avoid a lot of that complexity - with the right tooling.</p><p>We can’t expect to find huge wins, and some regressions might be too-hard-to-tackle (for example, LLVM finding a Rust function harder to optimize than the C version),
but it’s worth a shot, especially since aarch64 (my environment) is probably less optimized than x86_64.</p><p>My approach here was to:</p><ol><li>Use a sampling profiler to capture snapshots of both runs on the same input.</li><li>Use the optimized asm calls as “anchors” since they should match perfectly.</li><li>Compare the Rust and C versions function by function, and if there’s a big enough discrepancy, dive into that function.</li></ol><h2 id="baseline">Baseline</h2><p>First things first, we need to build and compare perf locally (using <code>hyperfine</code> and the sample files noted in the contest’s rules and <code>rav1d</code>’s <a href="https://github.com/memorysafety/rav1d/blob/main/.github/workflows/build-and-benchmark-x86.yml">CI</a>).</p><p>We’ll be using the single threaded version (<code>--threads 1</code>) to keep things simple.</p><p>For <code>rav1d</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ git clone git@github.com:memorysafety/rav1d.git <span>&amp;&amp;</span> <span>cd</span> rav1d <span>&amp;&amp;</span> git log -n1
</span></span><span><span>commit a654c1e82adb2d9a33ae50d2a82a7a747102cbb6
</span></span><span><span>$ rustc --version --verbose <span># set by rust-toolchain.toml</span>
</span></span><span><span>rustc 1.88.0-nightly <span>(</span>b45dd71d1 2025-04-30<span>)</span>
</span></span><span><span>...
</span></span><span><span>LLVM version: 20.1.2
</span></span><span><span>$ cargo build --release
</span></span><span><span>    Finished <span>`</span>release<span>`</span> profile <span>[</span>optimized<span>]</span> target<span>(</span>s<span>)</span> in ..
</span></span><span><span>$ hyperfine --warmup <span>2</span> <span>"target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1"</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ± σ<span>)</span>:     73.914 s ±  0.151 s    <span>[</span>User: 73.295 s, System: 0.279 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min … max<span>)</span>:   73.770 s … 74.132 s    <span>10</span> runs
</span></span></code></pre></div><p>For <code>dav1d</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ git clone https://code.videolan.org/videolan/dav1d.git <span>&amp;&amp;</span> <span>cd</span> dav1d <span>&amp;&amp;</span> git checkout 1.5.1
</span></span><span><span>$ brew install llvm@20 <span>&amp;&amp;</span> <span>export</span> CC<span>=</span>clang<span>;</span> $CC --version
</span></span><span><span>Homebrew clang version 20.1.4
</span></span><span><span>$ meson setup build <span>"-Dbitdepths=['8','16']"</span>
</span></span><span><span>$ bear -- ninja -C build tools/dav1d
</span></span><span><span>...
</span></span><span><span><span>[</span>88/88<span>]</span> Linking target tools/dav1d
</span></span><span><span>$ hyperfine --warmup <span>2</span> <span>"build/tools/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1"</span>
</span></span><span><span>Benchmark 1: build/tools/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ± σ<span>)</span>:     67.912 s ±  0.541 s    <span>[</span>User: 67.208 s, System: 0.282 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min … max<span>)</span>:   66.933 s … 68.948 s    <span>10</span> runs
</span></span></code></pre></div><p>So <code>rav1d</code> is about 9% (6 seconds) slower than <code>dav1d</code> for that sample file, at least on an M3 chip.</p><p>(Ideally, <code>clang</code> and <code>rustc</code> should use the same LLVM version, but a patch version difference is probably fine.)<br>(Measured on a MacBook Air M3 with 8 cores.)</p><h2 id="profiling">Profiling</h2><p>I used <a href="https://github.com/mstange/samply">samply</a> which is my current go-to sampling profiler:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>./dav1d $ sudo samply record ./build/tools/dav1d -q -i /Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>./rav1d $ sudo samply record ./target/release/dav1d -q -i /Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span></code></pre></div><p>(The Rust binary is also called <code>dav1d</code>, which is a bit confusing.)</p><p>By default, <code>samply</code> uses a rate of 1000Hz, which means that (for example) any diff of 500 samples in a function will account for about 0.5 second of runtime difference.</p><p>Usually, starting with the “inverted stack” view helps to narrow down interesting options (which we’ll explore in <a href="#profiling-again-but-inverted">the next section</a>),
but this time we want to focus on the anchors we know should match: the asm functions.</p><p>You can view the full profiler snapshots online in the Firefox Profiler (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Fdav1d_profile.json.gz/calltree/?assemblyView=2~11c50~174~dav1d_filter_sbrow_cdef_8bpc&amp;globalTrackOrder=0&amp;search=dav1d_cdef_brow_8bpc&amp;thread=0&amp;v=10">dav1d</a>, <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_baseline.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10">rav1d</a>),
but here are the relevant, filtered, clippings (<em>Note: these are not interactive. Check out the links if you want to explore more</em>).</p><p>First, here’s the <code>dav1d</code> (C) version (total number of samples: ~69,500):</p><p>Next, here’s the <code>rav1d</code> (Rust) version (total number of samples: ~75,150):</p><p>Look at the highlighted functions, <code>dav1d_cdef_brow_8bpc</code> and <code>rav1d_cdef_brow</code>.<br>The <em>Total</em> sample count is the number of samples where this function was seen “anywhere in the stack” which means it includes any “children” functions called by it.
The <em>Self</em> sample count is the number of samples in which this was the executing function, so it doesn’t include the children’s sample counts.</p><p>There is a slight divergence between <code>dav1d</code> and <code>rav1d</code>: while the <code>_neon</code> extension notes the Arm-specific assembly functions that are shared between the two binaries, we see that:</p><ol><li><code>dav1d</code> calls <code>cdef_filter_8x8_neon</code> and <code>cdef_filter_4x4_neon</code>, and each of them dispatches the relevant assembly functions (either the <code>8</code> or the <code>4</code> version, respectively).</li><li><code>rav1d</code> calls <code>cdef_filter_neon_erased</code>, which handles the dispatch of <em>all</em> the assembly functions.</li></ol><p>We can also see that <code>cdef_filter8_pri_sec_edged_8bpc_neon</code> has almost identical sample counts in both snapshots, which means we are on the right track.</p><p>Let’s ignore the <code>cdef_filter4_pri_edged_8bpc_neon</code> function which <em>doesn’t match</em>, at least for now (<em>foreshadowing a possible part 2 in the series</em>).</p><p>This means that (A) the <em>Self</em> sample count for <code>dav1d_cdef_brow_8bpc</code> should match <code>rav1d_cdef_brow</code>,
<strong>and</strong> (B) that summing both <code>cdef_filter_{8x8,4x4}_neon</code> <em>Self</em> sample counts should match <code>cdef_filter_neon_erased</code> <em>Self</em> sample count.</p><p>Now we see something interesting: focusing in the second part, the summed <em>Self</em> sample count of <code>cdef_filter_{8x8,4x4}_neon</code> is about 400 samples, while <code>rav1d</code>’s <code>cdef_filter_neon_erased</code> is almost 670 samples. We can also see that <code>dav1d_cdef_brow_8bpc</code> is 1790 samples, vs <code>rav1d_cdef_brow</code>’s 2350 samples.</p><p>Together, this difference accounts for about 1% of the total runtime of <code>rav1d</code>!</p><p>Jumping to the <code>cdef_filter_neon_erased</code> implementation, except for a bunch of pointer casting using <code>.cast()</code>,
there’s only one “big thing” going on that’s not part of the call-to-asm machinery:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[deny(unsafe_op_in_unsafe_fn)]</span>
</span></span><span><span><span>pub</span> <span>unsafe</span> <span>extern</span> <span>"C"</span> <span>fn</span> <span>cdef_filter_neon_erased</span><span>&lt;</span>
</span></span><span><span>    <span>BD</span>: <span>BitDepth</span><span>,</span>
</span></span><span><span>    <span>const</span> W: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> H: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> <span>TMP_STRIDE</span>: <span>usize</span><span>,</span>
</span></span><span><span>    <span>const</span> <span>TMP_LEN</span>: <span>usize</span><span>,</span>
</span></span><span><span><span>&gt;</span><span>(</span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span><span>)</span> <span>{</span>
</span></span><span><span>    <span>use</span> <span>crate</span>::src::align::Align16<span>;</span>
</span></span><span><span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>let</span> <span>mut</span> tmp_buf <span>=</span> Align16<span>([</span><span>0</span><span>u16</span><span>;</span> <span>TMP_LEN</span><span>]);</span>
</span></span><span><span>    <span>let</span> tmp <span>=</span> <span>&amp;</span><span>mut</span> tmp_buf<span>.</span><span>0</span><span>[</span><span>2</span> <span>*</span> <span>TMP_STRIDE</span> <span>+</span> <span>8</span><span>..</span><span>];</span>
</span></span><span><span>    
</span></span><span><span>    padding::<span>Fn</span>::neon::<span>&lt;</span><span>BD</span><span>,</span> W<span>&gt;</span><span>().</span>call::<span>&lt;</span><span>BD</span><span>&gt;</span><span>(</span>tmp<span>,</span> dst<span>,</span> stride<span>,</span> left<span>,</span> top<span>,</span> bottom<span>,</span> H<span>,</span> edges<span>);</span>
</span></span><span><span>    filter::<span>Fn</span>::neon::<span>&lt;</span><span>BD</span><span>,</span> W<span>&gt;</span><span>().</span>call<span>(</span>dst<span>,</span> stride<span>,</span> tmp<span>,</span> pri_strength<span>,</span> sec_strength<span>,</span> dir<span>,</span> damping<span>,</span> H<span>,</span> edges<span>,</span> bd<span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>With <code>TMP_LEN</code> being <code>12 * 16 + 8 = 200</code> or <code>12 * 8 + 8 = 104</code>, so <code>tmp_buf = [u16; 200]</code> in the worst case.
That’s a lot of memory to zero for a scratch buffer!</p><p>What does <code>dav1d</code> do here?</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>#define DEFINE_FILTER(w, h, tmp_stride)                                      \
</span></span></span><span><span><span>static void                                                                  \
</span></span></span><span><span><span>cdef_filter_##w##x##h##_neon(</span><span>/* .. snip .. */</span><span>)                               \
</span></span></span><span><span><span>{                                                                            \
</span></span></span><span><span><span>    ALIGN_STK_16(uint16_t, tmp_buf, 12 * tmp_stride + 8,);                   \
</span></span></span><span><span><span>    uint16_t *tmp = tmp_buf + 2 * tmp_stride + 8;                            \
</span></span></span><span><span><span>    BF(dav1d_cdef_padding##w, neon)(tmp, dst, stride,                        \
</span></span></span><span><span><span>                                    left, top, bottom, h, edges);            \
</span></span></span><span><span><span>    BF(dav1d_cdef_filter##w, neon)(dst, stride, tmp, pri_strength,           \
</span></span></span><span><span><span>                                   sec_strength, dir, damping, h, edges      \
</span></span></span><span><span><span>                                   HIGHBD_TAIL_SUFFIX);                      \
</span></span></span><span><span><span>}
</span></span></span><span><span><span></span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>8</span><span>,</span> <span>8</span><span>,</span> <span>16</span><span>)</span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>4</span><span>,</span> <span>8</span><span>,</span> <span>8</span><span>)</span>
</span></span><span><span><span>DEFINE_FILTER</span><span>(</span><span>4</span><span>,</span> <span>4</span><span>,</span> <span>8</span><span>)</span>
</span></span></code></pre></div><p>A few macro expansions later, we get <code>uint16_t tmp_buf[200] __attribute__((aligned(16)));</code></p><p>This means that <code>tmp_buf</code> isn’t initialized by the <code>cdef_filter_{8x8,4x4}_neon</code> functions:
instead, it is used as a write destination for the <code>padding</code> assembly function,
and later by the <code>filter</code> assembly function as-is.
It seems likely that the compiler doesn’t know this initialization can be eliminated,<br>and we can also use <code>--emit=llvm-ir</code> to see it more even directly:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ RUSTFLAGS<span>=</span><span>"--emit=llvm-ir"</span> cargo build --release --target aarch64-apple-darwin
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="llvm"><span><span><span>; rav1d::src::cdef::neon::cdef_filter_neon_erased
</span></span></span><span><span><span>; Function Attrs: nounwind
</span></span></span><span><span><span></span><span>define</span> <span>internal</span> <span>void</span> @_ZN5rav1d3src4cdef4neon23cdef_filter_neon_erased17h7e4dbe8ecff68724E<span>(</span><span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %left<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %top<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %bottom<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %pri_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %sec_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %dir<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %damping<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %edges<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %bitdepth_max<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_dst<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_top<span>,</span> <span>ptr</span> <span>nocapture</span> <span>readnone</span> %_bottom<span>)</span> <span>unnamed_addr</span> #1 <span>{</span>
</span></span><span><span><span>start:</span>
</span></span><span><span>  %tmp_buf <span>=</span> <span>alloca</span> <span>[</span><span>400</span> <span>x</span> <span>i8</span><span>],</span> <span>align</span> <span>16</span>
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.lifetime.start.p0<span>(</span><span>i64</span> <span>400</span><span>,</span> <span>ptr</span> <span>nonnull</span> %tmp_buf<span>)</span>
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.memset.p0.i64<span>(</span><span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> <span>align</span> <span>16</span> <span>dereferenceable</span><span>(</span><span>400</span><span>)</span> %tmp_buf<span>,</span> <span>i8</span> <span>0</span><span>,</span> <span>i64</span> <span>400</span><span>,</span> <span>i1</span> <span>false</span><span>)</span>
</span></span><span><span>  %_37 <span>=</span> <span>getelementptr</span> <span>inbounds</span> <span>nuw</span> <span>i8</span><span>,</span> <span>ptr</span> %tmp_buf<span>,</span> <span>i64</span> <span>80</span>
</span></span><span><span>  <span>call</span> <span>void</span> @dav1d_cdef_padding8_16bpc_neon<span>(</span><span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> %_37<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %left<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %top<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> %bottom<span>,</span> <span>i32</span> <span>no</span><span>undef</span> <span>8</span><span>,</span> <span>i32</span> <span>no</span><span>undef</span> %edges<span>)</span> #121
</span></span><span><span>  %edges2.i <span>=</span> <span>zext</span> <span>i32</span> %edges <span>to</span> <span>i64</span>
</span></span><span><span>  %_0.i.i.i.i <span>=</span> <span>and</span> <span>i32</span> %bitdepth_max<span>,</span> <span>65535</span>
</span></span><span><span>  <span>call</span> <span>void</span> @dav1d_cdef_filter8_16bpc_neon<span>(</span><span>ptr</span> <span>no</span><span>undef</span> %dst<span>,</span> <span>i64</span> <span>no</span><span>undef</span> %stride<span>,</span> <span>ptr</span> <span>no</span><span>undef</span> <span>nonnull</span> <span>readonly</span> <span>align</span> <span>2</span> %_37<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %pri_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %sec_strength<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %dir<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %damping<span>,</span> <span>i32</span> <span>no</span><span>undef</span> <span>8</span><span>,</span> <span>i64</span> <span>no</span><span>undef</span> %edges2.i<span>,</span> <span>i32</span> <span>no</span><span>undef</span> %_0.i.i.i.i<span>)</span> #121
</span></span><span><span>  <span>call</span> <span>void</span> @llvm.lifetime.end.p0<span>(</span><span>i64</span> <span>400</span><span>,</span> <span>ptr</span> <span>nonnull</span> %tmp_buf<span>)</span>
</span></span><span><span>  <span>ret</span> <span>void</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h3 id="avoid-needlessly-zeroing-buffers-with-maybeuninit">Avoid Needlessly Zeroing Buffers with <code>MaybeUninit</code></h3><p>This should be pretty easy actually! Rust has <a href="https://doc.rust-lang.org/std/mem/union.MaybeUninit.html"><code>std::mem::MaybeUninit</code></a> for just such an occasion:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>-let mut tmp_buf = Align16([0u16; TMP_LEN])
</span></span></span><span><span><span></span><span>+let mut tmp_buf = Align16([MaybeUninit::&lt;u16&gt;::uninit(); TMP_LEN]);
</span></span></span></code></pre></div><p>We can still take a sub-slice safely (<code>&amp;mut tmp_buf.0[2 * TMP_STRIDE + 8..]</code>), but we will need to update the signatures of the inner functions to use the new type (<code>tmp: *mut MaybeUninit&lt;u16&gt;</code>, <code>tmp: &amp;[MaybeUninit&lt;u16&gt;]</code>).</p><p>Since the code that used these was unsafe anyway, we don’t need to add any new unsafe blocks - only to verify that the existing code hasn’t changed (w.r.t <code>dav1d</code>) to rely on this buffer being zeroed.</p><p>Before, <code>cdef_filter_neon_erased</code> had 670 <em>Self</em> samples. Re-running the profiler, we get <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_after.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10">a new snapshot</a>:</p><p>Just 274 samples! Slightly less than <code>dav1d</code>’s <code>cdef_filter_{8x8,4x4}_neon</code> <em>Self</em> sample count.</p><p>Maybe this isn’t the only place where time is wasted zeroing buffers? A quick search for other big <code>Align16</code> buffers resulted in this lucky find:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span>(</span><span>crate</span><span>)</span> <span>fn</span> <span>rav1d_cdef_brow</span><span>&lt;</span><span>BD</span>: <span>BitDepth</span><span>&gt;</span><span>(</span><span>/* .. snip ..*/</span><span>)</span>
</span></span><span><span><span>{</span>
</span></span><span><span>    <span>// .. snip ..
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>for</span> by <span>in</span> <span>(</span>by_start<span>..</span>by_end<span>).</span>step_by<span>(</span><span>2</span><span>)</span> <span>{</span>
</span></span><span><span>        <span>// .. snip ..
</span></span></span><span><span><span></span>        <span>let</span> <span>mut</span> lr_bak <span>=</span>
</span></span><span><span>            Align16<span>([[[[</span><span>0.</span>into<span>();</span> <span>2</span> <span>/* x */</span><span>];</span> <span>8</span> <span>/* y */</span><span>];</span> <span>3</span> <span>/* plane */</span> <span>];</span> <span>2</span> <span>/* idx */</span><span>]);</span>
</span></span><span><span>        
</span></span><span><span>        <span>// .. snip ..
</span></span></span><span><span><span></span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Again, the matching code from <code>dav1d</code> doesn’t initialize this buffer.
Here, switching to <code>MaybeUninit</code> is more difficult, but we can still offer a modest improvement: we’ll only need to do the initialization <strong>once</strong> if we hoist <code>lr_bak</code> to the top level!</p><div><pre tabindex="0"><code data-lang="diff"><span><span>pub(crate) fn rav1d_cdef_brow&lt;BD: BitDepth&gt;(/* .. snip ..*/)
</span></span><span><span>{
</span></span><span><span>    // .. snip ..
</span></span><span><span><span>+   let mut lr_bak =
</span></span></span><span><span><span>+       Align16([[[[0.into(); 2 /* x */]; 8 /* y */]; 3 /* plane */ ]; 2 /* idx */]);
</span></span></span><span><span><span></span>        
</span></span><span><span>    for by in (by_start..by_end).step_by(2) {
</span></span><span><span>        // .. snip ..
</span></span><span><span><span>-       let mut lr_bak =
</span></span></span><span><span><span>-           Align16([[[[0.into(); 2 /* x */]; 8 /* y */]; 3 /* plane */ ]; 2 /* idx */]);
</span></span></span><span><span><span></span>        
</span></span><span><span>        // .. snip ..
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>Since <code>dav1d</code> never initialized it anyway, we know that logically any data read from this buffer was written beforehand with a valid value
(which really helps to drive home the idea that <a href="https://www.ralfj.de/blog/2021/11/18/ub-good-idea.html">Undefined Behavior deserves a better reputation</a>). The savings are very small here, but every penny counts!</p><p>Running the full benchmark, we get a nice speed boost from the original <code>73.914 s ± 0.151 s</code>:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ hyperfine --warmup <span>2</span> <span>"target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1"</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ± σ<span>)</span>:     72.644 s ±  0.250 s    <span>[</span>User: 72.023 s, System: 0.239 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min … max<span>)</span>:   72.281 s … 73.098 s    <span>10</span> runs
</span></span></code></pre></div><p>There’s still a way to go to <code>dav1d</code>’s <code>67.912 s ± 0.541 s</code>, but 1.2 seconds (1.5%) improvement in total runtime is a great start, and covers about 20% of the performance diff between the two.</p><h2 id="profiling-again-but-inverted">Profiling Again, But Inverted</h2><p>Let’s reload the profiler outputs from the start, but use the “inverted stack” view.<br><code>dav1d</code> (C) (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Fdav1d_profile.json.gz/calltree/?globalTrackOrder=0&amp;invertCallstack&amp;thread=0&amp;v=10">Link</a>): 
<code>rav1d</code> (Rust) (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_baseline.json.gz/calltree/?globalTrackOrder=0&amp;invertCallstack&amp;thread=0&amp;v=10">Link</a>): </p><p>There are a few options we can explore for optimization, but the function that got my attention was <code>add_temporal_candidate</code>: the difference between the Rust and the C version is significant enough (~400 samples, about 0.5 seconds),
and the function itself seems innocuous: it’s about 50 lines of <code>if</code>s and <code>for</code>s, with a few calls to short utility functions.</p><p>To help us find out where we are bleeding out the missing performance, we can try to recompile <code>rav1d</code> with debug symbols.
The <code>rav1d</code> project helpfully defines a <code>[profile.release-with-debug]</code> in its <code>Cargo.toml</code>, allowing us to run:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ cargo build --profile<span>=</span>release-with-debug
</span></span><span><span>$ sudo samply record target/release-with-debug/dav1d ...
</span></span></code></pre></div><p>What we get back is slightly different than before (<a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_with_debug.json.gz/calltree/?globalTrackOrder=0&amp;invertCallstack&amp;thread=0&amp;v=10">Link</a>): the <code>release-with-debug</code> profile will not be as-optimized,
and small functions calls appear bigger than they really are, but we get a <strong>line-by-line sample breakdown of the function</strong>, and it should steer us in the right direction.</p><p>If you scroll a little, one thing that will jump out to you will be that the <code>if cand.mv.mv[0] == mv {</code> and <code>if cand.mv == mvp {</code> lines seem to cover a combined 600 samples!</p><p>Let’s pull up <code>mv: Mv</code>’s definition:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[derive(Clone, Copy, PartialEq, Eq, Default, FromZeroes, FromBytes, AsBytes)]</span>
</span></span><span><span><span>#[repr(C)]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>Mv</span> <span>{</span>
</span></span><span><span>    <span>pub</span> y: <span>i16</span><span>,</span>
</span></span><span><span>    <span>pub</span> x: <span>i16</span><span>,</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Huh. How can this be slow? It’s just <code>#[derive(PartialEq)]</code>.</p><p><img src="https://ohadravid.github.io/2025-05-rav1d-faster/futurama_fry.jpg" alt="Futurama Fry Looking Suspicious" width="40%" height="414px"></p><p>And even more suspiciously, the <code>dav1d</code> version is slightly different, and uses <code>mvstack[n].mv.n == mvp.n</code> to do the same comparisons.
But what is <code>n</code>? Looking at <code>dav1d</code>’s definition of <code>mv</code>, we find:</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>typedef</span> <span>union</span> mv <span>{</span>
</span></span><span><span>    <span>struct</span> <span>{</span>
</span></span><span><span>        <span>int16_t</span> y<span>,</span> x<span>;</span>
</span></span><span><span>    <span>};</span>
</span></span><span><span>    <span>uint32_t</span> n<span>;</span>
</span></span><span><span><span>}</span> mv<span>;</span>
</span></span></code></pre></div><p>It seems like the <code>dav1d</code> authors knew that comparing two <code>i16</code>s can be slow, so when they compare two <code>mv</code>s, they treat them as <code>u32</code>s.</p><h3 id="replace-field-wise-equality-with-byte-wise-equality-that-optimizes-better">Replace Field-wise Equality with Byte-wise Equality that Optimizes Better</h3><p>Can this be the problem?<br>Defining <code>Mv</code> as a <code>union</code> has a big downside in Rust: it makes it <code>unsafe</code> to access any field of the <code>union</code>,
which will “infect” every usage of <code>Mv</code>, which is the opposite of what we usually want to do in Rust (trying to encapsulate unsafety in a safe API).</p><p>Fortunately, we have a different option: We can use <code>transmute</code> to re-interpret <code>Mv</code> as a <code>u32</code>, and use that to implement <code>PartialEq</code>.</p><p>Firing up <a href="https://godbolt.org/z/r9MfTeY8b">Godbolt</a>, we can inspect the generated code for the two ways to do the comparison:</p><p>Clearly the <code>transmute</code> version is superior, but can we avoid the <code>unsafe</code> block?<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><p>It turns out that the <code>zerocopy</code> crate can statically verify the <a href="https://docs.rs/zerocopy/latest/zerocopy/trait.IntoBytes.html#safety">safety requirements</a> for a <code>struct</code> to be represented as <code>&amp;[u8]</code>, allowing us to write:</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>use</span> zerocopy::<span>{</span>AsBytes<span>,</span> FromBytes<span>,</span> FromZeroes<span>};</span>
</span></span><span><span>
</span></span><span><span><span>#[derive(Clone, Copy, Eq, Default, FromZeroes, FromBytes, AsBytes)]</span>
</span></span><span><span><span>#[repr(C)]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>Mv</span> <span>{</span>
</span></span><span><span>    <span>pub</span> y: <span>i16</span><span>,</span>
</span></span><span><span>    <span>pub</span> x: <span>i16</span><span>,</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>impl</span> <span>PartialEq</span> <span>for</span> Mv <span>{</span>
</span></span><span><span>    <span>#[inline(always)]</span>
</span></span><span><span>    <span>fn</span> <span>eq</span><span>(</span><span>&amp;</span>self<span>,</span> other: <span>&amp;</span><span>Self</span><span>)</span> -&gt; <span>bool</span> <span>{</span>
</span></span><span><span>        self<span>.</span>as_bytes<span>()</span> <span>==</span> other<span>.</span>as_bytes<span>()</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Which produces the same (optimized) assembly we saw when we used <code>transmute</code>.</p><p>After implementing similar optimizations for <code>RefMvs{Mv,Ref}Pair</code>, we can re-run the benchmark:</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ hyperfine --warmup <span>2</span> <span>"target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads 1"</span>
</span></span><span><span>Benchmark 1: target/release/dav1d -q -i Chimera-AV1-8bit-1920x1080-6736kbps.ivf -o /dev/null --threads <span>1</span>
</span></span><span><span>  Time <span>(</span>mean ± σ<span>)</span>:     72.182 s ±  0.289 s    <span>[</span>User: 71.501 s, System: 0.242 s<span>]</span>
</span></span><span><span>  Range <span>(</span>min … max<span>)</span>:   71.850 s … 72.722 s    <span>10</span> runs
</span></span></code></pre></div><p>This is <em>another</em> 0.5 second improvement over our previous result (<code>72.644 s ± 0.250 s</code>), or a 2.3% improvement over the baseline (<code>73.914 s ± 0.151 s</code>).</p><p>We are now only 4.2 seconds from <code>dav1d</code>’s <code>67.912 s ± 0.541 s</code>, so we covered about 30% of the performance diff we saw at the start of this article.</p><p>You might be wondering why the default implementation of <code>PartialEq</code> results in bad code generation,
and <a href="https://github.com/memorysafety/rav1d/pull/1400#issuecomment-2891734817">a comment</a> on the PR adding these impls pointed to <a href="https://github.com/rust-lang/rust/issues/140167">Rust issue #140167</a>,
which tracks exactly this type of problem.</p><p>If you consider the C case, when using a <code>struct { int16_t y, x; }</code> it’s possible to initialize only <code>y</code> while leaving <code>x</code> uninitialized.
As long as equality is checked with <code>this.y == other.y &amp;&amp; this.x == other.x</code> and all <code>y</code>s are different, you don’t get any UB.</p><p>Therefore, it’s invalid to optimize this to a single memory load and compare <strong>unless the code can guarantee that all fields are always initialized</strong>.
However, quoting this <a href="https://github.com/rust-lang/rust/issues/140167#issuecomment-2895174679">comment</a> by @hanna-kruppe on the issue:</p><blockquote><p>That’s not simply a missed optimization opportunity. While the load of the second field can’t load poison/undef, that property is control-dependent. ..<br>Solving this seems hard: I don’t think LLVM has a way to express “loading through this pointer always reads initialized bytes”.</p></blockquote><h2 id="summary">Summary</h2><p>Using a few profiler snapshots from the <code>samply</code> profiler, we compared running <code>rav1d</code> and <code>dav1d</code> on the same input file, saw a 6-second (9%) runtime difference, and found two relatively low hanging fruits we could optimize:</p><ol><li>Avoiding an expensive zero-initialization in a hot, Arm-specific code path (<a href="https://github.com/memorysafety/rav1d/pull/1397">PR</a>), improving runtime by 1.2 seconds (-1.6%).</li><li>Switching the default <code>PartialEq</code> impls of small numeric <code>struct</code>s with an optimized version that re-interpret them as bytes (<a href="https://github.com/memorysafety/rav1d/pull/1400">PR</a>), improving runtime by 0.5 seconds (-0.7%).</li></ol><p>Each of these provide a nice speedup despite being only a few dozen lines in total, and without introducing new unsafety into the codebase.</p><p>The <code>rav1d</code> project maintainers were nice and responsive, and helped make these PRs more correct and better overall (big shout out to @kkysen 🚀).</p><p>There is still a gap of about 6% between the two implementations so there are still many more optimizations to discover,
and I suspect this approach of comparing between profiler snapshots of <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Fdav1d_profile.json.gz/calltree/?assemblyView=2~11c50~174~dav1d_filter_sbrow_cdef_8bpc&amp;globalTrackOrder=0&amp;search=dav1d_cdef_brow_8bpc&amp;thread=0&amp;v=10"><code>dav1d</code></a> and <a href="https://profiler.firefox.com/from-url/https%3A%2F%2Fohadravid.github.io%2F2025-05-rav1d-faster%2Frav1d_profile_after.json.gz/calltree/?globalTrackOrder=0&amp;search=rav1d_cdef_brow&amp;thread=0&amp;v=10"><code>rav1d</code></a> will yield at least some of them.</p><p>Go ahead and give this a try! Maybe <code>rav1d</code> can eventually become faster than <code>dav1d</code> 👀🦀.</p><p>Discuss on <a href="https://www.reddit.com/r/rust/comments/1ksnljw/making_the_rav1d_video_decoder_1_faster/">r/rust</a>, <a href="https://lobste.rs/s/j3mzif/making_rav1d_video_decoder_1_faster">lobsters</a>, <a href="https://news.ycombinator.com/item?id=44061160">HN</a>! 👋</p><p><em>If you liked this, you might also like <a href="https://ohadravid.github.io/posts/2025-01-debugging-vit-and-tensorrt/">Debugging a Vision Transformer Compilation Issue</a> and <a href="https://ohadravid.github.io/posts/2023-03-rusty-python/">Making Python 100x faster with less than 100 lines of Rust</a></em>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Planetfall (266 pts)]]></title>
            <link>https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/</link>
            <guid>44060305</guid>
            <pubDate>Thu, 22 May 2025 09:17:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/">https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/</a>, See on <a href="https://news.ycombinator.com/item?id=44060305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>Gentle readers, I have just wrapped up a fun side project that will be of great interest to a very small number of you. The result of one of the most technically demanding efforts of my career, I am very pleased to share it with you.</p>



<figure><a href="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg"><img data-attachment-id="8541" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/poster-with-rasters-2/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg" data-orig-size="4800,3600" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;Poster With Rasters&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Poster With Rasters" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=1024" width="4800" height="3600" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg 4800w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=150&amp;h=113 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=300&amp;h=225 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=768&amp;h=576 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/complete-layout-small-2025-05-20.jpg?w=1024&amp;h=768 1024w" sizes="(max-width: 4800px) 100vw, 4800px"></a><figcaption>Click to have a look at a detailed version. Contact me if you’re interested in a physical copy.</figcaption></figure>







<p>Most of you will wonder what this place is, but I hope that, for a few of you, the names clicked into place in your memory. This is the planet Chiron, the setting (and one of the main characters) of <em>Sid Meier’s Alpha Centauri</em>—a <a href="https://en.wikipedia.org/wiki/Sid_Meier%27s_Alpha_Centauri" target="_blank" rel="noreferrer noopener">computer game</a> from 1999 that has a cult following; I count myself among the cult.</p>



<p>I could go on about the game—which is deep and thought-provoking and has a remarkably beautiful and carefully considered visual language—but I’m here to talk about the map. This project pushed my skills into some new places, so even if you don’t care about the game, I think it’s worth talking about the technical details behind it.</p>



<p>But first, before we get in to those details, I want to mention something that this project helped teach me about the difference between real and fictional maps. Recently, after I’d told someone I was a cartographer, they asked if I mapped real or fictional places. To an outsider, it’s reasonable to put those two things on equal footing. But, to me, they felt like entirely different things. Other than the map above, I stick almost exclusively to the real world, and I observe this to be true for the majority of my colleagues. As I was grappling with this person’s question, I realized that this split might be because fantasy maps require a different (but overlapping) skill set than real maps. If a fantasy or sci-fi author asked me to construct a map for them, I would need to sit down and draw something new from scratch. I’m not trained to work that way. All of my experience centers on manipulating and styling geographic data, not on <em>creating</em> that geographic data.</p>



<p>Some mappers can, and do, handle both fictional and real places, but a lot of us, myself included, are primarily skilled in <a href="https://somethingaboutmaps.wordpress.com/2020/02/05/maps-in-the-kitchen/">cooking with ingredients</a> someone hands us, rather than growing the ingredients ourselves. Thus, the only reason I was able to make a fictional map was because there were actual pre-existing datasets I could use to build it.</p>



<hr>



<p>Before we jump in, I’ll mention that while this map is free to download, it was <em>quite</em> laborious. Your support helps me continue doing things like this, so consider clicking the buttons below, and/or sharing my work with others if you enjoyed it—it’s a big help!</p>







<h2>Getting the Data</h2>



<p>The game of <em>Alpha Centauri</em> is played on a map. The program can make up a new, random map for you, but there’s also an official, built-in map of the planet, <a href="https://archive.org/details/Sid_Meiers_Alpha_Centauri_Prima_Official_eGuide/page/n70/mode/1up?view=theater" target="_blank" rel="noreferrer noopener">carefully crafted</a> by one of the game designers, Chris Pine.</p>



<figure><a href="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png"><img data-attachment-id="8350" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-109/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png" data-orig-size="2500,1261" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=1024" loading="lazy" width="2500" height="1261" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=150&amp;h=76 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=300&amp;h=151 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=768&amp;h=387 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image.png?w=1024&amp;h=517 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></a></figure>



<p>The official map is 128 × 64 diamond-shaped pixels. Importantly, each pixel has several attributes: elevation, rainfall level, rockiness, and more. These attributes were the basis for my mapping. I sampled various datasets from the in-game map, and then used them to construct my poster. Though, that was often easier said than done.</p>



<figure><img data-attachment-id="8352" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-110/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png" data-orig-size="2150,1292" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=1024" loading="lazy" width="2150" height="1292" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png 2150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=150&amp;h=90 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=300&amp;h=180 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=768&amp;h=462 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-1.png?w=1024&amp;h=615 1024w" sizes="(max-width: 2150px) 100vw, 2150px"><figcaption>Part of the game map. Note the pixel highlighted in the center of the map. In the lower left, we can see data about the highlighted pixel: its elevation, the fact that it is arid and rocky, the presence of native lifeforms (xenofungus), and that it is at coordinates (16, 50).</figcaption></figure>



<p>I began by tackling the elevation data. While it isn’t uncommon for games (then or now) to have 3D terrain, <em>Alpha Centauri</em> is somewhat unusual in my experience, in that it actually tells you the exact elevation value for each map tile. You can see that in the lower left of the above image. So, I went over the map and wrote down the elevation value of every tile. All 8,192 of them. And then I double-checked them to make sure I had transcribed them correctly. As you can imagine, this took a very long time—many hours. I began this process sometime in 2022 and poked at it occasionally until wrapping up in 2025.</p>



<p>If you look back to the snippet of the in-game map above, you’ll notice that some tiles have what appears to be vegetation on them. This is the game’s way of visually symbolizing a tile’s average rainfall. There are only three levels: rainy, moist, and arid. Fortunately, I did <em>not</em> have to manually sample every pixel to get this information. My version of <em>Alpha Centauri</em> has a mod, not found in the original game, which shows you a thematic map of rainfall level.</p>



<figure><img data-attachment-id="8363" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-113/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png" data-orig-size="2494,1114" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=1024" loading="lazy" width="2494" height="1114" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png 2494w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=150&amp;h=67 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=300&amp;h=134 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=768&amp;h=343 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-4.png?w=1024&amp;h=457 1024w" sizes="(max-width: 2494px) 100vw, 2494px"></figure>



<p>I took screenshots of the whole map, and did a little Photoshop work to separate the different colors into three maps, one per raininess level.</p>



<figure><img data-attachment-id="8367" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-115/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png" data-orig-size="2628,1036" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=1024" loading="lazy" width="2628" height="1036" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png 2628w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=300&amp;h=118 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=768&amp;h=303 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-6.png?w=1024&amp;h=404 1024w" sizes="(max-width: 2628px) 100vw, 2628px"></figure>



<p>Then I brought each into QGIS, and dropped a grid of vector points on top, one per grid square. Each point sampled the underlying raster and absorbed its raininess level. The dataset still needed some double-checking and correcting, since there are other map items in the way that obscured the raininess data. But, I was able to get it done without too much hassle.</p>



<figure><img data-attachment-id="8365" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-114/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png" data-orig-size="2298,908" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=1024" loading="lazy" width="2298" height="908" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png 2298w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=300&amp;h=119 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=768&amp;h=303 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-5.png?w=1024&amp;h=405 1024w" sizes="(max-width: 2298px) 100vw, 2298px"></figure>



<p>I did the same procedure with the rockiness, which likewise comes in three levels—though I ended up not using that dataset. </p>



<p>Finally, there’s the xenofungus, which is one of the native lifeforms on Chiron. That’s the pink ground cover on the map above (though according to a game manual, it’s “crimson”). This dataset is binary: xenofungus is there, or not. There wasn’t a thematic map available in-game for this, so I just brought in the normal game map and sampled it with my grid points and looked for pixels that matched the pink/red range of colors, then did some corrections.</p>



<p>With that, I had all the ingredients I would need to make my map. While the elevation data took me years to finish sampling, the rest was done in a matter of hours.</p>



<h2>Projection</h2>



<p>Like a lot of game maps, <em>Alpha Centauri</em> effectively takes place on a cylinder. You can travel infinitely toward the left or the right, but once you reach the top or the bottom, you stop; the map/world ends.</p>



<figure><img data-attachment-id="8355" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/untitledx2/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png" data-orig-size="2400,1500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="untitledx2" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=1024" loading="lazy" width="2400" height="1500" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png 2400w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=150&amp;h=94 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=300&amp;h=188 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=768&amp;h=480 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/untitledx2.png?w=1024&amp;h=640 1024w" sizes="(max-width: 2400px) 100vw, 2400px"></figure>



<p>Though our map is cylindrical, the planet was certainly meant by the designers to be understood as spherical. Various in-game media, including the game’s title screen, show that Chiron is round. So I assumed that the in-game map was, like most maps, a 2D representation of a globe. But what was its projection?</p>



<p>Every time you build a base in the game, that base offers you control of a certain region, and the size of that region is always the same number of pixels, no matter where it is built. So, it seems reasonable to assume that each pixel represents the same amount of space on the planet—thus, the game map is on an equal-area projection. And given the rectilinear shape of the map (and the diagram above), we can reasonably assume a cylindrical projection. Thus, I assigned the game map a<a href="https://en.wikipedia.org/wiki/Cylindrical_equal-area_projection"> <em>cylindrical equal area projection</em></a>.</p>



<p>The aspect ratio of the cylindrical equal-area projection varies based on where you set the standard parallel. Here, on this excerpt from my <a href="https://somethingaboutmaps.wordpress.com/2022/12/19/projection-connections-a-very-nerdy-poster/">Projection Connections</a> poster, you can see a few named variations on this projection. Each just varies in where you set the standard parallel.</p>



<figure><img data-attachment-id="8358" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-111/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png" data-orig-size="2508,1276" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=1024" loading="lazy" width="2508" height="1276" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png 2508w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=150&amp;h=76 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=300&amp;h=153 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=768&amp;h=391 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-2.png?w=1024&amp;h=521 1024w" sizes="(max-width: 2508px) 100vw, 2508px"></figure>



<p>Setting it to 37.4° got me an aspect ratio that matched the in-game map, and that setting also means that we’re using the <a href="https://map-projections.net/single-view/trystan-edwards-snyder">Trystan Edwards projection</a>. To give you a better sense of all this, here’s how the game map and an earth map compare, under the same projection.</p>



<figure><img data-attachment-id="8360" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-112/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png" data-orig-size="3000,831" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=1024" loading="lazy" width="3000" height="831" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png 3000w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=150&amp;h=42 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=300&amp;h=83 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=768&amp;h=213 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-3.png?w=1024&amp;h=284 1024w" sizes="(max-width: 3000px) 100vw, 3000px"><figcaption>The map on the right is by daan Strebe, via Wikipedia.</figcaption></figure>



<p>All this projection work actually took place <em>before</em> the data sampling. So, when I say that I brought game maps into QGIS and dropped vector points on them, I already had a projection set up to handle the new datasets I was generating.</p>



<h2>Preparing the DEM</h2>



<p>At this point I had a projection, and I had a bunch of data in a very low-resolution (128 × 64) grid. Part of my interest in tackling this project was in figuring out how to make a map that was more detailed than the original game map, while still conforming to the known, in-game facts about Chiron. So, I wanted to explore how to turn my 8,192 elevation values into a much more detailed grid.</p>



<p>I tried many, many things. I spent enough hours at it that, in truth, I don’t remember a lot of the details of my attempts. But, after many dead ends, here’s the technique that I finally landed on. I started with the elevation grid—one elevation value per tile, just like we have in the game.</p>



<figure><img data-attachment-id="8384" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-124/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-15.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"><figcaption>A snippet of the original game elevations, near Planetneck.</figcaption></figure>



<p>Then I scattered a bunch of random points on it, while enforcing a minimum distance between them, so that they weren’t too bunched up. I ended up with about 1–3 points on each grid tile.</p>



<figure><img data-attachment-id="8385" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-125/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-16.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>A small number of grid tiles ended up without a point. I added extra points to cover those (I just found the centroids of each of those tiles). I then gave each point the value of the elevation of the grid tile it sat on top. So, I effectively turned my original elevation grid tiles into a scattered bunch of dots, with some elevation values repeated multiple times.</p>



<figure><img data-attachment-id="8386" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-126/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-17.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>And then I ran a TIN (triangulated irregular network) interpolation to generate an initial elevation model:</p>



<figure><img data-attachment-id="8387" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-127/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-18.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>Now, if I hadn’t done any of the point scattering, and had simply used my original elevation grid, here’s what it would have looked like:</p>



<figure><img data-attachment-id="8388" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-128/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-19.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>Notice how it’s much more grid-like and regular. By shifting all the points around, and sometimes duplicating them, I was able to break that grid up, and randomly give more weight to some areas vs. others. The first interpolation looks much more organic than the second, but it still uses the original elevation values.</p>



<p>That was only the first iteration of the process. I actually went back and refined it several times. While shifting all the elevation dots around, and duplicating them, was helpful, it wasn’t detailed enough. I wanted more elevation points, so I could have a finer scale of terrain. To do that, I first did a Delauney triangulation on my randomized points. This just draws triangles between all the points. Then I found the point at the center of each of those triangles.</p>



<p>And then I assigned an elevation value to each of those new points, simply by taking average of the three elevation values nearby. Finally, I added a bit of random noise: I nudged the elevation value of each point randomly up or down by a small amount, so that they would represent new bumps in the terrain, not a smooth continuation of the existing values. Here’s how that looks.</p>



<figure><img data-attachment-id="8390" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-129/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-20.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>I repeated this process of triangulation and bumping a couple more times (reducing the amount of bumping each time), until I had a very large number of points.</p>



<figure><img data-attachment-id="8392" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-130/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-21.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>And here’s what that looks like when interpolated into an elevation surface:</p>



<figure><img data-attachment-id="8394" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-131/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png" data-orig-size="2255,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=1024" loading="lazy" width="2255" height="880" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png 2255w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=150&amp;h=59 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=300&amp;h=117 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=768&amp;h=300 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-22.png?w=1024&amp;h=400 1024w" sizes="(max-width: 2255px) 100vw, 2255px"></figure>



<p>This is a big improvement over what we began with. It’s organic and detailed, and while it’s not perfect, it’s a large step in the right direction.</p>



<p>I made a few other adjustments to the field of elevation points before conducting the final interpolation. These were to help match my results with the canonical map of Chiron. Sometimes, due to the nature of the interpolation, or the random noise I introduced, things didn’t quite turn out as they should.</p>



<figure><img data-attachment-id="8396" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-132/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png" data-orig-size="2114,1258" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=1024" loading="lazy" width="2114" height="1258" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png 2114w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=150&amp;h=89 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=300&amp;h=179 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=768&amp;h=457 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-23.png?w=1024&amp;h=609 1024w" sizes="(max-width: 2114px) 100vw, 2114px"></figure>



<p>Above, you can see the canonical map, overlaid with a land/water mask from my interpolation. There’s an island in the top-center on the canonical map, but in my interpolated elevation model, it’s actually (barely) joined to the mainland. That occurs simply because I had some elevation points in the area that were above sea level, and some below, and the ones above sea level won out in the interpolation.</p>



<p>Here’s another situation requiring adjustment:</p>



<figure><img data-attachment-id="8399" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-133/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png" data-orig-size="2360,1608" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=1024" loading="lazy" width="2360" height="1608" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png 2360w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=150&amp;h=102 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=300&amp;h=204 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=768&amp;h=523 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-24.png?w=1024&amp;h=698 1024w" sizes="(max-width: 2360px) 100vw, 2360px"></figure>



<p>An island on the canonical map is only a couple of pixels wide in my interpolated elevation model. I would like it to be a bit bigger, so that it’s noticeable on the final map.</p>



<p>I manually reviewed the map to find places like these and added a few extra elevation data points, to correct things like closed straits, tiny islands, etc.</p>



<p>I also needed to manually adjust one major feature of the original map: Garland Crater. Here’s how it looks on the canonical map:</p>



<figure><img data-attachment-id="8405" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-137/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png" data-orig-size="2473,1145" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=1024" loading="lazy" width="2473" height="1145" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png 2473w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=150&amp;h=69 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=300&amp;h=139 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=768&amp;h=356 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-28.png?w=1024&amp;h=474 1024w" sizes="(max-width: 2473px) 100vw, 2473px"></figure>



<p>But remember, that map is based on an underlying diamond-shaped grid. When looking at the actual elevation values, the crater is much more of a square.</p>



<figure><img data-attachment-id="8406" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-138/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png" data-orig-size="2473,1145" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=1024" loading="lazy" width="2473" height="1145" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png 2473w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=150&amp;h=69 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=300&amp;h=139 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=768&amp;h=356 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-29.png?w=1024&amp;h=474 1024w" sizes="(max-width: 2473px) 100vw, 2473px"></figure>



<p>But, the crater is clearly meant to be a circle. The game designers even drew special map tiles to represent it as such. So, I un-squared this region of the map. I did a little rubbersheeting to shift my original elevation points around until I had something more circular.</p>



<figure><img data-attachment-id="8407" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-139/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png" data-orig-size="2488,1328" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=1024" loading="lazy" width="2488" height="1328" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png 2488w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=150&amp;h=80 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=300&amp;h=160 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=768&amp;h=410 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-30.png?w=1024&amp;h=547 1024w" sizes="(max-width: 2488px) 100vw, 2488px"><figcaption>It looks a bit vertically stretched here, but that’s just due to the projection.</figcaption></figure>



<p>With manual points added, and the crater circularized, I was ready to do the triangulation and interpolation steps once again. I’m explaining things a bit out of order here—there were many hours of trial and error and revision as part of this whole process. But this is effectively what I did.</p>



<p>Once I had the interpolation, I did a bit of smoothing on it. I do not remember the exact formula—it was a bit like mixing paints, in that I kept playing around until I got something that looked right. I did some focal statistics (mean and median), some line integral convolution via the <a href="https://plugins.qgis.org/plugins/karika/">Karika plugin</a>, and I also added just a bit of <a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin noise</a>. I mixed together smoothed layers with unsmoothed ones, and smoothed again. Again, a very seat-of-the-pants process.</p>



<p>In the end, here’s what I came up with:</p>



<figure><img data-attachment-id="8409" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-140/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png" data-orig-size="3304,1701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=1024" loading="lazy" width="3304" height="1701" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png 3304w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=150&amp;h=77 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=300&amp;h=154 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=768&amp;h=395 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-31.png?w=1024&amp;h=527 1024w" sizes="(max-width: 3304px) 100vw, 3304px"></figure>



<p>Compare that to a simple TIN interpolation of just the original, unmodified elevation points (before any randomization, manual patches, triangulation, etc.):</p>



<figure><img data-attachment-id="8411" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-141/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png" data-orig-size="3304,1701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=1024" loading="lazy" width="3304" height="1701" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png 3304w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=150&amp;h=77 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=300&amp;h=154 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=768&amp;h=395 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-32.png?w=1024&amp;h=527 1024w" sizes="(max-width: 3304px) 100vw, 3304px"><figcaption>Near the center of the image, and just a bit to the left, you can see the original square crater shape.</figcaption></figure>



<p>I think the process of all those adjustments yielded something much more organic-looking.</p>



<p>I wasn’t quite done yet with the elevation model, however. This interpolation was fine for most areas, but the poles needed attention. Given how stretched-out they are on the above map, I knew I would need to re-do the elevation interpolation for the north/south poles using projections that were less distorted in those areas. As an example, here’s the north pole DEM that I came up with, along with the original elevation point locations.</p>



<figure><img data-attachment-id="8413" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-142/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png" data-orig-size="2188,1884" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=1024" loading="lazy" width="2188" height="1884" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png 2188w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=150&amp;h=129 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=300&amp;h=258 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=768&amp;h=661 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-33.png?w=1024&amp;h=882 1024w" sizes="(max-width: 2188px) 100vw, 2188px"></figure>



<p>Since a large chunk of the pole had no game data, I sprinkled some mostly-random noise in the middle. And then I proceeded to randomize my point locations and add extra triangulated points, as I did with the main elevation model.</p>



<p>I made separate DEMs for the north and south poles, for use in any future cartography that might focus on those areas. And I also blended them back into the main DEM. With that, my elevation model was done!</p>



<p>Except it wasn’t. As I went through the rest of the mapmaking process, I kept finding small errors: missing lakes, tiny islands, etc. Much like the ones I showed above, except this time I didn’t catch them until I was done with the DEM. I patched those up as best I could, either through tiny changes to the elevation model, or, more often, by manually drawing lakes and islands later on in the cartographic process. Speaking of which, let’s actually start making a map with all these data…</p>



<h2>Fun with Projections</h2>



<p>For the main map in the layout, decided I wanted to show Chiron using an <a href="https://en.wikipedia.org/wiki/Armadillo_projection">orthoapsidal (Raisz Armadillo) projection</a>, which also happens to be one of my favorite projections.</p>



<figure><img data-attachment-id="8417" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-143/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png" data-orig-size="2068,1184" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=1024" loading="lazy" width="2068" height="1184" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png 2068w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=150&amp;h=86 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=300&amp;h=172 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=768&amp;h=440 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-34.png?w=1024&amp;h=586 1024w" sizes="(max-width: 2068px) 100vw, 2068px"><figcaption>Another daan Strebe map that I stole from Wikipedia.</figcaption></figure>



<p>I love that it explicitly reminds the reader that they’re dealing with a curved surface. Given that game players are accustomed to seeing only a flat representation, I thought was especially important to try and show something rounder to bring it to life. </p>



<p>I’d never gotten to make a map in this projection before. It’s not available in QGIS, so I had to create my own little projection script. Fortunately, Wikipedia had the formulae for the transformation. That info, plus a little Python knowledge (aided by ChatGPT, as I’m still not super-comfortable with PyQGIS yet) let me start using the orthoapsidal projection.</p>



<p>I ended up modifying the projection. To better fit Chiron’s landmasses, I adjusted the amount and the direction of the vertical tilt (the default is 20°, but I changed it to -10°). I also reduced the curvature on the sides. The downside of the orthoapsidal is that, by showing the curvature that gives such a nice sense of roundness, we lose some areas (Australia is cut off on the above map, as is Antarctica). But, with a clever bit of tweaking, I was able to reduce the curvature. I did this by first horizontally shrinking anything I was projecting:</p>



<figure><img data-attachment-id="8420" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-144/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png" data-orig-size="3347,1701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=1024" loading="lazy" width="3347" height="1701" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png 3347w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=150&amp;h=76 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=300&amp;h=152 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=768&amp;h=390 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-35.png?w=1024&amp;h=520 1024w" sizes="(max-width: 3347px) 100vw, 3347px"></figure>



<p>Then, when it was projected, it took up less horizontal space on the torus, and thus did not extend nearly as far to the sides:</p>



<figure><img data-attachment-id="8422" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-145/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png" data-orig-size="2806,1458" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=1024" loading="lazy" width="2806" height="1458" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png 2806w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=150&amp;h=78 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=300&amp;h=156 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=768&amp;h=399 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-36.png?w=1024&amp;h=532 1024w" sizes="(max-width: 2806px) 100vw, 2806px"><figcaption>I’m using Earth landmasses here, so that you can get a sense of the projection using more familiar shapes.</figcaption></figure>



<p>Thus I avoided the severe perspective distortion that occurred on the left/right edges. Notice the right edge of the above image, how the landmasses start to really compress along the edge of the projection as it curves away from the viewer. But, the green version does not do this as badly. By shrinking the landmass, I kept more of it on the parts of the torus that most directly face the viewer. Another way to think about it: In effect, I sort of made the torus larger, in relation to the map.</p>



<p>When I was done with the projection, I stretched things out horizontally a bit to un-do the shrinking I’d done.</p>



<p>My script unfortunately only handled vectors, and I was too impatient to dig in and make the code work with rasters. So, for projecting something like the elevation model, I did a hacky GIS thing and just turned each pixel into a polygon, projected it, and then re-rasterized it. That was not ideal, and meant long processing times, but it worked fine.</p>



<p>At this point, we can probably start breaking down my map layer-by-layer, as I’ve often done in this blog.</p>



<h2>Finally, Actual Cartography</h2>



<p>So, let’s break down the main map, which was constructed mostly in Photoshop. First we start with the bathymetry.</p>



<figure><img data-attachment-id="8427" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-146/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png" data-orig-size="2499,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=1024" loading="lazy" width="2499" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png 2499w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-37.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2499px) 100vw, 2499px"></figure>



<p>This is a black and white raster showing the parts of my (reprojected) DEM that had an elevation below zero. To this, I applied a gradient map to recolor everything into shades of blue.</p>



<figure><img data-attachment-id="8430" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-148/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png" data-orig-size="2351,1239" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=1024" loading="lazy" width="2351" height="1239" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png 2351w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=768&amp;h=405 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-39.png?w=1024&amp;h=540 1024w" sizes="(max-width: 2351px) 100vw, 2351px"></figure>



<p>A gradient map in Photoshop takes each pixel in the underlying raster and recolors based on its greyscale value. So, it can (among other things) take a black-to-white raster and turn those pixels into a new color ramp.</p>



<p>Next up: the initial land. I took my reprojected elevation model, clipped off any underwater values, and fed it through <a href="https://eduard.earth/">Eduard</a>, a program which simulates Swiss-style hand-shaded relief.</p>



<figure><img data-attachment-id="8432" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-149/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-40.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>I spent a while tweaking the settings to try and find a good balance. Remember, I made some of this information up, because I wanted a realistically detailed relief model. But, I also didn’t want to show <em>too</em> much of the noise I’d added. Once done, I added a bit of a coastal inner glow to help separate the land from the water.</p>



<figure><img data-attachment-id="8435" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-150/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-41.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>It also had the advantage of hiding the relief near the coastline, so that any nearby mountains gradually fade out as we hit the coast. That gives us a tiny bit of coastal plain. There are almost no flat areas in my map—this is a consequence of the very bumpy elevation values that are in the game. Mountains appear and disappear suddenly, often right next to the coast. I wanted to make the coasts just a little less rugged.</p>



<p>Next up, I used a hue/saturation layer to colorize the relief:</p>



<figure><img data-attachment-id="8437" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-151/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png" data-orig-size="2350,1238" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=1024" loading="lazy" width="2350" height="1238" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png 2350w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=768&amp;h=405 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-42.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2350px) 100vw, 2350px"></figure>



<p>I then sprinkled a little color noise onto it, to help it feel a bit more organic.</p>



<figure><img data-attachment-id="8439" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-152/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png" data-orig-size="2478,1354" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=1024" loading="lazy" width="2478" height="1354" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png 2478w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=150&amp;h=82 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=300&amp;h=164 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=768&amp;h=420 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-43.png?w=1024&amp;h=560 1024w" sizes="(max-width: 2478px) 100vw, 2478px"></figure>



<p>I did this just by having a couple of adjustment layers: another hue/saturation, and one brightness/contrast, and having these each masked by a channel that contained some small specks of noise:</p>



<figure><img data-attachment-id="8441" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-153/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png" data-orig-size="2102,1050" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=1024" loading="lazy" width="2102" height="1050" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png 2102w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=150&amp;h=75 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=300&amp;h=150 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=768&amp;h=384 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-44.png?w=1024&amp;h=512 1024w" sizes="(max-width: 2102px) 100vw, 2102px"></figure>



<p>Next up was the greenery. Just like the canonical map, I intended to use the rainfall intensity data to show vegetation. First off, I made a copy of the relief and tinted it green, including some noisy variations on those colors. </p>



<figure><img data-attachment-id="8445" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-156/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png" data-orig-size="2386,914" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=1024" loading="lazy" width="2386" height="914" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png 2386w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=150&amp;h=57 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=300&amp;h=115 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=768&amp;h=294 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-47.png?w=1024&amp;h=392 1024w" sizes="(max-width: 2386px) 100vw, 2386px"></figure>



<p>I did this in exactly the same way I did the earlier tan/brown relief: a hue/saturation layer to colorize it, plus a couple of adjustment layers to add some noisy speckles of lighter/darker areas.</p>



<p>Due to the darker green color, some of the relief was getting lost, so I also added a few more adjustments to emphasize it. These were just more adjustment layers that lightened/darkened the underlying image. </p>



<figure><img data-attachment-id="8447" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-157/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png" data-orig-size="3710,1542" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=1024" loading="lazy" width="3710" height="1542" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png 3710w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=150&amp;h=62 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=300&amp;h=125 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=768&amp;h=319 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-48.png?w=1024&amp;h=426 1024w" sizes="(max-width: 3710px) 100vw, 3710px"></figure>



<p>I used the shadows/highlights of the relief in the mask for these adjustment layers, so that they only lightened/darkened the right areas. Here’s a look inside one mask:</p>



<figure><img data-attachment-id="8449" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-158/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png" data-orig-size="3340,1150" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=1024" loading="lazy" width="3340" height="1150" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png 3340w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=150&amp;h=52 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=300&amp;h=103 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=768&amp;h=264 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-49.png?w=1024&amp;h=353 1024w" sizes="(max-width: 3340px) 100vw, 3340px"></figure>



<p>Now I blend the green vegetation version of the relief with the original brown relief.</p>



<figure><img data-attachment-id="8450" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-159/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-50.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>Some areas are still brown (those marked “arid” on the game map) and some are green (“rainy” or “moist” on the game map). But, how did I do this blending? Here’s a look at the mask that controls the opacity of the greenery layer:</p>



<figure><img data-attachment-id="8453" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-160/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-51.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>On a macro scale, it comes from those rainfall data I gathered a long while back. Remember, each grid tile in the game has one of 3 raininess levels, and I had a point representation of that dataset. I conducted a thin plate spline interpolation on the points (<em>after</em> reprojecting).</p>



<figure><img data-attachment-id="8455" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-161/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png" data-orig-size="2564,1442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=1024" loading="lazy" width="2564" height="1442" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png 2564w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=150&amp;h=84 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=300&amp;h=169 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=768&amp;h=432 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-52.png?w=1024&amp;h=576 1024w" sizes="(max-width: 2564px) 100vw, 2564px"></figure>



<p>That was the basis of my Photoshop mask. But, I also made it somewhat noisy, which you can see if you zoom into the Photoshop version:</p>



<figure><img data-attachment-id="8459" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-163/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png" data-orig-size="3338,1354" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=1024" loading="lazy" width="3338" height="1354" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png 3338w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=150&amp;h=61 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=300&amp;h=122 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=768&amp;h=312 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-54.png?w=1024&amp;h=415 1024w" sizes="(max-width: 3338px) 100vw, 3338px"></figure>



<p>This was done using a stack of Photoshop filters. First, I make use of the <em>dissolve</em> blending mode. This is one I’ve talked about <a href="https://somethingaboutmaps.wordpress.com/2016/10/03/terrain-in-photoshop/">before on this very blog</a>. Below, I have a solid black layer, and it’s masked with the rainfall data.</p>



<figure><img data-attachment-id="8460" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-164/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png" data-orig-size="2646,916" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=1024" loading="lazy" width="2646" height="916" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png 2646w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=150&amp;h=52 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=300&amp;h=104 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=768&amp;h=266 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-55.png?w=1024&amp;h=354 1024w" sizes="(max-width: 2646px) 100vw, 2646px"></figure>



<p>In the dissolve mode, basically Photoshop uses the mask layer to decide the likelihood of a pixel appearing. There are no semi-transparent pixels: they are either wholly transparent, or wholly opaque. So this turns our rainfall data into a scatter of dots.</p>



<p>I took this scatter of dots, flattened it out (with a white background) into a single layer, and then made a bunch of copies. I blurred each copy a different amount, and stacked them all together. And then did some re-sharpening. In the end I was left with a speckly, but soft, noise texture.</p>



<figure><img data-attachment-id="8462" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-165/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png" data-orig-size="2240,1092" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=1024" loading="lazy" width="2240" height="1092" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png 2240w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=150&amp;h=73 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=300&amp;h=146 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=768&amp;h=374 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-56.png?w=1024&amp;h=499 1024w" sizes="(max-width: 2240px) 100vw, 2240px"></figure>



<p>And <em>this</em> is what I used to control where the green layer blended into the tan. If we zoom in closely you can see it at work.</p>



<figure><img data-attachment-id="8464" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-166/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png" data-orig-size="2964,1386" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=1024" loading="lazy" width="2964" height="1386" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png 2964w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=150&amp;h=70 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=300&amp;h=140 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=768&amp;h=359 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-57.png?w=1024&amp;h=479 1024w" sizes="(max-width: 2964px) 100vw, 2964px"></figure>



<p>I did all this work to ensure that the vegetation layer looked more like <em>vegetation</em> (i.e., individual plants). Though, honestly, a lot of this doesn’t show up on the printed version of the map unless you’ve got a magnifier. But, most people will look at it on screen and so they’ll hopefully still appreciate it.</p>



<p>Here’s what it would look like if I’d just used the original, unspeckled, smooth rainfall layer as a mask:</p>



<figure><img data-attachment-id="8466" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-167/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png" data-orig-size="3120,1318" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=1024" loading="lazy" width="3120" height="1318" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png 3120w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=150&amp;h=63 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=300&amp;h=127 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=768&amp;h=324 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-58.png?w=1024&amp;h=433 1024w" sizes="(max-width: 3120px) 100vw, 3120px"></figure>



<p>Now as we go from a rainy area to a dry one, the vegetation just sort of slowly fades out. It doesn’t look quite natural. At the boundary of a desert, individual trees don’t suddenly become transparent. Instead, you still have trees, but you just have fewer and fewer of them. That’s what the speckling accomplishes.</p>



<p>Next up: xenofungus. This is one of the native lifeforms of Chiron. It’s show on the game map, and in various in-game media, as pinkish or reddish. I add this to the map using pretty much exactly the same process as the greenery. I interpolated the point data, smooth it out a bit, make it noisy, and use it as a mask for a reddish version of the relief.</p>



<figure><img data-attachment-id="8469" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-168/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-59.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>Next up : rivers. There are a handful of these which appear on the in-game map. They are very square, since they’re confined to the map grid. To make these a bit more real-looking, I redrew them manually in QGIS.</p>



<figure><img data-attachment-id="8473" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-170/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png" data-orig-size="2746,994" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=1024" loading="lazy" width="1024" height="370" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=1024" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=1022 1022w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=2044 2044w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=150 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=300 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-61.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>For the cartography side of things, I gave a slight taper to the source end of each river in Adobe Illustrator, and then brought them into Photoshop, colored them, and gave them a little bit of a downward bevel so that they seem incised into the land. It’s a tiny thing that no one will really notice on the final print, but you’d be able to tell if I <em>didn’t</em> do it.</p>



<figure><img data-attachment-id="8475" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-171/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png" data-orig-size="2168,822" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=1024" loading="lazy" width="2168" height="822" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png 2168w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=150&amp;h=57 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=300&amp;h=114 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=768&amp;h=291 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-62.png?w=1024&amp;h=388 1024w" sizes="(max-width: 2168px) 100vw, 2168px"></figure>



<p>I <em>did</em> look at generating a new set of rivers based on my DEM. I ran some hydrological analysis tools, but, due to the bumpy nature of the terrain (both because of my noise, and because of the original game map elevations) I mostly got a lot of endorheic basins and stubby rivers. Drawing my own was easier (and I looked at my relief model to make sure I didn’t cross over any ridges).</p>



<p>That wraps up all the stuff on land. I applied a clipping mask to all this stuff, so that the fungus/greenery/tan relief/etc. only show up where they are supposed to. I also put a little outer glow on the land, to lighten up the coastal waters. Here’s where we’re at right now in our Photoshop stack:</p>



<figure><img data-attachment-id="8480" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-174/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png" data-orig-size="3086,1342" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=1024" loading="lazy" width="3086" height="1342" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png 3086w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=150&amp;h=65 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=300&amp;h=130 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=768&amp;h=334 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-65.png?w=1024&amp;h=445 1024w" sizes="(max-width: 3086px) 100vw, 3086px"></figure>



<p>Just a few more layers to go! Next up is more xenofungus. The fungus occurs not only on land, but also in the water. I didn’t want to use exactly the same color, though, so I added the water fungus separately. Just a simple purple layer that uses the same speckled mask as the land fungus, but is also in a layer that keeps it clipped to the water only. It’s set to blend into the underlying water via a <em>linear burn</em> blending mode.</p>



<figure><img data-attachment-id="8481" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-175/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png" data-orig-size="3494,1832" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=1024" loading="lazy" width="3494" height="1832" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png 3494w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=300&amp;h=157 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=768&amp;h=403 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-66.png?w=1024&amp;h=537 1024w" sizes="(max-width: 3494px) 100vw, 3494px"></figure>



<p>Next up, I apply a saturation boost to the whole map, and add a graticule to help visualize the curvature. The increase in saturation was something I originally did by accident, just moving layers around in Photoshop. But, I liked how it looked, so I kept it.</p>



<figure><img data-attachment-id="8483" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-176/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-67.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>The result is a color scheme that’s a little outside of my comfort zone. I tend to lean toward less saturated colors, and am noted for my fondness of working in <a href="https://somethingaboutmaps.com/Monochrome" target="_blank" rel="noreferrer noopener">monochrome</a>. But, this is a side project, so it’s a good time for me to experiment.</p>



<p>Next up, I gave things a semi-painted look. I copied the map, flattened it into a single layer, shrank it down, and ran a <em>dry brush</em> filter in Photoshop. This gives things a somewhat painted look that I really like. </p>



<figure><img data-attachment-id="8485" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-177/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png" data-orig-size="2568,1066" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=1024" loading="lazy" width="2568" height="1066" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png 2568w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=150&amp;h=62 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=300&amp;h=125 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=768&amp;h=319 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-68.png?w=1024&amp;h=425 1024w" sizes="(max-width: 2568px) 100vw, 2568px"></figure>



<p>Above, you can see both the before and after. In the center, we have the pre-filtered version. Around the outside, you can see the much softer version after the effect is applied. I’ve been meaning for some time to do a tutorial on this technique, and that might happen this year.</p>



<p>The softness was a little <em>too</em> blurry, so I actually mixed it 50-50 with the original map, which I think gives a nice balance of detail and painterly softness.</p>



<figure><img data-attachment-id="8488" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-179/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png" data-orig-size="2322,1072" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=1024" loading="lazy" width="2322" height="1072" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png 2322w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=150&amp;h=69 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=300&amp;h=139 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=768&amp;h=355 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-70.png?w=1024&amp;h=473 1024w" sizes="(max-width: 2322px) 100vw, 2322px"></figure>



<p>We’re nearly done with this file. Here’s how it looks at this stage:</p>



<figure><img data-attachment-id="8490" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-180/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-71.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>Finally, I added a bit of darkening to emphasize the curved surface. I just did a couple of inner shadows to slightly darken the edges (by different amounts, depending on the edge), to give a sense of the light falling off in a way that perhaps suggests the reader that this is not a flat piece.</p>



<figure><img data-attachment-id="8492" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-181/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-72.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<h2>Labeling</h2>



<p>I did the initial labeling work in Adobe Illustrator, as it has significantly better tools for handling type than Photoshop does. When I was done, the labels were ported back over to Photoshop to blend into the map.</p>



<figure><img data-attachment-id="8494" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-182/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png" data-orig-size="2500,1316" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=1024" loading="lazy" width="2500" height="1316" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png 2500w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=150&amp;h=79 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=300&amp;h=158 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=768&amp;h=404 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-73.png?w=1024&amp;h=539 1024w" sizes="(max-width: 2500px) 100vw, 2500px"></figure>



<p>I blurred the map under each label, which helps with legibility by keeping the text <a href="https://somethingaboutmaps.wordpress.com/2021/06/01/on-edges/" target="_blank" rel="noreferrer noopener">edges sharper</a> than the background.</p>



<figure><img data-attachment-id="8496" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-183/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png" data-orig-size="2716,920" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=1024" loading="lazy" width="2716" height="920" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png 2716w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=150&amp;h=51 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=300&amp;h=102 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=768&amp;h=260 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-74.png?w=1024&amp;h=347 1024w" sizes="(max-width: 2716px) 100vw, 2716px"></figure>



<p>I also applied a glow to the land labels and the graticule, which you can see above. It’s just a darkened version of the original map, feathered out around the areas I chose to depict in semitransparent white text. I also added a light glow to the outside of the dark labels. Both of these help with contrast and let me get away with light text on a light background and dark text on a dark background.</p>



<figure><img data-attachment-id="8498" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-184/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png" data-orig-size="2936,864" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=1024" loading="lazy" width="2936" height="864" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png 2936w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=150&amp;h=44 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=300&amp;h=88 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=768&amp;h=226 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-75.png?w=1024&amp;h=301 1024w" sizes="(max-width: 2936px) 100vw, 2936px"></figure>



<p>I also applied a version of my <a href="https://somethingaboutmaps.wordpress.com/2018/10/28/smart-type-halos-in-photoshop-and-illustrator/" target="_blank" rel="noreferrer noopener">smart halos</a> technique to further strengthen the glow where needed.</p>



<p>I set all the labels in <a href="https://www.sarahbellmaps.com/typography-for-topography-belltopo-sans-free-font/" target="_blank" rel="noreferrer noopener">BellTopo Sans</a>, designed by my friend and colleague Sarah Bell. Finding the right typeface for this map was a bit of a challenge. I didn’t want to go with a serif—I thought most of them looked too delicate for the map’s bold color scheme, and also seemed out of place on a sci-fi map. Plus they wouldn’t hold up very well when set semi-transparent.</p>



<p>The game itself uses <a href="https://en.wikipedia.org/wiki/Eurostile" target="_blank" rel="noreferrer noopener">Eurostile</a>, but I thought it looked too hard for a map with a soft palette and style.</p>



<figure><img data-attachment-id="8501" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-185/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png" data-orig-size="2098,898" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=1024" loading="lazy" width="2098" height="898" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png 2098w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=150&amp;h=64 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=300&amp;h=128 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=768&amp;h=329 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-76.png?w=1024&amp;h=438 1024w" sizes="(max-width: 2098px) 100vw, 2098px"></figure>



<p><em>Most</em> sans serifs seemed a little too solid for the feel of the map. But, BellTopo Sans worked nicely. It has the strength of a sans serif, but also has some strong humanistic elements. For example, look at that colorful lowercase <strong>g</strong>, the rounded tops of the <strong>W</strong>, or the squished curve forming the bowl of the <strong>a</strong>. The whole typeface has hints of a hand-drawn character (it’s derived from old topo maps).</p>



<figure><img data-attachment-id="8504" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-186/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png" data-orig-size="1446,646" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=1024" loading="lazy" width="1446" height="646" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png 1446w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=150&amp;h=67 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=300&amp;h=134 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=768&amp;h=343 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-77.png?w=1024&amp;h=457 1024w" sizes="(max-width: 1446px) 100vw, 1446px"></figure>



<p>The biggest challenge in labeling was not one that I usually face in my work: I didn’t always know where features were. In the game, features are not always labeled clearly. They are simply bits of white text placed atop single grid tiles.</p>



<figure><img data-attachment-id="8506" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-187/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png" data-orig-size="3598,1596" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=1024" loading="lazy" width="3598" height="1596" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png 3598w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=150&amp;h=67 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=300&amp;h=133 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=768&amp;h=341 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-78.png?w=1024&amp;h=454 1024w" sizes="(max-width: 3598px) 100vw, 3598px"></figure>



<p>In many cases, I had to apply my own interpretation to decide the boundaries of ambiguous features. Planetneck, seen above, was the most challenging of these. Does it refer to that finger of water? To the bit of land crossing it? To make matters worse, that bit of land is itself ambiguous. There are several spots in the game marked with these thin brown lines that indicate that they are traversable on both land and water.</p>



<figure><img data-attachment-id="8508" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-188/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png" data-orig-size="1824,892" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=1024" loading="lazy" width="1824" height="892" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png 1824w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=150&amp;h=73 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=300&amp;h=147 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=768&amp;h=376 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-79.png?w=1024&amp;h=501 1024w" sizes="(max-width: 1824px) 100vw, 1824px"><figcaption>Blue arrows: You can move ships across the thin bits of land. Orange arrows: you can move land units along the thin bits of land.</figcaption></figure>



<p>So, there are plenty of areas that might reasonably be construed as land <em>or</em> as water. My randomization process used in creating the elevation model made it so I didn’t have to decide which was which, but it also still left me to decide what Planetneck is. I ended up deciding it was the narrow entrance to the long, finger-like bay.</p>



<figure><img data-attachment-id="8511" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-189/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png" data-orig-size="2202,1288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=1024" loading="lazy" width="2202" height="1288" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png 2202w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=150&amp;h=88 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=300&amp;h=175 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=768&amp;h=449 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-80.png?w=1024&amp;h=599 1024w" sizes="(max-width: 2202px) 100vw, 2202px"></figure>



<p>This was mostly an arbitrary decision, but it <em>is</em> supported by the fact that the game has names for a few other straits, so there were other features of this type already called out in the canonical map.</p>



<p>The official map only has 32 names, which is far less than I would usually have for a map like this. But, I didn’t feel comfortable coining any new ones. The choices of what to label are sometimes strange. Only two landmasses are named, and they’re mid-sized islands. The big continents are anonymous. There’s only one body of water called an “ocean,” and it’s smaller than some that are called “seas.” But, it is what it is. Maybe one of the game developers will find this and comment with their thoughts on how this came to be.</p>



<p>Finally, it was time to put this map into a poster. I added a few more, smaller maps, made from the same datasets and with the same techniques. One to show the elevation data more clearly, and two to show the polar regions with less distortion.</p>



<figure><img data-attachment-id="8514" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-190/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png" data-orig-size="3000,2250" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=1024" loading="lazy" width="3000" height="2250" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png 3000w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=150&amp;h=113 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=300&amp;h=225 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=768&amp;h=576 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-81.png?w=1024&amp;h=768 1024w" sizes="(max-width: 3000px) 100vw, 3000px"></figure>



<p>I continued to use BellTopo Sans for the poster text. I also added some faint scanlines to the background and the maps themselves.</p>



<figure><img data-attachment-id="8516" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-191/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png" data-orig-size="2400,1350" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=1024" loading="lazy" width="2400" height="1350" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png 2400w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=150&amp;h=84 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=300&amp;h=169 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=768&amp;h=432 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-82.png?w=1024&amp;h=576 1024w" sizes="(max-width: 2400px) 100vw, 2400px"></figure>



<p>This is an homage to the game interface (as is the blue color on the title and the frames surrounding the map). I think the scanlines also really do a great job of tying all the items in the layout together.</p>



<figure><img data-attachment-id="8518" data-permalink="https://somethingaboutmaps.wordpress.com/2025/05/20/planetfall/image-192/" data-orig-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png" data-orig-size="2160,1462" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-image-caption="" data-medium-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=300" data-large-file="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=1024" loading="lazy" width="2160" height="1462" src="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png" alt="" srcset="https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png 2160w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=150&amp;h=102 150w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=300&amp;h=203 300w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=768&amp;h=520 768w, https://somethingaboutmaps.wordpress.com/wp-content/uploads/2025/05/image-83.png?w=1024&amp;h=693 1024w" sizes="(max-width: 2160px) 100vw, 2160px"></figure>



<h2>Wrap-Up</h2>



<p>So there you have it! This was one of the most technically laborious projects of my career. Tedious sampling from the game data; lots of GIS work to convert those data into something higherer resolution; the demands of fine-tuning a custom projection that isn’t supported in QGIS; and many Photoshop layers to try and bring everything together with lots of carefully constructed noise.</p>



<p>I wish I could say I did all of these things in the order I presented them above, or as efficiently as I described them. But, the real process was months of meandering and false starts and dead ends.</p>



<p>Now that I have the datasets, I may do more with them. I’ll probably play around in the future with larger scale mapping, zooming in on smaller chunks of the planet. This will give me a chance to explore ways to generate even more realistic terrain that still conforms with the handful of known elevation values. Your suggestions are welcome. One thing I didn’t try was using the rockiness data. I only thought of that <em>after</em> I finished the map designs. I went back and quickly had a look at using that layer as a mask to smooth my elevation model. Fortunately, it didn’t really make much of a difference at this scale, but it would if I zoomed in.</p>



<p>The original DEM also still needs some further cleanup (I had to manually patch a couple of missing lakes and islands onto the map, but I didn’t fix them in the elevation model). Though I’ll probably just replace it with a better one, once I know how to generate something with a bit more realism.</p>



<p>As I said, I could only make this map because I had data to work with. I don’t presently have the skills to sit down and make something like this just by drawing. I’m a manipulator, rather than a generator, of data. But it was fun to try and take those skills and apply them to a fictional location. It took basically all of my fifteen years of growth as a cartographer to get to the point where I could do justice to this game from my past (and present, though it’s been a few years since I played much). As is so often the case, the more I work on something, the more niche its appeal tends to be, but I’m glad to know that there is a specific type of nerd out there who will really appreciate it.</p>



<hr>



<p>Speaking of, if you’re a nerd who appreciates this, I’ll once again remind you that your support helps me continue doing projects like this, so consider clicking the buttons below, and/or sharing my work with others if you enjoyed it—it’s a big help!</p>








	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why does Debian change software? (246 pts)]]></title>
            <link>https://blog.liw.fi/posts/2025/why-debian-changes/</link>
            <guid>44059411</guid>
            <pubDate>Thu, 22 May 2025 06:50:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.liw.fi/posts/2025/why-debian-changes/">https://blog.liw.fi/posts/2025/why-debian-changes/</a>, See on <a href="https://news.ycombinator.com/item?id=44059411">Hacker News</a></p>
<div id="readability-page-1" class="page"><article class="page">
    

    

    <section id="pagebody">
	<p>When I wrote <a href="https://blog.liw.fi/posts/2023/debian-reasons/">Why is Debian the way it is?</a>, a year and a half ago, I was asked to also cover why Debian changes the software it packages. Here’s a brief list of examples of why that happens:</p>
<ul>
<li><p>Software in Debian needs to follow certain policies as set by Debian over the years, and documented in the <a href="https://www.debian.org/doc/debian-policy/">Debian Policy Manual</a>. These are mostly mundane things like system wide configuration being in <code>/etc</code>, documentation in <code>/usr/share/doc</code>, and so on. Some of this is more intricate, like when names of executables can be the same in different packages.</p></li>
<li><p>Programs included in Debian need to work together in other ways. This might mean require changing one or both. As an example, they might need to agree where Unix domain socket exists, or what Unix user account they should run under.</p></li>
<li><p>Debian will remove code that “calls home” or tries to update software in a way that bypasses the Debian packaging system. This is done both for privacy reasons, and because updating software without going via the packaging system is usually problematic from a functional point of view, and always problematic from a security point of view.</p></li>
<li><p>Debian may fix bugs before they’re fixed in upstream, or may backport a bug fix to an earlier version. The goal here is to make life better for users of Debian. Debian does this especially for fixes to security problems, but also for other problems.</p></li>
<li><p>Debian avoids including anything in the main part of its package archive it can’t legally distribute. This applies to the source packages. This means, Debian may strip out those parts software that it doesn’t think are free according to the Debian Free Software Guidelines. The stripped-out parts might be moved to another package in the “non-free” part of Debian. An example might a manual that is licensed under the GNU Free Documentation License with immutable parts, or a logo that can’t be changed.</p></li>
<li><p>Debian has often added a manual page when the upstream doesn’t provide one.</p></li>
</ul>
<p>Thank you to Jonathan McDowell for help with this list. Opinions and mistakes are mine. Mine, I say!</p>

      </section>

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decibels Are Ridiculous (530 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/decibels-are-ridiculous</link>
            <guid>44058778</guid>
            <pubDate>Thu, 22 May 2025 04:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/decibels-are-ridiculous">https://lcamtuf.substack.com/p/decibels-are-ridiculous</a>, See on <a href="https://news.ycombinator.com/item?id=44058778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>I don’t have many pet peeves. That said, no matter how hard I try, I just can’t get over the sheer madness of the scientific “unit” we call the decibel (dB).</p><div id="vimeo-257122744" data-attrs="{&quot;videoId&quot;:&quot;257122744&quot;,&quot;videoKey&quot;:&quot;&quot;,&quot;belowTheFold&quot;:false}" data-component-name="VimeoToDOM"><p><iframe src="https://player.vimeo.com/video/257122744?autoplay=0" frameborder="0" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true"></iframe></p></div><p><span>What’s a decibel? Well, the most common answer is “uhh". The second most common answer is that it’s “a way to measure loudness”. But it isn’t! A decibel is not a unit in any conventional sense: it’s more akin to a prefix such as </span><em>mega-</em><span> in </span><em>megabyte. </em><span>It describes a change in magnitude.</span></p><p><span>On the face of it, the idea makes sense. We sometimes want to say that some quantity has increased 10× or 1,000× — and in engineering, these numbers can get large. In principle, we have several types of shorthand notations; for example, instead of “300,000” we can write “3e5”. That said, I don’t know how to pronounce “3e5”, so it’s not unreasonable to come up with a name for the exponent. “An increase of five </span><em>clerts</em><span>” could mean that something has grown by a factor of 10</span><sup>5</sup><span>. </span></p><p><span>This brings us to a pseudo-unit called the </span><em>bel</em><span>. At first blush, it works just like our clert: it’s specifies the exponent of a 10</span><em><sup>n </sup></em><span>multiplier. In other words, +1 bel is a 10× increase (10</span><em><sup>+1</sup></em><span>); meanwhile, -2 bels means a decrease of 100× (10</span><em><sup>-2</sup></em><span>). The bel is named in the honor of Alexander Bell; this is in the same tradition that prompted us to name the “wat” in honor of James Watt.</span></p><p><span>But wait, there’s a twist! The bel was originally devised for measuring power. In some cases, the dissipated power has a quadratic relationship to the applied voltage; for example, for resistive loads in electronics, we have </span><a href="https://lcamtuf.substack.com/p/primer-core-concepts-in-electronic" rel="">Joule’s law</a><span>:</span></p><p><span>In this scenario, if the applied voltage increases by a factor of </span><em><span>x</span><sup>1</sup></em><span>, the consumed power increases by a factor </span><em><span>x</span><sup>2</sup></em><span>. More generally, if voltage applied to a resistor increases </span><em>n </em><span>clerts, then power dissipation jumps up </span><em>2n</em><span> clerts. This is because (</span><em><span>x</span><sup>n</sup><span>)</span><sup>2</sup><span> = x</span><sup>2n</sup></em><span>. </span></p><p><span>Seeing this, some madman decided that 1 bel should always describe a 10× increase in power, </span><em>even if it’s applied to another base unit</em><span>. This means that if you’re talking about watts, +1 bel is an increase of 10×; but if you’re talking about volts, it’s an increase of √10×. This is nuts: it’s akin to saying that the </span><em>milli-</em><span> prefix should have different meanings depending on whether we’re talking about meters or liters. And what if you want to express other units on a similar scale — is frequency more like power or more like voltage?</span></p><p><span>The weirdness didn’t end there. For some reason, the bel — again, what started as a sensible 10× increment — was soon deemed too big to use. I don’t quite know why: in other aspects of life, decimal notation suits us just fine. Either way, instead of putting up with the occasional fractional value or switching to base 2, we divided the bel into ten steps known as </span><em>decibels</em><span>. In effect, we started raising 10 to a fractional power, producing irrational per-dB multipliers:</span></p><div data-component-name="Latex"><p><span>\(\begin{align}
\text{For power: } &amp; 10^\frac{1}{10} \approx 1.258925 \\
\text{For voltage: } &amp; \sqrt{10}^{\ \frac{1}{10}} = 10^{\frac{1}{2} \cdot \frac{1}{10}} = 10^\frac{1}{20} \approx 1.122018 \\
\end{align}\)</span></p></div><p>The original unit is long-forgotten; we use decibels exclusively.</p><p><span>What didn’t change from bels to decibels is that the concept describes nothing more than an exponent for a multiplier; the value is meaningless unless we know the base unit and the reference point (e.g., 1 V). As a matter of custom, however, both of these are often underspecified; in many fields, the decibel evolved into a standalone unit — or a collection thereof. This makes the decibel an </span><em>“if you know, you know”</em><span> kind of a deal.</span></p><p>For example, in acoustics, the “dB” unit actually corresponds to air pressure in pascals (quick quiz: is that a power-like or a voltage-like quantity?). As for the meaning of 0 dB, the measurement is usually indexed to a 1 kHz sound wave that exerts 20 μPa of pressure. This makes some sense: it’s roughly the threshold of human hearing. That said, no part of the “dB” label tells you that.</p><p>From this 0 dB origin, we derive two parallel acoustic scales. One measures the absolute sound pressure level with no regard to frequency; another is weighted to mimic human hearing. The latter peaks at 3 kHz, then tapers off sharply below 200 Hz and above 10 kHz.</p><p><span>Now, let’s imagine you’re trying to buy a microphone; the spec will give you a sensitivity figure such as -45 dB. Is that one of the two acoustic decibels? Fat chance! Here, the real unit is volts. The zero point on that scale describes a hypothetical microphone that produces a 1 V swing in response to a reference sound level. If the microphone you’re looking at is specified at -45 dB, it means that the measured swing is 1 V · 10</span><em><sup>-45/20</sup></em><span>, or about 6 mV.</span></p><p>But hold on: what’s that reference sound level? “Oh, I know,” you exclaim, “it’s the threshold of human hearing”. Nope! It’s 94 dB, roughly the loudness of a gas-powered lawnmower. We can play that game for a long time.</p><p><span>Even in situations where a decibel “unit” has a suffix to hint at what it describes, the naming schemes make little sense. For example, in radio applications, you can come across “dBm”. Is that decibel-meters? No, that would be silly: it’s a measurement of power relative to 1 milliwatt. So, it must follow that “dBμ” is related to 1 microwatt? Hah! That “μ” actually stands microvolts. And you </span><em>really</em><span> don’t want to confuse that with “dBu”…</span></p><p><em><span>I write </span><a href="https://lcamtuf.coredump.cx/offsite.shtml" rel="">well-researched, original articles</a><span> about geek culture, algorithms, and electronic circuit design (+ comparatively few rants about decibels). </span><strong>If you like the content, please subscribe.</strong><span> It’s increasingly difficult to stay in touch with readers via social media; my typical post on X is shown to less than 5% of my followers and gets a ~0.2% clickthrough rate.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT Is a Gimmick (112 pts)]]></title>
            <link>https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick</link>
            <guid>44058677</guid>
            <pubDate>Thu, 22 May 2025 04:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick">https://hedgehogreview.com/web-features/thr/posts/chatgpt-is-a-gimmick</a>, See on <a href="https://news.ycombinator.com/item?id=44058677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <article>
                    
                    <p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I recently attended a workshop on teaching with artificial intelligence at the university where I teach writing as a part-time adjunct. I had low hopes for the workshop, but I was also desperate. My students keep turning in essays that were obviously generated by AI, and I need to figure out what to do. I looked forward to hearing the keynote speaker, a former university president who made his name arguing that instructors should move all digital technology out of their classrooms so they and their students can focus on the human interactions that technology cannot replace.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>At the workshop, though, the first thing he asked us to do was open our laptops, navigate to a couple of LLMs, and enter a prompt. As we did this, he kept talking. What was I supposed to pay attention to? Him, or my screen? While he jabbered, I prompted Claude.ai to write a short essay in response to one of the “innovative” topics it had proposed to me: a “Change My View Challenge,” based on a Reddit forum. It was important that I use the word&nbsp;<em>innovative</em>&nbsp;in my prompt, the presenter insisted. Omit&nbsp;<em>innovative</em>&nbsp;and you get different, presumably more pedestrian, results. Claude spat out the paper and told me it was proud of its work, which, after all, had a “clear thesis statement.” When I said I couldn’t find the statement, Claude replied, “You’re right to question this. Looking at the essay more carefully, there isn’t a single, explicit thesis statement that clearly states the central argument.” Thanks, genius.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Later, the presenter wanted to demonstrate the amazing feats of pedagogical customization AI is capable of. So he asked a model to create, on the spot, a brief podcast summary of his own book, adding that the model should employ baseball metaphors, because, in this experiment, the user is a jock who cares only about baseball. The idea seemed to be that students would better appreciate the book’s content if it were expressed in terms they are already familiar with. He pressed play. The resulting summary, offered by credibly bland digital hosts, was astonishingly shallow and stupid, wedding one tired cliché (education is “lighting a lamp”) to another (“beware of the curveballs”). Was anybody really learning anything from this?</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The presenter seemed to be trying much too hard.&nbsp;<em>AI can do this! AI can do that!</em>&nbsp;He moved randomly from topic to topic, bouncing around the stage and making faux-shocked faces at his own pronouncements about the marvelous, career-disrupting implications of large language models. He seemed overstimulated and came across as impatient with the pace of human life and thought as it has&nbsp;been until now.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>That speaker was not the most frenzied AI advocate I came across this spring. A recent issue of the&nbsp;<em>Chronicle of Higher Education</em>&nbsp;ran a fevered&nbsp;</span><a href="https://www.chronicle.com/article/are-you-ready-for-the-ai-university"><span>fantasy</span></a><span>&nbsp;by one Scott Latham, a professor of strategy at the Manning School of Business at the University of Massachusetts at Lowell. It is a vision featuring AI “agents” that will provide students a bespoke experience running from orientation through course instruction to job placement, all the while tracking their every frown and furrowed brow (because students will stare endlessly into cameras) and responding at each moment with the perfect remedy. “Human interaction is not as important to today’s students,” Latham claims, and so presumably the AI university will offer little. Compared to the current model of college, he promises, all of this will be better and&nbsp;cheaper—never mind growing evidence that LLMs are deteriorating and becoming&nbsp;</span><a href="https://futurism.com/the-byte/openai-o3-cost-per-query"><span>more expensive</span></a><span>.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>And it&nbsp;<em>will</em>&nbsp;happen. In fact, the word&nbsp;<em>will</em>&nbsp;appears more than 130 times in the 4,000-word article.&nbsp;<em>Could</em>, only twice. “Predicting AI’s disruption is the easy part,” Latham writes. “The tough part is making people realize the inevitable.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The claim of inevitability is crucial to technology hype cycles, from the railroad to television to AI. “A key strategy for a technology to gain market share and buy-in,” the scholars David Gray Widder and Mar Hicks&nbsp;</span><a href="https://ash.harvard.edu/resources/watching-the-generative-ai-hype-bubble-deflate/"><span>write</span></a><span>, “is to present it as an inevitable and necessary part of future infrastructure, encouraging the development of new, anticipatory infrastructures around it.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The infrastructure demanded by AI is neither neutral nor cheap. It has well-known environmental costs, given the vast amount of electricity and water its data centers demand. Nor is the infrastructure only physical. It has already been built in the minds of students, who are becoming informational lotus-eaters, addicted to immediate, effort-free homework answers and adequate-seeming essays on demand.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Tot up the overeager salesmen, the questionable prophecies, and the clearly exorbitant costs, and it becomes clear: AI is not revolutionary. It’s a gimmick. It entices us with the prospect of sparing us drudgery, but it ultimately disappoints.&nbsp;</span>AI apologists are like hucksters at a county fair, fast-talking about some newfangled marvel. Universities are their gape-mouthed marks,&nbsp;&nbsp;<a href="https://www.insidehighered.com/news/tech-innovation/artificial-intelligence/2023/09/05/risks-and-rewards-higher-ed-should-know"><span>emptying their pockets</span></a><span>&nbsp;even while they are unsure about what they are buying or whether students will use it to learn or simply cheat with.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>We call something a gimmick, the literary scholar&nbsp;</span><a href="https://www.chronicle.com/article/the-professor-of-gimmicks"><span>Sianne Ngai</span></a><span>&nbsp;points out, when it seems to be simultaneously working too hard and not hard enough. It appears both to save labor and to inflate it, like a fanciful Rube Goldberg device that allows you to sharpen a pencil merely by raising the sash on a window, which only initiates a chain of causation involving strings, pulleys, weights, levers, fire, flora, and fauna, including an opossum. The apparatus of a large language model really is remarkable. It takes in billions of pages of writing and figures out the configuration of words that will delight me just enough to feed it another prompt. There’s nothing else like it.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But look at what people actually use this wonder for: brain-dead books and videos, scam-filled ads, polished but boring homework essays. Another presenter at the workshop I attended said he used AI to help him decide what to give his kids for breakfast that morning.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Such disappointment inevitably accompanies the gimmick. “The gimmick lets us down,” Ngai writes, “only because it has also managed to pump us up.”&nbsp;<em>A universal culture-producing machine? Remarkable!</em>&nbsp;Then we see its results. Widder and Hicks note that the failed promise of AI “is not surprising, as generative AI does not so much represent the wave of the future as it does the ebb and flow of waves past.” MOOCs, NFTs, AR: We should be wise to the tricks by now. AI progress in cultural production already seems to have slowed because the models have run out of human-generated writing to “learn” from and increasingly feed on AI-produced content, gulping down a vile soup of their own ever-concentrating ordure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The AI apologists must deny, or at least forestall, such disappointment. They insist that the time scales on AI’s technical progress are shortening—artificial general intelligence will be here in ten years; no, five; no, we’re just months away—even as they implore skeptics to give the technology a chance, because it’s still early days.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>But do the apologists even believe it themselves? Latham, the professor of strategy, gives away the game at the end of his reverie. “None of this can happen, though,” he writes, “if professors and administrators continue to have their heads in the sand.” So it’s not inevitable after all? Whoops.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The scholars hectoring their colleagues to adopt AI are not all so ham-fisted. A more subtle and psychologically interesting argument about AI in higher ed shows how fine the line is between the huckster and the mark. Writing recently in&nbsp;<em>The New Yorker</em>, Princeton history professor D. Graham Burnett takes up where Latham leaves off, drawing a distinction between himself and his denialist colleagues: “[E]everyone seems intent on pretending that the most significant revolution in the world of thought in the past century&nbsp;<em>isn’t happening</em>.” This pretense, he writes, “is, simply, madness. And it won’t hold for long.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett’s essay appears to issue from deep doubts about the value of humanities research. Given the capabilities of AI to sift through archives, detect patterns within the contents, and perhaps incrementally advance what has been said about them, the value of an academic monograph seems to fall to zero. “The making of books such as those on my shelves,” Burnett writes, “each the labor of years or decades, is quickly becoming a matter of well-designed prompts. The question is no longer whether we can write such books; they can be written endlessly, for us. The question is, do we want to read them?”</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The answer depends on the “we.” Does the median&nbsp;<em>New Yorker</em>&nbsp;subscriber want to read a monograph? No. Does a scholar fifty or five hundred years hence want to? Let’s put it out there and let them decide. That has been the value proposition of humanistic research for centuries.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett supposes there might still be merit in teaching, even if scholarship is dead. It is a maneuver many an Ivy League PhD has made who finds himself or herself in a job with a heavy teaching load. Indeed, the overwhelming majority of humanities PhDs are in this position. Most have teaching-intensive jobs as adjuncts or, if they are on the tenure track, they teach four or five or six courses per semester at community colleges and regional universities. They never publish a single monograph and read few if any after they finish graduate school. For them, academia already looks like the near-future Burnett envisions.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Burnett decided to merge his teaching with his interest in AI. He assigned students in an undergraduate class—so far as I can tell, the&nbsp;</span><a href="https://registrar.princeton.edu/course-offerings?term=1254&amp;keywords=burnett"><span>only one</span></a><span>&nbsp;he taught this spring—to engage with a chatbot about human attention and turn the text into a short paper. He marvels at their output:</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<blockquote>
<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Reading the results, on my living-room couch, turned out to be the most profound experience of my teaching career. I’m not sure how to describe it. In a basic way, I felt I was watching a new kind of creature being born, and also watching a generation come face to face with that birth: an encounter with something part sibling, part rival, part careless child-god, part mechanomorphic shadow—an alien familiar.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</blockquote>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In Burnett’s eyes, his students are not just creative in the ordinary sense of being able to turn nice phrases or make clever connections—already feats that lighten a grading load of dreary essays. No, Burnett’s students are conjurers, evokers, maybe minor deities, able to break the old material laws most of us labor under. I know this impulse. In moments of professional self-doubt, I have often tried to convince myself that my students were amazing, that their halting efforts were in fact brilliant, that my class, unique among the courses listed on their transcripts, unlocked something in them, that the students were teaching&nbsp;<em>me</em>, that, indeed, all I needed to do was get out of their way.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>It is a lie many teachers tell themselves. And why not? It is not as if someone can fact-check it. Your scholarship, or lack thereof, is public; your students’ work occurs behind the high fence of the Family Educational Rights and Privacy Act. You tell yourself a story at once self-abnegating and self-aggrandizing. You tell it to raise the students up; you’re there for&nbsp;<em>them</em>, after all. And that much is true; you are there for them. But you tell the story as well to put down your unfeeling, ossified colleagues, the ones who don’t get it. You tell it so you won’t feel as old as they are, because you are spiritually closer to the young. To Burnett, the AI dialogs offered something genuinely new under the sun. “Each sheaf of paper I picked up,” he writes, “was more astonishing than the last.” Sure.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is not to say I have never been astonished by my students’ writing. Burnett reports being moved to tears by one of his students’ interactions with a chatbot. I, too, have cried while reading student essays. I once had a student who started college only after he had retired from four decades working for the phone company. He was not a great writer, but he wrote from the heart. In a class on religious autobiography, he described going to the hospital to speak to the man, by then on his deathbed, who had murdered my student’s mother. The man asked my student to forgive him. My student did so. He forgave. The essay testified to a form of love few of us would be capable of. Sitting at my kitchen table, I read the essay a second time; I cried a second time. It was the rare piece of student writing that improved the world by its existence. It made mercy more widely known.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>In the end, Burnett is essentially in the same place as his ostrich-headed colleagues, though it is not clear he realizes it.&nbsp;“You can no longer&nbsp;<em>make</em>&nbsp;students do the reading or the writing,” he writes, because they can make a machine do it for them. “So what’s left? Only this: give them work they want to do. And help them want to do it.” Is this a slip-up? A signal that AI-savvy students don’t need teachers after all? If students want to do the work, then they don’t need help wanting it. No, the only task remains the paradoxical one identified as far back as in Plato’s&nbsp;<em>Meno</em>: Give students work they don’t know they need to do. And yes, help them want to do it.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I have found, to overcome students’ resistance to learning, you often have to trick them. There’s the old bait-and-switch of offering grades, then seeing a few students learn to love learning itself. Worksheets are tricks, as are small-group discussions and even a teacher’s charisma. I’m sure I have used baseball analogies in class, too. In the face of the difficulty of reforming students’ desires, you can trick yourself into believing you’re doing it, and sleep well at night. I don’t know anyone for whom it’s a straightforward task. It’s&nbsp;<em>the</em>&nbsp;challenge for any teacher, and AI offers a tempting illusion to students—and evidently to some teachers—that there could be a shortcut.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>The week Burnett’s article appeared, I visited the classroom of Ted Hadzi-Antich at Austin Community College. His honors political philosophy students were discussing the final section of James Baldwin’s&nbsp;<em>The Fire Next Time</em>. They wanted to talk about death. They sat in chairs in a&nbsp;circle, no desks walling them off from each other. I had hoped I could sit unobtrusively in the corner and take notes on the scene. “Learning doesn’t happen in the corner!” a student scolded. OK, fine. I was present. So I was implicated in what was about to occur.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>For eighty minutes, the eleven students and Hadzi-Antich talked. I chimed in at the end. No students texted. None disappeared into a screen. Two cried. They held their highlighted copies of the reading in their laps, but they didn’t talk much about it. Even so, they connected Baldwin’s ideas to their varied life experiences, including loss, addiction, and bigotry. Two students disagreed with each other about the uses of the past. Later, they told me they each describe the other as their “antagonist.” They seemed to respect each other, having read each other’s writing and debated in class all semester. They know each other’s voice.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>This is what Burnett seems to think is new, or newly exciting, thanks to his engagement with AI—something teachers such as Hadzi-Antich have been orchestrating in their classes since well before the dawn of AI. And in fact, Hadzi-Antich promotes text- and discussion-based education at community colleges through&nbsp;</span><a href="https://www.tgqf.org/"><span>The Great Questions Foundation</span></a><span>, which he directs. You don’t need to experience the technological sublime in order to see the value of giving students a book and asking them to read and discuss it. You do need to accept something like Max Weber’s idea of the teacher’s vocation, that of helping others “<em>to reckon with the ultimate meaning of</em>&nbsp;[their]&nbsp;<em>own actions</em>.” This reckoning is not a task you can expedite with a machine. If you try to spare yourself the labor, you fail.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<h2><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>* * *</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></h2>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Appropriately enough, this essay took me forever to write. I had a great deal of anger, frustration and sadness to draw on. I had evidence and critique, but I could not find an argument. I kept working. I changed the whole focus after I attended the AI workshop at school. That opened me up. I worked for days in the caesura between reading my students’ drafts and reading their finished research essays, knowing for sure that some had done all the work themselves because I had seen them put up the scaffolding and then build the essay, brick by brick. I knew that others would likely do very little, but I would be unable to prove it. As I worked, new articles and outrages about AI in education appeared daily, making me feel as though I were falling behind. Believe me, I wanted a shortcut.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I had not read&nbsp;<em>Theory of the Gimmick</em>&nbsp;before starting this essay, having only heard about it from my wife, a professor of literature. To get up to speed, I read an earlier article that Ngai turned into a chapter. I then skimmed my wife’s well-marked-up copy of the book, noting her underlines, her starred passages, her “hm!” in the margins. I leaned on the index to find passages on belief. I left most of the book unread.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I did these things because I know I am mortal. I cannot afford to read a 400-page work of frankly oblique theoretical prose. That is to cast no shade on Ngai. Her book deserves a close reading. I have only so much time. Even if that sounds like the same excuse students give for why they take AI shortcuts, I don’t think we mean the same thing.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Once I had a full draft, I asked my wife to read and comment on it. At the time, she was working on a paper about Herman Melville’s&nbsp;<em>The Confidence Man</em>&nbsp;and Max Weber’s “Science as a Vocation.” The latter is a locus classicus for our dinner-table conversation. She needed only to make a brief mention of a key passage in Weber for me to know what move the essay was missing. Her concerns, her attention, and the contingencies of her life and thought are all over this essay. It would have been different if she had been reading, say, William or Henry James that week. To a large extent, the intellectual circuit running through her life and mine, across two decades of talking about literature and culture, love and death, constitutes our life together.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>After I got her feedback, I finally asked ChatGPT if generative AI could be considered a gimmick in Ngai’s sense. I did not read its answer carefully. Whenever I see the words cascade down my computer screen, I get a sinking feeling.&nbsp;<em>Do I really have to read this?</em>&nbsp;I know I am unlikely to find anything truly interesting or surprising, and the ease with which the words appear really does cheapen them.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>ChatGPT reported back that it could indeed be considered a gimmick. Then I asked the gimmick how educators should approach its implementation in universities and schools. “The worst thing educational institutions could do is embrace AI uncritically as an inevitable ‘efficiency upgrade,’” it wrote, “because that would mean compounding the very gimmickry Ngai diagnoses: mistaking ease for value, and output for understanding.” Take that, Scott Latham. This is, of course, just what I hoped the machine would say. I have argued enough with the model that it probably knows I want it to be self-critical. I suspect it admits such things only to me. I wish it would tell university presidents, information officers, professors, and certainly students that even ChatGPT thinks it is a gimmick. But it only ever tells them what they want to hear, not what I want.&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Ted Hadzi-Antich’s students read and discussed these words by James Baldwin: “Perhaps the whole root of our trouble, the human trouble, is that we will sacrifice all the beauty of our lives, will imprison ourselves in totems, taboos, crosses, blood sacrifices, steeples, races, mosques, armies, flags, nations” —I might add technologies and hype bubbles—“in order to deny the fact of death, which is the only fact we have.”&nbsp;</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>Part of a teacher’s job—certainly in the humanities, but even in professional fields like business—is to help students break out of their prisons, at least for an hour, so they can see and enhance the beauty of their own minds. It is to help them learn, together, to defend how they want to live, precisely because they, too, unlike a machine, will one day die.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>

<p><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span><span>I will sacrifice some length of my days to add depth to another person’s experience of the rest of theirs. Many did this for me. The work is slow. Its results often go unseen for years. But it is no gimmick.</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>




                </article>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kotlin-Lsp: Kotlin Language Server and Plugin for Visual Studio Code (154 pts)]]></title>
            <link>https://github.com/Kotlin/kotlin-lsp</link>
            <guid>44058299</guid>
            <pubDate>Thu, 22 May 2025 02:46:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Kotlin/kotlin-lsp">https://github.com/Kotlin/kotlin-lsp</a>, See on <a href="https://news.ycombinator.com/item?id=44058299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Language Server for Kotlin</h2><a id="user-content-language-server-for-kotlin" aria-label="Permalink: Language Server for Kotlin" href="#language-server-for-kotlin"></a></p>
<p dir="auto"><a href="https://kotlinlang.org/docs/components-stability.html" rel="nofollow"><img src="https://camo.githubusercontent.com/c8add7fbcddc03009a577eb21a377434029f70f6926fdc9d3820351ab71585dc/68747470733a2f2f6b6f746c2e696e2f6261646765732f6578706572696d656e74616c2e737667" alt="Kotlin Alpha" data-canonical-src="https://kotl.in/badges/experimental.svg"></a>
<a href="https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub" rel="nofollow"><img src="https://camo.githubusercontent.com/3a1da9e18c90c77d3353249e6f25354aa47d4c693655086c1e53a5113f886e17/68747470733a2f2f6a622e67672f6261646765732f696e63756261746f722e737667" alt="JetBrains incubator project" data-canonical-src="https://jb.gg/badges/incubator.svg"></a></p>
<p dir="auto">Pre-alpha official Kotlin support for Visual Studio Code and an implementation of <a href="https://github.com/Microsoft/language-server-protocol">Language Server Protocol</a>
for the Kotlin language.</p>
<p dir="auto">The server is based on <a href="https://github.com/JetBrains/intellij-community">IntelliJ IDEA</a> and the <a href="https://github.com/JetBrains/intellij-community/tree/master/plugins/kotlin">IntelliJ IDEA Kotlin Plugin</a>
implementation.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Start</h3><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<ol dir="auto">
<li>Download the latest build of VSC extension via <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/RELEASES.md">RELEASES.md</a></li>
<li>Install it as a VSC Extension via <code>Extensions | More Action | Install from VSIX</code>
<ul dir="auto">
<li>Alternatively, it is possible to drag-and-drop VSIX extension directly into <code>Extensions</code> tool window</li>
</ul>
</li>
<li>Ensure that your Java version is 17 or above</li>
<li>Open a folder with JVM-only Kotlin Gradle project and the project will be immediately recognized and LSP activated</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Kotlin/kotlin-lsp/blob/main/images/quickstart_sample.gif"><img src="https://github.com/Kotlin/kotlin-lsp/raw/main/images/quickstart_sample.gif" alt="quickstart_sample.gif" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported features and Roadmap</h3><a id="user-content-supported-features-and-roadmap" aria-label="Permalink: Supported features and Roadmap" href="#supported-features-and-roadmap"></a></p>
<p dir="auto">The best way to track current capabilities and what is going to be supported in the next builds is this table:</p>
<blockquote>
<p dir="auto">Important note: currently, only JVM-only Kotlin Gradle projects are supported out-of-the box.</p>
</blockquote>
<ul>
<li> Project import
<ul>
<li> Gradle JVM project import</li>
<li> Gradle KMP project import</li>
<li> JSON-based build system agnostic import
<ul>
<li> Quickstart for JSON</li>
</ul>
</li>
<li> Maven/Amper import</li>
<li> Dumb mode for no build system at all</li>
</ul>
</li>
<li> Highlighting
<ul>
<li> Semantic highlighting</li>
</ul>
</li>
<li> Navigation
<ul>
<li> Navigation to Kotlin (source, binary)</li>
<li> Navigation to Kotlin builtins</li>
<li> Navigation to Java (source, binary)</li>
</ul>
</li>
<li> Code actions
<ul>
<li> Quickfixes (i.e. <code>replace with</code>)</li>
<li> Kotlin inspections</li>
<li> Organize imports</li>
<li> Go to reference</li>
</ul>
</li>
<li> Refactorings
<ul>
<li> Rename</li>
<li> Move</li>
<li> Change signature</li>
</ul>
</li>
<li> On-the-fly Kotlin diagnostics</li>
<li> Completion
<ul>
<li> Analysis-API based completion</li>
<li> IJ-based completion
<ul>
<li> Enable IJ-based completion</li>
</ul>
</li>
</ul>
</li>
<li> KDoc support
<ul>
<li> In-project documentation hovers</li>
<li> Dependencies/Java documentation hovers from <code>source.jar</code></li>
</ul>
</li>
<li> Code formatting</li>
<li> Fully-featured Windows support</li>
<li> Reactive updates from the filesystem</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Project Status</h3><a id="user-content-project-status" aria-label="Permalink: Project Status" href="#project-status"></a></p>
<p dir="auto"><strong>The project is in an experimental, pre-alpha, exploratory phase</strong> with the intention to be productionized.</p>
<p dir="auto">We <a href="https://xkcd.com/1428/" rel="nofollow">move fast, break things</a>, and explore various aspects of the seamless developer experience
including Java interoperability, limits of IntelliJ capabilities as a standalone server, native binaries of the LSP, and
debug capabilities.</p>
<p dir="auto">The LSP supports most of the essential parts, but its final shape is not near to be defined and
even the most basic and core parts are being changed on a regular basis.</p>
<p dir="auto">So we have the corresponding stability guarantees -- <strong>none</strong>. It is okay to use it in your toy
projects, to experiment with it and to provide your feedback, but it is not recommended
to depend on its stability in your day-to-day work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported platforms</h3><a id="user-content-supported-platforms" aria-label="Permalink: Supported platforms" href="#supported-platforms"></a></p>
<p dir="auto">In the current state, the golden path has been tested for Visual Studio Code with macOS and Linux platforms.</p>
<p dir="auto">You can use Kotlin LSP with other LSP-compliant editors, but configuration must be done manually.
Please note that Kotlin LSP uses pull-based diagnostics, so the editor must support that.</p>
<p dir="auto">You can find a standalone LSP launch script in <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/scripts/kotlin-lsp.sh">kotlin-lsp.sh</a> along
with <em>very experimental</em> (aka "works on someone's machine") instructions that setup LSP for other editors in <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/scripts">scripts</a> folder.
See <code>./kotlin-lsp.sh --help</code> for available options.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Source code</h3><a id="user-content-source-code" aria-label="Permalink: Source code" href="#source-code"></a></p>
<p dir="auto">Currently, the LSP implementation is partially closed-source, primarily for the sake of development speed convenience --
it heavily depends on parts of IntelliJ, Fleet, and our distributed Bazel build that allows us to
iterate quickly and experiment much faster, cutting corners and re-using internal infrastructure where it helps.
After the initial stabilization phase and defining the final set of capabilities, we will de-couple the LSP implementation from the internal repository
and build pipelines and open source it completely (with an explicit dependency on IntelliJ), this is a temporary constraint.
VSC extension is mirrored into <a href="https://github.com/Kotlin/kotlin-lsp/blob/main/kotlin-vscode">kotlin-vscode</a> as it does not depend on anything internal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Feedback and issues</h3><a id="user-content-feedback-and-issues" aria-label="Permalink: Feedback and issues" href="#feedback-and-issues"></a></p>
<p dir="auto">The best way to provide your feedback or report an issue is to file a bug <a href="https://github.com/Kotlin/kotlin-lsp/issues/new">in GitHub issues</a>.</p>
<p dir="auto">As a temporary limitation, direct contributions are not supported as this repository is a read-only mirror,
but it is possible to open a PR into the documentation, and it will be integrated manually by maintainers.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Getting a paper accepted (192 pts)]]></title>
            <link>https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/</link>
            <guid>44057841</guid>
            <pubDate>Thu, 22 May 2025 01:19:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/">https://maxwellforbes.com/posts/how-to-get-a-paper-accepted/</a>, See on <a href="https://news.ycombinator.com/item?id=44057841">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>In 2019, I submitted a paper that was rejected with review scores 2.5, 3, 3. One week later, I resubmitted it with minor changes, and it was accepted with scores 4, 4.5, 4.5.<sup><a href="#fn1" id="fnref1">01</a></sup> For context, that’s an almost unspeakably dramatic jump in scores, from “middling reject” to “strong accept.”</p>
<p>This post shows exactly those changes. We’ll frame them in two parts:</p>
<ol>
<li>Polish page 1 for acceptance</li>
<li>Use the remaining pages to avoid rejection</li>
</ol>
<p>Page 1 has four parts: title, abstract, Figure 1, and introduction. We’ll make them specific, memorable, clear, communicate value, and hook the reader. Reviewers mostly decide accept vs reject by page 1. So we optimize the judgment-before-scroll.</p>
<p>Then, to make sure our paper isn’t rejected, we’ll do due diligence in the rest of it by including stuff like baselines, ablations, statistical significance, and human evaluation.</p>
<p>The tweaks that get the paper accepted—unexpectedly, happily—also improve the actual science contribution. But if you’re tempted to be evil, read this footnote.<sup><a href="#fn2" id="fnref2">02</a></sup> The full rejected and accepted submissions are available for download at the end.</p>
<!-- TBD: Where  Writing (abstract / intro / conclusion): top-down, boring, focused on past work, vague, implying incremental and failed &rarr; specific, interesting, tension/release cycles, difficulty and benefit-motivated, with unique hooks, arguing **value** -->
<!-- Two axes that might not be worth it -->
<!-- These changes can be sorted into two buckets:
1. Completeness --- Baselines, ablations (often also: human evaluations)
2. Clarity --- Everything else. -->
<h2 id="page-1-is-80percent-of-your-paper" tabindex="-1">Page 1 Is 80% of Your Paper </h2>
<blockquote>
<p>A paper has five parts:</p>
<ol>
<li>Title</li>
<li>Figure 1</li>
<li>Abstract</li>
<li>Introduction</li>
<li>Rest of the paper</li>
</ol>
<p>Spend equal time on each of these.</p>
<p>— <em>Me misquoting<sup><a href="#fn3" id="fnref3">03</a></sup> <a href="https://www.youtube.com/watch?v=imEtTnQKt4M">Jitendra Malik quoting Don Geman</a></em></p>
</blockquote>
<p>Around 80% of a paper’s perceived quality is established on page 1. The title, Figure 1, abstract, and half the introduction are all there. It’s like a book’s cover.</p>
<p>Throughout this post, I’ll show the rejected and accepted versions of the paper I mentioned at the top with the dramatic score swing. Here are both page 1s:</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected page 1.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted page 1.</p>
<!-- This isn't totally trivial, because a big reason is "I didn't get it" (which is internalized as "it's boring" and then spelled "low impact" in the review). -->
<p>First, consider page 1’s first impression:<sup><a href="#fn4" id="fnref4">04</a></sup></p>
<ul>
<li>Is the Figure 1 colorful and eye-catching?</li>
<li>Is the title unexpected? Maybe it has one intriguing word?</li>
<li>Are there any curious terms (bolded or italicized)?</li>
<li>Is the introduction (hopefully not) full of citations?<sup><a href="#fn5" id="fnref5">05</a></sup></li>
</ul>
<h3 id="choose-a-specific-memorable-title" tabindex="-1">Choose A Specific Memorable Title </h3>
<p>Rejected: <em>Visually Grounded Comparative Language Generation</em> — too general. Any work that uses pictures and generates comparisons could use this title. I picked this title because I thought it argued for the generality of the method. But a too-general title is off-putting because it comes across as over-claiming. And a big part of our method <em>does</em> rely on our domain: we specifically use a biological taxonomy to create our dataset.</p>
<p>Accepted: <em>Neural Naturalist: Generating Fine-grained Image Comparisons</em> — specific and memorable. In addition to branding (more next), <em>naturalist</em> establishes the domain, and <em>fine-grained</em> narrows the task. Skeptical academics appreciate the clarity of saying what you did. The title is fully unique to our work.<sup><a href="#fn6" id="fnref6">06</a></sup></p>
<h3 id="maybe-add-branding" tabindex="-1">Maybe Add Branding </h3>
<p>I used to dislike branding in papers. It felt presumptuous to claim a proper noun for your research paper and to expect readers to memorize it. And many of the names sound corny.</p>
<p>Now, while I still often feel a pang of annoyance, it is outweighed by the recognition that it’s much easier to remember and discuss concepts which have a name. <em>Neural naturalist</em> or <em>Birds-to-Words</em> instead of “our 2019 EMNLP paper about generating comparative image captions…”</p>
<p>That said, I still dislike throwaway names—those with no conceptual link, or which don’t feel earned. I don’t think every paper needs one. But I think it helped for this paper.</p>
<h3 id="show-screamingly-obvious-value-in-figure-1" tabindex="-1">Show Screamingly Obvious Value in Figure 1 </h3>
<p>The main point is that your paper’s value should be <em>obvious,</em> not that is must be <em>enormous.</em></p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Figure 1.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Figure 1.</p>
<p>A Figure 1 should</p>
<ul>
<li>draw readers in</li>
<li>clearly demonstrate describe both what the work does and its value</li>
<li>be comprehensible without the caption.</li>
</ul>
<p>The old Figure 1 showed two separate comparisons, but the link between them wasn’t clear. The bottom row just all look like owls to a non-expert. And the descriptions are long and boring.</p>
<p>The new Figure 1 makes the works’ focus explicit by anchoring with the same left image, and labeling each comparison with a perceptual difficulty (“high” vs “medium”). It annotates the operation (“vs” = comparison) and the result (“highly detailed” vs “fewer details”). At this point, the paper’s mechanics and unique characteristic has been established: we use different language to compare things based on how similar they look. Finally, to make the long descriptions more approachable and interesting, we’ve highlighted two components (features and parts, with orange underlines and green bubbles).</p>
<p>A problem with making Figure 1s—and describing your research in general—is that you know so much about it, it’s impossible to mentally model what it’d be like to learn about your work for the first time. Spending time away from your work is extremely helpful here, if possible. I think I benefitted by having the conference review period (a month or two?) away from the paper, so I could come back to it with fresh eyes and rethink how best to illustrate it.</p>
<p>I’ve <a href="https://maxwellforbes.com/posts/figure-creation-tutorial-making-a-figure-1/">written about Figure 1s</a> before. Even at the peak of my Figure 1 game, it was normal to make ten drafts before submitting.</p>
<h3 id="end-each-caption-with-the-takeaway" tabindex="-1">End Each Caption with the Takeaway </h3>
<p>I think this is the single best paper-writing hack I’ve ever learned.</p>
<p>This Figure 1 is so information-dense nearly the whole caption is the takeaway (yellow). Compare vs the old caption which, has side note (red) taking nearly 1/3 of the (extremely valuable front-page) real estate!</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Fig 1 caption.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Fig 1 caption.</p>
<p>A takeaway message explains not what is literally being shown in the figure (that comes first), but what you should think about it.</p>
<p>It might feel strange to do this in scientific writing, because it feels like it crosses the boundary from description into interpretation. But I urge you to do it, especially for less formal fields like computer science because:</p>
<ul>
<li>
<p>You’re saving readers time trying to understand what point you are trying to make by just writing it out.<sup><a href="#fn7" id="fnref7">07</a></sup></p>
</li>
<li>
<p>With good captions, you can understand the whole paper by only looking at the figures. Many (most?) future readers will read your paper this way.</p>
</li>
<li>
<p>The scientific reader has a <em>grain of salt</em> mindset about everything you write anyway, so don’t stress about the ‘interpretation’ aspect.</p>
</li>
</ul>
<p>If you aren’t trying to prove a point, well, perhaps reconsider that figure.</p>
<p>I got even more brazen about takeaways in future papers, even writing bolded “Takeaway:” in the caption itself.<sup><a href="#fn8" id="fnref8">08</a></sup></p>
<h3 id="the-abstract:-a-specific-valuable-hook" tabindex="-1">The Abstract: A Specific Valuable Hook </h3>
<p>A classic mistake for a certain type of nerd (e.g., me) is to write top-down, going from general concepts to your specific topic. This is tempting because it feels orderly and taxonomic.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected abstract.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted abstract.</p>
<p>But this turns out terribly, as you can see in the rejected abstract. It’s both boring and feels over-claiming. After top-down framing, and an aside, there’s a ‘betrayal’ of scope when we reveal our actual task.<sup><a href="#fn9" id="fnref9">09</a></sup></p>
<p>Everything is more specific in the revised abstract: what we study, our contributions (dataset and model), all the way to literal descriptions of specific birds and the task done in human evaluations. There’s a results teaser, and a hint of a unique hook. It’s not only more specific, it’s more fun and compelling to read.</p>
<p>You don’t think your reader wants to have fun and read something compelling? Try reviewing conference papers. Enjoyable writing is like water in a desert. Reviewers won’t even realize why they’re happy, the’ll just like the paper. Read <a href="https://arxiv.org/pdf/1804.02767">YOLOv3</a> and tell me you don’t enjoy it.<sup><a href="#fn10" id="fnref10">10</a></sup></p>
<h3 id="use-tensionrelease-cycles-in-the-intro" tabindex="-1">Use Tension/Release Cycles in the Intro </h3>
<p>Can you believe we’re still on page 1? It’s that important.</p>
<p>Here we’re discussing specifically the <em>portion of the introduction visible on page 1.</em> We’re optimizing for what we could call judgment-before-scroll.</p>
<p>My original draft was so bad it’s easy to improve. But if I could write something this bad as a 4th year PhD student, others could too.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-04-intro-acl-annotated.moz90.jpg" loading="lazy" decoding="async" width="855" height="487" data-thumbhash-b64="ckgGFIaAypeXeIebeIh/dvSnhQ==" srcset="https://maxwellforbes.com/assets/eleventyImgs/sq1kFXzrR5-428.jpeg 428w, https://maxwellforbes.com/assets/eleventyImgs/sq1kFXzrR5-855.jpeg 855w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-04-intro-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="828" height="434" data-thumbhash-b64="dhgGFIK4xadqhoZvhoWDwGT15Q==" srcset="https://maxwellforbes.com/assets/eleventyImgs/09sVDo5ypm-414.jpeg 414w, https://maxwellforbes.com/assets/eleventyImgs/09sVDo5ypm-828.jpeg 828w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected page 1 intro. <strong>Bottom:</strong> Accepted page 1 intro.</p>
<p>My original introduction completely lacks any mention of a problem, and is devoid of tension. It begins with a top-down pile of related work, then side-swipes our own paper with negative implications.</p>
<p>The revised introduction launches straightaway into the problem.</p>
<p>It uses tension/release cycles at multiple resolutions to build up the stakes of the problem and the perceived value of solving it. First, at the paragraph-scale: ¶ 1+2 builds up the problem (tension), ¶ 3 presents our solution (release). Then at sentence-scale, unstable language creates tension: <em>“but,” “difficult,” “strain,” “while X, Y,” “unfortunately.”</em></p>
<p>On the backbone of these tension/release cycles, we spend the first two paragraphs setting up our task as being specific, difficult, valuable, and unique. And I really mean each of those adjectives. The final bit of visible text (on page 1) introduces a concrete contribution, our dataset.</p>
<p>I hesitate to recommend a video here because it’s both slightly abstract and eighty minutes long, but <a href="https://www.youtube.com/watch?v=vtIzMaLkCaM">Larry McEnerney’s talk on Effective Writing</a> is the single best material I’ve seen on thinking about your writing. I saw it way after grad school, but I wish I’d seen it during because I spent a lot of time blindly reverse engineering bits of it (on display in this essay). Some relevant key points:</p>
<ol>
<li>All of life before your job, people (teachers) have been paid to read your writing</li>
<li>Now that they’re not, your writing must deliver <em>value</em> (which is often entertainment)</li>
<li>High-value text poses <em>problems</em> with tension-filled language, articulating costs or benefits</li>
</ol>
<p>I didn’t understand this framing (of problem, tension, value) while writing the revision. But in hindsight, it’s shockingly clear how faithfully the improved draft adheres to it.</p>
<h2 id="use-the-rest-of-the-paper-to-avoid-all-reasons-for-rejection" tabindex="-1">Use the Rest of the Paper to Avoid All Reasons for Rejection </h2>
<p>If we’ve done our job, reviewers have now finished reading page 1 and want to accept our paper. Our job now is to let them. How?</p>
<p>Surprise, I have another great two-step process. It uses <a href="https://maxwellforbes.com/garage/thinking-in-reverse/">thinking in reverse</a>:</p>
<ol>
<li>Think of all the reasons a reviewer might reject your paper</li>
<li>Avoid everything in 1.</li>
</ol>
<p>The more obvious reasons for rejection have to do with completeness: “you didn’t compare against method X.” But those are often used as objective crutches to justify a gut decision based on lack of clarity. So we must ensure completeness and also polish up the clarity.</p>
<p>After page 1, the main changes I made are:</p>
<ul>
<li>improving all the figures and tables (clarity)</li>
<li>adding baselines (completeness)</li>
<li>adding ablations (completeness)</li>
<li>rewriting the conclusion (clarity)</li>
</ul>
<p>For reference, other common additions are:</p>
<ul>
<li>human evaluations (completeness)</li>
<li>statistical significance (completeness)</li>
</ul>
<p>The running text is nearly identical. This is great because someone skimming the paper—looking at only figures, tables, and the conclusion—can enjoy all the improvements.</p>
<h3 id="make-figures-dense-and-beautiful" tabindex="-1">Make Figures Dense and Beautiful </h3>
<p>There’s this complicated part of the paper called <em>pivot-branch sampling.</em> I was very excited about it but nobody else cared about it. (I think not even my coauthors, though they were too kind to ever say so).</p>
<p>I had the decency to relegate most of <em>pivot-branch sampling</em> to the appendix, but it has to be mentioned a little bit in the body because it’s in a dataset paper.</p>
<p>Still, the clarity just wasn’t there. Figure 2 was supposed to help, but it didn’t. In the revision, I added some graphics, which helps quickly get the idea across.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected Figure 2.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted Figure 2.</p>
<p>In the rejected version, I thought lighter gray text would be nice because there’s a design rule that you shouldn’t use pure black. But it contrasted weirdly with the paper’s body text, which has a maddeningly adjacent font and <em>is</em> pure black.</p>
<p>In the accepted version, I went with a sans-serif, black text which helped the figure feel solid and distinct. And more importantly, I used the real estate to illustrate a complicated thing with a natural visual (the <em>pivot-branch sampling</em>).</p>
<h3 id="go-ahead-and-invent-a-helpful-taxonomy" tabindex="-1">Go Ahead and Invent a Helpful Taxonomy </h3>
<p>The first reviewers were confused about our dataset. Was it interesting or valuable?</p>
<p>I had shot myself in the foot with crappy writing that situated the contribution as incremental and marginally different (see abstract and intro sections above), but there’s no harm in over-correcting, right?</p>
<p>We first introduced this table—new in the revision—just to contrast example sentences from the most related datasets. This alone would have been great because <a href="https://maxwellforbes.com/posts/use-examples/">examples are densely impactful brain magic</a>.</p>
<p>But one of the biggest brain blasts I had was realizing that I could simply invent helpful axes (circled) along which to compare the datasets.</p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-06-examples-emnlp-only-annotated-v3.moz90.jpg" alt="" loading="lazy" decoding="async" width="826" height="348" data-thumbhash-b64="ORgGC4Knhrh6dgd3d3CK9sg=" srcset="https://maxwellforbes.com/assets/eleventyImgs/1CUouNTKWs-413.jpeg 413w, https://maxwellforbes.com/assets/eleventyImgs/1CUouNTKWs-826.jpeg 826w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Dataset comparison table (new in accepted version).</p>
<p>Not only are examples incredibly helpful to get a flavor of things, the taxonomy I made up helps with quantitative (ish) framing.</p>
<p>Inventing the dataset taxonomy helped free up my brain from imaginary rules. For example, the data citations wouldn’t fit in the table without destroying the alignment. What to do? Well, I simply moved them to the caption. Can you do that? Nobody complained.</p>
<h3 id="sprinkle-in-graphics-for-variety" tabindex="-1">Sprinkle in Graphics for Variety </h3>
<p>A chart helps break up the visual rhythm of a paper. Plus, it can demonstrate a property that’s otherwise hard to grasp. (Here: that we have longer text than other datasets.)</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected dataset stats.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted dataset stats.</p>
<p>Don’t forget, we’re still putting the takeaway message at the end of the caption.</p>
<h3 id="make-your-contribution-shine" tabindex="-1">Make Your Contribution Shine </h3>
<p>I had done a bad job highlighting how interesting the model was. In the revision, I not only drew out the components we ablated (yellow, red), but I used color to link them to the results table later in the paper. As a bonus, we now have warm colors (yellow, red) for the encoder and cool colors (blue, green) for the decoder.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-08-model-acl.moz90.jpg" loading="lazy" decoding="async" width="813" height="356" data-thumbhash-b64="NggGA4KWpraEePiXBm5QwFc=" srcset="https://maxwellforbes.com/assets/eleventyImgs/1mTfhrkpJa-407.jpeg 407w, https://maxwellforbes.com/assets/eleventyImgs/1mTfhrkpJa-813.jpeg 813w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-09-model-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="807" height="455" data-thumbhash-b64="NRgCFIKZ0tuTZpdvm65LgAkiiA==" srcset="https://maxwellforbes.com/assets/eleventyImgs/b0VIsbgV7v-404.jpeg 404w, https://maxwellforbes.com/assets/eleventyImgs/b0VIsbgV7v-807.jpeg 807w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected model figure. <strong>Bottom:</strong> Accepted model figure.</p>
<p>I help the reader out by telling them in advance what configuration of the model works best as the takeaway sentence of the caption. This is another good trick to remember: don’t withhold information to surprise readers. They like to know early and often. I am guilty of this and it’s still a hard habit to break.</p>
<h3 id="delete-stuff-around-the-23-mark" tabindex="-1">Delete Stuff Around the 2/3 Mark </h3>
<p>Several changes above take up more space. Where do we cut?</p>
<p>In an eight-page paper, pages five through seven probably contain good candidates.</p>
<p>Fortunately, we already had a figure with an excessive number of outputs. I’m a <a href="https://maxwellforbes.com/posts/use-examples/#example-outputs">big fan</a> of showing your system’s outputs, so I’d included nine scenarios (i.e., eighteen total photos and paragraphs). This is great, but trimming to six scenarios still leaves plenty. Plus it let us be pickier with which ones were included.<sup><a href="#fn11" id="fnref11">11</a></sup></p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-10-outputs-acl-annotated.moz90.jpg" alt="" loading="lazy" decoding="async" width="835" height="798" data-thumbhash-b64="8hcGD4KYl/aSiHh7hkhnmIeEk3UIc74P" srcset="https://maxwellforbes.com/assets/eleventyImgs/lqFK_OJWcv-418.jpeg 418w, https://maxwellforbes.com/assets/eleventyImgs/lqFK_OJWcv-835.jpeg 835w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Example outputs. Top row removed in the resubmission.</p>
<p>Notice there’s no takeaway sentence here. Rules are guidelines. If the takeaway feels belabored and out-of-place, omit it.</p>
<h3 id="add-everything-you-might-ask-for" tabindex="-1">Add Everything You Might Ask For </h3>
<p>This is where the <em>thinking in reverse</em> part comes in at full force. Think of the most common reviewer complaints and avoid them.</p>
<p>The easiest reasons reviewers could give to reject you were:</p>
<ul>
<li>lack of baselines</li>
<li>lack of ablations</li>
<li>lack of human evaluation</li>
</ul>
<p>So, add those things.</p>

        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-11-results-acl.moz90.jpg" loading="lazy" decoding="async" width="812" height="283" data-thumbhash-b64="OwgCAoCVl4eviMiJAAAAAAA=" srcset="https://maxwellforbes.com/assets/eleventyImgs/zK_7WVSYJw-406.jpeg 406w, https://maxwellforbes.com/assets/eleventyImgs/zK_7WVSYJw-812.jpeg 812w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
        <div>
            <p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-11-results-emnlp-annotated.moz90.jpg" loading="lazy" decoding="async" width="806" height="366" data-thumbhash-b64="9hcGC4J1lqd/d5ealkLvmQs=" srcset="https://maxwellforbes.com/assets/eleventyImgs/NsjqcUwM5N-403.jpeg 403w, https://maxwellforbes.com/assets/eleventyImgs/NsjqcUwM5N-806.jpeg 806w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px">
            </p>
        </div>
        
<p><strong>Top:</strong> Rejected results. <strong>Bottom:</strong> Accepted results.</p>
<p>My favorite part is in the takeaway (yellow), we highlight and explain a weak-looking result (blue).</p>
<p>The baselines and ablations took relatively little work to run and probably improved the actual science contribution (more on that soon).</p>
<p>We already had a dream human evaluation, which is getting people to use the captions for an objective task (i.e., can you pick which animal is which?) rather than scoring them on subjective quality metrics (e.g., how fluent is the text 1–5?). No changes there.</p>
<h3 id="go-a-little-overboard" tabindex="-1">Go a Little Overboard </h3>
<p>Somehow we made space for an enormous table of ablations. Running lots of ablations<sup><a href="#fn12" id="fnref12">12</a></sup> is a luxury of having a small dataset.<sup><a href="#fn13" id="fnref13">13</a></sup></p>
<p><img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-12-ablations-emnlp-only-annotated.moz90.jpg" alt="" loading="lazy" decoding="async" width="823" height="547" data-thumbhash-b64="OggCDYLmyUWfVZdgd/d5Zv6QnKB5" srcset="https://maxwellforbes.com/assets/eleventyImgs/w2Mp4edgr_-412.jpeg 412w, https://maxwellforbes.com/assets/eleventyImgs/w2Mp4edgr_-823.jpeg 823w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"></p>
<p>Ablations table (new in the revision.)</p>
<p>You don’t have to go overboard in the ablations. Just maybe somewhere. In future papers, I went overboard in the appendix. Including lots of information (tastefully) shows that you really care and that you did a lot of work.</p>
<h3 id="the-three-sentence-conclusion" tabindex="-1">The Three-Sentence Conclusion </h3>
<p>To revise the conclusion, distill the advice from the abstract and introduction. Also, remove all the framing. We’re left with a concrete, three-sentence highlight reel.</p>

        
        
<p><span>
<span>Top:</span>
<span>Left:</span>
</span>
Rejected conclusion.<span>
<span>Bottom:</span>
<span>Right:</span>
</span>
Accepted conclusion.</p>
<p>Normal writing advice would say something like this: Write your conclusion using three sentences:</p>
<ol>
<li>What do we do?</li>
<li>Why is it great?</li>
<li>Why does it matter?</li>
</ol>
<p>But check out the rejected conclusion. It (roughly) follows this structure too! The real improvement in the revision is specificity.</p>
<h2 id="the-science-thing-was-improved" tabindex="-1">The Science Thing Was Improved </h2>
<p>After making these mostly aesthetic revisions and seeing the paper accepted with dramatically higher scores, the initial thrill inevitably wore off. I grew more cynical of science. While we had improved the framing of our work, I thought, the core <em>science thing</em> we achieved was the same—the dataset, the model, the human evaluation, and the overall task framing itself (which is the hardest part).</p>
<p>Now, I believe such seemingly-surface dressings actually strengthen the underlying <em>science thing.</em> Let me try to convince you why.</p>
<p>The primary objects of modern science are research papers. Research papers are acts of communication. Few people will actually download and use our dataset. Nobody will download and use our model—they can’t, it’s locked inside Google’s proprietary stack.<sup><a href="#fn14" id="fnref14">14</a></sup> But anyone who reads our paper could learn from what we did, and all the revisions to clarity and completeness improve how much they can learn per minute spent reading. And it’s not just a pace thing, there’s a threshold of clarity that divides <em>learned nothing</em> from <em>got at least one new idea.</em></p>
<p>Science is communication.<sup><a href="#fn15" id="fnref15">15</a></sup> Dramatically improving communication improves the science.</p>
<blockquote>
<p>Aside: The idea of ‘making a reader want to read more’ has an unexpected link to game development. You’d think there’d be no need for such antics in a scientific research paper, yet dull obtuse prose can scare off readers, obscure the message, and deflate the contribution’s impact. Getting readers to the end—at least of page 1—is a necessary goal to optimize for. Just so with game design and ‘hooks.’ Games employ several hooks to draw players along, which might quickly be lumped into: stories build tension, todo lists beg completion, and ‘number goes up.’ Omitting these entirely robs a game of ‘stickiness,’<sup><a href="#fn16" id="fnref16">16</a></sup> leading players to grow bored and stop early. In both papers and games, we must learn to make the object sufficiently engaging so that its consumer is driven to experience the bulk of our creation.</p>
</blockquote>
<h2 id="appendix:-full-pdfs" tabindex="-1">Appendix: Full PDFs </h2>
<p>If you’d like to check out the original, raw PDFs that we submitted, they’re available for download here. The appendices (i.e., supplementary material) are nearly identical, but I’ve also included them for completeness.</p>
<ul>
<li><a href="https://maxwellforbes.com/assets/posts/phd-metagame/ACL_2019___Visually_Grounded_Comparative_Language_Generation.pdf">(Rejected) ACL 2019 submission</a> and <a href="https://maxwellforbes.com/assets/posts/phd-metagame/_SUPPL__ACL_2019___Visually_Grounded_Comparative_Language_Generation.pdf">appendix</a></li>
<li><a href="https://maxwellforbes.com/assets/posts/phd-metagame/EMNLP_2019___Neural_Naturalist_-_submission.pdf">(Accepted) EMNLP 2019 submission</a> and <a href="https://maxwellforbes.com/assets/posts/phd-metagame/EMNLP_2019___Neural_Naturalist__Supplementary_Material_.pdf">appendix</a></li>
</ul>
<section>
<p>Footnotes</p>
<hr>
<ol>
<li id="fn1"><p>Review scores spanned 1–5, with 5 = “consider for best paper,” and 3 = “weak accept.” The conferences were both of equal prestige (ACL and EMNLP respectively). Also, I use “I” for simplicity, but as always, this was work done with coauthors. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>Please do good work before optimizing your paper. I’m assuming in this post that you are doing quality research, and you want it to be published to further your career. You need to get past the gatekeeping reviewers. In other words, please use this process for good and not evil. But if you do use it for evil, it’s not a big deal either. Another ignored paper will be in a conference instead of just on Arxiv. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>I added “Figure 1,” but I stand by my revision. Thanks to Kenneth Marino and David Freire for finding the source of this quote. Jitendra’s talk is great—I watched it after writing the first draft of this and couldn’t believe how much overlap there was! (I never saw his talk, but someone who went told me about that quote.) Also, aside, don’t get hung up on senior advisors thinking they actually spend as much time working on the title as you do writing the rest of the paper. Yes the title is really, really important, but they don’t. Let them think they do. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Its bird’s-eye (ahem) view. <a href="#fnref4">↩︎</a></p>
</li>
<li id="fn5"><p>It’s OK if so, but it’s a different vibe, and probably harder to pull off—more in line with an opinion piece. <a href="#fnref5">↩︎</a></p>
</li>
<li id="fn6"><p>Having now watched Jitendra’s talk (linked in the quote above), he articulates this brilliantly: the title should “evoke the key concept of the paper” and “be memorable.” But my favorite part: “think about it in terms of the conditional entropy;” your title should only be able to describe your paper and no one else’s (at a conference). <a href="#fnref6">↩︎</a></p>
</li>
<li id="fn7"><p>I must point out again that your point will be so obvious to you because it’s why you spent hours making the figure, but a new reader may barely spend enough time looking at your thing to understand what the axes are. Help them out. Even stuff like “higher is better” is helpful unless completely trivial. <a href="#fnref7">↩︎</a></p>
</li>
<li id="fn8"><p>E.g., check this one from <em><a href="https://arxiv.org/pdf/2107.01294">Scarecrow</a> (Dou &amp; me et al., 2022)</em> <img src="https://maxwellforbes.com/assets/posts/phd-metagame/paper-02-aside-takeaway-example.moz90.jpg" alt="" loading="lazy" decoding="async" width="556" height="601" data-thumbhash-b64="9wcGBwBUpfp9hZebeKaKuYeG+Ll7oEcC" srcset="https://maxwellforbes.com/assets/eleventyImgs/2GWTAXXnjl-278.jpeg 278w, https://maxwellforbes.com/assets/eleventyImgs/2GWTAXXnjl-556.jpeg 556w" sizes="(max-width: 30em) 100vw, (max-width: 704px) 100vw, 704px"> This is a great example because the table’s interpretation is so complicated that even I (who wrote it) had forgotten what the takeaway was supposed to be a few years later, and would not have easily rediscovered it.  <a href="#fnref8">↩︎</a></p>
</li>
<li id="fn9"><p>Why does do we feel betrayed? I think because there’s an implicit promise that if you’re talking about something, your paper is going to address it. So if you’re outlining broad swaths of a field, even if in an attempt to just situate your work, it can come across as implying that <em>you’re contributing to</em> this whole grand situation. There’s a delicate balance to strike. Some context in the intro or related work is often necessary. <a href="#fnref9">↩︎</a></p>
</li>
<li id="fn10"><p>As with everything, strike a balance. Engaging writing and very unique hooks—e.g., having the phrases ‘citizen science’ and ‘biodiversity’ in an NLP paper—must come as sprinkles on top of a solid contribution that appropriately satisfies the community’s expectations. <a href="#fnref10">↩︎</a></p>
</li>
<li id="fn11"><p>I think the other place we saved the most space was in the qualitative analysis. I could probably write eight pages of only qualitative model analysis, so I always end up with too much in the first draft. <a href="#fnref11">↩︎</a></p>
</li>
<li id="fn12"><p>The blind bolding of higher numbers without statistical significance tests is truly heinous, I know. I hope somebody has standardized tests that you run on output metrics by now to do this. (Just kidding, I’m sure they haven’t.) <a href="#fnref12">↩︎</a></p>
</li>
<li id="fn13"><p>Also, being somewhere like Google. DeepMind wasn’t busy with the TPUs that week so we added a bunch of flags and let them go brrr. But the dataset is so small that by the time Google’s ancient behemoth cluster system had made a dashboard where I could see how the run was going, it had already ran over the whole training dataset (potentially many times, memory is failing me). <a href="#fnref13">↩︎</a></p>
</li>
<li id="fn14"><p>Even if it were open source, let me tell you from first-hand experience that getting someone’s research code to run is no small feat, especially under even marginally different conditions. <a href="#fnref14">↩︎</a></p>
</li>
<li id="fn15"><p>See <a href="https://maxwellforbes.com/posts/dont-try-to-reform-science/">Science 1 vs Science 2</a> in this essay series for more of this argument. <a href="#fnref15">↩︎</a></p>
</li>
<li id="fn16"><p>On the other hand, leaning too hard into them and using darker patterns (like gambling mechanics) can cause addiction (and bankruptcy). <a href="#fnref16">↩︎</a></p>
</li>
</ol>
</section>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini Diffusion (804 pts)]]></title>
            <link>https://simonwillison.net/2025/May/21/gemini-diffusion/</link>
            <guid>44057820</guid>
            <pubDate>Thu, 22 May 2025 01:13:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/21/gemini-diffusion/">https://simonwillison.net/2025/May/21/gemini-diffusion/</a>, See on <a href="https://news.ycombinator.com/item?id=44057820">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><strong><a href="https://deepmind.google/models/gemini-diffusion/">Gemini Diffusion</a></strong>. Another of the announcements from Google I/O yesterday was Gemini Diffusion, Google's first LLM to use diffusion (similar to image models like Imagen and Stable Diffusion) in place of transformers.</p>
<p>Google describe it like this:</p>
<blockquote>
<p>Traditional autoregressive language models generate text one word – or token – at a time. This sequential process can be slow, and limit the quality and coherence of the output.</p>
<p>Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.</p>
</blockquote>
<p>The key feature then is <em>speed</em>. I made it through the waitlist and tried it out just now and <em>wow</em>, they are not kidding about it being fast.</p>
<p>In this video I prompt it with "Build a simulated chat app" and it responds at 857 tokens/second, resulting in an interactive HTML+JavaScript page (embedded in the chat tool, Claude Artifacts style) within single digit seconds.</p>


<p>The performance feels similar to <a href="https://simonwillison.net/2024/Oct/31/cerebras-coder/">the Cerebras Coder tool</a>, which used Cerebras to run Llama3.1-70b at around 2,000 tokens/second.</p>
<p>How good is the model? I've not seen any independent benchmarks yet, but Google's landing page for it promises "the performance of Gemini 2.0 Flash-Lite at 5x the speed" so presumably they think it's comparable to Gemini 2.0 Flash-Lite, one of their least expensive models.</p>
<p>Prior to this the only commercial grade diffusion model I've encountered is <a href="https://www.inceptionlabs.ai/introducing-mercury">Inception Mercury</a> back in February this year.</p>
<p><strong>Update</strong>: a correction from <a href="https://news.ycombinator.com/item?id=44057820#44057939">synapsomorphy on Hacker News</a>:</p>
<blockquote>
<p>Diffusion isn't in place of transformers, it's in place of autoregression. Prior diffusion LLMs like <a href="https://www.inceptionlabs.ai/introducing-mercury">Mercury</a> still use a transformer, but there's no causal masking, so the entire input is processed all at once and the output generation is obviously different. I very strongly suspect this is also using a transformer.</p>
</blockquote>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Display any CSV file as a searchable, filterable, pretty HTML table (239 pts)]]></title>
            <link>https://github.com/derekeder/csv-to-html-table</link>
            <guid>44057612</guid>
            <pubDate>Thu, 22 May 2025 00:31:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/derekeder/csv-to-html-table">https://github.com/derekeder/csv-to-html-table</a>, See on <a href="https://news.ycombinator.com/item?id=44057612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CSV to HTML Table</h2><a id="user-content-csv-to-html-table" aria-label="Permalink: CSV to HTML Table" href="#csv-to-html-table"></a></p>
<p dir="auto">Display any CSV file as a searchable, filterable, pretty <a href="https://www.scaler.com/topics/html/tables-in-html/" rel="nofollow">HTML table</a>. Done in 100% JavaScript.</p>
<p dir="auto">Check out the working demo: <a href="https://csv-to-html-table.netlify.app/" rel="nofollow">https://csv-to-html-table.netlify.app/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/919583/112696463-d7ddd000-8e53-11eb-8f0e-084794450943.png"><img src="https://user-images.githubusercontent.com/919583/112696463-d7ddd000-8e53-11eb-8f0e-084794450943.png" alt="Screen Shot 2021-03-26 at 4 53 39 PM"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Clone this repository (in the command line)</h4><a id="user-content-1-clone-this-repository-in-the-command-line" aria-label="Permalink: 1. Clone this repository (in the command line)" href="#1-clone-this-repository-in-the-command-line"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:derekeder/csv-to-html-table.git
cd csv-to-html-table"><pre>git clone git@github.com:derekeder/csv-to-html-table.git
<span>cd</span> csv-to-html-table</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Add your CSV file to the <code>data/</code> folder</h4><a id="user-content-2-add-your-csv-file-to-the-data-folder" aria-label="Permalink: 2. Add your CSV file to the data/ folder" href="#2-add-your-csv-file-to-the-data-folder"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. In <code>index.html</code> set your options in the <code>CsvToHtmlTable.init()</code> function</h4><a id="user-content-3-in-indexhtml-set-your-options-in-the-csvtohtmltableinit-function" aria-label="Permalink: 3. In index.html set your options in the CsvToHtmlTable.init() function" href="#3-in-indexhtml-set-your-options-in-the-csvtohtmltableinit-function"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="<script>
  CsvToHtmlTable.init({
    csv_path: 'data/Health Clinics in Chicago.csv', 
    element: 'table-container', 
    allow_download: true,
    csv_options: {separator: ',', delimiter: '&quot;'},
    datatables_options: {&quot;paging&quot;: false}
  });
</script>"><pre><span>&lt;</span><span>script</span><span>&gt;</span>
  <span>CsvToHtmlTable</span><span>.</span><span>init</span><span>(</span><span>{</span>
    <span>csv_path</span>: <span>'data/Health Clinics in Chicago.csv'</span><span>,</span> 
    <span>element</span>: <span>'table-container'</span><span>,</span> 
    <span>allow_download</span>: <span>true</span><span>,</span>
    <span>csv_options</span>: <span>{</span><span>separator</span>: <span>','</span><span>,</span> <span>delimiter</span>: <span>'"'</span><span>}</span><span>,</span>
    <span>datatables_options</span>: <span>{</span><span>"paging"</span>: <span>false</span><span>}</span>
  <span>}</span><span>)</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Available options</h5><a id="user-content-available-options" aria-label="Permalink: Available options" href="#available-options"></a></p>
<ul dir="auto">
<li><code>csv_path</code> Path to your CSV file.</li>
<li><code>element</code> The HTML element to render your table to. Defaults to <code>table-container</code></li>
<li><code>allow_download</code> if true, shows a link to download the CSV file. Defaults to <code>false</code></li>
<li><code>csv_options</code> jQuery CSV configuration. Use this if you want to use a custom <code>delimiter</code> or <code>separator</code> in your input file. See <a href="https://code.google.com/p/jquery-csv/wiki/API#$.csv.toArrays%28%29" rel="nofollow">their documentation</a>.</li>
<li><code>datatables_options</code> DataTables configuration. See <a href="http://datatables.net/reference/option/" rel="nofollow">their documentation</a>.</li>
<li><code>custom_formatting</code> <strong>New!</strong> A list of column indexes and custom functions to format your data (see below)</li>
</ul>
<p dir="auto"><h5 tabindex="-1" dir="auto">Custom formatting</h5><a id="user-content-custom-formatting" aria-label="Permalink: Custom formatting" href="#custom-formatting"></a></p>
<p dir="auto">If you want to do custom formatting for one or more column, you can pass in an array of arrays containing the index of the column and a custom function for formatting it. You can pass in multiple formatters and they will be executed in order.</p>
<p dir="auto">The custom functions must take in one parameter (the value in the cell) and return a HTML string:</p>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script>

  //my custom function that creates a hyperlink
  function format_link(link){
    if (link)
      return &quot;<a href='&quot; + link + &quot;' target='_blank'>&quot; + link + &quot;</a>&quot;;
    else
      return &quot;&quot;;
  }

  //initializing the table
  CsvToHtmlTable.init({
    csv_path: 'data/Health Clinics in Chicago.csv', 
    element: 'table-container', 
    allow_download: true,
    csv_options: {separator: ',', delimiter: '&quot;'},
    datatables_options: {&quot;paging&quot;: false},
    custom_formatting: [[4, format_link]] //execute the function on the 4th column of every row
  });
</script>"><pre><span>&lt;</span><span>script</span><span>&gt;</span>

  <span>//my custom function that creates a hyperlink</span>
  <span>function</span> <span>format_link</span><span>(</span><span>link</span><span>)</span><span>{</span>
    <span>if</span> <span>(</span><span>link</span><span>)</span>
      <span>return</span> <span>"&lt;a href='"</span> <span>+</span> <span>link</span> <span>+</span> <span>"' target='_blank'&gt;"</span> <span>+</span> <span>link</span> <span>+</span> <span>"&lt;/a&gt;"</span><span>;</span>
    <span>else</span>
      <span>return</span> <span>""</span><span>;</span>
  <span>}</span>

  <span>//initializing the table</span>
  <span>CsvToHtmlTable</span><span>.</span><span>init</span><span>(</span><span>{</span>
    <span>csv_path</span>: <span>'data/Health Clinics in Chicago.csv'</span><span>,</span> 
    <span>element</span>: <span>'table-container'</span><span>,</span> 
    <span>allow_download</span>: <span>true</span><span>,</span>
    <span>csv_options</span>: <span>{</span><span>separator</span>: <span>','</span><span>,</span> <span>delimiter</span>: <span>'"'</span><span>}</span><span>,</span>
    <span>datatables_options</span>: <span>{</span><span>"paging"</span>: <span>false</span><span>}</span><span>,</span>
    <span>custom_formatting</span>: <span>[</span><span>[</span><span>4</span><span>,</span> <span>format_link</span><span>]</span><span>]</span> <span>//execute the function on the 4th column of every row</span>
  <span>}</span><span>)</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto">Note that you should take care about HTML escaping to avoid <a href="https://www.owasp.org/index.php/Cross-site_Scripting_(XSS)" rel="nofollow">XSS</a> or broken layout.
jQuery has a nice function <a href="https://api.jquery.com/text/" rel="nofollow">text()</a> which safely escapes HTML from value.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Run it</h4><a id="user-content-4-run-it" aria-label="Permalink: 4. Run it" href="#4-run-it"></a></p>
<p dir="auto">You can run this locally using this handy python command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m SimpleHTTPServer"><pre>python -m SimpleHTTPServer</pre></div>
<p dir="auto">...or with Python 3:</p>

<p dir="auto">navigate to <a href="http://localhost:8000/" rel="nofollow">http://localhost:8000/</a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">5. Deploy it</h4><a id="user-content-5-deploy-it" aria-label="Permalink: 5. Deploy it" href="#5-deploy-it"></a></p>
<p dir="auto"><strong>GitHub pages</strong> You can host your table on GitHub pages for free! Once you've made all your changes and committed them, push everything in the <code>master</code> branch to <code>gh-pages</code> which automatically enables GitHub pages.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git push origin master:gh-pages"><pre>git push origin master:gh-pages</pre></div>
<p dir="auto">Then navigate to <a href="http://your-github-username.github.io/csv-to-html-table/" rel="nofollow">http://your-github-username.github.io/csv-to-html-table/</a></p>
<p dir="auto">Read more on working with <a href="https://help.github.com/articles/user-organization-and-project-pages/#project-pages">GitHub pages projects</a>.</p>
<p dir="auto"><strong>Web server</strong> This project should work on any web server. Upload this entire project (including all the <code>css</code>, <code>data</code>, <code>fonts</code> and <code>js</code> folders) to a public folder on your server using FTP.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">6. iframe it (optional)</h4><a id="user-content-6-iframe-it-optional" aria-label="Permalink: 6. iframe it (optional)" href="#6-iframe-it-optional"></a></p>
<p dir="auto">Want to embed your nifty table on your website? You can use an <a href="http://www.w3schools.com/tags/tag_iframe.asp" rel="nofollow">iframe</a>. Once you've deployed your table (above in step 5) you can link to it in an iframe right in your HTML.</p>
<div dir="auto" data-snippet-clipboard-copy-content="<iframe style=&quot;border-style: none;&quot; src=&quot;http://derekeder.github.io/csv-to-html-table/&quot; height=&quot;950&quot; width=&quot;600&quot;></iframe>"><pre><span>&lt;</span><span>iframe</span> <span>style</span>="<span>border-style: none;</span>" <span>src</span>="<span>http://derekeder.github.io/csv-to-html-table/</span>" <span>height</span>="<span>950</span>" <span>width</span>="<span>600</span>"<span>&gt;</span><span>&lt;/</span><span>iframe</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<ul dir="auto">
<li><a href="http://getbootstrap.com/" rel="nofollow">Bootstrap 4</a> - Responsive HTML, CSS and Javascript framework</li>
<li><a href="https://jquery.com/" rel="nofollow">jQuery</a> - a fast, small, and feature-rich JavaScript library</li>
<li><a href="https://github.com/evanplaice/jquery-csv/">jQuery CSV</a> - Parse CSV (Comma Separated Values) to Javascript arrays or dictionaries.</li>
<li><a href="http://datatables.net/" rel="nofollow">DataTables</a> - add advanced interaction controls to any HTML table.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Common issues/troubleshooting</h2><a id="user-content-common-issuestroubleshooting" aria-label="Permalink: Common issues/troubleshooting" href="#common-issuestroubleshooting"></a></p>
<p dir="auto">If your table isn't displaying any data, try the following:</p>
<ol dir="auto">
<li>Use the <a href="https://developers.google.com/chrome-developer-tools/docs/console" rel="nofollow">Chrome developer console</a> or install <a href="http://getfirebug.com/" rel="nofollow">Firebug</a> for FireFox. This will allow you to debug your javascript.</li>
<li>Open your table in the browser and open the javascript console
<ul dir="auto">
<li>Chrome developer console on a Mac: Option+Command+J</li>
<li>Chrome developer console on a PC: Control+Shift+J</li>
<li>Firebug in Firefox: Tools =&gt; Web Developer =&gt; Firebug =&gt; Open Firebug)</li>
</ul>
</li>
<li>If you do see javascript errors, the error will tell you what line it is failing on. Best to start by going there!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Errors / Bugs</h2><a id="user-content-errors--bugs" aria-label="Permalink: Errors / Bugs" href="#errors--bugs"></a></p>
<p dir="auto">If something is not behaving intuitively, it is a bug, and should be reported.
Report it here: <a href="https://github.com/derekeder/csv-to-html-table/issues">https://github.com/derekeder/csv-to-html-table/issues</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<ul dir="auto">
<li><a href="http://derekeder.com/" rel="nofollow">Derek Eder</a> - primary contributor</li>
<li><a href="https://github.com/ychaouche">ychaouche</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/30" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/30/hovercard">javascript tag fixes</a></li>
<li><a href="https://github.com/b-meson">Freddy Martinez</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/17" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/17/hovercard">localized javascript libraries</a></li>
<li><a href="https://github.com/stokito">Sergey Ponomarev</a> - <a href="https://github.com/derekeder/csv-to-html-table/pull/60" data-hovercard-type="pull_request" data-hovercard-url="/derekeder/csv-to-html-table/pull/60/hovercard">CSV escaped in HTML output</a></li>
<li><a href="https://github.com/djibe">djibe</a> - <a href="https://github.com/djibe/csv-to-html-table">Bootstrap 4 and latest DataTables</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Note on Patches/Pull Requests</h2><a id="user-content-note-on-patchespull-requests" aria-label="Permalink: Note on Patches/Pull Requests" href="#note-on-patchespull-requests"></a></p>
<ul dir="auto">
<li>Fork the project.</li>
<li>Make your feature addition or bug fix.</li>
<li>Send a pull request. Bonus points for topic branches.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Copyright</h2><a id="user-content-copyright" aria-label="Permalink: Copyright" href="#copyright"></a></p>
<p dir="auto">Copyright (c) 2018 Derek Eder. Released under the <a href="https://github.com/derekeder/csv-to-html-table/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Should I Block ICMP? (117 pts)]]></title>
            <link>http://shouldiblockicmp.com/</link>
            <guid>44057219</guid>
            <pubDate>Wed, 21 May 2025 23:17:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://shouldiblockicmp.com/">http://shouldiblockicmp.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44057219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div>
                        
                        <h2>No!!</h2>
                        </div>

                    <div id="TheProblem">
                        <h4>The Problem</h4>
                        <p>Many network administrators feel that ICMP is a security risk, and should therefore always be blocked
                        at the firewall.  It is true that ICMP does have some security issues associated with it, and that a lot
                        of ICMP should be blocked.  But this is no reason to block all ICMP traffic!</p>
                        <p>ICMP has many important features; some are useful for troubleshooting, while some are essential for a
                        network to function correctly.  Here are details of some of the important ICMP traffic that you should
                        know about, and consider allowing through your network.</p>
                        <br>
                    </div>

                    <div id="EchoReqReply">
                        <h4>Echo Request and Echo Reply<br>
                            <small>IPv4 - Echo Request (Type8, Code0) and Echo Reply (Type0, Code0)<br>
                                IPv6 - Echo Request (Type128, Code0) and Echo Reply (Type129, Code0)</small>
                        </h4>
                        <p>We all know these ones - ping is one of the first troubleshooting tools that we all learn.  Yes, if
                        you enable it, it means that your host is now discoverable - but wasn't your web server already
                        listening on port 80 anyway?  Sure, block this if you really want at your border to your DMZ, but
                        blocking ping traffic inside your network isn't going to get you much, except harder troubleshooting
                        ("Can you ping your default gateway?", "No, but I never can, so that doesn't tell me anything!").</p>
                        <p>Remember you can also allow this with a given direction in mind; you could decide to let Echo
                        Requests out from your network to the Internet, and Echo Replies from the Internet to your network, but
                        not vice versa.</p>
                        <br>
                    </div>

                    <div id="FragNeed">
                        <h4>Fragmentation Needed (IPv4) / Packet Too Big (IPv6)<br>
                            <small>IPv4 - (Type3, Code4)<br>
                                IPv6 - (Type2, Code0)</small>
                        </h4>
                        <p>These ones are important.  VERY important.  They are an essential component in 
                        <a href="http://en.wikipedia.org/wiki/Path_MTU_Discovery">Path MTU Discovery</a> (PMTUD), which is an
                        essential part of TCP that allows two hosts to adjust their 
                        <a href="http://en.wikipedia.org/wiki/Maximum_segment_size">TCP Maximum Segment Size</a> (MSS) value to
                        one that will fit in the smallest 
                        <a href="http://en.wikipedia.org/wiki/Maximum_transmission_unit">MTU</a> along the path of links
                        between the two hosts.  If two hosts have a smaller MTU than their own local link on the path between
                        them, and have no means of discovering this, traffic gets silently black-holed; in other words, "you're
                        gonna have a bad time".</p>
                        <p>IPv4 packets with the 
                        <a href="http://en.wikipedia.org/wiki/IPv4#Flags">DF bit</a> set (that's most of them!), or IPv6 packets
                        (remember there's no fragmentation by routers in IPv6), that are too large for a router to transmit
                        across an interface will result in that router dropping the packet, and generating a Fragmentation
                        Needed / Packet Too Big ICMP error back to the source, which also contains the MTU of the too-small
                        link.  If this error cannot get through to the sender, then the sender will just interpret the lack of
                        ACKs from the receiver as congestion/loss and re-transmit, which of course will also get dropped.  This
                        kind of behaviour is tough to troubleshoot because of course the TCP handshakes all work fine, as they
                        are small packets, but then the session appears to stall as soon as any bulk data transmission
                        occurs.</p>
                        <p><a href="https://www.ietf.org/rfc/rfc4821.txt">RFC 4821</a> was developed to help hosts get around
                        this problem using Packetization Layer Path MTU Discovery (PLPMTUD), which discovers the path MTU by
                        incrementally increasing the MSS to try to find a suitable value for the path.  This removes the
                        dependency on ICMP, and is available in most OS network stacks, but is not as efficient as learning
                        directly what the maximum MTU should be.  So please just allow these ICMP messages through in the first
                        place, okay?</p>
                        <br>
                    </div>

                    <div id="TimeExc">
                        <h4>Time Exceeded<br>
                            <small>IPv4 - (Type11, Code0)<br>
                                IPv6 - (Type3, Code0)</small>
                        </h4>
                        <p>Traceroute is a very useful tool for troubleshooting network connections between two hosts, detailing
                        each hop on the path.  It does this by sending a packet with a TTL of 1 so that the first hop sends back
                        a Time Exceeded message (including its own source IP), then sending a packet with a TTL of 2, and so on,
                        to discover each hop on the path.</p>
                        <p>Remember when you've run it before, and you get a hop or two that can't be discovered in the middle
                        of your trace?  Or even worse, you try to traceroute to a host and *every* hop can't be discovered.
                        Annoying, right?  That's because the person running those routers (or your local firewall) decided to
                        block ICMP Time Exceeded messages.  Don't be that guy, okay?</p>
                        <br>
                    </div>

                    <div id="NDP">
                        <h4>NDP and SLAAC (IPv6)<br>
                        <small>Router Solicitation (RS) (Type133, Code0)<br>
                            Router Advertisement (RA) (Type134, Code0)<br>
                            Neighbour Solicitation (NS) (Type135, Code0)<br>
                            Neighbour Advertisement (NA) (Type136, Code0)<br>
                            Redirect (Type137, Code0)</small>
                        </h4>
                        <p>While IPv4 used 
                        <a href="http://en.wikipedia.org/wiki/Address_Resolution_Protocol">Address Resolution Protocol</a> (ARP)
                        for layer 2 to 3 mappings, IPv6 takes a different approach, in the form of 
                        <a href="http://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">Neighbour Discovery Protocol</a>
                        (NDP).  NDP provides many functions, including router discovery, prefix discovery, address resolution,
                        and many more besides.  In addition to NDP, 
                        <a href="http://en.wikipedia.org/wiki/IPv6_address#Stateless_address_autoconfiguration">StateLess Address AutoConfiguration</a> 
                        (SLAAC) allows a host to be dynamically configured on the network, similar in concept to DHCP (although
                        DHCPv6 does exist for finer-grained control).</p>
                        <p>These five ICMP types should be permitted within your network (not across your border) in order for
                        these features of IPv6 to function correctly.</p>
                        <br>
                    </div>

                    <div id="RateLim">
                        <h4>A Word About Rate Limiting</h4>
                        <p>While ICMP messages like the ones covered on this page can be very useful, remember that generating
                        all of these messages takes CPU time on your routers, and generates traffic.  Do you really expect that
                        you should be getting 1000 pings a second through your firewall in a normal situation?  Would that be
                        considered legitimate traffic if you saw it?  Nope, probably not.  Rate limit all of these ICMP traffic
                        types as you see fit for your network; it's a good line of defence that should not be ignored.</p>
                        <br>
                    </div>

                    <div id="Read">
                        <h4>Read, Research, Understand</h4>
                        <p>Given that the "to block or not to block" discussion for ICMP seems to always result in confusion,
                        anger, and borderline fanatical disagreements, go ahead and read up on the topic yourself.  Spend time
                        understanding it as fully as you can; there are plenty of links throughout this page alone.  Then you
                        can form your own opinion and make an informed choice about what is best for your network.</p>
                        <br>
                    </div>

                    
                    <!-- Ad unit 1 -->
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I have tinnitus. I don't recommend it (165 pts)]]></title>
            <link>https://blog.greg.technology/2025/05/20/tinnitus.html</link>
            <guid>44057044</guid>
            <pubDate>Wed, 21 May 2025 22:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.greg.technology/2025/05/20/tinnitus.html">https://blog.greg.technology/2025/05/20/tinnitus.html</a>, See on <a href="https://news.ycombinator.com/item?id=44057044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody"><p>i went to a show in november of last year (it’s may right now) and ever since, i have tinnitus. i don’t recommend it.</p>

<p>i’ve been going to electronic music shows for a while and had been very careless with my ears - because it felt like i would always be okay. after the show, especially in the morning, you’d have the next-day tinnitus, but then it would go away.</p>

<p>of course, this one never went away. was i closer to the speakers that time, was the music louder, was it something about the frequencies. was it all of the accumulated careless listening.</p>

<p>i’ll say three things:</p>

<ol>
  <li>
    <p>if you attended a show at a venue that had cool lights, but it turned out that some people lost their sight because of the cool lasers there, the venue would be shut down, there would be an investigation, there would be lawsuits, etc.</p>

    <p>however, if you go to a venue today and the sound is so loud that people become permanently disabled because of that… as far as i know, nothing happens. why?</p>
  </li>
  <li>
    <p>i’ve been weirdly lucky in that - in addition to the tinnitus - it now physically hurts when there’s a loud sound. loud sounds didn’t use to hurt - so now, i am one of those people that plugs their ears when an emergency vehicle goes by with the siren blaring. plugging ears used to feel ridiculous to me (like, come on, it’s just a siren), but now it hurts and i get it.</p>

    <p>so there’s a blessing there of course (i <em>feel</em> when a sound is “too” loud), but i still wish i didn’t have tinnitus.</p>
  </li>
  <li>
    <p>some of my argentian friends and i joke about my very “dad” ways around them (it’s a love language, what do you want from me): i remind them to wear their helmets, and i myself have started to wear a reflective vest (in addition to a helmet) when biking. they call this “seguridad” which is both spanish for security and has “dad” at the end which is very funny. it’s a good nickname.</p>

    <p>so yeah, just to be that seguridad on you - do wear something in your ears when going to shows. do ride with a helmet. and absolutely don’t fuck around with lasers, especially the stupid green ones that people wave around. might as well wave an ak-47 around.</p>
  </li>
</ol>

<p>it is frighteningly easy to permanently injure yourself and become disabled. and it will leave you with sadness and regret and you will keep wondering - was it inevitable. you might as well protect yourself.</p>

<p>xx</p>
</div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ITXPlus: A ITX Sized Macintosh Plus Logicboard Reproduction (119 pts)]]></title>
            <link>https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/</link>
            <guid>44056659</guid>
            <pubDate>Wed, 21 May 2025 21:52:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/">https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/</a>, See on <a href="https://news.ycombinator.com/item?id=44056659">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-xf-init="lightbox select-to-quote" data-message-selector=".js-post" data-lb-id="thread-49715" data-lb-universal="1">
			
				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-559793" id="js-post-559793">

		<span id="post-559793"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559793" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559793/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559793" rel="nofollow">
						#1
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559793" data-lb-caption-desc="max1zzz · Apr 16, 2025 at 12:04 AM">

		
			

	

		

		<article>
			
				
			
			
				<div><p>I haven't done project thread in a while as writing is not my favourite past time but people seem to enjoy them so here it goes</p><p>

ITXPlus is a Mini-ITX sized Macintosh Plus logicboard clone that can be be assembled to a working state with no original parts and is intended for new build systems in modern cases</p><p>

ITXPlus features a onboard VGA output using GuruThree's Pico based video converter, power from standard 24pin ATX power supply, a onboard 50pin internal scsi header and 4MB of solderd RAM</p><p>

 ITXPlus also makes used of DosFox's discreet replacement for the Sony Sound IC and my implementation of his SCHWIM IWM bypass in a PLD as well as pgreenland / quortan's ATTiny based RTC replacement and Porchy / Hkz / Bolle's reverse engineerings of the Macintosh Plus PAL's </p><p>

(I hope I haven't forgotten to mention anyone's elses work I have used....)</p><p>

As it is designed to use a SCHWIM based IWM bypass it will not support floppy drives out of the box however a expansion heder is provided with all the signals required to connect a real IWM should you want to do this</p><p>

With execption of the 68000, connectors and a few other little bits the board is fully surface mount, I almost went with a surface mount 68000 too  but was convinced otherwise as it means the nice gold capped ceramic 68000's can be used <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"></p><p>

The board will look something roughly like this:<br>
<a href="https://68kmla.org/bb/index.php?attachments/1744757542915-png.85569/" target="_blank"><img src="https://68kmla.org/bb/data/attachments/85/85623-f2d67a1366a3f49163df6927c22655b2.jpg" alt="1744757542915.png" title="1744757542915.png" width="150" height="153" loading="lazy"></a><br>
Though as you can see there is still most of the routeing work to do so things may get moved around a little</p><p>

Finally to answer the inevitible question: Why base it on the Plus? why not the IIci? or the SE/30? I chose to base it on the plus simply because it can be built with no original parts, this is intended to be a project build not the fastest possible mac out there.</p><p>

When finished the design will be fully open source and posted to my GitHub <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"></p></div>
			
			
			
				
			
		</article>

		
			

	

		

		
			
	

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="cheesestraws" data-content="post-559795" id="js-post-559795">

		<span id="post-559795"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559795" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559795/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559795" rel="nofollow">
						#2
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559795" data-lb-caption-desc="cheesestraws · Apr 16, 2025 at 12:18 AM">

		

		<article>
			
				
			
			
				<p>I've been really looking forward to the first 'no original parts' board build.  This is a milestone and a half.  Well done.</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="luRaichu" data-content="post-559797" id="js-post-559797">

		<span id="post-559797"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559797" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559797/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559797" rel="nofollow">
						#3
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559797" data-lb-caption-desc="luRaichu · Apr 16, 2025 at 12:33 AM">

		

		<article>
			
				
			
			
				<p>Next: A Macintosh Plus built without original parts. I'm sure they're still making b/w CRTs somewhere in China</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-559798" id="js-post-559798">

		<span id="post-559798"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559798" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559798/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559798" rel="nofollow">
						#4
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559798" data-lb-caption-desc="max1zzz · Apr 16, 2025 at 12:46 AM">

		

		<article>
			
				
			
			
				<div><blockquote data-attributes="member: 19339" data-quote="cheesestraws" data-source="post: 559795">
	
		
	
	<div>
		
		<p>
			I've been really looking forward to the first 'no original parts' board build.  This is a milestone and a half.  Well done.
		</p>
		
	</div>
</blockquote><p>Thanks <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"> Though strictly speaking I think DosFox is the first one to do a "no original parts" build - he built a clone board based using my original Plus Reloaded board and most of the other bits mentioned above</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="cheesestraws" data-content="post-559799" id="js-post-559799">

		<span id="post-559799"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559799" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559799/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559799" rel="nofollow">
						#5
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559799" data-lb-caption-desc="cheesestraws · Apr 16, 2025 at 1:08 AM">

		

		<article>
			
				
			
			
				<p>Sorry, I meant 'design', as in actually a new board with new bits on.</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="Gothikon" data-content="post-559813" id="js-post-559813">

		<span id="post-559813"></span>

		
			
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-559952" id="js-post-559952">

		<span id="post-559952"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559952" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559952/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559952" rel="nofollow">
						#7
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559952" data-lb-caption-desc="max1zzz · Apr 18, 2025 at 12:15 AM">

		

		<article>
			
				
			
			
				<div><p>Getting there! Still quite a bit to do but it's finally at the point there are more red and green lines than white lines <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"></p><p>

<a href="https://68kmla.org/bb/index.php?attachments/1744931687273-png.85631/" target="_blank"><img src="https://68kmla.org/bb/data/attachments/85/85685-bd918627e2d26002f99cb91c33bc85f7.jpg" alt="1744931687273.png" title="1744931687273.png" width="158" height="150" loading="lazy"></a></p></div>
			
			
			
				
			
		</article>

		

		
			
	

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="mattsoft" data-content="post-559978" id="js-post-559978">

		<span id="post-559978"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559978" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/559978/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-559978" rel="nofollow">
						#8
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-559978" data-lb-caption-desc="mattsoft · Apr 18, 2025 at 5:31 AM">

		

		<article>
			
				
			
			
				<p>cool idea and cool project!</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560111" id="js-post-560111">

		<span id="post-560111"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560111" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560111/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560111" rel="nofollow">
						#9
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560111" data-lb-caption-desc="max1zzz · Apr 19, 2025 at 1:58 PM">

		

		<article>
			
				
			
			
				<p>MLB is done! Just need to draw up the riser with the keyboard / mouse /  serial ports on it</p>
			
			
			
				
			
		</article>

		

		
			
	
		
		<section>
			
			<ul>
				
					
						
	<li>
		<a id="attachment-85692"></a>
		
			
	


			
<a data-lb-sidebar-href="" data-lb-caption-extra-html="" href="https://68kmla.org/bb/index.php?attachments/mlb-jpg.85692/" target="_blank">
				<img src="https://68kmla.org/bb/data/attachments/85/85746-6b65534f25b32859793f284663676d7a.jpg" alt="MLB.JPG" width="151" height="150" loading="lazy">
			</a>
		

		<div>
				<p><span title="MLB.JPG">MLB.JPG</span></p><p>
					345.6 KB
					
					· Views: 83
					
				</p>
			</div>
	</li>

					
						
	<li>
		<a id="attachment-85693"></a>
		
			
	


			
<a data-lb-sidebar-href="" data-lb-caption-extra-html="" href="https://68kmla.org/bb/index.php?attachments/mlb_3d-jpg.85693/" target="_blank">
				<img src="https://68kmla.org/bb/data/attachments/85/85747-00f1a00a36631b023f1dc34ba6a66235.jpg" alt="MLB_3D.JPG" width="150" height="150" loading="lazy">
			</a>
		

		<div>
				<p><span title="MLB_3D.JPG">MLB_3D.JPG</span></p><p>
					112.8 KB
					
					· Views: 77
					
				</p>
			</div>
	</li>

					
				
			</ul>
		</section>
	

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="joshc" data-content="post-560119" id="js-post-560119">

		<span id="post-560119"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560119" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560119/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560119" rel="nofollow">
						#10
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560119" data-lb-caption-desc="joshc · Apr 19, 2025 at 4:11 PM">

		

		<article>
			
				
			
			
				<div><p>This is cool. I don’t need it but I want to build one… this seems like a great way to get a Plus without having another compact to find space for. A slim ITX case could be used. </p><p>

Although surface mount isn’t my favourite kind of soldering but I can understand why it was based on SMT. </p><p>

This would lend itself well to a kit where all the parts needed are supplied… <img loading="lazy" alt="🤔" title="Thinking face    :thinking:" src="https://cdn.jsdelivr.net/joypixels/assets/6.6/png/unicode/64/1f914.png" data-shortname=":thinking:"></p></div>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560123" id="js-post-560123">

		<span id="post-560123"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560123" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560123/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560123" rel="nofollow">
						#11
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560123" data-lb-caption-desc="max1zzz · Apr 19, 2025 at 4:53 PM">

		

		<article>
			
				
			
			
				<p>It may actually be possible to build one of these with more TH parts, Probably not fully TH but there might just be enough space to squeeze in DIP parts for all the logic and PAL's and maybe the SCSI controller, ROM / VIA / SCC would have to stay as SMT parts though as there just isn't the space for the DIP packages (Though maybe TH PLCC sockets could be used.......) It's something I might look to do in the future as I was quite surprised a how much free space there actually is</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560398" id="js-post-560398">

		<span id="post-560398"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560398" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560398/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560398" rel="nofollow">
						#12
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560398" data-lb-caption-desc="max1zzz · Apr 22, 2025 at 3:46 PM">

		

		<article>
			
				
			
			
				<p>Boards have been sent off to JLC, hopefully I should have them in hand by the end of next week <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"></p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="adespoton" data-content="post-560452" id="js-post-560452">

		<span id="post-560452"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560452" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560452/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560452" rel="nofollow">
						#13
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560452" data-lb-caption-desc="adespoton · Apr 23, 2025 at 12:23 AM">

		

		<article>
			
				
			
			
				<div><blockquote data-attributes="member: 1863" data-quote="max1zzz" data-source="post: 560398">
	
		
	
	<div>
		
		<p>
			Boards have been sent off to JLC, hopefully I should have them in hand by the end of next week <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)">
		</p>
		
	</div>
</blockquote><p>How large a run are you doing?</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560484" id="js-post-560484">

		<span id="post-560484"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560484" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560484/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560484" rel="nofollow">
						#14
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560484" data-lb-caption-desc="max1zzz · Apr 23, 2025 at 8:42 AM">

		

		<article>
			
				
			
			
				<p>At the moment just 5 to see if they work, I may do a larger run later though (It will also be open source so you will be able to make your own too <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"> )</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560834" id="js-post-560834">

		<span id="post-560834"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560834" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560834/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560834" rel="nofollow">
						#15
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560834" data-lb-caption-desc="max1zzz · Apr 28, 2025 at 5:30 PM">

		

		<article>
			
				
			
			
				<p>PCB's are here!<br>
<a href="https://68kmla.org/bb/index.php?attachments/img_4310-jpg.86040/" target="_blank"><img src="https://68kmla.org/bb/data/attachments/86/86094-9f6c46f79b7d77065632a3709142f23f.jpg" alt="IMG_4310.jpg" title="IMG_4310.jpg" width="200" height="150" loading="lazy"></a></p>
			
			
			
				
			
		</article>

		

		
			
	

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="Big Ben" data-content="post-560861" id="js-post-560861">

		<span id="post-560861"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560861" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560861/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560861" rel="nofollow">
						#16
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560861" data-lb-caption-desc="Big Ben · Apr 28, 2025 at 11:10 PM">

		

		<article>
			
				
			
			
				<div><p>This is the call for testing and glory!<br>
*ahem*</p><p>

Looks great! Can’t wait to see it in action, nice work!</p></div>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560864" id="js-post-560864">

		<span id="post-560864"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560864" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560864/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560864" rel="nofollow">
						#17
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560864" data-lb-caption-desc="max1zzz · Apr 28, 2025 at 11:39 PM">

		

		<article>
			
				
			
			
				<div><blockquote data-attributes="member: 18909" data-quote="Big Ben" data-source="post: 560861">
	
		
	
	<div>
		
		<p>
			This is the call for testing and glory!<br>
*ahem*
		</p>
		
	</div>
</blockquote><p>That'll be tomorrows job <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":)" title="Smile    :)" loading="lazy" data-shortname=":)"></p><p>

All the non programmables (and the ROM) are soldered down, will get all the PAL's programmed and installed tomorrow. Also still waiting for a pico to arrive as my spare one is AWOL<br>
<a href="https://68kmla.org/bb/index.php?attachments/img_4316-jpg.86062/" target="_blank"><img src="https://68kmla.org/bb/data/attachments/86/86116-82e199316f70a17979850f18078f3f7f.jpg" alt="IMG_4316.jpg" title="IMG_4316.jpg" width="200" height="150" loading="lazy"></a></p></div>
			
			
			
				
			
		</article>

		

		
			
	

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="max1zzz" data-content="post-560958" id="js-post-560958">

		<span id="post-560958"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560958" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560958/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560958" rel="nofollow">
						#18
					</a>
				</li>
			
		</ul>
	</header>

							

							<div>
							

								
									
	
	
	

								

								
									

	<div data-lb-id="post-560958" data-lb-caption-desc="max1zzz · Apr 30, 2025 at 12:47 AM">

		

		<article>
			
				
			
			
				<p>It's late and I'm not doing a full write-up tonight, but I'll just leave these here (excuse the potato video, it's that or a massive file and I cba with trying to compress it right now)<br>
<a href="https://68kmla.org/bb/index.php?attachments/2a7e93c6-225d-4f73-baf4-2d95fbdda02c-jpg.86096/" target="_blank"><img src="https://68kmla.org/bb/data/attachments/86/86150-1b1af659166641540b91ce109bb8a1eb.jpg" alt="2A7E93C6-225D-4F73-BAF4-2D95FBDDA02C.jpg" title="2A7E93C6-225D-4F73-BAF4-2D95FBDDA02C.jpg" width="200" height="150" loading="lazy"></a><br>
<a href="https://68kmla.org/bb/index.php?attachments/img_4320-mov.86097/" target="_blank">View attachment IMG_4320.MOV</a></p>
			
			
			
				
			
		</article>

		

		
			
	

		
	</div>

								

								
									
	
		<p>
			
				Last edited: <time dir="auto" datetime="2025-04-30T00:56:44+0100" data-time="1745971004" data-date-string="Apr 30, 2025" data-time-string="12:56 AM" title="Apr 30, 2025 at 12:56 AM" itemprop="dateModified">Apr 30, 2025</time>
			
		</p>
	

								

								
									
	

								

							
							</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="mg.man" data-content="post-560960" id="js-post-560960">

		<span id="post-560960"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560960" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560960/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560960" rel="nofollow">
						#19
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560960" data-lb-caption-desc="mg.man · Apr 30, 2025 at 12:54 AM">

		

		<article>
			
				
			
			
				<p>Wow! You go walkabout for a few, err... yrs(!) and this sort of thing happens!  <br>
Amazing! <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt=":oops:" title="Oops!    :oops:" loading="lazy" data-shortname=":oops:"></p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				

					

					
						

	

	

	
	<article data-author="Big Ben" data-content="post-560977" id="js-post-560977">

		<span id="post-560977"></span>

		
			<div>

							
								

	<header>
		

		<ul>
			
			<li>
				<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560977" data-xf-init="share-tooltip" data-href="/bb/index.php?posts/560977/share" aria-label="Share" rel="nofollow">
					
				</a>
			</li>
			
			
				<li>
					<a href="https://68kmla.org/bb/index.php?threads/itxplus-a-itx-sized-macintosh-plus-logicboard-reproduction.49715/post-560977" rel="nofollow">
						#20
					</a>
				</li>
			
		</ul>
	</header>

							

							<div data-lb-id="post-560977" data-lb-caption-desc="Big Ben · Apr 30, 2025 at 6:43 AM">

		

		<article>
			
				
			
			
				<p>It boots! <img loading="lazy" alt="😲" title="Astonished face    :astonished:" src="https://cdn.jsdelivr.net/joypixels/assets/6.6/png/unicode/64/1f632.png" data-shortname=":astonished:"><br>
Congrats!</p>
			
			
			
				
			
		</article>

		

		
	</div>

							
								
	

							
						</div>
		
	</article>

	
	

					

					

				
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Machine Stops (1909) (121 pts)]]></title>
            <link>https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops</link>
            <guid>44056407</guid>
            <pubDate>Wed, 21 May 2025 21:18:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops">https://standardebooks.org/ebooks/e-m-forster/short-fiction/text/the-machine-stops</a>, See on <a href="https://news.ycombinator.com/item?id=44056407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div epub:type="bodymatter z3998:fiction">
		<article id="the-machine-stops" epub:type="se:short-story">
			
			<section id="the-machine-stops-1" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">I</span>
					</h3>
					<p epub:type="title">The Airship</p>
				</hgroup>
				<p>Imagine, if you can, a small room, hexagonal in shape, like the cell of a bee. It is lighted neither by window nor by lamp, yet it is filled with a soft radiance. There are no apertures for ventilation, yet the air is fresh. There are no musical instruments, and yet, at the moment that my meditation opens, this room is throbbing with melodious sounds. An armchair is in the centre, by its side a reading-desk⁠—that is all the furniture. And in the armchair there sits a swaddled lump of flesh⁠—a woman, about five feet high, with a face as white as a fungus. It is to her that the little room belongs.</p>
				<p>An electric bell rang.</p>
				<p>The woman touched a switch and the music was silent.</p>
				<p>“I suppose I must see who it is,” she thought, and set her chair in motion. The chair, like the music, was worked by machinery, and it rolled her to the other side of the room, where the bell still rang importunately.</p>
				<p>“Who is it?” she called. Her voice was irritable, for she had been interrupted often since the music began. She knew several thousand people; in certain directions human intercourse had advanced enormously.</p>
				<p>But when she listened into the receiver, her white face wrinkled into smiles, and she said:</p>
				<p>“Very well. Let us talk, I will isolate myself. I do not expect anything important will happen for the next five minutes⁠—for I can give you fully five minutes, Kuno. Then I must deliver my lecture on ‘Music during the Australian Period.’ ”</p>
				<p>She touched the isolation knob, so that no one else could speak to her. Then she touched the lighting apparatus, and the little room was plunged into darkness.</p>
				<p>“Be quick!” she called, her irritation returning. “Be quick, Kuno; here I am in the dark wasting my time.”</p>
				<p>But it was fully fifteen seconds before the round plate that she held in her hands began to glow. A faint blue light shot across it, darkening to purple, and presently she could see the image of her son, who lived on the other side of the earth, and he could see her.</p>
				<p>“Kuno, how slow you are.”</p>
				<p>He smiled gravely.</p>
				<p>“I really believe you enjoy dawdling.”</p>
				<p>“I have called you before, mother, but you were always busy or isolated. I have something particular to say.”</p>
				<p>“What is it, dearest boy? Be quick. Why could you not send it by pneumatic post?”</p>
				<p>“Because I prefer saying such a thing. I want⁠—”</p>
				<p>“Well?”</p>
				<p>“I want you to come and see me.”</p>
				<p>Vashti watched his face in the blue plate.</p>
				<p>“But I can see you!” she exclaimed. “What more do you want?”</p>
				<p>“I want to see you not through the Machine,” said Kuno. “I want to speak to you not through the wearisome Machine.”</p>
				<p>“Oh, hush!” said his mother, vaguely shocked. “You mustn’t say anything against the Machine.”</p>
				<p>“Why not?”</p>
				<p>“One mustn’t.”</p>
				<p>“You talk as if a god had made the Machine,” cried the other. “I believe that you pray to it when you are unhappy. Men made it, do not forget that. Great men, but men. The Machine is much, but it is not everything. I see something like you in this plate, but I do not see you. I hear something like you through this telephone, but I do not hear you. That is why I want you to come. Come and stop with me. Pay me a visit, so that we can meet face to face, and talk about the hopes that are in my mind.”</p>
				<p>She replied that she could scarcely spare the time for a visit.</p>
				<p>“The airship barely takes two days to fly between me and you.”</p>
				<p>“I dislike airships.”</p>
				<p>“Why?”</p>
				<p>“I dislike seeing the horrible brown earth, and the sea, and the stars when it is dark. I get no ideas in an airship.”</p>
				<p>“I do not get them anywhere else.”</p>
				<p>“What kind of ideas can the air give you?”</p>
				<p>He paused for an instant.</p>
				<p>“Do you not know four big stars that form an oblong, and three stars close together in the middle of the oblong, and hanging from these stars, three other stars?”</p>
				<p>“No, I do not. I dislike the stars. But did they give you an idea? How interesting; tell me.”</p>
				<p>“I had an idea that they were like a man.”</p>
				<p>“I do not understand.”</p>
				<p>“The four big stars are the man’s shoulders and his knees. The three stars in the middle are like the belts that men wore once, and the three stars hanging are like a sword.”</p>
				<p>“A sword?”</p>
				<p>“Men carried swords about with them, to kill animals and other men.”</p>
				<p>“It does not strike me as a very good idea, but it is certainly original. When did it come to you first?”</p>
				<p>“In the airship⁠—” He broke off, and she fancied that he looked sad. She could not be sure, for the Machine did not transmit <em>nuances</em> of expression. It only gave a general idea of people⁠—an idea that was good enough for all practical purposes, Vashti thought. The imponderable bloom, declared by a discredited philosophy to be the actual essence of intercourse, was rightly ignored by the Machine, just as the imponderable bloom of the grape was ignored by the manufacturers of artificial fruit. Something “good enough” had long since been accepted by our race.</p>
				<p>“The truth is,” he continued, “that I want to see these stars again. They are curious stars. I want to see them not from the airship, but from the surface of the earth, as our ancestors did, thousands of years ago. I want to visit the surface of the earth.”</p>
				<p>She was shocked again.</p>
				<p>“Mother, you must come, if only to explain to me what is the harm of visiting the surface of the earth.”</p>
				<p>“No harm,” she replied, controlling herself. “But no advantage. The surface of the earth is only dust and mud, no life remains on it, and you would need a respirator, or the cold of the outer air would kill you. One dies immediately in the outer air.”</p>
				<p>“I know; of course I shall take all precautions.”</p>
				<p>“And besides⁠—”</p>
				<p>“Well?”</p>
				<p>She considered, and chose her words with care. Her son had a queer temper, and she wished to dissuade him from the expedition.</p>
				<p>“It is contrary to the spirit of the age,” she asserted.</p>
				<p>“Do you mean by that, contrary to the Machine?”</p>
				<p>“In a sense, but⁠—”</p>
				<p>His image in the blue plate faded.</p>
				<p>“Kuno!”</p>
				<p>He had isolated himself.</p>
				<p>For a moment Vashti felt lonely.</p>
				<p>Then she generated the light, and the sight of her room, flooded with radiance and studded with electric buttons, revived her. There were buttons and switches everywhere⁠—buttons to call for food, for music, for clothing. There was the hot-bath button, by pressure of which a basin of (imitation) marble rose out of the floor, filled to the brim with a warm deodorised liquid. There was the cold-bath button. There was the button that produced literature. And there were of course the buttons by which she communicated with her friends. The room, though it contained nothing, was in touch with all that she cared for in the world.</p>
				<p>Vashti’s next move was to turn off the isolation-switch, and all the accumulations of the last three minutes burst upon her. The room was filled with the noise of bells, and speaking-tubes. What was the new food like? Could she recommend it? Had she had any ideas lately? Might one tell her one’s own ideas? Would she make an engagement to visit the public nurseries at an early date?⁠—say this day month.</p>
				<p>To most of these questions she replied with irritation⁠—a growing quality in that accelerated age. She said that the new food was horrible. That she could not visit the public nurseries through press of engagements. That she had no ideas of her own but had just been told one⁠—that four stars and three in the middle were like a man: she doubted there was much in it. Then she switched off her correspondents, for it was time to deliver her lecture on Australian music.</p>
				<p>The clumsy system of public gatherings had been long since abandoned; neither Vashti nor her audience stirred from their rooms. Seated in her armchair she spoke, while they in their armchairs heard her, fairly well, and saw her, fairly well. She opened with a humorous account of music in the pre-Mongolian epoch, and went on to describe the great outburst of song that followed the Chinese conquest. Remote and primaeval as were the methods of I-San-So and the Brisbane school, she yet felt (she said) that study of them might repay the musician of today: they had freshness; they had, above all, ideas.</p>
				<p>Her lecture, which lasted ten minutes, was well received, and at its conclusion she and many of her audience listened to a lecture on the sea; there were ideas to be got from the sea; the speaker had donned a respirator and visited it lately. Then she fed, talked to many friends, had a bath, talked again, and summoned her bed.</p>
				<p>The bed was not to her liking. It was too large, and she had a feeling for a small bed. Complaint was useless, for beds were of the same dimension all over the world, and to have had an alternative size would have involved vast alterations in the Machine. Vashti isolated herself⁠—it was necessary, for neither day nor night existed under the ground⁠—and reviewed all that had happened since she had summoned the bed last. Ideas? Scarcely any. Events⁠—was Kuno’s invitation an event?</p>
				<p>By her side, on the little reading-desk, was a survival from the ages of litter⁠—one book. This was the Book of the Machine. In it were instructions against every possible contingency. If she was hot or cold or dyspeptic or at loss for a word, she went to the book, and it told her which button to press. The Central Committee published it. In accordance with a growing habit, it was richly bound.</p>
				<p>Sitting up in the bed, she took it reverently in her hands. She glanced round the glowing room as if someone might be watching her. Then, half ashamed, half joyful, she murmured “O Machine! O Machine!” and raised the volume to her lips. Thrice she kissed it, thrice inclined her head, thrice she felt the delirium of acquiescence. Her ritual performed, she turned to page 1,367, which gave the times of the departure of the airships from the island in the southern hemisphere, under whose soil she lived, to the island in the northern hemisphere, whereunder lived her son.</p>
				<p>She thought, “I have not the time.”</p>
				<p>She made the room dark and slept; she awoke and made the room light; she ate and exchanged ideas with her friends, and listened to music and attended lectures; she made the room dark and slept. Above her, beneath her, and around her, the Machine hummed eternally; she did not notice the noise, for she had been born with it in her ears. The earth, carrying her, hummed as it sped through silence, turning her now to the invisible sun, now to the invisible stars. She awoke and made the room light.</p>
				<p>“Kuno!”</p>
				<p>“I will not talk to you,” he answered, “until you come.”</p>
				<p>“Have you been on the surface of the earth since we spoke last?”</p>
				<p>His image faded.</p>
				<p>Again she consulted the book. She became very nervous and lay back in her chair palpitating. Think of her as without teeth or hair. Presently she directed the chair to the wall, and pressed an unfamiliar button. The wall swung apart slowly. Through the opening she saw a tunnel that curved slightly, so that its goal was not visible. Should she go to see her son, here was the beginning of the journey.</p>
				<p>Of course she knew all about the communication-system. There was nothing mysterious in it. She would summon a car and it would fly with her down the tunnel until it reached the lift that communicated with the airship station: the system had been in use for many, many years, long before the universal establishment of the Machine. And of course she had studied the civilisation that had immediately preceded her own⁠—the civilisation that had mistaken the functions of the system, and had used it for bringing people to things, instead of for bringing things to people. Those funny old days, when men went for change of air instead of changing the air in their rooms! And yet⁠—she was frightened of the tunnel: she had not seen it since her last child was born. It curved⁠—but not quite as she remembered; it was brilliant⁠—but not quite as brilliant as a lecturer had suggested. Vashti was seized with the terrors of direct experience. She shrank back into the room, and the wall closed up again.</p>
				<p>“Kuno,” she said, “I cannot come to see you. I am not well.”</p>
				<p>Immediately an enormous apparatus fell on to her out of the ceiling, a thermometer was automatically inserted between her lips, a stethoscope was automatically laid upon her heart. She lay powerless. Cool pads soothed her forehead. Kuno had telegraphed to her doctor.</p>
				<p>So the human passions still blundered up and down in the Machine. Vashti drank the medicine that the doctor projected into her mouth, and the machinery retired into the ceiling. The voice of Kuno was heard asking how she felt.</p>
				<p>“Better.” Then with irritation: “But why do you not come to me instead?”</p>
				<p>“Because I cannot leave this place.”</p>
				<p>“Why?”</p>
				<p>“Because, any moment, something tremendous may happen.”</p>
				<p>“Have you been on the surface of the earth yet?”</p>
				<p>“Not yet.”</p>
				<p>“Then what is it?”</p>
				<p>“I will not tell you through the Machine.”</p>
				<p>She resumed her life.</p>
				<p>But she thought of Kuno as a baby, his birth, his removal to the public nurseries, her one visit to him there, his visits to her⁠—visits which stopped when the Machine had assigned him a room on the other side of the earth. “Parents, duties of,” said the book of the Machine, “cease at the moment of birth. <abbr epub:type="z3998:initialism">P.</abbr> 422,327,483.” True, but there was something special about Kuno⁠—indeed there had been something special about all her children⁠—and, after all, she must brave the journey if he desired it. And “something tremendous might happen.” What did that mean? The nonsense of a youthful man, no doubt, but she must go. Again she pressed the unfamiliar button, again the wall swung back, and she saw the tunnel that curved out of sight. Clasping the Book, she rose, tottered on to the platform, and summoned the car. Her room closed behind her: the journey to the northern hemisphere had begun.</p>
				<p>Of course it was perfectly easy. The car approached and in it she found armchairs exactly like her own. When she signalled, it stopped, and she tottered into the lift. One other passenger was in the lift, the first fellow creature she had seen face to face for months. Few travelled in these days, for, thanks to the advance of science, the earth was exactly alike all over. Rapid intercourse, from which the previous civilisation had hoped so much, had ended by defeating itself. What was the good of going to Peking when it was just like Shrewsbury? Why return to Shrewsbury when it would be just like Peking? Men seldom moved their bodies; all unrest was concentrated in the soul.</p>
				<p>The airship service was a relic from the former age. It was kept up, because it was easier to keep it up than to stop it or to diminish it, but it now far exceeded the wants of the population. Vessel after vessel would rise from the vomitories of Rye or of Christchurch (I use the antique names), would sail into the crowded sky, and would draw up at the wharves of the south⁠—empty. So nicely adjusted was the system, so independent of meteorology, that the sky, whether calm or cloudy, resembled a vast kaleidoscope whereon the same patterns periodically recurred. The ship on which Vashti sailed started now at sunset, now at dawn. But always, as it passed above Rheims, it would neighbour the ship that served between Helsingfors and the Brazils, and, every third time it surmounted the Alps, the fleet of Palermo would cross its track behind. Night and day, wind and storm, tide and earthquake, impeded man no longer. He had harnessed Leviathan. All the old literature, with its praise of Nature, and its fear of Nature, rang false as the prattle of a child.</p>
				<p>Yet as Vashti saw the vast flank of the ship, stained with exposure to the outer air, her horror of direct experience returned. It was not quite like the airship in the cinematophote. For one thing it smelt⁠—not strongly or unpleasantly, but it did smell, and with her eyes shut she should have known that a new thing was close to her. Then she had to walk to it from the lift, had to submit to glances from the other passengers. The man in front dropped his Book⁠—no great matter, but it disquieted them all. In the rooms, if the Book was dropped, the floor raised it mechanically, but the gangway to the airship was not so prepared, and the sacred volume lay motionless. They stopped⁠—the thing was unforeseen⁠—and the man, instead of picking up his property, felt the muscles of his arm to see how they had failed him. Then someone actually said with direct utterance: “We shall be late”⁠—and they trooped on board, Vashti treading on the pages as she did so.</p>
				<p>Inside, her anxiety increased. The arrangements were old-fashioned and rough. There was even a female attendant, to whom she would have to announce her wants during the voyage. Of course a revolving platform ran the length of the boat, but she was expected to walk from it to her cabin. Some cabins were better than others, and she did not get the best. She thought the attendant had been unfair, and spasms of rage shook her. The glass valves had closed, she could not go back. She saw, at the end of the vestibule, the lift in which she had ascended going quietly up and down, empty. Beneath those corridors of shining tiles were rooms, tier below tier, reaching far into the earth, and in each room there sat a human being, eating, or sleeping, or producing ideas. And buried deep in the hive was her own room. Vashti was afraid.</p>
				<p>“O Machine! O Machine!” she murmured, and caressed her Book, and was comforted.</p>
				<p>Then the sides of the vestibule seemed to melt together, as do the passages that we see in dreams, the lift vanished, the Book that had been dropped slid to the left and vanished, polished tiles rushed by like a stream of water, there was a slight jar, and the airship, issuing from its tunnel, soared above the waters of a tropical ocean.</p>
				<p>It was night. For a moment she saw the coast of Sumatra edged by the phosphorescence of waves, and crowned by lighthouses, still sending forth their disregarded beams. These also vanished, and only the stars distracted her. They were not motionless, but swayed to and fro above her head, thronging out of one skylight into another, as if the universe and not the airship was careening. And, as often happens on clear nights, they seemed now to be in perspective, now on a plane; now piled tier beyond tier into the infinite heavens, now concealing infinity, a roof limiting forever the visions of men. In either case they seemed intolerable. “Are we to travel in the dark?” called the passengers angrily, and the attendant, who had been careless, generated the light, and pulled down the blinds of pliable metal. When the airships had been built, the desire to look direct at things still lingered in the world. Hence the extraordinary number of skylights and windows, and the proportionate discomfort to those who were civilised and refined. Even in Vashti’s cabin one star peeped through a flaw in the blind, and after a few hours’ uneasy slumber, she was disturbed by an unfamiliar glow, which was the dawn.</p>
				<p>Quick as the ship had sped westwards, the earth had rolled eastwards quicker still, and had dragged back Vashti and her companions towards the sun. Science could prolong the night, but only for a little, and those high hopes of neutralising the earth’s diurnal revolution had passed, together with hopes that were possibly higher. To “keep pace with the sun,” or even to outstrip it, had been the aim of the civilisation preceding this. Racing aeroplanes had been built for the purpose, capable of enormous speed, and steered by the greatest intellects of the epoch. Round the globe they went, round and round, westward, westward, round and round, amidst humanity’s applause. In vain. The globe went eastward quicker still, horrible accidents occurred, and the Committee of the Machine, at the time rising into prominence, declared the pursuit illegal, unmechanical, and punishable by Homelessness.</p>
				<p>Of Homelessness more will be said later.</p>
				<p>Doubtless the Committee was right. Yet the attempt to “defeat the sun” aroused the last common interest that our race experienced about the heavenly bodies, or indeed about anything. It was the last time that men were compacted by thinking of a power outside the world. The sun had conquered, yet it was the end of his spiritual dominion. Dawn, midday, twilight, the zodiacal path, touched neither men’s lives nor their hearts, and science retreated into the ground, to concentrate herself upon problems that she was certain of solving.</p>
				<p>So when Vashti found her cabin invaded by a rosy finger of light, she was annoyed, and tried to adjust the blind. But the blind flew up altogether, and she saw through the skylight small pink clouds, swaying against a background of blue, and as the sun crept higher, its radiance entered direct, brimming down the wall, like a golden sea. It rose and fell with the airship’s motion, just as waves rise and fall, but it advanced steadily, as a tide advances. Unless she was careful, it would strike her face. A spasm of horror shook her and she rang for the attendant. The attendant too was horrified, but she could do nothing; it was not her place to mend the blind. She could only suggest that the lady should change her cabin, which she accordingly prepared to do.</p>
				<p>People were almost exactly alike all over the world, but the attendant of the airship, perhaps owing to her exceptional duties, had grown a little out of the common. She had often to address passengers with direct speech, and this had given her a certain roughness and originality of manner. When Vashti swerved away from the sunbeams with a cry, she behaved barbarically⁠—she put out her hand to steady her.</p>
				<p>“How dare you!” exclaimed the passenger. “You forget yourself!”</p>
				<p>The woman was confused, and apologised for not having let her fall. People never touched one another. The custom had become obsolete, owing to the Machine.</p>
				<p>“Where are we now?” asked Vashti haughtily.</p>
				<p>“We are over Asia,” said the attendant, anxious to be polite.</p>
				<p>“Asia?”</p>
				<p>“You must excuse my common way of speaking. I have got into the habit of calling places over which I pass by their unmechanical names.”</p>
				<p>“Oh, I remember Asia. The Mongols came from it.”</p>
				<p>“Beneath us, in the open air, stood a city that was once called Simla.”</p>
				<p>“Have you ever heard of the Mongols and of the Brisbane school?”</p>
				<p>“No.”</p>
				<p>“Brisbane also stood in the open air.”</p>
				<p>“Those mountains to the right⁠—let me show you them.” She pushed back a metal blind. The main chain of the Himalayas was revealed. “They were once called the Roof of the World, those mountains.”</p>
				<p>“What a foolish name!”</p>
				<p>“You must remember that, before the dawn of civilisation, they seemed to be an impenetrable wall that touched the stars. It was supposed that no one but the gods could exist above their summits. How we have advanced, thanks to the Machine!”</p>
				<p>“How we have advanced, thanks to the Machine!” said Vashti.</p>
				<p>“How we have advanced, thanks to the Machine!” echoed the passenger who had dropped his Book the night before, and who was standing in the passage.</p>
				<p>“And that white stuff in the cracks?⁠—what is it?”</p>
				<p>“I have forgotten its name.”</p>
				<p>“Cover the window, please. These mountains give me no ideas.”</p>
				<p>The northern aspect of the Himalayas was in deep shadow: on the Indian slope the sun had just prevailed. The forests had been destroyed during the literature epoch for the purpose of making newspaper-pulp, but the snows were awakening to their morning glory, and clouds still hung on the breasts of Kinchinjunga. In the plain were seen the ruins of cities, with diminished rivers creeping by their walls, and by the sides of these were sometimes the signs of vomitories, marking the cities of today. Over the whole prospect airships rushed, crossing and intercrossing with incredible <em>aplomb</em>, and rising nonchalantly when they desired to escape the perturbations of the lower atmosphere and to traverse the Roof of the World.</p>
				<p>“We have indeed advanced, thanks to the Machine,” repeated the attendant, and hid the Himalayas behind a metal blind.</p>
				<p>The day dragged wearily forward. The passengers sat each in his cabin, avoiding one another with an almost physical repulsion and longing to be once more under the surface of the earth. There were eight or ten of them, mostly young males, sent out from the public nurseries to inhabit the rooms of those who had died in various parts of the earth. The man who had dropped his Book was on the homeward journey. He had been sent to Sumatra for the purpose of propagating the race. Vashti alone was travelling by her private will.</p>
				<p>At midday she took a second glance at the earth. The airship was crossing another range of mountains, but she could see little, owing to clouds. Masses of black rock hovered below her, and merged indistinctly into grey. Their shapes were fantastic; one of them resembled a prostrate man.</p>
				<p>“No ideas here,” murmured Vashti, and hid the Caucasus behind a metal blind.</p>
				<p>In the evening she looked again. They were crossing a golden sea, in which lay many small islands and one peninsula.</p>
				<p>She repeated, “No ideas here,” and hid Greece behind a metal blind.</p>
			</section>
			<section id="the-machine-stops-2" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">II</span>
					</h3>
					<p epub:type="title">The Mending Apparatus</p>
				</hgroup>
				<p>By a vestibule, by a lift, by a tubular railway, by a platform, by a sliding door⁠—by reversing all the steps of her departure did Vashti arrive at her son’s room, which exactly resembled her own. She might well declare that the visit was superfluous. The buttons, the knobs, the reading-desk with the Book, the temperature, the atmosphere, the illumination⁠—all were exactly the same. And if Kuno himself, flesh of her flesh, stood close beside her at last, what profit was there in that? She was too well-bred to shake him by the hand.</p>
				<p>Averting her eyes, she spoke as follows:</p>
				<p>“Here I am. I have had the most terrible journey and greatly retarded the development of my soul. It is not worth it, Kuno, it is not worth it. My time is too precious. The sunlight almost touched me, and I have met with the rudest people. I can only stop a few minutes. Say what you want to say, and then I must return.”</p>
				<p>“I have been threatened with Homelessness,” said Kuno.</p>
				<p>She looked at him now.</p>
				<p>“I have been threatened with Homelessness, and I could not tell you such a thing through the Machine.”</p>
				<p>Homelessness means death. The victim is exposed to the air, which kills him.</p>
				<p>“I have been outside since I spoke to you last. The tremendous thing has happened, and they have discovered me.”</p>
				<p>“But why shouldn’t you go outside!” she exclaimed. “It is perfectly legal, perfectly mechanical, to visit the surface of the earth. I have lately been to a lecture on the sea; there is no objection to that; one simply summons a respirator and gets an Egression-permit. It is not the kind of thing that spiritually-minded people do, and I begged you not to do it, but there is no legal objection to it.”</p>
				<p>“I did not get an Egression-permit.”</p>
				<p>“Then how did you get out?”</p>
				<p>“I found out a way of my own.”</p>
				<p>The phrase conveyed no meaning to her, and he had to repeat it.</p>
				<p>“A way of your own?” she whispered. “But that would be wrong.”</p>
				<p>“Why?”</p>
				<p>The question shocked her beyond measure.</p>
				<p>“You are beginning to worship the Machine,” he said coldly. “You think it irreligious of me to have found out a way of my own. It was just what the Committee thought, when they threatened me with Homelessness.”</p>
				<p>At this she grew angry. “I worship nothing!” she cried. “I am most advanced. I don’t think you irreligious, for there is no such thing as religion left. All the fear and the superstition that existed once have been destroyed by the Machine. I only meant that to find out a way of your own was⁠—Besides, there is no new way out.”</p>
				<p>“So it is always supposed.”</p>
				<p>“Except through the vomitories, for which one must have an Egression-permit, it is impossible to get out. The Book says so.”</p>
				<p>“Well, the Book’s wrong, for I have been out on my feet.”</p>
				<p>For Kuno was possessed of a certain physical strength.</p>
				<p>By these days it was a demerit to be muscular. Each infant was examined at birth, and all who promised undue strength were destroyed. Humanitarians may protest, but it would have been no true kindness to let an athlete live; he would never have been happy in that state of life to which the Machine had called him; he would have yearned for trees to climb, rivers to bathe in, meadows and hills against which he might measure his body. Man must be adapted to his surroundings, must he not? In the dawn of the world our weakly must be exposed on Mount Taygetus, in its twilight our strong will suffer euthanasia, that the Machine may progress, that the Machine may progress, that the Machine may progress eternally.</p>
				<p>“You know that we have lost the sense of space. We say ‘space is annihilated,’ but we have annihilated not space, but the sense thereof. We have lost a part of ourselves. I determined to recover it, and I began by walking up and down the platform of the railway outside my room. Up and down, until I was tired, and so did recapture the meaning of ‘Near’ and ‘Far.’ ‘Near’ is a place to which I can get quickly <em>on my feet</em>, not a place to which the train or the airship will take me quickly. ‘Far’ is a place to which I cannot get quickly on my feet; the vomitory is ‘far,’ though I could be there in thirty-eight seconds by summoning the train. Man is the measure. That was my first lesson. Man’s feet are the measure for distance, his hands are the measure for ownership, his body is the measure for all that is lovable and desirable and strong. Then I went further: it was then that I called to you for the first time, and you would not come.</p>
				<p>“This city, as you know, is built deep beneath the surface of the earth, with only the vomitories protruding. Having paced the platform outside my own room, I took the lift to the next platform and paced that also, and so with each in turn, until I came to the topmost, above which begins the earth. All the platforms were exactly alike, and all that I gained by visiting them was to develop my sense of space and my muscles. I think I should have been content with this⁠—it is not a little thing⁠—but as I walked and brooded, it occurred to me that our cities had been built in the days when men still breathed the outer air, and that there had been ventilation shafts for the workmen. I could think of nothing but these ventilation shafts. Had they been destroyed by all the food-tubes and medicine-tubes and music-tubes that the Machine has evolved lately? Or did traces of them remain? One thing was certain. If I came upon them anywhere, it would be in the railway-tunnels of the topmost story. Everywhere else, all space was accounted for.</p>
				<p>“I am telling my story quickly, but don’t think that I was not a coward or that your answers never depressed me. It is not the proper thing, it is not mechanical, it is not decent to walk along a railway-tunnel. I did not fear that I might tread upon a live rail and be killed. I feared something far more intangible⁠—doing what was not contemplated by the Machine. Then I said to myself, ‘Man is the measure,’ and I went, and after many visits I found an opening.</p>
				<p>“The tunnels, of course, were lighted. Everything is light, artificial light; darkness is the exception. So when I saw a black gap in the tiles, I knew that it was an exception, and rejoiced. I put in my arm⁠—I could put in no more at first⁠—and waved it round and round in ecstasy. I loosened another tile, and put in my head, and shouted into the darkness: ‘I am coming, I shall do it yet,’ and my voice reverberated down endless passages. I seemed to hear the spirits of those dead workmen who had returned each evening to the starlight and to their wives, and all the generations who had lived in the open air called back to me, ‘You will do it yet, you are coming.’ ”</p>
				<p>He paused, and, absurd as he was, his last words moved her. For Kuno had lately asked to be a father, and his request had been refused by the Committee. His was not a type that the Machine desired to hand on.</p>
				<p>“Then a train passed. It brushed by me, but I thrust my head and arms into the hole. I had done enough for one day, so I crawled back to the platform, went down in the lift, and summoned my bed. Ah, what dreams! And again I called you, and again you refused.”</p>
				<p>She shook her head and said:</p>
				<p>“Don’t. Don’t talk of these terrible things. You make me miserable. You are throwing civilisation away.”</p>
				<p>“But I had got back the sense of space and a man cannot rest then. I determined to get in at the hole and climb the shaft. And so I exercised my arms. Day after day I went through ridiculous movements, until my flesh ached, and I could hang by my hands and hold the pillow of my bed outstretched for many minutes. Then I summoned a respirator, and started.</p>
				<p>“It was easy at first. The mortar had somehow rotted, and I soon pushed some more tiles in, and clambered after them into the darkness, and the spirits of the dead comforted me. I don’t know what I mean by that. I just say what I felt. I felt, for the first time, that a protest had been lodged against corruption, and that even as the dead were comforting me, so I was comforting the unborn. I felt that humanity existed, and that it existed without clothes. How can I possibly explain this? It was naked, humanity seemed naked, and all these tubes and buttons and machineries neither came into the world with us, nor will they follow us out, nor do they matter supremely while we are here. Had I been strong, I would have torn off every garment I had, and gone out into the outer air unswaddled. But this is not for me, nor perhaps for my generation. I climbed with my respirator and my hygienic clothes and my dietetic tabloids! Better thus than not at all.</p>
				<p>“There was a ladder, made of some primaeval metal. The light from the railway fell upon its lowest rungs, and I saw that it led straight upwards out of the rubble at the bottom of the shaft. Perhaps our ancestors ran up and down it a dozen times daily, in their building. As I climbed, the rough edges cut through my gloves so that my hands bled. The light helped me for a little, and then came darkness and, worse still, silence which pierced my ears like a sword. The Machine hums! Did you know that? Its hum penetrates our blood, and may even guide our thoughts. Who knows! I was getting beyond its power. Then I thought: ‘This silence means that I am doing wrong.’ But I heard voices in the silence, and again they strengthened me.” He laughed. “I had need of them. The next moment I cracked my head against something.”</p>
				<p>She sighed.</p>
				<p>“I had reached one of those pneumatic stoppers that defend us from the outer air. You may have noticed them on the airship. Pitch dark, my feet on the rungs of an invisible ladder, my hands cut; I cannot explain how I lived through this part, but the voices still comforted me, and I felt for fastenings. The stopper, I suppose, was about eight feet across. I passed my hand over it as far as I could reach. It was perfectly smooth. I felt it almost to the centre. Not quite to the centre, for my arm was too short. Then the voice said: ‘Jump. It is worth it. There may be a handle in the centre, and you may catch hold of it and so come to us your own way. And if there is no handle, so that you may fall and are dashed to pieces⁠—it is still worth it: you will still come to us your own way.’ So I jumped. There was a handle, and⁠—”</p>
				<p>He paused. Tears gathered in his mother’s eyes. She knew that he was fated. If he did not die today he would die tomorrow. There was not room for such a person in the world. And with her pity disgust mingled. She was ashamed at having borne such a son, she who had always been so respectable and so full of ideas. Was he really the little boy to whom she had taught the use of his stops and buttons, and to whom she had given his first lessons in the Book? The very hair that disfigured his lip showed that he was reverting to some savage type. On atavism the Machine can have no mercy.</p>
				<p>“There was a handle, and I did catch it. I hung tranced over the darkness and heard the hum of these workings as the last whisper in a dying dream. All the things I had cared about and all the people I had spoken to through tubes appeared infinitely little. Meanwhile the handle revolved. My weight had set something in motion and I span slowly, and then⁠—</p>
				<p>“I cannot describe it. I was lying with my face to the sunshine. Blood poured from my nose and ears and I heard a tremendous roaring. The stopper, with me clinging to it, had simply been blown out of the earth, and the air that we make down here was escaping through the vent into the air above. It burst up like a fountain. I crawled back to it⁠—for the upper air hurts⁠—and, as it were, I took great sips from the edge. My respirator had flown goodness knows where, my clothes were torn. I just lay with my lips close to the hole, and I sipped until the bleeding stopped. You can imagine nothing so curious. This hollow in the grass⁠—I will speak of it in a minute⁠—the sun shining into it, not brilliantly but through marbled clouds⁠—the peace, the nonchalance, the sense of space, and, brushing my cheek, the roaring fountain of our artificial air! Soon I spied my respirator, bobbing up and down in the current high above my head, and higher still were many airships. But no one ever looks out of airships, and in my case they could not have picked me up. There I was, stranded. The sun shone a little way down the shaft, and revealed the topmost rung of the ladder, but it was hopeless trying to reach it. I should either have been tossed up again by the escape, or else have fallen in, and died. I could only lie on the grass, sipping and sipping, and from time to time glancing around me.</p>
				<p>“I knew that I was in Wessex, for I had taken care to go to a lecture on the subject before starting. Wessex lies above the room in which we are talking now. It was once an important state. Its kings held all the southern coast from the Andredswald to Cornwall, while the Wansdyke protected them on the north, running over the high ground. The lecturer was only concerned with the rise of Wessex, so I do not know how long it remained an international power, nor would the knowledge have assisted me. To tell the truth I could do nothing but laugh, during this part. There was I, with a pneumatic stopper by my side and a respirator bobbing over my head, imprisoned, all three of us, in a grass-grown hollow that was edged with fern.”</p>
				<p>Then he grew grave again.</p>
				<p>“Lucky for me that it was a hollow. For the air began to fall back into it and to fill it as water fills a bowl. I could crawl about. Presently I stood. I breathed a mixture, in which the air that hurts predominated whenever I tried to climb the sides. This was not so bad. I had not lost my tabloids and remained ridiculously cheerful, and as for the Machine, I forgot about it altogether. My one aim now was to get to the top, where the ferns were, and to view whatever objects lay beyond.</p>
				<p>“I rushed the slope. The new air was still too bitter for me and I came rolling back, after a momentary vision of something grey. The sun grew very feeble, and I remembered that he was in Scorpio⁠—I had been to a lecture on that too. If the sun is in Scorpio and you are in Wessex, it means that you must be as quick as you can, or it will get too dark. (This is the first bit of useful information I have ever got from a lecture, and I expect it will be the last.) It made me try frantically to breathe the new air, and to advance as far as I dared out of my pond. The hollow filled so slowly. At times I thought that the fountain played with less vigour. My respirator seemed to dance nearer the earth; the roar was decreasing.”</p>
				<p>He broke off.</p>
				<p>“I don’t think this is interesting you. The rest will interest you even less. There are no ideas in it, and I wish that I had not troubled you to come. We are too different, mother.”</p>
				<p>She told him to continue.</p>
				<p>“It was evening before I climbed the bank. The sun had very nearly slipped out of the sky by this time, and I could not get a good view. You, who have just crossed the Roof of the World, will not want to hear an account of the little hills that I saw⁠—low colourless hills. But to me they were living and the turf that covered them was a skin, under which their muscles rippled, and I felt that those hills had called with incalculable force to men in the past, and that men had loved them. Now they sleep⁠—perhaps forever. They commune with humanity in dreams. Happy the man, happy the woman, who awakes the hills of Wessex. For though they sleep, they will never die.”</p>
				<p>His voice rose passionately.</p>
				<p>“Cannot you see, cannot all your lecturers see, that it is we who are dying, and that down here the only thing that really lives is the Machine? We created the Machine, to do our will, but we cannot make it do our will now. It has robbed us of the sense of space and of the sense of touch, it has blurred every human relation and narrowed down love to a carnal act, it has paralysed our bodies and our wills, and now it compels us to worship it. The Machine develops⁠—but not on our lines. The Machine proceeds⁠—but not to our goal. We only exist as the blood corpuscles that course through its arteries, and if it could work without us, it would let us die. Oh, I have no remedy⁠—or, at least, only one⁠—to tell men again and again that I have seen the hills of Wessex as Ælfrid saw them when he overthrew the Danes.</p>
				<p>“So the sun set. I forgot to mention that a belt of mist lay between my hill and other hills, and that it was the colour of pearl.”</p>
				<p>He broke off for the second time.</p>
				<p>“Go on,” said his mother wearily.</p>
				<p>He shook his head.</p>
				<p>“Go on. Nothing that you say can distress me now. I am hardened.”</p>
				<p>“I had meant to tell you the rest, but I cannot: I know that I cannot: goodbye.”</p>
				<p>Vashti stood irresolute. All her nerves were tingling with his blasphemies. But she was also inquisitive.</p>
				<p>“This is unfair,” she complained. “You have called me across the world to hear your story, and hear it I will. Tell me⁠—as briefly as possible, for this is a disastrous waste of time⁠—tell me how you returned to civilisation.”</p>
				<p>“Oh⁠—that!” he said, starting. “You would like to hear about civilisation. Certainly. Had I got to where my respirator fell down?”</p>
				<p>“No⁠—but I understand everything now. You put on your respirator, and managed to walk along the surface of the earth to a vomitory, and there your conduct was reported to the Central Committee.”</p>
				<p>“By no means.”</p>
				<p>He passed his hand over his forehead, as if dispelling some strong impression. Then, resuming his narrative, he warmed to it again.</p>
				<p>“My respirator fell about sunset. I had mentioned that the fountain seemed feebler, had I not.”</p>
				<p>“Yes.”</p>
				<p>“About sunset, it let the respirator fall. As I said, I had entirely forgotten about the Machine, and I paid no great attention at the time, being occupied with other things. I had my pool of air, into which I could dip when the outer keenness became intolerable, and which would possibly remain for days, provided that no wind sprang up to disperse it. Not until it was too late, did I realize what the stoppage of the escape implied. You see⁠—the gap in the tunnel had been mended; the Mending Apparatus; the Mending Apparatus, was after me.</p>
				<p>“One other warning I had, but I neglected it. The sky at night was clearer than it had been in the day, and the moon, which was about half the sky behind the sun, shone into the dell at moments quite brightly. I was in my usual place⁠—on the boundary between the two atmospheres⁠—when I thought I saw something dark move across the bottom of the dell, and vanish into the shaft. In my folly, I ran down. I bent over and listened, and I thought I heard a faint scraping noise in the depths.</p>
				<p>“At this⁠—but it was too late⁠—I took alarm. I determined to put on my respirator and to walk right out of the dell. But my respirator had gone. I knew exactly where it had fallen⁠—between the stopper and the aperture⁠—and I could even feel the mark that it had made in the turf. It had gone, and I realized that something evil was at work, and I had better escape to the other air, and, if I must die, die running towards the cloud that had been the colour of a pearl. I never started. Out of the shaft⁠—it is too horrible. A worm, a long white worm, had crawled out of the shaft and was gliding over the moonlit grass.</p>
				<p>“I screamed. I did everything that I should not have done, I stamped upon the creature instead of flying from it, and it at once curled round the ankle. Then we fought. The worm let me run all over the dell, but edged up my leg as I ran. ‘Help!’ I cried. (That part is too awful. It belongs to the part that you will never know.) ‘Help!’ I cried. (Why cannot we suffer in silence?) ‘Help!’ I cried. Then my feet were wound together, I fell, I was dragged away from the dear ferns and the living hills, and past the great metal stopper (I can tell you this part), and I thought it might save me again if I caught hold of the handle. It also was enwrapped, it also. Oh, the whole dell was full of the things. They were searching it in all directions, they were denuding it, and the white snouts of others peeped out of the hole, ready if needed. Everything that could be moved they brought⁠—brushwood, bundles of fern, everything, and down we all went intertwined into hell. The last things that I saw, ere the stopper closed after us, were certain stars, and I felt that a man of my sort lived in the sky. For I did fight, I fought till the very end, and it was only my head hitting against the ladder that quieted me. I woke up in this room. The worms had vanished. I was surrounded by artificial air, artificial light, artificial peace, and my friends were calling to me down speaking-tubes to know whether I had come across any new ideas lately.”</p>
				<p>Here his story ended. Discussion of it was impossible, and Vashti turned to go.</p>
				<p>“It will end in Homelessness,” she said quietly.</p>
				<p>“I wish it would,” retorted Kuno.</p>
				<p>“The Machine has been most merciful.”</p>
				<p>“I prefer the mercy of God.”</p>
				<p>“By that superstitious phrase, do you mean that you could live in the outer air?”</p>
				<p>“Yes.”</p>
				<p>“Have you ever seen, round the vomitories, the bones of those who were extruded after the Great Rebellion?”</p>
				<p>“Yes.”</p>
				<p>“They were left where they perished for our edification. A few crawled away, but they perished, too⁠—who can doubt it? And so with the Homeless of our own day. The surface of the earth supports life no longer.”</p>
				<p>“Indeed.”</p>
				<p>“Ferns and a little grass may survive, but all higher forms have perished. Has any airship detected them?”</p>
				<p>“No.”</p>
				<p>“Has any lecturer dealt with them?”</p>
				<p>“No.”</p>
				<p>“Then why this obstinacy?”</p>
				<p>“Because I have seen them,” he exploded.</p>
				<p>“Seen <em>what</em>?”</p>
				<p>“Because I have seen her in the twilight⁠—because she came to my help when I called⁠—because she, too, was entangled by the worms, and, luckier than I, was killed by one of them piercing her throat.”</p>
				<p>He was mad. Vashti departed, nor, in the troubles that followed, did she ever see his face again.</p>
			</section>
			<section id="the-machine-stops-3" epub:type="chapter">
				<hgroup>
					<h3>
						<span epub:type="label">Part</span>
						<span epub:type="ordinal z3998:roman">III</span>
					</h3>
					<p epub:type="title">The Homeless</p>
				</hgroup>
				<p>During the years that followed Kuno’s escapade, two important developments took place in the Machine. On the surface they were revolutionary, but in either case men’s minds had been prepared beforehand, and they did but express tendencies that were latent already.</p>
				<p>The first of these was the abolition of respirators.</p>
				<p>Advanced thinkers, like Vashti, had always held it foolish to visit the surface of the earth. Airships might be necessary, but what was the good of going out for mere curiosity and crawling along for a mile or two in a terrestrial motor? The habit was vulgar and perhaps faintly improper: it was unproductive of ideas, and had no connection with the habits that really mattered. So respirators were abolished, and with them, of course, the terrestrial motors, and except for a few lecturers, who complained that they were debarred access to their subject-matter, the development was accepted quietly. Those who still wanted to know what the earth was like had after all only to listen to some gramophone, or to look into some cinematophote. And even the lecturers acquiesced when they found that a lecture on the sea was none the less stimulating when compiled out of other lectures that had already been delivered on the same subject. “Beware of firsthand ideas!” exclaimed one of the most advanced of them. “Firsthand ideas do not really exist. They are but the physical impressions produced by love and fear, and on this gross foundation who could erect a philosophy? Let your ideas be secondhand, and if possible tenth-hand, for then they will be far removed from that disturbing element⁠—direct observation. Do not learn anything about this subject of mine⁠—the French Revolution. Learn instead what I think that Enicharmon thought Urizen thought Gutch thought Ho-Yung thought Chi-Bo-Sing thought Lafcadio Hearn thought Carlyle thought Mirabeau said about the French Revolution. Through the medium of these eight great minds, the blood that was shed at Paris and the windows that were broken at Versailles will be clarified to an idea which you may employ most profitably in your daily lives. But be sure that the intermediates are many and varied, for in history one authority exists to counteract another. Urizen must counteract the scepticism of Ho-Yung and Enicharmon, I must myself counteract the impetuosity of Gutch. You who listen to me are in a better position to judge about the French Revolution than I am. Your descendants will be even in a better position than you, for they will learn what you think I think, and yet another intermediate will be added to the chain. And in time”⁠—his voice rose⁠—“there will come a generation that has got beyond facts, beyond impressions, a generation absolutely colourless, a generation</p>
				<blockquote epub:type="z3998:verse">
					<p>
						<span>‘seraphically free</span>
						<br>
						<span>From taint of personality,’</span>
					</p>
				</blockquote>
				<p>which will see the French Revolution not as it happened, nor as they would like it to have happened, but as it would have happened, had it taken place in the days of the Machine.”</p>
				<p>Tremendous applause greeted this lecture, which did but voice a feeling already latent in the minds of men⁠—a feeling that terrestrial facts must be ignored, and that the abolition of respirators was a positive gain. It was even suggested that airships should be abolished too. This was not done, because airships had somehow worked themselves into the Machine’s system. But year by year they were used less, and mentioned less by thoughtful men.</p>
				<p>The second great development was the reestablishment of religion.</p>
				<p>This, too, had been voiced in the celebrated lecture. No one could mistake the reverent tone in which the peroration had concluded, and it awakened a responsive echo in the heart of each. Those who had long worshipped silently, now began to talk. They described the strange feeling of peace that came over them when they handled the Book of the Machine, the pleasure that it was to repeat certain numerals out of it, however little meaning those numerals conveyed to the outward ear, the ecstasy of touching a button, however unimportant, or of ringing an electric bell, however superfluously.</p>
				<p>“The Machine,” they exclaimed, “feeds us and clothes us and houses us; through it we speak to one another, through it we see one another, in it we have our being. The Machine is the friend of ideas and the enemy of superstition: the Machine is omnipotent, eternal; blessed is the Machine.” And before long this allocution was printed on the first page of the Book, and in subsequent editions the ritual swelled into a complicated system of praise and prayer. The word “religion” was sedulously avoided, and in theory the Machine was still the creation and the implement of man. But in practice all, save a few retrogrades, worshipped it as divine. Nor was it worshipped in unity. One believer would be chiefly impressed by the blue optic plates, through which he saw other believers; another by the mending apparatus, which sinful Kuno had compared to worms; another by the lifts, another by the Book. And each would pray to this or to that, and ask it to intercede for him with the Machine as a whole. Persecution⁠—that also was present. It did not break out, for reasons that will be set forward shortly. But it was latent, and all who did not accept the minimum known as “undenominational Mechanism” lived in danger of Homelessness, which means death, as we know.</p>
				<p>To attribute these two great developments to the Central Committee, is to take a very narrow view of civilisation. The Central Committee announced the developments, it is true, but they were no more the cause of them than were the kings of the imperialistic period the cause of war. Rather did they yield to some invincible pressure, which came no one knew whither, and which, when gratified, was succeeded by some new pressure equally invincible. To such a state of affairs it is convenient to give the name of progress. No one confessed the Machine was out of hand. Year by year it was served with increased efficiency and decreased intelligence. The better a man knew his own duties upon it, the less he understood the duties of his neighbour, and in all the world there was not one who understood the monster as a whole. Those master brains had perished. They had left full directions, it is true, and their successors had each of them mastered a portion of those directions. But Humanity, in its desire for comfort, had overreached itself. It had exploited the riches of nature too far. Quietly and complacently, it was sinking into decadence, and progress had come to mean the progress of the Machine.</p>
				<p>As for Vashti, her life went peacefully forward until the final disaster. She made her room dark and slept; she awoke and made the room light. She lectured and attended lectures. She exchanged ideas with her innumerable friends and believed she was growing more spiritual. At times a friend was granted Euthanasia, and left his or her room for the homelessness that is beyond all human conception. Vashti did not much mind. After an unsuccessful lecture, she would sometimes ask for Euthanasia herself. But the death-rate was not permitted to exceed the birthrate, and the Machine had hitherto refused it to her.</p>
				<p>The troubles began quietly, long before she was conscious of them.</p>
				<p>One day she was astonished at receiving a message from her son. They never communicated, having nothing in common, and she had only heard indirectly that he was still alive, and had been transferred from the northern hemisphere, where he had behaved so mischievously, to the southern⁠—indeed, to a room not far from her own.</p>
				<p>“Does he want me to visit him?” she thought. “Never again, never. And I have not the time.”</p>
				<p>No, it was madness of another kind.</p>
				<p>He refused to visualize his face upon the blue plate, and speaking out of the darkness with solemnity said:</p>
				<p>“The Machine stops.”</p>
				<p>“What do you say?”</p>
				<p>“The Machine is stopping, I know it, I know the signs.”</p>
				<p>She burst into a peal of laughter. He heard her and was angry, and they spoke no more.</p>
				<p>“Can you imagine anything more absurd?” she cried to a friend. “A man who was my son believes that the Machine is stopping. It would be impious if it was not mad.”</p>
				<p>“The Machine is stopping?” her friend replied. “What does that mean? The phrase conveys nothing to me.”</p>
				<p>“Nor to me.”</p>
				<p>“He does not refer, I suppose, to the trouble there has been lately with the music?”</p>
				<p>“Oh no, of course not. Let us talk about music.”</p>
				<p>“Have you complained to the authorities?”</p>
				<p>“Yes, and they say it wants mending, and referred me to the Committee of the Mending Apparatus. I complained of those curious gasping sighs that disfigure the symphonies of the Brisbane school. They sound like someone in pain. The Committee of the Mending Apparatus say that it shall be remedied shortly.”</p>
				<p>Obscurely worried, she resumed her life. For one thing, the defect in the music irritated her. For another thing, she could not forget Kuno’s speech. If he had known that the music was out of repair⁠—he could not know it, for he detested music⁠—if he had known that it was wrong, “the Machine stops” was exactly the venomous sort of remark he would have made. Of course he had made it at a venture, but the coincidence annoyed her, and she spoke with some petulance to the Committee of the Mending Apparatus.</p>
				<p>They replied, as before, that the defect would be set right shortly.</p>
				<p>“Shortly! At once!” she retorted. “Why should I be worried by imperfect music? Things are always put right at once. If you do not mend it at once, I shall complain to the Central Committee.”</p>
				<p>“No personal complaints are received by the Central Committee,” the Committee of the Mending Apparatus replied.</p>
				<p>“Through whom am I to make my complaint, then?”</p>
				<p>“Through us.”</p>
				<p>“I complain then.”</p>
				<p>“Your complaint shall be forwarded in its turn.”</p>
				<p>“Have others complained?”</p>
				<p>This question was unmechanical, and the Committee of the Mending Apparatus refused to answer it.</p>
				<p>“It is too bad!” she exclaimed to another of her friends. “There never was such an unfortunate woman as myself. I can never be sure of my music now. It gets worse and worse each time I summon it.”</p>
				<p>“I too have my troubles,” the friend replied. “Sometimes my ideas are interrupted by a slight jarring noise.”</p>
				<p>“What is it?”</p>
				<p>“I do not know whether it is inside my head, or inside the wall.”</p>
				<p>“Complain, in either case.”</p>
				<p>“I have complained, and my complaint will be forwarded in its turn to the Central Committee.”</p>
				<p>Time passed, and they resented the defects no longer. The defects had not been remedied, but the human tissues in that latter day had become so subservient, that they readily adapted themselves to every caprice of the Machine. The sigh at the crisis of the Brisbane symphony no longer irritated Vashti; she accepted it as part of the melody. The jarring noise, whether in the head or in the wall, was no longer resented by her friend. And so with the mouldy artificial fruit, so with the bath water that began to stink, so with the defective rhymes that the poetry machine had taken to emit. All were bitterly complained of at first, and then acquiesced in and forgotten. Things went from bad to worse unchallenged.</p>
				<p>It was otherwise with the failure of the sleeping apparatus. That was a more serious stoppage. There came a day when over the whole world⁠—in Sumatra, in Wessex, in the innumerable cities of Courland and Brazil⁠—the beds, when summoned by their tired owners, failed to appear. It may seem a ludicrous matter, but from it we may date the collapse of humanity. The Committee responsible for the failure was assailed by complainants, whom it referred, as usual, to the Committee of the Mending Apparatus, who in its turn assured them that their complaints would be forwarded to the Central Committee. But the discontent grew, for mankind was not yet sufficiently adaptable to do without sleeping.</p>
				<p>“Someone is meddling with the Machine⁠—” they began.</p>
				<p>“Someone is trying to make himself king, to reintroduce the personal element.”</p>
				<p>“Punish that man with Homelessness.”</p>
				<p>“To the rescue! Avenge the Machine! Avenge the Machine!”</p>
				<p>“War! Kill the man!”</p>
				<p>But the Committee of the Mending Apparatus now came forward, and allayed the panic with well-chosen words. It confessed that the Mending Apparatus was itself in need of repair.</p>
				<p>The effect of this frank confession was admirable.</p>
				<p>“Of course,” said a famous lecturer⁠—he of the French Revolution, who gilded each new decay with splendour⁠—“of course we shall not press our complaints now. The Mending Apparatus has treated us so well in the past that we all sympathize with it, and will wait patiently for its recovery. In its own good time it will resume its duties. Meanwhile let us do without our beds, our tabloids, our other little wants. Such, I feel sure, would be the wish of the Machine.”</p>
				<p>Thousands of miles away his audience applauded. The Machine still linked them. Under the seas, beneath the roots of the mountains, ran the wires through which they saw and heard, the enormous eyes and ears that were their heritage, and the hum of many workings clothed their thoughts in one garment of subserviency. Only the old and the sick remained ungrateful, for it was rumoured that Euthanasia, too, was out of order, and that pain had reappeared among men.</p>
				<p>It became difficult to read. A blight entered the atmosphere and dulled its luminosity. At times Vashti could scarcely see across her room. The air, too, was foul. Loud were the complaints, impotent the remedies, heroic the tone of the lecturer as he cried: “Courage, courage! What matter so long as the Machine goes on? To it the darkness and the light are one.” And though things improved again after a time, the old brilliancy was never recaptured, and humanity never recovered from its entrance into twilight. There was an hysterical talk of “measures,” of “provisional dictatorship,” and the inhabitants of Sumatra were asked to familiarize themselves with the workings of the central power station, the said power station being situated in France. But for the most part panic reigned, and men spent their strength praying to their Books, tangible proofs of the Machine’s omnipotence. There were gradations of terror⁠—at times came rumours of hope⁠—the Mending Apparatus was almost mended⁠—the enemies of the Machine had been got under⁠—new “nerve-centres” were evolving which would do the work even more magnificently than before. But there came a day when, without the slightest warning, without any previous hint of feebleness, the entire communication-system broke down, all over the world, and the world, as they understood it, ended.</p>
				<p>Vashti was lecturing at the time and her earlier remarks had been punctuated with applause. As she proceeded the audience became silent, and at the conclusion there was no sound. Somewhat displeased, she called to a friend who was a specialist in sympathy. No sound: doubtless the friend was sleeping. And so with the next friend whom she tried to summon, and so with the next, until she remembered Kuno’s cryptic remark, “The Machine stops.”</p>
				<p>The phrase still conveyed nothing. If Eternity was stopping it would of course be set going shortly.</p>
				<p>For example, there was still a little light and air⁠—the atmosphere had improved a few hours previously. There was still the Book, and while there was the Book there was security.</p>
				<p>Then she broke down, for with the cessation of activity came an unexpected terror⁠—silence.</p>
				<p>She had never known silence, and the coming of it nearly killed her⁠—it did kill many thousands of people outright. Ever since her birth she had been surrounded by the steady hum. It was to the ear what artificial air was to the lungs, and agonizing pains shot across her head. And scarcely knowing what she did, she stumbled forward and pressed the unfamiliar button, the one that opened the door of her cell.</p>
				<p>Now the door of the cell worked on a simple hinge of its own. It was not connected with the central power station, dying far away in France. It opened, rousing immoderate hopes in Vashti, for she thought that the Machine had been mended. It opened, and she saw the dim tunnel that curved far away towards freedom. One look, and then she shrank back. For the tunnel was full of people⁠—she was almost the last in that city to have taken alarm.</p>
				<p>People at any time repelled her, and these were nightmares from her worst dreams. People were crawling about, people were screaming, whimpering, gasping for breath, touching each other, vanishing in the dark, and ever and anon being pushed off the platform on to the live rail. Some were fighting round the electric bells, trying to summon trains which could not be summoned. Others were yelling for Euthanasia or for respirators, or blaspheming the Machine. Others stood at the doors of their cells fearing, like herself, either to stop in them or to leave them. And behind all the uproar was silence⁠—the silence which is the voice of the earth and of the generations who have gone.</p>
				<p>No⁠—it was worse than solitude. She closed the door again and sat down to wait for the end. The disintegration went on, accompanied by horrible cracks and rumbling. The valves that restrained the Medical Apparatus must have been weakened, for it ruptured and hung hideously from the ceiling. The floor heaved and fell and flung her from her chair. A tube oozed towards her serpent fashion. And at last the final horror approached⁠—light began to ebb, and she knew that civilisation’s long day was closing.</p>
				<p>She whirled round, praying to be saved from this, at any rate, kissing the Book, pressing button after button. The uproar outside was increasing, and even penetrated the wall. Slowly the brilliancy of her cell was dimmed, the reflections faded from her metal switches. Now she could not see the reading-stand, now not the Book, though she held it in her hand. Light followed the flight of sound, air was following light, and the original void returned to the cavern from which it had been so long excluded. Vashti continued to whirl, like the devotees of an earlier religion, screaming, praying, striking at the buttons with bleeding hands.</p>
				<p>It was thus that she opened her prison and escaped⁠—escaped in the spirit: at least so it seems to me, ere my meditation closes. That she escapes in the body⁠—I cannot perceive that. She struck, by chance, the switch that released the door, and the rush of foul air on her skin, the loud throbbing whispers in her ears, told her that she was facing the tunnel again, and that tremendous platform on which she had seen men fighting. They were not fighting now. Only the whispers remained, and the little whimpering groans. They were dying by hundreds out in the dark.</p>
				<p>She burst into tears.</p>
				<p>Tears answered her.</p>
				<p>They wept for humanity, those two, not for themselves. They could not bear that this should be the end. Ere silence was completed their hearts were opened, and they knew what had been important on the earth. Man, the flower of all flesh, the noblest of all creatures visible, man who had once made god in his image, and had mirrored his strength on the constellations, beautiful naked man was dying, strangled in the garments that he had woven. Century after century had he toiled, and here was his reward. Truly the garment had seemed heavenly at first, shot with the colours of culture, sewn with the threads of self-denial. And heavenly it had been so long as it was a garment and no more, so long as man could shed it at will and live by the essence that is his soul, and the essence, equally divine, that is his body. The sin against the body⁠—it was for that they wept in chief; the centuries of wrong against the muscles and the nerves, and those five portals by which we can alone apprehend⁠—glozing it over with talk of evolution, until the body was white pap, the home of ideas as colourless, last sloshy stirrings of a spirit that had grasped the stars.</p>
				<p>“Where are you?” she sobbed.</p>
				<p>His voice in the darkness said, “Here.”</p>
				<p>“Is there any hope, Kuno?”</p>
				<p>“None for us.”</p>
				<p>“Where are you?”</p>
				<p>She crawled towards him over the bodies of the dead. His blood spurted over her hands.</p>
				<p>“Quicker,” he gasped, “I am dying⁠—but we touch, we talk, not through the Machine.”</p>
				<p>He kissed her.</p>
				<p>“We have come back to our own. We die, but we have recaptured life, as it was in Wessex, when Ælfrid overthrew the Danes. We know what they know outside, they who dwelt in the cloud that is the colour of a pearl.”</p>
				<p>“But, Kuno, is it true? Are there still men on the surface of the earth? Is this⁠—this tunnel, this poisoned darkness⁠—really not the end?”</p>
				<p>He replied:</p>
				<p>“I have seen them, spoken to them, loved them. They are hiding in the mist and the ferns until our civilisation stops. Today they are the Homeless⁠—tomorrow⁠—”</p>
				<p>“Oh, tomorrow⁠—some fool will start the Machine again, tomorrow.”</p>
				<p>“Never,” said Kuno, “never. Humanity has learnt its lesson.”</p>
				<p>As he spoke, the whole city was broken like a honeycomb. An airship had sailed in through the vomitory into a ruined wharf. It crashed downwards, exploding as it went, rending gallery after gallery with its wings of steel. For a moment they saw the nations of the dead, and, before they joined them, scraps of the untainted sky.</p>
			</section>
		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rocky Linux 10 Will Support RISC-V (178 pts)]]></title>
            <link>https://rockylinux.org/news/rockylinux-support-for-riscv</link>
            <guid>44056104</guid>
            <pubDate>Wed, 21 May 2025 20:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rockylinux.org/news/rockylinux-support-for-riscv">https://rockylinux.org/news/rockylinux-support-for-riscv</a>, See on <a href="https://news.ycombinator.com/item?id=44056104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>🏗️ Rocky Linux 10 Will Support RISC-V!</h2>
<p>We're excited to announce that <strong>Rocky Linux 10 will officially support RISC-V!</strong></p>
<p>Thanks to the incredible work of the Fedora RISC-V Community and Rocky's <a href="https://chat.rockylinux.org/">AltArch SIG</a>, this release will include a <strong>riscv64gc</strong> build, targeting the same platforms supported by Fedora—such as the <strong>StarFive VisionFive 2 (VF2)</strong>, <strong>QEMU</strong>, and the <strong>SiFive HiFive Premier P550</strong>.</p>
<blockquote>
<p>Learn more about Fedora’s RISC-V journey:<br>
<a href="https://fedoramagazine.org/risc-v-and-fedora-all-aboard/">fedoramagazine.org/risc-v-and-fedora-all-aboard</a></p>
</blockquote>
<h3>🔧 Highlights:</h3>
<ul>
<li>✅ Works out-of-the-box on the VisionFive 2 and in QEMU, using the standard EL10 kernel.</li>
<li>🧩 Supports the P550 and similar platforms via vendor kernels, though some features may be limited.</li>
<li>🧬 Built on an upstream-first approach—actively collaborating with the Fedora community to advance RISC-V support across the ecosystem.</li>
<li>🆕 New hardware targets and extensions (like RVA23) can be enabled by the AltArch SIG—jump in and get involved!</li>
</ul>
<p>Special thanks to <strong>RISC-V International</strong>, <strong>RISE</strong>, <strong>Rivos, Inc.</strong>, and the <strong>Fedora community</strong> for their ongoing technical and hardware support.</p>
<hr>
<h2>❓ Frequently Asked: Rocky Linux RISC-V</h2>
<h3>Is this a Primary architecture?</h3>
<p>The RISC-V builds for Rocky Linux 10 will be considered an Alternative Architecture--though unlike ppc64le and s390x, build failures for riscv64 will <strong>not</strong> be considered fatal and will not block the release of the other architectures. In short, package updates for Rocky Linux will not be bottlenecked on waiting for RISC-V versions to build, or on fixing failures unique to the archicture.</p>
<h3>🔍 What hardware is supported?</h3>






























<table><thead><tr><th>Hardware</th><th>Status</th><th>Notes</th></tr></thead><tbody><tr><td><strong>StarFive VisionFive 2</strong></td><td>✅ Supported</td><td>Recommended board; standard kernel support</td></tr><tr><td><strong>QEMU</strong></td><td>✅ Supported</td><td>Ideal for testing and evaluation</td></tr><tr><td><strong>SiFive HiFive P550</strong></td><td>⚠️ Limited support</td><td>Vendor kernel;  some feature limitations</td></tr><tr><td><strong>Milk-V / Banana Pi</strong></td><td>🚧 Not yet supported</td><td>Under consideration as mainline support matures</td></tr></tbody></table>
<h3>🌟 What makes this different?</h3>
<ul>
<li>This has been a <strong>community-driven initiative</strong> since early 2024, collaborating with upstream Fedora RISC-V efforts.</li>
<li>The compiler stack was <strong>bootstrapped from Fedora RISC-V</strong>, with necessary backports to EL10 to enable the port--with many already contributed upstreamed.</li>
<li>Expect <strong>rapid iteration and growth</strong>, with your help and feedback!</li>
</ul>
<hr>
<h2>🧭 Getting Started with RISC-V on Rocky</h2>
<ul>
<li>📥 <strong>Download</strong> the Rocky Linux 10 RISC-V image (coming soon).</li>
<li>📘 <strong>Read the install guide</strong> (also coming soon).</li>
<li>💬 <strong>Join the conversation</strong> in our Mattermost <code>SIG/Altarch</code> channel:<br>
<a href="https://chat.rockylinux.org/">chat.rockylinux.org</a></li>
</ul>
<p>Stay updated via <a href="https://bsky.app/profile/rockylinux.bsky.social">Bluesky</a>, <a href="https://www.linkedin.com/company/rocky-linux">LinkedIn</a>, or subscribe to our <strong>RSS feed</strong>.</p>
<hr>
<h2>🌍 The Road Ahead</h2>
<p>From x86_64 to Arm, PowerPC to S390X—and now RISC-V—<strong>Rocky Linux 10</strong> represents our biggest step yet toward a truly open, cross-architecture ecosystem.</p>
<p>Whether you're deploying rock-solid production systems or tinkering with open hardware, <strong>Rocky Linux 10 has a place for you.</strong></p>
<p>Let’s build it together.</p>
<p><em>By Neil Hanlon (Infrastructure Lead) &amp; Alexia Stein (Community Lead)</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ClipJS – Edit your videos from a PC or phone (131 pts)]]></title>
            <link>https://clipjs.vercel.app/</link>
            <guid>44055542</guid>
            <pubDate>Wed, 21 May 2025 19:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clipjs.vercel.app/">https://clipjs.vercel.app/</a>, See on <a href="https://news.ycombinator.com/item?id=44055542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Welcome to<span></span></p><p><span>Edit your videos from your PC or phone no downloads, no registration, no watermarks.</span><span>Online, Free and Open Source</span></p></div><div><h2><span>What Can it do?</span></h2><div><article><figure><img alt="No Watermarks" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/343214/no-sign.svg"></figure><div><h5>No Watermarks</h5><p>Edit your videos without any watermarks.</p></div></article><article><figure><img alt="No Ads" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/508210/ad.svg"></figure><div><h5>No Ads</h5><p>Edit and render without having to watch any ads.</p></div></article><article><figure><img alt="No Registration" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/318470/login.svg"></figure><div><h5>No Registration</h5><p>Start using the app instantly no sign-up, no account, just get to work.</p></div></article><article><figure><img alt="Speed" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/441028/speed.svg"></figure><div><h5>Speed</h5><p>Everything runs in your browser so there's no need to upload files to third-party services or wait around.</p></div></article><article><figure><img alt="Trim Videos" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/348321/cut.svg"></figure><div><h5>Trim Videos</h5><p>Trim video to remove unwanted parts, reduce videos to their most important sections.</p></div></article><article><figure><img alt="Combine" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" src="https://www.svgrepo.com/show/413830/combine.svg"></figure><div><h5>Combine</h5><p>Combine multiple videos, images, texts and audio into one.</p></div></article></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For algorithms, a little memory outweighs a lot of time (316 pts)]]></title>
            <link>https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/</link>
            <guid>44055347</guid>
            <pubDate>Wed, 21 May 2025 19:34:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/">https://www.quantamagazine.org/for-algorithms-a-little-memory-outweighs-a-lot-of-time-20250521/</a>, See on <a href="https://news.ycombinator.com/item?id=44055347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        <p>
            One computer scientist’s “stunning” proof is the first progress in 50 years on one of the most famous questions in computer science.        </p>
        
    </div>
<figure>
    
</figure>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp" alt="" decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Space-Complexity-Breakthrough_crIrene-Perez-Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
            <p>Irene Pérez for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p><span>O</span>ne July afternoon in 2024, <a href="https://people.csail.mit.edu/rrw/">Ryan Williams</a> set out to prove himself wrong. Two months had passed since he’d hit upon a startling discovery about the relationship between time and memory in computing. It was a rough sketch of a mathematical proof that memory was more powerful than computer scientists believed: A small amount would be as helpful as a lot of time in all conceivable computations. That sounded so improbable that he assumed something had to be wrong, and he promptly set the proof aside to focus on less crazy ideas. Now, he’d finally carved out time to find the error.</p>
<p>That’s not what happened. After hours of poring over his argument, Williams couldn’t find a single flaw.</p>
<p>“I just thought I was losing my mind,” said Williams, a theoretical computer scientist at the Massachusetts Institute of Technology. For the first time, he began to entertain the possibility that maybe, just maybe, memory really was as powerful as his work suggested.</p>
<p>Over the months that followed, he fleshed out the details, scrutinized every step, and solicited feedback from a handful of other researchers. In February, he finally <a href="https://arxiv.org/abs/2502.17779">posted his proof online</a>, to widespread acclaim.</p>
<p>“It’s amazing. It’s beautiful,” said <a href="https://www.math.ias.edu/avi/home">Avi Wigderson</a>, a theoretical computer scientist at the Institute for Advanced Study in Princeton, New Jersey. As soon as he heard the news, Wigderson sent Williams a congratulatory email. Its subject line: “You blew my mind.”</p>
<p>Time and memory (also called space) are the two most fundamental resources in computation: Every algorithm takes some time to run, and requires some space to store data while it’s running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there’s no way to do better. Williams’ proof established a mathematical procedure for transforming any algorithm — no matter what it does — into a form that uses much less space.</p>
</div>
    <figure>
        <div>
                            <p><img width="2301" height="1577" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp.webp 2301w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1720x1179.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-520x356.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-768x526.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-1536x1053.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-CloseUp-2048x1404.webp 2048w" sizes="(max-width: 2301px) 100vw, 2301px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Ryan Williams stunned his colleagues with a milestone proof about the relationship between time and space in computation.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>What’s more, this result — a statement about what you can compute given a certain amount of space — also implies a second result, about what you cannot compute in a certain amount of time. This second result isn’t surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams’ solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science.</p>
<p>“It’s a pretty stunning result, and a massive advance,” said <a href="https://www.cs.washington.edu/people/faculty/paul-beame/">Paul Beame</a>, a computer scientist at the University of Washington. “It’s a little bit less of a surprise that it’s Ryan doing this.”</p>
<h2><strong>Space to Roam</strong></h2>
<p>Williams, 46, has an open, expressive face and a hint of gray in his hair. His office, which looks out over the colorful spires of MIT’s Stata Center, is another illustration of the creative use of space. A pair of yoga mats have transformed a window ledge into a makeshift reading nook, and the desk is tucked into an oddly shaped corner, freeing up room for a couch facing a large whiteboard brimming with mathematical scribblings.</p>
<p>MIT is a long way from Williams’ childhood home in rural Alabama, where there was no shortage of space. He grew up on a 50-acre farm and first saw a computer at age 7, when his mother drove him across the county for a special academic enrichment class. He recalled being transfixed by a simple program for generating a digital fireworks display.</p>
<p>“It was taking a random color and sending it in a random direction from the middle of the monitor,” Williams said. “You couldn’t have predicted what picture you’re going to get.” The world of computers seemed a wild and wonderful playground, full of infinite possibilities. Young Williams was hooked.</p>
<p>“I was writing programs to myself, on paper, to be run on a future computer,” he said. “My parents didn’t really know what to do with me.”</p>
</div>
    <figure>
        <div>
                            <p><img width="1799" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-scaled.webp 1799w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1209x1720.webp 1209w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-365x520.webp 365w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-768x1093.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1079x1536.webp 1079w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Window-1439x2048.webp 1439w" sizes="(max-width: 1799px) 100vw, 1799px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams’ office, like his new result, makes creative use of space.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>As he grew older, Williams advanced from imaginary computers to real ones. For his last two years of high school, he transferred to the Alabama School of Math and Science, a prestigious public boarding school, where he first encountered the theoretical side of computer science.</p>
<p>“I realized that there was a wider world of things out there, and that there was a way to think mathematically about computers,” he said. “That’s what I wanted to do.”</p>
<p>Williams was especially drawn to a branch of theoretical computer science called computational complexity theory. It deals with the resources (such as time and space) that are needed to solve computational problems such as sorting lists or factoring numbers. Most problems can be solved by many different algorithms, each with its own demands on time and space. Complexity theorists sort problems into categories, called complexity classes, based on the resource demands of the best algorithms for solving them — that is, the algorithms that run fastest or use the least amount of space.</p>
<p>But how do you make the study of computational resources mathematically rigorous? You won’t get far if you try to analyze time and space by comparing minutes to megabytes. To make any progress, you need to start with the right definitions.</p>
<h2><strong>Getting Resourceful</strong></h2>

<p>Those definitions emerged from the work of Juris Hartmanis, a pioneering computer scientist who had experience making do with limited resources. He was born in 1928 into a prominent Latvian family, but his childhood was disrupted by the outbreak of World War II. Occupying Soviet forces arrested and executed his father, and after the war Hartmanis finished high school in a refugee camp. He went on to university, where he excelled even though he couldn’t afford textbooks.</p>
<p>“I compensated by taking very detailed notes in lectures,” he recalled in a <a href="https://dl.acm.org/doi/10.1145/1141880.1775727">2009 interview</a>. “There is a certain advantage to [having] to improvise and overcome difficulties.” Hartmanis immigrated to the United States in 1949, and worked a series of odd jobs — building agricultural machinery, manufacturing steel and even serving as a butler — while studying mathematics at the University of Kansas City. He went on to a spectacularly successful career in the young field of theoretical computer science.</p>
<p>In 1960, while working at the General Electric research laboratory in Schenectady, New York, Hartmanis met Richard Stearns, a graduate student doing a summer internship. In a pair of <a href="https://www.ams.org/journals/tran/1965-117-00/S0002-9947-1965-0170805-7/">groundbreaking</a> <a href="https://ieeexplore.ieee.org/document/5397244/">papers</a> they established precise mathematical definitions for time and space. These definitions gave researchers the language they needed to compare the two resources and sort problems into complexity classes.</p>
</div>
    <figure>
        <div>
                            <p><img width="1800" height="1033" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis.webp 1800w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1720x987.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-520x298.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-768x441.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Juris-Hartmanis-1536x881.webp 1536w" sizes="(max-width: 1800px) 100vw, 1800px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>In the 1960s, Juris Hartmanis established the definitions that computer scientists use to analyze space and time.</p>
            <p>Division of Rare and Manuscript Collections, Cornell University Library</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>One of the most important classes goes by the humble name “P.” Roughly speaking, it encompasses all problems that can be solved in a reasonable amount of time. An analogous complexity class for space is dubbed “PSPACE.”</p>
<p>The relationship between these two classes is one of the central questions of complexity theory. Every problem in P is also in PSPACE, because fast algorithms just don’t have enough time to fill up much space in a computer’s memory. If the reverse statement were also true, the two classes would be equivalent: Space and time would have comparable computational power. But complexity theorists suspect that PSPACE is a much larger class, containing many problems that aren’t in P. In other words, they believe that space is a far more powerful computational resource than time. This belief stems from the fact that algorithms can use the same small chunk of memory over and over, while time isn’t as forgiving — once it passes, you can’t get it back.</p>
<p>“The intuition is just so simple,” Williams said. “You can reuse space, but you can’t reuse time.”</p>
<p>But intuition isn’t good enough for complexity theorists: They want rigorous proof. To prove that PSPACE is larger than P, researchers would have to show that for some problems in PSPACE, fast algorithms are categorically impossible. Where would they even start?</p>
<h2><strong>A Space Odyssey</strong></h2>
<p>As it happened, they started at Cornell University, where Hartmanis moved in 1965 to head the newly established computer science department. Under his leadership it quickly became a center of research in complexity theory, and in the early 1970s, a pair of researchers there, John Hopcroft and Wolfgang Paul, set out to establish a precise link between time and space.</p>

<p>Hopcroft and Paul knew that to resolve the P versus PSPACE problem, they’d have to prove that you can’t do certain computations in a limited amount of time. But it’s hard to prove a negative. Instead, they decided to flip the problem on its head and explore what you can do with limited space. They hoped to prove that algorithms given a certain space budget can solve all the same problems as algorithms with a slightly larger time budget. That would indicate that space is at least slightly more powerful than time — a small but necessary step toward showing that PSPACE is larger than P.</p>
<p>With that goal in mind, they turned to a method that complexity theorists call simulation, which involves transforming existing algorithms into new ones that solve the same problems, but with different amounts of space and time. To understand the basic idea, imagine you’re given a fast algorithm for alphabetizing your bookshelf, but it requires you to lay out your books in dozens of small piles. You might prefer an approach that takes up less space in your apartment, even if it takes longer. A simulation is a mathematical procedure you could use to get a more suitable algorithm: Feed it the original, and it’ll give you a new algorithm that saves space at the expense of time.</p>
<p>Algorithm designers have long studied these space-time trade-offs for specific tasks like sorting. But to establish a general relationship between time and space, Hopcroft and Paul needed something more comprehensive: a space-saving simulation procedure that works for every algorithm, no matter what problem it solves. They expected this generality to come at a cost. A universal simulation can’t exploit the details of any specific problem, so it probably won’t save as much space as a specialized simulation. But when Hopcroft and Paul started their work, there were no known universal methods for saving space at all. Even saving a small amount would be progress.</p>
<p>The breakthrough came in 1975, after Hopcroft and Paul teamed up with a young researcher named <a href="https://people.seas.harvard.edu/~valiant/">Leslie Valiant</a>. The trio devised a <a href="https://dl.acm.org/doi/10.1145/322003.322015">universal simulation procedure</a> that always saves a bit of space. No matter what algorithm you give it, you’ll get an equivalent one whose space budget is slightly smaller than the original algorithm’s time budget.</p>
<p>“Anything you can do in so much time, you can also do in slightly less space,” Valiant said. It was the first major step in the quest to connect space and time.</p>
<figure>
    <p><img width="1600" height="1556" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-520x506.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-768x747.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/Leslie-Valiant_cr-Katherine-Taylor-1536x1494.webp 1536w" sizes="(max-width: 1600px) 100vw, 1600px">    </p>
            <figcaption>
                            <p>In 1975, Leslie Valiant helped prove that space is a slightly more powerful computational resource than time.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>But then progress stalled, and complexity theorists began to suspect that they’d hit a fundamental barrier. The problem was precisely the universal character of Hopcroft, Paul and Valiant’s simulation. While many problems can be solved with much less space than time, some intuitively seemed like they’d need nearly as much space as time. If so, more space-efficient universal simulations would be impossible. Paul and two other researchers soon proved that they are <a href="https://dl.acm.org/doi/10.1145/800113.803643">indeed impossible</a>, provided you make one seemingly obvious assumption: Different chunks of data can’t occupy the same space in memory at the same time.</p>
<p>“Everybody took it for granted that you cannot do better,” Wigderson said.</p>
<p>Paul’s result suggested that resolving the P versus PSPACE problem would require abandoning simulation altogether in favor of a different approach, but nobody had any good ideas. That was where the problem stood for 50 years — until Williams finally broke the logjam.</p>
<p>First, he had to get through college.</p>
<h2><strong>Complexity Classes</strong></h2>
<p>In 1996, the time came for Williams to apply to colleges. He knew that pursuing complexity theory would take him far from home, but his parents made it clear that the West Coast and Canada were out of the question. Among his remaining options, Cornell stood out to him for its prominent place in the history of his favorite discipline.</p>
<p>“This guy Hartmanis defined the time and space complexity classes,” he recalled thinking. “That was important for me.”</p>
<p>Williams was admitted to Cornell with generous financial aid and arrived in the fall of 1997, planning to do whatever it took to become a complexity theorist himself. His single-mindedness stuck out to his fellow students.</p>
<p>“He was just super-duper into complexity theory,” said <a href="https://www.cs.utexas.edu/people/faculty-researchers/scott-aaronson">Scott Aaronson</a>, a computer scientist at the University of Texas, Austin, who overlapped with Williams at Cornell.</p>
</div>
    <figure>
        <div>
                            <p><img width="2560" height="1398" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1720x939.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-520x284.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-768x419.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-1536x839.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-YellowWall-2048x1118.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams grew interested in the relationship between space and time as an undergraduate, but never found an opportunity to work on it until last year.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>But by sophomore year, Williams was struggling to keep up in courses that emphasized mathematical rigor over intuition. After he got a middling grade in a class on the theory of computing, the teacher suggested he consider other careers. But Williams wouldn’t give up his dream. He doubled down and enrolled in a graduate theory course, hoping that a stellar grade in the harder class would look impressive on his grad school applications. The professor teaching that graduate course was Hartmanis, by then an elder statesman in the field.</p>
<p>Williams began attending Hartmanis’ office hours every week, where he was almost always the only student who showed up. His tenacity paid off: he earned an A in the course, and Hartmanis agreed to advise him on an independent research project the following semester. Their weekly meetings continued throughout Williams’ time in college. Hartmanis encouraged him to cultivate an individual approach to complexity research and gently steered him away from dead ends.</p>
<p>“I was deeply influenced by him then,” Williams said. “I guess I still am now.”</p>

<p>But despite earning a coveted graduate research fellowship from the National Science Foundation, Williams was rejected by every doctoral program he applied to. On hearing the news, Hartmanis phoned a colleague, then turned around and congratulated Williams on getting accepted into a yearlong master’s program at Cornell. A year later Williams again applied to various doctoral programs, and with that extra research experience under his belt, he found success.</p>
<p>Williams continued working in complexity theory in grad school and the years that followed. In 2010, four years after receiving his doctorate, he proved a <a href="https://ieeexplore.ieee.org/document/5959801">milestone result</a> — a small step, but the largest in decades, toward solving the <a href="https://www.quantamagazine.org/complexity-theorys-50-year-journey-to-the-limits-of-knowledge-20230817/">most famous question in theoretical computer science</a>, about the nature of hard problems. That result cemented Williams’ reputation, and he went on to write dozens of other papers on different topics in complexity theory.</p>
<p>P versus PSPACE wasn’t one of them. Williams had been fascinated by the problem since he first encountered it in college. He’d even supplemented his computer science curriculum with courses in logic and philosophy, seeking inspiration from other perspectives on time and space, to no avail.</p>
<p>“It’s always been in the back of my mind,” Williams said. “I just couldn’t think of anything interesting enough to say about it.”</p>
<p>Last year, he finally found the opportunity he’d been waiting for.</p>
<h2><strong>Squishy Pebbles</strong></h2>
<p>The story of Williams’ new result started with progress on a different question about memory in computation: What problems can be solved with extremely limited space? In 2010, the pioneering complexity theorist Stephen Cook and his collaborators invented a task, called the <a href="http://arxiv.org/abs/1005.2642">tree evaluation problem</a>, that they proved would be impossible for any algorithm with a space budget below a specific threshold. But there was a loophole. The proof relied on the same commonsense assumption that Paul and his colleagues had made decades earlier: Algorithms can’t store new data in space that’s already full.</p>
<p>For over a decade, complexity theorists tried to close that loophole. Then, in 2023, <a href="https://www.falsifian.org/">Cook’s son James</a> and a researcher named <a href="https://iuuk.mff.cuni.cz/~iwmertz/">Ian Mertz</a> blew it wide open, devising <a href="https://dl.acm.org/doi/10.1145/3618260.3649664">an algorithm</a> that solved the tree evaluation problem using <a href="https://www.quantamagazine.org/catalytic-computing-taps-the-full-power-of-a-full-hard-drive-20250218/">much less space</a> than anyone thought possible. The elder Cook’s proof had assumed that bits of data were like pebbles that have to occupy separate places in an algorithm’s memory. But it turns out that’s not the only way to store data. “We can actually think about these pebbles as things that we can squish a little bit on top of each other,” Beame said.</p>
</div>
    <figure>
        <div>
                            <p><img width="2101" height="2379" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36.webp 2101w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1519x1720.webp 1519w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-459x520.webp 459w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-768x870.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1357x1536.webp 1357w, https://www.quantamagazine.org/wp-content/uploads/2025/05/JamesCook_crColinMoris-36-1809x2048.webp 1809w" sizes="(max-width: 2101px) 100vw, 2101px">                </p>
                                <p><img width="1716" height="1817" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky.webp 1716w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1624x1720.webp 1624w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-491x520.webp 491w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-768x813.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/IanMertz_crMichalKoucky-1451x1536.webp 1451w" sizes="(max-width: 1716px) 100vw, 1716px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>James Cook (left) and Ian Mertz recently devised a new algorithm that solved a specific problem using much less space than anyone thought possible.</p>
            <p>Colin Morris; Michal Koucký</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Cook and Mertz’s algorithm roused the curiosity of many researchers, but it wasn’t clear that it had any applications beyond the tree evaluation problem. “Nobody saw how central it is to time versus space itself,” Wigderson said.</p>
<p>Ryan Williams was the exception. In spring 2024, a group of students gave a presentation about the Cook and Mertz paper as their final project in a class he was teaching. Their enthusiasm inspired him to take a closer look. As soon as he did, an idea jumped out at him. Cook and Mertz’s method, he realized, was really a general-purpose tool for reducing space usage. Why not use it to power a new universal simulation linking time and space — like the one designed by Hopcroft, Paul and Valiant, but better?</p>
<p>That classic result was a way to transform any algorithm with a given time budget into a new algorithm with a slightly smaller space budget. Williams saw that a simulation based on squishy pebbles would make the new algorithm’s space usage much smaller — roughly equal to the square root of the original algorithm’s time budget. That new space-efficient algorithm would also be much slower, so the simulation was not likely to have practical applications. But from a theoretical point of view, it was nothing short of revolutionary.</p>

<p>For 50 years, researchers had assumed it was impossible to improve Hopcroft, Paul and Valiant’s universal simulation. Williams’ idea — if it worked — wouldn’t just beat their record — it would demolish it.</p>
<p>“I thought about it, and I was like, ‘Well, that just simply can’t be true,’” Williams said. He set it aside and didn’t come back to it until that fateful day in July, when he tried to find the flaw in the argument and failed. After he realized that there was no flaw, he spent months writing and rewriting the proof to make it as clear as possible.</p>
<p>At the end of February, Williams finally <a href="https://arxiv.org/abs/2502.17779">put the finished paper online</a>. Cook and Mertz were as surprised as everyone else. “I had to go take a long walk before doing anything else,” Mertz said.</p>
<p>Valiant got a sneak preview of Williams’ improvement on his decades-old result during his morning commute. For years, he’s taught at Harvard University, just down the road from Williams’ office at MIT. They’d met before, but they didn’t know they lived in the same neighborhood until they bumped into each other on the bus on a snowy February day, a few weeks before the result was public. Williams described his proof to the startled Valiant and promised to send along his paper.</p>
<p>“I was very, very impressed,” Valiant said. “If you get any mathematical result which is the best thing in 50 years, you must be doing something right.”</p>
<h2><strong>PSPACE: The Final Frontier</strong></h2>
<p>With his new simulation, Williams had proved a positive result about the computational power of space: Algorithms that use relatively little space can solve all problems that require a somewhat larger amount of time. Then, using just a few lines of math, he flipped that around and proved a negative result about the computational power of time: At least a few problems can’t be solved unless you use more time than space. That second, narrower result is in line with what researchers expected. The weird part is how Williams got there, by first proving a result that applies to all algorithms, no matter what problems they solve.</p>
<p>“I still have a hard time believing it,” Williams said. “It just seems too good to be true.”</p>
</div>
    <figure>
        <div>
                            <p><img width="1967" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp" alt="" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-scaled.webp 1967w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1322x1720.webp 1322w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-400x520.webp 400w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-768x999.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1180x1536.webp 1180w, https://www.quantamagazine.org/wp-content/uploads/2025/05/RyanWilliams-cr.KatherineTaylor-Stairs-1574x2048.webp 1574w" sizes="(max-width: 1967px) 100vw, 1967px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Williams used Cook and Mertz’s technique to establish a stronger link between space and time — the first progress on that problem in 50 years.</p>
            <p>Katherine Taylor for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Phrased in qualitative terms, Williams’ second result may sound like the long-sought solution to the P versus PSPACE problem. The difference is a matter of scale. P and PSPACE are very broad complexity classes, while Williams’ results work at a finer level. He established a quantitative gap between the power of space and the power of time, and to prove that PSPACE is larger than P, researchers will have to make that gap much, much wider.</p>
<p>That’s a daunting challenge, akin to prying apart a sidewalk crack with a crowbar until it’s as wide as the Grand Canyon. But it might be possible to get there by using a modified version of Williams’ simulation procedure that repeats the key step many times, saving a bit of space each time. It’s like a way to repeatedly ratchet up the length of your crowbar — make it big enough, and you can pry open anything. That repeated improvement doesn’t work with the current version of the algorithm, but researchers don’t know whether that’s a fundamental limitation.</p>
        
        
<p>“It could be an ultimate bottleneck, or it could be a 50-year bottleneck,” Valiant said. “Or it could be something which maybe someone can solve next week.”</p>
<p>If the problem is solved next week, Williams will be kicking himself. Before he wrote the paper, he spent months trying and failing to extend his result. But even if such an extension is not possible, Williams is confident that more space exploration is bound to lead somewhere interesting — perhaps progress on an entirely different problem.</p>
<p>“I can never prove precisely the things that I want to prove,” he said. “But often, the thing I prove is way better than what I wanted.”</p>
<p><em>Editor’s note: Scott Aaronson is a member of&nbsp;</em>Quanta Magazine<em>’s&nbsp;</em><a href="https://www.quantamagazine.org/about/"><em>advisory board</em></a><em>.</em></p>
</div>
                
                
            </div></div>]]></description>
        </item>
    </channel>
</rss>