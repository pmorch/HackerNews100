(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Aug 2025 13:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Go is still not good (192 pts)]]></title>
            <link>https://blog.habets.se/2025/07/Go-is-still-not-good.html</link>
            <guid>44982491</guid>
            <pubDate>Fri, 22 Aug 2025 09:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.habets.se/2025/07/Go-is-still-not-good.html">https://blog.habets.se/2025/07/Go-is-still-not-good.html</a>, See on <a href="https://news.ycombinator.com/item?id=44982491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">


  

  <div itemprop="articleBody">
    <p>Previous posts <a href="https://blog.habets.se/2013/10/Why-Go-is-not-my-favourite-language.html">Why Go is not my favourite language</a> and <a href="https://blog.habets.se/2022/02/Go-programs-are-not-portable.html">Go programs
are not portable</a> have me critiquing Go for over a decade.</p>

<p>These things about Go are bugging me more and more. Mostly because they’re so
unnecessary. The world knew better, and yet Go was created the way it was.</p>

<p>For readers of previous posts you’ll find some things repeated here. Sorry
about that.</p>

<h2 id="error-variable-scope-is-forced-to-be-wrong">Error variable scope is forced to be wrong</h2>

<p>Here’s an example of the language forcing you to do the wrong thing. It’s very
helpful for the reader of code (and code is read more often than it’s written),
to minimize the scope of a variable. If by mere syntax you can tell the reader
that a variable is just used in these two lines, then that’s a good thing.</p>

<p>Example:</p>

<div><pre><code><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>foo</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
   </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>(enough has been said about this verbose repeated boilerplate that I don’t have
to. I also don’t particularly care)</p>

<p>So that’s fine. The reader knows <code>err</code> is here and only here.</p>

<p>But then you encounter this:</p>

<div><pre><code><span>bar</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>foo</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>=</span><span> </span><span>foo2</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>[</span><span>…</span><span> </span><span>a</span><span> </span><span>lot</span><span> </span><span>of</span><span> </span><span>code</span><span> </span><span>below</span><span> </span><span>…</span><span>]</span><span>
</span></code></pre>
</div>

<p>Wait, what? Why is <code>err</code> reused for <code>foo2()</code>? Is there’s something subtle I’m
not seeing? Even if we change that to <code>:=</code>, we’re left to wonder why <code>err</code> is
in scope for (potentially) the rest of the function. Why? Is it read later?</p>

<p>Especially when looking for bugs, an experienced coder will see these things
and slow down, because here be dragons. Ok, now I’ve wasted a couple of seconds
on the red herring of reusing <code>err</code> for <code>foo2()</code>.</p>

<p>Is a bug perhaps that the function ends with this?</p>

<div><pre><code><span>// Return foo99() error. (oops, that's not what we're doing)</span><span>
</span><span>foo99</span><span>()</span><span>
</span><span>return</span><span> </span><span>err</span><span> </span><span>// This is `err` from way up there in the foo() call.</span><span>
</span></code></pre>
</div>

<p>Why does the scope of <code>err</code> extend way beyond where it’s relevant?</p>

<p>The code would have been so much easier to read if only <code>err</code>’s scope had been
smaller. But that’s not syntactically possible in Go.</p>

<p>This was not thought through. Deciding on this was not thinking, it was typing.</p>

<h2 id="two-types-of-nil">Two types of nil</h2>

<p>Look at this nonsense:</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>type</span><span> </span><span>I</span><span> </span><span>interface</span><span>{}</span><span>
</span><span>type</span><span> </span><span>S</span><span> </span><span>struct</span><span>{}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>var</span><span> </span><span>i</span><span> </span><span>I</span><span>
    </span><span>var</span><span> </span><span>s</span><span> </span><span>*</span><span>S</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>// nil nil</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>i</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>s</span><span> </span><span>==</span><span> </span><span>i</span><span>)</span><span> </span><span>// t,t,f: They're equal, but they're not.</span><span>
    </span><span>i</span><span> </span><span>=</span><span> </span><span>s</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>// nil nil</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>i</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>s</span><span> </span><span>==</span><span> </span><span>i</span><span>)</span><span> </span><span>// t,f,t: They are not equal, but they are.</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Go was not satisfied with one <a href="https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/">billion dollar
mistake</a>,
so they decided to have <strong>two</strong> flavors of <code>NULL</code>.</p>

<p>“What color is your nil?” — The two billion dollar mistake.</p>

<p>The reason for the difference boils down to again, not thinking, just typing.</p>

<h2 id="its-not-portable">It’s not portable</h2>

<p>Adding comment near the top of the file for conditional compilation must be the
dumbest thing ever. Anybody who’s actually tried to maintain a portable program
will tell you this will only cause suffering.</p>

<p>It’s an <a href="https://en.wikipedia.org/wiki/Aristotelian_physics">Aristotle way of the science</a> of designing a language; lock
yourself up in a room, and never test your hypotheses against reality.</p>

<p>The problem is that this is not year 350 BCE. We actually have experience that
aside from air resistance, heavy and light objects actually fall at the same
speed. And we have experience with portable programs, and would not do
something this dumb.</p>

<p>If this had been the year 350 BCE, then this could be forgiven. Science as we
know it hadn’t been invented yet. But this is after decades of very widely
available experience in portability.</p>

<p>More details in <a href="https://blog.habets.se/2022/02/Go-programs-are-not-portable.html">this post</a>.</p>

<h2 id="append-with-no-defined-ownership"><code>append</code> with no defined ownership</h2>

<p>What does this print?</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>func</span><span> </span><span>foo</span><span>(</span><span>a</span><span> </span><span>[]</span><span>string</span><span>)</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>=</span><span> </span><span>append</span><span>(</span><span>a</span><span>,</span><span> </span><span>"NIGHTMARE"</span><span>)</span><span>
</span><span>}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>:=</span><span> </span><span>[]</span><span>string</span><span>{</span><span>"hello"</span><span>,</span><span> </span><span>"world"</span><span>,</span><span> </span><span>"!"</span><span>}</span><span>
    </span><span>foo</span><span>(</span><span>a</span><span>[</span><span>:</span><span>1</span><span>])</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>a</span><span>)</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Probably <code>[hello NIGHTMARE !]</code>. Who wants that? Nobody wants that.</p>

<p>Ok, how about this?</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>func</span><span> </span><span>foo</span><span>(</span><span>a</span><span> </span><span>[]</span><span>string</span><span>)</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>=</span><span> </span><span>append</span><span>(</span><span>a</span><span>,</span><span> </span><span>"BACON"</span><span>,</span><span> </span><span>"THIS"</span><span>,</span><span> </span><span>"SHOULD"</span><span>,</span><span> </span><span>"WORK"</span><span>)</span><span>
</span><span>}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>:=</span><span> </span><span>[]</span><span>string</span><span>{</span><span>"hello"</span><span>,</span><span> </span><span>"world"</span><span>,</span><span> </span><span>"!"</span><span>}</span><span>
    </span><span>foo</span><span>(</span><span>a</span><span>[</span><span>:</span><span>1</span><span>])</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>a</span><span>)</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>If you guessed <code>[hello world !]</code>, then you know more than anybody should have
to know about quirks of a stupid programming language.</p>

<h2 id="defer-is-dumb"><code>defer</code> is dumb</h2>

<p>Even in a GC language, sometimes you just can’t wait to destroy a resource. It
really does need to run as we leave the local code, be it by normal return, or
via an exception (aka panic).</p>

<p>What we clearly want is RAII, or something like it.</p>

<p>Java has it:</p>

<div><pre><code><span>try</span> <span>(</span><span>MyResource</span> <span>r</span> <span>=</span> <span>new</span> <span>MyResource</span><span>())</span> <span>{</span>
  <span>/*
  work with resource r, which will be cleaned up when the scope ends via
  .close(), not merely when the GC feels like it.
  */</span>
<span>}</span>
</code></pre>
</div>

<p>Python has it. Though Python is <em>almost</em> entirely refcounted, so one can pretty
much rely on the <code>__del__</code> finalizer being called. But if it’s important, then
there’s the <code>with</code> syntax.</p>

<div><pre><code><span>with</span> <span>MyResource</span><span>()</span> <span>as</span> <span>res</span><span>:</span>
  <span># some code. At end of the block __exit__ will be called on res.</span>
</code></pre>
</div>

<p>Go? Go makes you go read the manual and see if this particular resource needs
to have a defer function called on it, and which one.</p>

<div><pre><code><span>foo</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>myResource</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>defer</span><span> </span><span>foo</span><span>.</span><span>Close</span><span>()</span><span>
</span></code></pre>
</div>

<p>This is so dumb. Some resources need a defer destroy. Some don’t. Which ones?
Good fucking luck.</p>

<p>And you also regularly end up with stuff like this monstrosity:</p>

<div><pre><code><span>f</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>openFile</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>defer</span><span> </span><span>f</span><span>.</span><span>Close</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>f</span><span>.</span><span>Write</span><span>(</span><span>something</span><span>());</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>f</span><span>.</span><span>Close</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Yes, this is what you NEED to do to safely write something to a file in Go.</p>

<p>What’s this, a <em>second</em> <code>Close()</code>? Oh yeah, of course that’s needed. Is it even
safe to double-close, or does my defer need to check for that? It happens to be
safe on <code>os.File</code>, but on other things: WHO KNOWS?!</p>

<h2 id="the-standard-library-swallows-exceptions-so-all-hope-is-lost">The standard library swallows exceptions, so all hope is lost</h2>

<p>(Largely a repeat of part of <a href="https://blog.habets.se/2013/10/Why-Go-is-not-my-favourite-language.html">a previous post</a>)</p>

<p>Go says it doesn’t have exceptions. Go makes it extremely awkward to use
exceptions, because they want to punish programmers who use them.</p>

<p>Ok, fine so far.</p>

<p>But all Go programmers must still write exception safe code. Because while
<em>they</em> don’t use exceptions, other code will. Things will panic.</p>

<p>So you need, not should, NEED, to write code like:</p>

<div><pre><code><span>func</span><span> </span><span>(</span><span>f</span><span> </span><span>*</span><span>Foo</span><span>)</span><span> </span><span>foo</span><span>()</span><span> </span><span>{</span><span>
    </span><span>f</span><span>.</span><span>mutex</span><span>.</span><span>Lock</span><span>()</span><span>
    </span><span>defer</span><span> </span><span>f</span><span>.</span><span>mutex</span><span>.</span><span>Unlock</span><span>()</span><span>
    </span><span>f</span><span>.</span><span>bar</span><span>()</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>What is this stupid middle endian system? That’s dumb just like putting the day
in the middle of a date is dumb. MMDDYY, honestly? (separate rant)</p>

<p>But panic will terminate the program, they say, so why do you care if you
unlock a mutex five milliseconds before it exits anyway?</p>

<p>Because what if something swallows that exception and carries on as normal, and
you’re now stuck with a locked mutex?</p>

<p>But surely nobody would do that? Reasonable and strict coding standards would
surely prevent it, under penalty of being fired?</p>

<p>The standard library does that. <code>fmt.Print</code> when calling <code>.String()</code>, and the
standard library HTTP server does that, for exceptions in the HTTP handlers.</p>

<p>All hope is lost. You MUST write exception safe code. But you can’t use
exceptions. You can only have the downsides of exceptions be thrust upon you.</p>

<p>Don’t let them gaslight you.</p>

<h2 id="sometimes-things-arent-utf-8">Sometimes things aren’t UTF-8</h2>

<p>If you stuff random binary data into a <code>string</code>, Go just steams along, as
described <a href="https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride">in this post</a>.</p>

<p>Over the decades I have lost data to tools skipping non-UTF-8 filenames. I
should not be blamed for having files that were named before UTF-8 existed.</p>

<p>Well… I had them. They’re gone now. They were silently skipped in a
backup/restore.</p>

<p>Go wants you to continue losing data. Or at least, when you lose data, it’ll
say “well, what (encoding) was the data wearing?”.</p>

<p>Or how about you just do something more thought through, when you design a
language? How about doing the right thing, instead of the obviously wrong
simple thing?</p>

<h2 id="memory-use">Memory use</h2>

<p>Why do I care about memory use? RAM is cheap. Much cheaper than the time it
takes to read this blog post. I care because my service runs on a cloud
instance where you actually pay for RAM. Or you run containers, and you want to
run a thousand of them on the same machine. Your data may <a href="https://yourdatafitsinram.net/">fit in
RAM</a>, but it’s still expensive if you have to
give your thousand containers 4TiB of RAM instead of 1TiB.</p>

<p>You can manually trigger a GC run with <code>runtime.GC()</code>, but “oh no don’t do
that”, they say, “it’ll run when it has to, just trust it”.</p>

<p>Yeah, 90% of the time, that works every time. But then it doesn’t.</p>

<p>I rewrote some stuff in another language because over time the Go version would
use more and more memory.</p>

<h2 id="it-didnt-have-to-be-this-way">It didn’t have to be this way</h2>

<p>We knew better. This was not the COBOL debate over whether to use symbols or
English words.</p>

<p>And it’s not like when we didn’t know at the time that <a href="https://blog.habets.se/2022/08/Java-a-fractal-of-bad-experiments.html">Java’s ideas were
bad</a>, because we did know Go’s ideas were bad.</p>

<p>We already knew better than Go, and yet now we’re stuck with bad Go codebases.</p>

<h2 id="other-peoples-posts">Other people’s posts</h2>

<ul>
  <li>https://www.uber.com/en-GB/blog/data-race-patterns-in-go/</li>
  <li>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</li>
  <li>https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride</li>
</ul>


  </div>

  
  
  
  
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Io_uring, kTLS and Rust for zero syscall HTTPS server (294 pts)]]></title>
            <link>https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html</link>
            <guid>44980865</guid>
            <pubDate>Fri, 22 Aug 2025 03:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html">https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=44980865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">


  

  <div itemprop="articleBody">
    <p>Around the turn of the century we started to get a bigger need for high
capacity web servers. For example there was <a href="https://en.wikipedia.org/wiki/C10k_problem">the C10k problem</a> paper.</p>

<p>At the time, the kinds of things done to reduce work done per request was
pre-forking the web server. This means a request could be handled without an
expensive process creation.</p>

<p>Because yes, creating a new process for every request used to be something
perfectly normal.</p>

<p>Things did get better. People learned how to create threads, making things more
light weight. Then they switched to using <code>poll()</code>/<code>select()</code>, in order to not
just spare the process/thread creation, but the whole context switch.</p>

<p>I remember a comment on <a href="https://en.wikipedia.org/wiki/Kuro5hin">Kuro5hin</a> from anakata, the creator of both The
Pirate Bay and the web server that powered it, along the lines of “I am select()
of borg, resistance is futile”, mocking someone for not understanding how to
write a scalable web server.</p>

<p>But <code>select()</code>/<code>poll()</code> also doesn’t scale. If you have ten thousand
connections, that’s an array of ten thousand integers that need to be sent to
the kernel for every single iteration of your request handling loop.</p>

<p>Enter <code>epoll</code> (<code>kqueue</code> on other operating systems, but I’m focusing on Linux
here). Now that’s better. The main loop is now:</p>

<div><pre><code>  set_up_epoll()
  while True:
    new, read, write = epoll()
    epoll_add_connections(new)
    for con in read:
      process(con.read())
      if con.read_all_we_need:
        epoll_remove_read_op(con)
    for con in write:
      con.write_buffer()
      if con.buffer_empty:
        epoll_remove_write_op(con)
</code></pre>
</div>

<p>All the syscalls are pretty cheap. <code>epoll()</code> only deals in deltas, and it
doesn’t have to be re-told the thousands of active connections.</p>

<p>But they’re not without cost. Once we’ve gotten this far, the cost of a syscall
is actually a significant part of the total remaining cost.</p>

<p>We’re here going to ignore improvements like <code>sendfile()</code> and <code>splice()</code>, and
instead jump to…</p>



<p>Instead of performing a syscall for everything we want to do, commanding the
kernel to do this or that, io_uring lets us just keep writing orders to a
queue, and letting the kernel consume that queue asynchronously.</p>

<p>For example, we can put <code>accept()</code> into the queue. The kernel will pick that
up, wait for an incoming connection, and when it arrives it’ll put a
“completion” into the completion queue.</p>

<p>The web server can then check the completion queue. If there’s a completion
there, it can act on it.</p>

<p>This way the web server can queue up all kinds of operations that were
previously “expensive” syscalls by simply writing them to memory. That’s it.
And then it’ll read the results from another part of memory. That’s it.</p>

<p>In order to avoid busy looping, both the kernel and the web server will only
busy-loop checking the queue for a little bit (configurable, but think
milliseconds), and if there’s nothing new, the web server will do a syscall to
“go to sleep” until something gets added to the queue.</p>

<p>Similarly on the kernel side, the kernel will stop busy-looping if there’s
nothing new, and needs a syscall to start busylooping again.</p>

<p>This sounds like it would be tricky to optimize, but it’s not. In the end the
web server just puts stuff on the queue, and calls a library function that only
does that syscall if the kernel actually has stopped busylooping.</p>

<p>This means that a busy web server can serve all of its queries without even once
(after setup is done) needing to do a syscall. As long as queues keep getting
added to, <code>strace</code> will show <em>nothing</em>.</p>

<h2 id="one-thread-per-core">One thread per core</h2>

<p>Since CPUs today have many cores, ideally you want to run exactly one thread
per core, bind it to that core, and not share any read-write data structure.</p>

<p>For <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> hardware, you also want to make sure that a thread only
accesses memory on the local NUMA node. <a href="https://youtu.be/36qZYL5RlgY">This netflix talk</a> has some
interesting stuff on NUMA and high volume HTTP delivery.</p>

<p>The request load will still not be perfectly balanced between the threads (and
therefore cores), but I guess fixing that would have to be the topic of a
future post.</p>

<h2 id="memory-allocations">Memory allocations</h2>

<p>We will still have memory allocations though, both on the kernel and web server
side. Memory allocations in user space will eventually need syscalls.</p>

<p>For the web server side, you can pre-allocate a fixed chunk for every
connection, and then have everything about that connection live there. That way
new connections don’t need syscalls, memory doesn’t get fragmented, and you
don’t run the risk of running out of memory.</p>

<p>On the kernel side each connection will still need buffers for incoming and
outgoing bytes. This may be somewhat controllable via socket options, but again
it’ll have to be the subject of a future post.</p>

<p>Try to not run out of RAM. Bad things tend to happen.</p>

<h2 id="ktls">kTLS</h2>

<p><a href="https://docs.kernel.org/networking/tls-offload.html">kTLS</a> is a feature of the Linux kernel where an application can hand off
the job of encryption/decryption to the kernel. The application still has to
perform the TLS handshake, but after that it can enable kTLS and pretend that
it’s all sent in plaintext.</p>

<p>You may say that this doesn’t actually speed anything up, it just moves <em>where</em>
encryption was done. But there are gains:</p>

<ol>
  <li>This means that <code>sendfile()</code> can be used, removing the need to copy a bunch
of data between user space and kernel space.</li>
  <li>If the network card has hardware support for it, the crypto operation may
actually be offloaded from the CPU onto the network card, leaving the CPU to
do better things.</li>
</ol>

<h2 id="descriptorless-files">Descriptorless files</h2>

<p>Another optimization is to avoid passing file descriptors back and forth
between user space and kernel space. The mapping between file descriptors and
io_uring apparently has overhead.</p>

<p>So in comes <a href="https://lwn.net/Articles/863071/">descriptorless files</a> via
<a href="https://docs.rs/io-uring/latest/io_uring/struct.Submitter.html#method.register_files"><code>register_files</code></a>.</p>

<p>Now the supposed file descriptor numbers that user space sees are just
integers. They don’t show up in <code>/proc/pid/fd</code>, and can only be used with
io_uring. They’re still capped by the <code>ulimit</code> file descriptor limit, though.</p>

<h2 id="tarweb">tarweb</h2>

<p>In order to learn these technologies better, I built <a href="https://github.com/ThomasHabets/tarweb">a web server
incorporating all these things</a>.</p>

<p>It’s named <code>tarweb</code> because it’s a web server that serves the content of a
single tar file.</p>

<p>Rust, io_uring, and kTLS. Not exactly the most common combination. I found that
io_uring and kTLS didn’t play super well together. Enabling kTLS requires three
<code>setsockopt()</code> calls, and io_uring doesn’t support <code>setsockopt</code> (until they
merge <a href="https://github.com/tokio-rs/io-uring/pull/320">my PR</a>, that is).</p>

<p>And the <code>ktls</code> crate, part of <code>rustls</code>, only allows you to call the synchronous
<code>setsockopt()</code>, not export the needed struct for me to pass to my new
io_uring <code>setsockopt</code>. <a href="https://github.com/rustls/ktls/pull/54">Another pr sent</a>.</p>

<p>So with those two PRs merged, it’s working great.</p>

<p>tarweb is far from perfect. The code needs a lot of work, and there’s no
guarantee that the TLS library (rustls) doesn’t do memory allocations during
handshakes. But it does serve https without even one syscall on a per request
basis. And that’s pretty cool.</p>

<h2 id="benchmarks">Benchmarks</h2>

<p>I have not done any benchmarks yet. I want to clean the code up first.</p>

<h2 id="io-uring-and-safety">io-uring and safety</h2>

<p>One thing making io_uring more complex than synchronous syscalls is that any
buffer needs to stay in memory until the operation is marked completed by
showing up in the completion queue.</p>

<p>For example when submitting a <code>write</code> operation, the memory location of those
bytes must not be deallocated or overwritten.</p>

<p>The <code>io-uring</code> crate doesn’t help much with this. The API doesn’t allow the
borrow checker to protect you at compile time, and I don’t see it doing any
runtime checks either.</p>

<p>I feel like I’m back in C++, where any mistake can blow your whole leg off.
It’s a miracle that I’ve not seen a segfault.</p>

<p>Someone should make a <code>safer-ring</code> crate or similar, using the powers of
<a href="https://blog.cloudflare.com/pin-and-unpin-in-rust/">pinning</a> and/or borrows or something, to achieve Rust’s normal “if it
compiles, then it’s correct”.</p>


  </div>

  
  
  
  
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything Is Correlated (171 pts)]]></title>
            <link>https://gwern.net/everything</link>
            <guid>44980339</guid>
            <pubDate>Fri, 22 Aug 2025 02:05:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwern.net/everything">https://gwern.net/everything</a>, See on <a href="https://news.ycombinator.com/item?id=44980339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-metadata">
          
        <p>Anthology of sociology, statistical, or psychological papers discussing the observation that all real-world variables have non-zero correlations and the implications for statistical theory such as ‘null hypothesis testing’.</p>
        
      </div><div id="markdownBody"><div>
<blockquote>
<p>Statistical folklore asserts that “everything is correlated”: in any real-world dataset, most or all measured variables will have non-zero correlations, even between variables which appear to be completely independent of each other, and that these correlations are not merely sampling error flukes but will appear in large-scale datasets to arbitrarily designated levels of <a href="https://en.wikipedia.org/wiki/Statistical_significance" id="_P2KpkSBK" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_significance#bodyContent" title="Statistical-significance">statistical-significance</a> or posterior probability.</p>
<p>This raises serious questions for null-hypothesis statistical-significance testing, as it implies the null hypothesis of 0 will always be rejected with sufficient data, meaning that a failure to reject only implies insufficient data, and provides no actual test or confirmation of a theory. Even a directional prediction is minimally confirmatory since there is a 50% chance of picking the right direction at random.</p>
<p>It also has implications for conceptualizations of theories &amp; causal models, interpretations of structural models, and other statistical principles such as the “sparsity principle”.</p>
</blockquote>
</div>
<p>Knowing one variable tells you (a little) about everything else. In statistics &amp; psychology folklore, this idea circulates under many names: “everything is correlated”, “everything is related to everything else”, “crud factor”, “the null hypothesis is always false”, “coefficients are never zero”, “ambient correlational noise”, <a href="https://en.wikipedia.org/wiki/Edward_Thorndike" id="_eD7_4_zC" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Edward_Thorndike#bodyContent" title="Edward Thorndike">Thorndike’s</a> dictum (“in human nature good traits go together”<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>), etc. Closely related are the <a href="https://gwern.net/doc/www/citeseerx.ist.psu.edu/75d8bce1345921c88e4a5c0aa10ebf503f3f1429.pdf#page=518" id="_BDPfCSfr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/citeseerx.ist.psu.edu/75d8bce1345921c88e4a5c0aa10ebf503f3f1429.pdf#page=518" data-url-original="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.644.2669&amp;rep=rep1&amp;type=pdf#page=518" title="'In praise of sparsity and convexity', Tibshirani 2014">“bet on sparsity principle”</a><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>, <a href="https://en.wikipedia.org/wiki/Anna_Karenina_principle" id="_zDohaOlu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Anna_Karenina_principle#bodyContent" title="Anna Karenina principle">Anna Karenina principle</a>, <a href="https://en.wikipedia.org/wiki/Barry_Commoner" id="_OEwL_s8W" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Barry_Commoner#bodyContent" title="Barry Commoner">Barry Commoner’s</a> “first law of ecology” (“Everything is connected to everything else”) &amp; <a href="https://en.wikipedia.org/wiki/Waldo_R._Tobler" id="_2174Ovvd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Waldo_R._Tobler#bodyContent" title="Waldo R. Tobler">Waldo R. Tobler’s</a> <a href="https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography" id="_ToNCz_VP" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Tobler%27s_first_law_of_geography#bodyContent" title="Tobler's first law of geography">“first law of geography”</a> (<a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/a068a23457b8ececa6a739f92c264d24c511085f.pdf" id="tobler-1970" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/a068a23457b8ececa6a739f92c264d24c511085f.pdf" data-url-original="https://pdfs.semanticscholar.org/eaa5/eefedd4fa34b7de7448c0c8e0822e9fdf956.pdf" data-filesize-bytes="587763" data-filesize-percentage="37" title="'A computer movie simulating urban growth in the Detroit region', Tobler 1970">“everything is related to everything else, but near things are more related than distant things”</a>).<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>The core idea here is that in any real-world dataset, it is exceptionally unlikely that any particular relationship will be exactly 0 for reasons of arithmetic (eg. it may be impossible for a binary variable to be an equal percentage in 2 unbalanced groups); prior probability (0 is only one number out of the infinite reals); and because real-world properties &amp; traits are linked by a myriad of causal networks, dynamics, &amp; <a href="https://en.wikipedia.org/wiki/Latent_and_observable_variables" id="_hM0a8AZP" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Latent_and_observable_variables#bodyContent" title="Latent and observable variables">latent</a> variables (eg. the <a href="https://en.wikipedia.org/wiki/Genetic_correlation" id="_EFM6e8jH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genetic_correlation#bodyContent" title="Genetic correlation">genetic correlations</a> which affect all human traits, see <a href="#genetic-correlations">heat maps in appendix</a> for visualizations) which mutually affect each other which will produce genuine correlations between apparently-independent variables, and these correlations may be of surprisingly large &amp; important size.</p>
<p>These reasons are unaffected by sample size and are not simply due to ‘small <em>n</em>’. If we simulate out uncorrelated random variables, even with small sizes, they quickly approach absolute correlations of ~0, and few will be of meaningful size like |<em>r</em>| &gt; 0.10:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span># Simulation of the 'crud factor' in psychology, where variables seem to always be intercorrelated non-zero, with a rough absolute correlation of 0.1.</span></span>
<span id="cb1-2"><span># I would like to simulate out the null hypothesis of completely uncorrelated variables in plausible dataset sizes.</span></span>
<span id="cb1-3"><span># So this is a R Monte Carlo simulation to simulate a multivariate normal distribution of 1,000 independent, uncorrelated N(0,1) variables and drawing 1,000 datapoints from it; calculate the sample r correlation of all the variables, and what fraction are correlated r &gt; |0.1|. Then let's Monte Carlo that, say, 100 times (to avoid taking too long) and plot the averaged distributions of the |r| histogram.</span></span>
<span id="cb1-4"><span># Output:</span></span>
<span id="cb1-5"><span># # Mean proportion of |r| &gt; 0.1: 0.002 (SD: 0.000)</span></span>
<span id="cb1-6"><span>library</span>(MASS)</span>
<span id="cb1-7"><span>library</span>(ggplot2)</span>
<span id="cb1-8"><span>library</span>(reshape2)</span>
<span id="cb1-9"><span>library</span>(parallel)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span># Parameters</span></span>
<span id="cb1-12">n_vars <span>&lt;-</span> <span>1000</span></span>
<span id="cb1-13">n_samples <span>&lt;-</span> <span>1000</span></span>
<span id="cb1-14">n_sims <span>&lt;-</span> <span>100</span></span>
<span id="cb1-15">threshold <span>&lt;-</span> <span>0.1</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"><span># Function to run one simulation and return correlation metrics</span></span>
<span id="cb1-18">run_simulation <span>&lt;-</span> <span>function</span>(seed, n_vars, n_samples, threshold) {</span>
<span id="cb1-19">  <span>set.seed</span>(seed)</span>
<span id="cb1-20">  sigma <span>&lt;-</span> <span>diag</span>(n_vars)</span>
<span id="cb1-21">  data <span>&lt;-</span> <span>mvrnorm</span>(<span>n =</span> n_samples,</span>
<span id="cb1-22">                  <span>mu =</span> <span>rep</span>(<span>0</span>, n_vars),</span>
<span id="cb1-23">                  <span>Sigma =</span> sigma)</span>
<span id="cb1-24"></span>
<span id="cb1-25">  cors <span>&lt;-</span> <span>cor</span>(data)</span>
<span id="cb1-26">  cors_upper <span>&lt;-</span> cors[<span>upper.tri</span>(cors)]</span>
<span id="cb1-27"></span>
<span id="cb1-28">  <span># Return both the histogram counts and proportion above threshold</span></span>
<span id="cb1-29">  hist_data <span>&lt;-</span> <span>hist</span>(<span>abs</span>(cors_upper), <span>breaks =</span> <span>seq</span>(<span>0</span>, <span>1</span>, <span>by =</span> <span>0.01</span>), <span>plot =</span> <span>FALSE</span>)</span>
<span id="cb1-30"></span>
<span id="cb1-31">  <span>return</span>(<span>list</span>(</span>
<span id="cb1-32">    <span>hist_counts =</span> hist_data<span>$</span>counts,</span>
<span id="cb1-33">    <span>prop_above_thresh =</span> <span>mean</span>(<span>abs</span>(cors_upper) <span>&gt;</span> threshold)</span>
<span id="cb1-34">  ))</span>
<span id="cb1-35">}</span>
<span id="cb1-36"></span>
<span id="cb1-37"><span># Set up parallel processing</span></span>
<span id="cb1-38">n_cores <span>&lt;-</span> <span>detectCores</span>() <span>-</span> <span>1</span></span>
<span id="cb1-39">cl <span>&lt;-</span> <span>makeCluster</span>(n_cores)</span>
<span id="cb1-40"></span>
<span id="cb1-41"><span># Export required packages and variables to the cluster</span></span>
<span id="cb1-42"><span>clusterEvalQ</span>(cl, {</span>
<span id="cb1-43">  <span>library</span>(MASS)</span>
<span id="cb1-44">})</span>
<span id="cb1-45"><span>clusterExport</span>(cl, <span>c</span>(<span>"n_vars"</span>, <span>"n_samples"</span>, <span>"threshold"</span>))</span>
<span id="cb1-46"></span>
<span id="cb1-47"><span># Run simulations in parallel</span></span>
<span id="cb1-48">seeds <span>&lt;-</span> <span>1</span><span>:</span>n_sims</span>
<span id="cb1-49">results <span>&lt;-</span> <span>parLapply</span>(cl, seeds, run_simulation,</span>
<span id="cb1-50">                    <span>n_vars =</span> n_vars,</span>
<span id="cb1-51">                    <span>n_samples =</span> n_samples,</span>
<span id="cb1-52">                    <span>threshold =</span> threshold)</span>
<span id="cb1-53"></span>
<span id="cb1-54"><span># Clean up</span></span>
<span id="cb1-55"><span>stopCluster</span>(cl)</span>
<span id="cb1-56"></span>
<span id="cb1-57"><span># Process results</span></span>
<span id="cb1-58">props <span>&lt;-</span> <span>sapply</span>(results, <span>function</span>(x) x<span>$</span>prop_above_thresh)</span>
<span id="cb1-59">mean_prop <span>&lt;-</span> <span>mean</span>(props)</span>
<span id="cb1-60"></span>
<span id="cb1-61"><span># Average the histogram counts across simulations</span></span>
<span id="cb1-62">avg_counts <span>&lt;-</span> <span>Reduce</span>(<span>'+'</span>, <span>lapply</span>(results, <span>function</span>(x) x<span>$</span>hist_counts)) <span>/</span> n_sims</span>
<span id="cb1-63"></span>
<span id="cb1-64"><span># Calculate total possible correlations for one simulation</span></span>
<span id="cb1-65">total_cors <span>&lt;-</span> (n_vars <span>*</span> (n_vars <span>-</span> <span>1</span>)) <span>/</span> <span>2</span></span>
<span id="cb1-66"></span>
<span id="cb1-67"><span># Create plotting data</span></span>
<span id="cb1-68">plot_data <span>&lt;-</span> <span>data.frame</span>(</span>
<span id="cb1-69">  <span>correlation =</span> <span>seq</span>(<span>0</span>, <span>0.99</span>, <span>by =</span> <span>0.01</span>)[<span>1</span><span>:</span><span>length</span>(avg_counts)],</span>
<span id="cb1-70">  <span>count =</span> avg_counts</span>
<span id="cb1-71">)</span>
<span id="cb1-72"></span>
<span id="cb1-73"><span># Create histogram</span></span>
<span id="cb1-74">p1 <span>&lt;-</span> <span>ggplot</span>(plot_data, <span>aes</span>(<span>x =</span> correlation, <span>y =</span> count<span>/</span>total_cors <span>*</span> <span>100</span>)) <span>+</span></span>
<span id="cb1-75">  <span>geom_bar</span>(<span>stat =</span> <span>"identity"</span>, <span>fill =</span> <span>"blue"</span>, <span>alpha =</span> <span>0.7</span>) <span>+</span></span>
<span id="cb1-76">  <span>theme_bw</span>(<span>base_size =</span> <span>40</span>) <span>+</span></span>
<span id="cb1-77">  <span>geom_vline</span>(<span>xintercept =</span> threshold, <span>color =</span> <span>"red"</span>, <span>linetype =</span> <span>"dashed"</span>, <span>size=</span><span>3</span>) <span>+</span></span>
<span id="cb1-78">  <span>theme</span>(<span>plot.title =</span> <span>element_text</span>(<span>face =</span> <span>"bold"</span>)) <span>+</span></span>
<span id="cb1-79">  <span>scale_y_continuous</span>(</span>
<span id="cb1-80">    <span>labels =</span> <span>function</span>(x) <span>paste0</span>(<span>round</span>(x, <span>1</span>), <span>"%"</span>),</span>
<span id="cb1-81">    <span>name =</span> <span>"Percentage of All Inter-Correlations"</span>,</span>
<span id="cb1-82">    <span>expand =</span> <span>c</span>(<span>0</span>, <span>0</span>),</span>
<span id="cb1-83">    <span>limits =</span> <span>c</span>(<span>0</span>, <span>NA</span>)</span>
<span id="cb1-84">  ) <span>+</span></span>
<span id="cb1-85">  <span>scale_x_continuous</span>(</span>
<span id="cb1-86">    <span>name =</span> <span>expression</span>(<span>paste</span>(<span>"Absolute Correlation (|"</span>, <span>italic</span>(<span>"r"</span>), <span>"|)"</span>)),</span>
<span id="cb1-87">    <span>breaks =</span> <span>seq</span>(<span>0</span>, <span>0.13</span>, <span>by =</span> <span>0.02</span>),</span>
<span id="cb1-88">    <span>expand =</span> <span>c</span>(<span>0</span>, <span>0</span>),</span>
<span id="cb1-89">    <span>limits =</span> <span>c</span>(<span>0</span>, <span>0.13</span>)</span>
<span id="cb1-90">  ) <span>+</span></span>
<span id="cb1-91">  <span>annotate</span>(<span>"text"</span>,</span>
<span id="cb1-92">           <span>x =</span> <span>0.101</span>,</span>
<span id="cb1-93">           <span>y =</span> <span>max</span>(avg_counts<span>/</span>total_cors <span>*</span> <span>100</span>)<span>/</span><span>3</span>,</span>
<span id="cb1-94">           <span>label =</span> <span>substitute</span>(<span>paste</span>(value, <span>"% exceed |"</span>, <span>italic</span>(<span>"r"</span>), <span>"| &gt; "</span>, thresh),</span>
<span id="cb1-95">                            <span>list</span>(<span>value =</span> <span>sprintf</span>(<span>"%.1f"</span>, <span>100</span> <span>*</span> mean_prop),</span>
<span id="cb1-96">                                 <span>thresh =</span> <span>sprintf</span>(<span>"%.2f"</span>, threshold))),</span>
<span id="cb1-97">           <span>size =</span> <span>10</span>,</span>
<span id="cb1-98">           <span>hjust =</span> <span>0</span>) <span>+</span></span>
<span id="cb1-99">  <span>labs</span>(<span>title =</span> <span>"Distribution of Absolute Correlations"</span>)</span>
<span id="cb1-100"></span>
<span id="cb1-101"><span># Print summary statistics</span></span>
<span id="cb1-102"><span>cat</span>(<span>sprintf</span>(<span>"Mean proportion of |r| &gt; 0.1: %.3f (SD: %.3f)</span><span>\n</span><span>"</span>, <span>mean</span>(props), <span>sd</span>(props)))</span>
<span id="cb1-103"></span>
<span id="cb1-104"><span>print</span>(p1)</span></code></pre></div>
<figure>
<p><img alt="A Monte Carlo simulation of an uncorrelated multivariate normal in R shows that even with p = 1,000 and n = 1,000 (many variables and a small dataset), we rarely will observe ‘crud factor’-style correlations between uncorrelated variables, and so the crud factor is not a statistical triviality." data-aspect-ratio="350 / 221" decoding="async" height="884" loading="lazy" src="https://gwern.net/doc/statistics/probability/2025-01-29-gwern-r-uncorrelatedmultivariatenormalsarerarelycorrelatedrgreaterthanpoint1.jpg" width="1400"></p>
<figcaption><p>A <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method" id="_Yik66BZ1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Monte_Carlo_method#bodyContent" title="Monte Carlo method">Monte Carlo simulation</a> of an uncorrelated multivariate normal in R shows that even with <em>p</em> = 1,000 and <em>n</em> = 1,000 (many variables and a small dataset), we rarely will observe ‘crud factor’-style correlations between uncorrelated variables, and so the crud factor is not a statistical triviality.</p></figcaption>
</figure>
<p>The claim is generally backed up by personal experience and reasoning, although in a few instances like Meehl large datasets are mentioned in which almost all variables are correlated at high levels of statistical-significance.</p>
<section id="importance">
<h2><a href="#importance" title="Link to section: § 'Importance'">Importance</a></h2>
<p>This claim has several implications:</p>
<ol>
<li><p><strong>Sharp null hypotheses are meaningless</strong>: The most commonly mentioned, and the apparent motivation for early discussions, is that in the null-hypothesis <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_test" id="_OtaIDcz3" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_hypothesis_test#bodyContent" title="Statistical hypothesis test">significance-testing</a> paradigm dominant in psychology and many sciences, any sharp null-hypothesis such as a parameter (like a Pearson’s <em>r</em> correlation) being exactly equal to 0 is known—in advance—to already be false and so it will inevitably be rejected as soon as sufficient data collection permits sampling to the foregone conclusion.</p>
<p>The existence of pervasive correlations, in addition to the presence of systematic error<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, guarantees nonzero ‘effects’. This renders the meaning of significance-testing unclear; it is calculating precisely the odds of the data under scenarios known <em>a priori</em> to be false.</p></li>
<li><p><strong>Directional hypotheses are little better</strong>: better null-hypotheses, such as &gt;0 or &lt;0, are also problematic since if the true value of a parameter is never 0 then one’s theories have at least a 50-50 chance of guessing the right direction and so correct ‘predictions’ of the sign count for little.</p>
<p>This renders any successful predictions of little value.</p></li>
<li><p><strong>Model interpretation is difficult</strong>: This extensive intercorrelation threatens many naive statistical models or theoretical interpretations thereof, quite aside from <em>p</em>-values</p>
<p>For example, given the large amounts of <a href="https://en.wikipedia.org/wiki/Observational_error" id="_5Yg7bDxK" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Observational_error#bodyContent" title="Observational error">measurement error</a> in most sociological or psychological traits such as <a href="https://en.wikipedia.org/wiki/Socioeconomic_status" id="__fxxFvC3" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Socioeconomic_status#bodyContent" title="Socioeconomic status">SES</a>, home environment, or IQ, fully ‘controlling for’ a latent variable based on measured variables is difficult or impossible and said variable will in fact be correlated with the primary variable of interest, leading to <span id="gwern-notes-regression"></span> <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">“residual confounding”</a></p></li>
<li><p><strong>Intercorrelation implies causal networks</strong>: The empirical fact of extensive intercorrelations is consistent with the <a href="https://gwern.net/causality" id="gwern-causality" data-filesize-bytes="54123" data-filesize-percentage="42" title="'Why Correlation Usually ≠ Causation', Gwern 2014">existence of complex causal networks &amp; latent variables</a> (often factors) linking all measured traits, such as extensive heritability &amp; genetic correlations of human traits, leading to <a href="https://gwern.net/correlation" id="gwern-correlation" data-filesize-bytes="144066" data-filesize-percentage="63" title="'How Often Does Correlation=Causality?', Gwern 2014">extensive examples of correlation ≠ causation</a>.</p>
<p>The existence of both “everything is correlated” and the success of the “bet on sparsity” principle suggests that these causal networks may be best thought of as having hubs or latent variables: there are a relatively few variables such as ‘arousal’ or ‘IQ’ which play central roles, explaining much of <a href="https://en.wikipedia.org/wiki/Variance" id="_yzXH8kID" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Variance#bodyContent" title="Variance">variance</a>, followed by almost all other variables accounting for a little bit each with most of their influence mediated through the key variables.</p>
<p>The fact that these variables can be successfully modeled as substantively linear or additive further implies that interactions between variables will be typically rare or small or both (implying further that most such hits will be false positives, as interactions are already harder to detect than main effects, and more so if they are <em>a priori</em> unlikely or of small size). Even extremely large &amp; deeply phenotyped datasets may struggle to achieve impressive improvements over baselines using the core variables (eg. <a href="https://www.pnas.org/doi/10.1073/pnas.1915006117" id="_0OTythr1" data-link-icon="PNAS" data-link-icon-type="text,quad" data-link-icon-color="#1f75b9" title="Measuring the predictability of life outcomes with a scientific mass collaboration"><span><span title="et al">Salganik</span><span> et al </span><span>2020</span></span></a>).</p>
<p>To the extent that these key variables are unmodifiable, the many peripheral variables may also be unmodifiable (which may be related to the <a href="https://gwern.net/doc/sociology/1987-rossi" id="rossi-2012" data-filesize-bytes="84889" data-filesize-percentage="50" title="The Iron Law Of Evaluation And Other Metallic Rules">broad failure of social intervention programs</a>). Any intervention on those peripheral variables, being ‘downstream’, will tend to either be ‘hollow’ or fade out or have no effect at all on the true desired goals no matter how consistently they are correlated.</p>
<p>On a more contemporary note, these theoretical &amp; empirical considerations also throw doubt on concerns about ‘algorithmic bias’ or inferences drawing on ‘protected classes’: not drawing on them may not be desirable, possible, or even meaningful.</p></li>
<li><p><strong>Uncorrelated variables may be meaningless</strong>: given this empirical reality, any variable which is uncorrelated with the major variables is suspicious (somewhat like the <a href="https://gwern.net/doc/genetics/heritable/2015-polderman.pdf" id="polderman-et-al-2015-02" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="10395460" data-filesize-percentage="95" title="'Meta-analysis of the heritability of human traits based on 50 years of twin studies', Polderman et al 2015">pervasiveness of heritability</a> in human traits renders traits with zero heritability suspicious, suggesting issues like measuring at the wrong time). The lack of correlation suggests that the analysis is underpowered, something has gone wrong in the construction of the variable/dataset, or that the variable is part of a system whose causal network renders conventional analyses dangerously misleading.</p>
<p>For example, the dataset may be corrupted by a systematic bias such as <a href="https://en.wikipedia.org/wiki/Statistical_conclusion_validity#Restriction_of_range" id="_FX37JfKA" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_conclusion_validity#bodyContent" title="Statistical conclusion validity § Restriction of range">range restriction</a> or a selection effect such as <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox" id="_vvExW5j4" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Simpson%27s_paradox#bodyContent" title="Simpson's paradox">Simpson’s paradox</a>, which erases from the data a correlation that actually exists. Or the data may be random noise, due to software error or fraud or extremely high levels of measurement error (such as <a href="https://gwern.net/doc/sociology/survey/lizardman/index" id="_aSDAgurb" data-filesize-bytes="26046" data-filesize-percentage="29" title="'Lizardman Constant in Surveys', Gwern 2013">“lizardman constant”</a> respondents answering at random); or the variable may not be real in the first place. Another possibility is that the variable is causally connected, in feedback loops (especially common in economics or biology), to another variable, in which case the standard statistical machinery is misleading—the classic example is Milton Friedman’s thermostat, noting that a thermostat would be almost entirely uncorrelated with room temperature.</p></li>
</ol>
<p>This idea, as suggested by the many names, is not due to any single theoretical or empirical result or researcher, but has been made many times by many different researchers in many contexts, circulating as informal ‘folklore’. To bring some order to this, I have compiled excerpts of some relevant references in chronological order. (Additional citations are welcome.)</p>
</section>
<section id="gosset-student-1904">
<h2><a href="#gosset-student-1904" title="Link to section: § 'Gosset / Student1904'">Gosset / <span><span>Student</span><span>1904</span></span></a></h2>
<p>A version of this is attributed to <a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset" id="_oxQwsTeg" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/William_Sealy_Gosset#bodyContent" title="William Sealy Gosset">William Sealy Gosset</a> (Student) in his <a href="https://gwern.net/doc/statistics/decision/1904-gosset.pdf" id="gosset-1904" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="608997" data-filesize-percentage="63" title="‘The Application Of The &quot;Law Of Error&quot; To The Work Of The Brewery’, Gosset 1904">1904 internal report</a><a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> by <a href="https://gwern.net/doc/statistics/decision/2008-ziliak.pdf" id="ziliak-2008" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="190871" data-filesize-percentage="32" title="Guinnessometrics: The Economic Foundation of 'Student's' _t_"><span><span>Ziliak</span><span>2008</span></span></a>:</p>
<blockquote>
<p>In early November <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, Gosset discussed his first breakthrough in an internal report entitled “The Application of the ‘Law of Error’ to the Work of the Brewery” (<span><span>Gosset</span><span>1904</span></span>; <em>Laboratory Report</em><span>, Nov. 3, <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, pg3). Gosset (p.&nbsp;3–16) wrote:</span></p>
<blockquote>
<p>Results are only valuable when the amount by which they probably differ from the truth is so small as to be insignificant for the purposes of the experiment. What the odds should be depends</p>
<ol type="1">
<li><p>On the degree of accuracy which the nature of the experiment allows, and</p></li>
<li><p>On the importance of the issues at stake.</p></li>
</ol>
</blockquote>
<p>Two features of Gosset’s report are especially worth highlighting here. First, he suggested that judgments about “significant” differences were not a purely probabilistic exercise: they depend on the “importance of the issues at stake.” Second, Gosset underscored a positive correlation in the <a href="https://en.wikipedia.org/wiki/Normal_distribution" id="_T4iJJGn_" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Normal_distribution#bodyContent" title="Normal distribution">normal distribution</a> curve between “the square root of the number of observations” and the level of statistical-significance. Other things equal, he wrote, “the greater the number of observations of which means are taken [the larger the sample size], the smaller the [probable or standard] error” (pg5). “And the curve which represents their frequency of error”, he illustrated, “becomes taller and narrower” (pg7).</p>
<p>Since its discovery in the early 19<sup>th</sup> century, tables of the normal probability curve had been created for large samples…The relation between sample size and “significance” was rarely explored. For example, while looking at biometric samples with up to thousands of observations, Karl Pearson declared that a result departing by more than 3 standard deviations is “definitely significant.”<sup>12</sup> Yet Gosset, a self-trained statistician, found that at such large samples, nearly everything is <em>statistically</em> “significant”—though not, in Gosset’s terms, economically or scientifically “important.” Regardless, Gosset didn’t have the luxury of large samples. One of his earliest experiments employed a sample size of 2 (Gosset, <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, p.7) and in fact in <a href="https://gwern.net/doc/www/bayes.wustl.edu/62eb6502694eb3955a937b5683e29fc6252a45a4.pdf" id="_zGZU9cFr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/bayes.wustl.edu/62eb6502694eb3955a937b5683e29fc6252a45a4.pdf" data-url-original="https://bayes.wustl.edu/Manual/Student.pdf" data-filesize-bytes="815807" data-filesize-percentage="45">“The Probable Error of a Mean”</a> he calculated a <em>t</em> statistic for <em>n</em><span> = 2 (Student, <span>1908<sub><span title="1908 was 117 years ago.">117ya</span></sub></span>b, p.&nbsp;23).</span></p>
<p>…the “degree of certainty to be aimed at”, Gosset wrote, depends on the opportunity cost of following a result as if true, added to the opportunity cost of conducting the experiment itself. Gosset never deviated from this central position.15 [See, for example, Student (<a href="https://gwern.net/doc/statistics/decision/1923-student.pdf" id="gosset-1923" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2735480" data-filesize-percentage="88" title="On Testing Varieties of Cereals">1923, p.&nbsp;271, paragraph one</a>: “The object of testing varieties of cereals is to find out which will pay the farmer best.”) and Student (<a href="https://archive.org/details/in.ernet.dli.2015.233812/page/n168" id="_r4jMMmxR" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.233812/page/n168?view=theater" title="Yield trials">1931c</a>, p.&nbsp;1342, paragraph one) reprinted in Student (<span>1942<sub><span title="1942 was 83 years ago.">83ya</span></sub></span>, p.&nbsp;90 and p.&nbsp;150).]</p>
</blockquote>
</section>
<section id="thorndike-1920">
<h2><a href="#thorndike-1920" title="Link to section: § 'Thorndike1920'"><span><span>Thorndike</span><span>1920</span></span></a></h2>
<p><a href="https://gwern.net/doc/iq/1920-thorndike-2.pdf" id="thorndike-1920b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="10033577" data-filesize-percentage="95" title="'Intelligence and Its Uses', Thorndike 1920b">“Intelligence and Its Uses”</a>, Edward L. <span><span>Thorndike</span><span>1920</span></span> (<a href="https://en.wikipedia.org/wiki/Harper%27s_Magazine" id="_Pfbos5Iu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Harper%27s_Magazine#bodyContent" title="Harper's Magazine"><em>Harper’s Monthly</em></a>):</p>
<blockquote>
<p>…the significance of intelligence for success in a given activity of life is measured by the coefficient of correlation between them. Scientific investigations of these matters is just beginning; and it is a matter of great difficulty and expense to measure the intelligence of, say, a thousand clergymen, and then secure sufficient evidence to rate them accurately for their success as ministers of the Gospel. Consequently, one can report no final, perfectly authoritative results in this field. One can only organize reasonable estimates from the various partial investigations that have been made. Doing this, I find the following:</p>
<ul>
<li><p>Intelligence and success in the elementary schools, <em>r</em> = +0.80</p></li>
<li><p>Intelligence and success in high-school and colleges in the case of those who go, <em>r</em> = +0.60; but if all were forced to try to do this advanced work, the correlation would be +0.80 or more.</p></li>
<li><p>Intelligence and salary, <em>r</em> = +0.35.</p></li>
<li><p>Intelligence and success in athletic sports, <em>r</em> = +0.25</p></li>
<li><p>Intelligence and character, <em>r</em> = +0.40</p></li>
<li><p>Intelligence and popularity, <em>r</em> = +0.20</p></li>
</ul>
<p>Whatever be the eventual exact findings, two sound principles are illustrated by our provisional list. First, there is always some resemblance; intellect always counts. Second, the resemblance varies greatly; intellect counts much more in some lines than in others.</p>
<p>The first fact is in part a consequence of a still broader fact or principle—namely, that in human nature good traits go together. To him that hath a superior intellect is given also on the average a superior character; the quick boy is also in the long run more accurate; the able boy is also more industrious. There is no principle of compensation whereby a weak intellect is offset by a strong will, a poor memory by good judgment, or a lack of ambition by an attractive personality. Every pair of such supposed compensating qualities that have been investigated has been found really to show correspondence. Popular opinion has been misled by attending to striking individual cases which attracted attention partly because they were really exceptions to the rule. The rule is that desirable qualities are positively correlated. Intellect is good in and of itself, and also for what it implies about other traits.</p>
</blockquote>
</section>
<section id="berkson-1938">
<h2><a href="#berkson-1938" title="Link to section: § 'Berkson1938'"><span><span>Berkson</span><span>1938</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stats.org.uk/8a04b5f47c2d6c77a0baf79c2cc528f32d35f11e.pdf" id="_ZzMhya3F" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stats.org.uk/8a04b5f47c2d6c77a0baf79c2cc528f32d35f11e.pdf" data-url-original="http://www.stats.org.uk/statistical-inference/Berkson1938.pdf" data-filesize-bytes="261405" data-filesize-percentage="19">“Some difficulties of interpretation encountered in the application of the chi-square test”</a>, <a href="https://en.wikipedia.org/wiki/Joseph_Berkson" id="_gLxQXR7d" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Joseph_Berkson#bodyContent" title="Joseph Berkson">Berkson</a> <span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>:</p>
<blockquote>
<p>I believe that an observant statistician who has had any considerable experience with applying the chi-square test repeatedly will agree with my statement that, as a matter of observation, when the numbers in the data are quite large, the <em>P</em>’s tend to come out small. Having observed this, and on reflection, I make the following dogmatic statement, referring for illustration to the normal curve: “If the normal curve is fitted to a body of data representing any real observations whatever of quantities in the physical world, then if the number of observations is extremely large—for instance, on an order of 200,000—the chi-square <em>P</em> will be small beyond any usual limit of significance.”</p>
<p>This dogmatic statement is made on the basis of an extrapolation of the observation referred to and can also be defended as a prediction from <em>a priori</em> considerations. For we may assume that it is practically certain that any series of real observations does not actually follow a normal curve <em>with absolute exactitude</em> in all respects, and no matter how small the discrepancy between the normal curve and the true curve of observations, the chi-square <em>P</em> will be small if the sample has a sufficiently large number of observations in it.</p>
<p>If this be so, then we have something here that is apt to trouble the conscience of a reflective statistician using the chi-square test. For I suppose it would be agreed by statisticians that a large sample is always better than a small sample. If, then, we know in advance the <em>P</em> that will result from an application of a chi-square test to a large sample, there would seem to be no use in doing it on a smaller one. But since the result of the former test is known, it is no test at all.</p>
</blockquote>
</section>
<section id="thorndike-1939">
<h2><a href="#thorndike-1939" title="Link to section: § 'Thorndike1939'"><span><span>Thorndike</span><span>1939</span></span></a></h2>
<p><a href="https://gwern.net/doc/sociology/1939-thorndike-yourcity.pdf" id="_gq5r6RUr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="14015086" data-filesize-percentage="96"><em>Your City</em></a>, <span><span>Thorndike</span><span>1939</span></span> (and the followup <a href="https://gwern.net/doc/sociology/1940-thorndike-144smallercities.pdf" id="_VHX7vokH" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="7693205" data-filesize-percentage="94"><em>144 Smaller Cities</em></a> providing tables for 144 cities) compiles various statistics about American cities such as infant mortality, spending on the arts, crime etc and finds extensive intercorrelations &amp; factors.</p>
<p>The general factor of socioeconomic status or ‘S-factor’ also applies across countries as well: economic growth is by far the largest influence on all measures of well-being, and attempts at computing international rankings of things like maternal founder on this fact, as they typically wind up simply reproducing GDP rank-orderings and being redundant. For example, <a href="https://gwern.net/doc/www/klenow.com/64bf9a5047655615c9424980fac2ed32eb00a789.pdf" id="jones-klenow-2016" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/klenow.com/64bf9a5047655615c9424980fac2ed32eb00a789.pdf" data-url-original="http://klenow.com/Jones_Klenow.pdf" data-filesize-bytes="199811" data-filesize-percentage="15" title="Beyond GDP? Welfare across Countries and Time"><span><span>Jones &amp; Klenow</span><span>2016</span></span></a> compute an international wellbeing metric using “life expectancy, the ratio of consumption to income, annual hours worked per capita, the standard deviation of log consumption, and the standard deviation of annual hours worked” to incorporate factors like inequality, but this still winds up just being equivalent to GDP (<em>r</em> = 0.98). Or <a href="https://gwern.net/doc/www/mpra.ub.uni-muenchen.de/d1ff1393c3161ebe2b1239b226ce11af1f098543.pdf" id="dill-gebhart-2016" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/mpra.ub.uni-muenchen.de/d1ff1393c3161ebe2b1239b226ce11af1f098543.pdf" data-url-original="https://mpra.ub.uni-muenchen.de/74268/1/MPRA_paper_74268.pdf" data-filesize-bytes="202133" data-filesize-percentage="15" title="Redundancy, Unilateralism and Bias beyond GDP -- results of a Global Index Benchmark"><span><span>Gill &amp; Gebhart</span><span>2016</span></span></a>, who note that of 9 international indices they consider, all correlate positively with per capita GDP, &amp; 6 have <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" id="_Iosl62Lz" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#bodyContent" title="Kendall rank correlation coefficient">rank-correlations</a> τ &gt; 0.5.</p>
</section>
<section id="good-1950">
<h2><a href="#good-1950" title="Link to section: § 'Good1950'"><span><span>Good</span><span>1950</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/bayes/1950-good-probabilityandtheweighingofevidence.pdf#page=96" id="good-1950-page-96" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2376912" data-filesize-percentage="86" title="‘Probability and the Weighing of Evidence § pg96’, Good 1950 (page 96)"><em>Probability and the Weighing of Evidence</em></a>, <a href="https://en.wikipedia.org/wiki/I._J._Good" id="_WWZm3Ipg" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/I._J._Good#bodyContent" title="I. J. Good">I. J. Good</a>:</p>
<blockquote>
<p>The general question of significance tests was raised in 7.3 and a simple example will now be considered. Suppose that a die is thrown <em>n</em> times and that it shows an <em>r</em>-face on <em>m<sub>r</sub></em> occasions (<em>r</em> = 1, 2, …, 6). The question is whether the die is loaded. The answer depends on the meaning of “loaded”. From one point of view, it is unnecessary to look at the statistics since it is obvious that no die could be absolutely symmetrical. [It would be no contradiction of 4.3 (2) to say that the hypothesis that the die is absolutely symmetrical is almost impossible. In fact, this hypothesis is an idealised proposition rather than an empirical one.] It is possible that a similar remark applies to all experiments—even to the ESP experiment, since there may be no way of designing it so that the probabilities are <em>exactly</em> equal to <span>1⁄2</span>.</p>
</blockquote>
</section>
<section id="hodges-lehmann-1954">
<h2><a href="#hodges-lehmann-1954" title="Link to section: § 'Hodges &amp; Lehmann1954'"><span><span>Hodges &amp; Lehmann</span><span>1954</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/decision/1954-hodges.pdf" id="_3uOo3wa-" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="947907" data-filesize-percentage="73">“Testing the approximate validity of statistical hypotheses”</a>, <span><span>Hodges &amp; Lehmann</span><span>1954</span></span>:</p>
<blockquote>
<p>When testing statistical hypotheses, we usually do not wish to take the action of rejection unless the hypothesis being tested is false to an extent sufficient to matter. For example, we may formulate the hypothesis that a population is normally distributed, but we realize that no natural population is ever exactly normal. We would want to reject normality only if the departure of the actual distribution from the normal form were great enough to be material for our investigation. Again, when we formulate the hypothesis that the sex ratio is the same in two populations, we do not really believe that it could be exactly the same, and would only wish to reject equality if they are sufficiently different. Further examples of the phenomenon will occur to the reader.</p>
</blockquote>
</section>
<section id="savage-1954">
<h2><a href="#savage-1954" title="Link to section: § 'Savage1954'"><span><span>Savage</span><span>1954</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.223806/page/n267" id="_w-S3_i7-" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.223806/page/n267?view=theater" title="The Foundations Of Statistics"><em>The Foundations of Statistics</em> 1<sup>st</sup> edition</a>, <a href="https://en.wikipedia.org/wiki/Leonard_Jimmie_Savage" id="_wPytl7Vw" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Leonard_Jimmie_Savage#bodyContent" title="Leonard Jimmie Savage">Leonard Jimmie</a> <span><span>Savage</span><span>1954</span></span><a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>, pg252–255:</p>
<blockquote>
<p>The development of the theory of testing has been much influenced by the special problem of simple dichotomy, that is, testing problems in which <em>H</em><sub>0</sub> and <em>H</em><sub>1</sub> have exactly one element each. Simple dichotomy is susceptible of neat and full analysis (as in Exercise 7.5.2 and in §14.4), likelihood-ratio tests here being the only admissible tests; and simple dichotomy often gives insight into more complicated problems, though the point is not explicitly illustrated in this book.</p>
<p>Coin and ball examples of simple dichotomy are easy to construct, but instances seem rare in real life. The astronomical observations made to distinguish between the Newtonian and Einsteinian hypotheses are a good, but not perfect, example, and I suppose that research in Mendelian genetics sometimes leads to others. There is, however, a tradition of applying the concept of simple dichotomy to some situations to which it is, to say the best, only crudely adapted. Consider, for example, the decision problem of a person who must buy, <strong>f</strong><sub>0</sub>, or refuse to buy, <strong>f</strong><sub>1</sub>, a lot of manufactured articles on the basis of an observation <em>x</em>. Suppose that <em>i</em> is the difference between the value of the lot to the person and the price at which the lot is offered for sale, and that <em>P(x | i)</em> is known to the person. Clearly, <em>H</em><sub>0</sub>, <em>H</em><sub>1</sub>, and <em>N</em> are sets characterized respectively by <em>i</em> &gt; 0, <em>i</em> &lt; 0, <em>i</em> = 0. This analysis of this, and similar, problems has recently been explored in terms of the minimax rule, for example by Sprowls [S16] and a little more fully by Rudy [R4], and by Allen [A3]. It seems to me natural and promising for many fields of application, but it is not a traditional analysis. On the contrary, much literature recommends, in effect, that the person pretend that only two values of <em>i</em>, <em>i</em><sub>0</sub> &gt; 0 and <em>i</em><sub>1</sub> &lt; 0, are possible and that the person then choose a test for the resulting simple dichotomy. The selection of the two values <em>i</em><sub>0</sub> and <em>i</em><sub>1</sub> is left to the person, though they are sometimes supposed to correspond to the person’s judgment of what constitutes good quality and poor quality—terms really quite without definition. The emphasis on simple dichotomy is tempered in some acceptance-sampling literature, where it is recommended that the person choose among available tests by some largely unspecified overall consideration of operating characteristics and costs, and that he facilitate his survey of the available tests by focusing on a pair of points that happen to interest him and considering the test whose operating characteristic passes (economically, in the case of sequential testing) through the pair of points. These traditional analyses are certainly inferior in the theoretical framework of the present discussion, and I think they will be found inferior in practice.</p>
<p>…I turn now to a different and, at least for me, delicate topic in connection with applications of the theory of testing. Much attention is given in the literature of statistics to what purport to be tests of hypotheses, in which the null hypothesis is such that it would not really be accepted by anyone. The following 3 propositions, though playful in content, are typical in form of these <em>extreme</em> null hypotheses, as I shall call them for the moment.</p>
<ul>
<li><p>A. The mean noise output of the cereal Krakl is a linear function of the atmospheric pressure, in the range 900–1,100 millibars.</p></li>
<li><p>B. The basal metabolic consumption of sperm whales is normally distributed [Wll].</p></li>
<li><p>C. New York taxi drivers of Irish, Jewish, and Scandinavian extraction are equally proficient in abusive language.</p></li>
</ul>
<p>Literally to test such hypotheses as these is preposterous. If, for example, the loss associated with <strong>f</strong><sub>1</sub> is zero, except in case Hypothesis A is exactly satisfied, what possible experience with Krakl could dissuade you from adopting <strong>f</strong><sub>1</sub>?</p>
<p>The unacceptability of extreme null hypotheses is perfectly well known; it is closely related to the often heard maxim that science disproves, but never proves, hypotheses. The role of extreme hypotheses in science and other statistical activities seems to be important but obscure. In particular, though I, like everyone who practices statistics, have often “tested” extreme hypotheses, I cannot give a very satisfactory analysis of the process, nor say clearly how it is related to testing as defined in this chapter and other theoretical discussions. None the less, it seems worth while to explore the subject tentatively; I will do so largely in terms of two examples.</p>
<p>Consider first the problem of a cereal dynamicist who must estimate the noise output of Krakl at each of 10 atmospheric pressures between 900 and 1,100 millibars. It may well be that he can properly regard the problem as that of estimating the 10 parameters in question, in which case there is no question of testing. But suppose, for example, that one or both of the following considerations apply. First, the engineer and his colleagues may attach considerable personal probability to the possibility that A is very nearly satisfied—very nearly, that is, in terms of the dispersion of his measurements. Second, the administrative, computational, and other incidental costs of using 10 individual estimates might be considerably greater than that of using a linear formula.</p>
<p>It might be impractical to deal with either of these considerations very rigorously. One rough attack is for the engineer first to examine the observed data <em>x</em> and then to proceed either as though he actually believed Hypothesis A or else in some other way. The other way might be to make the estimate according to the objectivistic formulae that would have been used had there been no complicating considerations, or it might take into account different but related complicating considerations not explicitly mentioned here, such as the advantage of using a quadratic approximation. It is artificial and inadequate to regard this decision between one class of basic acts or another as a test, but that is what in current practice we seem to do. The choice of which test to adopt in such a context is at least partly motivated by the vague idea that the test should readily accept, that is, result in acting as though the extreme null hypotheses were true, in the farfetched case that the null hypothesis is indeed true, and that the worse the approximation of the null hypotheses to the truth the less probable should be the acceptance.</p>
<p>The method just outlined is crude, to say the best. It is often modified in accordance with common sense, especially so far as the second consideration is concerned. Thus, if the measurements are sufficiently precise, no ordinary test might accept the null hypotheses, for the experiment will lead to a clear and sure idea of just what the departures from the null hypotheses actually are. But, if the engineer considers those departures unimportant for the context at hand, he will justifiably decide to neglect them.</p>
<p>Rejection of an extreme null hypothesis, in the sense of the foregoing discussion, typically gives rise to a complicated subsidiary decision problem. Some aspects of this situation have recently been explored, for example by Paulson [P3], [P4]; Duncan [Dll], [D12]; <a href="https://en.wikipedia.org/wiki/John_Tukey" id="_XocxmpNj" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/John_Tukey#bodyContent" title="John Tukey">Tukey</a> [T4], [<a href="https://gwern.net/doc/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" id="raffel-et-al-2019" data-link-icon="alphabet" data-link-icon-type="svg" data-link-icon-color="#4285f4" data-href-mobile="https://arxiv.org/html/1910.10683?fallback=original#google" data-url-archive="/doc/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" data-url-original="https://arxiv.org/abs/1910.10683#google" title="'T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', Raffel et al 2019">T5</a>]; Schefte [S7]; and W. D. Fisher [F7].</p>
</blockquote>
</section>
<section id="fisher-1956">
<h2><a href="#fisher-1956" title="Link to section: § 'Fisher1956'"><span><span>Fisher</span><span>1956</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.134555/page/n47" id="_RTbkUo-u" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.134555/page/n47?view=theater" title="Statistical Methods And Scientific Inference"><em>Statistical Methods and Scientific Inference</em></a>, <a href="https://en.wikipedia.org/wiki/Ronald_Fisher" id="_58PQ6Izd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Ronald_Fisher#bodyContent" title="Ronald Fisher">R. A.</a> <span><span>Fisher</span><span>1956</span></span> (pg42):</p>
<blockquote>
<p>…However, the calculation [of error rates of ‘rejecting the null’] is absurdly academic, for in fact no scientific worker has a fixed level of significance at which from year to year and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas. Further, the calculation is based solely on a hypothesis, which, in the light of the evidence, is often not believed to be true at all, so that the actual probability of erroneous decision, supposing such a phrase to have any meaning, may be much less than the frequency specifying the level of significance.</p>
</blockquote>
</section>
<section id="wallis-roberts-1956">
<h2><a href="#wallis-roberts-1956" title="Link to section: § 'Wallis &amp; Roberts1956'"><span><span>Wallis &amp; Roberts</span><span>1956</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.214331/page/n421" id="_vbtsA3rU" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.214331/page/n421?view=theater" title="Statistics A New Approach"><em>Statistics: A New Approach</em></a>, <span><span>Wallis &amp; Roberts</span><span>1956</span></span> (pg384–388):</p>
<blockquote>
<p>A difficulty with this viewpoint is that it is often known that the hypothesis tested could not be precisely true. No coin, for example, has a probability of precisely <span>1⁄2</span> of coming heads. The true probability will always differ from <span>1⁄2</span> even if it differs by only 0.000,000,000,1. Neither will any treatment cure <em>precisely</em> one-third of the patients in the population to which it might be applied, nor will the proportion of voters in a presidential election favoring one candidate be <em>precisely</em> <span>1⁄2</span>. Recognition of this leads to the notion of differences that are or are not of practical importance. “Practical importance” depends on the actions that are going to be taken on the basis of the data, and on the losses from taking certain actions when others would be more appropriate.</p>
<p>Thus, the focus is shifted to decisions: Would the same decision about practical action be appropriate if the coin produces heads 0.500,000,000,1 of the time as if it produces heads 0.5 of the time precisely? Does it matter whether the coin produces heads 0.5 of the time or 0.6 of the time, and if so does it matter enough to be worth the cost of the data needed to decide between the actions appropriate to these situations? Questions such as these carry us toward a comprehensive theory of rational action, in which the consequences of each possible action are weighed in the light of each possible state of reality. The value of a correct decision, or the costs of various degrees of error, are then balanced against the costs of reducing the risks of error by collecting further data. It is this viewpoint that underlies the definition of statistics given in the first sentence of this book. [“Statistics is a body of methods for making wise decisions in the face of uncertainty.”]</p>
</blockquote>
</section>
<section id="savage-1957">
<h2><a href="#savage-1957" title="Link to section: § 'Savage1957'"><span><span>Savage</span><span>1957</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/decision/1957-savage.pdf" id="_B5Sgn6TG" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1574465" data-filesize-percentage="81">“Nonparametric statistics”</a>, <a href="https://projecteuclid.org/journals/statistical-science/volume-14/issue-1/A-conversation-with-I-Richard-Savage-with-the-assistance-of/10.1214/ss/1009211808.full" id="sampson-1999" title="'A conversation with I. Richard Savage (with the assistance of Bruce Spencer)', Sampson 1999">I. Richard Savage</a><a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span>1957<sub><span title="1957 was 68 years ago.">68ya</span></sub></span>:</p>
<blockquote>
<p>Siegel does not explain why his interest is confined to tests of significance; to make measurements and then ignore their magnitudes would ordinarily be pointless. Exclusive reliance on tests of significance obscures the fact that statistical-significance does not imply substantive significance. The tests given by Siegel apply only to null hypotheses of “no difference.” In research, however, null hypotheses of the form “Population A has a median at least 5 units <em>larger</em> than the median of Population B” arise. Null hypotheses of no difference are usually known to be false before the data are collected [<a href="#fisher-1956" title="_Statistical Methods and Scientific Inference_, pg42 (R. A. Fisher 1956)">9</a>, p.&nbsp;42; <a href="#wallis-roberts-1956" title="_Statistics: A New Approach_, Wallis &amp; Roberts 1956">48</a>, pp.&nbsp;384–8]; when they are, their rejection or acceptance simply reflects the size of the sample and the power of the test, and is not a contribution to science.</p>
</blockquote>
</section>
<section id="nunnally-1960">
<h2><a href="#nunnally-1960" title="Link to section: § 'Nunnally1960'"><span><span>Nunnally</span><span>1960</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1960-nunnally.pdf" id="_Ad3FDxg3" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="519361" data-filesize-percentage="59">“The place of statistics in psychology”</a>, <span><span>Nunnally</span><span>1960</span></span>:</p>
<blockquote>
<p>The most misused and misconceived hypothesis-testing model employed in psychology is referred to as the “null-hypothesis” model. Stating it crudely, one null hypothesis would be that two treatments do not produce different mean effects in the long run. Using the obtained means and sample estimates of”population” variances, probability statements can be made about the acceptance or rejection of the null hypothesis. Similar null hypotheses are applied to correlations, complex experimental designs, factor-analytic results, and most all experimental results.</p>
<p>Although from a mathematical point of view the null-hypothesis models are internally neat, they share a crippling flaw: in the real world the null hypothesis is almost never true, and it is usually nonsensical to perform an experiment with the <em>sole</em> aim of rejecting the null hypothesis. This is a personal point of view, and it cannot be proved directly. However, it is supported both by common sense and by practical experience. The common-sense argument is that different psychological treatments will almost always (in the long run) produce differences in mean effects, even though the differences may be very small. Also, just as nature abhors a vacuum, it probably abhors zero correlations between variables.</p>
<p>…Experience shows that when large numbers of subjects are used in studies, nearly all comparisons of means are “significantly” different and all correlations are “significantly” different from zero. The author once had occasion to use 700 subjects in a study of public opinion. After a <a href="https://en.wikipedia.org/wiki/Factor_analysis" id="_dUO1Hqah" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Factor_analysis#bodyContent" title="Factor analysis">factor analysis</a> of the results, the factors were correlated with individual-difference variables such as amount of education, age, income, sex, and others. In looking at the results I was happy to find so many “significant” correlations (under the null-hypothesis model)-indeed, nearly all correlations were significant, including ones that made little sense. Of course, with an <em>N</em> of 700 correlations as large as 0.08 are “beyond the 0.05 level.” Many of the “significant” correlations were of no theoretical or practical importance.</p>
<p>The point of view taken here is that if the null hypothesis is not rejected, it usually is because the <em>N</em> is too small. If enough data is gathered, the hypothesis will generally be rejected. If rejection of the null hypothesis were the real intention in psychological experiments, there usually would be no need to gather data.</p>
<p>…Statisticians are not to blame for the misconceptions in psychology about the use of statistical methods. They have warned us about the use of the hypothesis-testing models and the related concepts. In particular they have criticized the null-hypothesis model and have recommended alternative procedures similar to those recommended here (See <a href="#savage-1957">Savage, 1957</a>; <a href="https://gwern.net/doc/statistics/decision/1954-tukey.pdf" id="tukey-1954" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1207818" data-filesize-percentage="77" title="Unsolved Problems of Experimental Statistics">Tukey, 1954</a>; and <a href="https://gwern.net/doc/statistics/causality/1951-yates.pdf" id="yates-1951" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="950352" data-filesize-percentage="73" title="The Influence of Statistical Methods for Research Workers on the Development of the Science of Statistics">Yates, 1951</a>).</p>
</blockquote>
</section>
<section id="smith-1960">
<h2><a href="#smith-1960" title="Link to section: § 'Smith1960'"><span><span>Smith</span><span>1960</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1960-smith.pdf" id="_NH3lG8Zr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="506803" data-filesize-percentage="59">“Review of N. T. J. Bailey, <em>Statistical methods in biology</em>”</a>, <span><span>Smith</span><span>1960</span></span>:</p>
<blockquote>
<p>However, it is interesting to look at this book from another angle. Here we have set before us with great clarity a panorama of modern statistical methods, as used in biology, medicine, physical science, social and mental science, and industry. How far does this show that these methods fulfil their aims of analysing the data reliably, and how many gaps are there still in our knowledge?…One feature which can puzzle an outsider, and which requires much more justification than is usually given, is the setting up of unplausible null hypotheses. For example, a statistician may set out a test to see whether two drugs have exactly the same effect, or whether a regression line is exactly straight. These hypotheses can scarcely be taken literally, but a statistician may say, quite reasonably, that he wishes to test whether there is an appreciable difference between the effects of the two drugs, or an appreciable curvature in the regression line. But this raises at once the question: how large is ‘appreciable’? Or in other words, are we not really concerned with some kind of estimation, rather than significance?</p>
</blockquote>
</section>
<section id="edwards-1963">
<h2><a href="#edwards-1963" title="Link to section: § 'Edwards1963'"><span><span>Edwards</span><span>1963</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/53877bf2fb57846c8a024744f8205dd53220fb9d.pdf" id="_XTGtSKns" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/53877bf2fb57846c8a024744f8205dd53220fb9d.pdf" data-url-original="https://pdfs.semanticscholar.org/f5ee/44c98c68fc1d9a6a7a10d3af3ba13fc9d058.pdf" data-filesize-bytes="2097666" data-filesize-percentage="64">“Bayesian statistical inference for psychological research”</a>, <span><span title="et al">Edwards</span><span> et al </span><span>1963</span></span>:</p>
<blockquote>
<p>The most popular notion of a test is, roughly, a tentative decision between two hypotheses on the basis of data, and this is the notion that will dominate the present treatment of tests. Some qualification is needed if only because, in typical applications, one of the hypotheses—the null hypothesis—is known by all concerned to be false from the outset (<a href="#berkson-1938" title="Some difficulties of interpretation encountered in the application of the chi-square test"><span><span>Berkson</span><span>1938</span></span></a>; <a href="#hodges-lehmann-1954" title="Testing the approximate validity of statistical hypotheses"><span><span>Hodges &amp; Lehmann</span><span>1954</span></span></a>; <a href="https://gwern.net/doc/statistics/decision/1959-lehmann-testingstatisticalhypotheses.pdf" id="lehmann-1959" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="6805671" data-filesize-percentage="93" title="_Testing Statistical Hypotheses_"><span><span>Lehmann</span><span>1959</span></span></a><a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>; <a href="#savage-1957">I. R. <span><span>Savage</span><span>1957</span></span></a>; L. <a href="#savage-1954">J. <span><span>Savage</span><span>1954</span></span>, p.&nbsp;254</a>); some ways of resolving the seeming absurdity will later be pointed out, and at least one of them will be important for us here…Classical procedures sometimes test null hypotheses that no one would believe for a moment, no matter what the data; our list of situations that might stimulate hypothesis tests earlier in the section included several examples. Testing an unbelievable null hypothesis amounts, in practice, to assigning an unreasonably large prior probability to a very small region of possible values of the true parameter. In such cases, the more the procedure is against the null hypothesis, the better. The frequent reluctance of empirical scientists to accept null hypotheses which their data do not classically reject suggests their appropriate skepticism about the original plausibility of these null hypotheses.</p>
</blockquote>
</section>
<section id="bakan-1966">
<h2><a href="#bakan-1966" title="Link to section: § 'Bakan1966'"><span><span>Bakan</span><span>1966</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/stats.org.uk/9f378d39eb9e41287e3e353f8ec7e47ece9e0486.pdf" id="_MEcbLpo0" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/stats.org.uk/9f378d39eb9e41287e3e353f8ec7e47ece9e0486.pdf" data-url-original="http://stats.org.uk/statistical-inference/Bakan1966.pdf" data-filesize-bytes="873397" data-filesize-percentage="46">“The test of significance in psychological research”</a>, <span><span>Bakan</span><span>1966</span></span>:</p>
<blockquote>
<p>Let us consider some of the difficulties associated with the null hypothesis.</p>
<ol type="1">
<li><p>The <em>a priori</em> reasons for believing that the null hypothesis is generally false anyway. One of the common experiences of research workers is the very high frequency with which significant results are obtained with large samples. Some years ago, the author had occasion to run a number of tests of significance on a battery of tests collected on about 60,000 subjects from all over the United States. Every test came out significant. Dividing the cards by such arbitrary criteria as east versus west of the Mississippi River, Maine versus the rest of the country, North versus South, etc., all produced significant differences in means. In some instances, the differences in the sample means were quite small, but nonetheless, the <em>p</em> values were all very low. <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a> has reported a similar experience involving correlation coefficients on 700 subjects. <a href="#berkson-1938" title="Some difficulties of interpretation encountered in the application of the chi-square test">Joseph <span><span>Berkson</span><span>1938</span></span></a> made the observation almost 30 years in connection with chi-square:</p></li>
</ol>
<blockquote>
<p>I believe that an observant statistician who has had any considerable experience with applying the chi-square test repeatedly will agree with my statement that, as a matter of observation, when the numbers in the data are quite large, the P’s tend to come out small. Having observed this, and on reflection, I make the following dogmatic statement, referring for illustration to the normal curve: “If the normal curve is fitted to a body of data representing any real observations whatever of quantities in the physical world, then if the number of observations is extremely large—for instance, on an order of 200,000—the chi-square <em>p</em> will be small beyond any usual limit of significance.”</p>
<p>This dogmatic statement is made on the basis of an extrapolation of the observation referred to and can also be defended as a prediction from <em>a priori</em> considerations. For we may assume that it is practically certain that any series of real observations does not actually follow a normal curve <em>with absolute exactitude</em> in all respects, and no matter how small the discrepancy between the normal curve and the true curve of observations, the chi-square <em>P</em> will be small if the sample has a sufficiently large number of observations in it.</p>
<p>If this be so, then we have something here that is apt to trouble the conscience of a reflective statistician using the chi-square test. For I suppose it would be agreed by statisticians that a large sample is always better than a small sample. If, then, we know in advance the <em>P</em> that will result from an application of a chi-square test to a large sample, there would seem to be no use in doing it on a smaller one. But since the result of the former test is known, it is no test at all [pp.&nbsp;526–527].</p>
</blockquote>
<p>As one group of authors has put it, “in typical applications . . . the null hypothesis . . . is known by all concerned to be false from the outset [<a href="#edwards-1963"><span><span title="et al">Edwards</span><span> et al </span><span>1963</span></span></a>, p.&nbsp;214].” The fact of the matter is that <em>there is really no good reason to expect the null hypothesis to be true in any population.</em> Why should the mean, say, of all scores east of the Mississippi be <em>identical</em> to all scores west of the Mississippi? Why should any correlation coefficient be <em>exactly</em> 0.00 in the population? Why should we expect the ratio of males to females be <em>exactly</em> 50:50 in any population? Or why should different drugs have <em>exactly</em> the same effect on any population parameter (<a href="#smith-1960"><span><span>Smith</span><span>1960</span></span></a>)? <em>A glance at any set of statistics on total populations will quickly confirm the rarity of the null hypothesis in nature.</em></p>
<p>…Should there be any deviation from the null hypothesis in the population, <em>no matter how small</em>—and we have little doubt but that such a deviation usually exists—a sufficiently large number of observations will lead to the rejection of the null hypothesis. As <span><span>Nunnally</span><span>1960</span></span> put it,</p>
<blockquote>
<p>if the null hypothesis is not rejected, it is usually because the <em>N</em> is too small. If enough data are gathered, the hypothesis will generally be rejected. If rejection of the null hypothesis were the real intention in psychological experiments, there usually would be no need to gather data [p.&nbsp;643].</p>
</blockquote>
</blockquote>
</section>
<section id="meehl-1967">
<h2><a href="#meehl-1967" title="Link to section: § 'Meehl1967'"><span><span>Meehl</span><span>1967</span></span></a></h2>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.8918&amp;rep=rep1&amp;type=pdf" id="_K1sUG73X" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Theory-testing in psychology and physics: A methodological paradox”</a>, <span><span>Meehl</span><span>1967</span></span></p>
<blockquote>
<p>One reason why the directional null hypothesis (<em>H</em><sub>02</sub>: μ<sub>g</sub> ≤ μ<sub>b</sub>) is the appropriate candidate for experimental refutation is the universal agreement that the old point-null hypothesis (<em>H</em><sub>0</sub>: μ<sub>g</sub> = μ<sub>b</sub>) is [quasi-] always false in biological and social science. Any dependent variable of interest, such as I.Q., or academic achievement, or perceptual speed, or emotional reactivity as measured by skin resistance, or whatever, depends mainly upon a finite number of “strong” variables characteristic of the organisms studied (embodying the accumulated results of their genetic makeup and their learning histories) plus the influences manipulated by the experimenter. Upon some complicated, unknown mathematical function of this finite list of “important” determiners is then superimposed an indefinitely large number of essentially “random” factors which contribute to the intragroup variation and therefore boost the error term of the statistical-significance test. In order for two groups which differ in some identified properties (such as social class, intelligence, diagnosis, racial or religious background) to differ not at all in the “output” variable of interest, it would be necessary that all determiners of the output variable have precisely the same average values in both groups, or else that their values should differ by a <em>pattern of amounts of difference</em> which precisely counterbalance one another to yield a net difference of zero. Now our general background knowledge in the social sciences, or, for that matter, even “common sense” considerations, makes such an exact equality of all determining variables, or a precise “accidental” counterbalancing of them, so extremely unlikely that no psychologist or statistician would assign more than a negligibly small probability to such a state of affairs.</p>
<p>…<em>Example</em>: Suppose we are studying a simple perceptual-verbal task like rate of color-naming in school children, and the independent variable is father’s religious preference. Superficial consideration might suggest that these two variables would not be related, but a little thought leads one to conclude that they will almost certainly be related by <em>some</em> amount, however small. Consider, for instance, that a child’s reaction to any sort of school-context task will be to some extent dependent upon his social class, since the desire to please academic personnel and the desire to achieve at a performance (just because it is a <em>task</em>, regardless of its intrinsic interest) are both related to the kinds of sub-cultural and personality traits in the parents that lead to upward mobility, economic success, the gaining of further education, and the like. Again, since there is known to be a sex difference in color naming, it is likely that fathers who have entered occupations more attractive to “feminine” males will (on the average) provide a somewhat more feminine father figure for identification on the part of their male offspring, and that a more refined color vocabulary, making closer discriminations between similar hues, will be characteristic of the ordinary language of such a household. Further, it is known that there is a correlation between a child’s general intelligence and its father’s occupation, and of course there will be <em>some</em> relation, even though it may be small, between a child’s general intelligence and his color vocabulary, arising from the fact that <em>vocabulary in general</em> is heavily saturated with the general intelligence factor. Since religious preference is a correlate of social class, all of these social class factors, as well as the intelligence variable, would tend to influence color-naming performance. Or consider a more extreme and faint kind of relationship. It is quite conceivable that a child who belongs to a more liturgical religious denomination would be somewhat more color-oriented than a child for whom bright colors were not associated with the religious life. Everyone familiar with psychological research knows that numerous “puzzling, unexpected” correlations pop up all the time, and that it requires only a moderate amount of motivation-plus-ingenuity to construct very plausible alternative theoretical explanations for them.</p>
<p>…These armchair considerations are borne out by the finding that in psychological and sociological investigations involving very large numbers of subjects, it is regularly found that almost all correlations or differences between means are statistically-significant. See, for example, the papers by <a href="#bakan-1966" title="The test of significance in psychological research"><span><span>Bakan</span><span>1966</span></span></a> and <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a>. Data currently being analyzed by Dr.&nbsp;David Lykken and myself<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>, derived from a huge sample of over 55,000 Minnesota high school seniors, reveal statistically-significant relationships in 91% of pairwise associations among a congeries of 45 miscellaneous variables such as sex, birth order, religious preference, number of siblings, vocational choice, club membership, college choice, mother’s education, dancing, interest in woodworking, liking for school, and the like. The 9% of non-statistically-significant associations are heavily concentrated among a small minority of variables having dubious <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)" id="_T1eYyggy" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Reliability_(statistics)#bodyContent" title="Reliability (statistics)">reliability</a>, or involving arbitrary groupings of non-homogeneous or nonmonotonic sub-categories. The majority of variables exhibited significant relationships <em>with all but 3 of the others</em>, often at a very high confidence level (<em>p</em> &lt; 10<sup>−6</sup>).</p>
<p>…Considering the fact that “everything in the brain is connected with everything else”, and that there exist several “general state-variables” (such as arousal, attention, anxiety, and the like) which are known to be at least <em>slightly</em> influenceable by practically any kind of stimulus input, it is highly unlikely that <em>any</em> psychologically discriminable stimulation which we apply to an experimental subject would exert literally <em>zero</em> effect upon any aspect of his performance. The psychological literature abounds with examples of small but detectable influences of this kind. Thus it is known that if a subject memorizes a list of nonsense syllables in the presence of a faint odor of peppermint, his recall will be facilitated by the presence of that odor. Or, again, we know that individuals solving intellectual problems in a “messy” room do not perform quite as well as individuals working in a neat, well-ordered surround. Again, cognitive processes undergo a detectable facilitation when the thinking subject is concurrently performing the irrelevant, noncognitive task of squeezing a hand dynamometer. It would require considerable ingenuity to concoct experimental manipulations, except the most minimal and trivial (such as a very slight modification in the word order of instructions given a subject) where one could have confidence that the manipulation would be utterly without effect upon the subject’s motivational level, attention, arousal, fear of failure, achievement drive, desire to please the experimenter, distraction, social fear, etc., etc. So that, for example, while there is no very “interesting” psychological theory that links hunger drive with color-naming ability, I myself would confidently predict a significant difference in color-naming ability between persons tested after a full meal and persons who had not eaten for 10 hours, provided the sample size were sufficiently large and the color-naming measurements sufficiently reliable, since one of the effects of the increased hunger drive is heightened “arousal”, and anything which heightens arousal would be expected to affect a perceptual-cognitive performance like color-naming. Suffice it to say that there are very good reasons for expecting at least <em>some</em> slight influence of almost any experimental manipulation which would differ sufficiently in its form and content from the manipulation imposed upon a control group to be included in an experiment in the first place. In what follows I shall therefore assume that the point-null hypothesis <em>H</em><sub>0</sub> is, in psychology, [quasi-] always false.</p>
</blockquote>
<p>See also <a href="#waller-2004"><span><span>Waller</span><span>2004</span></span></a>, and <span><span>Meehl’s</span><span>2003</span></span> CSS talk, <a href="https://meehl.umn.edu/files/aumeehl2003sigtests-trimmedmp3#.mp3" id="_2eyixAdk">“Critique of Null Hypothesis Significance Testing”</a> (MP3 audio; <a href="https://meehl.umn.edu/sites/g/files/pua1696/f/aumeehl2003ccshandout.pdf" id="_uz1_bxZb" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">slides</a>).</p>
</section>
<section id="lykken-1968">
<h2><a href="#lykken-1968" title="Link to section: § 'Lykken1968'"><span><span>Lykken</span><span>1968</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/citeseerx.ist.psu.edu/0eb3f8d79d2999fc884c23757f07bfa591917e79.pdf" id="_OWw-8sgE" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/citeseerx.ist.psu.edu/0eb3f8d79d2999fc884c23757f07bfa591917e79.pdf" data-url-original="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.5553&amp;rep=rep1&amp;type=pdf" data-filesize-bytes="490526" data-filesize-percentage="31">“Statistical-Significance in Psychological Research”</a>, <span><span>Lykken</span><span>1968</span></span>:</p>
<blockquote>
<p>Most theories in the areas of personality, clinical, and social psychology predict no more than the direction of a correlation, group difference, or treatment effect. Since the null hypothesis is never strictly true, such predictions have about a 50-50 chance of being confirmed by experiment when the theory in question is false, since the statistical-significance of the result is a function of the sample size.</p>
<p>…Most psychological experiments are of 3 kinds: (1) studies of the effect of some treatment on some output variables, which can be regarded as a special case of (2) studies of the difference between two or more groups of individuals with respect to some variable, which in turn are a special case of (3) the study of the relationship or correlation between two or more variables within some specified population. Using the bivariate correlation design as paradigmatic, then, one notes first that the strict null hypothesis must always be assumed to be false (this idea is not new and has recently been illuminated by <a href="#bakan-1966"><span><span>Bakan</span><span>1966</span></span></a>). Unless one of the variables is wholly unreliable so that the values obtained are strictly random, it would be foolish to suppose that the correlation between any two variables is identically equal to 0.0000 . . . (or that the effect of some treatment or the difference between two groups is exactly <em>zero</em>). The molar dependent variables employed in psychological research are extremely complicated in the sense that the measured value of such a variable tends to be affected by the interaction of a vast number of factors, both in the present situation and in the history of the subject organism. It is exceedingly unlikely that any two such variables will not share at least some of these factors and equally unlikely that their effects will exactly cancel one another out.</p>
<p>It might be argued that the more complex the variables the smaller their average correlation ought to be since a larger pool of common factors allows more chance for mutual cancellation of effects in obedience to the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" id="_c2MypS07" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Law_of_large_numbers#bodyContent" title="Law of large numbers">Law of Large Numbers</a><a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>. However, one knows of a number of unusually potent and pervasive factors which operate to unbalance such convenient symmetries and to produce correlations large enough to rival the effects of whatever causal factors the experimenter may have had in mind. Thus, we know that (1) “good” psychological and physical variables tend to be positively correlated; (6) experimenters, without deliberate intention, can somehow subtly bias their findings in the expected direction (Rosenthal, <span>1963<sub><span title="1963 was 62 years ago.">62ya</span></sub></span>); (3) the effects of common method are often as strong as or stronger than those produced by the actual variables of interest (eg. in a large and careful study of the factorial structure of adjustment to stress among officer candidates, Holtzman &amp; Bitterman, <span>1956<sub><span title="1956 was 69 years ago.">69ya</span></sub></span>, found that their 101 original variables contained 5 main common factors representing, respectively, their rating scales, their perceptual-motor tests, the McKinney Reporting Test, their GSR variables, and the <a href="https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory" id="_Tr4qHl-1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory#bodyContent" title="Minnesota Multiphasic Personality Inventory">MMPI</a>); (4) transitory state variables such as the subject’s anxiety level, fatigue, or his desire to please, may broadly affect all measures obtained in a single experimental session. This average shared variance of “unrelated” variables can be thought of as a kind of ambient noise level characteristic of the domain. It would be interesting to obtain empirical estimates of this quantity in our field to serve as a kind of <a href="https://en.wikipedia.org/wiki/Waterline" id="_5xRFcnIp" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Waterline#bodyContent" title="Waterline">Plimsoll mark</a> against which to compare obtained relationships predicted by some theory under test. If, as I think, it is not unreasonable to suppose that “unrelated” molar psychological variables share on the average about 4% to 5% of common variance, then the expected correlation between any such variables would be about 0.20 in absolute value and the expected difference between any two groups on some such variable would be nearly 0.5 standard deviation units. (Note that these estimates assume zero measurement error. One can better explain the near-zero correlations often observed in psychological research in terms of unreliability of measures than in terms of the assumption that the true scores are in fact unrelated.)</p>
</blockquote>
</section>
<section id="nichols-1968">
<h2><a href="#nichols-1968" title="Link to section: § 'Nichols1968'"><span><span>Nichols</span><span>1968</span></span></a></h2>
<p><a href="https://gwern.net/doc/iq/1968-nichols.pdf" id="nichols-1968" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1813883" data-filesize-percentage="83" title="'Heredity, Environment, and School Achievement', Nichols 1968">“Heredity, Environment, and School Achievement”</a>, <span><span>Nichols</span><span>1968</span></span>:</p>
<blockquote>
<p>There are 3 main factors or types of variables that seem likely to have an important influence on ability and school achievement. These are (1) the school factor or organized educational influences; (2) the family factor or all of the social influences of family life on a child; and (3) the genetic factor…the separation of the effects of the major types of influences has proved to be extraordinarily difficult, and all of the research so far has not resulted in a clear-cut conclusion.</p>
<p>…This messy situation is due primarily to the fact that in human society all good things tend to go together. The most intelligent parents—those with the best genetic potential—also tend to provide the most comfortable and intellectually stimulating home environments for their children, and also tend to send their children to the most affluent and well-equipped schools. Thus, the ubiquitous correlation between family socio-economic status and school achievement is ambiguous in meaning, and isolating the independent contribution of the factors involved is difficult. However, the strong emotionally motivated attitudes and vested interests in this area have also tended to inhibit the sort of dispassionate, objective evaluation of the available evidence that is necessary for the advance of science.</p>
</blockquote>
</section>
<section id="hays-1973">
<h2><a href="#hays-1973" title="Link to section: § 'Hays1973'"><span><span>Hays</span><span>1973</span></span></a></h2>
<p><a href="https://www.amazon.com/Statistics-Social-Sciences-W-Hays/dp/B000OEF3EK" id="_f5fmFVKL" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>Statistics for the social sciences</em></a> (2nd edition), <span><span>Hays</span><span>1973</span></span>; <a href="https://gwern.net/doc/statistics/causality/1973-hays.pdf" id="_zzU6fTYC" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1810898" data-filesize-percentage="83">chapter 10</a>, page 413–417:</p>
<blockquote>
<p>10.19: Testmanship, or how big is a difference?</p>
<p>…As we saw in Chapter 4, the complete absence of a statistical relation, or no association, occurs only when the conditional distribution of the dependent variable is the same regardless of which treatment is administered. Thus if the independent variable is not associated at all with the dependent variable the population distributions must be identical over the treatments. If, on the other hand, the means of the different treatment populations are different, the conditional distributions themselves must be different and the independent and dependent variables must be associated. The rejection of the hypothesis of no difference between population means is tantamount to the assertion that the treatment given does have some statistical association with the dependent variable score.</p>
<p>…However, the occurrence of a significant result says nothing at all about the strength of the association between treatment and score. A significant result leads to the inference that some association exists, but in no sense does this mean that an important degree of association necessarily exists. Conversely, evidence of a strong statistical association can occur in data even when the results are not significant. The game of inferring the true degree of statistical association has a joker: this is the sample size. The time has come to define the notion of the strength of a statistical association more sharply, and to link this idea with that of the true difference between population means.</p>
<p>. When does it seem appropriate to say that a strong association exists between the experimental factor <em>X</em> and the dependent variable <em>Y</em>? Over all of the different possibilities for <em>X</em> there is a probability distribution of <em>Y</em> values, which is the marginal distribution of <em>Y</em> over (<em>x</em>,<em>y</em>) events. The existence of this distribution implies that we do not know exactly what the <em>Y</em> value for any observation will be; we are always uncertain about <em>Y</em> to some extent. However, given any particular <em>X</em>, there is also a conditional distribution of <em>Y</em>, and it may be that in this conditional distribution the highly probable values of <em>Y</em> tend to “shrink” within a much narrower range than in the marginal distribution. If so, we can say that the information about <em>X</em> tends to reduce uncertainty about <em>Y</em>. <strong>In general we will say that the strength of a statistical relation is reflected by the extent to which knowing <em>X</em> reduces uncertainty about <em>Y</em>.</strong> One of the best indicators of our uncertainty about the value of a variable is σ<sup>2</sup>, the variance of its distribution…This index reflects the predictive power afforded by a relationship: when <em>w</em><sup>2</sup> is zero, then <em>X</em> does not aid us at all in predicting the value of <em>Y</em>. On the other hand, when <em>w</em><sup>2</sup> is 1.00, this tells us that <em>X</em> lets us know <em>Y</em> exactly…About now you should be wondering what the index <em>w</em><sup>2</sup> has to do with the difference between population means.</p>
<p>…When the difference <em>u</em><sub>1</sub> - <em>u</em><sub>2</sub> is zero, then <em>w</em><sup>2</sup> must be zero. In the usual <em>t</em>-test for a difference, the hypothesis of no difference between means is equivalent to the hypothesis that <em>w</em><sup>2</sup> = 0. On the other hand, when there is any difference at all between population means, the value of <em>w</em><sup>2</sup> must be greater than 0. In short, a true difference is “big” in the sense of predictive power only if the square of that difference is large relative to <span><span><span><span aria-label="\sigma^2_Y"></span></span></span></span>. However, in significance tests such as <em>t</em>, we compare the difference we get with an estimate of σ<sub>diff</sub>. The standard error of the difference can be made almost as small as we choose if we are given a free choice of sample size. Unless sample size is specified, there is no <em>necessary</em> connection between significance and the true strength of association.</p>
<p>This points up the fallacy of evaluating the “goodness” of a result in terms of statistical-significance alone, without allowing for the sample size used. All significant results do not imply the same degree of true association between independent and dependent variables.</p>
<p>It is sad but true that researchers have been known to capitalize on this fact. There is a certain amount of “testmanship” involved in using inferential statistics. <em>Virtually any study can be made to show significant results if one uses enough subjects, regardless of how nonsensical the content may be.</em> There is surely nothing on earth that is completely independent of anything else. The strength of an association may approach zero, but it should seldom or never be exactly zero. If one applies a large enough sample of the study of any relation, trivial or meaningless as it may be, sooner or later he is almost certain to achieve a significant result. Such a result may be a valid finding, but only in the sense that one can say with assurance that some association is not exactly zero. The degree to which such a finding enhances our knowledge is debatable. If the criterion of strength of association is applied to such a result, it becomes obvious that little or nothing is actually contributed to our ability to predict one thing from another.</p>
<p>For example, suppose that two methods of teaching first grade children to read are being compared. A random sample of 1000 children are taught to read by method I, another sample of 1000 children by method II. The results of the instruction are evaluated by a test that provides a score, in whole units, for each child. Suppose that the results turned out as follows:</p>
<div>
<table>
<thead>
<tr>
<th><p>Method I</p></th>
<th><p>Method II</p></th>
</tr>
</thead>
<tbody>
<tr>
<td><p><em>M</em><sub>1</sub> = 147.21</p></td>
<td><p><em>M</em><sub>2</sub> = 147.64</p></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><p><em>N</em><sub>1</sub> = 1000</p></td>
<td><p><em>N</em><sub>2</sub> = 1000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Then, the estimated standard error of the difference is about 0.145, and the <em>z</em> value is</p>

<p>This certainly permits rejection of the null hypothesis of no difference between the groups. However, does it really tell us very much about what to expect of an individual child’s score on the test, given the information that he was taught by method I or method II? If we look at the group of children taught by method II, and assume that the distribution of their scores is approximately normal, we find that about 45% of these children fall <em>below</em> the mean score for children in group I. Similarly, about 45% of children in group I fall above the mean score for group II. Although the difference between the two groups is significant, the two groups actually overlap a great deal in terms of their performances on the test. In this sense, the two groups are really not very different at all, even though the difference between the means is quite significant in a purely statistical sense.</p>
<p>Putting the matter in a slightly different way, we note that the grand mean of the two groups is 147.425. Thus, our best bet about the score of any child, not knowing the method of his training, is 147.425. If we guessed that any child drawn at random from the combined group should have a score above 147.425, we should be wrong about half the time. However, among the original groups, according to method I and method II, the proportions falling above and below this grand mean are approximately as follows:</p>
<div>
<table>
<thead>
<tr>
<th></th>
<th><p>Below 147.425</p></th>
<th><p>Above 147.425</p></th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Method I</p></td>
<td><p>0.51</p></td>
<td><p>0.49</p></td>
</tr>
<tr>
<td><p>Method II</p></td>
<td><p>0.49</p></td>
<td><p>0.51</p></td>
</tr>
</tbody>
</table>
</div>
<p>This implies that if we know a child is from group I, and we guess that this score is below the grand mean, then we will be wrong about 49% of the time. Similarly, if a child is from group II, and we guess his score to be above the grand mean, we will be wrong about 49% of the time. If we are not given the group to which the child belongs, ad we guess either above or below the grand mean, we will be wrong about 50% of the time. Knowing the group does reduce the probability of error in such a guess, but it does not reduce it very much. The method by which the child was trained simply doesn’t tell us a great deal about what the child’s score will be, even though the difference in mean scores is significant in the statistical sense.</p>
<p>This kind of testmanship flourishes best when people pay too much attention to the significance test and too little to the degree of statistical association the finding represents. This clutters up the literature with findings that are often not worth pursuing, and which serve only to obscure the really important predictive relations that occasionally appear. The serious scientist owes it to himself and his readers to ask not only, “Is there any association between <em>X</em> and <em>Y</em>?” but also, “How much does my finding suggest about the power to predict <em>Y</em> from <em>X</em>?” Much too much emphasis is paid to the former, at the expense of the latter, question.</p>
</blockquote>
</section>
<section id="oakes-1975">
<h2><a href="#oakes-1975" title="Link to section: § 'Oakes1975'"><span><span>Oakes</span><span>1975</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1975-oakes.pdf" id="oakes-1975" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="847436" data-filesize-percentage="71" title="'On the alleged falsity of the null hypothesis', Oakes 1975">“On the alleged falsity of the null hypothesis”</a>, <span><span>Oakes</span><span>1975</span></span>:</p>
<blockquote>
<p>Consideration is given to the contention by <a href="#bakan-1966">Bakan</a>, <a href="#meehl-1967">Meehl</a>, <a href="#nunnally-1960">Nunnally</a>, and others that the null hypothesis in behavioral research is generally false in nature and that the <em>N</em> is large enough, it will always be rejected. A distinction is made between self-selected-groups research designs and true experiments, and it is suggested that the null hypothesis probably is generally false in the case of research involving the former design, but is not in the case of research involving the latter. Reasons for the falsity of the null hypothesis in the one case but not in the other are suggested.</p>
<p>The U.S. <a href="https://en.wikipedia.org/wiki/Office_of_Economic_Opportunity" id="_eXhkNQ_h" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Office_of_Economic_Opportunity#bodyContent" title="Office of Economic Opportunity">Office of Economic Opportunity</a> has recently reported the results of research on performance contracting. With 23,000 <em>Ss</em>—13,000 experimental and 10,000 control—the null hypothesis was not rejected. The experimental <em>Ss</em>, who received special instruction in reading and mathematics for 2 hours per day during the <span title="The date range 1970–1971 lasted 1 year, ending 54 years ago.">1970–1971<sub><span title="1970 was 54 years ago.">54ya</span></sub></span> school year, did not differ statistically-significantly from the controls in achievement gains (American Institutes for <span><span>Research</span><span>1972</span></span>, pg 5). Such an inability to reject the null hypothesis might not be surprising to the typical classroom teacher or to most educational psychologists, but in view of the huge <em>N</em> involved, it should give pause to <a href="#bakan-1966"><span><span>Bakan</span><span>1966</span></span></a>, who contends that the null hypothesis is generally false in behavioral research, as well as to those writers such as <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a> and <a href="#meehl-1967"><span><span>Meehl</span><span>1967</span></span></a>, who agree with that contention. They hold that if the <em>N</em> is large enough, the null is sure to be rejected in behavioral research. This paper will suggest that the Falsity contention does not hold in the case of experimental research—that the null hypothesis is not generally false in such research.</p>
<ul>
<li><p>American Institutes For Research. <span>1972<sub><span title="1972 was 53 years ago.">53ya</span></sub></span>. “OEO reports performance contracting a failure”. <em>Behavioral Sciences Newsletter for Research Planning</em>, 9, 4–5. [see also <a href="https://gwern.net/doc/sociology/1972-page.pdf" id="page-1972" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="443476" data-filesize-percentage="55" title="'How We <em>All</em> Failed In Performance Contracting', Page 1972">“How We <em>All</em> Failed In Performance Contracting”</a>, <span><span>Page</span><span>1972</span></span>]</p></li>
</ul>
</blockquote>
</section>
<section id="loehlin-nichols-1976">
<h2><a href="#loehlin-nichols-1976" title="Link to section: § 'Loehlin &amp; Nichols1976'"><span><span>Loehlin &amp; Nichols</span><span>1976</span></span></a></h2>
<p><span><span>Loehlin &amp; Nichols</span><span>1976</span></span>, <a href="https://gwern.net/doc/genetics/heritable/1976-loehlin-heredityenvironmentandpersonality.pdf" id="loehlin-nichols-1976-link" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="25148441" data-filesize-percentage="97" title="'<em>Heredity, Environment, &amp; Personality: A Study of 850 Sets of Twins</em>', Loehlin &amp; Nichols 1976"><em>Heredity, Environment and Personality: A Study of 850 Sets of Twins</em></a> (see also <a href="https://gwern.net/doc/genetics/heritable/1979-nichols-heredityandenvironment.pdf" id="_sa0-lqzd" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="3280662" data-filesize-percentage="89"><em>Heredity and Environment: Major Findings from Twin Studies of Ability, Personality, and Interests</em></a><span>, <span><span>Nichols</span><span>1976/1979</span></span>):</span></p>
<blockquote>
<p>This volume reports on a study of 850 pairs of twins who were tested to determine the influence of heredity and environment on individual differences in personality, ability, and interests. It presents the background, research design, and procedures of the study, a complete tabulation of the test results, and the authors’ extensive analysis of their findings. Based on one of the largest studies of twin behavior conducted in the twentieth century, the book challenges a number of traditional beliefs about genetic and environmental contributions to personality development.</p>
<p>The subjects were chosen from participants in the National Merit Scholarship Qualifying Test of <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span> and were mailed a battery of personality and interest questionnaires. In addition, parents of the twins were sent questionnaires asking about the twins’ early experiences. A similar sample of nontwin students who had taken the merit exam provided a comparison group. The questions investigated included how twins are similar to or different from non-twins, how identical twins are similar to or different from fraternal twins, how the personalities and interests of twins reflect genetic factors, how the personalities and interests of twins reflect early environmental factors, and what implications these questions have for the general issue of how heredity and environment influence the development of psychological characteristics. In attempting to answer these questions, the authors shed light on the importance of both genes and environment and form the basis for different approaches in behavior genetic research.</p>
</blockquote>
<p>The book is largely a discussion of comprehensive <a href="https://en.wikipedia.org/wiki/Summary_statistics" id="_hUejtaYs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Summary_statistics#bodyContent" title="Summary statistics">summary statistics</a> of twin correlations from an early large-scale twin study (canvassed via the National Merit Scholarship Qualifying Test, <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span>). They attempted to compile a large-scale twin sample without the burden of a full-blown twin registry by an extensive mail survey of the <em>n</em><span> = <span>1507<sub><span title="1507 was 518 years ago.">518ya</span></sub></span> 11th-grade adolescent pairs of participants in the high school National Merit Scholarship Qualifying Test of <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span> (total </span><em>n</em>~600,000) who indicated they were twins (as well as a control sample of non-twins), yielding 514 identical twin &amp; 336 (same-sex) fraternal twin pairs; they were questioned as follows:</p>
<blockquote>
<p>…to these [participants] were mailed a battery of personality and interest tests, including the California Psychological Inventory (CPI), the Holland Vocational Preference Inventory (VPI), an experimental Objective Behavior Inventory (OBI), an Adjective Check List (ACL), and a number of other, briefer self-rating scales, attitude measures, and other items. In addition, a parent was asked to fill out a questionnaire describing the early experiences and home environment of the twins. Other brief questionnaires were sent to teachers and friends, asking them to rate the twins on a number of personality traits; because these ratings were available for only part of our basic sample, they have not been analyzed in detail and will not be discussed further in this book. (The parent and twin questionnaires, except for the CPI, are reproduced in Appendix A.)</p>
</blockquote>
<p>Unusually, the book includes appendices reporting raw twin-pair correlations for all of the reported items, not a mere handful of selected analyses on full test-scales or subfactors. (Because of this, I was able to extract variables related to leisure time preferences &amp; activities for <a href="https://gwern.net/amuse#loehlin-nichols-1976-a-study-of-850-sets-of-twins" id="gwern-amuse--loehlin-nichols-1976-a-study-of-850-sets-of-twins" data-filesize-bytes="190311" data-filesize-percentage="69" title="'Amusing Ourselves to Death? § Loehlin &amp; Nichols 1976: <em>A Study of 850 Sets of Twins</em>', Gwern 2018">another analysis</a>.) One can see that even down to the item level, heritabilities tend to be non-zero and most variables are correlated within-individuals or with environments as well.</p>
</section>
<section id="meehl-1978">
<h2><a href="#meehl-1978" title="Link to section: § 'Meehl1978'"><span><span>Meehl</span><span>1978</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/1978-meehl.pdf" id="meehl-1978" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="376480" data-filesize-percentage="51" title="'Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology', Meehl 1978">“Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology”</a>, <span><span>Meehl</span><span>1978</span></span>:</p>
<blockquote>
<p>Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of “significant differences” are little more than complex, causally uninterpretable outcomes of <a href="https://en.wikipedia.org/wiki/Power_of_a_test" id="_4HjKZF87" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Power_of_a_test#bodyContent" title="Statistical power">statistical power</a> functions.</p>
<p>The kinds of theories and the kinds of theoretical risks to which we put them in soft psychology when we use significance testing as our method are <em>not</em> like testing Meehl’s theory of weather by seeing how well it forecasts the number of inches it will rain on certain days. Instead, they are depressingly close to testing the theory by seeing whether it rains in April at all, or rains several days in April, or rains in April more than in May. It happens mainly because, as I believe is generally recognized by statisticians today and by thoughtful social scientists, the null hypothesis, taken literally, is always false. I shall not attempt to document this here, because among sophisticated persons it is taken for granted. (See Morrison &amp; Henkel, <span>1970<sub><span title="1970 was 55 years ago.">55ya</span></sub></span> [<em>The Significance Test Controversy: A Reader</em>], especially the chapters by <a href="#bakan-1966">Bakan</a>, Hogben, <a href="#lykken-1968">Lykken</a>, <a href="#meehl-1967">Meehl</a>, and <a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" id="rozeboom-1960" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" data-url-original="https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf" data-filesize-bytes="653742" data-filesize-percentage="40" title="The fallacy of the null-hypothesis significance test">Rozeboom</a>.) A little reflection shows us why it has to be the case, since an output variable such as adult IQ, or academic achievement, or effectiveness at communication, or whatever, will always, in the social sciences, be a function of a sizable but finite number of factors. (The smallest contributions may be considered as essentially a random variance term.) In order for two groups (males and females, or whites and blacks, or manic depressives and <a href="https://en.wikipedia.org/wiki/Schizophrenia" id="_Gh5HccJm" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Schizophrenia#bodyContent" title="Schizophrenia">schizophrenics</a>, or Republicans and Democrats) to be <em>exactly</em> equal on such an output variable, we have to imagine that they are exactly equal <em>or</em> delicately counterbalanced on all of the contributors in the causal equation, which will never be the case.</p>
<p>Following the general line of reasoning (presented by myself and several others over the last decade), from the fact that the null hypothesis is always false in soft psychology, it follows that the probability of refuting it depends wholly on the sensitivity of the experiment—its logical design, the net (attenuated) construct validity of the measures, and, most importantly, the sample size, which determines where we are on the statistical power function. Putting it crudely, if you have enough cases and your measures are not totally unreliable, the null hypothesis will always be falsified, <em>regardless of the truth of the substantive theory</em>. Of course, it could be falsified in the wrong direction, which means that as the power improves, the probability of a corroborative result approaches one-half. However, if the theory has no verisimilitude—such that we can imagine, so to speak, picking our empirical results randomly out of a directional hat apart from any theory—the probability of refuting by getting a significant difference in the wrong direction also approaches one-half. Obviously, this is quite unlike the situation desired from either a Bayesian, a Popperian, or a commonsense scientific standpoint. As I have pointed out elsewhere (<a href="#meehl-1967">Meehl, 1967</a>/<span>1970<sub><span title="1970 was 55 years ago.">55ya</span></sub></span>b; but see criticism by <a href="#oakes-1975">Oakes, 1975</a>; <a href="https://gwern.net/doc/statistics/causality/1973-keuth.pdf" id="keuth-1973" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="437871" data-filesize-percentage="55" title="On prior probabilities of rejecting statistical hypotheses">Keuth, 1973</a>; and rebuttal by <a href="https://gwern.net/doc/statistics/causality/1975-swoyer.pdf" id="swoyer-monson-1975" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="346599" data-filesize-percentage="49" title="Theory confirmation in psychology">Swoyer &amp; Monson, 1975</a>), an improvement in instrumentation or other sources of experimental accuracy tends, in physics or astronomy or chemistry or genetics, to subject the theory to a greater risk of refutation <em>modus tollens</em>, whereas improved precision in null hypothesis testing usually decreases this risk. A successful significance test of a substantive theory in soft psychology provides a feeble corroboration of the theory because the procedure has subjected the theory to a feeble risk.</p>
<p>…I am not making some nit-picking statistician’s correction. I am saying that the whole business is so radically defective as to be scientifically almost pointless… I am making a philosophical complaint or, if you prefer, a complaint in the domain of scientific method. I suggest that when a reviewer tries to “make theoretical sense” out of such a table of favorable and adverse significance test results, what the reviewer is actually engaged in, willy-nilly or unwittingly, is meaningless substantive constructions on the properties of the statistical power function, and almost nothing else.</p>
<p>…You may say, “But, Meehl, R. A. Fisher was a genius, and we all know how valuable his stuff has been in agronomy. Why shouldn’t it work for soft psychology?” Well, I am not intimidated by Fisher’s genius, because my complaint is not in the field of mathematical statistics, and as regards inductive logic and philosophy of science, it is well-known that Sir Ronald permitted himself a great deal of dogmatism. I remember my amazement when the late <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap" id="_R973FqQu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Rudolf_Carnap#bodyContent" title="Rudolf Carnap">Rudolf Carnap</a> said to me, the first time I met him, “But, of course, on this subject Fisher is just mistaken: surely you must know that.” My statistician friends tell me that it is not clear just how useful the significance test has been in biological science either, but I set that aside as beyond my competence to discuss.</p>
</blockquote>
</section>
<section id="loftus-loftus-1982">
<h2><a href="#loftus-loftus-1982" title="Link to section: § 'Loftus &amp; Loftus1982'"><span><span>Loftus &amp; Loftus</span><span>1982</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1982-loftus-essenceofstatistics.pdf" id="loftus-loftus-1982" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="89687276" data-filesize-percentage="99" title="'Essence of Statistics (Second Edition)', Loftus &amp; Loftus 1982"><em>Essence of Statistics</em></a>, <span><span>Loftus &amp; Loftus</span><span>1982/1988</span></span><span> (2nd ed), pg515–516 (pg498–499 in the <span>1982<sub><span title="1982 was 43 years ago.">43ya</span></sub></span> printing):</span></p>
<blockquote>
<p><strong>Relative Importance Of These 3 Measures</strong>. It is a matter of some debate as to which of these 3 measures [σ<sup>2</sup>/<em>p</em>/R<sup>2</sup>] we should pay the most attention to in an experiment. It’s our opinion that finding a “significant effect” really provides very little information because it’s almost certainly true that <em>some</em> relationship (however small) exists between <em>any</em> two variables. And in general <em>finding</em> a significant effect simply means that enough observations have been collected in the experiment to make the statistical test of the experiment powerful enough to detect whatever effect there is. The smaller the effect, the more powerful the experiments needs to be of course, but no matter how small the effect, it’s always possible in principle to design an experiment sufficiently powerful to detect it. We saw a striking example of this principle in the office hours experiment. In this experiment there was a relationship between the two variables—and since there were so many subjects in the experiment (that is, since the test was so powerful), this relationship was revealed in the statistical analysis. But was it anything to write home about? Certainly not. In any sort of practical context the size of the effect, although nonzero, is so small it can almost be ignored.</p>
<p>It is our judgment that accounting for variance is really much more meaningful than testing for significance.</p>
</blockquote>
</section>
<section id="meehl-1990-1">
<h2><a href="#meehl-1990-1" title="Link to section: § 'Meehl1990 (1)'"><span><span>Meehl</span><span>1990</span></span> (1)</a></h2>
<p><a href="https://meehl.umn.edu/sites/g/files/pua1696/f/144whysummaries.pdf" id="_zj-iY4Ip" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Why summaries of research on psychological theories are often uninterpretable”</a>, <span><span>Meehl</span><span>1990a</span></span> (also discussed in <span><span>Cohen’s</span><span>1994</span></span> paper <a href="https://gwern.net/doc/www/www.sjsu.edu/3936a51363629ca29bb8ee525ed25d2eda692757.pdf" id="cohen-1994" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.sjsu.edu/3936a51363629ca29bb8ee525ed25d2eda692757.pdf" data-url-original="https://www.sjsu.edu/faculty/gerstman/misc/Cohen1994.pdf" data-filesize-bytes="389513" data-filesize-percentage="26">“The Earth is Round (<em>p</em> &lt; 0.05)”</a>):</p>
<blockquote>
<p>Problem 6. <em>Crud factor</em>: In the social sciences and arguably in the biological sciences, “everything correlates to some extent with everything else.” This truism, which I have found no competent psychologist disputes given 5 minutes reflection, does not apply to pure experimental studies in which attributes that the subjects bring with them are not the subject of study (except in so far as they appear as a source of error and hence in the denominator of a significance test).<sup>6</sup> There is nothing mysterious about the fact that in psychology and sociology everything correlates with everything. Any measured trait or attribute is some function of a list of partly known and mostly unknown causal factors in the genes and life history of the individual, and both genetic and environmental factors are known from tons of empirical research to be themselves correlated. To take an extreme case, suppose we construe the null hypothesis literally (objecting that we mean by it “almost null” gets ahead of the story, and destroys the rigor of the Fisherian mathematics!) and ask whether we expect males and females in Minnesota to be precisely equal in some arbitrary trait that has individual differences, say, color naming. In the case of color naming we could think of some obvious differences right off, but even if we didn’t know about them, what is the causal situation? If we write a causal equation (which is not the same as a regression equation for pure predictive purposes but which, if we had it, would serve better than the latter) so that the score of an individual male is some function (presumably nonlinear if we knew enough about it but here supposed linear for simplicity) of a rather long set of causal variables of genetic and environmental type <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, … <em>X</em><sub>m</sub>. These values are operated upon by regression coefficients <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, …<em>b</em><sub>m</sub>.</p>
<p>…Now we write a similar equation for the class of females. Can anyone suppose that the beta coefficients for the two sexes will be exactly the same? Can anyone imagine that the mean values of all of the <em>X</em>s will be exactly the same for males and females, even if the culture were not still considerably sexist in child-rearing practices and the like? If the betas are not exactly the same for the two sexes, and the mean values of the <em>X</em>s are not exactly the same, what kind of Leibnitzian preestablished harmony would we have to imagine in order for the mean color-naming score to come out exactly equal between males and females? It boggles the mind; it simply would never happen. As Einstein said, “the Lord God is subtle, but He is not malicious.” We cannot imagine that nature is out to fool us by this kind of delicate balancing. Anybody familiar with large scale research data takes it as a matter of course that when the <em>N</em> gets big enough she will not be looking for the statistically-significant correlations but rather looking at their patterns, since almost all of them will be significant. In saying this, I am not going counter to what is stated by mathematical statisticians or psychologists with statistical expertise. For example, the standard psychologist’s textbook, the excellent treatment by Hays (<a href="#hays-1973">1973, page 415</a>), explicitly states that, taken literally, the null hypothesis is always false.</p>
<p>20 ago David Lykken and I conducted an exploratory study of the crud factor which we never published but I shall summarize it briefly here. (I offer it not as “empirical proof”—that <em>H</em><sub>0</sub> taken literally is quasi-always false hardly needs proof and is generally admitted—but as a punchy and somewhat amusing example of an insufficiently appreciated truth about soft correlational psychology.) In <span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span>, the University of Minnesota Student Counseling Bureau’s Statewide Testing Program administered a questionnaire to 57,000 high school seniors, the items dealing with family facts, attitudes toward school, vocational and educational plans, leisure time activities, school organizations, etc. We cross-tabulated a total of 15 (and then 45) variables including the following (the number of categories for each variable given in parentheses): father’s occupation (7), father’s education (9), mother’s education (9), number of siblings (10), birth order (only, oldest, youngest, neither), educational plans after high school (3), family attitudes towards college (3), do you like school (3), sex (2), college choice (7), occupational plan in 10 years (20), and religious preference (20). In addition, there were 22 “leisure time activities” such as “acting”, “model building”, “cooking”, etc., which could be treated either as a single 22-category variable or as 22 dichotomous variables. There were also 10 “high school organizations” such as “school subject clubs”, “farm youth groups”, “political clubs”, etc., which also could be treated either as a single ten-category variable or as 10 dichotomous variables. Considering the latter two variables as multichotomies gives a total of 15 variables producing 105 different cross-tabulations. All values of χ<sup>2</sup> for these 105 cross-tabulations were statistically-significant, and 101 (96%) of them were significant with a probability of less than 10<sup>−6</sup>.</p>
<p>…If “leisure activity” and “high school organizations” are considered as separate dichotomies, this gives a total of 45 variables and 990 different crosstabulations. Of these, 92% were statistically-significant and more than 78% were significant with a probability less than 10<sup>−6</sup>. Looked at in another way, the median number of significant relationships between a given variable and all the others was 41 out of a possible 44!</p>
<p>We also computed <a href="https://en.wikipedia.org/wiki/Medical_College_Admission_Test" id="_RKnM_1Nc" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Medical_College_Admission_Test#bodyContent" title="Medical College Admission Test">MCAT</a> scores by category for the following variables: number of siblings, birth order, sex, occupational plan, and religious preference. Highly significant deviations from chance allocation over categories were found for each of these variables. For example, the females score higher than the males; MCAT score steadily and markedly decreases with increasing numbers of siblings; eldest or only children are statistically-significantly brighter than youngest children; there are marked differences in MCAT scores between those who hope to become nurses and those who hope to become nurses aides, or between those planning to be farmers, engineers, teachers, or physicians; and there are substantial MCAT differences among the various religious groups. We also tabulated the 5 principal Protestant religious denominations (Baptist, Episcopal, Lutheran, Methodist, and Presbyterian) against all the other variables, finding highly significant relationships in most instances. For example, only children are nearly twice as likely to be Presbyterian than Baptist in Minnesota, more than half of the Episcopalians “usually like school” but only 45% of Lutherans do, 55% of Presbyterians feel that their grades reflect their abilities as compared to only 47% of Episcopalians, and Episcopalians are more likely to be male whereas Baptists are more likely to be female. 83% of Baptist children said that they enjoyed dancing as compared to 68% of Lutheran children. More than twice the proportion of Episcopalians plan to attend an out of state college than is true for Baptists, Lutherans, or Methodists. The proportion of Methodists who plan to become conservationists is nearly twice that for Baptists, whereas the proportion of Baptists who plan to become receptionists is nearly twice that for Episcopalians.</p>
<p>In addition, we tabulated the 4 principal Lutheran Synods (Missouri, ALC, LCA, and Wisconsin) against the other variables, again finding highly significant relationships in most cases. Thus, 5.9% of Wisconsin Synod children have no siblings as compared to only 3.4% of Missouri Synod children. 58% of ALC Lutherans are involved in playing a musical instrument or singing as compared to 67% of Missouri Synod Lutherans. 80% of Missouri Synod Lutherans belong to school or political clubs as compared to only 71% of LCA Lutherans. 49% of ALC Lutherans belong to debate, dramatics, or musical organizations in high school as compared to only 40% of Missouri Synod Lutherans. 36% of LCA Lutherans belong to organized non-school youth groups as compared to only 21% of Wisconsin Synod Lutherans. [Preceding text courtesy of D. T. Lykken.]</p>
<p>These relationships are not, I repeat, Type I errors. They are facts about the world, and with <em>N</em> = 57,000 they are pretty stable. Some are theoretically easy to explain, others more difficult, others completely baffling. The “easy” ones have multiple explanations, sometimes competing, usually not. Drawing theories from a pot and associating them whimsically with variable pairs would yield an impressive batch of <em>H</em><sub>0</sub>-refuting “confirmations.”</p>
<p>Another amusing example is the behavior of the items in the 550 items of the MMPI pool with respect to sex. Only 60 items appear on the Mf scale, about the same number that were put into the pool with the hope that they would discriminate femininity. It turned out that over half the items in the scale were not put in the pool for that purpose, and of those that were, a bare majority did the job. Scale derivation was based on item analysis of a small group of criterion cases of male homosexual invert syndrome, a significant difference on a rather small <em>N</em> of Dr.&nbsp;Starke Hathaway’s private patients being then conjoined with the requirement of discriminating between male normals and female normals. When the <em>N</em> becomes very large as in the data published by <a href="https://gwern.net/doc/statistics/causality/1973-wendell-anmmpisourcebook.pdf" id="_AWpWvtby" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="42307651" data-filesize-percentage="98">Swenson, Pearson, and Osborne (<span>1973<sub><span title="1973 was 52 years ago.">52ya</span></sub></span>; <em>An MMPI Source Book: Basic Item, Scale, And Pattern Data On 50,000 Medical Patients</em>. Minneapolis, MN: University of Minnesota Press.)</a>, approximately 25,000 of each sex tested at the Mayo Clinic over a period of years, it turns out that 507 of the 550 items discriminate the sexes. Thus in a heterogeneous item pool we find only 8% of items failing to show a significant difference on the sex dichotomy. The following are sex-discriminators, the male/female differences ranging from a few percentage points to over 30%:<sup>7</sup></p>
<ul>
<li><p>Sometimes when I am not feeling well I am cross.</p></li>
<li><p>I believe there is a Devil and a Hell in afterlife.</p></li>
<li><p>I think nearly anyone would tell a lie to keep out of trouble.</p></li>
<li><p>Most people make friends because friends are likely to be useful to them.</p></li>
<li><p>I like poetry.</p></li>
<li><p>I like to cook.</p></li>
<li><p>Policemen are usually honest.</p></li>
<li><p>I sometimes tease animals.</p></li>
<li><p>My hands and feet are usually warm enough.</p></li>
<li><p>I think Lincoln was greater than Washington.</p></li>
<li><p>I am certainly lacking in self-confidence.</p></li>
<li><p>Any man who is able and willing to work hard has a good chance of succeeding.</p></li>
</ul>
<p>I invite the reader to guess which direction scores “feminine.” Given this information, I find some items easy to “explain” by one obvious theory, others have competing plausible explanations, still others are baffling.</p>
<p>Note that we are not dealing here with some source of statistical error (the occurrence of random sampling fluctuations). That source of error is limited by the significance level we choose, just as the probability of Type II error is set by initial choice of the statistical power, based upon a pilot study or other antecedent data concerning an expected average difference. Since in social science everything correlates with everything to some extent, due to complex and obscure causal influences, in considering the crud factor we are talking about <em>real</em> differences, <em>real</em> correlations, <em>real</em> trends and patterns for which there is, of course, some true but complicated multivariate causal theory. I am not suggesting that these correlations are fundamentally unexplainable. They would be completely explained if we had the knowledge of Omniscient Jones, which we don’t. The point is that we are in the weak situation of corroborating our particular substantive theory by showing that <em>X</em> and <em>Y</em> are “related in a nonchance manner”, when our theory is too weak to make a numerical prediction or even (usually) to set up a range of admissible values that would be counted as corroborative.</p>
<p>…Some psychologists play down the influence of the ubiquitous crud factor, what <a href="#lykken-1968" title="Statistical-Significance in Psychological Research">David Lykken (<span>1968<sub><span title="1968 was 57 years ago.">57ya</span></sub></span>)</a> calls the “ambient correlational noise” in social science, by saying that we are not in danger of being misled by small differences that show up as significant in gigantic samples. How much that softens the blow of the crud factor’s influence depends upon the crud factor’s average size in a given research domain, about which neither I nor anybody else has accurate information. <em>But the notion that the correlation between arbitrarily paired trait variables will be, while not literally zero, of such minuscule size as to be of no importance, is surely wrong.</em> Everybody knows that there is a set of demographic factors, some understood and others quite mysterious, that correlate quite respectably with a variety of traits. (Socioeconomic status, SES, is the one usually considered, and frequently assumed to be only in the “input” causal role.) The clinical scales of the MMPI were developed by empirical keying against a set of disjunct nosological categories, some of which are phenomenologically and psychodynamically opposite to others. Yet the 45 pairwise correlations of these scales are almost always positive (scale Ma provides most of the negatives) and a representative size is in the neighborhood of 0.35 to 0.40. The same is true of the scores on the Strong Vocational Interest Blank, where I find an average absolute value correlation close to 0.40. The malignant influence of so-called “methods covariance” in psychological research that relies upon tasks or tests having certain kinds of behavioral similarities such as questionnaires or ink blots is commonplace and a regular source of concern to clinical and personality psychologists. For further discussion and examples of crud factor size, see <a href="#meehl-1990-2" title="Appraising and amending theories: the strategy of Lakatosian defense and two principles that warrant using it">Meehl (<span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>)</a>.</p>
<p>Now suppose we imagine a society of psychologists doing research in this soft area, and each investigator sets his experiments up in a whimsical, irrational manner as follows: First he picks a theory at random out of the theory pot. Then he picks a pair of variables randomly out of the observable variable pot. He then arbitrarily assigns a direction (you understand there is no intrinsic connection of content between the substantive theory and the variables, except once in a while there would be such by coincidence) and says that he is going to test the randomly chosen substantive theory by pretending that it predicts—although in fact it does not, having no intrinsic contentual relation—a positive correlation between randomly chosen observational variables <em>X</em> and <em>Y</em>. Now suppose that the crud factor operative in the broad domain were 0.30, that is, the average correlation between all of the variables pairwise in this domain is 0.30. This is not sampling error but the true correlation produced by some complex unknown network of genetic and environmental factors. Suppose he divides a normal distribution of subjects at the median and uses all of his cases (which frequently is not what is done, although if properly treated statistically that is not methodologically sinful). Let us take variable <em>X</em> as the “input” variable (never mind its causal role). The mean score of the cases in the top half of the distribution will then be at one mean deviation, that is, in standard score terms they will have an average score of 0.80. Similarly, the subjects in the bottom half of the <em>X</em> distribution will have a mean standard score of -0.80. So the mean difference in standard score terms between the high and low <em>X</em>s, the one “experimental” and the other “control” group, is 1.6. If the regression of output variable <em>Y</em> on <em>X</em> is approximately linear, this yields an expected difference in standard score terms of 0.48, so the difference on the arbitrarily defined “output” variable <em>Y</em> is in the neighborhood of half a standard deviation.</p>
<p>When the investigator runs a <em>t</em>-test on these data, what is the probability of achieving a statistically-significant result? This depends upon the statistical power function and hence upon the sample size, which varies widely, more in soft psychology because of the nature of the data collection problems than in experimental work. I do not have exact figures, but an informal scanning of several issues of journals in the soft areas of clinical, abnormal, and social gave me a representative value of the number of cases in each of two groups being compared at around <em>N</em><sub>1</sub> = <em>N</em><sub>2</sub> = 37 (that’s a median because of the skewness, sample sizes ranging from a low of 17 in one clinical study to a high of 1,000 in a social survey study). Assuming equal variances, this gives us a standard error of the mean difference of 0.2357 in sigma-units, so that our <em>t</em> is a little over 2.0. The substantive theory in a real life case being almost invariably predictive of a direction (it is hard to know what sort of significance testing we would be doing otherwise), the 5% level of confidence can be legitimately taken as one-tailed and in fact could be criticized if it were not (assuming that the 5% level of confidence is given the usual special magical significance afforded it by social scientists!). The directional 5% level being at 1.65, the expected value of our <em>t</em>-test in this situation is approximately 0.35 <em>t</em> units from the required significance level. Things being essentially normal for 72 df, this gives us a power of detecting a difference of around 0.64.</p>
<p>However, since in our imagined “experiment” the assignment of direction was random, the probability of detecting a difference in the predicted direction (even though in reality this prediction was not mediated by any rational relation of content) is only half of that. Even this conservative power based upon the assumption of a completely random association between the theoretical substance and the pseudopredicted direction should give one pause. We find that the probability of getting a positive result from a theory with no verisimilitude whatsoever, associated in a totally whimsical fashion with a pair of variables picked randomly out of the observational pot, is <em>one chance in 3</em>! This is quite different from the 0.05 level that people usually think about. Of course, the reason for this is that the 0.05 level is based upon strictly holding <em>H</em><sub>0</sub> if the theory were false. Whereas, because in the social sciences everything is correlated with everything, for epistemic purposes (despite the rigor of the mathematician’s tables) the true baseline—if the theory has nothing to do with reality and has only a chance relationship to it (so to speak, “any connection between the theory and the facts is purely coincidental”) - is 6 or 7 times as great as the reassuring 0.05 level upon which the psychologist focuses his mind. If the crud factor in a domain were running around 0.40, the power function is 0.86 and the “directional power” for random theory/prediction pairings would be 0.43.</p>
<p>…A similar situation holds for psychopathology, and for many variables in personality measurement that refer to aspects of social competence on the one hand or impairment of interpersonal function (as in mental illness) on the other. <a href="#thorndike-1920">Thorndike had a dictum</a> “All good things tend to go together.”</p>
</blockquote>
</section>
<section id="meehl-1990-2">
<h2><a href="#meehl-1990-2" title="Link to section: § 'Meehl1990 (2)'"><span><span>Meehl</span><span>1990</span></span> (2)</a></h2>
<p><a href="https://gwern.net/doc/www/meehl.umn.edu/577e7c2d422171276a0b7dc8ebba9da1962cf36b.pdf" id="_2PL7SjGU" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/meehl.umn.edu/577e7c2d422171276a0b7dc8ebba9da1962cf36b.pdf" data-url-original="https://meehl.umn.edu/sites/meehl.umn.edu/files/files/147appraisingamending.pdf" data-filesize-bytes="1338000" data-filesize-percentage="55">“Appraising and amending theories: the strategy of Lakatosian defense and two principles that warrant using it”</a>, <span><span>Meehl</span><span>1990b</span></span>:</p>
<blockquote>
<p>Research in the behavioral sciences can be experimental, correlational, or field study (including clinical); only the first two are addressed here. For reasons to be explained (<a href="#meehl-1990-1">Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c</a>), I treat as correlational those experimental studies in which the chief theoretical test provided involves an interaction effect between an experimental manipulation and an individual-differences variable (whether trait, status, or demographic). In correlational research there arises a special problem for the social scientist from the empirical fact that “everything is correlated with everything, more or less.” My colleague David Lykken presses the point further to include most, if not all, purely experimental research designs, saying that, speaking causally, “Everything influences everything”, a stronger thesis that I neither assert nor deny but that I do not rely on here. The obvious fact that everything is more or less correlated with everything in the social sciences is readily foreseen from the armchair on common-sense considerations. These are strengthened by more advanced theoretical arguments involving such concepts as genetic linkage, auto-catalytic effects between cognitive and affective processes, traits reflecting influences such as child-rearing practices correlated with intelligence, ethnicity, social class, religion, and so forth. If one asks, to take a trivial and theoretically uninteresting example, whether we might expect to find social class differences in a color-naming test, there immediately spring to mind numerous influences, ranging from (1) verbal intelligence leading to better verbal discriminations and retention of color names to (2) class differences in maternal teaching behavior (which one can readily observe by watching mothers explain things to their children at a zoo) to (3) more subtle—but still nonzero—influences, such as upper-class children being more likely Anglicans than Baptists, hence exposed to the changes in liturgical colors during the church year! Examples of such multiple possible influences are so easy to generate, I shall resist the temptation to go on. If somebody asks a psychologist or sociologist whether she might expect a nonzero correlation between dental caries and IQ, the best guess would be yes, small but statistically-significant. A small negative correlation was in fact found during the 1920s, misleading some hygienists to hold that IQ was lowered by toxins from decayed teeth. (The received explanation today is that dental caries and IQ are both correlates of social class.) More than 75 years ago, Edward Lee Thorndike enunciated the famous dictum, “All good things tend to go together, as do all bad ones.” Almost all human performance (work competence) dispositions, if carefully studied, are saturated to some extent with the general intelligence factor <em>g</em>, which for psychodynamic and ideological reasons has been somewhat neglected in recent years but is due for a comeback (Betz, <span>1986<sub><span title="1986 was 39 years ago.">39ya</span></sub></span>).<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>The ubiquity of nonzero correlations gives rise to what is methodologically disturbing to the theory tester and what I call, following Lykken, the crud factor. I have discussed this at length elsewhere (Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c), so I only summarize and provide a couple of examples here. The main point is that, when the sample size is sufficiently large to produce accurate estimates of the population values, almost any pair of variables in psychology will be correlated to some extent. Thus, for instance, less than 10% of the items in the MMPI item pool were put into the pool with masculinity-femininity in mind, and the empirically derived <em>Mf</em><span> scale contains only some of those plus others put into the item pool for other reasons, or without any theoretical considerations. When one samples thousands of individuals, it turns out that only 43 of the 550 items (8%) fail to show a significant difference between males and females. In an unpublished study (but see Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c) of the hobbies, interests, vocational plans, school course preferences, social life, and home factors of Minnesota college freshmen, when Lykken and I ran chi squares on all possible pairwise combinations of variables, 92% were significant, and 78% were significant at </span><em>p</em> &lt; 10<sup>−6</sup>. Looked at another way, the median number of significant relationships between a given variable and all the others was 41 of a possible 44. One finds such oddities as a relationship between which kind of shop courses boys preferred in high school and which of several Lutheran synods they belonged to!</p>
<p>…The third objection is somewhat harder to answer because it would require an encyclopedic survey of research literature over many domains. It is argued that, although the crud factor is admittedly ubiquitous—that is, almost no correlations of the social sciences are literally zero (as required by the usual significance test)—the crud factor is in most research domains not large enough to be worth worrying about. Without making a claim to know just how big it is, I think this objection is pretty clearly unsound. Doubtless the average correlation of any randomly picked pair of variables in social science depends on the domain, and also on the instruments employed (eg. it is well known that personality inventories often have as much methods-covariance as they do criterion validities).</p>
<p>A representative pairwise correlation among MMPI scales, despite the marked differences (sometimes amounting to phenomenological “oppositeness”) of the nosological rubrics on which they were derived, is in the middle to high 0.30s, in both normal and abnormal populations. The same is true for the occupational keys of the <a href="https://en.wikipedia.org/wiki/Strong_Interest_Inventory" id="_Or9gFIDD" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Strong_Interest_Inventory#bodyContent" title="Strong Interest Inventory">Strong Vocational Interest Bank</a>. Deliberately aiming to diversify the qualitative features of cognitive tasks (and thus “purify” the measures) in his classic studies of primary mental abilities (“pure factors”, orthogonal), Thurstone (<span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>; <a href="https://gwern.net/doc/iq/1941-thurstone-factorialstudiesofintelligence.pdf" id="thurstone-thurstone-1941" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="7114649" data-filesize-percentage="94" title="_Factorial Studies of Intelligence_">Thurstone &amp; Thurstone, 1941</a>) still found an average intertest correlation of .28 (range = 0.01 to .56!) in the cross-validation sample. In the set of 20 <a href="https://en.wikipedia.org/wiki/California_Psychological_Inventory" id="_7g_r5857" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/California_Psychological_Inventory#bodyContent" title="California Psychological Inventory">California Psychological Inventory (CPI)</a> scales built to cover broadly the domain of (normal range) “folk-concept”<span> traits, Gough (<span>1987<sub><span title="1987 was 38 years ago.">38ya</span></sub></span>) found an average pairwise correlation of .44 among both males and females. Guilford’s Social </span><a href="https://en.wikipedia.org/wiki/Extraversion_and_introversion" id="_HAZaloBe" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Extraversion_and_introversion#bodyContent" title="Extraversion and introversion">Introversion</a>, Thinking Introversion, Depression, Cycloid Tendencies, and Rhathymia or Freedom From Care scales, constructed on the basis of (orthogonal) factors, showed pairwise correlations ranging from -.02 to .85, with 5 of the 10 <em>r</em>s ≥ 0.33 despite the purification effort (<a href="https://gwern.net/doc/psychology/personality/1941-evans.pdf" id="evans-mcconnell-1941" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="633152" data-filesize-percentage="64" title="A new measure of introversion-extroversion">Evans &amp; McConnell, 1941</a>). Any treatise on factor analysis exemplifying procedures with empirical data suffices to make the point convincingly. For example, in <a href="https://archive.org/details/ModernFactorAnalysis" id="_EkOsaEjc" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/ModernFactorAnalysis?view=theater" title="_Modern Factor Analysis_">Harman (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>)</a>, 8 “emotional” variables correlate .10 to .87, median <em>r</em>= 0.44 (p.&nbsp;176), and 8 “political” variables correlate .03 to .88, median (absolute value) <em>r</em> = 0.62 (p.&nbsp;178). For highly diverse acquiescence-corrected measures (personality traits, interests, hobbies, psychopathology, social attitudes, and religious, political, and moral opinions), estimating individuals’ (orthogonal!) factor scores, one can hold mean <em>r</em>s down to an average of .12, means .04–.20, still some individual <em>r</em><span>s &gt; 0.30 (Lykken, personal communication, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>; cf. McClosky &amp; Meehl, in preparation). Public opinion polls and attitude surveys routinely disaggregate data with respect to several demographic variables (eg. age, education, section of country, sex, ethnicity, religion, education, income, rural/urban, self-described political affiliation) because these factors are always correlated with attitudes or electoral choices, sometimes strongly so. One must also keep in mind that socioeconomic status, although intrinsically interesting (especially to sociologists) is probably often functioning as a proxy for other unmeasured personality or status characteristics that are not part of the definition of social class but are, for a variety of complicated reasons, correlated with it. The proxy role is important because it prevents adequate </span>“controlling for” unknown (or unmeasured) crud-factor influences by statistical procedures (matching, partial correlation, analysis of covariance, <a href="https://en.wikipedia.org/wiki/Path_analysis_(statistics)" id="_sNgHsjac" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Path_analysis_(statistics)#bodyContent" title="Path analysis (statistics)">path analysis</a>). [ie <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">“residual confounding”</a>]</p>
<ul>
<li><p>Thurstone, L. L. (<span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>). <em>Primary mental abilities</em>. Chicago: University of Chicago Press. <!-- TODO: pricewatched --></p></li>
<li><p>Gough, H. G. (<span>1987<sub><span title="1987 was 38 years ago.">38ya</span></sub></span>). <em>CPI, Administrator’s guide</em>. Palo Alto, CA: Consulting Psychologists Press.</p></li>
<li><p>McClosky, Herbert, &amp; Meehl, P. E. (in preparation). <em>Ideologies in conflict</em>.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a></p></li>
</ul>
</blockquote>
</section>
<section id="tukey-1991">
<h2><a href="#tukey-1991" title="Link to section: § 'Tukey1991'"><span><span>Tukey</span><span>1991</span></span></a></h2>
<p><a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1177011945" id="_5qNrA51l" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“The philosophy of multiple comparisons”</a>, <span><span>Tukey</span><span>1991</span></span>:</p>
<blockquote>
<p>Statisticians classically asked the wrong question—and were willing to answer with a lie, one that was often a downright lie. They asked “Are the effects of A and B different?” and they were willing to answer “no”.</p>
<p>All we know about the world teaches us that the effects of A and B are always different—in some decimal place—for any A and B. Thus asking “Are the effects different?” is foolish.</p>
<p>What we should be answering first is “Can we tell the direction in which the effects differ from the effects of B?” In other words, can we be confident about the direction from A to B? Is it “up”, “down”, or “uncertain”?</p>
</blockquote>
</section>
<section id="raftery-1995">
<h2><a href="#raftery-1995" title="Link to section: § 'Raftery1995'"><span><span>Raftery</span><span>1995</span></span></a></h2>
<p><a href="https://wwwlegacy.stat.washington.edu/research/online/1994/bic.ps" id="_5ObbZvwb">“Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)”</a>, <span><span>Raftery</span><span>1995</span></span>:</p>
<blockquote>
<p>In the past 15 years, however, some quantitative sociologists have been attaching less importance to <em>p</em>-values because of practical difficulties and counter-intuitive results. These difficulties are most apparent with large samples, where <em>p</em>-values tend to indicate rejection of the null hypothesis even when the null model seems reasonable theoretically and inspection of the data fails to reveal any striking discrepancies with it. Because much sociological research is based on survey data, often with thousands of cases, sociologists frequently come up against this problem. In the early 1980s, some sociologists dealt with this problem by ignoring the results of <em>p</em>-value-based tests when they seemed counter-intuitive, and by basing model selection instead on theoretical considerations and informal assessment of discrepancies between model and data (eg. Fienberg and Mason, <span>1979<sub><span title="1979 was 46 years ago.">46ya</span></sub></span>; Hout, <span>1983<sub><span title="1983 was 42 years ago.">42ya</span></sub></span>, <span>1984<sub><span title="1984 was 41 years ago.">41ya</span></sub></span>; Grusky and Hauser, <span>1984<sub><span title="1984 was 41 years ago.">41ya</span></sub></span>).</p>
<p>…It is clear that models 1 and 2 are unsatisfactory and should be rejected in favor of model 3.<sup>3</sup> By the standard test, model 3 should also be rejected, in favor of model 4, given the deviance difference of 150 on 16 degrees of freedom, corresponding to a <em>p</em>-value of about 10<sup>−120</sup> . <span><span>Grusky &amp; Hauser</span><span>1984</span></span> nevertheless adopted model 3 because it explains most (99.7%) of the deviance under the baseline model of independence, fits well in the sense that the differences between observed and expected counts are a small proportion of the total, and makes good theoretical sense. This seems sensible, and yet is in dramatic conflict with the <em>p</em>-value-based test. This type of conflict often arises in large samples, and hence is frequent in sociology with its survey data sets comprising thousands of cases. The main response to it has been to claim that there is a distinction between “statistical” and “substantive” significance, with differences that are statistically-significant not necessarily being substantively important.</p>
</blockquote>
</section>
<section id="thompson-1995">
<h2><a href="#thompson-1995" title="Link to section: § 'Thompson1995'"><span><span>Thompson</span><span>1995</span></span></a></h2>
<p><a href="https://files.eric.ed.gov/fulltext/ED392819.pdf#page=9" id="_71HtzEaV" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Editorial Policies Regarding Statistical-Significance Testing: 3 Suggested Reforms”</a>, <span><span>Thompson</span><span>1995</span></span>:</p>
<blockquote>
<p>One serious problem with this statistical testing logic is that the in reality <em>H</em><sub>0</sub> is never true in the population, as recognized by any number of prominent statisticians (<a href="#tukey-1991">Tukey, 1991</a>), ie. there will always be some differences in population parameters, although the differences may be incredibly trivial. Near 40 years ago Savage (<a href="#savage-1957">1957</a>, pp.&nbsp;332–333) noted that, “Null hypotheses of no difference are usually known to be false before the data are collected.” Subsequently, Meehl (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>, p.822) argued, “As I believe is generally recognized by statisticians today and by thoughtful social scientists, the null hypothesis, taken literally, is always false.” Similarly, noted statistician <a href="#hays-1973">Hays</a><span> (<span>1981<sub><span title="1981 was 44 years ago.">44ya</span></sub></span>, p.&nbsp;293 [</span><em>Statistics</em>], 3<sup>rd</sup> ed.) pointed out that “[t]here is surely nothing on earth that is completely independent of anything else. The strength of association may approach zero, but it should seldom or never be exactly zero.” And <a href="#loftus-loftus-1982">Loftus and Loftus</a><span> (<span>1982<sub><span title="1982 was 43 years ago.">43ya</span></sub></span>, pp.&nbsp;498–499) argued that, </span>“finding a ‘[statistically] significant effect’ really provides very little information, because it’s almost certain that some relationship (however small) exists between any two variables.” The very important implication of all this is that statistical-significance testing primarily becomes only a test of researcher endurance, because “virtually any study can be made to show [statistically] significant results if one uses enough subjects”<span> (Hays, <span>1981<sub><span title="1981 was 44 years ago.">44ya</span></sub></span>, p.&nbsp;293). As </span><a href="#nunnally-1960">Nunnally</a><span> (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>, p.&nbsp;643) noted some 35 years ago, </span>“If the null hypothesis is not rejected, it is usually because the <em>N</em> is too small. If enough data are gathered, the hypothesis will generally be rejected.” The implication is that:</p>
<blockquote>
<p>Statistical-significance testing can involve a tautological logic in which tired researchers, having collected data from hundreds of subjects, then conduct a statistical test to evaluate whether there were a lot of subjects, which the researchers already know, because they collected the data and know they’re tired. This tautology has created considerable damage as regards the cumulation of knowledge… (<a href="https://gwern.net/doc/psychology/1992-thompson.pdf" id="thompson-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="395049" data-filesize-percentage="52" title="Two and One-Half Decades of Leadership in Measurement and Evaluation">Thompson, 1992</a>, p.&nbsp;436)</p>
</blockquote>
</blockquote>
</section>
<section id="mulaik-et-al-1997">
<h2><a href="#mulaik-et-al-1997" title="Link to section: § 'Mulaik et al 1997'"><span><span title="et al">Mulaik</span><span> Et Al </span><span>1997</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1997-muzaik.pdf" id="mulaik-et-al-1997" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2857133" data-filesize-percentage="88" title="‘There Is a Time and a Place for Significance Testing’, Mulaik et al 1997">“There Is a Time and a Place for Statistical-Significance Testing”</a>, <span><span title="et al">Mulaik</span><span> et al </span><span>1997</span></span> (in <em>What If There Were No Significance Tests</em><span> ed <span><span title="et al">Harlow</span><span> et al </span><span>1997</span></span>):</span></p>
<blockquote>
<p>Most of these articles expose misconceptions about significance testing common among researchers and writers of psychological textbooks on statistics and measurement. But the criticisms do not stop with misconceptions about significance testing. Others like <a href="#meehl-1967"><span><span>Meehl</span><span>1967</span></span></a> expose the limitations of a statistical practice that focuses only on testing for zero differences between means and zero correlations instead of testing predictions about specific nonzero values for parameters derived from theory or prior experience, as is done in the physical sciences. Still others emphasize that significance tests do not alone convey the information needed to properly evaluate research findings and perform accumulative research.</p>
<p>…Other than emphasizing a need to properly understand the interpretation of <a href="https://en.wikipedia.org/wiki/Confidence_interval" id="_i_2OI0BT" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Confidence_interval#bodyContent" title="Confidence interval">confidence intervals</a>, we have no disagreements with these criticisms and proposals. But a few of the critics go even further. In this chapter we will look at arguments made by <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.780&amp;rep=rep1&amp;type=pdf" id="_9h6u89Tu" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" title="The Case Against Statistical-Significance Testing">Carver (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>)</a>, <a href="#cohen-1994">Cohen (<span>1994<sub><span title="1994 was 31 years ago.">31ya</span></sub></span>)</a>, <a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" id="rozeboom-1960" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" data-url-original="https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf" data-filesize-bytes="653742" data-filesize-percentage="40" title="The fallacy of the null-hypothesis significance test">Rozeboom (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>)</a>, Schmidt (<a href="https://gwern.net/doc/statistics/meta-analysis/1992-schmidt.pdf" id="_2WeqSE4u" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1070079" data-filesize-percentage="75" title="What Do Data Really Mean? Research Findings, Meta-Analysis, and Cumulative Knowledge in Psychology">1992</a>, <span>1996<sub><span title="1996 was 29 years ago.">29ya</span></sub></span>), and Schmidt and Hunter (chapter 3 of this volume), in favor of not merely recommending the reporting of point estimates of <a href="https://en.wikipedia.org/wiki/Effect_size" id="_7H3QCJ9x" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Effect_size#bodyContent" title="Effect size">effect sizes</a><span> and confidence intervals based on them, but of abandoning altogether the use of significance tests in research. Our focus will be principally on Schmidt’s (<span>1992<sub><span title="1992 was 33 years ago.">33ya</span></sub></span>, <span>1996<sub><span title="1996 was 29 years ago.">29ya</span></sub></span>) papers, because they incorporate arguments from earlier papers, especially Carver’s (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>), and also carry the argument to its most extreme conclusions. Where appropriate, we will also comment on Schmidt and Hunter’s (chapter 3 of this volume) rebuttal of arguments against their position.</span></p>
<p><strong>The Null Hypothesis Is Always False?</strong></p>
<p>Cohen (<span>1994<sub><span title="1994 was 31 years ago.">31ya</span></sub></span>), influenced by Meehl (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>), argued that “the nil hypothesis is always false”<span> (p. 1000). Get a large enough sample and you will always reject the null hypothesis. He cites a number of eminent statisticians in support of this view. He quotes Tukey (<span>1991<sub><span title="1991 was 34 years ago.">34ya</span></sub></span>, p. 100) to the effect that there are always differences between experimental treatments-for some decimal places. Cohen cites an unpublished study by Meehl and Lykken in which cross tabulations for 15 Minnesota Multiphasic Personality Inventory (MMPI) items for a sample of 57,000 subjects yielded 105 chi-square tests of association and every one of them was significant, and 96% of them were significant at </span><em>p</em><span> &lt; 0.000001 (Cohen, 1994, p.&nbsp;1000). Cohen cites Meehl (<span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>) as suggesting that this reflects a </span>“crud factor” in nature. “Everything is related to everything else” to some degree. So, the question is, why do a significance test if you know it will always be significant if the sample is large enough? But if this is an empirical hypothesis, is it not one that is established using significance testing?</p>
<p>But the example may not be an apt demonstration of the principle Cohen sought to establish: It is generally expected that responses to different items responded to by the same subjects are not independently distributed across subjects, so it would not be remarkable to find significant correlations between many such items.</p>
<p>Much more interesting would be to demonstrate systematic and replicable significant treatment effects when subjects are assigned at random to different treatment groups but the <em>same</em> treatments are administered to each group. But in this case, small but significant effects in studies with high power that deviate from expectations of no effect when no differences in treatments are administered are routinely treated as systematic experimenter errors, and knowledge of experimental technique is improved by their detection and removal or control. Systematic error and experimental artifact must always be considered a possibility when rejecting the null hypothesis. Nevertheless, do we know a priori that a test will <em>always</em> be significant if the sample is large enough? Is the proposition “Every statistical hypothesis is false” an <em>axiom</em> that needs no testing? Actually, we believe that to regard this as an axiom would introduce an internal contradiction into statistical reasoning, comparable to arguing that all propositions and descriptions are false. You could not think and reason about the world with such an axiom. So it seems preferable to regard this as some kind of empirical generalization. But no empirical generalization is ever incorrigible and beyond testing. Nevertheless, if indeed there is a phenomenon of nature known as “the crud factor”, then it is something we know to be objectively a fact only because of significance tests. Something in the background noise stands out as a signal against that noise, because we have sufficiently powerful tests using huge samples to detect it. At that point it may become a challenge to science to develop a better understanding of what produces it. However, it may tum out to reflect only experimenter artifact. But in any case the hypothesis of a crud factor is not beyond further testing.</p>
<p>The point is that it doesn’t matter if the null hypothesis is always judged false at some sample size, as long as we regard this as an empirical phenomenon. What matters is whether <em>at the sample size we have</em> we can distinguish observed deviations from our hypothesized values to be sufficiently large and improbable under a hypothesis of chance that we can treat them reasonably but provisionally as not due to chance error. There is no a priori reason to believe that one will always reject the null hypothesis at any given sample size. On the other hand, accepting the null hypothesis does not mean the hypothesized value is true, but rather that the evidence observed is not distinguishable from what we would regard as due to chance if the null hypothesis were true and thus is not sufficient to disprove it. The remaining uncertainty regarding the truth of our null hypothesis is measured by the width of the region of acceptance or a function of the standard error. And this will be closely related to the power of the test, which also provides us with information about our uncertainty. The fact that the width of the region of acceptance shrinks with increasing sample size, means we are able to reduce our uncertainty regarding the provisional validity of an accepted null hypothesis with larger samples. In huge samples the issue of uncertainty due to chance looms not as important as it does in small- and moderate-size samples.</p>
</blockquote>
</section>
<section id="waller-2004">
<h2><a href="#waller-2004" title="Link to section: § 'Waller2004'"><span><span>Waller</span><span>2004</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www3.nd.edu/c3eda49a209b2c858b278a543e7c1b08c086e578.pdf" id="_90Zj0N8r" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www3.nd.edu/c3eda49a209b2c858b278a543e7c1b08c086e578.pdf" data-url-original="https://www3.nd.edu/~ghaeffel/Waller2004%20Applied%20&amp;%20Preventive%20Psychology.pdf" data-filesize-bytes="93599" data-filesize-percentage="6">“The fallacy of the null hypothesis in soft psychology”</a>, <span><span>Waller</span><span>2004</span></span>:</p>
<blockquote>
<p>In his classic article on the fallacy of the null hypothesis in soft psychology, <a href="#meehl-1978">Paul Meehl</a> claimed that, in nonexperimental settings, the probability of rejecting the null hypothesis of nil group differences in favor of a directional alternative was 0.50—a value that is an order of magnitude higher than the customary Type I error rate. In a series of real data simulations, using Minnesota Multiphasic Personality Inventory-Revised (MMPI-2) data collected from more than 80,000 individuals, I found strong support for Meehl’s claim.</p>
<p>…Before running the experiments I realized that, to be fair to Meehl, I needed a large data set with a broad range of biosocial variables. Fortunately, I had access to data from 81,485 individuals who earlier had completed the 567 items of the Minnesota Multiphasic Personality Inventory-Revised (MMPI-2; Butcher, Dahlstrom, Graham, Tellegen, &amp; Kaemmer, <span>1989<sub><span title="1989 was 36 years ago.">36ya</span></sub></span>). The MMPI-2, in my opinion, is an ideal vehicle for testing Meehl’s claim because it includes items in such varied content domains as general health concerns; personal habits and interests; attitudes towards sex, marriage, and family; affective functioning; normal range personality; and extreme manifestations of psychopathology (for a more complete description of the latent content of the MMPI, see Waller, <span>1999<sub><span title="1999 was 26 years ago.">26ya</span></sub></span>, “Searching for structure in the MMPI”).</p>
<p>…Next, the computer selected (without replacement) a random item from the pool of MMPI-2 items. Using data from the 41,491 males and 39,994 females, it then (1) performed a difference of proportions test on the item group means; (2) recorded the signed <em>z</em>-value; and (3) recorded the associated significance level. Finally, the program tallied the number of “significant” test results (ie. those with |<em>z</em>|≥1.96). The results of this mini simulation were enlightening and in excellent accord with the outcome of Meehl’s <em>gedanken</em> experiment. Specifically, 46% of the directional hypotheses were supported at significance levels that far exceeded traditional <em>p</em>-value cutoffs. A summary of the results is portrayed in Fig. 1. Notice in this figure, which displays the distribution of <em>z</em>-values for the 511 tests, that many of the item mean differences were 50–100 times larger than their associated standard errors!</p>
<figure>
<p><img alt="Figure 1: Distribution of z -values for 511 hypothesis tests." data-aspect-ratio="1031 / 775" decoding="async" height="775" loading="lazy" src="https://gwern.net/doc/statistics/2004-waller-figure1.png" width="1031"></p>
<figcaption><p><strong>Figure 1</strong>: Distribution of <em>z</em>-values for 511 hypothesis tests.</p></figcaption>
</figure>
<figure>
<p><img alt="Figure 2: Distribution of the frequency of rejected null hypotheses, in favor of a randomly chosen directional alternative, in 320,922 hypothesis test." data-aspect-ratio="959 / 832" decoding="async" height="832" loading="lazy" src="https://gwern.net/doc/statistics/2004-waller-figure2.png" width="959"></p>
<figcaption><p><strong>Figure 2</strong>: Distribution of the frequency of rejected null hypotheses, in favor of a randomly chosen directional alternative, in 320,922 hypothesis test.</p></figcaption>
</figure>
</blockquote>
<p>Waller also highlights Bill <span><span>Thompson’s</span><span>2001</span></span> bibliography <a href="https://gwern.net/doc/statistics/causality/2001-thompson.html" id="_BHa3BLOQ" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="76213" data-filesize-percentage="14">“402 Citations Questioning the Indiscriminate Use of Null Hypothesis Significance Tests in Observational Studies”</a> as a source for criticisms of NHST but unfortunately it’s unclear which of them might bear on the specific criticism of ‘the null hypothesis is always false’.</p>
</section>
<section id="kilgarriff-2005">
<h2><a href="#kilgarriff-2005" title="Link to section: § 'Kilgarriff2005'"><span><span>Kilgarriff</span><span>2005</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/linguistics/2005-kilgarriff.pdf" id="kilgarriff-2005" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="80665" data-filesize-percentage="15" title="‘Language is never, ever, ever, random’, Kilgarriff 2005">“Language is never, ever, ever, random”, <span><span>Kilgarriff</span><span>2005</span></span></a></p>
</section>
<section id="starbuck-2006">
<h2><a href="#starbuck-2006" title="Link to section: § 'Starbuck2006'"><span><span>Starbuck</span><span>2006</span></span></a></h2>
<p><a href="https://www.amazon.com/Production-Knowledge-Challenge-Science-Research/dp/0199288534" id="_UI1YVajl" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>The Production of Knowledge: The Challenge of Social Science Research</em></a>, <span><span>Starbuck</span><span>2006</span></span>, pg47–49:</p>
<blockquote>
<p>Induction requires distinguishing meaningful relationships (signals) in the midst of an obscuring background of <a href="https://en.wikipedia.org/wiki/Confounding" id="_L43L1NuO" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Confounding#bodyContent" title="Confounding">confounding</a> relationships (noise). The weak and meaningless or substantively secondary correlations in the background make induction untrustworthy. In many tasks, people can distinguish weak signals against rather strong background noise. The reason is that both the signals and the background noise match familiar patterns. For example, a driver traveling to a familiar destination focuses on landmarks that experience has shown to be relevant. People have trouble making such distinctions where signals and noise look much alike or where signals and noise have unfamiliar characteristics. For example, a driver traveling a new road to a new destination is likely to have difficulty spotting landmarks and turns on a recommended route.</p>
<p>Social science research has the latter characteristics. This activity is called research because its outputs are unknown; and the signals and noise look a lot alike in that both have systematic components and both contain components that vary erratically. Therefore, researchers rely upon statistical techniques to distinguish signals from noise. However, these techniques assume: (1) that the so-called random errors really do cancel each other out so that their average values are close to zero; and (2) that the so-called random errors in different variables are uncorrelated. These are very strong assumptions because they presume that the researchers’ hypotheses encompass absolutely all of the systematic effects in the data, including effects that the researchers have not foreseen or measured. When these assumptions are not met, the statistical techniques tend to mistake noise for signal, and to attribute more importance to the researchers’ hypotheses than they deserve.</p>
<p>I remembered what <a href="https://gwern.net/doc/economics/1961-ames.pdf" id="ames-reiter-1961" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1617916" data-filesize-percentage="82" title="Distributions of correlation coefficients in economic time series"><span><span>Ames &amp; Reiter</span><span>1961</span></span></a> had said about how easy it is for macroeconomists to discover statistically-significant correlations that have no substantive significance, and I could see 5 reasons why a similar phenomenon might occur with cross-sectional data. Firstly, a few broad characteristics of people and social systems pervade social science data—examples being sex, age, intelligence, social class, income, education, or organization size. Such characteristics correlate with many behaviors and with each other. Secondly, researchers’ decisions about how to treat data can create correlations between variables. For example, when the Aston researchers used factor analysis to create aggregate variables, they implicitly determined the correlations among these aggregate variables. Thirdly, so-called ‘samples’ are frequently not random, and many of them are complete subpopulations—say, every employee of a company—even though study after study has turned up evidence that people who live close together, who work together, or who socialize together tend to have more attitudes, beliefs, and behaviors in common than do people who are far apart physically and socially. Fourthly, some studies obtain data from respondents at one time and through one method. By including items in a single questionnaire or interview, researchers suggest to respondents that relationships exist among these items. Lastly, most researchers are intelligent people who are living successful lives. They are likely to have some intuitive ability to predict the behaviors of people and of social systems. They are much more likely to formulate hypotheses that accord with their intuition than ones that violate it; they are quite likely to investigate correlations and differences that deviate from zero; and they are less likely than chance would imply to observe correlations and differences near zero.</p>
<p><a href="https://gwern.net/doc/economics/1988-webster.html" id="_9Ox1oap9" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="723500" data-filesize-percentage="67" title="Theory Building In Industrial And Organizational Psychology">Webster and I hypothesized</a> that statistical tests with a null hypothesis of no correlation are biased toward statistical-significance. Webster culled through <em>Administrative Science Quarterly</em>, the <em>Academy of Management Journal</em>, and the <em>Journal of Applied Psychology</em> seeking matrices of correlations. She tabulated only complete matrices of correlations in order to observe the relations among all of the variables that the researchers perceived when drawing inductive inferences, not only those variables that researchers actually included in hypotheses. Of course, some researchers probably gathered data on additional variables beyond those published, and then omitted these additional variables because they correlated very weakly with the dependent variables. We estimated that 64% of the correlations in our data were associated with researchers’ hypotheses.</p>
<figure>
<p><img alt="Figure 2.6: Correlations reported in 3 journals" data-aspect-ratio="1400 / 869" decoding="async" height="869" loading="lazy" src="https://gwern.net/doc/statistics/2006-starbuck-websterstarbuck1988-figure26-managementsciencecorrelations.jpg" width="1400"></p>
<figcaption><p><strong>Figure 2.6</strong>: Correlations reported in 3 journals</p></figcaption>
</figure>
<p>Figure 2.6 shows the distributions of 14,897 correlations. In all 3 journals, both the mean correlation and the median correlation were close to +0.09 and the distributions of correlations were very similar. Finding significant correlations is absurdly easy in this population of variables, especially when researchers make two-tailed tests with a null hypothesis of no correlation. Choosing two variables utterly at random, a researcher has 2-to-1 odds of finding a significant correlation on the first try, and 24-to-1 odds of finding a significant correlation within 3 tries (also see <a href="https://gwern.net/doc/www/web-archive.southampton.ac.uk/a9470a09988d6f45ceeb702b8b4e729433320617.pdf" id="_Cg0DLM91" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/web-archive.southampton.ac.uk/a9470a09988d6f45ceeb702b8b4e729433320617.pdf" data-url-original="https://web-archive.southampton.ac.uk/cogprints.org/5178/1/are_null_results_becoming.pdf" data-filesize-bytes="32340" data-filesize-percentage="2" title="Are Null Results Becoming an Endangered Species in Marketing?">Hubbard and <span><span>Armstrong</span><span>1992</span></span></a>). Furthermore, the odds are better than 2-to-1 that an observed correlation will be positive, and positive correlations are more likely than negative ones to be statistically-significant. Because researchers gather more data when they are getting small correlations, studies with large numbers of observations exhibit slightly less positive bias. The mean correlation in studies with fewer than 70 observations is about twice the mean correlation in studies with over 180 observations. The main inference I drew from these statistics was that the social sciences are drowning in statistically-significant but meaningless noise. Because the differences and correlations that social scientists test have distributions quite different from those assumed in hypothesis tests, social scientists are using tests that assign statistical-significance to confounding background relationships. Because social scientists equate statistical-significance with meaningful relationships, they often mistake confounding background relationships for theoretically important information. One result is that social science research creates a cloud of statistically-significant differences and correlations that not only have no real meaning but also impede scientific progress by obscuring the truly meaningful relationships.</p>
<p>Suppose that roughly 10% of all observable relations could be theoretically meaningful and that the remaining 90% either have no meanings or can be deduced as implications of the key 10%. However, we do not know now which relations constitute the key 10%, and so our research resembles a search through a haystack in which we are trying to separate needles from more numerous straws. Now suppose that we adopt a search method that makes almost every straw look very much like a needle and that turns up thousands of apparent needles annually; 90% of these apparent needles are actually straws, but we have no way of knowing which ones. Next, we fabricate a theory that ‘explains’ these apparent needles. Some of the propositions in our theory are likely to be correct, merely by chance; but many, many more propositions are incorrect or misleading in that they describe straws. Even if this theory were to account rationally for all of the needles that we have supposedly discovered in the past, which is extremely unlikely, the theory has very little chance of making highly accurate predictions about the consequences of our actions unless the theory itself acts as a powerful self-fulfilling prophecy (Eden and <span><span>Ravid</span><span>1982</span></span>). Our theory would make some correct predictions, of course, because with so many correlated variables, even a completely false theory would have a reasonable chance of generating predictions that come true. Thus, we dare not even take correct predictions as dependable evidence of our theory’s correctness (<a href="https://archive.org/details/psychologyasscie0000dees" id="_Fy_hLQfz" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/psychologyasscie0000dees?view=theater" title="<em>Psychology as science and art</em>"><span><span>Deese</span><span>1972</span></span>: 61–67</a> [<em>Psychology as Science and Art</em>]).</p>
</blockquote>
</section>
<section id="smith-et-al-2007">
<h2><a href="#smith-et-al-2007" title="Link to section: § 'Smith et al 2007'"><span><span title="et al">Smith</span><span> Et Al </span><span>2007</span></span></a></h2>
<p><a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352" id="smith-et-al-2007-link" data-link-icon="plos" data-link-icon-type="svg" title="'Clustered Environments and Randomized Genes: A Fundamental Distinction between Conventional and Genetic Epidemiology', Smith et al 2007">“Clustered Environments and Randomized Genes: A Fundamental Distinction between Conventional and Genetic Epidemiology”</a>, <span><span title="et al">Smith</span><span> et al </span><span>2007</span></span>:</p>
<blockquote>
<p>…We examined the extent to which genetic variants, on the one hand, and nongenetic environmental exposures or phenotypic characteristics on the other, tend to be associated with each other, to assess the degree of confounding that would exist in conventional epidemiological studies compared with <a href="https://en.wikipedia.org/wiki/Mendelian_randomization" id="_eB48Cm87" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Mendelian_randomization#bodyContent" title="Mendelian Randomization">Mendelian Randomization</a> studies. <em>Methods &amp; Findings</em>: We estimated pairwise correlations between [96] nongenetic baseline variables and genetic variables in a cross-sectional study [British Women’s Heart and Health Study; <em>n</em> = 4,286] comparing the number of correlations that were statistically-significant at the 5%, 1%, and 0.01% level (α = 0.05, 0.01, and 0.0001, respectively) with the number expected by chance if all variables were in fact uncorrelated, using a two-sided binomial exact test. We demonstrate that behavioral, socioeconomic, and physiological factors are strongly interrelated, with 45% of all possible pairwise associations between 96 nongenetic characteristics (<em>n</em> = 4,560 correlations) being significant at the <em>p</em> &lt; 0.01 level (the ratio of observed to expected significant associations was 45; <em>p</em>-value for difference between observed and expected &lt; 0.000001). Similar findings were observed for other levels of significance.</p>
<p>…The 96 nongenetic variables generated 4,560 pairwise comparisons, of which, assuming no associations existed, 5 in 100 (total 228) would be expected to be associated by chance at the 5% significance level (α = 0.05). However, 2,447 (54%) of the correlations were significant at the α = 0.05 level, giving an observed to expected (O:E) ratio of 11, <em>p</em> for difference O:E &lt; 0.000001 (Table 1). At the 1% significance level, 45.6 of the correlations would be expected to be associated by chance, but we found that 2,036 (45%) of the pairwise associations were statistically-significant at α = 0.01, giving an O:E ratio of 45, <em>p</em> for difference O:E &lt; 0.000001 (Table 2). At the 0.01% significance level, 0.456 of the correlations would be expected to be associated by chance, but we found that 1,378 (30%) were statistically-significantly associated at α = 0.0001, giving an O:E ratio of 3,022, <em>p</em> for difference O:E &lt; 0.000001.</p>
<p>…Over 50% of the pairwise associations between baseline nongenetic characteristics in our study were statistically-significant at the 0.05 level; an 11-fold increase from what would be expected, assuming these characteristics were independent. Similar findings were found for statistically-significant associations at the 0.01 level (45-fold increase from expected) and the 0.0001 level (3,000-fold increase from expected). This illustrates the considerable difficulty of determining which associations are valid and potentially causal from a background of highly correlated factors, reflecting that behavioral, socioeconomic, and physiological characteristics tend to cluster. This tendency will mean that there will often be high levels of confounding when studying any single factor in relation to an outcome. Given the complexity of such confounding, even after formal statistical adjustment, a lack of data for some confounders, and measurement error in assessed confounders will leave considerable scope for <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">residual confounding</a> <a href="https://gwern.net/doc/statistics/causality/1992-phillips.pdf" id="phillips-smith-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="589586" data-filesize-percentage="62" title="'Bias in relative odds estimation owing to imprecise measurement of correlated exposures', Phillips &amp; Smith 1992">[4]</a>. When epidemiological studies present adjusted associations as a reflection of the magnitude of a causal association, they are assuming that all possible confounding factors have been accurately measured and that their relationships with the outcome have been appropriately modelled. We think this is unlikely to be the case in most observational epidemiological studies <a href="https://gwern.net/doc/statistics/causality/1991-phillips.pdf" id="phillips-smith-1991" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1044312" data-filesize-percentage="75" title="'How independent are `independent` effects? Relative risk estimation when correlated exposures are measured imprecisely', Phillips &amp; Smith 1991">[26]</a>.</p>
<p>Predictably, such confounded relationships will be particularly marked for highly socially and culturally patterned risk factors, such as dietary intake. This high degree of confounding might underlie the poor concordance of observational epidemiological studies that identified dietary factors (such as beta carotene, vitamin E, and vitamin C intake) as protective against cardiovascular disease and cancer, with the findings of <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial" id="_q1b5B-dQ" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Randomized_controlled_trial#bodyContent" title="Randomized controlled trial">randomized controlled trials</a> of these dietary factors [1,27]. Indeed, with 45% of the pairwise associations of nongenetic characteristics being “statistically-significant” at the <em>p</em> &lt; 0.01 level in our study, and our study being unexceptional with regard to the levels of confounding that will be found in observational investigations, it is clear that the large majority of associations that exist in observational databases will not reach publication. We suggest that those that do achieve publication will reflect apparent biological plausibility (a weak causal criterion <a href="https://gwern.net/doc/statistics/causality/1992-smith.pdf" id="smith-et-al-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="585231" data-filesize-percentage="62" title="'Smoking as `independent` risk factor for suicide: illustration of an artifact from observational epidemiology?', Smith et al 1992">[28]</a>) and the interests of investigators. Examples exist of investigators reporting provisional analyses in abstracts—such as antioxidant vitamin intake being apparently protective against future cardiovascular events in women with clinical evidence of cardiovascular disease [29]—but not going on to full publication of these findings, perhaps because randomized controlled trials appeared soon after the presentation of the abstracts [30] that rendered their findings as being unlikely to reflect causal relationships. Conversely, it is likely that the large majority of null findings will not achieve publication, unless they contradict high-profile prior findings, as has been demonstrated in molecular genetic research [31].</p>
<figure>
<p><img alt="Figure 1: Histogram of Statistically-Significant (at α = 1%) Age-Adjusted Pairwise Correlation Coefficients between 96 Nongenetic Characteristics. British Women Aged 60–79 y" data-aspect-ratio="1400 / 951" decoding="async" height="951" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2007-smith-figure1-correlationdistribution.jpg" title="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352#pmed-0040352-g001" width="1400"></p>
<figcaption><p><strong>Figure 1</strong>: Histogram of Statistically-Significant (at α = 1%) Age-Adjusted Pairwise Correlation Coefficients between 96 Nongenetic Characteristics. British Women Aged 60–79 y</p></figcaption>
</figure>
<p>The magnitudes of most of the significant correlations between nongenetic characteristics were small (see Figure 1), with a median value at <em>p</em> ≤ 0.01 and <em>p</em> ≤ 0.05 of 0.08, and it might be considered that such weak associations are unlikely to be important sources of confounding. However, so many associated nongenetic variables, even with weak correlations, can present a very important potential for residual confounding. For example, we have previously demonstrated how 15 socioeconomic and behavioral risk factors, each with weak but statistically independent (at <em>p</em> ≤ 0.05) associations with both vitamin C levels and coronary heart disease (CHD), could together account for an apparent strong protective effect (odds ratio = 0.60 comparing top to bottom quarter of vitamin C distribution) of vitamin C on CHD (<a href="https://gwern.net/doc/www/www.thelancet.com/a74f7f8ed2c6f8eb6e9bb9b17d2fd765b549d550.html" id="lawlor-2004" data-link-icon="L" data-link-icon-type="text" data-link-icon-color="#004582" data-url-archive="/doc/www/www.thelancet.com/a74f7f8ed2c6f8eb6e9bb9b17d2fd765b549d550.html" data-url-original="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(04)16925-0/fulltext" data-filesize-bytes="5464992" data-filesize-percentage="84" title="'Observational versus randomized trial evidence', Lawlor et al 2004a">32</a> [see also <a href="https://gwern.net/doc/statistics/causality/2004-lawlor.pdf" id="_iw3VVuYg" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="87279" data-filesize-percentage="16" title="Those confounded vitamins: what can we learn from the differences between observational versus randomized trial evidence?"><span><span title="et al">Lawlor</span><span> et al </span><span>2004b</span></span></a>]).</p>
</blockquote>
</section>
<section id="hecht-moxley-2009">
<h2><a href="#hecht-moxley-2009" title="Link to section: § 'Hecht &amp; Moxley2009'"><span><span>Hecht &amp; Moxley</span><span>2009</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/brenthecht.com/0221a1daddb4bd1bd7760fa1bbd7dc30d9f56431.pdf" id="_weRAfeh6" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/brenthecht.com/0221a1daddb4bd1bd7760fa1bbd7dc30d9f56431.pdf" data-url-original="https://brenthecht.com/papers/cosit2009.pdf" data-filesize-bytes="704097" data-filesize-percentage="41">“Terabytes of Tobler: evaluating the first law in a massive, domain-neutral representation of world knowledge”</a>, <span><span>Hecht &amp; Moxley</span><span>2009</span></span>:</p>
<blockquote>
<p>The First Law of Geography states, “everything is related to everything else, but near things are more related than distant things.” Despite the fact that it is to a large degree what makes “spatial special”, the law has never been empirically evaluated on a large, domain-neutral representation of world knowledge. We address the gap in the literature about this critical idea by statistically examining the multitude of entities and relations between entities present across 22 different language editions of Wikipedia. We find that, at least according to the myriad authors of Wikipedia, the First Law is true to an overwhelming extent regardless of language-defined cultural domain.</p>
</blockquote>
</section>
<section id="andrew-gelman">
<h2><a href="#andrew-gelman" title="Link to section: § 'Andrew Gelman'">Andrew Gelman</a></h2>
<section id="gelman-2004">
<h2><a href="#gelman-2004" title="Link to section: § 'Gelman2004'"><span><span>Gelman</span><span>2004</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/" id="_1T2mSs3H" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“Type 1, type 2, type S, and type M errors”</a></p>
<blockquote>
<p>I’ve never in my professional life made a Type I error <em>or</em> a Type II error. But I’ve made lots of errors. How can this be?</p>
<p>A Type 1 error occurs only if the null hypothesis is true (typically if a certain parameter, or difference in parameters, equals zero). In the applications I’ve worked on, in social science and public health, I’ve never come across a null hypothesis that could actually be true, or a parameter that could actually be zero.</p>
</blockquote>
</section>
<section id="gelman-2007">
<h2><a href="#gelman-2007" title="Link to section: § 'Gelman2007'"><span><span>Gelman</span><span>2007</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2007/10/05/significance_te/" id="_fcoQuEiQ" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“Significance testing in economics: McCloskey, Ziliak, Hoover, and Siegler”</a>:</p>
<blockquote>
<p>I think that McCloskey and Ziliak, and also Hoover and Siegler, would agree with me that the null hypothesis of zero coefficient is essentially always false. (The paradigmatic example in economics is program evaluation, and I think that just about every program being seriously considered will have effects—positive for some people, negative for others—but not averaging to exactly zero in the population.) From this perspective, the point of hypothesis testing (or, for that matter, of confidence intervals) is not to assess the null hypothesis but to give a sense of the uncertainty in the inference. As Hoover and Siegler put it, “while the economic significance of the coefficient does not depend on the statistical-significance, our certainty about the accuracy of the measurement surely does. . . . Significance tests, properly used, are a tool for the assessment of signal strength and not measures of economic significance.” Certainly, I’d rather see an estimate with an assessment of statistical-significance than an estimate without such an assessment.</p>
</blockquote>
</section>
<section id="gelman-2010a">
<h2><a href="#gelman-2010a" title="Link to section: § 'Gelman2010a'"><span><span>Gelman</span><span>2010a</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/stat.columbia.edu/193cb31928dffa84cf2992bce1764921c63ace86.pdf" id="_mt2mUNNw" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/stat.columbia.edu/193cb31928dffa84cf2992bce1764921c63ace86.pdf" data-url-original="https://stat.columbia.edu/~gelman/research/published/gelman_discussion_of_efron.pdf" data-filesize-bytes="295353" data-filesize-percentage="21">“Bayesian Statistics Then and Now”</a>, <span><span>Gelman</span><span>2010a</span></span>:</p>
<blockquote>
<p>My third meta-principle is that <em>different applications demand different philosophies</em>. This principle comes up for me in Efron’s discussion of hypothesis testing and the so-called false discovery rate, which I label as “so-called” for the following reason. In Efron’s formulation (which follows the classical <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem" id="_r85cb20T" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Multiple_comparisons_problem#bodyContent" title="Multiple comparisons problem">multiple comparisons</a> literature), a “false discovery” is a zero effect that is identified as nonzero, whereas, in my own work, I never study zero effects. The effects I study are sometimes small but it would be silly, for example, to suppose that the difference in voting patterns of men and women (after controlling for some other variables) could be exactly zero. My problems with the “false discovery” formulation are partly a matter of taste, I’m sure, but I believe they also arise from the difference between problems in genetics (in which some genes really have essentially zero effects on some traits, so that the classical hypothesis-testing model is plausible) and in social science and environmental health (where essentially everything is connected to everything else, and effect sizes follow a continuous distribution rather than a mix of large effects and near-exact zeroes).</p>
</blockquote>
</section>
<section id="gelman-2010b">
<h2><a href="#gelman-2010b" title="Link to section: § 'Gelman2010b'"><span><span>Gelman</span><span>2010b</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stat.columbia.edu/93660606e46677d54674aed7e28b11d49b0bfd3f.pdf" id="_6Yl2evaM" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/93660606e46677d54674aed7e28b11d49b0bfd3f.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/published/causalreview4.pdf" data-filesize-bytes="76010" data-filesize-percentage="4">“Causality and Statistical Learning”</a>, <span><span>Gelman</span><span>2010b</span></span>:</p>
<blockquote>
<p><em>There are (almost) no true zeroes: difficulties with the research program of learning causal structure</em></p>
<p>We can distinguish between learning within a causal model (that is, inference about parameters characterizing a specified directed graph) and learning causal structure itself (that is, inference about the graph itself). In social science research, I am extremely skeptical of this second goal.</p>
<p>The difficulty is that, in social science, there are no true zeroes. For example, religious attendance is associated with attitudes on economic as well as social issues, and both these correlations vary by state. And it does not interest me, for example, to test a model in which social class affects vote choice through party identification but not along a direct path.</p>
<p>More generally, anything that plausibly could have an effect will not have an effect that is exactly zero. I can respect that some social scientists find it useful to frame their research in terms of conditional independence and the testing of null effects, but I don’t generally find this approach helpful—and I certainly don’t believe that it is necessary to think in terms of conditional independence in order to study causality. Without structural zeroes, it is impossible to identify graphical <a href="https://en.wikipedia.org/wiki/Structural_equation_modeling" id="_u_O3tbiW" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Structural_equation_modeling#bodyContent" title="Structural equation modeling">structural equation models</a>.</p>
<p>The most common exceptions to this rule, as I see it, are independences from design (as in a designed or <a href="https://en.wikipedia.org/wiki/Natural_experiment" id="_BjPM3vhm" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Natural_experiment#bodyContent" title="Natural experiment">natural experiment</a>) or effects that are zero based on a plausible scientific hypothesis (as might arise, for example, in genetics where genes on different chromosomes might have essentially independent effects), or in a study of ESP. In such settings I can see the value of testing a null hypothesis of zero effect, either for its own sake or to rule out the possibility of a conditional correlation that is supposed not to be there.</p>
<p>Another sort of exception to the “no zeroes” rule comes from information restriction: a person’s decision should not be affected by knowledge that he or she doesn’t have. For example, a consumer interested in buying apples cares about the total price he pays, not about how much of that goes to the seller and how much goes to the government in the form of taxes. So the restriction is that the utility depends on prices, not on the share of that going to taxes. That is the type of restriction that can help identify demand functions in economics.</p>
<p>I realize, however, that my perspective that there are no zeroes (information restrictions aside) is a minority view among social scientists and perhaps among people in general, on the evidence of psychologist Sloman’s book. For example, from chapter 2: “A good politician will know who is motivated by greed and who is motivated by larger principles in order to discern how to solicit each one’s vote when it is needed.” I can well believe that people think in this way but I don’t buy it! Just about everyone is motivated by greed and by larger principles! This sort of discrete thinking doesn’t seem to me to be at all realistic about how people behave-although it might very well be a good model about how people characterize others!</p>
<p>In the next chapter, Sloman writes, “No matter how many times A and B occur together, mere co-occurrence cannot reveal <em>whether</em> A causes B, or B causes A, or something else causes both.” [italics added] Again, I am bothered by this sort of discrete thinking. I will return in a moment with an example, but just to speak generally, if A <em>could</em> cause B, and B <em>could</em> cause A, then I would think that, yes, they could cause each other. And if something else <em>could</em> cause them both, I imagine that could be happening along with the causation of A on B and of B on A.</p>
<p>Here we’re getting into some of the differences between a normative view of science, a descriptive view of science, and a descriptive view of how people perceive the world. Just as there are limits to what “folk physics” can tell us about the motion of particles, similarly I think we have to be careful about too closely identifying “folk causal inference” from the stuff done by the best social scientists. To continue the analogy: it is interesting to study how we develop physical intuitions using commonsense notions of force, energy, momentum, and so on—but it’s also important to see where these intuitions fail. Similarly, ideas of causality are fundamental but that doesn’t stop ordinary people and even experts from making basic mistakes.</p>
<p>Now I would like to return to the graphical model approach described by Sloman. In chapter 5, he discusses an example with 3 variables:</p>
<blockquote>
<p>If two of the variables are dependent, say, intelligence and socioeconomic status, but conditionally independent given the third variable [beer consumption], then either they are related by one of two chains:</p>
<pre><code>(Intelligence → Amount of beer consumed → Socioeconomic status)
(Socio-economic status → Amount of beer consumed → Intelligence)</code></pre>
<p>or by a fork:</p>
<pre><code>                           Socioeconomic status
                         ⤴
 Amount of beer consumed
                         ⤵
                           Intelligence</code></pre>
<p>and then we must use some other means [other than observational data] to decide between these 3 possibilities. In some cases, common sense may be sufficient, but we can also, if necessary, run an experiment. If we intervene and vary the amount of beer consumed and see that we affect intelligence, that implies that the second or third model is possible; the first one is not. Of course, all this assumes that there aren’t other variables mediating between the ones shown that provide alternative explanations of the dependencies.</p>
</blockquote>
<p>This makes no sense to me. I don’t see why only one of the 3 models can be true. This is a mathematical possibility, but it seems highly implausible to me. And, in particular, running an experiment that reveals one of these causal effects does <em>not</em> rule out the other possible paths. For example, suppose that Sloman were to perform the above experiment (finding that beer consumption affects intelligence) and then <em>another</em> experiment, this time varying intelligence (in some way; the method of doing this can very well determine the causal effect) and finding that it affects the amount of beer consumed.</p>
<p>Beyond this fundamental problem, I have a statistical critique, which is that in social science you won’t have these sorts of conditional independencies, except from design or as artifacts of small sample sizes that do not allow us to distinguish small dependencies from zero.</p>
<p>I think I see where Sloman is coming from, from a psychological perspective: you see these variables that are related to each other, and you want to know which is the cause and which is the effect. But I don’t think this is a useful way of understanding the world, just as I don’t think it’s useful to categorize political players as being motivated either by greed or by larger principles, but not both. Exclusive-or might feel right to us internally, but I don’t think it works as science.</p>
<p>One important place where I agree with Sloman (and thus with Pearl and Sprites et al.) is in the emphasis that causal structure cannot in general be learned from observational data alone; they hold the very reasonable position that we can use observational data to rule out possibilities and formulate hypotheses, and then use some sort of intervention or experiment (whether actual or hypothetical) to move further. In this way they connect the observational/experimental division to the hypothesis/deduction formulation that is familiar to us from the work of Popper, Kuhn, and other modern philosophers of science.</p>
<p>The place where I think Sloman is misguided is in his formulation of scientific models in an either/or way, as if, in truth, social variables are linked in simple causal paths, with a scientific goal of figuring out if A causes B or the reverse. I don’t know much about intelligence, beer consumption, and socioeconomic status, but I certainly don’t see any simple relationships between income, religious attendance, party identification, and voting—and I don’t see how a search for such a pattern will advance our understanding, at least given current techniques. I’d rather start with description and then go toward causality following the approach of economists and statisticians by thinking about potential interventions one at a time. I’d love to see Sloman’s and Pearl’s ideas of the interplay between observational and experimental data developed in a framework that is less strongly tied to the notion of choice among simple causal structures.</p>
</blockquote>
</section>
<section id="gelman-2012">
<h2><a href="#gelman-2012" title="Link to section: § 'Gelman2012'"><span><span>Gelman</span><span>2012</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2012/03/16/hot-hand-debate-is-warming-up/" id="_AnBciugt" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“The”hot hand” and problems with hypothesis testing”</a>, <span><span>Gelman</span><span>2012</span></span>:</p>
<blockquote>
<p>The effects are certainly not zero. We are not machines, and anything that can affect our expectations (for example, our success in previous tries) should affect our performance…Whatever the latest results on particular sports, I can’t see anyone overturning the basic finding of Gilovich, Vallone, and Tversky that players and spectators alike will <em>perceive</em> the hot hand even when it does not exist and dramatically <em>overestimate</em> the magnitude and consistency of any hot-hand phenomenon that does exist. In summary, this is yet another problem where much is lost by going down the standard route of null hypothesis testing.</p>
</blockquote>
</section>
<section id="gelman-et-al-2013">
<h2><a href="#gelman-et-al-2013" title="Link to section: § 'Gelman et al 2013'"><span><span title="et al">Gelman</span><span> Et Al </span><span>2013</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stat.columbia.edu/60e7c1c3045b451638455140757bd9f43a6005e2.pdf" id="_no1ItuWq" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/60e7c1c3045b451638455140757bd9f43a6005e2.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/published/GRR18.pdf" data-filesize-bytes="217500" data-filesize-percentage="16">“Inherent difficulties of non-Bayesian likelihood-based inference, as revealed by an examination of a recent book by Aitkin”</a> (<a href="https://gwern.net/doc/www/www.stat.columbia.edu/7ce2ebe4b0087248eeea99ee1881e5f70aa311e6.pdf" id="_pgpeRBzi" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/7ce2ebe4b0087248eeea99ee1881e5f70aa311e6.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/unpublished/GRR16.pdf" data-filesize-bytes="206900" data-filesize-percentage="15">earlier version</a>):</p>
<blockquote>
<ol start="7" type="1">
<li><p><em>Solving non-problems</em></p></li>
</ol>
<p>Several of the examples in <em>Statistical Inference</em> represent solutions to problems that seem to us to be artificial or conventional tasks with no clear analogy to applied work.</p>
<blockquote>
<p>“They are artificial and are expressed in terms of a survey of 100 individuals expressing support (Yes/No) for the president, before and after a presidential address (…) The question of interest is whether there has been a change in support between the surveys (…). We want to assess the evidence for the hypothesis of equality <em>H</em><sub>1</sub> against the alternative hypothesis <em>H</em><sub>2</sub> of a change.” —<em>Statistical Inference</em>, page 147</p>
</blockquote>
<p>Based on our experience in public opinion research, this is not a real question. Support for any political position is always changing. The real question is how much the support has changed, or perhaps how this change is distributed across the population.</p>
<p>A defender of Aitkin (and of classical hypothesis testing) might respond at this point that, yes, everybody knows that changes are never exactly zero and that we should take a more “grown-up” view of the null hypothesis, not that the change is zero but that it is nearly zero. Unfortunately, the metaphorical interpretation of hypothesis tests has problems similar to the theological doctrines of the Unitarian church. Once you have abandoned literal belief in the Bible, the question soon arises: why follow it at all? Similarly, once one recognizes the inappropriateness of the point null hypothesis, we think it makes more sense not to try to rehabilitate it or treat it as treasured metaphor but rather to attack our statistical problems directly, in this case by performing inference on the change in opinion in the population.</p>
<p>To be clear: we are not denying the value of hypothesis testing. In this example, we find it completely reasonable to ask whether observed changes are statistically-significant, i.e. whether the data are consistent with a null hypothesis of zero change. What we do not find reasonable is the statement that “the question of interest is whether there has been a change in support.”</p>
<p>All this is application-specific. Suppose public opinion was observed to really be flat, punctuated by occasional changes, as in the left graph in Figure 7.1. In that case, Aitkin’s question of “whether there has been a change” would be well-defined and appropriate, in that we could interpret the null hypothesis of no change as some minimal level of baseline variation.</p>
<p>Real public opinion, however, does not look like baseline noise plus jumps, but rather shows continuous movement on many time scales at once, as can be seen from the right graph in Figure 7.1, which shows actual presidential approval data. In this example, we do not see Aitkin’s question as at all reasonable. Any attempt to work with a null hypothesis of opinion stability will be inherently arbitrary. It would make much more sense to model opinion as a continuously-varying process. The statistical problem here is not merely that the null hypothesis of zero change is nonsensical; it is that the null is in no sense a reasonable approximation to any interesting model. The sociological problem is that, from <a href="#savage-1954">Savage (<span>1954<sub><span title="1954 was 71 years ago.">71ya</span></sub></span>)</a> onward, many Bayesians have felt the need to mimic the classical null-hypothesis testing framework, even where it makes no sense.</p>
</blockquote>
</section>
</section>
<section id="lin-et-al-2013">
<h2><a href="#lin-et-al-2013" title="Link to section: § 'Lin et al 2013'"><span><span title="et al">Lin</span><span> Et Al </span><span>2013</span></span></a></h2>
<p><a href="https://www.galitshmueli.com/system/files/Print%20Version.pdf" id="_TMFRsqiq" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" title="Site map">“Too Big to Fail: Large Samples and the <em>p</em>-Value Problem”</a>, <span><span title="et al">Lin</span><span> et al </span><span>2013</span></span>:</p>
<blockquote>
<p>The Internet has provided IS researchers with the opportunity to conduct studies with extremely large samples, frequently well over 10,000 observations. There are many advantages to large samples, but researchers using statistical inference must be aware of the <em>p</em>-value problem associated with them. In very large samples, <em>p</em>-values go quickly to zero, and solely relying on <em>p</em>-values can lead the researcher to claim support for results of no practical significance. In a survey of large sample IS research, we found that a significant number of papers rely on a low <em>p</em>-value and the sign of a regression coefficient alone to support their hypotheses. This research commentary recommends a series of actions the researcher can take to mitigate the <em>p</em>-value problem in large samples and illustrates them with an example of over 300,000 camera sales on eBay. We believe that addressing the <em>p</em>-value problem will increase the credibility of large sample IS research as well as provide more insights for readers.</p>
<p>…A key issue with applying small-sample statistical inference to large samples is that even minuscule effects can become statistically-significant. The increased power leads to a dangerous pitfall as well as to a huge opportunity. The issue is one that statisticians have long been aware of: “the <em>p</em>-value problem.” Chatfield (<span>1995<sub><span title="1995 was 30 years ago.">30ya</span></sub></span>, p.&nbsp;70 [<em>Problem Solving: A Statistician’s Guide, 2<sup>nd</sup> ed</em>]) comments, “The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical-significance, what is the practical significance of the results?” The increased power of large samples means that researchers can detect smaller, subtler, and more complex effects, but relying on <em>p</em>-values alone can lead to claims of support for hypotheses of little or no practical significance.</p>
<p>…In reviewing the literature, we found only a few mentions of the large-sample issue and its effect on <em>p</em>-values; we also saw little recognition that the authors’ low <em>p</em>-values might be an artifact of their large-sample sizes. Authors who recognized the “large-sample, small <em>p</em>-values” issue addressed it by one of the following approaches: reducing the significance level threshold<sup>5</sup> (which does not really help), by recomputing the <em>p</em>-value for a small sample (Gefen and <span><span>Carmel</span><span>2008</span></span>), or by focusing on practical significance and commenting about the uselessness of statistical-significance (<a href="https://gwern.net/doc/www/terpconnect.umd.edu/47df2c4ced4e178de4bad4630e42e69b863d3c71.pdf" id="_qOPgNhUs" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/terpconnect.umd.edu/47df2c4ced4e178de4bad4630e42e69b863d3c71.pdf" data-url-original="https://terpconnect.umd.edu/~smithas/papers/mithaslucas2010ms.pdf" data-filesize-bytes="199929" data-filesize-percentage="15" title="Are foreign IT workers cheaper? US visa policies and compensation of information technology professionals">Mithas and <span><span>Lucas</span><span>2010</span></span></a>).</p>
</blockquote>
</section>
<section id="schwitzgebel-2013">
<h2><a href="#schwitzgebel-2013" title="Link to section: § 'Schwitzgebel2013'"><span><span>Schwitzgebel</span><span>2013</span></span></a></h2>
<p><a href="https://philosophycommons.typepad.com/xphi/2013/02/preliminary-evidence-that-the-world-is-simple-an-exercise-in-stupid-epistemology.html" id="_mMWLLtZL">“Preliminary Evidence That the World Is Simple (An Exercise in Stupid Epistemology)”</a> (humorous blog post)</p>
<blockquote>
<p>Here’s what I did. I thought up 30 pairs of variables that would be easy to measure and that might relate in diverse ways. Some variables were physical (the distance versus apparent brightness of nearby stars), some biological (the length versus weight of sticks found in my back yard), and some psychological or social (the S&amp;P 500 index closing value versus number of days past). Some I would expect to show no relationship (the number of pages in a library book versus how high up it is shelved in the library), some I would expect to show a roughly linear relationship (distance of McDonald’s franchises from my house versus <a href="https://en.wikipedia.org/wiki/MapQuest" id="_zALzj9Ko" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/MapQuest#bodyContent" title="MapQuest">MapQuest</a> estimated driving time), and some I expected to show a curved or complex relationship (forecasted temperature versus time of day, size in KB of a JPG photo of my office versus the angle at which the photo was taken). <a href="https://gwern.net/doc/www/schwitzsplintersunderblog.blogspot.com/aa86fa2004e20effba2df61d144affd0891a352a.html" id="_OWI49Lia" data-url-archive="/doc/www/schwitzsplintersunderblog.blogspot.com/aa86fa2004e20effba2df61d144affd0891a352a.html" data-url-original="https://schwitzsplintersunderblog.blogspot.com/2013/02/variables-measured-for-post-preliminary.html" data-filesize-bytes="1626708" data-filesize-percentage="59" title="Variables Measured for the Post 'Preliminary Evidence That the World Is Simple'">See here</a> for the full list of variables. I took 11 measurements of each variable pair. Then I analyzed the resulting data.</p>
<p>Now, if the world is massively complex, then it should be difficult to predict a third datapoint from any two other data points. Suppose that two measurements of some continuous variable yield values of 27 and 53. What should I expect the third measured value to be? Why not 1,457,002? Or 3.22 × 10<sup>−17</sup>? There are just as many functions (that is, infinitely many) containing 27, 53, and 1,457,002 as there are containing 27, 53, and some more pedestrian-seeming value like 44.</p>
<p>…To conduct the test, I used each pair of dependent variables to predict the value of the next variable in the series (the 1<sup>st</sup> and 2<sup>nd</sup> observations predicting the value of the 3<sup>rd</sup>, the 2<sup>nd</sup> and 3<sup>rd</sup> predicting the value of the 4<sup>th</sup>, etc.), yielding 270 predictions for the 30 variables. I counted an observation “wild” if its absolute value was 10 times the maximum of the absolute value of the two previous observations or if its absolute value was below <span>1⁄10</span> of the minimum of the absolute value of the two previous observations. Separately, I also looked for flipped signs (either two negative values followed by a positive or two positive values followed by a negative), though most of the variables only admitted positive values. This measure of wildness yielded 3 wild observations out of 270 (1%) plus another 3 flipped-sign cases (total 2%). (A few variables were capped, either top or bottom, in a way that would make an above-10x or below-1/10<sup>th</sup> observation analytically unlikely, but excluding such variables wouldn’t affect the result much.) So it looks like the Wild Complexity Thesis might be in trouble.</p>
</blockquote>
</section>
<section id="ellenberg-2014">
<h2><a href="#ellenberg-2014" title="Link to section: § 'Ellenberg2014'"><span><span>Ellenberg</span><span>2014</span></span></a></h2>
<p>Jordan Ellenberg, <a href="https://deadspin.com/the-myth-of-the-myth-of-the-hot-hand-1588112937/" id="_oYIHLSa_">“The Myth Of The Myth Of The Hot Hand”</a> (excerpted from <em>How Not to Be Wrong: The Power of Mathematical Thinking</em>, <span>2014<sub><span title="2014 was 11 years ago.">11ya</span></sub></span>):</p>
<blockquote>
<p>A significance test is a scientific instrument, and like any other instrument, it has a certain degree of precision. If you make the test more sensitive—by increasing the size of the studied population, for example—you enable yourself see ever-smaller effects. That’s the power of the method, but also its danger. The truth is, the null hypothesis is probably <em>always</em> false! When you drop a powerful drug into a patient’s bloodstream, it’s hard to believe the intervention literally has zero effect on the probability that the patient will develop esophageal cancer, or thrombosis, or bad breath. Each part of the body speaks to every other, in a complex feedback loop of influence and control. Everything you do either gives you cancer or prevents it. And in principle, if you carry out a powerful enough study, you can find out which it is. But those effects are usually so minuscule that they can be safely ignored. Just because we can detect them doesn’t always mean they matter…The right question isn’t, “Do basketball players sometimes temporarily get better or worse at making shots?”—the kind of yes/no question a significance test addresses. The right question is “How <em>much</em> does their ability vary with time, and to what extent can observers detect in real time whether a player is hot?” Here, the answer is surely “not as much as people think, and hardly at all.”</p>
</blockquote>
</section>
<section id="lakens-2014">
<h2><a href="#lakens-2014" title="Link to section: § 'Lakens2014'"><span><span>Lakens</span><span>2014</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/daniellakens.blogspot.com/92fba058c348a440c26d2952267e27fa62c87f6e.html" id="_NPjSj1Yl" data-url-archive="/doc/www/daniellakens.blogspot.com/92fba058c348a440c26d2952267e27fa62c87f6e.html" data-url-original="https://daniellakens.blogspot.com/2014/06/the-null-is-always-false-except-when-it.html" data-filesize-bytes="600011" data-filesize-percentage="38" title="The 20% Statistician: The Null Is Always False (Except When It Is True)">“The Null Is Always False (Except When It Is True)”</a>, Daniel Lakens:</p>
<blockquote>
<p>The more important question is whether it is true that there are always real differences in the real world, and what the ‘real world’ is. Let’s consider the population of people in the real world. While you read this sentence, some individuals in this population have died, and some were born. For most questions in psychology, the population is surprisingly similar to an eternally running Monte Carlo simulation. Even if you could measure all people in the world in a millisecond, and the test-retest correlation was perfect, the answer you would get now would be different from the answer you would get in an hour. Frequentists (the people that use NHST) are not specifically interested in the exact value now, or in one hour, or next week Thursday, but in the average value in the ‘long’ run. The value in the real world today might never be zero, but it’s never anything, because it’s continuously changing. If we want to make generalizable statements about the world, I think the fact that the null-hypothesis is never precisely true at any specific moment is not a problem. I’ll ignore more complex questions for now, such as how we can establish whether effects vary over time.</p>
<p>…Meehl talks about how in psychology every individual-difference variable (eg. trait, status, demographic) correlates with every other variable, which means the null is practically never true. In these situations, it’s not that testing against the null-hypothesis is meaningless, but it’s not informative. If everything correlates with everything else, you need to create good models, and test those. A simple null-hypothesis significance test will not get you very far. I agree.</p>
<p><strong><em>Random Assignment versus Crud</em></strong></p>
<p>To illustrate when NHST can be used to as a source of information in large samples, and when NHST is not informative in large samples, I’ll analyze data of large dataset with 6344 participants from the Many Labs project. I’ve analyzed 10 dependent variables to see whether they were influenced by (1) Gender, and (2) Assignment to the high or low anchoring condition in the first study. Gender is a measured individual difference variable, and not a manipulated variable, and might thus be affected by what Meehl calls the crud factor. Here, I want to illustrate this is (1) probably often true for individual difference variables, but perhaps not always true, and (2) it is probably never true for when analyzing differences between groups individuals were randomly assignment to.</p>
<p>…When we analyze the 10 dependent variables as a function of the anchoring condition, none of the differences are statistically-significant (even though there are more than 6000 participants). You can play around with the script, repeating the analysis for the conditions related to the other 3 anchoring questions (remember to correct for multiple comparisons if you perform many tests), and see how randomization does a pretty good job at returning non-statistically-significant results even in very large sample sizes. If the null is always false, it is remarkably difficult to reject. Obviously, when we analyze the answer people gave on the first anchoring question, we find a huge effect of the high versus low anchoring condition they were randomly assigned to. Here, NHST works. There is probably something going on. If the anchoring effect was a completely novel phenomenon, this would be an important first finding, to be followed by replications and extensions, and finally model building and testing.</p>
<p>The results change dramatically if we use Gender as a factor. There are Gender effects on dependent variables related to quote attribution, system justification, the gambler’s fallacy, imagined contact, the explicit evaluation of arts and math, and the norm of reciprocity. There are no significant differences in political identification (as conservative or liberal), on the response scale manipulation, or on gain versus loss framing (even though <em>p</em> = 0.025, such a high <em>p</em>-value is stronger support for the null-hypothesis than for the alternative hypothesis with 5500 participants). It’s surprising that the null-hypothesis (gender does not influence the responses participants give) is rejected for 7 out of 10 effects. Personally (perhaps because I’ve got very little expertise in gender effects) I was actually extremely surprised, even though the effects are small (with Cohen <em>d</em>’s or around 0.09). This, ironically, shows that NHST works—I’ve learned gender effects are much more widespread than I’d have though before I wrote this blog post.</p>
</blockquote>
</section>
<section id="kirkegaard-2014">
<h2><a href="#kirkegaard-2014" title="Link to section: § 'Kirkegaard2014'"><span><span>Kirkegaard</span><span>2014</span></span></a></h2>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.669.378&amp;rep=rep1&amp;type=pdf" id="__p8dCFOz" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“The international general socioeconomic factor: Factor analyzing international rankings”</a>:</p>
<blockquote>
<p>Many studies have examined the correlations between national IQs and various country-level indexes of well-being. The analyses have been unsystematic and not gathered in one single analysis or dataset. In this paper I gather a large sample of country-level indexes and show that there is a strong general socioeconomic factor (S factor) which is highly correlated (.86–.87) with national cognitive ability using either Lynn and Vanhanen’s dataset or Altinok’s. Furthermore, the method of correlated vectors shows that the correlations between variable loadings on the S factor and cognitive measurements are .99 in both datasets using both cognitive measurements, indicating that it is the S factor that drives the relationship with national cognitive measurements, not the remaining variance.</p>
</blockquote>
<p>See also <a href="https://www.npr.org/sections/goatsandsoda/2019/06/14/730257541/countries-are-ranked-on-everything-from-health-to-happiness-whats-the-point" id="_WCO12Fz0" data-link-icon="npr" data-link-icon-type="text,tri,sans" data-link-icon-color="#237bbd" title="International Rankings: What Are The Benefits And Drawbacks?">“Countries Are Ranked On Everything From Health To Happiness. What’s The Point?”</a>:</p>
<blockquote>
<p>It’s a brand new ranking. Called the Sustainable Development Goals Gender Index, it gives 129 countries a score for progress on achieving gender equality by 2030. Here’s the quick summary: Things are “good” in much of Europe and North America. And “very poor” in much of sub-Saharan Africa. In fact, that’s the way it looks in many international rankings, which tackle everything from the worst places to be a child to the most corrupt countries to world happiness…As for the fact that many rankings look the same at the top and bottom, one reason has to do with money. Many indexes are correlated with GDP per capita, a measure of a country’s prosperity, says Kenny. That includes the World Bank’s Human Capital Index, which measures the economic productivity of a country’s young people; and Freedom House’s Freedom in the World index, which ranks the world by its level of democracy, including economic freedom. And countries that have more money can spend more money on health, education and infrastructure.</p>
</blockquote>
</section>
<section id="shen-et-al-2014">
<h2><a href="#shen-et-al-2014" title="Link to section: § 'Shen et al 2014'"><span><span title="et al">Shen</span><span> Et Al </span><span>2014</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/2014-shen.pdf" id="shen-et-al-2014-link" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="101042" data-filesize-percentage="18" title="'When Correcting for Unreliability of Job Performance Ratings, the Best Estimate Is Still 0.52', Shen et al 2014">“When Correcting for Unreliability of Job Performance Ratings, the Best Estimate Is Still 0.52”</a>, <span><span title="et al">Shen</span><span> et al </span><span>2014</span></span>:</p>
<blockquote>
<p><strong>Is Too Much Variance Explained?</strong> It is interesting that historically the I-O literature has bemoaned the presence of a “validity ceiling”, and the field seemed to be unable to make large gains in the prediction of job performance (Highhouse, <span>2008<sub><span title="2008 was 17 years ago.">17ya</span></sub></span>). In contrast, LeBreton et al. appear to have the opposite concern—that we maybe able to predict too much, perhaps even all, of the variance in job performance once accounting for statistical artifacts. In addition to their 4 focal predictors (ie. GMA, integrity, structured interview, work sample), LeBreton et al. list an additional 24 variables that have been shown to be related to job performance meta-analytically. However, we believe that many of the variables LeBreton et al. included in their list are variables that Sackett, Borneman, and Connelly (<span>2009<sub><span title="2009 was 16 years ago.">16ya</span></sub></span>) would argue are likely unknowable at time of hire.</p>
<p>…Furthermore, in contrast to LeBreton et al.’s assertion that organizational variables, such as procedural justice, are likely unrelated to their focal predictors, our belief is that many of these variables are likely to be at least moderately correlated–limiting the incremental validity we could expect with the inclusion of these additional variables. For example, research has shown that integrity tests mostly tap into <a href="https://en.wikipedia.org/wiki/Conscientiousness#Personality_models" id="_LpC9lLj1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Conscientiousness#bodyContent" title="Conscientiousness § Personality models">Conscientiousness</a>, <a href="https://en.wikipedia.org/wiki/Agreeableness" id="__Qr-IYeG" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Agreeableness#bodyContent" title="Agreeableness">Agreeableness</a>, and Emotional Stability (Ones &amp; Viswesvaran, <span>2001<sub><span title="2001 was 24 years ago.">24ya</span></sub></span>), and a recent <a href="https://en.wikipedia.org/wiki/Meta-analysis" id="_tKvFlpVk" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Meta-analysis#bodyContent" title="Meta-analysis">meta-analysis</a> of organizational justice shows that all 3 personality traits are moderately related to one’s experience of procedural justice (ρ=0.19–0.23; <span><span title="et al">Hutchinson</span><span> et al </span><span>2014</span></span><span>), suggesting that even apparently unrelated variables can share a surprising amount of construct-level variance. In support of this perspective, Paterson, Harms, and Crede (<span>2012<sub><span title="2012 was 13 years ago.">13ya</span></sub></span>) [</span>“The meta of all metas: 30 years of meta-analysis reviewed”] conducted a meta-analysis of over 200 meta-analyses and found an average correlation of 0.27, suggesting that most variables we study are at least somewhat correlated and validating the first author’s long-held personal assumption that the world is correlated 0.30 (on average; see also <a href="#meehl-1990-1">Meehl’s, 1990</a>, crud factor)!</p>
</blockquote>
</section>
<section id="gordon-et-al-2019">
<h2><a href="#gordon-et-al-2019" title="Link to section: § 'Gordon et al 2019'"><span><span title="et al">Gordon</span><span> Et Al </span><span>2019</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/2019-gordon.pdf" id="gordon-et-al-2019-2" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2814049" data-filesize-percentage="88" title="'A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook', Gordon et al 2019">“A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook”</a>, <span><span title="et al">Gordon</span><span> et al </span><span>2019</span></span>:</p>
<blockquote>
<p>We examine how common techniques used to measure the causal impact of ad exposures on users’ conversion outcomes compare to the “gold standard” of a true experiment (randomized controlled trial). Using data from 12 US advertising lift studies at Facebook comprising 435 million user-study observations and 1.4 billion total impressions we contrast the experimental results to those obtained from observational methods, such as comparing exposed to unexposed users, matching methods, model-based adjustments, synthetic matched-markets tests, and before-after tests. We show that observational methods often fail to produce the same results as true experiments even after conditioning on information from thousands of behavioral variables and using non-linear models. We explain why this is the case. Our findings suggest that common approaches used to measure advertising effectiveness in industry fail to measure accurately the true effect of ads.</p>
<p>An important input to <a href="https://en.wikipedia.org/wiki/Propensity_score_matching" id="_KiZ5d_hD" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Propensity_score_matching#bodyContent" title="Propensity score matching">propensity score matching</a> (PSM) is the set of variables used to predict the propensity score itself. We tested 3 different PSM specifications for study 4, each of which used a larger set of inputs.</p>
<ol type="1">
<li><p><strong>PSM 1</strong>: In addition to age and gender, the basis of our exact matching (EM) approach, this specification uses common Facebook variables, such as how long users have been on Facebook, how many Facebook friends the have, their reported relationship status, and their phone OS, in addition to other user characteristics.</p></li>
<li><p><strong>PSM 2</strong>: In addition to the variables in PSM 1, this specification uses Facebook’s estimate of the user’s zip code of residence to associate with each user nearly 40 variables drawn from the most recent Census and American Communities Surveys (ACS).</p></li>
<li><p><strong>PSM 3</strong>: In addition to the variables in PSM 2, this specification adds a composite metric of Facebook data that summarizes thousands of behavioral variables. This is a machine-learning based metric used by Facebook to construct target audiences that are similar to consumers that an advertiser has identified as desirable.<sup><a href="https://www.facebook.com/business/help/164749007013531" id="_L-BscyZP" data-link-icon="facebook" data-link-icon-type="svg" data-link-icon-color="#1877f2" title="About Lookalike Audiences: A Lookalike Audience is a way to reach new people who are likely to be interested in your business because they're similar to your best existing customers.">16</a></sup> Using this metric bases the estimation of our propensity score on a non-linear machine-learning model with thousands of features.<sup>17</sup></p></li>
</ol>
<p>…When we go from exact matching (EM) to our most parsimonious propensity score matching model (PSM 1), the conversion rate for unexposed users increases from 0.032% to 0.042%, decreasing the implied advertising lift from 221% to 147%. PSM 2 performs similarly to PSM 1, with an implied lift of 154%.<sup>21</sup> Finally, adding the composite measure of Facebook variables in PSM 3 improves the fit of the propensity model (as measured by a higher AUC/ROC) and further increases the conversion rate for matched unexposed users to 0.051%. The result is that our best performing PSM model estimates an advertising lift of 102%…We summarize the result of all our propensity score matching and regression methods for study 4 in Figure 7.</p>
<figure>
<p><img alt="Figure 7: Summary of lift estimates and confidence intervals." data-aspect-ratio="1105 / 861" decoding="async" height="861" loading="lazy" src="https://gwern.net/doc/statistics/2016-gordon-figure7-propensityscoringresults.png" width="1105"></p>
<figcaption><p><strong>Figure 7</strong>: Summary of lift estimates and confidence intervals.</p></figcaption>
</figure>
</blockquote>
<p>While not directly testing statistical-significance in its propensity scoring, the increasing accuracy in estimating the true causal effect of adding in additional behavioral variables implies that (especially at Facebook-scale, using billions of datapoints) the correlations of the thousands of used variables with the advertising behavior would be statistically-significant and demonstrate that everything is correlated. (See also my <a href="https://gwern.net/banner" id="gwern-banner" data-filesize-bytes="155867" data-filesize-percentage="65" title="'Banner Ads Considered Harmful', Gwern 2017">ad harms</a> &amp; <a href="https://gwern.net/correlation" id="gwern-correlation" data-filesize-bytes="144066" data-filesize-percentage="63" title="'How Often Does Correlation=Causality?', Gwern 2014">“How Often Does Correlation=Causality?”</a> pages.)</p>
</section>
<section id="kirkegaard-2020">
<h2><a href="#kirkegaard-2020" title="Link to section: § 'Kirkegaard2020'"><span><span>Kirkegaard</span><span>2020</span></span></a></h2>
<p><a href="https://emilkirkegaard.dk/en/2020/03/enhancing-archival-datasets-with-machine-learned-psychometrics/" id="_tR6rAS91" title="Enhancing archival datasets with machine learned psychometrics">“Enhancing archival datasets with machine learned psychometrics”</a>, <span><span>Kirkegaard</span><span>2020</span></span> (published as <a href="https://gwern.net/doc/iq/2021-kirkegaard.pdf" id="kirkegaard-nyborg-2021" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="492309" data-filesize-percentage="58" title="Intelligence and General Psychopathology in the Vietnam Experience Study: A Closer Look"><span><span>Kirkegaard &amp; Nyborg</span><span>2021</span></span></a>):</p>
<blockquote>
<p>In our ISIR 2019 presentation (<a href="https://docs.google.com/presentation/d/1xxfYTWP2R0ZFvbI24sR1jVyFC_qN_Lw85782zE5Q2Jo/edit" id="_9uNcP5Wd" data-link-icon="word-doc" data-link-icon-type="svg" data-link-icon-color="#4285f4" title="Machine learning psychometrics: ISIR 2019">“Machine learning psychometrics: Improved cognitive ability validity from supervised training on item level data”</a>), we showed that one can use machine learning on cognitive data to improve the predictive validity of it. The effect sizes can be quite large, eg. one could predict educational attainment in the Vietnam Experience Study (VES) sample (<em>n</em> = 4.5k US army recruits) at R<sup>2</sup>=32.3% with <a href="https://en.wikipedia.org/wiki/Ridge_regression" id="_uewD694v" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Ridge_regression#bodyContent" title="Ridge regression">ridge regression</a> versus 17.7% with <a href="https://en.wikipedia.org/wiki/Item_response_theory" id="_ztWiV6Yy" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Item_response_theory#bodyContent" title="Item response theory">IRT</a>. Prediction is more than <em>g</em>, after all. What if we had a dataset of 185 diverse items, and we train the model to predict IRT-based <em>g</em> from the full set, but using only a limited set using the LASSO? How many items do we need when optimally weighted? Turns out that with 42 items, one can get a test that correlates at 0.96 with the full <em>g</em>. That’s an abbreviation of nearly 80%!</p>
<p>Now comes the fancy part. What if we have archival datasets with only a few cognitive items (eg. datasets with <a href="https://en.wikipedia.org/wiki/Mini%E2%80%93mental_state_examination" id="_2RpeWFt8" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Mini%E2%80%93mental_state_examination#bodyContent" title="Mini–mental state examination">MMSE</a> items) or maybe even no items. Can we improve things here? Maybe! If the dataset has a lot of other items, we may be able to train an machine learning (ML) model that predict <em>g</em> quite well from them, even if they seem unrelated. Every item has some variance overlap with <em>g</em> however small (crud factor), it is only a question of having a good enough algorithm and enough data to exploit this covariance. For instance, I have found that if one uses the 556 items in the <a href="https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory" id="_Tr4qHl-1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory#bodyContent" title="Minnesota Multiphasic Personality Inventory">MMPI</a> in the VES to predict the very well measured <em>g</em> based on all the cognitive data (18 tests), how well can one do? I was surprised to learn that one can do <em>extremely</em> well:</p>
<p><a href="https://gwern.net/doc/iq/2020-kirkegaard-elasticnetpredictionofiqfrommmpiintheves.png" id="_T0P3KUgb" data-link-icon="image" data-link-icon-type="svg" data-filesize-bytes="72272" data-filesize-percentage="13" data-image-height="1080" data-image-width="1464" data-aspect-ratio="1400 / 1033">“Elastic net prediction of <em>g</em>: <em>r</em> = 0.83 (0.82–0.84), <em>n</em> = 4,320”</a></p>
<p>[There are 203 (elastic)/217 (<a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" id="_Zk9ZgDXA" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Lasso_(statistics)#bodyContent" title="Lasso (statistics)">lasso</a>) non-zero coefficients out of 556]</p>
<p>Thus, one can measure <em>g</em> as well as one could with a decent test like Wonderlic, or Raven’s without having any cognitive data at all! The big question here is whether these models generalize well. If one can train a model to predict <em>g</em> from MMPI items in dataset 1, and then apply it to dataset 2 without much loss of accuracy, this means that one could impute <em>g</em> in potentially thousands of old archival datasets that include the same MMPI items, or a subset of them.</p>
</blockquote>
<p>A similar analysis is done by <span><span title="et al">Revelle</span><span> et al </span><span>2020</span></span>’s <a href="https://www.sciencedirect.com/science/article/pii/S0191886920300945" id="revelle-et-al-2020" data-link-icon="E" data-link-icon-type="text" data-link-icon-color="#eb6500" title="'Exploring the persome: The power of the item in understanding personality structure', Revelle et al 2020">“Exploring the persome: The power of the item in understanding personality structure”</a> (especially “Study 4: Profile correlations using 696 items”); they do not directly report an equivalent to posteriors/<em>p</em>-values or non-zero correlations after penalized regression or anything like that, but the pervasiveness of correlation is apparent from their results &amp; data visualizations.</p>
</section>
<section id="ferguson-heene-2021">
<h2><a href="#ferguson-heene-2021" title="Link to section: § 'Ferguson &amp; Heene2021'"><span><span>Ferguson &amp; Heene</span><span>2021</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/2021-ferguson.pdf" id="ferguson-heene-2021" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="101899" data-filesize-percentage="18" title="'Providing a lower-bound estimate for psychology’s “crud factor”: The case of aggression', Ferguson &amp; Heene 2021">“Providing a Lower-Bound Estimate for Psychology’s ‘Crud Factor’: The Case of Aggression”</a>, <span><span>Ferguson &amp; Heene</span><span>2021</span></span>:</p>
<blockquote>
<p>When conducting research on large data sets, statistically-significant findings having only trivial interpretive meaning may appear. Little consensus exists whether such small effects can be meaningfully interpreted. The current analysis examines the possibility that trivial effects may emerge in large datasets, but that some such effects may lack interpretive value. When such results match an investigator’s hypothesis, they may be over-interpreted.</p>
<p>The current study examines this issue as related to aggression research in 2 large samples. Specifically, in the first study, the National Longitudinal Study of Adolescent to Adult Health (Add Health) dataset was used. 15 variables with little theoretical relevance to aggression were selected, then correlated with self-reported delinquency. For the second study, the Understanding Society database was used. As with Study 1, 14 nonsensical variables were correlated with conduct problems.</p>
<p>Many variables achieved “statistical-significance” and some effect-sizes approached or exceeded <em>r</em> = 0.10, despite little theoretical relevance between the variables.</p>
<p>It is recommended that effect sizes below <em>r</em> = 0.10 should not be interpreted as hypothesis supportive.</p>
<figure>
<p><img alt="Table 1: Correlations Between Crud and Delinquency for Study 1" data-aspect-ratio="700 / 611" decoding="async" height="1222" loading="lazy" src="https://gwern.net/doc/psychology/2021-ferguson-table1-correlationsbetweenirrelevantfactorsandjuveniledelinquencyinaddhealth.png" width="1400"></p>
<figcaption><p><strong>Table 1</strong>: Correlations Between Crud and Delinquency for Study 1</p></figcaption>
</figure>
<figure>
<p><img alt="Table 2: Correlations Between Crud and Conduct Problems for Study 2" data-aspect-ratio="1400 / 911" decoding="async" height="911" loading="lazy" src="https://gwern.net/doc/psychology/2021-ferguson-table2-correlationsbetweenirrelevantfactorsandconductproblemsinunderstandingsocietywave1.png" width="1400"></p>
<figcaption><p><strong>Table 2</strong>: Correlations Between Crud and Conduct Problems for Study 2</p></figcaption>
</figure>
</blockquote>
</section>
<section id="iliev-bennis-2023">
<h2><a href="#iliev-bennis-2023" title="Link to section: § 'Iliev &amp; Bennis2023'"><span><span>Iliev &amp; Bennis</span><span>2023</span></span></a></h2>
<p><a href="https://link.springer.com/article/10.1007/s10902-023-00631-9" id="iliev-bennis-2023" data-link-icon="springerlink" data-link-icon-type="svg" title="‘The Convergence of Positivity: Are Happy People All Alike?’, Iliev &amp; Bennis 2023">“The Convergence of Positivity: Are Happy People All Alike?”, <span><span>Iliev &amp; Bennis</span><span>2023</span></span></a></p>
</section>
<section id="downey-2023">
<h2><a href="#downey-2023" title="Link to section: § 'Downey2023'"><span><span>Downey</span><span>2023</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.allendowney.com/28a1c5ff15e3b7eeda5038806eb0f78e35dbed25.html" id="downey-2023" data-url-archive="/doc/www/www.allendowney.com/28a1c5ff15e3b7eeda5038806eb0f78e35dbed25.html" data-url-original="https://www.allendowney.com/blog/2023/08/20/how-correlated-are-you/" data-filesize-bytes="3274596" data-filesize-percentage="74" title="‘How Correlated Are You?’, Downey 2023">“How Correlated Are You?”, <span><span>Downey</span><span>2023</span></span></a></p>
</section>
<section id="external-links">
<h2><a href="#external-links" title="Link to section: § 'External Links'">External Links</a></h2>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/All_models_are_wrong" id="_Vsq3rXeR" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/All_models_are_wrong#bodyContent" title="All models are wrong">All models are wrong</a></p></li>
<li><p><a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/9dca508527d7956aa713dfe247b6d995862f4f7f.pdf" id="_3U4ckghn" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/9dca508527d7956aa713dfe247b6d995862f4f7f.pdf" data-url-original="https://pdfs.semanticscholar.org/8d4e/4958856859323ecacca91807a0ea2e847dfc.pdf" data-filesize-bytes="95262" data-filesize-percentage="6">“Stereotype (In)Accuracy in Perceptions of Groups and Individuals”</a>, <span><span title="et al">Jussim</span><span> et al </span><span>2015</span></span></p></li>
<li><p><a href="https://www.amazon.com/Handbook-Social-Status-Correlates-Ellis/dp/0128053712" id="_VaQLollj" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>Handbook of Social Status Correlates</em></a>, <span><span title="et al">Ellis</span><span> et al </span><span>2018</span></span></p></li>
<li><p><a href="https://slatestarcodex.com/2015/02/11/black-people-less-likely/" id="_cxzrdn56" data-link-icon="SSC" data-link-icon-type="text,tri" data-link-icon-color="#5175c2" title="Black People Less Likely">“Black People Less Likely”</a></p></li>
<li><p><a href="https://gwern.net/doc/psychology/personality/2006-ozer.pdf" id="ozer-benet-martínez-2006" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="162726" data-filesize-percentage="28" title="'Personality and the Prediction of Consequential Outcomes', Ozer &amp; Benet-Martínez 2006">“Personality and the Prediction of Consequential Outcomes”</a>, Ozer &amp; Benet-<span><span>Martínez</span><span>2006</span></span></p></li>
<li><p><a href="https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst" id="_jvrieZPz" data-link-icon="LW" data-link-icon-type="text" data-link-icon-color="#7faf83" data-url-iframe="https://www.greaterwrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst?format=preview&amp;theme=classic" title="Against NHST">“Against NHST”</a></p></li>
<li><p><a href="https://gwern.net/doc/www/arxiv.org/ca13ba28ad7682a74385d3bf547af7fb242c8e80.pdf" id="valberg-et-al-2017" data-link-icon="𝛘" data-link-icon-type="text" data-link-icon-color="#b31b1b" data-href-mobile="https://arxiv.org/html/1707.00014?fallback=original" data-url-archive="/doc/www/arxiv.org/ca13ba28ad7682a74385d3bf547af7fb242c8e80.pdf" data-url-original="https://arxiv.org/abs/1707.00014" data-filesize-bytes="1836479" data-filesize-percentage="61" title="'The surprising implications of familial association in disease risk', Valberg et al 2017">“The surprising implications of familial association in disease risk”</a>, <span><span title="et al">Valberg</span><span> et al </span><span>2017</span></span></p></li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2858332/" id="yarkoni-2010" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="'The Abbreviation of Personality, or how to Measure 200 Personality Scales with 200 Items', Yarkoni 2010">“The Abbreviation of Personality, or how to Measure 200 Personality Scales with 200 Items”</a>, <span><span>Yarkoni</span><span>2010</span></span></p></li>
<li><p><a href="https://gwern.net/doc/psychology/okcupid/thebestquestionsforafirstdate.html" id="_EFyCPCAz" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="2277764" data-filesize-percentage="86">“The Best Questions For A First Date”</a>, Christian <span><span>Rudder</span><span>2011</span></span> (OKCupid)</p></li>
<li><p><a href="https://gwern.net/doc/www/eighteenthelephant.com/e4045221f824564d5263546ec1341a28ae4f5f77.html" id="_NAWi5rnl" data-url-archive="/doc/www/eighteenthelephant.com/e4045221f824564d5263546ec1341a28ae4f5f77.html" data-url-original="https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/" data-filesize-bytes="2599291" data-filesize-percentage="68" title="Pushed around by stars">“Pushed around by stars”</a></p></li>
<li><p><a href="https://gwern.net/doc/statistics/bias/2022-wilson.pdf" id="wilson-et-al-2022" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1025322" data-filesize-percentage="75" title="'Theoretical false positive psychology', Wilson et al 2022">“Theoretical false positive psychology”</a>, <span><span title="et al">Wilson</span><span> et al </span><span>2022</span></span></p></li>
<li><p>Discussion: <a href="https://gwern.net/doc/www/news.ycombinator.com/05f4bff1b78c1eece9bbe3fe3d595d867fd8cd8b.html" id="_OEdPy7W0" data-link-icon="hacker-news" data-link-icon-type="svg" data-link-icon-color="#f26522" data-url-archive="/doc/www/news.ycombinator.com/05f4bff1b78c1eece9bbe3fe3d595d867fd8cd8b.html" data-url-original="https://news.ycombinator.com/item?id=19797844" data-filesize-bytes="96956" data-filesize-percentage="6">HN</a></p></li>
</ul>
</section>
<section id="appendix">
<h2><a href="#appendix" title="Link to section: § 'Appendix'">Appendix</a></h2>
<section id="genetic-correlations">
<h2><a href="#genetic-correlations" title="Link to section: § 'Genetic correlations'">Genetic Correlations</a></h2>
<p>Modern genomics has found large-scale <a href="https://en.wikipedia.org/wiki/Biobank" id="_ZVNUWNLH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Biobank#bodyContent" title="Biobank">biobanks</a> &amp; summary-statistic-only methods to be a fruitful area for identifying <a href="https://en.wikipedia.org/wiki/Genetic_correlation" id="_EFM6e8jH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genetic_correlation#bodyContent" title="Genetic correlation">genetic correlations</a> as the power of publicly-released PGSes have steadily grown with increasing <em>n</em> (stabilizing estimates &amp; making ever more genetic correlations pass statistical-significance thresholds), which also frequently mirror phenotypic correlations in all organisms (“Cheverud’s conjecture”<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>).</p>
<p>Example graphs drawn from the broader analyses (primarily visualized as heatmaps):</p>
<ul>
<li><p><a href="https://gwern.net/doc/genetics/heritable/correlation/2015-krapohl.pdf" id="_btxoW5yh" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="541123" data-filesize-percentage="60">“Phenome-wide analysis of genome-wide polygenic scores”</a>, <span><span title="et al">Krapohl</span><span> et al </span><span>2015</span></span>:</p>
<figure>
<p><img alt="Krapohl et al 2015: “Figure 1. Correlations between 13 genome-wide polygenic scores and 50 traits from the behavioral phenome. These results are based on GPS constructed using a GWAS P-value threshold (PT)=0.30; results for PT = 0.10 and 0.05 (Supplementary Figures 1a and b and Supplementary Table 3). P-values that pass Nyholt–Sidak correction (see Supplementary Methods 1) are indicated with two asterisks, whereas those reaching nominal significance (thus suggestive evidence) are shown with a single asterisk.”" data-aspect-ratio="700 / 439" decoding="async" height="878" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2015-krapohl-figure1-rgs.png" width="1400"></p>
<figcaption><p><span><span title="et al">Krapohl</span><span> et al </span><span>2015</span></span>: “Figure 1. Correlations between 13 genome-wide <a href="https://en.wikipedia.org/wiki/Polygenic_score" id="_fxPG6SR9" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Polygenic_score#bodyContent" title="Polygenic score">polygenic scores</a> and 50 traits from the behavioral phenome. These results are based on GPS constructed using a <a href="https://en.wikipedia.org/wiki/Genome-wide_association_study" id="_p74YXe0t" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genome-wide_association_study#bodyContent" title="Genome-wide association study">GWAS</a> <em>P</em>-value threshold (<em>P<sub>T</sub></em>)=0.30; results for <em>P<sub>T</sub></em> = 0.10 and 0.05 (Supplementary Figures 1a and b and Supplementary Table 3). <em>P</em>-values that pass Nyholt–Sidak correction (see Supplementary Methods 1) are indicated with two asterisks, whereas those reaching nominal significance (thus suggestive evidence) are shown with a single asterisk.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/mp2015225" id="hagenaars-et-al-2016-1" data-link-icon="n" data-link-icon-type="text" title="'Shared genetic aetiology between cognitive functions and physical and mental health in UK Biobank (<em>n</em> = 112,151) and 24 GWAS consortia', Hagenaars et al 2016">“Shared genetic aetiology between cognitive functions and physical and mental health in UK Biobank (<em>n</em> = 112 151) and 24 GWAS consortia”</a>, <span><span title="et al">Hagenaars</span><span> et al </span><span>2016</span></span>:</p>
<figure>
<p><img alt="Hagenaars et al 2016: “Figure 1. Heat map of genetic correlations calculated using LD regression between cognitive phenotypes in UK Biobank and health-related variables from GWAS consortia. Hues and colors depict, respectively, the strength and direction of the genetic correlation between the cognitive phenotypes in UK Biobank and the health-related variables. Red and blue indicate positive and negative correlations, respectively. Correlations with the darker shade associated with a stronger association. Based on results in Table 2. ADHD, attention deficit hyperactivity disorder; FEV1, forced expiratory volume in 1 s; GWAS, genome-wide association study; LD, linkage disequilibrium; NA, not available.”" data-aspect-ratio="719 / 925" decoding="async" height="925" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2016-hagenaars-figure1-intelligencergs.jpg" width="719"></p>
<figcaption><p><span><span title="et al">Hagenaars</span><span> et al </span><span>2016</span></span>: “Figure 1. Heat map of genetic correlations calculated using <a href="https://en.wikipedia.org/wiki/Linkage_disequilibrium" id="_1CNzLoPs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Linkage_disequilibrium#bodyContent" title="Linkage disequilibrium">LD</a> regression between cognitive phenotypes in UK Biobank and health-related variables from GWAS consortia. Hues and colors depict, respectively, the strength and direction of the genetic correlation between the cognitive phenotypes in UK Biobank and the health-related variables. Red and blue indicate positive and negative correlations, respectively. Correlations with the darker shade associated with a stronger association. Based on results in Table 2. <a href="https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder" id="_R4loa9f4" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder#bodyContent" title="Attention deficit hyperactivity disorder">ADHD</a>, attention deficit hyperactivity disorder; FEV1, forced expiratory volume in 1 s; GWAS, genome-wide association study; LD, linkage disequilibrium; NA, not available.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/043000.full" id="hill-et-al-2016-1" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Molecular genetic contributions to social deprivation and household income in UK Biobank (<em>n</em> = 112,151)', Hill et al 2016">“Molecular genetic contributions to social deprivation and household income in UK Biobank (<em>n</em> = 112,151)”</a>, <span><span title="et al">Hill</span><span> et al </span><span>2016</span></span>:</p>
<figure>
<p><img alt="Hill et al 2016 figure: “Genetic correlations between household incomes and health variables”" data-aspect-ratio="286 / 157" decoding="async" height="628" loading="lazy" src="https://gwern.net/doc/genetics/heritable/2016-hill-ses-health-geneticcorrelations.jpg" width="1144"></p>
<figcaption><p><span><span title="et al">Hill</span><span> et al </span><span>2016</span></span> figure: “Genetic correlations between household incomes and health variables”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/203257.full" id="socrates-et-al-2017" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Polygenic risk scores applied to a single cohort reveal pleiotropy among hundreds of human phenotypes', Socrates et al 2017">“Polygenic risk scores applied to a single cohort reveal pleiotropy among hundreds of human phenotypes”</a>, <span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span> (<a href="https://gwern.net/doc/www/www.biorxiv.org/00eb3a5d399d72007a43620f879504d7aa048b7e.pdf#page=1" id="_-onJZd9P" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" data-url-archive="/doc/www/www.biorxiv.org/00eb3a5d399d72007a43620f879504d7aa048b7e.pdf#page=1" data-url-original="https://www.biorxiv.org/content/biorxiv/suppl/2017/10/14/203257.DC1/203257-1.pdf#page=1">supplement w/full heatmaps</a>)</p>
<figure>
<p><img alt="Socrates et al 2017: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (x-axis) and NFBC196659ya traits (y-axis) for self-reported disorders, medical and psychiatric conditions verified or treated by a doctor, controlled for sex, BMI, and SES”" data-aspect-ratio="699 / 547" decoding="async" height="1094" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-socrates-geneticcorrelations-figure3-psychiatrichealth.png" width="1398"></p>
<figcaption><p><span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span>: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (<em>x</em>-axis) and NFBC<span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span> traits (<em>y</em>-axis) for self-reported disorders, medical and psychiatric conditions verified or treated by a doctor, controlled for sex, <a href="https://en.wikipedia.org/wiki/Body_mass_index" id="_qYc4DCzQ" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Body_mass_index#bodyContent" title="Body mass index">BMI</a>, and SES”</p></figcaption>
</figure>
<figure>
<p><img alt="Socrates et al 2017: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (x-axis) and NFBC196659ya traits (y-axis) from questionnaires lifestyle and social factors”" data-aspect-ratio="175 / 156" decoding="async" height="1248" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-socrates-geneticcorrelations-figure5-lifestylesocialfactors.png" width="1400"></p>
<figcaption><p><span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span>: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (<em>x</em>-axis) and NFBC<span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span> traits (<em>y</em>-axis) from questionnaires lifestyle and social factors”</p></figcaption>
</figure></li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5971142/" id="docherty-et-al-2017" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="'Polygenic prediction of the phenome, across ancestry, in emerging adulthood', Docherty et al 2017">“Polygenic prediction of the phenome, across ancestry, in emerging adulthood”</a>, <span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>:</p>
<figure>
<p><img alt="Docherty et al 2017: “Figure 2: Phenome on GPS regression q-values in European Sample (EUR). GPS displayed with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱ = q-value < 0.01, ✱✱ = q-value < 0.05, ✱ = q-value < 0.16. Blue values reflect a negative association, and red reflect positive association. Intensity of color indicates −log10 p value.”" data-aspect-ratio="920 / 1069" decoding="async" height="1069" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-docherty-figure2-phenome-eur.jpg" width="920"></p>
<figcaption><p><span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>: “Figure 2: Phenome on GPS regression q-values in European Sample (EUR). GPS displayed with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱ = <em>q</em>-value &lt; 0.01, ✱✱ = <em>q</em>-value &lt; 0.05, ✱ = <em>q</em>-value &lt; 0.16. Blue values reflect a negative association, and red reflect positive association. Intensity of color indicates −log10 <em>p</em> value.”</p></figcaption>
</figure>
<figure>
<p><img alt="Docherty et al 2017: “Figure 3: Genetic Overlap and Co-Heritability of GPS in European Sample (EUR). Heatmap of partial correlation coefficients between GPS with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱✱ = q-value < 0.0001, ✱✱✱ = q-value < 0.001, ✱✱ = q value < 0.01, ✱ = q value < 0.05, and ~ = suggestive significance at q value < 0.16. Blue values reflect a negative correlation, and red reflect positive correlation.”" data-aspect-ratio="8 / 7" decoding="async" height="805" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-docherty-figure3-rgs.jpg" width="920"></p>
<figcaption><p><span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>: “Figure 3: Genetic Overlap and Co-Heritability of GPS in European Sample (EUR). Heatmap of partial correlation coefficients between GPS with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱✱ = <em>q</em>-value &lt; 0.0001, ✱✱✱ = <em>q</em>-value &lt; 0.001, ✱✱ = <em>q</em> value &lt; 0.01, ✱ = <em>q</em> value &lt; 0.05, and ~ = suggestive significance at <em>q</em> value &lt; 0.16. Blue values reflect a negative correlation, and red reflect positive correlation.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s41467-017-00934-5" id="joshi-et-al-2017-1" data-link-icon="n" data-link-icon-type="text" title="'Genome-wide meta-analysis associates HLA-DQA1/DRB1 and LPA and lifestyle factors with human longevity', Joshi et al 2017">“Genome-wide meta-analysis associates <em>HLA-DQA1/DRB1</em> and <em>LPA</em> and lifestyle factors with human longevity”</a>, <span><span title="et al">Joshi</span><span> et al </span><span>2017</span></span>:</p>
<figure>
<p><img alt="“Figure 5: Genetic correlations between trait clusters that associate with mortality. The upper panel shows whole genetic correlations, the lower panel, partial correlations. T2D, type 2 diabetes; BP, blood pressure; BC, breast cancer; CAD, coronary artery disease; Edu, educational attainment; RA, rheumatoid arthritis; AM, age at menarche; DL/WHR Dyslipidemia/Waist-Hip ratio; BP, blood pressure”" data-aspect-ratio="102 / 179" decoding="async" height="1790" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-joshi-figure5-mortalityrgs.jpg" width="1020"></p>
<figcaption><p>“Figure 5: Genetic correlations between trait clusters that associate with mortality. The upper panel shows whole genetic correlations, the lower panel, partial correlations. T2D, type 2 diabetes; <a href="https://en.wikipedia.org/wiki/Bipolar_disorder" id="_2AmaRRtd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Bipolar_disorder#bodyContent" title="Bipolar disorders research">BP</a>, blood pressure; BC, breast cancer; CAD, coronary artery disease; Edu, educational attainment; RA, rheumatoid arthritis; AM, age at menarche; DL/WHR Dyslipidemia/Waist-Hip ratio; BP, blood pressure”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s41380-017-0001-5" id="hill-et-al-2018-1" data-link-icon="n" data-link-icon-type="text" title="'A combined analysis of genetically correlated traits identifies 187 loci and a role for neurogenesis and myelination in intelligence', Hill et al 2018">“A combined analysis of genetically correlated traits identifies 187 loci and a role for neurogenesis and myelination in intelligence”</a>, <span><span title="et al">Hill</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="“Fig. 4: Heat map showing the genetic correlations between the meta-analytic intelligence phenotype, intelligence, education with 29 cognitive, SES, mental health, metabolic, health and wellbeing, anthropometric, and reproductive traits. Positive genetic correlations are shown in green and negative genetic correlations are shown in red. Statistical-significance following FDR (using Benjamini-Hochberg procedure [51]) correction is indicated by an asterisk.”" data-aspect-ratio="115 / 37" decoding="async" height="296" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-hill-figure5-intelligencergs.jpg" width="920"></p>
<figcaption><p>“Fig. 4: Heat map showing the genetic correlations between the meta-analytic intelligence phenotype, intelligence, education with 29 cognitive, SES, mental health, metabolic, health and wellbeing, anthropometric, and reproductive traits. Positive genetic correlations are shown in green and negative genetic correlations are shown in red. Statistical-significance following FDR (using Benjamini-Hochberg procedure [51]) correction is indicated by an asterisk.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/500090.full" id="watanabe-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'A global overview of pleiotropy and genetic architecture in complex traits', Watanabe et al 2018">“A global view of pleiotropy and genetic architecture in complex traits”</a>, <span><span title="et al">Watanabe</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="Watanabe et al 2018: “Fig. 2. Within and between domains genetic correlations. (a.) Proportion of trait pairs with significant rg (top) and average |_rg_| for significant trait pairs (bottom) within domains. Dashed lines represent the proportion of trait pairs with significant rg (top) and average |rg| for significant trait pairs (bottom) across all 558 traits, respectively. Connective tissue, muscular and infection domains are excluded as these each contains less than 3 traits. (b.) Heatmap of proportion of trait pairs with significant rg (upper right triangle) and average |rg| for significant trait pairs (lower left triangle) between domains. Connective tissue, muscular and infection domains are excluded as each contains less than 3 traits. The diagonal represents the proportion of trait pairs with significant rg within domains. Stars denote the pairs of domains in which the majority (>50%) of significant rg are negative.”" data-aspect-ratio="632 / 1001" decoding="async" height="2002" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-watanabe-figure2-rgs.png" width="1264"></p>
<figcaption><p><span><span title="et al">Watanabe</span><span> et al </span><span>2018</span></span>: “Fig. 2. Within and between domains genetic correlations. (a.) Proportion of trait pairs with significant <em>r<sub>g</sub></em> (top) and average |_r<sub>g_</sub>| for significant trait pairs (bottom) within domains. Dashed lines represent the proportion of trait pairs with significant <em>r<sub>g</sub></em> (top) and average |<em>r<sub>g</sub></em>| for significant trait pairs (bottom) across all 558 traits, respectively. Connective tissue, muscular and infection domains are excluded as these each contains less than 3 traits. (b.) Heatmap of proportion of trait pairs with significant <em>r<sub>g</sub></em> (upper right triangle) and average |<em>r<sub>g</sub></em>| for significant trait pairs (lower left triangle) between domains. Connective tissue, muscular and infection domains are excluded as each contains less than 3 traits. The diagonal represents the proportion of trait pairs with significant <em>r<sub>g</sub></em> within domains. Stars denote the pairs of domains in which the majority (&gt;50%) of significant <em>r<sub>g</sub></em> are negative.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/457515.full" id="abdellaoui-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Genetic Consequences of Social Stratification in Great Britain', Abdellaoui et al 2018">“Genetic Consequences of Social Stratification in Great Britain”</a>, <span><span title="et al">Abdellaoui</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="Abdellaoui et al 2018: “Figure 6: Genetic correlations based on LD score regression. Colored is significant after FDR correction. The green numbers in the left part of the Figure below the diagonal of 1’s are the phenotypic correlations between the regional outcomes of coal mining, religiousness, and regional political preference. The blue stars next to the trait names indicate that UK Biobank was part of the GWAS of the trait.”" data-aspect-ratio="1400 / 643" decoding="async" height="643" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-abdellaoui-figure6-geneticcorrelations-politicsminingreligion.png" width="1400"></p>
<figcaption><p><span><span title="et al">Abdellaoui</span><span> et al </span><span>2018</span></span>: “Figure 6: Genetic correlations based on LD score regression. Colored is significant after FDR correction. The green numbers in the left part of the Figure below the diagonal of 1’s are the phenotypic correlations between the regional outcomes of coal mining, religiousness, and regional political preference. The blue stars next to the trait names indicate that UK Biobank was part of the GWAS of the trait.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s42003-019-0290-0" id="_OpoaL0TI" data-link-icon="n" data-link-icon-type="text">“Identification of 12 genetic loci associated with human healthspan”</a>, <span><span title="et al">Zenin</span><span> et al </span><span>2019</span></span>:</p>
<figure>
<p><img alt="“Figure 4. 35 traits with significant and high genetic correlations with healthspan (|rg| ≥ 0.3; p ≤ 4.3 × 10−5). PMID references are placed in square brackets. Note the absence of genetic correlation between the healthspan and Alzheimer disease traits (rg = −0.03)”" data-aspect-ratio="340 / 261" decoding="async" height="783" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2019-zenin-figure5-35rgs.png" width="1020"></p>
<figcaption><p>“Figure 4. 35 traits with significant and high genetic correlations with healthspan (|<em>r<sub>g</sub></em>| ≥ 0.3; <em>p</em> ≤ 4.3 × 10<sup>−5</sup>). PMID references are placed in square brackets. Note the absence of genetic correlation between the healthspan and Alzheimer disease traits (<em>r<sub>g</sub></em> = −0.03)”</p></figcaption>
</figure></li>
<li><p><a href="https://gwern.net/doc/genetics/heritable/correlation/2019-liu.pdf" id="liu-et-al-2019-4" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1556548" data-filesize-percentage="81" title="'Association studies of up to 1.2 million individuals yield new insights into the genetic etiology of tobacco and alcohol use', Liu et al 2019">“Association studies of up to 1.2 million individuals yield new insights into the genetic etiology of tobacco and alcohol use”</a>, <span><span title="et al">Li</span><span> et al </span><span>2019</span></span>:</p>
<figure>
<p><img alt="Liu et al 2019: “Fig. 1 | Genetic correlations between substance use phenotypes and phenotypes from other large GWAS. Genetic correlations between each of the phenotypes are shown in the first 5 rows, with heritability estimates displayed down the diagonal. All genetic correlations and heritability estimates were calculated using LD score regression. Purple shading represents negative genetic correlations, and red shading represents positive correlations, with increasing color intensity reflecting increasing correlation strength. A single asterisk reflects a significant genetic correlation at the p < 0.05 level. Double asterisks reflect a significant genetic correlation at the Bonferroni-correction p < 0.000278 level (corrected for 180 independent tests). Note that SmkCes was oriented such that higher scores reflected current smoking, and for AgeSmk, lower scores reflect earlier ages of initiation, both of which are typically associated with negative outcomes.”" data-aspect-ratio="947 / 1121" decoding="async" height="1121" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2019-liu-figure1-drugusergs.png" width="947"></p>
<figcaption><p><span><span title="et al">Liu</span><span> et al </span><span>2019</span></span>: “Fig. 1 | Genetic correlations between substance use phenotypes and phenotypes from other large GWAS. Genetic correlations between each of the phenotypes are shown in the first 5 rows, with heritability estimates displayed down the diagonal. All genetic correlations and heritability estimates were calculated using LD score regression. Purple shading represents negative genetic correlations, and red shading represents positive correlations, with increasing color intensity reflecting increasing correlation strength. A single asterisk reflects a significant genetic correlation at the <em>p</em> &lt; 0.05 level. Double asterisks reflect a significant genetic correlation at the Bonferroni-correction <em>p</em> &lt; 0.000278 level (corrected for 180 independent tests). Note that <code>SmkCes</code> was oriented such that higher scores reflected current smoking, and for <code>AgeSmk</code>, lower scores reflect earlier ages of initiation, both of which are typically associated with negative outcomes.”</p></figcaption>
</figure></li>
</ul>

</section>
</section>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Sometimes paraphrased as “All good things tend to go together, as do all bad ones”.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span><span>Tibshirani</span><span>2014</span></span>:</p>
<blockquote>
<p>In describing some of this work, <a href="https://gwern.net/doc/www/hastie.su.domains/dd44844dfc0ecce9c57a6d334ba02f08b75221dd.pdf#page=630" id="_C9ypmOog" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/hastie.su.domains/dd44844dfc0ecce9c57a6d334ba02f08b75221dd.pdf#page=630" data-url-original="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf#page=630" title="_The Elements of Statistical Learning_: §16.2.2, 'The `Bet on Sparsity` Principle'"><span><span title="et al">Hastie</span><span> et al </span><span>2001</span></span></a> coined the informal “Bet on Sparsity” principle [“Use a procedure that does well in sparse problems, since no procedure does well in dense problems.”]. The ℓ<sub>1</sub> methods assume that the truth is sparse, in some basis. If the assumption holds true, then the parameters can be efficiently estimated using ℓ<sub>1</sub> penalties. If the assumption does not hold—so that the truth is dense—then no method will be able to recover the underlying model without a large amount of data per parameter. This is typically not the case when <em>p</em> ≫ <em>N</em>, a commonly occurring scenario.</p>
</blockquote>
<p>This can be seen as a kind of decision-theoretic justification for Occam-style assumptions: if the real world is not predictable in the sense of being predictable by simple/fast algorithms, or induction doesn’t work at all, then no method works in expectation, and the “regret” (difference between <a href="https://en.wikipedia.org/wiki/Expected_value" id="_4Jur8wYN" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Expected_value#bodyContent" title="Expected value">expected value</a> of actual decision and expected value of optimal decision) from mistakenly assuming that the world is simple/sparse is zero. So one should assume the world is simple.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A machine learning practitioner as of 2019, will be struck by the thought that Tobler’s first law nicely encapsulates the principle behind the “unreasonable effectiveness” of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" id="_ZQFNQqCj" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Convolutional_neural_network#bodyContent" title="Convolutional neural network">convolutions in applications of neural networks</a> to so many domains far beyond images; this connection has been made by <a href="https://gwern.net/doc/www/blogs.loc.gov/01d254a360a477ee11c3b50503eba8af26d5544a.html" id="_2jsEjQNz" data-url-archive="/doc/www/blogs.loc.gov/01d254a360a477ee11c3b50503eba8af26d5544a.html" data-url-original="https://blogs.loc.gov/maps/2016/04/alphago-neural-networks-and-toblers-first-law/" data-filesize-bytes="3552758" data-filesize-percentage="76" title="[Computing Space VIII] Games Cartographers Play: AlphaGo, Neural Networks and Tobler's First Law">John Hessler</a>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The most interesting example of this is ESP/psi parapsychology research: the more rigorously conducted the ESP experiments are, the smaller the effects become—but, while discrediting all claims of human ESP, frequently they aren’t pushed to <em>exactly</em> zero and are “statistically-significant”. <a href="https://gwern.net/modus" id="gwern-modus" data-filesize-bytes="110019" data-filesize-percentage="58" title="'One Man’s Modus Ponens', Gwern 2012">There must be</a> some residual crud factor in the experiments, even when conducted &amp; analyzed as best as we know how.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span><span>Gosset</span><span>1904</span></span> has been discussed in several sources, like <a href="https://gwern.net/doc/statistics/decision/1939-pearson.pdf" id="pearson-1939" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="5273313" data-filesize-percentage="92" title="'Student' as Statistician"><span><span>Pearson</span><span>1939</span></span></a>.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The version in the second edition, <a href="https://gwern.net/doc/statistics/decision/1972-savage-foundationsofstatistics.pdf#page=270" id="_5H-0IOn3" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="28637068" data-filesize-percentage="97"><em>The Foundations of Statistics</em>, 2<sup>nd</sup> edition, <span><span>Savage</span><span>1972</span></span></a>, is identical to the first.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Note: “I. Richard Savage” is not to be confused with his brother, <em>Leonard Jimmie</em> Savage, who also worked in <a href="https://en.wikipedia.org/wiki/Bayesian_statistics" id="_yza4YDhb" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Bayesian_statistics#bodyContent" title="Bayesian statistics">Bayesian statistics</a> &amp; is cited previously.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://gwern.net/doc/statistics/decision/1986-lehmann-testingstatisticalhypotheses.pdf" id="_FW_geoDn" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="16264064" data-filesize-percentage="96">2nd edition, 1986</a>; after skimming the 2<sup>nd</sup> edition, I have not been able to find a relevant passage, but Lehmann remarks that he substantially rewrote the textbook for a more robust decision-theoretic approach, so it may have been removed.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>This analysis was never published, according to <a href="#meehl-1990-1"><span><span>Meehl</span><span>1990a</span></span></a>.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>I would note there is <a href="https://gwern.net/note/statistic#expectations-are-not-expected-deviations-and-large-number-of-variables-are-not-large-samples" id="gwern-note-statistic--expectations-are-not-expected-deviations-and-large-number-of-variables-are-not-large-samples" data-filesize-bytes="456894" data-filesize-percentage="88">a dangerous fallacy here</a> even if one does believe the Law of Large Numbers should apply here with an expectation of zero effect: even if the expectation of the pairwise correlation of 2 arbitrary variables was in fact precisely zero (as is not too implausible in some domains such as optimization or feedback loops—such as the famous example of the thermostat/room-temperature), that does not mean any specific pair will be exactly zero no matter how many numbers get added up to create their relationship, as the absolute size of the deviation increases.</p>
<p>So for example, imagine 2 genetic traits which may be genetically-correlated, and their heritability may be caused by a number of genes ranging from 1 (monogenic) to tens of thousands (highly polygenic); the specific overlap is created by a chance draw of evolutionary processes throughout the organism’s evolution; does the Law of Large Numbers justify saying that while 2 monogenic traits may have a substantial correlation, 2 highly polygenic traits must have much closer to zero correlation simply because they are influenced by more genes? No, because the distribution around the expectation of 0 can become wider &amp; wider the more relevant genes there are.</p>
<p>To reason otherwise is, as Samuelson noted, to think like an insurer who is worried about losing $100 on an insurance contract so it goes out &amp; makes 100 more $100 contracts.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span><span>Betz</span><span>1986</span></span> special issue’s contents:</p>
<ol>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson.pdf" id="gottfredson-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="280177" data-filesize-percentage="43" title="'The <em>g</em> factor in employment', Gottfredson 1986">“The <em>g</em> factor in employment”</a>, <span><span>Gottfredson</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-avery.pdf" id="avery-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="292550" data-filesize-percentage="44" title="'Origins of and Reactions to the PTC conference on 'The <em>g</em> Factor In Employment Testing'', Avery 1986">“Origins of and Reactions to the PTC conference on <em>The <em>g</em> Factor In Employment Testing</em>”</a>, <span><span>Avery</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-jensen-2.pdf" id="jensen-1986b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2349355" data-filesize-percentage="86" title="'g: Artifact or Reality?', Jensen 1986">“<em>g</em>: Artifact or reality?”</a>, <span><span>Jensen</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-thorndike.pdf" id="thorndike-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="588128" data-filesize-percentage="62" title="'The role of general ability in prediction', Thorndike 1986">“The role of general ability in prediction”</a>, <span><span>Thorndike</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-hunter.pdf" id="hunter-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1615904" data-filesize-percentage="82" title="'Cognitive ability, cognitive aptitudes, job knowledge, and job performance', Hunter 1986">“Cognitive ability, cognitive aptitudes, job knowledge, and job performance”</a>, <span><span>Hunter</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson-2.pdf" id="gottfredson-crouse-1986b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1224257" data-filesize-percentage="78" title="'Validity versus utility of mental tests: Example of the SAT', Gottfredson &amp; Crouse 1986b">“Validity versus utility of mental tests: Example of the SAT”</a>, <span><span>Gottfredson &amp; Crouse</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson-3.pdf" id="gottfredson-1986c" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2429864" data-filesize-percentage="86" title="'Societal consequences of the <em>g</em> factor in employment', Gottfredson 1986c">“Societal consequences of the <em>g</em> factor in employment”</a>, <span><span>Gottfredson</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-hawk.pdf" id="hawk-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="284066" data-filesize-percentage="43" title="'Real world implications of g', Hawk 1986">“Real world implications of <em>g</em>”</a>, <span><span>Hawk</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-arvey.pdf" id="arvey-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="396905" data-filesize-percentage="52" title="'General ability in employment: A discussion', Arvey 1986">“General ability in employment: A discussion”</a>, <span><span>Arvey</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-humphreys.pdf" id="humphreys-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1245324" data-filesize-percentage="78" title="'Commentary [on 'The _g: factor in employment special issue']', Humphreys 1986">“Commentary”</a>, <span><span>Humphreys</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-linn.pdf" id="linn-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="507027" data-filesize-percentage="59" title="'Comments on the <em>g</em> factor in employment testing', Linn 1986">“Comments on the <em>g</em> factor in Employment Testing”</a>, <span><span>Linn</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-tyler.pdf" id="tyler-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="411770" data-filesize-percentage="53" title="'Back to Spearman?', Tyler 1986">“Back to Spearman?”</a>, <span><span>Tyler</span><span>1986</span></span></p></li>
</ol>
<a href="#fnref11" role="doc-backlink">↩︎</a></li>
<li id="fn12"><p>This work does not seem to have been published, as I can find no books published by them jointly, or nor nay McClosky books published between <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span> &amp; his death in <span>2004<sub><span title="2004 was 21 years ago.">21ya</span></sub></span>.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>For definitions &amp; evidence for, see: <a href="https://gwern.net/doc/genetics/heritable/correlation/1988-cheverud.pdf" id="_M3xg-5cz" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1427491" data-filesize-percentage="80" title="A comparison of genetic and phenotypic correlations"><span><span>Cheverud</span><span>1988</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/1995-roff.pdf" id="technologies-1995" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="748280" data-filesize-percentage="68" title="The estimation of genetic correlations from phenotypic correlations---a test of Cheverud's conjecture"><span><span>Roff</span><span>1996</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/2008-kruuk.pdf" id="_CTwhZi2G" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1558571" data-filesize-percentage="81" title="New answers for old questions: The evolutionary quantitative genetics of wild animal populations"><span><span title="et al">Kruuk</span><span> et al </span><span>2008</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/2011-dochtermann.pdf" id="dochtermann-2011" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="176446" data-filesize-percentage="30" title="Testing Cheverud's conjecture for behavioral correlations and behavioral syndromes"><span><span>Dochtermann</span><span>2011</span></span></a>, <a href="https://www.biorxiv.org/content/10.1101/311332.full" id="jordan-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="The landscape of pervasive horizontal pleiotropy in human genetic variation is driven by extreme polygenicity of human traits and diseases"><span><span title="et al">Jordan</span><span> et al </span><span>2018</span></span></a>, &amp; <a href="https://www.biorxiv.org/content/10.1101/291062.full" id="sodini-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="Comparison of Genotypic and Phenotypic Correlations: Cheverud's Conjecture in Humans"><span><span title="et al">Sodini</span><span> et al </span><span>2018</span></span></a>.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

      
      
      <section id="backlinks-section">
        <h2><a href="#backlinks-section" title="Link to section: § 'Backlinks'">Backlinks</a></h2>
        <a id="backlinks" href="https://gwern.net/metadata/annotation/backlink/%252Feverything.html" title="Reverse citations/backlinks for this page (the list of other pages which link to this page)." data-link-icon="arrows-pointing-inwards-to-dot" data-link-icon-type="svg">[Backlinks (what links here)]</a>
      </section>
      <section id="similars-section">
        <h2><a href="#similars-section" title="Link to section: § 'Similar Links'">Similar Links</a></h2>
        <a id="similars" href="https://gwern.net/metadata/annotation/similar/%252Feverything.html" title="Similar links for this link (by text embedding). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="≈" data-link-icon-type="text">[Similar links by topic]</a>
        </section>
      <section id="link-bibliography-section">
        <h2><a href="#link-bibliography-section" title="Link to section: § 'Bibliography'">Bibliography</a></h2> <!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
        <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Feverything.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="bibliography" data-link-icon-type="svg">[Bibliography of links/references used in page]</a>
          </section>
      
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Control shopping cart wheels with your phone (2021) (202 pts)]]></title>
            <link>https://www.begaydocrime.com/</link>
            <guid>44980004</guid>
            <pubDate>Fri, 22 Aug 2025 00:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.begaydocrime.com/">https://www.begaydocrime.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44980004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
                <div>
                    <div>
                        <h2>About</h2><p>
                        Play the below sounds through your phone speaker and hold it next to a Gatekeeper Systems wheels to lock/unlock. Check me out on twitter @stoppingcart 
                    </p></div>
                    
                    <div>
                        <h2>How It Works</h2><p>
                        Most electronic shopping cart wheels listen for a 7.8 kHz signal from an underground wire to know when to lock and unlock. A management remote can send a different signal at 7.8 kHz to the wheel to unlock it.
        
                        Since 7.8 kHz is in the audio range, you can use the parasitic EMF from your phone's speaker to "transmit" a similar code by playing a crafted audio file. 
                    </p></div>

                    <p><a href="https://www.youtube.com/watch?v=fBICDODmCPI">Link to my original DEFCON 29 Talk</a>

                </p></div>
                <div>
                    <p>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/fBICDODmCPI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                    </p>
                </div>
            </div>
            
            

			<p><a href="https://www.counter12.com/"><img src="https://www.counter12.com/img-ZAZ7zcybZcwWdDzB-33.gif" alt="free counter"></a></p>
         
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google scores six-year Meta cloud deal worth over $10B (101 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html</link>
            <guid>44979855</guid>
            <pubDate>Fri, 22 Aug 2025 00:34:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html">https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html</a>, See on <a href="https://news.ycombinator.com/item?id=44979855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108189400" data-test="InlineImage"><p>Meta CEO Mark Zuckerberg makes a keynote speech at the Meta Connect annual event at the company's headquarters in Menlo Park, Calif., on Sept. 25, 2024.</p><p>Manuel Orbegozo | Reuters</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> has agreed to spend more than $10 billion on <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> cloud services, according to two people familiar with the matter.</p><p>The agreement spans six years, said the people, who asked not to be named because the terms are confidential. The deal was reported earlier by <a href="https://www.theinformation.com/articles/meta-signs-10-billion-plus-cloud-deal-google" target="_blank">The Information</a>.</p><p>Google is aiming to land big cloud contracts as it chases larger rivals <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-4"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Web Services and <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Azure in cloud infrastructure. Earlier this year Google <a href="https://www.cnbc.com/2025/07/16/openai-googles-cloud-chatgpt.html">won cloud business</a> from OpenAI, which had earlier been deeply dependent on Microsoft's Azure infrastructure.</p><p>Alphabet said in July that the Google Cloud unit, which contains productivity software subscriptions in addition to infrastructure, produced $2.83 billion in operating income on $13.6 billion in revenue during the second quarter. Revenue growth of 32% outpaced expansion of 13.8% for the company as a whole.</p><p>Meta's deal with Google is mainly around artificial intelligence infrastructure, said one of the people. Meta said in its <a href="https://www.cnbc.com/2025/07/30/metas-big-ai-spending-blitz-will-continue-into-2026-.html">earnings report</a> last month that it expects total expenses for 2025 to come in the range of $114 billion and $118 billion. It's investing heavily in AI infrastructure and talent, building out its Llama family of models and adding AI across its portfolio of services. &nbsp;</p><p>Meta and Google have long been rivals in online ads. But Meta needs all the cloud infrastructure it can access. The company operates data centers and <a href="https://www.datacenterdynamics.com/en/news/metafacebook-turns-to-aws-as-long-term-strategic-cloud-provider-for-acquisitions-third-party-collaborations-and-ai/" target="_blank">has made</a> commitments to use cloud services from <a href="https://www.datacenterdynamics.com/en/news/metafacebook-turns-to-aws-as-long-term-strategic-cloud-provider-for-acquisitions-third-party-collaborations-and-ai/" target="_blank">Amazon</a> and <a href="https://azure.microsoft.com/en-us/blog/meta-selects-azure-as-strategic-cloud-provider-to-advance-ai-innovation-and-deepen-pytorch-collaboration/" target="_blank">Microsoft</a>.</p><p>Google declined to comment. </p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/08/21/antitrust-ruling-could-end-googleas-26-billion-default-deals-but-experts-see-upside-for-ai.html">Antitrust ruling could end Google’s $26 billion default deals, but experts see upside for AI</a></p></div><div id="Placeholder-ArticleBody-Video-108189198" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000386412" aria-labelledby="Placeholder-ArticleBody-Video-108189198"><p><img src="https://image.cnbcfm.com/api/v1/image/108189205-1755794550294-1755794011-41285020012-hd.jpg?v=1755794587&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Antitrust ruling could end Google’s $26 billion default deals, but experts see upside for AI"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From GPT-4 to GPT-5: Measuring progress through MedHELM [pdf] (110 pts)]]></title>
            <link>https://www.fertrevino.com/docs/gpt5_medhelm.pdf</link>
            <guid>44979107</guid>
            <pubDate>Thu, 21 Aug 2025 22:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fertrevino.com/docs/gpt5_medhelm.pdf">https://www.fertrevino.com/docs/gpt5_medhelm.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44979107">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Onion Brought Back Its Print Edition. The Gamble Is Paying Off (169 pts)]]></title>
            <link>https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c</link>
            <guid>44978869</guid>
            <pubDate>Thu, 21 Aug 2025 22:28:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c">https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c</a>, See on <a href="https://news.ycombinator.com/item?id=44978869">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists No Longer Find X Professionally Useful, and Have Switched to Bluesky (125 pts)]]></title>
            <link>https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&amp;login=false</link>
            <guid>44978815</guid>
            <pubDate>Thu, 21 Aug 2025 22:22:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&#x26;login=false">https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&#x26;login=false</a>, See on <a href="https://news.ycombinator.com/item?id=44978815">Hacker News</a></p>
Couldn't get https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&login=false: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[CEO pay and stock buybacks have soared at the 100 largest low-wage corporations (212 pts)]]></title>
            <link>https://ips-dc.org/report-executive-excess-2025/</link>
            <guid>44978655</guid>
            <pubDate>Thu, 21 Aug 2025 22:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ips-dc.org/report-executive-excess-2025/">https://ips-dc.org/report-executive-excess-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44978655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
<h2>Introduction</h2>



<p>This 31st annual Institute for Policy Studies Executive Excess report takes an in-depth look at the 100 S&amp;P 500 corporations with the lowest median worker pay, a group we have dubbed the “Low-Wage 100.” For each of these companies, we analyze CEO and worker pay trends since 2019. We also compare, for the past six years, what these companies have spent on stock buybacks to pump up short-term share prices with what they have invested in long-term capital improvements.</p>



<p>The report’s overall finding: At a time when many American workers are struggling with high costs for groceries and housing, the nation’s largest low-wage employers are fixated on making their overpaid CEOs even richer.</p>



<p>Our analysis ends with realizable policy solutions for pushing Corporate America in a more equitable direction.</p>



<p>For detailed analysis , including our methodology, see the full PDF below. A summary follows.</p>


            
        


<div>
<h2>Key findings</h2>



<ol>
<li><strong>CEO pay at Low-Wage 100 firms has soared since 2019 while median worker pay has lagged behind U.S. inflation.</strong>
<ul>
<li>Between 2019 and 2024, average CEO compensation within this group rose 34.7 percent in nominal — unadjusted for inflation — terms, more than double the 16.3 percent increase in these firms’ average median worker pay. The U.S. inflation rate over this same period: 22.6 percent.</li>



<li>Average CEO compensation within the Low-Wage 100 hit $17.2 million in 2024. The group’s average median worker pay sat at just $35,570.</li>



<li><strong>The average CEO-worker pay ratio of Low-Wage 100 firms has widened by 12.9 percent, from 560 to 1 in 2019 to 632 to 1 in 2024.</strong></li>



<li><strong>The nominal value of median pay actually <em>fell</em> at 22 Low-Wage 100 corporations during this period.</strong></li>



<li><strong>The Starbucks pay gap hit 6,666 to 1 last year</strong>, the Low-Wage 100’s widest spread by far. In 2024, the Starbucks CEO pocketed $95.8 million.<strong> </strong>Over the past six years, amid worker discontent fueling union-organizing drives at hundreds of Starbucks stores, the firm’s median pay rose just 4.2 percent in real terms to $14,674. Only seven S&amp;P 500 firms have lower median pay.</li>



<li><strong>Ulta Beauty reported the Low-Wage 100’s steepest drop in median pay</strong>. Between 2019 and 2024, a period when the cosmetic retailer significantly expanded the part-time worker share of its workforce, the company’s real median pay plunged by 46 percent to $11,078.</li>
</ul>
</li>



<li><strong>From 2019 through 2024, the Low-Wage 100 spent $644 billion on stock buybacks.</strong>
<ul>
<li><strong>Over the past six years, all but three Low-Wage 100 firms spent corporate dollars on stock buybacks</strong>. By repurchasing their own shares, companies artificially inflate executive stock-based pay and siphon resources out of worker wages and productive long-term investments.</li>



<li><strong>Lowe’s ranks as the Low-Wage 100’s buyback leader. The company spent $46.6 billion on share repurchases from 2019 through 2024</strong>. Over that span, this sum could have funded an annual $28,456 bonus for each of the firm’s 273,000 employees — or added 88 employees to each of the firm’s retail outlets. <strong>In 2024, Lowe’s CEO Marvin Ellison enjoyed a total compensation of $20.2 million —</strong> <strong>659 times more than the retailer’s $30,606 median annual worker pay</strong>.</li>



<li><strong>Home Depot currently sits second in the Low-Wage 100 buyback rankings</strong>. The big-box chain spent $37.9 billion on share repurchases between 2019 and 2024. That outlay would have been enough to give each of Home Depot’s 470,100 global employees six annual $13,423 bonuses. The Home Depot median pay: just $35,196.</li>
</ul>
</li>



<li><strong>From 2019 through 2024, a majority of Low-Wage 100 firms spent more on stock buybacks than on long-term capital expenditures.</strong>
<ul>
<li>Over the past six years, <strong>56 Low-Wage 100 companies plowed more corporate cash into buying back their own shares of stock than investing in capital improvements</strong>.</li>



<li>If we exclude capital expenditure outlier Amazon from the calculation, <strong>the Low-Wage 100 as a whole spent more on buybacks than on “CapEx” during this period</strong>.</li>
</ul>
</li>



<li><strong>At least 32 billionaires owe their wealth to Low-Wage 100 companies.</strong>
<ul>
<li>Five of these firms have spawned multiple billionaires still living today: Walmart (eight), Estee Lauder (four), DoorDash (three), Public Storage (two), and Tyson Foods (two).</li>
</ul>
</li>



<li><strong>Policy changes can prevent wasteful stock buybacks and excessive CEO payouts.</strong>
<ul>
<li><strong>Taxing extreme CEO-worker pay gaps: </strong>In<strong> </strong><a href="https://www.filesforprogress.org/datasets/2024/4/dfp_progressive_leg_agenda_tabs.pdf">one recent survey</a>, 80 percent of likely voters expressed support for a tax hike on corporations that pay their CEO over 50 or more times what they pay their median employees.</li>



<li><strong>Increasing the buybacks tax:</strong> If Congress in 2022 had set our current <a href="https://inequality.org/research/congress-takes-historic-step-to-tax-stock-buybacks/">1 percent excise tax</a> on stock buybacks at 4 percent, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on share repurchases in 2023 and 2024.</li>



<li><strong>Restricting buybacks and CEO pay through federal contracts and subsidies:</strong> The Biden administration made modest progress on this front through the CHIPS semiconductor subsidy program. But the federal government could be doing much more to leverage the power of the public purse against wasteful stock buybacks and excessive CEO pay.</li>
</ul>
</li>
</ol>
</div>



<h2><strong>Recommendations</strong></h2>



<p>We highlight three particularly promising areas for CEO pay policy reform.</p>



<h4>Subjecting corporations with excessive levels of CEO pay to higher tax levies.</h4>



<p>Higher tax rates on companies with wide CEO-worker pay gaps would create an incentive to both rein in executive pay and raise worker wages, all the while generating significant new capital for vital public investments. Laws that share those goals are already generating revenue in <a href="https://inequality.org/action/corporate-pay-equity/">two major cities</a>, San Francisco and Portland, Oregon.</p>



<p>Members of the U.S. Congress have also introduced several related bills, including:</p>



<p>The<strong> Curtailing Executive Overcompensation (CEO) Act: </strong>This <a href="https://www.congress.gov/bill/118th-congress/senate-bill/3176">bill</a> applies an excise tax to publicly traded and private companies with CEO-to-median-worker pay disparities that run over 50 to 1. Under this excise tax formula, the tax owed would be proportional to the degree a company’s pay ratio exceeds 50 to 1 and to the level of the CEO’s compensation. In other words, companies with large pay gaps would owe extra taxes — and those companies with extremely high CEO pay would owe Uncle Sam even more.</p>



<p>Revenue estimate: This bill, had it been in effect in 2022, would have raised <a href="https://www.whitehouse.senate.gov/news/release/whitehouse-lee-ocasio-cortez-introduce-legislation-to-increase-worker-pay-rein-in-runaway-ceo-compensation/">over $10 billion</a> in annual revenue from the <em>Fortune</em> 100 largest U.S. companies alone.</p>



<p>The<strong> Tax Excessive CEO Pay Act:</strong> This <a href="https://www.congress.gov/bill/118th-congress/senate-bill/3620?q=%7B%22search%22%3A%22Tax+Excessive+CEO+Pay+Act%22%7D&amp;s=1&amp;r=3">legislation</a> ties a company’s federal corporate tax rate to the size of the gap between its CEO and median worker pay. Tax penalties would begin at 0.5 percentage points for companies that pay their top executives between 50 and 100 times more than their median workers. The highest penalty would apply to companies that pay top executives over 500 times worker pay.</p>



<p>Revenue estimate: <a href="https://www.sanders.senate.gov/press-releases/news-sanders-and-colleagues-introduce-legislation-to-combat-corporate-greed-and-end-outrageous-ceo-pay-2/">$150 billion over 10 years</a>. The <a href="https://www.congress.gov/bill/117th-congress/house-bill/3301">CEO Accountability and Responsibility Act</a> imposes similar tax penalties for large ratios and ties the pay ratio to government contracting.</p>



<p>A <a href="https://www.filesforprogress.org/datasets/2024/4/dfp_progressive_leg_agenda_tabs.pdf">May 2024 survey</a> suggests that such taxes would be enormously popular. Overall, 80 percent of likely voters favor a tax hike on corporations that pay their CEOs over 50 or more times more than what they pay their median employees.</p>



<h4>Taxing and restricting stock buybacks.</h4>



<p>A <a href="https://inequality.org/research/congress-takes-historic-step-to-tax-stock-buybacks/">1 percent federal excise tax</a> on the repurchase of corporate stock went into effect in 2023. With nearly $209 billion in combined stock buybacks for 2023 and 2024, the Low-Wage 100 owed approximately $2.1 billion in federal taxes over those two years.</p>



<p>A Senate bill, the <a href="https://www.congress.gov/bill/118th-congress/senate-bill/413?q=%7B%22search%22%3A%22chamberActionDateCode%3A%5C%222023-02-14%7C118%7C10000%5C%22+AND+billIsReserved%3A%5C%22N%5C%22%22%7D&amp;s=1&amp;r=16">Stock Buyback Accountability Act</a>, would quadruple this excise tax. If that 4 percent tax had been in place in 2023 and 2024, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on their share repurchases, assuming no change in their buyback behavior. That increase would be enough to cover the cost of <a href="https://www.nationalpriorities.org/interactive-data/trade-offs/?state=00&amp;program=111111">327,218</a> public housing units each year for two years.</p>



<p>Another Senate bill, the<strong> </strong><a href="https://www.warner.senate.gov/public/index.cfm/2023/3/warner-introduces-legislation-to-encourages-companies-to-prioritize-long-term-investments-over-short-term-gains-by-instituting-holding-periods-for-stock-buybacks">ALIGN Act</a>, would ban executives from selling their shares within a year of a stock buyback announcement. This would prevent CEOs from timing share repurchases to cash in personally on a short-term price pop they themselves have artificially created.</p>



<h4>Using federal contracts and subsidies to discourage wide corporate pay gaps.</h4>



<p>The Biden administration took several steps to use the power of the public purse to discourage CEO pay-inflating stock buybacks. During the Biden White House years, the <a href="https://www.nist.gov/system/files/documents/2023/02/28/CHIPS_NOFO-1_Fact_Sheet_0.pdf">Department of Commerce</a> gave preferential treatment in the awarding of $39 billion in CHIPS subsidies for domestic semiconductor production to firms that committed to refraining from all stock buybacks for five years.</p>



<p>Future administrations could do much more to leverage the power of the public purse against extreme pay disparities. The <a href="https://www.congress.gov/bill/117th-congress/house-bill/4186?s=1&amp;r=5#:~:text=Introduced%20in%20House%20(06%2F25%2F2021)&amp;text=This%20bill%20establishes%20new%20requirements,taxes%2C%20and%20private%20equity%20firms.">Patriotic Corporations Act</a> could serve as a model. This bill would grant preferential treatment in federal contracting to firms with CEO-worker pay ratios of 100 to 1 or less, among other benchmarks. The <a href="https://progressives.house.gov/_cache/files/6/8/68a45450-d818-4c0c-a564-cd85ea395d47/88561AA25D995B4AF31FCC6F40CA177D.cpc-recommendations-for-executive-action-3-17-22.pdf">Congressional Progressive Caucus</a> has supported such incentives.</p>



<p>By encouraging major corporations to narrow their pay gaps, a president can also help ensure that taxpayers get the biggest bang for federal contract bucks. <a href="https://inequality.org/action/corporate-pay-equity/#academic-research">Studies have shown</a> that companies with narrow gaps in CEO-worker compensation tend to perform at higher levels than firms with wide gaps.</p>



<h2>Social Shareables</h2>



<figure></figure>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uv format: Code Formatting Comes to uv (experimentally) (294 pts)]]></title>
            <link>https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/</link>
            <guid>44977645</guid>
            <pubDate>Thu, 21 Aug 2025 20:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/">https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/</a>, See on <a href="https://news.ycombinator.com/item?id=44977645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The latest <a href="https://pydevtools.com/handbook/reference/uv/">uv</a> release (<a href="https://github.com/astral-sh/uv/blob/main/CHANGELOG.md#0813" target="_blank" rel="noopener">0.8.13</a>) quietly introduced an experimental new command that Python developers have been waiting for: <code>uv format</code>. This addition brings code formatting directly into uv’s toolkit, eliminating the need to juggle multiple tools for basic Python development workflows.</p><h2>What is uv format?<span id="what-is-uv-format"></span>
<a href="#what-is-uv-format" aria-label="Permalink for this section"></a></h2><p>The <code>uv format</code> command provides Python code formatting through uv’s interface. Under the hood, it calls <a href="https://pydevtools.com/handbook/reference/ruff/">Ruff</a>’s formatter to automatically style your code according to consistent standards.</p><h2>Getting Started<span id="getting-started"></span>
<a href="#getting-started" aria-label="Permalink for this section"></a></h2><p>First, make sure you’re running uv 0.8.13 or later. If you need to upgrade, check out our guide on <a href="https://pydevtools.com/handbook/how-to/how-to-upgrade-uv/">upgrading uv</a>.</p><p>Once upgraded, formatting your project is straightforward:</p><p>The command works just like running <code>ruff format</code> in your project root, but through uv’s interface.</p><h2>Passing Arguments to Ruff<span id="passing-arguments-to-ruff"></span>
<a href="#passing-arguments-to-ruff" aria-label="Permalink for this section"></a></h2><p>You can pass additional arguments to Ruff by placing them after <code>--</code>:</p><p>This flexibility means you can customize formatting behavior without losing uv’s conveniences.</p><div><p>Warning</p><div><p>Since this is an experimental feature, expect some rough edges:</p><ul><li>The command may change in future releases</li><li>Integration with uv’s project model might evolve</li><li>Error handling and output formatting could improve</li></ul></div></div><p>Try out <code>uv format</code> in your next project and see how it fits into your development workflow. The experimental nature means your feedback could help shape how this feature evolves.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crimes with Python's Pattern Matching (2022) (217 pts)]]></title>
            <link>https://www.hillelwayne.com/post/python-abc/</link>
            <guid>44977189</guid>
            <pubDate>Thu, 21 Aug 2025 19:47:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hillelwayne.com/post/python-abc/">https://www.hillelwayne.com/post/python-abc/</a>, See on <a href="https://news.ycombinator.com/item?id=44977189">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en">
    

    
    

    <div>
  

<p>One of my favorite little bits of python is <code>__subclasshook__</code>. Abstract Base Classes with <code>__subclasshook__</code> can define what counts as a subclass of the ABC, even if the target doesn’t know about the ABC. For example:</p>
<div><pre><code data-lang="python"><span></span><span>class</span> <span>PalindromicName</span><span>(ABC):</span>
   
  <span>@classmethod</span>
  <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>C):</span>
    <span>name</span> <span>=</span> <span>C</span><span>.</span><span>__name__</span><span>.</span><span>lower()</span>
    <span>return</span> <span>name[::</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>name</span>

<span>class</span> <span>Abba</span><span>:</span>
  <span>...</span>

<span>class</span> <span>Baba</span><span>:</span>
  <span>...</span>

<span>&gt;&gt;&gt;</span> <span>isinstance(Abba(),</span> <span>PalindromicName)</span>
<span>True</span>
<span>&gt;&gt;&gt;</span> <span>isinstance(Baba(),</span> <span>PalindromicName)</span>
<span>False</span>
</code></pre></div>

<p>You can do some weird stuff with this. Back in 2019 I used it to create <a href="https://www.hillelwayne.com/negatypes/">non-monotonic types</a>, where something counts as a <code>NotIterable</code> if it <em>doesn’t</em> have the <code>__iter__</code> method. There wasn’t anything too diabolical you could do with this: nothing in Python really interacted with ABCs, limiting the damage you could do with production code.</p>

<p>Then Python 3.10 <a href="https://www.python.org/dev/peps/pep-0634/">added pattern matching</a>.</p>

<p><img src="https://c.tenor.com/_pj7e4VKqDoAAAAC/elmo-burning.gif" alt=""></p>

<h3 id="a-quick-overview-of-pattern-matching">A quick overview of pattern matching</h3>

<p>From the <a href="https://www.python.org/dev/peps/pep-0636/">pattern matching tutorial</a>:</p>
<div><pre><code data-lang="py"><span></span><span>match</span> <span>command</span><span>.</span><span>split():</span>
    <span>case</span> <span>[</span><span>"quit"</span><span>]:</span>
        <span>print(</span><span>"Goodbye!"</span><span>)</span>
        <span>quit_game()</span>
    <span>case</span> <span>[</span><span>"look"</span><span>]:</span>
        <span>current_room</span><span>.</span><span>describe()</span>
    <span>case</span> <span>[</span><span>"get"</span><span>,</span> <span>obj]:</span>
        <span>character</span><span>.</span><span>get(obj,</span> <span>current_room)</span>
</code></pre></div>

<p>You can match on arrays, dictionaries, and custom objects. To support matching objects, Python uses <code>isinstance(obj, class)</code>, which checks</p>

<ol>
<li>If <code>obj</code> is of type <code>class</code></li>
<li>If <code>obj</code> is a transitive subtype of <code>class</code></li>
<li>If <code>class</code> is an ABC and defines a <code>__subclasshook__</code> that matches the type of <code>obj</code>.</li>
</ol>

<p>That made me wonder if ABCs could “hijack” a pattern match. Something like this:</p>
<div><pre><code data-lang="py"><span></span><span>from</span> <span>abc</span> <span>import</span> <span>ABC</span>

<span>class</span> <span>NotIterable</span><span>(ABC):</span>

    <span>@classmethod</span>
    <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>C):</span>
        <span>return</span> <span>not</span> <span>hasattr(C,</span> <span>"__iter__"</span><span>)</span>

<span>def</span> <span>f</span><span>(x):</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>NotIterable():</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is not iterable"</span><span>)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is iterable"</span><span>)</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
    <span>f(</span><span>10</span><span>)</span>
    <span>f(</span><span>"string"</span><span>)</span>
    <span>f([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
</code></pre></div>

<p>But surely Python clamps down on this chicanery, right?</p>

<pre><code>$ py10 abc.py
10 is not iterable
string is iterable
[1, 2, 3] is iterable
</code></pre>

<p>Oh.</p>

<p>Oh my.</p>

<h3 id="making-it-worse">Making it worse</h3>

<p>Pattern matching can also <a href="https://www.python.org/dev/peps/pep-0636/#adding-a-ui-matching-objects">destructure object fields</a>:</p>
<div><pre><code data-lang="py"><span></span><span>match</span> <span>event</span><span>.</span><span>get():</span>
    <span>case</span> <span>Click(position</span><span>=</span><span>(x,</span> <span>y)):</span>
        <span>handle_click_at(x,</span> <span>y)</span>
</code></pre></div>

<p>We can only get the field <em>after</em> we’ve decided the object. We can’t match “any object that has the <code>foo</code> field”… unless we use ABCs.<sup id="fnref:property"><a href="#fn:property">1</a></sup></p>
<div><pre><code data-lang="python"><span></span><span>from</span> <span>abc</span> <span>import</span> <span>ABC</span>
<span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span>
<span>from</span> <span>math</span> <span>import</span> <span>sqrt</span>

<span>class</span> <span>DistanceMetric</span><span>(ABC):</span>

    <span>@classmethod</span>
    <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>C):</span>
        <span>return</span> <span>hasattr(C,</span> <span>"distance"</span><span>)</span>

<span>def</span> <span>f</span><span>(x):</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>DistanceMetric(distance</span><span>=</span><span>d):</span>
            <span>print(d)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is not a point"</span><span>)</span>

<span>@dataclass</span>
<span>class</span> <span>Point2D</span><span>:</span>
    <span>x:</span> <span>float</span>
    <span>y:</span> <span>float</span>

    <span>@property</span>
    <span>def</span> <span>distance</span><span>(self):</span>
        <span>return</span> <span>sqrt(self</span><span>.</span><span>x</span><span>**</span><span>2</span> <span>+</span> <span>self</span><span>.</span><span>y</span><span>**</span><span>2</span><span>)</span>

<span>@dataclass</span>
<span>class</span> <span>Point3D</span><span>:</span>
    <span>x:</span> <span>float</span>
    <span>y:</span> <span>float</span>
    <span>z:</span> <span>float</span>

    <span>@property</span>
    <span>def</span> <span>distance</span><span>(self):</span>
        <span>return</span> <span>sqrt(self</span><span>.</span><span>x</span><span>**</span><span>2</span> <span>+</span> <span>self</span><span>.</span><span>y</span><span>**</span><span>2</span> <span>+</span> <span>self</span><span>.</span><span>z</span><span>**</span><span>2</span><span>)</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
    <span>f(Point2D(</span><span>10</span><span>,</span> <span>10</span><span>))</span>
    <span>f(Point3D(</span><span>5</span><span>,</span> <span>6</span><span>,</span> <span>7</span><span>))</span>
    <span>f([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
</code></pre></div>

<pre><code>14.142135623730951
10.488088481701515
[1, 2, 3] is not a point
</code></pre>

<p>It gets better! While the ABC decides the match, the object decides the destructuring, meaning we can do stuff like this:</p>
<div><pre><code data-lang="py"><span></span><span>def</span> <span>f</span><span>(x):</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>DistanceMetric(z</span><span>=</span><span>3</span><span>):</span>
            <span>print(</span><span>f"A point with a z-coordinate of 3"</span><span>)</span>
        <span>case</span> <span>DistanceMetric(z</span><span>=</span><span>z):</span>
            <span>print(</span><span>f"A point with a z-coordinate that's not 3"</span><span>)</span>
        <span>case</span> <span>DistanceMetric():</span>
            <span>print(</span><span>f"A point without a z-coordinate"</span><span>)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is not a point"</span><span>)</span>
</code></pre></div>

<h3 id="combinators">Combinators</h3>

<p>The pattern matching is flexible but also fairly limited. It can only match on an object’s type, meaning we have to make a separate ABC for each thing we want to test. Fortunately, there’s a way around this. Python is dynamically typed. 99% of the time this just means “you don’t need static types if you’re okay with things crashing at runtime”. But it <em>also</em> means that type information exists at runtime, and that <strong>types can be created at runtime</strong>.</p>

<p>Can we use this for pattern matching? Let’s try it:</p>
<div><pre><code data-lang="py"><span></span><span>def</span> <span>Not</span><span>(cls):</span>
    <span>class</span> <span>_Not</span><span>(ABC):</span>
        <span>@classmethod</span>
        <span>def</span> <span>__subclasshook__</span><span>(_,</span> <span>C):</span>
            <span>return</span> <span>not</span> <span>issubclass(C,</span> <span>cls)</span>
    <span>return</span> <span>_Not</span>

<span>def</span> <span>f</span><span>(x):</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>Not(DistanceMetric)():</span> 
            <span>print(</span><span>f"{</span><span>x</span><span>} is not a point"</span><span>)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is a point"</span><span>)</span>
</code></pre></div>

<p><code>Not</code> is a function that takes a class, defines a <em>new</em> ABC, sets the hook for that ABC to “anything that’s not the class”, and then returns that ABC.</p>

<p>We try this and…</p>

<pre><code>    case Not(DistanceMetric)():
                            ^
SyntaxError: expected ':'
</code></pre>

<p>It’s an error! We’ve finally hit the limits of pattern matching on ABCs. Then again, it’s “just” a syntax error. Maybe it would work if we tweak the syntax a little?</p>
<div><pre><code data-lang="diff"><span></span><span>+   n = Not(DistanceMetric)</span>
<span> </span>   match x:
<span>-       case Not(DistanceMetric)(): </span>
<span>+       case n(): </span>
</code></pre></div>

<pre><code>PlanePoint(x=10, y=10) is a point
SpacePoint(x=5, y=6, z=7) is a point
[1, 2, 3] is not a point
</code></pre>

<p>Success! And just to test that this is composable, let’s write an <code>And</code>.</p>
<div><pre><code data-lang="py"><span></span><span>from</span> <span>abc</span> <span>import</span> <span>ABC</span>
<span>from</span> <span>dataclasses</span> <span>import</span> <span>dataclass</span>
<span>from</span> <span>collections.abc</span> <span>import</span> <span>Iterable</span>

<span>def</span> <span>Not</span><span>(cls):</span>
    <span>class</span> <span>_Not</span><span>(ABC):</span>
        <span>@classmethod</span>
        <span>def</span> <span>__subclasshook__</span><span>(_,</span> <span>C):</span>
            <span>return</span> <span>not</span> <span>issubclass(C,</span> <span>cls)</span>
    <span>return</span> <span>_Not</span>

<span>def</span> <span>And</span><span>(cls1,</span> <span>cls2):</span>
    <span>class</span> <span>_And</span><span>(ABC):</span>
        <span>@classmethod</span>
        <span>def</span> <span>__subclasshook__</span><span>(_,</span> <span>C):</span>
            <span>return</span> <span>issubclass(C,</span> <span>cls1)</span> <span>and</span> <span>issubclass(C,</span> <span>cls2)</span>
    <span>return</span> <span>_And</span>


<span>def</span> <span>f</span><span>(x):</span>
    <span>n</span> <span>=</span> <span>And(Iterable,</span> <span>Not(str))</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>n():</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is a non-string iterable"</span><span>)</span>
        <span>case</span> <span>str():</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is a string"</span><span>)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is a string or not-iterable"</span><span>)</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
    <span>f(</span><span>"abc"</span><span>)</span>
    <span>f([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
</code></pre></div>

<p>This works as “”“expected”“”.</p>

<h3 id="caching-rules-everything-around-me">Caching Rules Everything Around Me</h3>

<p>This got me thinking: what if <code>__subclasshook__</code> wasn’t a pure function? Could I make an ABC that matched the <em>first</em> value of each type passed in, but not subsequent ones?</p>
<div><pre><code data-lang="py"><span></span><span>from</span> <span>abc</span> <span>import</span> <span>ABC</span>

<span>class</span> <span>OneWay</span><span>(ABC):</span>
    <span>seen_classes</span> <span>=</span> <span>set()</span>

    <span>@classmethod</span>
    <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>C):</span>
        <span>print(</span><span>f"trying {</span><span>C</span><span>}"</span><span>)</span>
        <span>if</span> <span>C</span> <span>in</span> <span>cls</span><span>.</span><span>seen_classes:</span>
            <span>return</span> <span>False</span>
        <span>cls</span><span>.</span><span>seen_classes</span> <span>|=</span> <span>{C}</span>
        <span>return</span> <span>True</span>




<span>def</span> <span>f</span><span>(x):</span>
    <span>match</span> <span>x:</span>
        <span>case</span> <span>OneWay():</span>
            <span>print(</span><span>f"{</span><span>x</span><span>} is a new class"</span><span>)</span>
        <span>case</span><span> </span><span>_</span><span>:</span>
            <span>print(</span><span>f"we've seen {</span><span>x</span><span>}'s class before"</span><span>)</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
    <span>f(</span><span>"abc"</span><span>)</span>
    <span>f([</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
    <span>f(</span><span>"efg"</span><span>)</span>
</code></pre></div>

<p>Sadly, this was all for naught.</p>

<pre><code>trying &lt;class 'str'&gt;
abc is a new class
trying &lt;class 'list'&gt;
[1, 2, 3] is a new class
efg is a new class
</code></pre>

<p>It looks like <code>__subclasshook__</code> caches the results for a given type check. CPython assumes that people don’t want to shove side effects into esoteric corners of the language. Show’s how much <em>they</em> know.</p>

<p>We can still have fun with side effects, though. This ABC lets through every-other type.</p>
<div><pre><code data-lang="python"><span></span><span>class</span> <span>FlipFlop</span><span>(ABC):</span>
    <span>flag</span> <span>=</span> <span>False</span>

    <span>@classmethod</span>
    <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>_):</span>
        <span>cls</span><span>.</span><span>flag</span> <span>=</span> <span>not</span> <span>cls</span><span>.</span><span>flag</span>
        <span>return</span> <span>cls</span><span>.</span><span>flag</span>
</code></pre></div>

<p>And this ABC asks the user what it should do for each type.</p>
<div><pre><code data-lang="python"><span></span><span>class</span> <span>Ask</span><span>(ABC):</span>
    <span>first_class</span> <span>=</span> <span>None</span>

    <span>@classmethod</span>
    <span>def</span> <span>__subclasshook__</span><span>(cls,</span> <span>C):</span>
        <span>choice</span> <span>=</span> <span>input(</span><span>f"hey should I let {</span><span>C</span><span>} though [y/n]  "</span><span>)</span>
        <span>if</span> <span>choice</span> <span>==</span> <span>'y'</span><span>:</span>
            <span>print(</span><span>"okay we'll pass em through"</span><span>)</span>
            <span>return</span> <span>True</span>
        <span>return</span> <span>False</span>
</code></pre></div>

<p>Try them in a pattern match. They both work!</p>

<h2 id="should-i-use-this">Should I use this?</h2>

<p>God no.</p>

<p>The pattern matching feature is, on the whole, pretty reasonably designed, and people will expect it to behave in reasonable ways. Whereas <code>__subclasshook__</code> is <em>extremely</em> dark magic. This kind of chicanery <em>might</em> have a place in the dark beating heart of a complex library, certainly not for any code your coworkers will have to deal with.</p>

<p>So yeah, you didn’t learn anything useful. I just like horrible things ¯\_(ツ)_/¯</p>

<p><em>Thanks to <a href="https://predr.ag/">Predrag Gruevski</a> for feedback. Title is from <a href="https://christine.website/blog/gonads-2022-04-24">Crimes with Go Generics</a>.</em></p>

<p><em>I shared an early version of this post on my <a href="https://buttondown.email/hillelwayne/">weekly newsletter</a> where I announce new blog posts and write additional essays. If you enjoyed this, <a href="https://buttondown.email/hillelwayne/">why not subscribe</a>?</em></p>


</div>

    



  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In the long run, LLMs make us dumber (106 pts)]]></title>
            <link>https://desunit.com/blog/in-the-long-run-llms-make-us-dumber/</link>
            <guid>44976815</guid>
            <pubDate>Thu, 21 Aug 2025 19:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://desunit.com/blog/in-the-long-run-llms-make-us-dumber/">https://desunit.com/blog/in-the-long-run-llms-make-us-dumber/</a>, See on <a href="https://news.ycombinator.com/item?id=44976815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The comfort we get when offloading our cognitive load to LLMs is bad for us. Cognitive load should exist, and if we reduce it too much – if we stop thinking – we can actually unlearn how to think.</p>



<p>Kids who always choose the easy route and copy their homework from other students eventually find themselves completely clueless about what’s going on in school. Someone who always lets their spouse handle all the bills and banking may one day be unable to manage even a simple payment on their own. A person who never bothers to learn street names or routes will be lost when their phone dies, not even knowing how to get home.</p>



<p>It all comes back to what Nassim Taleb talks about with hormesis in his insightful book <a href="https://www.amazon.com/Antifragile-Things-That-Disorder-Incerto/dp/0812979680">Antifragile</a> – the idea that small doses of stress or discomfort make us stronger.</p>



<ul>
<li>Muscles grow by lifting weights.</li>



<li>Immunity builds through exposure.</li>



<li>Confidence grows by taking risks.</li>



<li>Skills sharpen through repetition.</li>



<li>Creativity expands by solving hard problems.</li>
</ul>



<p>The mind works the same way. The friction of thinking, the awkward struggle to find the right words – that’s mental weightlifting.</p>



<figure><a href="https://www.amazon.com/Antifragile-Things-That-Disorder-Incerto/dp/0812979680"><img decoding="async" width="338" height="522" src="https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image.png" alt="" srcset="https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image.png 338w, https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image-194x300.png 194w" sizes="(max-width: 338px) 100vw, 338px"></a></figure>



<p>Maybe a slightly different example, but of a similar spirit – the <a href="https://en.wikipedia.org/wiki/Broken_windows_theory">Broken Windows theory</a>. The theory argues that visible signs of disorder – like graffiti, litter, or broken windows – signal that neglect is tolerated, lowering informal controls and inviting further misbehavior and even serious crime. In other words, when small cracks go unaddressed, they can cascade into much bigger fault lines. A constant reliance on LLMs will push this further and further until we’ve outsourced all our thinking, becoming little more than biological puppets.</p>



<p>There’s even <a href="https://arxiv.org/pdf/2506.08872v1">recent research</a> that backs this up. In one study, participants were split into three groups:</p>



<ul>
<li>Brain-only: wrote essays without assistance.</li>



<li>Search Engine: used Google search.</li>



<li>LLM (ChatGPT): relied entirely on ChatGPT.</li>
</ul>



<p>And the results were:</p>



<ul>
<li>In the LLM group, 83% couldn’t quote anything from their own essays shortly after writing, whereas nearly everyone in the other groups could. </li>



<li>Participants moving from LLM to writing solo exhibited reduced neural activity and continued under-engagement.</li>



<li>Participants transitioning from brain-only to using LLM <strong>retained strong memory recall</strong> and showed neural activation patterns similar to the Search Engine group.</li>
</ul>



<figure><img decoding="async" width="930" height="596" src="https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image-1.png" alt="In the LLM group, 83% couldn’t quote anything from their own essays shortly after writing, whereas nearly everyone in the other groups could. " srcset="https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image-1.png 930w, https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image-1-300x192.png 300w, https://d2u08wkl163w0i.cloudfront.net/blog/wp-content/uploads/2025/08/image-1-768x492.png 768w" sizes="(max-width: 930px) 100vw, 930px"></figure>



<p><em>Percentage of participants within each group who struggled to quote anything from their essays in Session</em></p>



<p>They coined the term “<strong>cognitive debt</strong>” to describe the tradeoff: immediate convenience from AI assistance may come at the cost of long-term cognitive capabilities like critical thinking, memory retention, and creative autonomy.&nbsp;You borrow mental energy from the machine but with interest – and that cost shows up later when your own thinking has weakened.</p>



<p>I keep saying that to my kids – LLM is a really powerful tool but use it wisely. Don’t ask it to solve your math equation; instead, provide your own solution and have it explain where you might be wrong. Starting with independent thinking and then integrating AI will be healthier for cognitive development. It’s like nuclear energy – you can use it for mass destruction or as a clean source of power.&nbsp;</p>



<p>Consistent reliance on AI tools might undermine learning, memory, and creativity. Discomfort isn’t just a nuisance – it’s a training ground. </p>



<p>Look for discomfort, seek it out, and encourage it.&nbsp;</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-v3.1 Release (598 pts)]]></title>
            <link>https://api-docs.deepseek.com/news/news250821</link>
            <guid>44976764</guid>
            <pubDate>Thu, 21 Aug 2025 19:06:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://api-docs.deepseek.com/news/news250821">https://api-docs.deepseek.com/news/news250821</a>, See on <a href="https://news.ycombinator.com/item?id=44976764">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Introducing DeepSeek-V3.1: our first step toward the agent era! 🚀</p>
<ul>
<li>
<p>🧠 Hybrid inference: Think &amp; Non-Think — one model, two modes</p>
</li>
<li>
<p>⚡️ Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528</p>
</li>
<li>
<p>🛠️ Stronger agent skills: Post-training boosts tool use and multi-step agent tasks</p>
</li>
</ul>
<p>Try it now — toggle Think/Non-Think via the "DeepThink" button: <a href="https://chat.deepseek.com/" target="_blank" rel="noopener noreferrer">https://chat.deepseek.com/</a></p>
<hr>
<h2 id="api-update-️">API Update ⚙️<a href="#api-update-️" aria-label="Direct link to API Update ⚙️" title="Direct link to API Update ⚙️">​</a></h2>
<ul>
<li>
<p>🔹 deepseek-chat → non-thinking mode</p>
</li>
<li>
<p>🔹 deepseek-reasoner → thinking mode</p>
</li>
<li>
<p>🧵 128K context for both</p>
</li>
<li>
<p>🔌 Anthropic API format supported: <a href="https://api-docs.deepseek.com/guides/anthropic_api" target="_blank" rel="noopener noreferrer">https://api-docs.deepseek.com/guides/anthropic_api</a></p>
</li>
<li>
<p>✅ Strict Function Calling supported in Beta API: <a href="https://api-docs.deepseek.com/guides/function_calling" target="_blank" rel="noopener noreferrer">https://api-docs.deepseek.com/guides/function_calling</a></p>
</li>
<li>
<p>🚀 More API resources, smoother API experience</p>
</li>
</ul>
<hr>
<h2 id="tools--agents-upgrades-">Tools &amp; Agents Upgrades 🧰<a href="#tools--agents-upgrades-" aria-label="Direct link to Tools &amp; Agents Upgrades 🧰" title="Direct link to Tools &amp; Agents Upgrades 🧰">​</a></h2>
<ul>
<li>
<p>📈 Better results on SWE / Terminal-Bench</p>
</li>
<li>
<p>🔍 Stronger multi-step reasoning for complex search tasks</p>
</li>
<li>
<p>⚡️ Big gains in thinking efficiency</p>
</li>
</ul>
<p><img src="https://cdn.deepseek.com/api-docs/v3.1_benchmark_1.webp" width="600"></p>
<p><img src="https://cdn.deepseek.com/api-docs/v3.1_benchmark_2.webp" width="600"></p>
<p><img src="https://cdn.deepseek.com/api-docs/v3.1_benchmark_3.webp" width="600"></p>
<hr>
<h2 id="model-update-">Model Update 🤖<a href="#model-update-" aria-label="Direct link to Model Update 🤖" title="Direct link to Model Update 🤖">​</a></h2>
<ul>
<li>
<p>🔹 V3.1 Base: 840B tokens continued pretraining for long context extension on top of V3</p>
</li>
<li>
<p>🔹 Tokenizer &amp; chat template updated — new tokenizer config: <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/tokenizer_config.json" target="_blank" rel="noopener noreferrer">https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/tokenizer_config.json</a></p>
</li>
<li>
<p>🔗 V3.1 Base Open-source weights: <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base" target="_blank" rel="noopener noreferrer">https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base</a></p>
</li>
<li>
<p>🔗 V3.1 Open-source weights: <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1" target="_blank" rel="noopener noreferrer">https://huggingface.co/deepseek-ai/DeepSeek-V3.1</a></p>
</li>
</ul>
<hr>
<h2 id="pricing-changes-">Pricing Changes 💳<a href="#pricing-changes-" aria-label="Direct link to Pricing Changes 💳" title="Direct link to Pricing Changes 💳">​</a></h2>
<ul>
<li>
<p>🔹 New pricing starts &amp; off-peak discounts end at Sep 5th, 2025, 16:00 (UTC Time)</p>
</li>
<li>
<p>🔹 Until then, APIs follow current pricing</p>
</li>
<li>
<p>📝 Pricing page: <a href="https://api-docs.deepseek.com/quick_start/pricing/" target="_blank" rel="noopener noreferrer">https://api-docs.deepseek.com/quick_start/pricing/</a></p>
</li>
</ul>
<p><img src="https://cdn.deepseek.com/api-docs/v3.1_price_en.jpeg" width="600"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI tooling must be disclosed for contributions (655 pts)]]></title>
            <link>https://github.com/ghostty-org/ghostty/pull/8289</link>
            <guid>44976568</guid>
            <pubDate>Thu, 21 Aug 2025 18:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ghostty-org/ghostty/pull/8289">https://github.com/ghostty-org/ghostty/pull/8289</a>, See on <a href="https://news.ycombinator.com/item?id=44976568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <task-lists disabled="" sortable="">
    <div>
      <p dir="auto">I think, at this stage of AI, it is a common courtesy to disclose this.</p>
<p dir="auto">In a perfect world, AI assistance would produce equal or higher quality work than any human. That isn't the world we live in today, and in many cases it's generating slop. I say this despite being a fan of and using them successfully myself (with heavy supervision)! I think the major issue is <strong>inexperienced human drivers of AI</strong> that aren't able to <strong>adequately review their generated code.</strong> As a result, they're pull requesting code that I'm sure they would be ashamed of if they knew how bad it was.</p>
<p dir="auto">The disclosure is to help maintainers assess how much attention to give a PR. While we aren't obligated to in any way, I try to assist inexperienced contributors and coach them to the finish line, because getting a PR accepted is an achievement to be proud of. But if it's just an AI on the other side, I don't need to put in this effort, and it's rude to trick me into doing so.</p>
<p dir="auto"><strong>I'm a fan of AI assistance and use AI tooling myself.</strong> But, we need to be responsible about what we're using it for and respectful to the humans on the other side that may have to review or maintain this code.</p>
<p dir="auto">(In the spirit of this PR... none of this PR was AI generated. lol.)</p>
    </div>
  </task-lists>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building AI products in the probabilistic era (157 pts)]]></title>
            <link>https://giansegato.com/essays/probabilistic-era</link>
            <guid>44976468</guid>
            <pubDate>Thu, 21 Aug 2025 18:42:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://giansegato.com/essays/probabilistic-era">https://giansegato.com/essays/probabilistic-era</a>, See on <a href="https://news.ycombinator.com/item?id=44976468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I was recently trying to convince a friend of mine that ChatGPT hasn't memorized every possible medical record, and that when she was passing her blood work results the model was doing pattern matching in ways that even OpenAI couldn't really foresee. She couldn't believe me, and I totally understand why. It's hard to accept that we invented a technology that we don't fully comprehend, and that exhibits behaviors that we didn't explicitly expect.</p>
<p>Dismissal is a common reaction when witnessing AI’s rate of progress. People struggle to reconcile their world model with what AI can now do, and <em>how</em>.</p>
<p>This isn't new. Mainstream intuition and cultural impact always lag behind new technical capabilities. When we started building businesses on the Internet three decades ago, the skepticism was similar. Sending checks to strangers and giving away services for free felt absurd. But those who grasped a new reality made of zero marginal costs and infinitely scalable distribution became incredibly wealthy. They understood that the old assumptions baked into their worldview no longer applied, and acted on it.</p>
<p>Eventually the world caught up<sup><a href="https://giansegato.com/essays/dawn-new-startup-era" rel="noopener noreferrer" target="_blank">1</a></sup>, and we reached a new equilibrium. In the last couple of decades the tech industry has evolved, developing a strong instinct for how to build and grow digital products online. We invented new jobs, from product management to head of growth, while others evolved, from engineering leadership to performance marketing. All have created their own playbooks to thrive.</p>
<p>AI is now shuffling the deck again.</p>
<p>Many of those playbooks have become obsolete. Something fundamental has shifted. General purpose artificial intelligence has created a rupture in the fabric of the tech industry, upending how we design, engineer, build, and grow software — and thus businesses that have software at their core.</p>
<p>We're now in a liminal moment, where our tools have outpaced our frameworks for understanding them.<sup><a href="https://jzhao.xyz/thoughts/information-scaling-threshold" rel="noopener noreferrer" target="_blank">2</a></sup> This is a technical, epistemological, and organizational change, one where exceptional AI companies have started operating significantly differently from what we’ve known in the last decades.</p>
<p>Just as physics underwent a conceptual revolution when we moved past Newton's deterministic universe, and into a strange and counterintuitive place made by wave functions, software too is undergoing its own quantum shift. We're leaving a world where code reliably and deterministically takes certain inputs to produce specific outputs, and entering a very different one where machines now produce statistical distributions instead.</p>
<p>Building probabilistic software is like nothing we've done before.</p>
<h2 id="i-the-classical-world">I. The Classical World</h2>
<p>Today’s tech industry has been shaped by the core nature of software.</p>
<p>Like the software they’re made of, digital products map known inputs to expected results, deterministically. On Instagram, users can upload pictures, send messages, leave comments, follow accounts. On Netflix, they can search an item, pick an item, stream the video item. They choose an action, and expect a result to happen.</p>
<p>Let’s frame these products as functions <code>F: X → Y</code>.</p>
<p><img src="https://giansegato.com/images/probabilistic-era/func.png" alt=""></p>
<p>Each input <code>x</code> is a user action inside the product, like “send a message on WhatsApp”, “book a ride on Uber”, and “search an address in Google Maps”. For each action <code>x</code>, the product yields an outcome <code>y</code>: the message will be sent, the ride will be booked, the address will be searched.</p>
<p>Through this framework, it’s possible to appreciate why startups and tech companies work the way they work.</p>
<p>This is most evident in engineering management.</p>
<p>The <em>classical</em> way of managing the performance of a software engineering team looks like this:</p>
<p><img src="https://giansegato.com/images/probabilistic-era/slo.jpg" alt=""></p>
<p>If you’ve ever had a developer job, you’ll immediately recognize a SLO Datadog dashboard. It’s what quantifies the reliability of a system. Since the engineer’s job is to build <code>F(x)</code>, we ask: does taking action <code>x</code> produce outcome <code>y</code> <em>every time</em>? Does it do so <em>reliably</em> no matter how many times you try? The goal is 100%: the function should always work as we expect.</p>
<p>That’s why layering, cautious refactors, and test-driven development are hallmarks of software engineering: they all stem from the ontological nature of the <code>F</code> mapping function. As the job to be done is always producing <code>y</code> when inputting <code>x</code>, we should write tests that make sure that’s always the case, we should be very wary of general refactorings, and careful when introducing new features that could impact the reliability of the arrow.</p>
<p>Product management and design too are about reliably mapping <code>x</code> to <code>y</code> — just at a different level of abstraction. For those teams, it’s about constructing a function <code>F</code> where the input <code>x</code> looks more like “the user watched an Instagram story for the first time”, producing a real-world outcome <code>y</code> like “the user is still using the product a month later”.</p>
<p>Good PM’ing is about drilling users through value funnels. The cardinality of the features is known beforehand: the input space <code>X</code> is a limited, pre-determined set of features and growth experiments. The goal is also pre-determined: designers and PMs know in advance what goals they're optimizing for. This means that, like for engineers, they too are striving to reach a 100% score on users going from experiencing a feature to yielding a business outcome. For these teams, it looks more like this:</p>
<p><img src="https://giansegato.com/images/probabilistic-era/funnel.jpg" alt=""></p>
<p>Good design is not always quantifiable. Tasteful products aren’t about metrics. But ultimately they do exist to deliver value, and value is always delivered in funnels. When people stick around longer because they appreciate the tasteful design of a specific feature, that too qualifies as a funnel: from experiencing that feature, to keep paying long term.</p>
<p>Conversion, activation, and retention are all ratios that require countable, pre-defined inputs and outcomes. The reason why we can count those numbers and construct the ratios is because both the numerator and the denominator are <em>limited</em> and <em>pre-determined</em>: <strong>what <code>x</code> and <code>y</code> can look like are known before the function is created!</strong> By knowing their cardinality, we can create the funnel.</p>
<p>That’s why products like Amplitude and Mixpanel revolve around funnels, and the work of product and growth teams revolves around optimizing conversion rates.</p>
<p>All these ratios, be them engineering reliability goals or growth conversion targets, are how we make both strategic as well as tactical decisions in building and growing software products. How we measure performance, how we structure our work, how we design and implement our playbooks. The entire operating system of the tech industry relies on them.</p>
<p>The problem is that these rules, in the probabilistic world of AI, have the potential to become actively counterproductive.</p>
<h2 id="ii-the-quantum-regime">II. The Quantum Regime</h2>
<p>Things started to change in the late 2010s, when we started to witness in AI models the first signs of true generalization. Until that point, machine learning systems were essentially “narrow experts” rather than “competent generalists.”<sup><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener noreferrer" target="_blank">3</a></sup></p>
<p>In the last decade, however, researchers have realized that pre-training deep learning models on a lot of data “causes the model to develop <em>general-purpose</em> abilities and knowledge that can then be transferred to downstream tasks”<sup><a href="https://arxiv.org/abs/1910.10683" rel="noopener noreferrer" target="_blank">4</a></sup>. The idea is that if you focus on teaching AI the fundamental structure of the entire domain you’re interested in (say, language), you can unlock an entire class of tasks <em>all at once</em>, without the need to define them beforehand!, from identifying spam emails to answering trivia questions to role-playing fictional characters. Google’s T5 showed that “pre-training causes the model to develop <em>general-purpose</em> abilities and knowledge”, while OpenAI’s GPT-2 moved us “towards a more <em>general system</em> that can perform many tasks.”</p>
<p><img src="https://giansegato.com/images/probabilistic-era/t5.jpg" alt=""></p>
<p>The crucial bit here is that these models were <em>not</em> explicitly trained on all these tasks. When pre-trained on a large corpus of data and only fine-tuned on certain instructions, we can “substantially improve zero-shot performance on <em>unseen</em> tasks.” <sup><a href="https://arxiv.org/pdf/2109.01652" rel="noopener noreferrer" target="_blank">5</a></sup></p>
<p>I cannot emphasize enough how important “unseen” and “general-purpose” are, here. They’re what made this a truly watershed moment in the history of computing. It's no longer about teaching machines to recognize spam: it's about teaching them to speak. It's no longer about teaching machines to recognize bikes or tell animals apart: it's about giving them the sense of sight itself.</p>
<p>Think about it: we’ve built a special kind of function <code>F'</code> that for all we know can now accept <em>anything</em> — compose poetry, translate messages, even debug code! — and we expect it to always reply with something reasonable. For all intents and purposes, it’s the first time we stopped controlling the input space, which is now suddenly open-ended. <code>F(x)</code> has become <code>F'(?)</code>.</p>
<p><img src="https://giansegato.com/images/probabilistic-era/inf-func-1.png" alt=""></p>
<p>You can ask ChatGPT <em>anything</em>, from legal advice to romantic support, from spreadsheets analysis using code, to astrology predictions. You can ask Claude to produce <em>any</em> piece of software, from scripts, to websites, to marketing pages, to video games. Even inputting gibberish will still produce <em>something</em>. From a practical and philosophical standpoint, the cardinality of the input space is now basically infinite.</p>
<p>This is quite head scratching if you’re building AI products. What will users do with them? How can you make sure all your customers will always have a great experience? What if they discover an amazing new use case that these models can perform that you haven’t foreseen? What if they discover a <em>bad</em> use case that your marketing implied in the attempt of staying generic enough?</p>
<p>To make things worse, the correctness of the output isn't always guaranteed! A reply constructed to be reasonable doesn't mean it's going to be <em>correct</em>. What this new function will reply with is <em>an</em> <em>estimate</em>. Sometimes, a hallucination.</p>
<p>Can we solve hallucination? Well, we <em>could</em> train perfect systems to always try to reply <em>correctly</em>, but some questions simply don't have "correct" answers. What even is the "correct" when the question is "should I leave him?".</p>
<p>See, the problem is that the most interesting questions are not well defined. You can only have perfect answers when asking perfect questions, but more often than not, humans don’t know what they want. “Make me a landing page of my carwash business”. How many possible ways of achieving that objective are there? Nearly infinite. AI shines in ambiguity and uncertainty, precisely thanks to its emergent properties. If you need to know what’s 1+1, you should use a calculator, but knowing what your latest blood work results mean for your health requires nuance.</p>
<p>That's why when building an interface between humans and machines, the best form factor is a probability distribution ("you <em>likely</em> want this HTML with this hero banner", "you <em>likely</em> have a Vitamin D deficiency and should touch grass more"). It’s a shape that can naturally handle nuance.</p>
<p>That's the critical reason why we inject randomness into the output, and sampling at inference time: <strong>prompting the product with the exact same inputs, will yield two different results</strong>, making the output <em>stochastic</em>. This is a fundamental property of the technology, and what makes it work so well, as it provides users with a way to efficiently navigate complex problem spaces. It allows them to add their own taste to the final output, and navigate the probability distribution of all reasonable outputs according to their own judgement.</p>
<p>This is the result of two identical prompts using Claude 3.7:</p>
<p><img src="https://giansegato.com/images/probabilistic-era/variance.jpg" alt=""></p>
<p>It may feel subtle at the beginning, but over a sequence of chained generations composing a long trajectory, the difference greatly magnifies.</p>
<p>Output stochasticity and emergent behavior are the reasons why we can't expect perfect reliability from AI, not in the traditional sense. We are no longer guaranteed what <code>x</code> is going to be, and we're no longer certain about the output <code>y</code> either, because it's now drawn from a distribution.</p>
<p><strong>In moving to an AI-first world, we transitioned from funnels to <em>infinite fields</em></strong>.</p>
<p><img src="https://giansegato.com/images/probabilistic-era/inf-func-2.png" alt=""></p>
<p>Stop for a moment to realize what this means. When building on top of this technology, our products can now succeed in ways we’ve never even imagined, and fail in ways we never intended.</p>
<p>This is incredibly new, not just for modern technology, but for human toolmaking itself. Any good engineer will know how the Internet works: we designed it! We know how packets of data move around, we know how bytes behave, even in uncertain environments like faulty connections. Any good aerospace engineer will tell you how to approach the moon with spaceships: we invented them! Knowledge is perfect, a cornerstone of the engineering discipline. If there’s a bug, there’s always a knowable reason: it’s just a matter of time to hunt it down and fix it.</p>
<p>With AI products, all this is no longer true. <strong>These models are <em>discovered</em>, not engineered.</strong> There's some deep unknowability about them that is both powerful and scary. Not even model makers know exactly what their creations can fully do when they train them. It's why "vibe" is such an apt word: faced with this inherent uncertainty, we're left to trust our own gut and intuition when judging what these models are truly capable of.</p>
<p>For people interacting with products harnessing the power of these models, this is a lot to take in, to accept, and to develop an intuition for. Users really dislike the inherent uncertainty of dealing with AI systems. They’re not used to it! They’re expecting a digital product like every other product they know: you instruct the app to perform an action, and the app will perform it. Unfortunately, prompting Replit in the wrong way may very well introduce a bug in your work, depending on your request and on the probability distribution that maps to. Consumers really struggle to understand this: it makes them very mad when the AI doesn’t do what they expect it to do.</p>
<p>The core reason why they get so frustrated is because <strong>for the first time in the digital era marginal costs are way larger than zero</strong>. In fact, they’re so large that they completely invalidate the business model and growth playbooks that dominated the Internet since the 90s. This may change in the future, depending on innovation and commoditization at hardware level, but for now the cost of intelligence is surprisingly stable (and not really as deflationary as we expected it to be until last year).</p>
<p>We have a class of products with deterministic cost and stochastic outputs: a built-in unresolved tension. Users insert the coin with <em>certainty</em>, but will be <em>uncertain</em> of whether they'll get back what they expect. This fundamental mismatch between deterministic mental models and probabilistic reality produces frustration — a gap the industry hasn't yet learned to bridge.</p>
<p>Software used to be magic black boxes offering a set of pre-defined options to pick from, and producing pre-determined results for practically-zero-margin cost. AI models are completely different entities: they accept a field of infinite possibilities, and produce probability distributions that collapse to potentially-unexpected observations after charging users very significant money.</p>
<p>It’s clear that the way to produce products around these two technologies needs to be radically different.</p>
<h2 id="iii-it-takes-a-scientist">III. It Takes A Scientist</h2>
<p>To thrive in the quantum era, <strong>successful organizations need to transition from engineering to empiricism</strong>.</p>
<p>The tendency of old-school engineering leadership, when dealing with probabilistic software, is to slap reliability metrics on top of it. Good old SLO, like before. It's muscle memory, a reflex. But it creates perverse incentives: to achieve the reliability goal of 100%, classical engineering leadership naturally start adding more and more rails and constraints around the model, trying to reign it in and control it.</p>
<p>This doesn’t work anymore.</p>
<p>The more you try to control the model, the more you’ll nerf it, ultimately damaging the product itself. <strong>Past a certain point, intelligence and control start becoming opposing needs.</strong></p>
<p>The goal isn’t perfection: by definition you can’t nor should aim for it. <strong>The goal is to manage the uncertainty</strong>. As an AI product builder, you should determine what’s the acceptable unpredictability that keeps the model capable of dealing with complexity, given the market you’re operating in and the audience you’re serving. Think in terms of Minimum Viable Intelligence: the lowest quality threshold that is both accepted by the market (some may be more sensitive than others), while preserving its inherent flexibility and generalization capacity.</p>
<p>The notion of Minimum Viable Intelligence stems from the emergent properties of these models. Companies building AI products may not know how powerful their product <em>actually</em> is, and may inadvertently constrain the model too much when trying to keep it on its rails. Think of how frustrating it would be for a user if asking Claude Code to add a watermark to each image in a folder would result in “sorry I can’t help with that: I can only make websites”. Because we know it can! If you do keep a healthy balance, your product could be so much larger than what you originally envisioned, thanks to the surprising things the models can do <em>without us even knowing</em>.</p>
<p>This is what forces a transition from engineering to empiricism: what will determine the correct way of building the product itself, and not just typical product management A/B testing, is now the scientific method.</p>
<p><strong>It takes a scientist to build AI products.</strong></p>
<p><img src="https://giansegato.com/images/probabilistic-era/ab.jpg" alt=""></p>
<p>The old wisdom of always building incrementally on top of what’s already there doesn’t hold up anymore. If anything, that too is actively harmful. Every time a new model drops, be it a new generation of an existing one (say, from Sonnet to Opus), or a completely new one (say swapping GPT for Gemini), all previous assumptions about its behavior should be disregarded.</p>
<p>In fact, when a new model drops, you should even consider literally <em>tearing down the full system</em>, and building it back from the ground up. There are no sacred cows.</p>
<p>When Replit moved from Sonnet 3.5 to 3.7, Replit’s President Michele had the company rewrite the <em>entire product</em> in less than 3 weeks. We called it Replit v2, but that was quite an understatement. In reality, it was a brand new product. The architecture, data model, prompting technique, context management, streaming design… it was all new. 3.7 was highly agentic in an entirely novel way, Michele understood it, and decided to lean into it instead of trying to control it. The team had to go through weeks of sleepless nights trying to beat the competition to market it successfully. Can you imagine what it takes to completely re-architect a product that was making almost $20m ARR at the time, rebuild it from the ground up in just three weeks, and to see its revenue inflect and end up growing to $100m ARR less than a quarter later? <em>It takes a scientist</em>. It’s no coincidence that Michele is, indeed, a scientist by trade.</p>
<p>Swapping models at the app layer is a big deal — which makes frontier labs stickier than they may look from the outside. It’s not “just an API commodity”. These models are not flat interfaces: they have personalities, and quirks. That’s why Gemini Pro 2.5 was not a Claude 3.5 killer despite still being an exceptionally good model. It takes hard work to fully prove that any given model is superior to any other one.</p>
<p>Every model update doesn’t necessarily mean a complete rewrite every time, but it does force you to fundamentally rethink your assumptions each time, making a rewrite a perfectly plausible hypothesis. You have to follow an empirical method, where the only valid assumption is “I don’t know”. Being an empiricist first is diametrically opposed to being an engineer first.</p>
<p>Even 'simple' improvements on the same base model require specialized data work. Any feature that ships needs to be tested, both in lab conditions using synthetic evals and in production with real-world usage. The test needs to be rigorous and thorough: it can't simply be a collection of binary "pass / not pass". What if a particular prompt change meaningfully impacts the model tendency to prefer a certain tool over another, actively altering the unit of economics for a certain segment of users? What if a particular design change impacts how users think about the input distribution, which in turn shapes the model output distribution in unforeseen ways?</p>
<p>The need for statistics when testing seemingly-atomic software improvements defies classical programmers' intuition. Many engineers consider this grueling data work not part of the job description, and they may be right!</p>
<p>It’s the job description that has changed.</p>
<h2 id="iv-data-is-the-new-operating-system">IV. Data is the New Operating System</h2>
<p>Despite the fact that model behavior is intrinsically unknowable and uncertain, figuring out how to build an effective data function around it is incredibly difficult. Models’ emergent properties make synthetic testing elusive.</p>
<p>Engineers need to keep the eval dataset up to date with the distribution of actual user behavior, but by definition such dataset will constantly lag behind. Since the input space is no longer limited, you can't simply write a few selected tests and follow test-driven development: you'll risk breaking critical features without even being aware of their existence. Tweaking the prompt or swapping the base model for Replit meant unlocking latent features we never initially thought of, like game development. Having a good system in place to constantly sample the right test cases from real-world past usage is one of the most critical new pieces of work required today to build great AI products.</p>
<p>That’s also the reason why testing in production, with traditional A/B tests, is also critical: this way you can make sure to stay as close as possible to the general population you’re serving, and have a higher chance to test a long-tail outcome. Production testing, however, is potentially even harder than evals testing.</p>
<p>The elephant in the room when it comes to real-world live A/B testing is that it assumes you know what to optimize for in the first place. It implies knowing and quantifying the definition of success. In AI products, you basically can’t.</p>
<p>Users are exploring fields of possibilities, navigating through space composing trajectories: it’s really really hard to understand whether your product is accomplishing what it’s set to do! Say you just shipped a great new feature for the Replit agent: how do you know whether users are making “better software”? Longer chains of messages? Maybe they’re just debugging in frustration. Shorter and more efficient messages? Maybe they’re giving up faster. Sure you can measure long-term retention, but you can’t afford to wait weeks (or, worse, months!) to ship features.</p>
<p>This is what makes high-velocity AI shipping so challenging. Yet not impossible. Fundamentally it’s about moving from traditional growth funnels, to finding ways of aggregating “user trajectories” — paths through the field of possible tasks and model states.</p>
<p><img src="https://giansegato.com/images/probabilistic-era/trajectory.jpg" alt=""></p>
<p>The easiest way of approaching it is by segmenting user inputs. You use smaller models to classify user requests to larger models, which allows you to segment your data in “regions of usage”. It’s a crude way of clustering user journeys. For Replit’s coding agent, this could be coding use cases: “what’s the likelihood of getting a positive message from the user after 3 chat interactions, for all users that submitted a prompt about React web apps?” To push things further, you can use the same approach to define milestones to achieve across different paths, which might mean classifying model internal states.</p>
<p>This clearly impacts product management, design, go-to-market, and even (especially!) finance. As features become emergent, binary analytics events are no longer as useful as before to understand user behavior. Knowing that users acquired through TikTok are more likely to build games, which are more expensive to generate on a per-token basis and therefore impact the margin calculus, is <em>incredibly</em> valuable across the entire company: from engineers making sure that games are efficiently generated, to marketers shifting their top-of-the-funnel strategy to a more sustainable channel, to the finance team appropriately segmenting their CAC and LTV analysis. A 20% shift from game-building users to professional web apps might mean the difference between sustainable unit economics and bleeding money on every free user — yet this insight only emerges from analyzing the actual content of AI interactions, not traditional funnel metrics. That’s why classifying states is so crucial.</p>
<p>It all boils down to data. The value is being generated by the model, and data lives upstream and downstream of the model. We have years of literature about upstream data (for training), and the industry is keenly aware of its importance. But downstream data is something new, because we had true emergence at global scale for only a couple of years. Such scale is what makes the problem expensive, hard, complex, and requiring heavy data engineering. Architecting an AI product is no small feat, an increasingly sophisticated cross-disciplinary art.</p>
<p>More and more, in an era of stochastic unpredictable behaviors, data is becoming a crucial differentiating point when determining the success of an enterprise. It’s the shared operating system, longitudinal to the entire organization: a shared language and context that can describe reality and prescribe actions to shape it.</p>
<p>None of the core components of a tech company can afford to work in silos anymore. Things like customer attribution (in marketing, sales), observability (engineering), and A/B testing (product, design) used to be separate. In AI products, they collapse into one holistic system view where the core behavior of the product influences both the top of the funnel as well as the bottom line, from conversions to retention.</p>
<p>Only data can provide the map to understand this new, unknown function <code>F'</code>, and describe the journeys that users take when exploring and meandering through the emergent properties of AI products. Only data can inform where they’re going, whether they’re successful in reaching their destination, whether they can afford to get there.</p>
<p><strong>Data is not just the new oil to train AI models.</strong></p>
<p><strong>It’s also how we can truly harness its power.</strong></p>
<h2 id="v-this-time-is-different">V. This Time is Different</h2>
<p>After decades of technical innovation, the world has (rightfully) developed some anti-bodies to tech hype. Mainstream audiences have become naturally skeptical of big claims of “the world is changing”. There’s now even a popular meme: “nothing ever happens”.</p>
<p>I strongly believe that when it comes to AI, something <em>is</em> happening. This time it <em>does</em> feel different.</p>
<p>It's ontologically different. We're moving away from deterministic mechanicism, a world of perfect information and perfect knowledge, and walking into one made of emergent unknown behaviors, where instead of planning and engineering we observe and hypothesize.</p>
<p>The shift is real, and it affects every part of the tech industry, altering how we make products, how we study and design them, and how we structure work around them. Organizations that build using an empirical approach, think in probabilities, and measure complex trajectories will define the next era of technology. The rest will keep trying to squeeze wave functions into spreadsheets, wondering why their perfectly deterministic dashboards can't capture what makes their products magical.</p>
<p>It’s a new world, a world of wonder and possibilities, a world to discover and understand.</p>
<p>Welcome to the Probabilistic Era.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unbearable slowness of AI coding (125 pts)]]></title>
            <link>https://joshuavaldez.com/the-unbearable-slowness-of-ai-coding/</link>
            <guid>44976437</guid>
            <pubDate>Thu, 21 Aug 2025 18:39:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshuavaldez.com/the-unbearable-slowness-of-ai-coding/">https://joshuavaldez.com/the-unbearable-slowness-of-ai-coding/</a>, See on <a href="https://news.ycombinator.com/item?id=44976437">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://joshuavaldez.com/">
      <h2>
        Joshua Valdez
      </h2>
    </a>
    <nav>
      <p><a href="https://joshuavaldez.com/">Home</a> <a href="https://joshuavaldez.com/about/">About</a> <a href="https://joshuavaldez.com/blog/">Blog</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-08-19T20:14Z">
                    19 Aug, 2025
                </time>
            </i>
        </p>
    

    <p>I’ve been coding entirely with Claude Code for the past two months. At first it was exhilarating. I was speeding through tasks. I was committing like mad.</p>
<p>Now, as I’ve built up a fairly substantial app, it’s slowed to a crawl. Ironically, the app I’m building lets me parallelize many instances of Claude Code at once.</p>
<p>Often, I’ll have 5 instances running while I’m thinking about new features.</p>
<p>The slowness comes in when I actually need to review all the PRs. One by one, I have to apply them locally. One by one, I have to step through the console logs. One by one, I have to tell Claude to fix the issues it created.</p>
<p>Yes, it’s faster. I’m committing an incredible amount of code these days—more than I ever have.</p>
<p>It also feels incredibly, maddeningly slow. Once you’ve felt that first boost of speed with Claude Code, you want every coding task to feel like that. <a href="https://steipete.me/posts/just-one-more-prompt">It’s addictive.</a></p>
<p>Instead, as you build, you still have to serve as Claude’s QA engineer.</p>
<p>Maybe one day we’ll solve that. But I’m skeptical it will be in the form of a CLAUDE.md. I can barely get Claude to consistently follow the bare set of rules I have, much less ensure it performs a complex integration test for a web app.</p>
<p>Until then, I’ll keep pulling PRs locally, adding more git hooks to enforce code quality, and zooming through coding tasks—only to realize ChatGPT and Claude hallucinated library features and I now have to rip out Clerk and implement GitHub OAuth from scratch.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The contrarian physics podcast subculture (194 pts)]]></title>
            <link>https://timothynguyen.org/2025/08/21/physics-grifters-eric-weinstein-sabine-hossenfelder-and-a-crisis-of-credibility/</link>
            <guid>44975378</guid>
            <pubDate>Thu, 21 Aug 2025 17:13:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timothynguyen.org/2025/08/21/physics-grifters-eric-weinstein-sabine-hossenfelder-and-a-crisis-of-credibility/">https://timothynguyen.org/2025/08/21/physics-grifters-eric-weinstein-sabine-hossenfelder-and-a-crisis-of-credibility/</a>, See on <a href="https://news.ycombinator.com/item?id=44975378">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-681">
	<!-- .entry-header -->

	<div>
		
<p><em>This is the story of how a circle of popular science communicators, who built their brands on championing free inquiry, worked to suppress scientific critique. Of how Eric Weinstein, the man who condemns the scientific community for suppressing his and his family’s work, nearly succeeded in cancelling me through intimidation and false threats. And of how Sabine Hossenfelder spins the truth for the sake of audience capture and podcast hosts Brian Keating and Curt Jaimungal prioritize tribe loyalty over the scientific process. In revealing personal details I have kept private for years, this account shows the lengths to which the individuals involved have gone in order to deceive the public.&nbsp;</em></p>



<hr>



<p>Science communication, at its best, serves a noble purpose: to act as a bridge between the intricate, often intimidating world of scientific research and the public’s curiosity. Skilled communicators translate complexity into clarity, demystify the scientific process, and inspire a shared sense of wonder. Yet a growing and troubling trend has emerged: the rise of the contrarian science communicator. These are not easily dismissed cranks. They are skilled performers who blend legitimate science with dubious claims, making it hard to separate the valuable from the misleading.</p>



<p>One of the most prominent examples is the contrarian physics subculture centered around Eric Weinstein, which includes Sabine Hossenfelder, Brian Keating, and Curt Jaimungal. These figures command millions of followers across social media and have built their reputations by tackling charged topics in physics, such as the validity of string theory or the claim that theoretical physics faces a crisis. Their YouTube channels feature long, thoughtful discussions with leading physicists like Roger Penrose and Leonard Susskind, and both Hossenfelder and Keating are professional physicists with undeniable expertise in their respective areas. Taken at face value, their content and profiles thus suggest that they are doing a valuable service in making science accessible and entertaining to the public.</p>



<p>But this engagement with legitimate science conceals a concerted effort to suppress criticism and mislead the public. The deception exposes the central problem facing these prominent science communicators: they are willing to trade scientific integrity for audience capture and tribal loyalty. A prime example of this dilemma is that of Weinstein’s so-called “Geometric Unity” (GU), a proposed theory of everything first unveiled in 2013 and revived in the 2020s through podcast appearances. Despite its lack of seriousness as a scientific theory, GU continues to be entertained by Hossenfelder, Keating, and Jaimungal. As an author of the first scientifically-detailed rebuttal of GU, I have directly witnessed how this contrarian cohort reacts when their ideas or allies face substantive criticism. It lays bare their gross hypocrisy of claiming to be champions for unorthodox views while working hard to ignore or suppress challenges to one of their own. This post is a firsthand account of their campaign to silence dissent.</p>



<p><strong>Note:</strong> In what follows, I will discuss Geometric Unity as if it were unambiguously known to be unserious and flawed. For those uncertain or new to the subject, this will be justified later in the section “The Jury Is Already In”.</p>



<h2>The Eric Weinstein Affair: A Short Recap</h2>



<p>Eric Weinstein wears many hats. With a PhD in mathematics from Harvard, he has worked as a managing director of Thiel Capital, founded the “Intellectual Dark Web,” and regularly comments on a wide range of topics on popular podcasts. Crucially however, Weinstein sits squarely outside the scientific establishment, having left academia a few years after completing his doctorate in the early 1990s with only a single published and forgotten paper. His Geometric Unity proposal, therefore, has all the hallmarks of an outsider attempting to revolutionize physics, casting him as an Einstein-like figure toiling alone at the patent office.</p>



<p>But the noteworthy aspect of GU is not its scientific merit or lack thereof. Rather, it is how GU ties into Eric Weinstein’s narrative of being an outcast decrying the profound failures of our institutions. Indeed, Weinstein has built his public persona around the concept of a “Distributed Idea Suppression Complex” (<a href="https://youtu.be/QxnkGymKuuI?si=BVtwZSG_22tGD2Jy">DISC</a>), an alleged establishment in academia and science that marginalizes or silences brilliant outsiders with revolutionary ideas. Within this framework, GU is presented as a transformative proposal that powerful institutions are too fearful to engage with. This creates a self-reinforcing loop: when physicists ignore the work, it confirms that the DISC is real, but when they criticize it, they are cast as bad-faith agents protecting their entrenched paradigms. In this way, Geometric Unity functions as a foundation for Weinstein’s personal brand of scientific and institutional grievances.</p>



<p>Until April 2021, the only public material on GU was a YouTube <a href="https://www.youtube.com/watch?v=Z7rd04KzLcg&amp;pp=ygUeZXJpYyB3ZWluc3RlaW4gZ2VvbWV0cmljIHVuaXR5">video</a> of Weinstein’s highly technical 2013 Oxford lecture. That few could follow it allowed Weinstein’s grievances to go unchecked. The situation changed in February 2021 when a detailed scientific <a href="https://files.timothynguyen.org/geometric_unity.pdf">rebuttal</a>, authored by myself and Theo Polya, was released.<sup data-fn="4c352d1d-79df-4aea-9105-5366061e4738"><a href="#4c352d1d-79df-4aea-9105-5366061e4738" id="4c352d1d-79df-4aea-9105-5366061e4738-link">1</a></sup> My critique directly tested Weinstein’s narrative: how would he respond to the thorough feedback he long claimed to want, which was free from the institutional and academic conventions he so strongly condemned?<sup data-fn="5f5a9faf-dd45-4cf3-8a2c-613959c6204a"><a href="#5f5a9faf-dd45-4cf3-8a2c-613959c6204a" id="5f5a9faf-dd45-4cf3-8a2c-613959c6204a-link">2</a></sup> The outcome was that instead of engaging with the substance of the critique, Weinstein and his circle deployed a playbook of tactics designed to suppress, deflect, and protect his contrarian brand at all costs.&nbsp;</p>



<h2>The Anatomy of the Grift</h2>



<h2>Suppress Your Critics&nbsp;</h2>



<p>When I initially released my response paper to Geometric Unity in 2021, unknown to the public, Weinstein immediately tried to suppress it. Now that four years have passed, and the risk of personal drama overshadowing the scientific legitimacy of the rebuttal is gone, I feel that the time has come to reveal the full story.&nbsp;</p>



<p>The most direct incident concerns Weinstein’s attempt to block my podcast <a href="https://www.youtube.com/watch?v=o31cGMENDTI">episode</a> on <em>Eigenbros</em>, a show specializing in physics hosted by two physics graduates. In our episode, I presented a two-hour whiteboard lecture giving a gentle exposition of my paper and a refutation of Weinstein’s baseless <a href="https://youtu.be/8_uiqjO1IEU?si=9uuereikfD_Rv8bW&amp;t=5078">claim</a> that he had originally discovered what are now known as the Seiberg-Witten equations. Our episode was released on Friday June 18, 2021. The following morning I received a message from <em>Eigenbros </em>on Discord, who wanted to speak to me on the phone. What they revealed was utterly despicable.&nbsp;</p>



<p>Weinstein had called them earlier and implied that if they kept the video online, there could be “legal action”. The intimidation worked; <em>Eigenbros </em>took the video down for the better part of that Saturday. In response, I immediately reassured them that the threat was a baseless bluff: what would be the basis for legal action? The <a href="https://timothynguyen.org/wp-content/uploads/2025/08/f752e-1mtc4frxmdfsfv5tj3itbrg.webp">copyright</a> notice on his Geometric Unity paper? Bullshit. After my phone call with <em>Eigenbros</em>, the video went back up. Rumors circulated about Weinstein’s potential involvement in the disruption, e.g. in a Facebook thread shown below.&nbsp;</p>



<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeuArp68LFzjWKrMJMm9P7pnfwNxJtMUnDUBEGIdh0dtSdp7xDvlCBY12JhVmWhmE7nRjxR3jlAnECZccmtbParRUk9A5Q-DQgSAAHCFFPEBGoJ67itqRhEFMKa_crMzj0nAFEuOg?key=lmBFMXKQq2DciE8O8OstVA" alt=""></figure>



<p>But at the request of the hosts who wanted to avoid public controversy, I never spoke of the situation publicly until now. This is the suppression incident I alluded to in my <a href="https://www.youtube.com/watch?v=j86WIfRfPDk&amp;t=6339s">interview</a> in the following month with Bob Wright, which Bob duly noted in his <a href="https://www.nonzero.org/p/is-eric-weinstein-a-crackpot">retrospective</a> of our conversation.&nbsp;</p>



<p>In fact, Weinstein’s suppression efforts began months earlier. Immediately after our paper’s release, Curt Jaimungal, a friend of Weinstein and host of the popular <em>Theories of Everything</em> podcast, reached out to invite me and my co-author on his show. He proposed a date “6-8 weeks” out, which in hindsight was clearly meant to coordinate with Weinstein’s own paper release on April 1st. I agreed to the interview, noting my co-author’s wish for anonymity. Curt was pleased and said he would follow up.</p>



<p>But the follow-up never came. When I reached out in May, Curt punted, citing a hectic schedule. The truth about the situation came out a month later during the <em>Eigenbros </em>affair. In his call, Weinstein revealed to the hosts that Curt had cancelled his interview with me. It’s not hard to put two and two together: Weinstein told Curt to call off my interview and then tried to use that cancellation as leverage to discredit me.</p>



<p>We thus have a disturbing truth. Eric Weinstein, the man who waxes poetic about a Distributed Idea Suppression Complex, is a hypocrite willing to use his own influence to squash criticism. Weinstein’s grievances and tale of persecution are frequently invoked to serve his narrative, yet when he receives opposition, he is willing to use his own power to suppress others.</p>



<h2>Attack the Person, Not the Science</h2>



<p>A month after the release of the GU paper, Weinstein was asked publicly about my critique in a now infamous Clubhouse <a href="https://ok.ru/video/2552683563745">discussion</a>, hosted by Brian Keating, a close friend of Weinstein, distinguished professor of physics at UC San Diego, and host of the popular <em>Into the Impossible</em> podcast. When pressed on scientific details, Weinstein didn’t address the physics and instead demanded to know the identity of my anonymous co-author Theo Polya.<sup data-fn="636785f9-94c8-4831-8051-8a3a00800328"><a href="#636785f9-94c8-4831-8051-8a3a00800328" id="636785f9-94c8-4831-8051-8a3a00800328-link">3</a></sup> Furthermore, he launched into an incoherent word salad of accusations about Theo Polya (and thus by extension myself), invoking references to misogyny, rape jokes, and 4chan. To borrow a phrase of Weinstein’s during his recent <a href="https://youtu.be/5m7LnLgvMnM?si=d-Zr_Hjeq2Ma2VIu&amp;t=2360">debate</a> with Sean Carroll: “How dare you Eric”.&nbsp;</p>



<p>The takeaway from that cringeworthy session was that Weinstein, together with Keating, refused to engage with our critique because they didn’t know who Theo Polya was. I feel it is hardly worth stating but I’ll do so anyway: what does the anonymity of one of two authors have to do with the merits of a critique? If Geometric Unity is so visionary, why would it matter? It’s a classic bait-and-switch. Weinstein, who demands his ideas be judged on merit alone, retreats from the science and falls back on personal attacks as soon as his ideas are actually challenged.&nbsp;</p>



<h2>Protect Your Tribe&nbsp;</h2>



<p>It is illuminating to see how Sabine Hossenfelder, Brian Keating, and Curt Jaimungal have been continuing to bolster Weinstein over the years, either directly through promoting his ideas when they know better or else indirectly by amplifying ambiguity about the status of his work.</p>



<p>I already discussed how Curt was complicit in cancelling my interview, which would have been critical of GU. Despite the seriousness he projects by hosting top scientists, he continues to treat GU as a substantive proposal, inviting many guests to give their (uninformed) opinion on GU while giving zero attention to the singularly critical work that is my rebuttal.<sup data-fn="a14f1ba7-9bfb-4219-a5b4-b1f10a812ce7"><a href="#a14f1ba7-9bfb-4219-a5b4-b1f10a812ce7" id="a14f1ba7-9bfb-4219-a5b4-b1f10a812ce7-link">4</a></sup> In fact, he requested a copy of my interview from <em>Eigenbros</em> when the video was briefly taken down, though he has never made any reference to it whatsoever. Even if one were to suppose Curt had disagreements with the critique, he is someone who platforms people with a diversity of viewpoints, including outsiders like Chris Langan and UAP specialists. That Curt would compromise his intellectual openness and rigor in order to promote and protect Weinstein is a tragic consequence of tribalism.&nbsp;</p>



<p>A similar and more pronounced story holds for Brian Keating, Weinstein’s most prominent promoter and staunch defender. Indeed, Brian continually hosts Eric Weinstein on his podcast (over 30 times as of this writing), many of which explicitly promote Geometric Unity. His strong support of Weinstein takes on many forms, including hosting him as a <a href="https://youtu.be/AzsZO3_WhDA?si=DqaR4LR-HPj5BOWz&amp;t=3330">scholar in residence</a> and <a href="https://www.youtube.com/watch?v=BVkUya368Es">speaker</a> at UC San Diego, expressing interest in experimentally <a href="https://youtu.be/AzsZO3_WhDA?si=dOd3orolxR5GKS0O&amp;t=3341">testing</a> GU, and inviting scientists to engage with Weinstein and his work (<a href="https://www.youtube.com/watch?v=N_aN8NnoeO0">here</a>, <a href="https://www.youtube.com/watch?v=OI0AZ4Y4Ip4">here</a>, and <a href="https://youtu.be/nhGwJLXzHs8?si=-IWdTLp8LBh5vgoe&amp;t=11638">here</a>). So if there is anyone else other than Weinstein who has a vested interest in getting to the bottom of my critique of GU, it would be Brian.</p>



<p>Two weeks after the release of my response paper and shortly after Sabine hosted my blogpost, Theo Polya and I sent the following message to Brian:</p>



<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfDsai3NIsE5uZ_7vs49HYf8oJ_1bzxzS_HGUKZ-2fSSLk87s_qsR5dM9ftTLXk1R1FbkUDa-OhgRvDpaHQGheKU1b-O8sYUNO_BOkJrfgiW5ADG0-HFHjmiJ2HpzPkoQE-X0OMkg?key=lmBFMXKQq2DciE8O8OstVA" alt=""></figure>



<p>We received no response, not even a reply to ask “Who is Theo Polya?”, the question used by both Weinstein and Brian in their Clubhouse discussion two months later to deflect engaging with our critique.</p>



<p>A few months after this email, I was contacted by Brandon Van Dyck shortly after he heard of my appearance on Wright’s podcast to record a follow-up <a href="https://www.youtube.com/watch?v=88E2pp7xafo">interview</a> (later provocatively titled “The Eric Weinstein/Timothy Nguyen Affair”). By pure coincidence, Brian had reached out to Brandon at the same time, expressing an interest in being on Brandon’s show. Brandon and I took the opportunity to propose to Brian that he could have a conversation with me (either about Weinstein, his book <em>Losing the Nobel Prize</em>, or any other topic of his choosing). In response, Brian withdrew his self-invitation, saying he was “not interested”, as we <a href="https://www.youtube.com/watch?v=88E2pp7xafo&amp;t=4980s">discussed</a> in our interview. The dodging continued when Michael Shermer and I publicly invited both Brian and Eric Weinstein for a discussion with me on Michael’s podcast (<a href="https://x.com/IAmTimNguyen/status/1706363756299563078?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1706363756299563078%7Ctwgr%5E18eddeb15fb70f561beaae1b214254eb0cfea785%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.redditmedia.com%2Fmediaembed%2F16ryy7c%2F%3Fresponsive%3Dtrueis_nightmode%3Dfalse">here</a> and <a href="https://x.com/michaelshermer/status/1706455867925602789">here</a>), after Michael was unsuccessful in asking Brian privately to participate. Again, no response.</p>



<p>It’s one thing to be unwilling to speak to critics. Casting aspersions on them to avoid defending your position is another matter entirely. On other podcasts, Brian has claimed I am <a href="https://www.youtube.com/live/1VIur9mfS-A?si=fRONBXLeltdR70Hk&amp;t=2449">not acting in good-faith</a> and that I’m trying to <a href="https://youtu.be/Hz3waSvyUDg?si=xB4XA2EqTyy7rX0u&amp;t=5635">“bait” him</a>, which are just additional examples of how Brian is going after the messenger rather than sticking to the science. The simple truth is that he and Weinstein have refused every opportunity to address the criticisms of GU in order to keep the charade running. Brian’s criticism of my character is also difficult to process in light of Weinstein’s attack of me on Clubhouse and the attempted suppression of my work. It is an embarrassing state of affairs to see a distinguished professor of physics so obviously acting in bad-faith.</p>



<p>Finally, we get to Sabine Hossenfelder, a theoretical physicist with over 1.5 million followers on YouTube who has built a reputation for giving sharp no-nonsense scientific critiques. Her role in the GU affair is more complicated and, in my case, unfortunate. Until recently, Sabine had been dismissive of Weinstein’s work. However, a recent <a href="https://www.youtube.com/watch?v=KiFYcuoK490&amp;vl=en">video</a> of hers (titled <em>Physicists are afraid of Eric Weinstein — and they should be</em>) paints a very different picture. In it, she waffles all over the place about her opinion of Weinstein’s work and how much time she’s spent looking at it. She’s inconsistent with her messaging, <a href="https://youtu.be/KiFYcuoK490?si=eEPbqjeYh6xg9VrH&amp;t=83">saying</a> that “she never looked into [Geometric Unity] in any detail” but clearly saying the <a href="https://youtu.be/mdu9KvLxHFg?si=55RPnltcqOzaMGzI&amp;t=386">opposite</a> in an older video. It honestly doesn’t interest me to micro-police how Sabine chooses to express her opinions – her statements have all the flair of entertainment and so cannot be taken too literally. The problem is that Sabine appears to be employing ambiguity to present two incompatible positions: on the one hand, she wants to play honest critic on scientific matters (“Eric’s theory is a waste of time”), but she also wants to claim that Weinstein and his bandwagon deserve credit for being contrarian. It’s this latter step where she’s acting dishonestly, as I will now explain.</p>



<p>Rewinding to July of 2021, I reached out to Sabine, whom I had recently <a href="https://www.youtube.com/watch?v=QX0nhdHCU60">interviewed</a> at Google, and I explained to her my frustration with the aforementioned Clubhouse episode (which she had watched). To my delight, she revealed she was speaking to Brian soon and would ask him about me. But when Sabine got back to me a few weeks later over Zoom, I was shocked by the message she brought. Sabine repeated and supported Brian’s claims that my co-author’s anonymity was the cause of Brian’s refusal to speak to me. She recounts that she has had to deal with anonymous trolls herself and that she doesn’t feel the need to respond to them. When I raised the fact that <em>I</em> wasn’t anonymous, she said that Brian deflected, noting that GU was Weinstein’s theory and that it wasn’t his prerogative to talk to me about it. When asked why he declined going on Brandon’s podcast to talk to me, his excuse was that it was insulting to be invited only to speak about Weinstein and not himself (contrary to what was offered to Brian and that Brian had in fact invited himself onto Brandon’s podcast). In summary, I got nowhere with Sabine talking to Brian on my behalf.&nbsp;</p>



<p>At the time, I owed Sabine a tremendous debt of gratitude for hosting my blogpost in March 2021 that advertised my critique of GU. I had no public profile back then and her platform gave my work the reach and legitimacy it needed to get off the ground. Because of this, I kept Sabine’s confusing report about Brian to myself. I even <a href="https://x.com/IAmTimNguyen/status/1705617321388015865">met</a> Sabine in person at the HowTheLightGetsIn Festival in London 2023 and found her very agreeable and sincere in person. Nevertheless, I always had the suspicion that Sabine might really have been shielding Brian by gaslighting me. That intuition was confirmed in Sabine’s recent video in which she claims that Brian “deserves credit for not chickening out and standing for Eric” and that Curt’s explainer video of GU was “courageous”. Such words are baffling given our private conversation and her being qualified to understand the validity of my critique of GU hosted on her own blog. I now clearly see the situation for what it is. Sabine is just as guilty of grifting as Brian and Curt.</p>



<p>This incident is particularly unfortunate in Sabine’s case, as she has cultivated a reputation as a respected physics communicator and science writer known for her sharp critiques of theoretical physics. Regardless of one’s stance with her contrarian views, she has in the past offered many perspectives on the state of theoretical physics worthy of attention, both on her blog and her book <em>Lost in Math</em>. Thus, her most recent video concerning Weinstein reflects poorly on her not only because of the personal circumstances I’ve now disclosed but also due to its strong departure from reason: in it, she also labels the theoretical physics community a “f-cking hypocrisy” and “scam” by equating the quality of their work with that of Weinstein’s Geometric Unity. This claim is so outrageous that physicist Christian Ferko shortly afterwards made a detailed presentation <a href="https://www.youtube.com/watch?v=oipI5TQ54tA">debunking</a> this absurd false equivalence. It is a sad state of affairs to see Sabine undermine her own credibility by exploiting the sensationalism surrounding Weinstein and GU merely to incite outrage and air her grievances against academic and physics communities. And like Weinstein, she is willing to censor her critics rather than address them.<sup data-fn="40b141e3-7725-496a-b19e-421482b40630"><a href="#40b141e3-7725-496a-b19e-421482b40630" id="40b141e3-7725-496a-b19e-421482b40630-link">5</a></sup></p>



<h2>The Jury is Already In</h2>



<p>Scientific disagreements are intricate matters that require the attention of highly trained experts. However, for laypersons to be able to make up their own minds on such issues, they have to rely on proxies for credibility such as persuasiveness and conviction. This is the vulnerability that contrarians exploit, as they are often skilled in crafting the optics and rhetoric to support their case. Indeed, Weinstein and Hossenfelder’s strong personalities and their sowing of distrust in institutions enable them to persuade others of the correctness of their views when they deviate from those of experts. Thus, I include this section to show that even if one were to rely on social cues alone, there is in fact no controversy about the illegitimacy of Geometric Unity among those who are close to Weinstein or who are qualified to judge. The success of physics grifters has relied on the fact that they make more noise than those who have quietly moved on.</p>



<h2>Lex Fridman</h2>



<p>Let’s start with podcasting star Lex Fridman. Word got to Lex of my paper with Theo Polya when it was released and all three of us got onto a video call in March 2021 to discuss the situation. Lex proposed hosting us alongside Weinstein for a discussion of Geometric Unity on his podcast and we all agreed (Theo Polya was willing to reveal himself given his affinity for Lex’s podcast). The only question is whether Weinstein, who had already been on Lex’s show four times, would agree.&nbsp;</p>



<p>Two weeks later, when I got in touch with Lex via email, he disappointingly changed topics and said he did not discuss what we had proposed with Weinstein (or Joe Rogan). In hindsight, this was clearly not the case. Weinstein released his Geometric Unity paper on April 1, debuting it on Joe Rogan’s podcast and then on Brian Keating’s podcast the following day. Conspicuously, Weinstein did not appear on Lex’s podcast. On Rogan’s podcast, Rogan’s skepticism and pushback was full-on with his interview with Weinstein, noting “there has been some criticism” and not letting Weinstein off the hook from his obscurantism. Rogan certainly hadn’t read Sabine’s blog to become aware of my critique; Lex had tipped him off. Four years later, Weinstein has not returned to Lex’s show.</p>



<h2>Marcus du Sautoy</h2>



<p>Marcus is the Oxford mathematician who hosted Weinstein’s GU talk and who knows Weinstein well from their overlapping time in academia. I first met Marcus virtually in 2022 when I hosted him for a <a href="https://youtu.be/1wJ6LMqPm9I?si=WPAb8ekHlDDoxjPi&amp;t=3305">talk</a> at Google. During the Q&amp;A of that conversation, I asked Marcus, who has been conspicuously absent from the Geometric Unity saga since 2013, about Weinstein and his work (it was a carefully thought-out question as Marcus was initially unwilling to field a question about Weinstein during our preparation). Marcus’s reply shows a man distancing himself from Geometric Unity, stating he’s less qualified than others to assess it, a sharp U-turn from his glowing <a href="https://www.theguardian.com/science/blog/2013/may/23/roll-over-einstein-meet-weinstein#:~:text=What%20are%20we%20to%20make,most%20intractable%20problems%20in%20physics%3F&amp;text=There%20are%20a%20lot%20of,universe%20is%20missing%2C%20for%20example.">comparison</a> of Weinstein with Einstein in 2013. More recently, I also met Marcus in person at the <a href="https://www.eafestival.com/2025-ea-festival-lineup">2025 East Anglia Festival</a>. Over lunch with Marcus and his wife, Weinstein’s name briefly came up, but Marcus offered no positive comments about him or his ideas, and our conversation continued.</p>



<h2>Economists, Computer Scientists, and Physicists</h2>



<p>Despite advocating against established institutions and credentialism, Eric Weinstein readily leverages visits to prestigious institutions to enhance his public image. One notable instance was his November 2021 <a href="https://x.com/EricRWeinstein/status/1459180900789501960">visit</a> to UChicago, where he presented <a href="https://web.archive.org/web/20211111031144/https://economics.uchicago.edu/sites/economics.uchicago.edu/files/Welfare_Chicago_Draft.pdf">work</a> co-authored with his wife, Pia Malaney, applying gauge theory to economics. Unlike his Geometric Unity work, which lacked sufficient detail to merit serious attention (consisting only of a YouTube lecture and an inadequately written paper), this UChicago presentation was accompanied by a well-formatted paper containing significant technical details. This allowed me to quickly analyze the paper, write a <a href="https://arxiv.org/abs/2112.03460">rebuttal</a> (this time without an anonymous co-author), and upload it to arXiv.</p>



<p>Note that before this incident, the Malaney-Weinstein work received little attention due to its limited significance and impact. Despite this, Weinstein has <a href="https://youtu.be/QxnkGymKuuI?t=3665">suggested</a> that it is worthy of a Nobel prize and <a href="https://www.youtube.com/watch?v=uFirZANoiHI">claimed</a> (with the support of Brian Keating) that it is “the most deep insight in mathematical economics of the last 25-50 years”. In that same podcast episode, Weinstein also makes the incendiary claim that Juan Maldacena stole such ideas from him and his wife. For those unable to judge the situation, I offer this appreciative reply from one of the economics professors at UChicago who sponsored Eric’s visit:</p>



<figure><img src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfDBkh3lmYvEZVJDl-OhbvQA1u4-UCVaxAHP1xG-MpN31xWS_euHpCigkNHdPu53O8mY8qAseiAvBRRcSvD9oJnudAtwk8PqGaUxfxN8fHFW07zkhxC7bevc0fI-8fLk-RzonyRkg?key=lmBFMXKQq2DciE8O8OstVA" alt=""></figure>



<p>Has Weinstein responded to my economics rebuttal now that the anonymous Theo Polya is no longer an author? You already know the answer.</p>



<p>And in case you’re wondering, there’s also equivalent endorsement of my response to Geometric Unity. For instance, see my <a href="https://www.youtube.com/watch?v=wd-0COLM8oc&amp;t=928s">interview</a> with computer scientist Scott Aaronson. There’s also Sean Carroll, who after his debate with Weinstein on Piers Morgan, confided to me that he directs everyone who asks him about Weinstein’s work to the rebuttal I published. Most recently, Christian Ferko has done an excellent job giving his own <a href="https://youtu.be/jz7Trp5rTOY?si=kpMFc7wJgefIoXyt&amp;t=1837">take</a> on the flaws of GU based on the groundwork laid out by my analysis.</p>



<p>Overall, the consistent theme is that the few professional scientists who have examined Weinstein’s work and are not influenced by audience capture have supported the critique I put forth. The fact that most scientists have ignored GU says less about a failing within the scientific establishment and more about the group of contrarians who continue to entertain Weinstein’s ideas.</p>



<h2>Conclusion: The Real Cost of the Grift</h2>



<p>When I wrote my critique of Geometric Unity, I thought I was simply engaging in math and physics. I never imagined it would take me on a journey of hypocrisy and censorship. To dismiss this story as mere internet drama is to overlook the troubling reality underneath it all: Eric Weinstein and several of our most prominent science communicators – nay, science <em>populists</em> – are willing to distort the truth to suit their own interests. Many eyes and ears tune into Hossenfelder, Keating, and Jaimungal, who frequently appear alongside distinguished scientists that are likely unaware of their involvement in the grift. While these popularizers are able to fulfill their audience’s needs to understand science, they simultaneously enable them to hold unconventional views that may contradict the very science they promote. That the three of them have done valuable work in making science accessible to the public is precisely what makes their conduct disconcerting.</p>



<p>As a former fan of Weinstein who became a critic, I wrote my rebuttal to Geometric Unity expecting a scientific debate. Instead, I received a grim lesson about the state of modern science communication: when personal brands and tribal loyalties become the main focus, scientific integrity is sacrificed. The unfortunate truth is that some of the most visible voices in science are more interested in being celebrated than in being correct. And in a world where public trust in expertise is already in peril, that is a betrayal we simply cannot afford.</p>


<ol><li id="4c352d1d-79df-4aea-9105-5366061e4738">The rebuttal as well as the attention it received through various podcasts and discussions have been compiled together <a href="http://www.timothynguyen.org/geometric-unity">here</a>. I would like to note that I found my way to Weinstein by initially being a fan of him and his podcast, until I realized, like many others have, that his rhetoric is mostly performative. My <a href="https://decoding-the-gurus.captivate.fm/episode/special-episode-interview-with-tim-nguyen-on-geometric-unity">interview</a> on <em>Decoding the Gurus</em> goes into this episode in detail. <a href="#4c352d1d-79df-4aea-9105-5366061e4738-link">↩︎</a></li><li id="5f5a9faf-dd45-4cf3-8a2c-613959c6204a">See <a href="https://youtu.be/YjsPb3kBGnk?si=V2N9ZEay69hNbjEm&amp;t=1755">here</a>, <a href="https://www.youtube.com/live/QCKCQNFsJUw?si=upCV9obE6dtHvwcM&amp;t=4936">here</a>, and <a href="https://youtu.be/ifX_JnBfxTY?si=TWoVuol_M4jexLV8&amp;t=7850">here</a>. <a href="#5f5a9faf-dd45-4cf3-8a2c-613959c6204a-link">↩︎</a></li><li id="636785f9-94c8-4831-8051-8a3a00800328">It should be noted that all evidence suggests that Weinstein does know who Theo Polya is. Under the Discord handle FieldTheorist, Theo Polya had several public Discord conversations with Weinstein in 2020. Weinstein also viewed Theo Polya’s LinkedIn profile sometime after the release of our paper. <a href="#636785f9-94c8-4831-8051-8a3a00800328-link">↩︎</a></li><li id="a14f1ba7-9bfb-4219-a5b4-b1f10a812ce7">Curt created a three hour explainer <a href="https://www.youtube.com/watch?v=AThFAxF7Mgw&amp;pp=0gcJCfwAo7VqN5tD">video</a> on Geometric Unity which completely ignores the problems with the work (in particular, my rebuttal). Curt’s followup <a href="https://www.youtube.com/watch?v=ILlhFKuu3NQ&amp;pp=0gcJCfwAo7VqN5tD">interview</a> with Weinstein immediately after his debate with Sean Carroll on the Piers Morgan Show also made no attempt to engage in the content of Geometric Unity.<br> <a href="#a14f1ba7-9bfb-4219-a5b4-b1f10a812ce7-link">↩︎</a></li><li id="40b141e3-7725-496a-b19e-421482b40630">I ended up confronting Sabine about her video on X <a href="https://x.com/IAmTimNguyen/status/1945731721577566684">here</a> and <a href="https://x.com/IAmTimNguyen/status/1945732372428632468">here</a>, which I will admit was less than measured and could have been repackaged as a private email instead of a public callout. In my defence, I was speaking in a manner consistent with Sabine’s confrontational style and argumentativeness which she frequently displays on&nbsp; X. In her own <a href="https://youtu.be/E3y-Z0pgupg?si=zeBihAShJHXatf4x&amp;t=2506">words</a>, “I’ve always been more the kind of person who said, I think that’s bullshit. And then I just say that it’s bullshit.” Nevertheless, Sabine felt it proper to not only block me on X, but also delete my blogpost criticizing Weinstein (Wayback <a href="https://t.co/in6MtE71Oj">link</a>). <a href="#40b141e3-7725-496a-b19e-421482b40630-link">↩︎</a></li></ol>


<p><em>Acknowledgements: I would like to thank Ieva Cepaite, Richard Easther, Daniel Gilbert, Chris Kavanaugh, and Tim Scarfe for their valuable feedback on earlier drafts of this post.</em></p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Core of Rust (166 pts)]]></title>
            <link>https://jyn.dev/the-core-of-rust/</link>
            <guid>44974688</guid>
            <pubDate>Thu, 21 Aug 2025 16:27:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jyn.dev/the-core-of-rust/">https://jyn.dev/the-core-of-rust/</a>, See on <a href="https://news.ycombinator.com/item?id=44974688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        
<article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  

	

  
    

  <div itemprop="articleBody">
    <p><strong>NOTE: this is not a rust tutorial.</strong></p>
<blockquote>
<p>Every year it was an incredible challenge to fit teaching Rust into lectures since you basically need all the concepts right from the start to understand a lot of programs. I never knew how to order things. The flip side was that usually when you understand all the basic components in play lots of it just fits together. i.e. there's some point where the interwovenness turns from a barrier into something incredibly valuable and helpful.
—<a href="https://donsz.nl/">Jana Dönszelmann</a></p>
</blockquote>
<h2 id="vision">Vision<a href="#vision" aria-label="Anchor link for: vision"></a>
</h2>
<p>One thing I admire in a language is a strong vision. <a href="https://www.uiua.org/">Uiua</a>, for example, has a very strong vision: what does it take to eliminate all local named variables from a language? <a href="https://ziglang.org/">Zig</a> similarly has a strong vision: explicit, simple language features, easy to cross compile, drop-in replacement for C.</p>
<p>Note that you don’t have to agree with a language’s vision to note that it <em>has</em> one. I expect most people to find Uiua unpleasant to program in. That’s fine. You are not the target audience.</p>
<p>There’s a famous quote by Bjarne Strousup that goes “Within C++, there is a much smaller and cleaner language struggling to get out.” Within Rust, too, there is a much smaller and cleaner language struggling to get out: one with a clear vision, goals, focus. One that is coherent, because its features <em>cohere</em>. This post is about that language.</p>
<hr>
<h2 id="learning-rust-requires-learning-many-things-at-once">Learning Rust requires learning many things at once<a href="#learning-rust-requires-learning-many-things-at-once" aria-label="Anchor link for: learning-rust-requires-learning-many-things-at-once"></a>
</h2>
<p>Rust is hard to learn. Not for lack of trying—many, many people have spent person-years on improving the diagnostics, documentation, and APIs—but because it’s complex. When people first learn the language, they are learning many different interleaving concepts:</p>
<ul>
<li>first class functions</li>
<li>enums</li>
<li>pattern matching</li>
<li>generics</li>
<li>traits</li>
<li>references</li>
<li>the borrow checker</li>
<li><code>Send</code>/<code>Sync</code></li>
<li><code>Iterator</code>s</li>
</ul>
<p>These concepts interlock. It is very hard to learn them one at a time because they interact with each other, and each affects the design of the others. Additionally, the standard library uses all of them heavily.</p>
<p>Let’s look at a Rust program that does something non-trivial:<sup id="fr-1-1"><a href="#fn-1">1</a></sup></p>
<pre data-lang="rust"><code data-lang="rust"><span><span>#</span><span>!</span><span>/</span>usr<span>/</span>bin<span>/</span>env <span>-</span>S cargo <span>-</span>Zscript
</span><span><span>-</span><span>-</span><span>-</span>
</span><span>package<span>.</span>edition <span>=</span> <span><span>"</span>2024<span>"</span></span>
</span><span><span><span>[</span>dependencies<span>]</span></span>
</span><span>notify <span>=</span> <span><span>"</span>=8.2.0<span>"</span></span>
</span><span><span>-</span><span>-</span><span>-</span>
</span><span><span>use</span> <span>std<span>::</span></span><span>path<span>::</span></span>Path<span>;</span>
</span><span><span>use</span> <span>notify<span>::</span></span><span><span>{</span>RecursiveMode<span>,</span> Watcher</span><span><span>}</span></span><span>;</span>
</span><span><span><span><span>fn</span> </span><span>main</span></span><span><span><span>(</span></span><span><span><span>)</span></span></span></span><span> <span><span>-&gt;</span> <span><span>Result</span><span>&lt;</span><span>(</span><span>)</span>, <span>notify<span>::</span></span>Error<span>&gt;</span></span></span> </span><span><span><span>{</span>
</span></span></span><span><span><span>    <span>let</span> paths <span>=</span> <span><span>[</span><span><span>"</span>pages<span>"</span></span><span>,</span> <span><span>"</span>templates<span>"</span></span><span>,</span> <span><span>"</span>static<span>"</span></span><span>]</span></span><span>;</span>
</span></span></span><span><span><span>    <span>let</span> <span>mut</span> watcher <span>=</span> <span>notify<span>::</span></span>recommended_watcher<span><span>(</span><span><span><span>|</span></span></span><span><span><span>result</span><span>:</span> <span><span>Result</span><span>&lt;</span><span>notify<span>::</span></span>Event, <span>_</span><span>&gt;</span></span><span>|</span></span> </span><span><span><span>{</span>
</span></span></span></span></span></span><span><span><span><span><span><span>        <span>if</span> <span>let</span> <span>Ok</span><span><span>(</span>event</span><span><span>)</span></span> <span>=</span> result <span><span>{</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            <span>let</span> paths<span>:</span> <span><span>Vec</span><span>&lt;</span><span>String</span><span>&gt;</span></span> <span>=</span> event<span>.</span>paths
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                <span>.</span><span>into_iter</span><span><span>(</span></span><span><span>)</span></span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                <span>.</span><span>map</span><span><span>(</span><span><span><span>|</span></span></span><span><span><span>path</span><span>|</span></span> </span><span>path<span>.</span><span>display</span><span><span>(</span></span><span><span>)</span></span><span>.</span><span>to_string</span><span><span>(</span></span><span><span>)</span></span></span></span><span><span>)</span></span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>                <span>.</span><span>collect</span><span><span>(</span></span><span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>            <span>println!</span><span><span>(</span></span><span><span><span>"</span>:<span>{:?}</span> <span>{}</span><span>"</span></span></span><span><span>,</span> event<span>.</span>kind<span>,</span> paths<span>.</span><span>join</span><span><span>(</span><span><span>"</span> <span>"</span></span></span><span><span>)</span></span><span>)</span></span><span>;</span>
</span></span></span></span></span></span></span><span><span><span><span><span><span><span>        </span><span><span>}</span></span>
</span></span></span></span></span></span><span><span><span><span><span><span>    </span><span><span>}</span></span></span></span><span><span>)</span></span><span>?</span><span>;</span>
</span></span></span><span><span><span>    <span>for</span> path <span>in</span> paths <span><span>{</span>
</span></span></span></span><span><span><span><span>        watcher<span>.</span><span>watch</span><span><span>(</span><span>Path<span>::</span></span>new<span><span>(</span>path</span><span><span>)</span></span><span>,</span> <span>RecursiveMode<span>::</span></span>Recursive</span><span><span>)</span></span><span>?</span><span>;</span>
</span></span></span></span><span><span><span><span>    </span><span><span>}</span></span>
</span></span></span><span><span><span>    <span>loop</span> <span><span>{</span> <span>std<span>::</span></span><span>thread<span>::</span></span>park<span><span>(</span></span><span><span>)</span></span><span>;</span> </span><span><span>}</span></span> <span> sleep forever
</span></span></span></span><span><span><span></span><span><span>}</span></span></span>
</span></code></pre>
<p>I tried to make this program as simple as possible: I used only the simplest iterator combinators, I don't touch <code>std::mpsc</code> at all, I don't use async, and I don't do any complicated error handling.</p>
<p>Already, this program has many interleaving concepts. I'll ignore the module system and macros, which are mostly independent of the rest of the language. To understand this program, you need to know that:</p>
<ul>
<li><code>recommended_watcher</code> and <code>map</code> take a function as an argument. In our program, that function is constructed inline as an anonymous function (closure).</li>
<li>Errors are handled using something called <code>Result</code>, not with exceptions or error codes. I happened to use <code>fn main() -&gt; Result</code> and <code>?</code>, but you would still need to understand Result even without that, because Rust does not let you access the value inside unless you check for an error condition first.</li>
<li>Result takes a generic error; in our case, <code>notify::Error</code>.</li>
<li>Result is an data-holding enum that can be either Ok or Err, and you can check which variant it is using pattern matching.</li>
<li>Iterators can be traversed either with a <code>for</code> loop or with <code>into_iter()</code>. <sup id="fr-2-1"><a href="#fn-2">2</a></sup> <code>for</code> is eager and <code>into_iter</code> is lazy. <code>iter</code> has different ownership semantics than `into_iter.</li>
</ul>
<p>If you want to modify this program, you need to know some additional things:</p>
<ul>
<li><code>println</code> can only print things that implement the traits <code>Display</code> or <code>Debug</code>. As a result, <code>Path</code>s cannot be printed directly.</li>
<li><code>path.display()</code> returns a struct that borrows from the path. Sending it to another thread (e.g. through a channel) won't work, because <code>event.paths</code> goes out of scope when the closure passed to <code>recommended_watcher</code> finishes running. You need to convert it to an owned value or pass <code>event.paths</code> as a whole.
<ul>
<li>As an aside, this kind of thing encourages people to break work into "large" chunks instead of "small" chunks, which I think is often good for performance in CPU-bound programs, although as always it depends.</li>
</ul>
</li>
<li><code>recommended_watcher</code> only accepts functions that are <code>Send + 'static</code>. Small changes to this program, such as passing the current path into the closure, will give a compile error related to ownership. Fixing it requires learning the <code>move</code> keyword, knowing that closures borrow their arguments by default, and the meaning of <code>'static</code>.
<ul>
<li>If you are using <code>Rc&lt;RefCell&lt;String&gt;&gt;</code>, which is often recommended for beginners<sup id="fr-3-1"><a href="#fn-3">3</a></sup>, your program will need to be rewritten from scratch (either to use Arc/Mutex or to use exterior mutability). For example, if you wanted to print changes from the main thread instead of worker threads to avoid interleaving output, you couldn't simply push to the end of an <code>all_changes</code> collection, you would have to use <code>Arc&lt;Mutex&lt;Vec&lt;Path&gt;&gt;&gt;</code> in order to communicate between threads.</li>
</ul>
</li>
</ul>
<p>This is a <em>lot</em> of concepts for a 20 line program. For comparison, here is an equivalent javascript program:</p>
<pre data-lang="javascript"><code data-lang="javascript"><span><span><span>const</span> <span><span><span>fs</span></span> </span><span>=</span> <span><span>require</span></span><span>(</span><span><span>'</span>fs<span>'</span></span><span>)</span></span><span>;</span>
</span><span><span><span>const</span> <span><span><span>paths</span></span> </span><span>=</span><span> <span>[</span><span><span>'</span>pages<span>'</span></span><span>,</span> <span><span>'</span>templates<span>'</span></span><span>,</span> <span><span>'</span>static<span>'</span></span><span>]</span></span></span><span>;</span>
</span><span><span>for</span> <span>(</span><span><span>let</span> <span><span><span>path</span></span> </span></span><span>of</span> <span>paths</span><span>)</span> <span><span>{</span>
</span></span><span><span>  <span><span>fs</span><span>.</span><span>watch</span></span><span>(</span><span>path</span><span>,</span><span> <span><span>(</span><span>eventType</span><span>,</span> <span>filename</span><span>)</span></span> </span><span><span>=&gt;</span> <span><span>{</span>
</span></span></span></span><span><span><span><span>    <span>if</span> <span>(</span><span>filename</span> <span>!==</span> <span>null</span><span>)</span> <span><span>{</span>
</span></span></span></span></span><span><span><span><span><span>      <span><span>console</span><span>.</span><span>log</span></span><span>(</span><span><span>`</span><span><span>${</span></span><span><span><span>kind</span></span><span>}</span></span> <span><span>${</span></span><span><span><span>filename</span></span><span>}</span></span><span>`</span></span><span>)</span>
</span></span></span></span></span><span><span><span><span><span>    <span>}</span></span>
</span></span></span></span><span><span><span><span>  <span>}</span></span></span><span>)</span><span>;</span>
</span></span><span><span><span>}</span></span>
</span><span><span>await</span> <span>(</span><span><span>new</span> <span>Promise</span><span>(</span><span><span><span>(</span><span>)</span></span> </span><span><span>=&gt;</span> <span><span>{</span><span>}</span></span></span><span>)</span></span><span>)</span><span>;</span> <span><span>//</span></span><span> sleep forever</span>
</span></code></pre>
<p>For this JS program, you need to understand:</p>
<ul>
<li>first class functions</li>
<li>nullability</li>
<li>yeah that's kinda it.</li>
</ul>
<p>I'm cheating a little here because <code>notify</code> returns a list of paths and <code>node:fs/watch</code> doesn't. But only a little.</p>
<p>My point is not that JS is a simpler language; that's debatable. My point is that you can do things in JS without understanding the whole language. It's very hard to do non-trivial things in Rust without understanding the whole core.</p>
<h2 id="rust-s-core-is-interwoven-on-purpose">Rust's core is interwoven on purpose<a href="#rust-s-core-is-interwoven-on-purpose" aria-label="Anchor link for: rust-s-core-is-interwoven-on-purpose"></a>
</h2>
<p>The previous section makes it out to seem like I'm saying all these concepts are bad. I'm not. Rather the opposite, actually. Because these language features were designed in tandem, they interplay very nicely:</p>
<ul>
<li>Enums without pattern matching <a href="https://en.cppreference.com/w/cpp/utility/variant.html">are very painful to work with</a> and pattern matching without enums <a href="https://www.hillelwayne.com/post/python-abc/">has very odd semantics</a></li>
<li><code>Result</code> and <code>Iterator</code>s are impossible to implement without generics (or duck-typing, which I think of as type-erased generics)</li>
<li><code>Send</code>/<code>Sync</code>, and the preconditions to <code>println</code>, are impossible to encode without traits—and this often comes up in other languages, for example printing a function in clojure shows something like <code>#object[clojure.core$map 0x2e7de98a "clojure.core$map@2e7de98a"]</code>. In Rust it gives a compile error unless you opt-in with Debug.</li>
<li><code>Send</code> / <code>Sync</code> are only possible to enforce because the borrow checker does capture analysis for closures. Java, which is <em>wildly</em> committed to thread-safety by the standards of most languages, cannot verify this at compile time and so has to <a href="https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/text/SimpleDateFormat.html#synchronization">document synchronization concerns explicitly</a> instead.</li>
</ul>
<p>There are more interplays than I can easily describe in a post, and all of them are what make Rust what it is.</p>
<p>Rust has other excellent language features—for example the <a href="https://doc.rust-lang.org/nightly/reference/inline-assembly.html">inline assembly syntax</a> is a work of art, props to <a href="https://github.com/amanieu">Amanieu</a>. But they are not interwoven into the standard library in the same way, and they do not affect the way people <em>think</em> about writing code in the same way.</p>
<h2 id="a-smaller-rust">A smaller Rust<a href="#a-smaller-rust" aria-label="Anchor link for: a-smaller-rust"></a>
</h2>
<p>without.boats wrote a post in 2019 titled <a href="https://without.boats/blog/notes-on-a-smaller-rust/">"Notes on a smaller Rust"</a> (and a follow-up <a href="https://without.boats/blog/revisiting-a-smaller-rust/">revisiting</a> it). In a manner of speaking, that smaller Rust <em>is</em> the language I fell in love with when I first learned it in 2018. Rust is a lot bigger today, in many ways, and the smaller Rust is just a nostalgic rose-tinted memory. But I think it's worth studying as an example of how well <a href="https://en.wikipedia.org/wiki/Orthogonality#Computer_science">orthogonal</a> features can compose when they're designed as one cohesive whole.</p>
<p>If you liked this post, consider reading <a href="https://matklad.github.io/2020/07/15/two-beautiful-programs.html">Two Beautiful Rust Programs</a> by matklad.</p>
<!-- UnsafeCell -->
<!--I see two main periods at which people have trouble. The second is when they try and do type shenanigans. There’s a lot to say here but I don’t -->


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bank forced to rehire workers after lying about chatbot productivity, union says (294 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/08/bank-forced-to-rehire-workers-after-lying-about-chatbot-productivity-union-says/</link>
            <guid>44974365</guid>
            <pubDate>Thu, 21 Aug 2025 15:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/08/bank-forced-to-rehire-workers-after-lying-about-chatbot-productivity-union-says/">https://arstechnica.com/tech-policy/2025/08/bank-forced-to-rehire-workers-after-lying-about-chatbot-productivity-union-says/</a>, See on <a href="https://news.ycombinator.com/item?id=44974365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>As banks around the world <a href="https://archive.ph/asfc6">prepare</a> to replace many thousands of workers with AI, Australia's biggest bank is scrambling to rehire 45 workers after allegedly lying about chatbots besting staff by handling higher call volumes.</p>
<p>In a <a href="https://www.fsunion.org.au/Hub/Content/News_and_publications/Member_updates/2025/WIN-CBA-backflips-on-customer-service-job-cuts.aspx">statement</a> Thursday flagged <a href="https://www.bloomberg.com/news/articles/2025-08-21/commonwealth-bank-reverses-job-cuts-decision-over-ai-chatbots">by Bloomberg</a>, Australia's main financial services union, the Finance Sector Union (FSU), claimed a "massive win" for 45 union members whom the Commonwealth Bank of Australia (CBA) had replaced with an AI-powered "voice bot."</p>
<p>The FSU noted that some of these workers had been with CBA for decades. Those workers in particular were shocked when CBA announced last month that their jobs had become redundant. At that time, CBA claimed that launching the chatbot supposedly "led to a reduction in call volumes" by 2,000 a week, FSU said.</p>
<p>But "this was an outright lie," fired workers told FSU. Instead, call volumes had been increasing at the time they were dismissed, with CBA supposedly "scrambling"—offering staff overtime and redirecting management to join workers answering phones to keep up.</p>
<p>To uncover the truth, FSU escalated the dispute to a fair work tribunal, where the union accused CBA of failing to explain how workers' roles were ruled redundant. The union also alleged that CBA was hiring for similar roles in India, Bloomberg noted, which made it appear that CBA had perhaps used the chatbot to cover up a shady pivot to outsource jobs.</p>
<p>While the dispute was being weighed, CBA admitted that "they didn’t properly consider that an increase in calls" happening while staff was being fired "would continue over a number of months," FSU said.</p>
<p>"This error meant the roles were not redundant," CBA confirmed at the tribunal.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forced every engineer to take sales calls.They rewrote our platform in 2 weeks (299 pts)]]></title>
            <link>https://old.reddit.com/r/Entrepreneur/comments/1mw5yfg/forced_every_engineer_to_take_sales_calls_they/</link>
            <guid>44974230</guid>
            <pubDate>Thu, 21 Aug 2025 15:46:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/Entrepreneur/comments/1mw5yfg/forced_every_engineer_to_take_sales_calls_they/">https://old.reddit.com/r/Entrepreneur/comments/1mw5yfg/forced_every_engineer_to_take_sales_calls_they/</a>, See on <a href="https://news.ycombinator.com/item?id=44974230">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Submission/commenting Rules:</h2>

<p>1) <strong>10 comment karma in <a href="https://old.reddit.com/r/Entrepreneur">/r/Entrepreneur</a> to post</strong><br>
To lead a discussion in <a href="https://old.reddit.com/r/Entrepreneur">/r/Entrepreneur</a>, we require a minimum of 10 comment karma within our subreddit. This is earned by contributing thoughtful comments to others' discussions. This policy not only helps reduce spam but ensures that you understand the community's dynamics—what types of posts resonate, and what might be overlooked or poorly received.</p>

<p>Our goal is to help your posts gain traction. Take some time to engage with the community and discover how we can best support you.</p>

<p>2) <strong>No Promotion</strong><br>
Posts and comments must NOT be made for the primary purpose of selling or promoting yourself, your company or any service.</p>

<p>Dropping URLs, asking users to DM you, check your profile, or comment for private resources will all lead to a permanent ban.</p>

<p>It is acceptable to cite your sources, however, there should not be an explicit solicitation, advertisement, or clear promotion for the intent of awareness.</p>

<p>3) <strong>No Personal Attacks</strong><br>
Constructive criticism of ideas, concepts, approaches, or plans is encouraged. Embracing reality, even when it is challenging, is a crucial aspect of business. However, personal attacks, hostility, or incitement to conflict are strictly prohibited and will not be tolerated in this community.</p>

<p>If you encounter threats or harassment, please report them immediately. Avoid engaging with users who are harassing you or others.</p>

<p>4) <strong>Links Allowed as Supporting Material ONLY</strong><br>
This rule exists to foster meaningful discussion. If you wish to share links, they must be directly relevant, clearly described, and essential to the conversation. Your post should include sufficient context within Reddit to enable a productive dialogue.</p>

<p>5) <strong>No “How To Get Rich Quick” posts</strong><br>
This community is NOT for quick money schemes. Entrepreneurship is often a challenging and demanding journey, requiring effort, hard work, grit, and determination. Posts asking how to make $X, without a specific or reasonable idea, will not be tolerated, as they do not contribute meaningfully to the discussions in this subreddit.</p>

<p>6) <strong>Avoid unprofessional communication</strong><br>
As a professional subreddit, we expect all members to uphold a standard of reasonable decorum. Treat fellow entrepreneurs with the same respect you would show a colleague. While we don't have an HR department, that’s no excuse for aggressive, foul, or unprofessional behavior. NSFW topics are permitted, but they must be clearly labeled. When in doubt, label it.</p>

<p>AI-generated content is not acceptable to be posted. If your posts or comments were generated with AI, you may face a permanent ban.  </p>

<p>7) <strong>No free offerings threads</strong><br>
Our "Thank You Thursday" threads are designated for any promotions, offers, or discounts you wish to share with the community. Please refrain from posting free offerings outside of this weekly thread that is pinned to the top of the community. Any such posts made elsewhere will be removed.</p>

<p>8) <strong>Follow the <a href="https://www.redditinc.com/policies/content-policy">Rules of Reddit</a></strong><br>
They apply everywhere on Reddit. Also consider <a href="https://www.reddit.com/wiki/reddiquette/">Reddiquette</a> The latter isn't mandatory but might be used in considering the intent of your post.</p>

<p>9) <strong>Unsolicited Opinions and Duplicate Thoughts</strong><br>
Posts in Entrepreneur should be unique. If a duplicate post can be found on one or more similar subreddits or on a blog, then it's not a good fit for this subreddit.</p>

<p>Please refrain from using this community as your personal blog. There is no need to share opinions or use this space as a diary for daydreams, thoughts, or subjective reflections.</p>

<p>10) <strong>No Investment Solicitation</strong><br>
Posts about specific investments, such as cryptocurrency, are not permitted. Please refrain from soliciting, promoting, or offering any form of investment advice. There are other subreddits better suited for those discussions.</p>

<hr>

<table><thead>
<tr>
<th>Flair Filters</th>
<th><a href="https://www.reddit.com/message/compose?to=%2Fr%2FEntrepreneur">Want More?</a></th>
</tr>
</thead><tbody>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Case+Study%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Case Study</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27AMA%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">AMA</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Startup+Help%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Startup Help</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Tools%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Tools</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27How+Do+I%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">How Do I?</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Investor+Wanted%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Investor Wanted</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Operations%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Operations</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Other%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Other</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Internship+Offers%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Internship Offers</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Recommendations%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Recommendations</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Best+Practices%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Best Practices</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Young+Entrepreneur%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Young Entrepreneur</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27How+to+Grow%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">How to Grow</a></td>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Lessons+Learned%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Lessons Learned</a></td>
</tr>
<tr>
<td><a href="https://www.reddit.com/r/Entrepreneur/search?q=flair%3A%27Feedback+Please%27&amp;sort=new&amp;restrict_sr=on&amp;t=all&amp;feature=legacy_search">Feedback Please</a></td>
<td><a href="https://www.reddit.com/message/compose?to=%2Fr%2FEntrepreneur">Ask Mods for Other Flair.</a></td>
</tr>
</tbody></table>

<hr>

<h2>Our Weekly Sticky Posts</h2>

<p><strong>NooB Monday</strong> - For the most basic of questions </p>

<p><strong>Marketplace Tuesday</strong> - Post jobs or internships you're looking to fill or about services you can offer</p>

<p><strong>Wantrepreneur Wednesday</strong> - Looking to ask a question a bit beyond the super basic but don't need a whole thread? Ask here!</p>

<p><strong>Thank You Thursday</strong> - Thank the <a href="https://old.reddit.com/r/Entrepreneur">/r/Entrepreneur</a> community by offering free stuff, contests, discounts, electronic courses, ebooks and the best deals you know of. Consolidate such offers here!</p>

<p><strong>Feedback Friday</strong> - Get feedback on your website or portfolio. </p>

<p><strong>Accomplishments and Lessons Learned Saturday</strong> - Tell us what you have accomplished or alternatively what you will hope to never do again</p>

<hr>

<h2>How to schedule an AMA</h2>

<p>Contact the Mod group <a href="https://www.reddit.com/message/compose/?to=/r/Entrepreneur">via Modmail</a>.</p>

<p><strong>Subject:</strong> AMA Request (Your Name)</p>

<p><strong>Body:</strong> Date(M-F only), your background/experience, some form of validation (LinkedIn, Website, IG, etc). </p>

<p>Please note: In order for us to consider your ama, you must be a regular on the subreddit and the AMA must respect our rules, including rule 2. </p>

<hr>

<h2>Related Subreddits</h2>

<ul>
<li><a href="https://old.reddit.com/r/accounting">/r/accounting</a></li>
<li><a href="https://old.reddit.com/r/bookkeeping">/r/bookkeeping</a></li>
<li><a href="https://old.reddit.com/r/business">/r/business</a></li>
<li><a href="https://old.reddit.com/r/cofounder">/r/cofounder</a> </li>
<li><a href="https://old.reddit.com/r/finance">/r/finance</a></li>
<li><a href="https://old.reddit.com/r/inventions">/r/inventions</a></li>
<li><a href="https://old.reddit.com/r/marketing">/r/marketing</a></li>
<li><a href="https://old.reddit.com/r/motivation">/r/motivation</a></li>
<li><a href="https://old.reddit.com/r/negotiation">/r/negotiation</a> </li>
<li><a href="https://old.reddit.com/r/sales">/r/sales</a></li>
<li><a href="https://old.reddit.com/r/smallbusiness">/r/smallbusiness</a></li>
<li><a href="https://old.reddit.com/r/socialmedia">/r/socialmedia</a></li>
<li><a href="https://old.reddit.com/r/startups">/r/startups</a></li>
<li><a href="http://www.reddit.com/r/entrepreneur+accounting+business+finance+nventions+marketing+motivation+smallbusiness+socialmedia+socialmedia+startups">Multi-Reddit</a></li>
</ul>

<hr>

<p><strong>Featured Subreddits</strong></p>

<ul>
<li><a href="https://old.reddit.com/r/AppBusiness">/r/AppBusiness</a> </li>
<li><a href="https://old.reddit.com/r/ArtisanGifts">/r/ArtisanGifts</a></li>
<li><a href="https://old.reddit.com/r/Consulting">/r/Consulting</a></li>
<li><a href="https://old.reddit.com/r/BusinessHub">/r/BusinessHub</a></li>
<li><a href="https://old.reddit.com/r/contentstrategist">/r/contentstrategist</a></li>
<li><a href="https://old.reddit.com/r/cottage_industry">/r/cottage_industry</a> </li>
<li><a href="https://old.reddit.com/r/digitalnomad">/r/digitalnomad</a></li>
<li><a href="https://old.reddit.com/r/EntrepreneurRideAlong">/r/EntrepreneurRideAlong</a></li>
<li><a href="https://old.reddit.com/r/growmybusiness">/r/growmybusiness</a> </li>
<li><a href="https://old.reddit.com/r/hwstartups">/r/hwstartups</a></li>
<li><a href="https://old.reddit.com/r/LadyBusiness">/r/LadyBusiness</a></li>
<li><a href="https://old.reddit.com/r/mutualcollaboration">/r/mutualcollaboration</a></li>
<li><a href="https://old.reddit.com/r/socialpreneur">/r/socialpreneur</a></li>
<li><a href="https://old.reddit.com/r/SideProject">/r/SideProject</a></li>
<li><a href="https://old.reddit.com/r/StockNews">/r/StockNews</a></li>
<li><a href="https://old.reddit.com/r/Shutdown">/r/Shutdown</a></li>
<li><a href="https://old.reddit.com/r/tax">/r/tax</a></li>
<li><a href="https://old.reddit.com/r/venturecapital">/r/venturecapital</a></li>
</ul>

<hr>

<p><strong>Share!</strong></p>

<p>Become a community resource &amp; share your successes, failures and insights. Anything relevant to Entrepreneurship is welcome  </p>

<ul>
<li>Do you have specialist knowledge on patents? Share the knowledge </li>
<li>Tips on cash flow management? Let us know</li>
<li>Experience in raising funds? Help those who have none</li>
<li>know of an app that helps you create or stay productive? People may have never heard of it!</li>
</ul>

<p>Have a business? </p>

<ul>
<li>Describe your business. </li>
<li>how you launched </li>
<li>What problems have you had? </li>
<li>What has been successful? </li>
<li>What are some valuable lessons learned?</li>
</ul>

<hr>

<p><strong>Learn!</strong></p>

<p>Ask questions, share thoughts and gain insight from the community</p>

<ul>
<li><p>If you need help in validating a concept, Don't be afraid to share your idea,  there are some very knowledgeable people on this sub who can help. Remember: we'll need a bit more info then "I have a concept that will change my industry, I cant tell you anything about it but HELP!" to actually help</p></li>
<li><p>Provide as much information as you can. Everything depends on context. The more information you provide, the better the feedback you'll get and the more focused the conversation will be.</p></li>
<li><p>Try to answer simple questions yourself before starting a thread. Check the FAQ's,use Google search and the Reddit search bar</p></li>
</ul>

<p><strong>Community Overview</strong></p>

<ul>
<li><p>This community is for giving &amp; receiving advice on all aspects of Entrepreneurship. Help and encourage each other, it's hard enough out there!</p></li>
<li><p>Strive for a professional but relaxed atmosphere. Be polite and contribute to discussions in a constructive manner, everyone benefits from this.</p></li>
<li><p>Promotion of your business is not encouraged. Don't spam: it won't go down well and you will be banned. </p></li>
<li><p>Please don't post asking users if they would be interested in a guide to something - just post the guide! Post it in series (Part 1, Part 2, etc).</p></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[95% of Companies See 'Zero Return' on $30B Generative AI Spend (400 pts)]]></title>
            <link>https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/</link>
            <guid>44974104</guid>
            <pubDate>Thu, 21 Aug 2025 15:36:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/">https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=44974104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-%E2%80%98Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds.jpg" data-rel="penci-gallery-image-content">							                                <img width="1170" height="658" src="https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-%E2%80%98Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-1170x658.jpg" alt="" title="95% of Companies See ‘Zero Return’ on $30 Billion Generative AI Spend, MIT Report Finds" data-sizes="(max-width: 767px) 585px, 1170px" data-srcset="https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-‘Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-585x329.jpg 585w,https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-‘Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-1170x658.jpg" data-src="https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-‘Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-1170x658.jpg" srcset="https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-%E2%80%98Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-585x329.jpg 585w,https://thedailyadda.com/wp-content/uploads/2025/08/95-of-Companies-See-%E2%80%98Zero-Return-on-30-Billion-Generative-AI-Spend-MIT-Report-Finds-1170x658.jpg">
</a></p><p>The OpenAI logo appears on a smartphone in front of a computer screen showing ChatGPT output, March 21, 2023, in Boston. (AP Photo/Michael Dwyer, File)</p></div><div id="penci-post-entry-inner"><p data-start="516" data-end="726">Over the last three years, companies worldwide have invested between 30 and 40 billion dollars into generative artificial intelligence projects. Yet most of these efforts have brought no real business return.</p><p data-start="728" data-end="905">A new study from <a href="https://nanda.media.mit.edu/ai_report_2025.pdf" target="_blank" rel="noopener">MIT found</a> that 95 percent of enterprise organizations report zero measurable gains from the adoption of AI tools. Only a small group has seen strong benefits.</p><p data-start="907" data-end="1082">“Just five percent of integrated AI pilots are extracting millions in value,” the report said. In contrast, the vast majority showed no impact on revenue or earnings at all.</p><p data-start="1084" data-end="1280">Many companies rushed to test programs such as ChatGPT, Copilot, and other large language model platforms. Surveys show that over 80 percent of major firms have already explored or piloted them.</p><p data-start="1282" data-end="1499">Nearly 40 percent of companies reported deploying these systems at some level. But researchers found most use cases were limited to boosting individual productivity rather than improving a company’s overall profits.</p><p data-start="1501" data-end="1706">One major reason is that generative AI tools often fail to match real work processes. The report described “brittle workflows, lack of contextual learning, and poor alignment with day-to-day operations.”</p><p data-start="1708" data-end="1903">Unlike humans, most generative AI models cannot retain past feedback or build new reasoning ability over time. They also struggle to adapt to context or transfer lessons across different tasks.</p><p data-start="1905" data-end="2083">“Most GenAI systems do not retain feedback, adapt to context, or improve over time,” the study said. Without these traits, long-term integration remains costly and ineffective.</p><p data-start="2085" data-end="2281">The hype around generative AI led to high expectations in boardrooms. But the report suggests that many of the investments have not translated into better profits or meaningful cost savings yet.</p><p data-start="2283" data-end="2464">Some companies use AI for customer service, marketing, or writing assistance. While these tools can save time for workers, they rarely add direct earnings for the business itself.</p><p data-start="2466" data-end="2647">The report also downplayed fears that generative AI will cause sweeping job losses in the near term. Instead, its effect is more likely to be in reducing external costs for firms.</p><p data-start="2649" data-end="2849">“Until AI systems achieve contextual adaptation and autonomous operation, organizational impact will manifest through external cost optimization rather than internal restructuring,” the report said.</p><p data-start="2851" data-end="2992">This means businesses may cut expenses on outsourced tasks but are less likely to replace large groups of staff with machines anytime soon.</p><p data-start="2994" data-end="3174">That conclusion goes against common public belief that generative AI will replace millions of jobs quickly. Researchers argue the technology is far from reaching such capability.</p><p data-start="3176" data-end="3345">Experts say many failures come from misunderstanding what AI can and cannot do. A program may generate text or code quickly, but it cannot truly learn as humans learn.</p><p data-start="3347" data-end="3532">For instance, an employee can adjust based on new instructions, previous mistakes, and situational needs. A generative AI model cannot carry that memory across tasks unless retrained.</p><p data-start="3534" data-end="3719">Investors and executives still show strong interest in AI, hoping that ongoing advances will close these gaps. But the short-term outlook points to slower progress than many expected.</p><p data-start="3721" data-end="3899">The findings suggest that while the promise of AI is large, businesses should temper expectations. The technology is not yet ready to deliver across every industry or workflow.</p><p data-start="3901" data-end="4100">The report also highlights the need for smarter planning around adoption. Organizations may need to focus on narrow use cases where AI can bring immediate, measurable savings or productivity gains.</p><p data-start="4102" data-end="4300">This might include customer support scripts, coding aids, or document drafting, but not full company-wide transformation. Widespread integration is still considered premature and prone to failure.</p><p data-start="4302" data-end="4484">As one researcher noted, “AI is powerful at tasks, not strategy.” Companies that expect it to replace entire decision-making processes are setting themselves up for disappointment.</p><p data-start="4486" data-end="4663">For now, the business case for generative AI rests mainly on selective success stories. A handful of firms report large value, but most see only minor help with routine tasks.</p><p data-start="4665" data-end="4812">The lesson, according to the MIT study, is clear. Companies should see generative AI as a limited tool rather than a guaranteed engine of growth.</p><p data-start="4814" data-end="5008">While interest remains high, experts caution against chasing hype. Until the systems learn to adapt more like humans, profits from AI adoption are likely to remain out of reach for most firms.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Watch wearable foundation model (213 pts)]]></title>
            <link>https://arxiv.org/abs/2507.00191</link>
            <guid>44973375</guid>
            <pubDate>Thu, 21 Aug 2025 14:39:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2507.00191">https://arxiv.org/abs/2507.00191</a>, See on <a href="https://news.ycombinator.com/item?id=44973375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2507.00191">View PDF</a>
    <a href="https://arxiv.org/html/2507.00191v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Wearable devices record physiological and behavioral signals that can improve health predictions. While foundation models are increasingly used for such predictions, they have been primarily applied to low-level sensor data, despite behavioral data often being more informative due to their alignment with physiologically relevant timescales and quantities. We develop foundation models of such behavioral signals using over 2.5B hours of wearable data from 162K individuals, systematically optimizing architectures and tokenization strategies for this unique dataset. Evaluated on 57 health-related tasks, our model shows strong performance across diverse real-world applications including individual-level classification and time-varying health state prediction. The model excels in behavior-driven tasks like sleep prediction, and improves further when combined with representations of raw sensor data. These results underscore the importance of tailoring foundation model design to wearables and demonstrate the potential to enable new health applications.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Joseph Futoma [<a href="https://arxiv.org/show-email/a2bcb751/2507.00191" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 30 Jun 2025 19:01:00 UTC (2,155 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity reintroduces the Runtime Fee through its Industry license (213 pts)]]></title>
            <link>https://unity.com/products/unity-industry</link>
            <guid>44973269</guid>
            <pubDate>Thu, 21 Aug 2025 14:31:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unity.com/products/unity-industry">https://unity.com/products/unity-industry</a>, See on <a href="https://news.ycombinator.com/item?id=44973269">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!--$--><section data-sentry-component="Hero" data-sentry-source-file="Hero.tsx"><img alt="hero-background-image" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fa15cb90f2c064ea7395e825ddee370611b4d14e0-1920x1080.png&amp;w=3840&amp;q=100"><div data-sentry-component="HeroContent" data-sentry-source-file="HeroContent.tsx"><p>Transform your 3D data into interactive applications that deliver measurable results, enhance efficiency, and achieve your business goals.</p></div></section><div><ul><li data-sentry-component="DesktopLinkItem" data-sentry-source-file="AlternateNavigation.tsx"><a target="_self" data-sentry-element="NextLink" data-sentry-source-file="AlternateNavigation.tsx" href="https://unity.com/products/unity-industry#overview">Overview</a></li><li data-sentry-component="DesktopLinkItem" data-sentry-source-file="AlternateNavigation.tsx"><a target="_self" data-sentry-element="NextLink" data-sentry-source-file="AlternateNavigation.tsx" href="https://unity.com/products/unity-industry#key-benefits">Key benefits</a></li><li data-sentry-component="DesktopLinkItem" data-sentry-source-file="AlternateNavigation.tsx"><a target="_self" data-sentry-element="NextLink" data-sentry-source-file="AlternateNavigation.tsx" href="https://unity.com/products/unity-industry#what's-included">What's included</a></li><li data-sentry-component="DesktopLinkItem" data-sentry-source-file="AlternateNavigation.tsx"><a target="_self" data-sentry-element="NextLink" data-sentry-source-file="AlternateNavigation.tsx" href="https://unity.com/products/unity-industry#get-started">Get started</a></li><li data-sentry-component="DesktopLinkItem" data-sentry-source-file="AlternateNavigation.tsx"><a target="_self" data-sentry-element="NextLink" data-sentry-source-file="AlternateNavigation.tsx" href="https://unity.com/products/unity-industry#faq">FAQ</a></li></ul></div><div data-sentry-component="FullWidthBlock" data-sentry-source-file="FullWidthBlock.tsx"><h2 data-sentry-element="Tag" data-sentry-component="Title" data-sentry-source-file="Title.tsx">Discover Unity Industry</h2><p>Join top companies in manufacturing, automotive, retail, and more using real-time 3D technology to gain a competitive edge, speed time-to-market, and optimize resources. From product visualization to simulations, Unity Industry powers immersive experiences.</p></div><div data-sentry-component="TitleDescriptionBlock" data-sentry-source-file="TitleDescriptionBlock.tsx"><p><h2>Transform Your Business with 3D Solutions</h2></p><p>Harness Unity’s solutions to integrate data, build impactful applications, and deploy across platforms to drive business success.</p></div><div data-sentry-component="Cards" data-sentry-source-file="Cards.tsx"><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Empower Smarter Decisions with Real-Time 3D</h2><p>Transform 3D data into immersive applications with real-time insights. Enable decision-makers with spatial visualizations to improve collaboration and accelerate time-to-market through clear, actionable insights.</p></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Optimize Efficiency and Lower Costs</h2><p>Seamlessly integrate and manage 3D data to streamline workflows, reduce redundancies, and enhance collaboration. Improve design efficiency, optimize resource use, lower production costs, and boost overall operational effectiveness.</p></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Reach Your Audience Anywhere</h2><p>Develop once and deploy immersive applications across 20+ platforms, including mobile, desktop, AR, and VR. Ensure accessibility and engagement wherever stakeholders and customers interact with your solutions.</p></div></div><div data-sentry-component="FullWidthBlock" data-sentry-source-file="FullWidthBlock.tsx"><div data-sentry-component="renderMedia" data-sentry-source-file="FullWidthBlock.tsx"><p>Image courtesy of Norconsult</p></div><div><h2 data-sentry-element="Tag" data-sentry-component="Title" data-sentry-source-file="Title.tsx">Optimize Your 3D Data Workflows</h2><p>Integrate CAD, BIM, and 3D data into real-time 3D projects with ease. Centralize asset management to improve collaboration, optimize processes, and deliver immersive experiences efficiently.</p></div></div><div data-sentry-component="FullWidthBlock" data-sentry-source-file="FullWidthBlock.tsx"><h2 data-sentry-element="Tag" data-sentry-component="Title" data-sentry-source-file="Title.tsx">Create Immersive Real-Time 3D Experiences</h2><p>Create lifelike scenes, simulate complex processes, and prototype quickly. Leverage expert tools and a global community to accelerate development, delivering impactful real-time 3D applications that achieve business meaningful business outcomes.</p></div><div data-sentry-component="FullWidthBlock" data-sentry-source-file="FullWidthBlock.tsx"><h2 data-sentry-element="Tag" data-sentry-component="Title" data-sentry-source-file="Title.tsx">Deploy Real-Time 3D Anywhere</h2><p>Deliver immersive applications across AR, VR, web, mobile, and desktop platforms. Reach your audience with seamless multi-platform support, leverage cutting-edge devices, and create interactive experiences that engage customers and stakeholders wherever they are.</p></div><div data-sentry-component="TitleDescriptionBlock" data-sentry-source-file="TitleDescriptionBlock.tsx"><p><h2>What's included</h2></p><p>Unity Industry includes the products and support needed to create, manage, and scale multiplatform applications. Designed for industry creators, it covers every stage from data integration to deployment.</p></div><div data-sentry-component="Cards" data-sentry-source-file="Cards.tsx"><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><p><img alt="Unity logo" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F19d1656d78819d6634202b73c53c29a6e2933353-811x455.png&amp;w=3840&amp;q=100"></p><div><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Unity 6</h2><p>Unity 6 empowers industry creators with faster rendering, enhanced visuals, runtime AI integration, and seamless multiplatform reach. Build collaborative, scalable applications with advanced workflows and improved productivity tools, all optimized for real-time 3D development.</p></div></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><p><img alt="pixyz logo icon" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F43777b49bfbfa72c599b12dad6adeb1da6b43315-810x455.png&amp;w=3840&amp;q=100"></p><div><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Unity Asset Transformer Toolkit</h2><div><p>Unity Asset Transformer Toolkit offers the easiest way to bring CAD and 3D models into Unity. Prepare data from 70+ file formats, customize levels of detail for target devices, and streamline workflows with automated import tools—designed for seamless use by Unity developers.</p><p><em>(Formerly known as Pixyz Plugin.)</em></p></div></div></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><p><img alt="Unity Asset Manager logo" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F0f8330e77ba19a0842e581a1a5c2d3d6d026355f-810x456.png&amp;w=3840&amp;q=100"></p><div><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Unity Asset Manager</h2><p>Unity Asset Manager is a cloud-based digital asset management (DAM) solution that streamlines how teams can upload, manage, transform, share, and access their complex 3D assets. It converts your industrial 3D data to real-time 3D assets, making it available to anyone, on any device.</p></div></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><p><img alt="Build automation icon" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2F1c79700de38f6d7bdb9a0f82334cc6e9744cd06d-809x455.png&amp;w=3840&amp;q=100"></p><div><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Build Server</h2><p>Unity Build Server streamlines development by offloading project builds to OnPrem hardware, keeping developers focused and accelerating iterations. It includes built-in Quality Assurance infrastructure to support teams as they scale and deliver high-quality applications.</p></div></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><p><img alt="Unity editor logo" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=640&amp;q=100 640w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=750&amp;q=100 750w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=828&amp;q=100 828w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=1080&amp;q=100 1080w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=1200&amp;q=100 1200w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=1920&amp;q=100 1920w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=2048&amp;q=100 2048w, https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=3840&amp;q=100 3840w" src="https://unity.com/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Ffuvbjjlp%2Fproduction%2Fb8bf43e38692760a76784ee6304b56df7ea2c048-810x455.png&amp;w=3840&amp;q=100"></p><div><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Industry Success</h2><p>Industry Success helps overcome onboarding challenges and speed time to market with Success Management, On-Demand Training, and premium technical support to upskill your team and deliver results efficiently.</p></div></div></div><div data-sentry-component="TitleDescriptionBlock" data-sentry-source-file="TitleDescriptionBlock.tsx"><p><h2>Unity Industry Partner Programs</h2></p><p>Whether you are delivering creative consulting services or building Unity-powered software solutions, we have a partner program tailored to your needs.</p></div><div data-sentry-component="Cards" data-sentry-source-file="Cards.tsx"><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Service Partner Program</h2><p>The Unity Service Partner Program empowers partners with tools, training, and incentives to grow their business with Unity.</p></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Independent Software Vendor Program</h2><p>The Unity ISV Program empowers software companies building commercial Unity-based solutions with resources, validation, and collaboration opportunities to scale and succeed.</p></div><div data-sentry-component="Card" data-sentry-source-file="Card.tsx"><h2 data-sentry-element="CardTitleTag" data-sentry-source-file="Cards.tsx">Reseller Partner Program</h2><p>The Unity Reseller Partner Program empowers value-added resellers with tools, expertise, and support to drive real-time 3D adoption across industries.</p></div></div><section data-sentry-component="Faq" data-sentry-source-file="Faq.tsx"><p data-sentry-component="FaqTitle" data-sentry-source-file="FaqTitle.tsx"><h2>Frequently asked questions</h2></p><div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>How can I start using Unity Industry?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>Is there a free trial of Unity Industry?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>How do I know Unity Industry is the right solution for me?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>How much does Unity Industry cost?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>Is there a support plan?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>What learning resources exist?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>Can industry customers still buy Unity Pro or Unity Enterprise?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>How do I upgrade from Unity Pro to Unity Industry?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>What is Unity Cloud?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>How do I get started with Unity Cloud?</h3><p><span>+</span></p></div><div role="button" tabindex="0" data-link-location="undefined-video-play" data-link-type="CTA" data-sentry-component="Accordion" data-sentry-source-file="Accordion.tsx"><h3>What happens if I am distributing the runtime for commercial purposes?</h3><p><span>+</span></p></div></div></section><!--$--><!--/$--><!--$--><!--/$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[7 years later, Valve's Proton has been a game-changer for Linux (104 pts)]]></title>
            <link>https://www.gamingonlinux.com/2025/08/7-years-later-valves-proton-has-been-an-incredible-game-changer-for-linux/</link>
            <guid>44973227</guid>
            <pubDate>Thu, 21 Aug 2025 14:28:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamingonlinux.com/2025/08/7-years-later-valves-proton-has-been-an-incredible-game-changer-for-linux/">https://www.gamingonlinux.com/2025/08/7-years-later-valves-proton-has-been-an-incredible-game-changer-for-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=44973227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>It has been 7 years since Valve revealed Proton, their compatibility layer to run Windows games on Linux systems. <em>What an incredible time it has been.</em></p>

<p>Hard to imagine Linux gaming without Proton now isn't it? Unless games use some funky video codecs or <a href="https://www.gamingonlinux.com/anticheat/" rel="noopener" target="_blank">kernel-level anti-cheat</a>, a lot of the time they do just work with the click of a button. We went from getting a handful of indie games and some AAA scraps from porting companies, to multiple tens of thousands of games — we're spoilt for choice on what to actually play. Going by ProtonDB there's at least 15,855 games rated playable by at least two reports, and Valve's Deck Verified system shows 21,694 games rated at least playable for SteamOS / Steam Deck. Incredible numbers and that's only what have actually seen some checks.</p>

<p>I still <em>vividly</em> remember the <a href="https://www.gamingonlinux.com/2018/08/valve-officially-confirm-a-new-version-of-steam-play-which-includes-a-modified-version-of-wine/" rel="noopener" target="_blank">original announcement</a>, shaking with excitement on what it would mean for the future. It has opened up a world of possibilities, where you (for the most part) don't need to be attached to Windows to play some of the best games around.</p>

<p>Valve's commitment to Linux and open source is very much self-serving of course, they are a company with goals. All of this was progress towards creating their own ecosystem with the likes of the Steam Deck / SteamOS and other potential hardware to come. Linux desktop users are mostly just along for the ride and reap all the benefits.</p>

<p>Thanks to all the work with Proton and the Steam Deck, the Linux user share on Steam is also <a href="https://www.gamingonlinux.com/steam-tracker/" rel="noopener" target="_blank">on the cusp of breaking through 3%</a> which is impressive considering the massive mountain that is Windows. You don't beat or even get remotely close to such an entrenched system overnight, movement like this just takes time. Slow and steady wins the race right? Valve continue playing the long game here.</p>

<p>What's truly incredible about Proton though is how it gives Valve a set platform to continue building on. So when we hopefully see a Steam Deck 2, or a Steam Machine TV console, it will be plug and play with all the existing games. Linking into why I love the whole idea of Valve using Linux with Proton, SteamOS / Steam Deck over traditional consoles is just having access to all the same games - no need to buy them again.</p>

<p>And with more publishers like Microsoft and Sony pushing their games on Steam too, we all win.</p>

<p>Here's to Valve and Proton, 7 years on. Cheers!</p>
						<p><span>Article taken from <a href="https://www.gamingonlinux.com/">GamingOnLinux.com.</a></span>
						
					</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In a first, Google has released data on how much energy an AI prompt uses (106 pts)]]></title>
            <link>https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/</link>
            <guid>44972808</guid>
            <pubDate>Thu, 21 Aug 2025 13:52:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/">https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/</a>, See on <a href="https://news.ycombinator.com/item?id=44972808">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Google has just released a technical report detailing how much energy its Gemini apps use for each query. In total, the median prompt—one that falls in the middle of the range of energy demand—consumes 0.24 watt-hours of electricity, the equivalent of running a standard microwave for about one second. The company also provided average estimates for the water consumption and carbon emissions associated with a text prompt to Gemini.</p>  <p>It’s the most transparent estimate yet from a Big Tech company with a popular AI product, and the report includes detailed information about how the company calculated its final estimate. As AI has become more widely adopted, there’s been <a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/">a growing effort to understand its energy use</a>. But public efforts attempting to directly measure the energy used by AI have been hampered by a lack of full access to the operations of a major tech company.&nbsp;</p> </div><div> <p>Earlier this year, <em>MIT Technology Review</em> <a href="https://www.technologyreview.com/energy-ai">published a comprehensive series on AI and energy</a>, at which time none of the major AI companies would reveal their per-prompt energy usage. Google’s new publication, at last, allows for a peek behind the curtain that researchers and analysts have long hoped for.</p>  <p>The study focuses on a broad look at energy demand, including not only the power used by the AI chips that run models but also by all the other infrastructure needed to support that hardware.&nbsp;</p> 
 <p>“We wanted to be quite comprehensive in all the things we included,” said Jeff Dean, Google’s chief scientist, in an exclusive interview with <em>MIT Technology Review</em> about the new report.</p>  <p>That’s significant, because in this measurement, the AI chips—in this case, Google’s custom TPUs, the company’s proprietary equivalent of GPUs—account for just 58% of the total electricity demand of 0.24 watt-hours.&nbsp;</p> 
 <p>Another large portion of the energy is used by equipment needed to support AI-specific hardware: The host machine’s CPU and memory account for another 25% of the total energy used. There’s also backup equipment needed in case something fails—these idle machines account for 10% of the total. The final 8% is from overhead associated with running a data center, including cooling and power conversion.&nbsp;</p>  <p>This sort of report shows the value of industry input to energy and AI research, says Mosharaf Chowdhury, a professor at the University of Michigan and one of the heads of the <a href="https://ml.energy/leaderboard/?__theme=light">ML.Energy leaderboard</a>, which tracks energy consumption of AI models.&nbsp;</p>  <p>Estimates like Google’s are generally something that only companies can produce, because they run at a larger scale than researchers are able to and have access to behind-the-scenes information. “I think this will be a keystone piece in the AI energy field,” says Jae-Won Chung, a PhD candidate at the University of Michigan and another leader of the ML.Energy effort. “It’s the most comprehensive analysis so far.”</p>  <p>Google’s figure, however, is not representative of all queries submitted to Gemini: The company handles a huge variety of requests, and this estimate is calculated from a median energy demand, one that falls in the middle of the range of possible queries.</p> </div><div> <p>So some Gemini prompts use much more energy than this: Dean gives the example of feeding dozens of books into Gemini and asking it to produce a detailed synopsis of their content. “That’s the kind of thing that will probably take more energy than the median prompt,” Dean says. Using a reasoning model could also have a higher associated energy demand because these models take more steps before producing an answer.</p>  <p>This report was also strictly limited to text prompts, so it doesn’t represent what’s needed to generate an image or a video. (Other analyses, including <a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/">one in <em>MIT Technology Review</em>’s Power Hungry series</a> earlier this year, show that these tasks can require much more energy.)</p>  <p>The report also finds that the total energy used to field a Gemini query has fallen dramatically over time. The median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025, according to Google. The company points to advancements in its models and other software optimizations for the <a href="https://www.technologyreview.com/2025/05/20/1116337/ai-energy-use-optimism/">improvements</a>.&nbsp;&nbsp;</p>  <p>Google also estimates the greenhouse gas emissions associated with the median prompt, which they put at 0.03 grams of carbon dioxide. To get to this number, the company multiplied the total energy used to respond to a prompt by the average emissions per unit of electricity.</p> 

 <p>Rather than using an emissions estimate based on the US grid average, or the average of the grids where Google operates, the company instead uses a market-based estimate, which takes into account electricity purchases that the company makes from clean energy projects. The company has signed agreements to buy over <a href="https://sustainability.google/operations/">22 gigawatts</a> of power from sources including solar, wind, geothermal, and advanced nuclear projects since 2010. Because of those purchases, Google’s emissions per unit of electricity on paper are roughly one-third of those on the average grid where it operates.</p>  <p>AI data centers also consume water for cooling, and Google estimates that each prompt consumes 0.26 milliliters of water, or about five drops.&nbsp;</p>  <p>The goal of this work was to provide users a window into the energy use of their interactions with AI, Dean says.&nbsp;</p>  <p>“People are using [AI tools] for all kinds of things, and they shouldn’t have major concerns about the energy usage or the water usage of Gemini models, because in our actual measurements, what we were able to show was that it’s actually equivalent to things you do without even thinking about it on a daily basis,” he says, “like watching a few seconds of TV or consuming five drops of water.”</p> </div><div><p>The publication greatly expands what’s known about AI’s resource usage. It follows recent increasing pressure on companies to release more information about the energy toll of the technology. “I’m really happy that they put this out,” says Sasha Luccioni, an AI and climate researcher at Hugging Face. “People want to know what the cost is.”</p>  <p>This estimate and the supporting report contain more public information than has been available before, and it’s helpful to get more information about AI use in real life, at scale, by a major company, Luccioni adds. However, there are still details that the company isn’t sharing in this report. One major question mark is the total number of queries that Gemini gets each day, which would allow estimates of the AI tool’s total energy demand.&nbsp;</p>  <p>And ultimately, it’s still the company deciding what details to share, and when and how. “We’ve been trying to push for a standardized AI energy score,” Luccioni says, a standard for AI similar to the Energy Star rating for appliances. “This is not a replacement or proxy for standardized comparisons.”</p>  </div></div>]]></description>
        </item>
    </channel>
</rss>