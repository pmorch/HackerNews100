<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 15 Mar 2024 19:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Ollama now supports AMD graphics cards (123 pts)]]></title>
            <link>https://ollama.com/blog/amd-preview</link>
            <guid>39718558</guid>
            <pubDate>Fri, 15 Mar 2024 17:47:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/amd-preview">https://ollama.com/blog/amd-preview</a>, See on <a href="https://news.ycombinator.com/item?id=39718558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <h2>March 14, 2024</h2>
      <section>
        <p><a href="https://ollama.com/download"><img src="https://ollama.com/public/blog/amd-preview.png" alt="Ollama AMD"></a></p>

<p>Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for <a href="https://ollama.com/download/linux">Linux</a> and <a href="https://ollama.com/download/windows">Windows</a>.</p>

<video autoplay="" controls="">
  <source src="https://github.com/ollama/ollama/assets/3325447/671a8031-1915-448e-b033-16b367b359d9" type="video/mp4">
</video>

<h2>Supported graphics cards</h2>

<table>
<thead>
<tr>
<th>Family</th>
<th>Supported cards and accelerators</th>
</tr>
</thead>

<tbody>
<tr>
<td>AMD Radeon RX</td>
<td><code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <br><code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code><br><code>Vega 64</code> <code>Vega 56</code></td>
</tr>

<tr>
<td>AMD Radeon PRO</td>
<td><code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <br><code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code><br><code>V620</code> <code>V420</code> <code>V340</code> <code>V320</code><br><code>Vega II Duo</code> <code>Vega II</code> <code>VII</code> <code>SSG</code></td>
</tr>

<tr>
<td>AMD Instinct</td>
<td><code>MI300X</code> <code>MI300A</code> <code>MI300</code><br><code>MI250X</code> <code>MI250</code> <code>MI210</code> <code>MI200</code><br><code>MI100</code> <code>MI60</code> <code>MI50</code></td>
</tr>
</tbody>
</table>
<p>Support for more AMD graphics cards is coming soon.</p>

<h2>Get started</h2>

<p>To get started with Ollama with support for AMD graphics cards, download Ollama for <a href="https://ollama.com/download/linux">Linux</a> or <a href="https://ollama.com/download/windows">Windows</a>.</p>

      </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC and DOJ want to free McDonald's ice cream machines from DMCA repair rules (129 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</link>
            <guid>39717558</guid>
            <pubDate>Fri, 15 Mar 2024 16:30:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/">https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=39717558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      I scream, you scream, we all scream for 1201(c)3 exemptions    â€”
</h4>
            
            <h2 itemprop="description">McFlurries are a notable part of petition for commercial and industrial repairs.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-800x536.jpg" alt="Taylor ice cream machine, with churning spindle removed by hand.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/03/ice_cream_fix-scaled.jpg" data-height="1714" data-width="2560">Enlarge</a> <span>/</span> Taylor's C709 Soft Serve Freezer isn't so much mechanically complicated as it is a software and diagnostic trap for anyone without authorized access.</p></figcaption>  </figure>

  




<!-- cache hit 149:single/related:70bb8fbbd06d2f1057c24bd5f3501ac0 --><!-- empty -->
<p>Many devices have been made difficult or financially nonviable to repair, whether by design or because of a lack of parts, manuals, or specialty tools. Machines that make ice cream, however, seem to have a special place in the hearts of lawmakers. Those machines are often broken and locked down for only the most profitable repairs.</p>
<p>The Federal Trade Commission and the antitrust division of the Department of Justice have <a href="https://www.ftc.gov/system/files/ftc_gov/pdf/ATR-FTC-JointComment.pdf">asked the US Copyright Office</a> (PDF) to exempt "commercial soft serve machines" from the anti-circumvention rules of <a href="https://www.copyright.gov/1201/2018/">Section 1201</a> of the Digital Millennium Copyright Act (DMCA). The governing bodies also submitted proprietary diagnostic kits, programmable logic controllers, and enterprise IT devices for DMCA exemptions.</p>
<p>"In each case, an exemption would give users more choices for third-party and self-repair and would likely lead to cost savings and a better return on investment in commercial and industrial equipment," the joint comment states. Those markets would also see greater competition in the repair market, and companies would be prevented from using DMCA laws to enforce monopolies on repair, according to the comment.</p>
<p>The joint comment builds upon a petition filed by repair vendor and advocate iFixit and interest group Public Knowledge, which advocated for broad reforms while keeping a relatable, ingestible example at its center. McDonald's soft serve ice cream machines, which are <a href="https://mcbroken.com/">famously frequently broken</a>, are supplied by industrial vendor Taylor. <a href="https://publicknowledge.org/public-knowledge-petitions-copyright-office-for-dmca-exemption-for-ice-cream-machines/">Taylor's C709 Soft Serve Freezer</a> requires lengthy, finicky warm-up and cleaning cycles, produces obtuse error codes, and, perhaps not coincidentally, costs $350 per 15 minutes of service for a Taylor technician to fix. iFixit <a href="https://www.ifixit.com/News/80215/whats-inside-that-mcdonalds-ice-cream-machine-broken-copyright-law">tore down such a machine</a>, confirming the lengthy process between plugging in and soft serving.
</p><p>After one company built a Raspberry Pi-powered device, the <a href="https://www.kytch.com/landing">Kytch</a>, that could provide better diagnostics and insights, Taylor moved to ban franchisees from installing the device, then offered up its own competing product. Kytch has <a href="https://www.wired.com/story/kytch-ice-cream-machine-hackers-sue-mcdonalds-900-million/">sued Taylor for $900 million</a>&nbsp;in a case that is still pending.</p>                                            
                                                        
<p>Beyond ice cream, the petitions to the Copyright Office would provide more broad exemptions for industrial and commercial repairs that require some kind of workaround, decryption, or other software tinkering. Going past technological protection measures (TPMs) was made illegal by the 1998 DMCA, which was put in place largely because of the concerns of media firms facing what they considered rampant piracy.</p>
<p>Every three years, the Copyright Office allows for petitions to exempt certain exceptions to DMCA violations (and renew prior exemptions). Repair advocates have won exemptions for farm equipment repair, <a href="https://arstechnica.com/tech-policy/2021/10/us-copyright-office-oks-right-to-repair-for-video-game-console-optical-drives/">video game consoles</a>, cars, and certain medical gear. The exemption is often granted for device fixing if a repair person can work past its locks, but not for the distribution of tools that would make such a repair far easier. The esoteric nature of such "release valve" offerings has led groups like the EFF to <a href="https://arstechnica.com/tech-policy/2016/07/eff-sues-us-government-saying-copyright-rules-on-drm-are-unconstitutional/">push for the DMCA's abolishment</a>.</p>
<p>DMCA exemptions occur on a parallel track to <a href="https://arstechnica.com/tech-policy/2024/03/oregon-oks-right-to-repair-bill-that-bans-the-blocking-of-aftermarket-parts/">state right-to-repair bills</a> and broader federal action. President Biden issued <a href="https://arstechnica.com/tech-policy/2021/07/bidens-right-to-repair-order-could-stop-companies-from-blocking-diy-fixes/">an executive order</a> that included a push for repair reforms. The FTC has issued studies that call out <a href="https://www.ftc.gov/system/files/documents/reports/nixing-fix-ftc-report-congress-repair-restrictions/nixing_the_fix_report_final_5521_630pm-508_002.pdf">unnecessary repair restrictions</a> and has taken <a href="https://www.ftc.gov/news-events/news/press-releases/2022/10/ftc-approves-final-orders-right-repair-cases-against-harley-davidson-mwe-investments-weber">action</a> against firms like Harley-Davidson, Westinghouse, and grill maker Weber for tying warranties to an authorized repair service.</p>
<p><i>Disclosure: Kevin Purdy previously worked for iFixit. He has no financial ties to the company.</i></p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reversing for dummies â€“ x86 assembly and C code (Beginner/ADHD friendly) (103 pts)]]></title>
            <link>https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</link>
            <guid>39716494</guid>
            <pubDate>Fri, 15 Mar 2024 14:58:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html">https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html</a>, See on <a href="https://news.ycombinator.com/item?id=39716494">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3 id="context">Context</h3>

          <p>
            Before I got into reverse engineering, executables always seemed
            like black magic to me. I always wondered how stuff worked under the
            hood, and how binary code is represented inside .exe files, and how
            hard it is to modify this â€˜compiled codeâ€™ without access to the
            original source code.
          </p>

          <p>
            But one of the main intimidating hurdles always seemed to be the
            assembly language, itâ€™s the thing that scares most people away from
            trying to learn about this field.
          </p>

          <p>
            Thatâ€™s the main reason why I thought of writing this
            straight-to-the-point article that only contains the essential stuff
            that you encounter the most when reversing, albeit missing crucial
            details for the sake of brevity, and assumes the reader has a reflex
            of finding answers online, looking up definitions, and more
            importantly, coming up with examples/ideas/projects to practice on.
          </p>

          <p>
            The goal is to hopefully guide an aspiring reverse engineer and
            arouse motivation towards learning more about this seemingly elusive
            passion.
          </p>

          <p>
            <strong><em>Note</em></strong>: This article assumes the reader has elementary knowledge
            regarding the
            <a href="https://en.wikipedia.org/wiki/Hexadecimal" rel="noopener noreferrer" target="_blank">hexadecimal numeral system</a>, as well as the
            <a href="https://en.wikipedia.org/wiki/C_(programming_language)" rel="noopener noreferrer" target="_blank">C programming language</a>, and is based on a 32-bit Windows executable case study - results
            might differ across different OSes/architectures.
          </p>

          <h3 id="introduction">Introduction</h3>

          <h4 id="compilation">Compilation</h4>

          <p>
            After writing code using a
            <a href="https://en.wikipedia.org/wiki/Compiled_language" rel="noopener noreferrer" target="_blank">compiled language</a>, a compilation takes place <del>(duh)</del>, in order to generate
            the output binary file (an example of such is an .exe file).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/compilation-c-to-exe-file.png">
          </p>

          <p>
            Compilers are sophisticated programs which do this task. They make
            sure the syntax of your <del>ugly</del> code is correct, before
            compiling and optimizing the resulting machine code by minimizing
            its size and improving its performance, whenever applicable.
          </p>

          <h4 id="binary-code">Binary code</h4>

          <p>
            As we were saying, the resulting output file contains binary code,
            which can only be â€˜understoodâ€™ by a CPU, itâ€™s essentially a
            succession of varying-length instructions to be executed in order -
            hereâ€™s what some of them look like:
          </p>

          <table>
            <thead>
              <tr>
                <th>CPU-readable instruction data (in hex)</th>
                <th>Human-readable interpretation</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>55</td>
                <td>push ebp</td>
              </tr>
              <tr>
                <td>8B EC</td>
                <td>mov ebp, esp</td>
              </tr>
              <tr>
                <td>83 EC 08</td>
                <td>sub esp, 8</td>
              </tr>
              <tr>
                <td>33 C5</td>
                <td>xor eax, ebp</td>
              </tr>
              <tr>
                <td>83 7D 0C 01</td>
                <td>cmp dword ptr [ebp+0Ch], 1</td>
              </tr>
            </tbody>
          </table>

          <p>
            These instructions are predominantly arithmetical, and they
            manipulate CPU registers/flags as well as volatile memory, as
            theyâ€™re executed.
          </p>

          <h4 id="cpu-registers">CPU registers</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Processor_register" rel="noopener noreferrer" target="_blank">A CPU register</a>
            is almost like a temporary integer variable - thereâ€™s a small fixed
            number of them, and they exist because theyâ€™re quick to access,
            unlike memory-based variables, and they help the CPU keep track of
            its data (results, operands, counts, etc.) during execution.
          </p>

          <p>
            Itâ€™s important to note the presence of a special register called the
            <a href="https://en.wikipedia.org/wiki/FLAGS_register" rel="noopener noreferrer" target="_blank"><code>FLAGS</code>
              register</a>
            (<code>EFLAGS</code> on
            32-bit), which houses a bunch of flags (boolean indicators), which
            hold information about the state of the CPU, which include details
            about the last arithmetic operation (zero:
            <code>ZF</code>,
            overflow:
            <code>OF</code>,
            parity:
            <code>PF</code>, sign:
            <code>SF</code>, etc.).
          </p>

          <p>
            <img src="https://i.0x44.cc/b/x32dbg-cpu-registers.png">
            <small>CPU registers visualized while debugging a 32-bit process on
              x64dbg, a debugging tool.</small>
          </p>

          <p>
            Some of these registers can also be spotted on the assembly excerpt
            mentioned <a href="#binary-code">previously</a>, namely:
            <code>EAX</code>,
            <code>ESP</code> (stack
            pointer) and
            <code>EBP</code> (base
            pointer).
          </p>

          <h4 id="memory-access">Memory access</h4>

          <p>
            As the CPU executes stuff, it needs to access and interact with
            memory, thatâ€™s when the role of the <em>stack</em> and the
            <em>heap</em> comes.
          </p>

          <p>
            These are (without getting into too much detail) the 2 main ways of
            â€˜keeping track of variable dataâ€™ during the execution of a program:
          </p>

          <h5 id="-stack">ðŸ¥ž <em>Stack</em></h5>
          <p>
            The simpler and faster of the two - itâ€™s a linear contiguous LIFO
            (last in = first out) data structure with a push/pop mechanism, it
            serves to remember function-scoped variables, arguments, and keeps
            track of calls (ever heard of a
            <a href="https://en.wikipedia.org/wiki/Stack_trace" rel="noopener noreferrer" target="_blank">stack trace</a>?)
          </p>

          <h5 id="-heap">â›° <em>Heap</em></h5>
          <p>
            The heap, however, is pretty unordered, and is for more complicated
            data structures, itâ€™s typically used for dynamic allocations, where
            the size of the buffer isnâ€™t initially known, and/or if itâ€™s too
            big, and/or needs to be modified later.
          </p>

          <h3 id="assembly-instructions">Assembly instructions</h3>

          <p>
            As Iâ€™ve mentioned earlier, assembly instructions have a varying
            â€˜byte-sizeâ€™, and a varying number of arguments.
          </p>

          <p>
            Arguments can also be either immediate (â€˜hardcodedâ€™), or they can be
            registers, depending on the instruction:
          </p>

          <div>
              <pre><code>55         push    ebp     ; size: 1 byte,  argument: register
6A 01      push    1       ; size: 2 bytes, argument: immediate
</code></pre>
            </div>

          <p>
            Letâ€™s quickly run through a very small set of some of the common
            ones weâ€™ll get to see - feel free to do your own research for more
            detail:
          </p>

          <h4 id="stack-operations">Stack operations</h4>
          <ul>
            <li>
              <strong>push
                <code>value</code></strong>
              <em>; pushes a value into the stack (decrements
                <code>ESP</code> by
                4, the size of one stack â€˜unitâ€™).</em>
            </li>
            <li>
              <strong>pop
                <code>register</code></strong>
              <em>; pops a value to a register (increments
                <code>ESP</code> by
                4).</em>
            </li>
          </ul>

          <h4 id="data-transfer">Data transfer</h4>
          <ul>
            <li>
              <strong>mov
                <code>destination</code>,
                <code>source</code></strong>
              ; <em><del>moves</del> copies a value from/to a register.</em>
            </li>
            <li>
              <strong>mov
                <code>destination</code>, [<code>expression</code>]</strong>
              ;
              <em>copies a value from a memory address resolved from a â€˜register
                expressionâ€™ (single register or arithmetic expression involving
                one or more registers) into a register.</em>
            </li>
          </ul>

          <h4 id="flow-control">Flow control</h4>
          <ul>
            <li>
              <strong>jmp
                <code>destination</code></strong>
              ;
              <em>jumps into a code location (sets
                <code>EIP</code>
                (instruction pointer)).</em>
            </li>
            <li>
              <strong>jz/je
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code>
                (the zero flag) is set.</em>
            </li>
            <li>
              <strong>jnz/jne
                <code>destination</code></strong>
              ;
              <em>jumps into a code location if
                <code>ZF</code> is
                not set.</em>
            </li>
          </ul>

          <h4 id="operations">Operations</h4>
          <ul>
            <li>
              <strong>cmp
                <code>operand1</code>,
                <code>operand2</code></strong>
              ;
              <em>compares the 2 operands and sets
                <code>ZF</code> if
                theyâ€™re equal.</em>
            </li>
            <li>
              <strong>add
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 += operand2;</em>
            </li>
            <li>
              <strong>sub
                <code>operand1</code>,
                <code>operand2</code></strong>
              ; <em>operand1 -= operand2;</em>
            </li>
          </ul>

          <h4 id="function-transitions">Function transitions</h4>
          <ul>
            <li>
              <strong>call
                <code>function</code></strong>
              ;
              <em>calls a function (pushes current
                <code>EIP</code>,
                then jumps to the function).</em>
            </li>
            <li>
              <strong>retn</strong> ;
              <em>returns to caller function (pops back the previous
                <code>EIP</code>).</em>
            </li>
          </ul>

          <p>
            <strong><em>Note</em></strong>: You might notice the words â€˜equalâ€™ and â€˜zeroâ€™ being used
            interchangeably in x86 terminology - thatâ€™s because comparison
            instructions internally perform a subtraction, which means if the 2
            operands are equal,
            <code>ZF</code> is set.
          </p>

          <h3 id="assembly-patterns">Assembly patterns</h3>

          <p>
            Now that we have a rough idea of the main elements used during the
            execution of a program, letâ€™s get familiarized with the patterns of
            instructions that you can encounter reverse engineering your average
            everyday 32-bit
            <a href="https://en.wikipedia.org/wiki/Portable_Executable" rel="noopener noreferrer" target="_blank">PE</a>
            binary.
          </p>

          <h4 id="function-prologue">Function prologue</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Function_prologue" rel="noopener noreferrer" target="_blank">function prologue</a>
            is some initial code embedded in the beginning of most functions, it
            serves to set up a new stack frame for said function.
          </p>

          <p>It typically looks like this (X being a number):</p>

          <div>
              <pre><code>55          push    ebp        ; preserve caller function's base pointer in stack
8B EC       mov     ebp, esp   ; caller function's stack pointer becomes base pointer (new stack frame)
83 EC XX    sub     esp, X     ; adjust the stack pointer by X bytes to reserve space for local variables
</code></pre>
            </div>

          <h4 id="function-epilogue">Function epilogue</h4>

          <p>
            The
            <a href="https://en.wikipedia.org/wiki/Function_epilogue" rel="noopener noreferrer" target="_blank">epilogue</a>
            is simply the opposite of the prologue - it undoes its steps to
            restore the stack frame of the caller function, before it returns to
            it:
          </p>

          <div>
              <pre><code>8B E5    mov    esp, ebp    ; restore caller function's stack pointer (current base pointer) 
5D       pop    ebp         ; restore base pointer from the stack
C3       retn               ; return to caller function
</code></pre>
            </div>

          <p>
            Now at this point, you might be wondering - how do functions talk to
            each other? How exactly do you send/access arguments when calling a
            function, and how do you receive the return value? Thatâ€™s precisely
            why we have calling conventions.
          </p>

          <h4 id="calling-conventions-__cdecl">Calling conventions: __cdecl</h4>

          <p>
            A
            <a href="https://en.wikipedia.org/wiki/Calling_convention" rel="noopener noreferrer" target="_blank">calling convention</a>
            is basically a protocol used to communicate with functions, thereâ€™s
            a few variations of them, but they share the same principle.
          </p>

          <p>
            We will be looking at the
            <a href="https://en.wikipedia.org/wiki/X86_calling_conventions#cdecl" rel="noopener noreferrer" target="_blank">__cdecl (C declaration) convention</a>, which is the standard one when compiling C code.
          </p>

          <p>
            In __cdecl (32-bit), function arguments are passed on the stack
            (pushed in reverse order), while the return value is returned in the
            <code>EAX</code>
            register (assuming itâ€™s not a float).
          </p>

          <p>
            This means that a
            <code>func(1, 2, 3);</code>
            call will generate the following:
          </p>

          <div>
              <pre><code>6A 03             push    3
6A 02             push    2
6A 01             push    1
E8 XX XX XX XX    call    func
</code></pre>
            </div>

          <h4 id="putting-everything-together">Putting everything together</h4>

          <p>
            Assuming
            <code>func()</code>
            simply does an addition on the arguments and returns the result, it
            would probably look like this:
          </p>

          <div>
              <pre><code>int __cdecl func(int, int, int):

           prologue:
55           push    ebp               ; save base pointer
8B EC        mov     ebp, esp          ; new stack frame

           body:
8B 45 08     mov     eax, [ebp+8]      ; load first argument to EAX (return value)
03 45 0C     add     eax, [ebp+0Ch]    ; add 2nd argument
03 45 10     add     eax, [ebp+10h]    ; add 3rd argument

           epilogue:
5D           pop     ebp               ; restore base pointer
C3           retn                      ; return to caller
</code></pre>
            </div>

          <p>
            Now if youâ€™ve been paying attention and youâ€™re still confused, you
            might be asking yourself one of these 2 questions:
          </p>

          <p>
            1) Why do we have to adjust
            <code>EBP</code> by 8
            to get to the first argument?
          </p>

          <ul>
            <li>
              If you
              <a href="#assembly-instructions">check the definition</a> of the
              <code>call</code>
              instruction we mentioned earlier, youâ€™ll realize that, internally,
              it actually pushes
              <code>EIP</code> to
              the stack. And if you also check the definition for
              <code>push</code>,
              youâ€™ll realize that it decrements
              <code>ESP</code>
              (which is copied to
              <code>EBP</code>
              after the prologue) by 4 bytes. In addition, the prologueâ€™s first
              instruction is also a
              <code>push</code>, so
              we end up with 2 decrements of 4, hence the need to add 8.
            </li>
          </ul>

          <p>
            2) What happened to the prologue and epilogue, why are they
            seemingly â€˜truncatedâ€™?
          </p>

          <ul>
            <li>
              Itâ€™s simply because we havenâ€™t had a use for the stack during the
              execution of our function - if youâ€™ve noticed, we havenâ€™t modified
              <code>ESP</code> at
              all, which means we also donâ€™t need to restore it.
            </li>
          </ul>

          <h4 id="if-conditions">If conditions</h4>

          <p>
            To demo the flow control assembly instructions, Iâ€™d like to add one
            more example to show how an if condition was compiled to assembly.
          </p>

          <p>Assume we have the following function:</p>

          <div>
              <pre><code><span>void</span> <span>print_equal</span><span>(</span><span>int</span> <span>a</span><span>,</span> <span>int</span> <span>b</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>a</span> <span>==</span> <span>b</span><span>)</span> <span>{</span>
        <span>printf</span><span>(</span><span>"equal"</span><span>);</span>
    <span>}</span>
    <span>else</span> <span>{</span>
        <span>printf</span><span>(</span><span>"nah"</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre>
            </div>

          <p>
            After compiling it, hereâ€™s the disassembly that I got with the help
            of
            <a href="https://hex-rays.com/ida-pro/" rel="noopener noreferrer" target="_blank">IDA</a>:
          </p>

          <div>
              <pre><code>void __cdecl print_equal(int, int):

     10000000   55                push   ebp
     10000001   8B EC             mov    ebp, esp
     10000003   8B 45 08          mov    eax, [ebp+8]       ; load 1st argument
     10000006   3B 45 0C          cmp    eax, [ebp+0Ch]     ; compare it with 2nd
  â”Œâ”… 10000009   75 0F             jnz    short loc_1000001A ; jump if not equal
  â”Š  1000000B   68 94 67 00 10    push   offset aEqual  ; "equal"
  â”Š  10000010   E8 DB F8 FF FF    call   _printf
  â”Š  10000015   83 C4 04          add    esp, 4
â”Œâ”€â”Šâ”€ 10000018   EB 0D             jmp    short loc_10000027
â”‚ â”Š
â”‚ â”” loc_1000001A:
â”‚    1000001A   68 9C 67 00 10    push   offset aNah    ; "nah"
â”‚    1000001F   E8 CC F8 FF FF    call   _printf
â”‚    10000024   83 C4 04          add    esp, 4
â”‚
â””â”€â”€ loc_10000027:
     10000027   5D                pop    ebp
     10000028   C3                retn
</code></pre>
            </div>

          <p>
            Give yourself a minute and try to make sense of this disassembly
            output (for simplicityâ€™s sake, Iâ€™ve changed the real addresses and
            made the function start from
            <code>10000000</code>
            instead).
          </p>

          <p>
            In case youâ€™re wondering about the
            <code>add esp, 4</code>
            part, itâ€™s simply there to adjust
            <code>ESP</code> back
            to its initial value (same effect as a
            <code>pop</code>,
            except without modifying any register), since we had to
            <code>push</code> the
            printf string argument.
          </p>

          <h3 id="basic-data-structures">Basic data structures</h3>

          <p>
            Now letâ€™s move on and talk about how data is stored (integers and
            strings especially).
          </p>

          <h4 id="endianness">Endianness</h4>

          <p>
            <a href="https://en.wikipedia.org/wiki/Endianness" rel="noopener noreferrer" target="_blank">Endianness</a>
            is the order of the sequence of bytes representing a value in
            computer memory.
          </p>

          <p>Thereâ€™s 2 types - big-endian and little-endian:</p>

          

          <p>
            For reference, x86 family processors (the ones on pretty much any
            computer you can find) always use little-endian.
          </p>

          <p>
            To give you a live example of this concept, Iâ€™ve compiled a Visual
            Studio C++ console app, where I declared an
            <code>int</code>
            variable with the value
            <code>1337</code>
            assigned to it, then I printed the variableâ€™s address using
            <code>printf()</code>,
            on the main function.
          </p>

          <p>
            Then I ran the program attached to the debugger in order to check
            the printed variableâ€™s address on the memory hex view, and hereâ€™s
            the result I obtained:
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-debug-memory-view.png" alt="">
          </p>

          <p>
            To elaborate more on this -
            <code>int</code>
            variables are 4 bytes long (32 bits) (in case you didnâ€™t know), so
            this means that if the variable starts from the address
            <code>D2FCB8</code> it
            would end right before
            <code>D2FCBC</code>
            (+4).
          </p>

          <p>
            To go from human readable value to memory bytes, follow these steps:
          </p>

          <p>
            decimal:
            <code>1337</code> -&gt;
            hex:
            <code>539</code> -&gt;
            bytes:
            <code>00 00 05 39</code>
            -&gt; little-endian:
            <code>39 05 00 00</code>
          </p>

          <h4 id="signed-integers">Signed integers</h4>

          <p>
            This part is interesting yet relatively simple. What you should know
            here is that integer signing (positive/negative) is typically done
            on computers with the help of a concept called
            <a href="https://en.wikipedia.org/wiki/Signed_number_representations#Two's_complement" rel="noopener noreferrer" target="_blank">twoâ€™s complement</a>.
          </p>

          <p>
            The gist of it is that the lowest/first half of an integer is
            reserved for positive numbers, while the highest/last half is for
            negative numbers, hereâ€™s what this looks like in hex, for a 32-bit
            signed int (highlighted = hex, in parenthesis = decimal):
          </p>

          <p>
            Positives (1/2):
            <code>00000000</code>
            (0) -&gt;
            <code>7FFFFFFF</code>
            (2,147,483,647 or
            <code>INT_MAX</code>)
          </p>

          <p>
            Negatives (2/2):
            <code>80000000</code>
            (-2,147,483,648 or
            <code>INT_MIN</code>)
            -&gt;
            <code>FFFFFFFF</code>
            (-1)
          </p>

          <p>
            If youâ€™ve noticed, weâ€™re always <em>ascending</em> in value. Whether
            we go up in hex or decimal. And thatâ€™s the crucial point of this
            concept - arithmetical operation do not have to do anything special
            to handle signing, they can simply treat all values as
            unsigned/positive, and the result would still be interpreted
            correctly (as long as we donâ€™t go beyond
            <code>INT_MAX</code> or
            <code>INT_MIN</code>),
            and thatâ€™s because integers will also <em>â€˜rolloverâ€™</em> on
            overflow/underflow by design, kinda like an analog odometer.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/odometer-rollover.jpg">
          </p>

          <p>
            <strong><em>Protip</em></strong>: The Windows calculator is a very helpful tool - you can set it to
            programmer mode and set the size to DWORD (4 bytes), then enter
            negative decimal values and visualize them in hex and binary, and
            have fun performing operations on them.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-int-signing.png" alt="">
          </p>

          <h4 id="strings">Strings</h4>

          <p>
            In C, strings are stored as
            <code>char</code>
            arrays, therefore, thereâ€™s nothing special to note here, except for
            something called null termination.
          </p>

          <p>
            If you ever wondered how
            <code>strlen()</code>
            is able to know the size of a string, itâ€™s very simple - strings
            have a character that indicates their end, and thatâ€™s the null
            byte/character -
            <code>00</code> or
            <code>'\0'</code>.
          </p>

          <p>
            If you declare a string constant in C code, and hover over it in
            Visual Studio, for instance, it will tell you the size of the
            generated array, and as you can see, for this reason, itâ€™s one
            element more than the â€˜visibleâ€™ string size.
          </p>

          <p>
            <img src="https://i.0x44.cc/b/vs-null-termination.png" alt="">
          </p>

          <p>
            <strong><em>Note</em></strong>: The endianness concept is not applicable on arrays, only on
            single variables. Therefore, the order of characters in memory would
            be normal here - low to high.
          </p>

          <h3 id="making-sense-of-call-and-jmp-instructions">
            Making sense of
            <code>call</code> and
            <code>jmp</code>
            instructions
          </h3>

          <p>
            Now that you know all of this, youâ€™re likely able to start making
            sense of some machine code, and emulate a CPU with your brain, to
            some extent, so to speak.
          </p>

          <p>
            Letâ€™s take the
            <a href="#if-conditions"><code>print_equal()</code>
              example</a>, but letâ€™s only focus on the
            <code>printf()</code>
            <code>call</code>
            instructions this time.
          </p>

          <div>
              <pre><code>void print_equal(int, int):
...
     10000010   E8 DB F8 FF FF    call   _printf
...
     1000001F   E8 CC F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            You might be wondering to yourself - wait a second, if these are the
            same instructions, then why are their bytes different?
          </p>

          <p>
            Thatâ€™s because,
            <code>call</code> (and
            <code>jmp</code>)
            instructions (usually) take an <em>offset</em> (relative address) as
            an argument, not an absolute address.
          </p>

          <p>
            An offset is basically the difference between the current location,
            and the destination, which also means that it can be either negative
            or positive.
          </p>

          <p>
            As you can see, the
            <a href="https://en.wikipedia.org/wiki/Opcode" rel="noopener noreferrer" target="_blank">opcode</a>
            of a
            <code>call</code>
            instruction that takes a 32-bit offset, is
            <code>E8</code>, and is
            followed by said offset - which makes the full instruction:
            <code>E8 XX XX XX XX</code>.
          </p>

          <p>
            Pull out your calculator,
            <del>whyâ€™d you close it so early?!</del> and calculate the
            difference between the offset of both instructions (donâ€™t forget the
            endianness).
          </p>

          <p>
            Youâ€™ll notice that (the absolute value of) this difference is the
            same as the one between the instruction addresses (<code>1000001F</code>
            -
            <code>10000010</code> =
            <code>F</code>):
          </p>

          <p>
            <img src="https://i.0x44.cc/b/calcexe-call-inst-diff.png" alt="">
          </p>

          <p>
            Another small detail that we should add, is the fact that the CPU
            only executes an instruction after fully â€˜readingâ€™ it, which means
            that by the time the CPU starts â€˜executingâ€™,
            <code>EIP</code> (the
            instruction pointer) is already pointing at the
            <em>next</em> instruction to be executed.
          </p>

          <p>
            Thatâ€™s why these offsets are actually accounting for this behaviour,
            which means that in order to get the <em>real</em> address of the
            target function, we have to also <em>add</em> the size of the
            <code>call</code>
            instruction: 5.
          </p>

          <p>
            Now letâ€™s apply all these steps in order to resolve
            <code>printf()</code>â€™s
            address from the first instruction on the example:
          </p>

          <div>
              <pre><code>10000010   E8 DB F8 FF FF    call   _printf
</code></pre>
            </div>

          <p>
            1) Extract the offset from the instruction:
            <code>E8 (DB F8 FF FF)</code>
            -&gt;
            <code>FFFFF8DB</code>
            (-1829)
          </p>

          <p>
            2) Add it to the instruction address:
            <code>10000010</code> +
            <code>FFFFF8DB</code> =
            <code>0FFFF8EB</code>
          </p>

          <p>
            3) And finally, add the instruction size:
            <code>0FFFF8EB</code> +
            5 =
            <code>0FFFF8F0</code>
            (<code>&amp;printf</code>)
          </p>

          <p>
            The exact same principle applies to the
            <code>jmp</code>
            instruction:
          </p>

          <div>
              <pre><code>...
â”Œâ”€â”€â”€ 10000018   EB 0D             jmp    short loc_10000027
...
â””â”€â”€ loc_10000027:
     10000027   5D                pop    ebp
...
</code></pre>
            </div>
          <p>
            The only difference in this example is that
            <code>EB XX</code> is a
            short version
            <code>jmp</code>
            instruction - which means it only takes an 8-bit (1 byte) offset.
          </p>

          <p>
            Therefore:
            <code>10000018</code> +
            <code>0D</code> + 2 =
            <code>10000027</code>
          </p>

          <h3 id="conclusion">Conclusion</h3>

          <p>
            Thatâ€™s it! You should now have enough information (and hopefully,
            motivation) to start your journey reverse engineering executables.
          </p>

          <p>
            Start by writing dummy C code, compiling it, and debugging it while
            single-stepping through the disassembly instructions (Visual Studio
            allows you to do this, by the way).
          </p>

          <p>
            <a href="https://godbolt.org/" rel="noopener noreferrer" target="_blank">Compiler Explorer</a>
            is also an extremely helpful website which compiles C code to
            assembly for you in real time using multiple compilers (select the
            <code>x86 msvc</code>
            compiler for Windows 32-bit).
          </p>

          <p>
            After that, you can try your luck with closed-source native
            binaries, by the help of disassemblers such as
            <a href="https://ghidra-sre.org/" rel="noopener noreferrer" target="_blank">Ghidra</a>
            and
            <a href="https://hex-rays.com/ida-free" rel="noopener noreferrer" target="_blank">IDA</a>, and debuggers such as
            <a href="https://x64dbg.com/" rel="noopener noreferrer" target="_blank">x64dbg</a>.
          </p>

          <p>
            <strong><em>Note</em></strong>: If youâ€™ve noticed inaccurate information, or room for improvement
            regarding this article, and would like to improve it, feel free to
            <a href="https://github.com/thedroidgeek/0x44.cc/edit/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">submit a pull request</a>
            on GitHub.
          </p>

          <p>Thanks for reading!</p>

          <p>
            <a href="https://github.com/thedroidgeek/0x44.cc/commits/master/_posts/2021-07-21-reversing-x86-and-c-code-for-beginners.md" rel="noopener noreferrer" target="_blank">(edited)</a>
          </p>

          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing Whistleblower: "If Anything Happens to Me, It's Not Suicide" (129 pts)]]></title>
            <link>https://twitter.com/WallStreetSilv/status/1768517997285482626</link>
            <guid>39715161</guid>
            <pubDate>Fri, 15 Mar 2024 13:20:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/WallStreetSilv/status/1768517997285482626">https://twitter.com/WallStreetSilv/status/1768517997285482626</a>, See on <a href="https://news.ycombinator.com/item?id=39715161">Hacker News</a></p>
Couldn't get https://twitter.com/WallStreetSilv/status/1768517997285482626: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[IAM Is the Worst (199 pts)]]></title>
            <link>https://matduggan.com/iam-is-the-worst/</link>
            <guid>39714155</guid>
            <pubDate>Fri, 15 Mar 2024 10:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matduggan.com/iam-is-the-worst/">https://matduggan.com/iam-is-the-worst/</a>, See on <a href="https://news.ycombinator.com/item?id=39714155">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>Imagine your job was to clean a giant office building. You go from floor to floor, opening doors, collecting trash, getting a vacuum out of the cleaning closet and putting it back. It's a normal job and part of that job is someone gives you a key. The key opens every door everywhere. Everyone understands the key is powerful, but they also understand you need to do your job. </p><p>Then your management hears about someone stealing janitor keys. So they take away your universal key and they say "you need to tell Suzie, our security engineer, which keys you need at which time". But the keys don't just unlock one door, some unlock a lot of doors and some desk drawers, some open the vault (imagine this is the Die Hard building), some don't open any doors but instead turn on the coffee machine. Obviously the keys have titles, but the titles mean nothing. Do you need the "executive_floor/admin" key or the "executive_floor/viewer" key? </p><p>But you are a good employee and understand that security is a part of the job. So you dutifully request the keys you think you need, try to do your job, open a new ticket when the key doesn't open a door you want, try it again, it still doesn't open the door you want so then there's another key. Soon your keyring is massive, just a clanging sound as you walk down the hallway. It mostly works, but a lot of the keys open stuff you don't need, which makes you think maybe this entire thing was pointless. </p><p>The company is growing and we need new janitors, but they don't want to give all the new janitors your key ring. So they roll out a new system which says "now the keys can only open doors that we have written down that this key can open, even if it says "executive_floor/admin". The problem is people move offices all the time, so even if the list of what doors that key opened was true when it was issued, it's not true tomorrow. The Security team and HR share a list, but the list sometimes drifts or maybe someone moves offices without telling the right people. </p><p>Soon nobody is really 100% sure what you can or cannot open, including you. Sure someone can audit it and figure it out, but the risk of removing access means you cannot do your job and the office doesn't get cleaned. So practically speaking the longer someone works as a janitor the more doors they can open until eventually they have the same level of access as your original master key even if that wasn't the intent. </p><p>That's IAM (Identity and access management) in cloud providers today. </p><h3 id="stare-into-madness">Stare Into Madness</h3><figure><img src="https://docs.aws.amazon.com/images/IAM/latest/UserGuide/images/PolicyEvaluationHorizontal111621.png" alt="" loading="lazy"><figcaption><span>AWS IAM Approval Flow</span></figcaption></figure><figure><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_copy_3.max-2000x2000.jpg" alt="" loading="lazy"><figcaption><span>GCP IAM Approval Flow</span></figcaption></figure><figure><img src="https://images.fastcompany.com/upload/Simple.jpg" alt="It's Not Natural, It's Just Simple: Food Branding Co-Opts Another Mean" loading="lazy"></figure><p>Honestly I don't even know why I'm complaining. Of course it's entirely reasonable to expect anyone working in a cloud environment to understand the dozen+ ways that they may or may not have access to a particular resource. Maybe they have permissions at a folder level, or an org level, but that permission is gated by specific resources. </p><p>Maybe they don't even have access but the tool they're interacting with the resource with has permission to do it, so they can do it but only as long as they are SSH'd into host01, not if they try to do it through some cloud shell. Possibly they had access to it before, but now they don't since they moved teams. Perhaps the members of this team were previously part of some existing group but now new employees aren't added to that group so some parts of the team can access X but others cannot. Or they actually have the correct permissions to the resource but the resource is located in another account and they don't have the right permission to traverse the networking link between the two VPCs.</p><p>Meanwhile someone is staring at these flowcharts trying to figure out what in hell is even happening here. As someone who has had to do this multiple times in my life, let me tell you the real-world workflow that ends up happening. </p><ul><li>Developer wants to launch a new service using new cloud products. They put in a ticket for me to give them access to the correct "roles" to do this. </li><li>I need to look at two elements of it, both what are the permissions the person needs in order to see if the thing is working and then the permissions the service needs in order to complete the task it is trying to complete. </li><li>So I go through my giant list of roles and try to cobble together something that I think based on the names will do what I want. Do you feel like a <code>roles/datastore.viewer</code> or more of a <code>roles/datastore.keyVisualizerViewer</code>? To run backups is <code>roles/datastore.backupsAdmin</code> sufficient or do I need to add <code>roles/datastore.backupSchedulesAdmin</code> in there as well?</li><li>They try it and it doesn't work. Reopen the ticket with "I still get authorizationerror:foo". I switch that role with a different role, try it again. Run it through the simulator, it seems to work, but they report a new different error because actually in order to use service A you need to also have a role in service B. Go into bathroom, scream into the void and return to your terminal.</li><li>We end up cobbling together a custom role that includes all the permissions that this application needs and the remaining 90% of permissions are something it will never ever use but will just sit there as a possible security hole. </li><li>Because /* permissions are the work of Satan, I need to scope it to specific instances of that resource and just hope nobody ever adds a SQS queue without....checking the permissions I guess. In theory we should catch it in the non-prod environments but there's always the chance that someone messes up something at a higher level of permissions that does something in non-prod and doesn't exist in prod so we'll just kinda cross our fingers there. </li></ul><h3 id="gcp-makes-it-worse">GCP Makes It Worse</h3><p>So that's effectively the AWS story, which is terrible but at least it's possible to cobble together something that works and you can audit. Google looked at this and said "what if we could express how much we hate Infrastructure teams as a service?" Expensive coffee robots were engaged, colorful furniture was sat on and the brightest minds of our generation came up with a system so punishing you'd think you did something to offend them personally. </p><p>Google looked at AWS and said "this is a tire fire" as corporations put non-prod and prod environments in the same accounts and then tried to divide them by conditionals. So they came up with a folder structure:</p><figure><img src="https://infosec.rodeo/assets/img/blog/gcp_resource_hierarchy.png" alt="GCP Resource Hierarchy" loading="lazy"></figure><p>The problem is that this design encourages unsafe practices by promoting "groups should be set at the folder level with one of the default basic roles". It makes sense logically at first that you are a viewer, editor or owner. But as GCP adds more services this model breaks down quickly because each one of these encompasses thousands upon thousands of permissions. So additional IAM predefined roles were layered on. </p><p>People were encouraged to move away from the basic roles and towards the predefined roles. There are ServiceAgent roles that were designated for service accounts, aka the permissions you actual application has and then everything else. Then there are 1687 other roles for you to pick from to assign to your groups of users. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-2.png" alt="" loading="lazy" width="860" height="334" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-2.png 600w, https://matduggan.com/content/images/2024/03/image-2.png 860w" sizes="(min-width: 720px) 720px"></figure><p>The problem is none of this is actually best practice. Even when assigning users "small roles", we're still not following the principal of least privilege. Also the roles don't remain static. As new services come online permissions are added to roles. </p><figure><img src="https://matduggan.com/content/images/2024/03/image-4.png" alt="" loading="lazy" width="2000" height="1254" srcset="https://matduggan.com/content/images/size/w600/2024/03/image-4.png 600w, https://matduggan.com/content/images/size/w1000/2024/03/image-4.png 1000w, https://matduggan.com/content/images/size/w1600/2024/03/image-4.png 1600w, https://matduggan.com/content/images/size/w2400/2024/03/image-4.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>The above is an automated process that pulls down all the roles from the gcloud CLI tool and updates them for latest. It is a constant state of flux with roles with daily changes. It gets even more complicated though. </p><p>You also need to check the launch stage of a role. </p><blockquote>Custom roles include a launch stage as part of the role's metadata. The most common launch stages for custom roles are ALPHA, BETA, and GA. These launch stages are informational; they help you keep track of whether each role is ready for widespread use. Another common launch stage is DISABLED. This launch stage lets you disable a custom role.</blockquote><blockquote>We recommend that you use launch stages to convey the following information about the role:</blockquote><blockquote>EAP or ALPHA: The role is still being developed or tested, or it includes permissions for Google Cloud services or features that are not yet public. It is not ready for widespread use.<br>BETA: The role has been tested on a limited basis, or it includes permissions for Google Cloud services or features that are not generally available.<br>GA: The role has been widely tested, and all of its permissions are for Google Cloud services or features that are generally available.<br>DEPRECATED: The role is no longer in use.</blockquote><h3 id="who-cares">Who Cares?</h3><p>Why would anyone care if Google is constantly changing roles? Well it matters because with GCP to make a custom role, you cannot combine predefined roles. Instead you need to go down to the permission level to list out all of the things those roles can do, then feed that list of permissions into the definition of your custom role and push that up to GCP. </p><p>In order to follow best practices this is what you have to do. Otherwise you will always be left with users that have a ton of unused permissions along with the fear of a security breach allowing someone to execute commands in your GCP account through an applications service account that cause way more damage than the actual application justifies. </p><p>So you get to build automated tooling which either queries the predefined roles for change over time and roll those into your custom roles so that you can assign a user or group one specific role that lets them do everything they need. Or you can assign these same folks multiple of the 1600+ predefined roles, accept that they have permissions they don't need and also just internalize that day to day you don't know how much the scope of those permissions have changed. </p><h3 id="the-obvious-solution">The Obvious Solution</h3><p>Why am I ranting about this? Because the solution is so blindly obvious I don't understand why we're not already doing it. It's a solution I've had to build, myself, multiple times and at this point am furious that this keeps being my responsibility as I funnel hundreds of thousands of dollars to cloud providers. </p><p>What is this obvious solution? You, an application developer, need to launch a new service. I give you a service account that lets you do almost everything inside of that account along with a viewer account for your user that lets you go into the web console and see everything. You churn away happily, writing code that uses all those new great services. Meanwhile, we're tracking all the permissions your application and you are using. </p><p>At some time interval, 30 or 90 or whatever days, my tool looks at the permissions your application has used over the last 90 days and says "remove the global permissions and scope it to these". I don't need to ask you what you need, because I can see it. In the same vein I do the same thing with your user or group permissions. You don't need viewer everywhere because I can see what you've looked at. </p><p>Both GCP and AWS support this and have all this functionality baked in. GCP has the <a href="https://cloud.google.com/policy-intelligence/docs/role-recommendations-overview" rel="noreferrer">role recommendations</a> which tracks exactly what I'm talking about and recommends lowering the role. <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html" rel="noreferrer">AWS tracks the exact same information</a> and can be used to do the exact same thing. </p><p><strong>What if the user needs different permissions in a hurry?</strong></p><p>This is not actually that hard to account for and <em>again</em> is something I and countless others have been forced to make over and over. You can issue expiring permissions in both situations where a user can request a role be temporarily granted to them and then it disappears in 4 hours. I've seen every version of these, from Slack bots to websites, but they're all the same thing. If user is in X group they're allowed to request Y temporary permissions. OR if the user is on-call as determined with an API call to the on-call provider they get more powers. Either design works fine. </p><p><strong>That seems like a giant security hole</strong></p><p>Compared to what? Team A guessing what Team B needs even though they don't ever do the work that Team B does? Some security team receiving a request for permissions and trying to figure out if the request "makes sense" or not? At least this approach is based on actual data and not throwing darts at a list of IAM roles and seeing what "feels right". </p><h3 id="conclusion">Conclusion</h3><p>IAM started out as an easy idea that as more and more services were launched, started to become nightmarish to organize. It's too hard to do the right thing now and it's even harder to do the right thing in GCP compared to AWS. The solution is not complicated. We have all the tools, all the data, we understand how they fit together. We just need one of the providers to be brave enough to say "obviously we messed up and this legacy system you all built your access control on is bad and broken". It'll be horrible, we'll all grumble and moan but in the end it'll be a better world for us all. </p><p>Feedback: <a href="https://c.im/@matdevdug">https://c.im/@matdevdug</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Matrix Multiplication with Half the Multiplications (155 pts)]]></title>
            <link>https://github.com/trevorpogue/algebraic-nnhw</link>
            <guid>39714053</guid>
            <pubDate>Fri, 15 Mar 2024 10:36:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trevorpogue/algebraic-nnhw">https://github.com/trevorpogue/algebraic-nnhw</a>, See on <a href="https://news.ycombinator.com/item?id=39714053">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repository contains the source code for ML hardware architectures that require nearly half the number of multiplier units to achieve the same performance, by executing alternative inner-product algorithms that trade nearly half the multiplications for cheap low-bitwidth additions, while still producing identical output as the conventional inner product. This increases the theoretical throughput and compute efficiency limits of ML accelerators. See the following journal publication for the full details:</p>
<p dir="auto">T. E. Pogue and N. Nicolici, "Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators," in IEEE Transactions on Computers, vol. 73, no. 2, pp. 495-509, Feb. 2024, doi: 10.1109/TC.2023.3334140.</p>

<p dir="auto">Article URL: <a href="https://ieeexplore.ieee.org/document/10323219" rel="nofollow">https://ieeexplore.ieee.org/document/10323219</a></p>
<p dir="auto">Open-access version: <a href="https://arxiv.org/abs/2311.12224" rel="nofollow">https://arxiv.org/abs/2311.12224</a></p>
<p dir="auto">Abstract: We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.</p>
<p dir="auto">The following diagram shows an overview of the ML accelerator system implemented in this source code:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE"><img src="https://private-user-images.githubusercontent.com/12535207/285502293-11a7d485-04a3-4e9d-b9fb-91c35c80086f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8yODU1MDIyOTMtMTFhN2Q0ODUtMDRhMy00ZTlkLWI5ZmItOTFjMzVjODAwODZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNjZTJkM2MyZDdkODdkNDZlODg3NTJkZDcyZmYzNjhhMzJiYmM1NzRlNDk3MjM3YTM3MzMwMjNkODllOTdiZGUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9a6yVkzLncS7pXRGFoxzDOrjm01sfCz7jQEFtOGf5oE" width="450"></a></p>
<p dir="auto">The FIP and FFIP systolic array/MXU processing elements (PE)s shown below in (b) and (c) implement the FIP and FFIP inner-product algorithms and each individually provide the same effective computational power as the two baseline PEs shown in (a) combined which implement the baseline inner product as in previous systolic-array ML accelerators:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU"><img src="https://private-user-images.githubusercontent.com/12535207/300184475-d9b956a2-25fa-4173-8ba9-8fd27d02f0c1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDAxODQ0NzUtZDliOTU2YTItMjVmYS00MTczLThiYTktOGZkMjdkMDJmMGMxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTRkMWY4NjIyODBkNDAwNjExNzZkZTJiMmJiZDk0YTRlY2I1Zjk0MDE1YjZlM2UyMGNiMzIyYzA1NjQyZGY0OTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.433P7aTz05yurSzH_kAJDMyMGRZZAi1IUDxKuesO_zU" width="450"></a></p>
<p dir="auto">The following is a diagram of the MXU/systolic array and shows how the PEs are connected:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0"><img src="https://private-user-images.githubusercontent.com/12535207/300986120-baf3e2f7-1767-49ec-811e-7cb44fac8d92.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA1MTg3MDUsIm5iZiI6MTcxMDUxODQwNSwicGF0aCI6Ii8xMjUzNTIwNy8zMDA5ODYxMjAtYmFmM2UyZjctMTc2Ny00OWVjLTgxMWUtN2NiNDRmYWM4ZDkyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDE2MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTBlYWRkZWQ0MjM3YWJkNWRkZjYyOGNmZTc5ZWQ1M2RkNjEyYmFiOGZjOWFkYTg4ZGZhNjlhNmUwOThkOTViNzEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QnLh8a3ntuJUB_ICABvi0rG-Czg0Yt2kVquJBTFXLZ0" width="450"></a></p>
<p dir="auto">The source code organization is as follows:</p>
<ul dir="auto">
<li>compiler
<ul dir="auto">
<li>A compiler for parsing Python model descriptions into accelerator instructions that allow it to accelerate the model. This part also includes code for interfacing with a PCIe driver for initiating model execution on the accelerator, reading back results and performance counters, and testing the correctness of the results.</li>
</ul>
</li>
<li>rtl
<ul dir="auto">
<li>Synthesizable SystemVerilog RTL.</li>
</ul>
</li>
<li>sim
<ul dir="auto">
<li>Scripts for setting up simulation environments for testing.</li>
</ul>
</li>
<li>tests
<ul dir="auto">
<li>UVM-based testbench source code for verifying the accelerator in simulation using Cocotb.</li>
</ul>
</li>
<li>utils
<ul dir="auto">
<li>Additional Python packages and scripts used in this project that the author created for general development utilities and aids.</li>
</ul>
</li>
</ul>
<p dir="auto">The files rtl/top/define.svh and rtl/top/pkg.sv contain a number of configurable parameters such as FIP_METHOD in define.svh which defines the systolic array type (baseline, FIP, or FFIP), SZI and SZJ which define the systolic array height/width, and LAYERIO_WIDTH/WEIGHT_WIDTH which define the input bitwidths.</p>
<p dir="auto">The directory rtl/arith includes mxu.sv and mac_array.sv which contain the RTL for the baseline, FIP, and, FFIP systolic array architectures (depending on the value of the parameter FIP_METHOD).</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking (206 pts)]]></title>
            <link>https://arxiv.org/abs/2403.09629</link>
            <guid>39713634</guid>
            <pubDate>Fri, 15 Mar 2024 09:24:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.09629">https://arxiv.org/abs/2403.09629</a>, See on <a href="https://news.ycombinator.com/item?id=39713634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.09629">Download PDF</a>
    <a href="https://arxiv.org/html/2403.09629v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eric Zelikman [<a href="https://arxiv.org/show-email/094380da/2403.09629">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 14 Mar 2024 17:58:16 UTC (510 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sunlight, a Certificate Transparency log implementation (118 pts)]]></title>
            <link>https://letsencrypt.org/2024/03/14/introducing-sunlight.html</link>
            <guid>39713370</guid>
            <pubDate>Fri, 15 Mar 2024 08:37:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2024/03/14/introducing-sunlight.html">https://letsencrypt.org/2024/03/14/introducing-sunlight.html</a>, See on <a href="https://news.ycombinator.com/item?id=39713370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p><img alt="Logo for Sunlight" src="https://letsencrypt.org/images/blog/sunlight_logo_main.png">
</p>
<p>Letâ€™s Encrypt is proud to introduce Sunlight, a new implementation of a Certificate Transparency log that we built from the ground up with modern Web PKI opportunities and constraints in mind. In partnership with <a href="https://filippo.io/">Filippo Valsorda</a>, who led the design and implementation, we incorporated feedback from the broader transparency logging community, including the Chrome and TrustFabric teams at Google, the Sigsum project, and other CT log and monitor operators. Their insights have been instrumental in shaping the projectâ€™s direction.</p>
<p>CT plays an important role in the Web PKI, enhancing the ability to monitor and research certificate issuance. The operation of a CT log, however, faces growing challenges with the increasing volume of certificates. For instance, Letâ€™s Encrypt issues over four million certificates daily, each of which must be logged in two separate CT logs. Our well-established â€œOakâ€ log currently holds over 700 million entries, reflecting the significant scale of these challenges.</p>
<p>In this post, weâ€™ll explore the motivation behind Sunlight and how its design aims to improve the robustness and diversity of the CT ecosystem, while also improving the reliability and performance of Letâ€™s Encryptâ€™s logs.</p>
<h2 id="bottlenecks-from-the-database">Bottlenecks from the Database</h2>
<p>Letâ€™s Encrypt has been <a href="https://letsencrypt.org/docs/ct-logs/">running public CT logs</a> since 2019, and weâ€™ve gotten a lot of operational experience with running them, but it hasnâ€™t been trouble-free. The biggest challenge in the architecture weâ€™ve deployed for our â€œOakâ€ log is that the data is stored in a relational database. Weâ€™ve <a href="https://letsencrypt.org/2022/05/19/nurturing-ct-log-growth">scaled that up</a> by splitting each yearâ€™s worth of data into a â€œshardâ€ with its own database, and then later shrinking the shards to cover six months instead of a full year.</p>
<p>The approach of splitting into more and more databases is not something we want to continue doing forever, as the operational burden and costs increase. The current storage size of a CT log shard is between 5 and 10 terabytes. Thatâ€™s big enough to be concerning for a single database: We previously had a test log fail when we ran into a <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.KnownIssuesAndLimitations.html#MySQL.Concepts.Limits.FileSize">16TiB limit</a> in MySQL.</p>
<p>Scaling read capacity up requires large database instances with fast disks and lots of RAM, which are not cheap. Weâ€™ve had numerous instances of CT logs becoming overloaded by clients attempting to read all the data in the log, overloading the database in the process. When rate limits are imposed to prevent overloading, clients are forced to slowly crawl the API, diminishing CTâ€™s efficiency as a fast mechanism for detecting mis-issued certificates.</p>
<h2 id="serving-tiles">Serving Tiles</h2>
<p>Initially, Letâ€™s Encrypt only planned on building a new CT log implementation. However, our discussions with Filippo made us realize that other transparency systems had improved on the original Certificate Transparency design, and we could make our logs even more robust and scalable by changing the read path APIs. In particular, the <a href="https://golang.org/design/25530-sumdb">Go Checksum Database</a> is inspired by Certificate Transparency, but uses a more efficient format for publishing its data as a series of easily stored and cached tiles.</p>
<p>Certificate Transparency logs are a binary tree, with every node containing a hash of its two children. The â€œleafâ€ level contains the actual entries of the log: the certificates, appended to the right side of the tree. The top of the tree is digitally signed. This forms a cryptographically verifiable structure called a Merkle Tree, which can be used to check if a certificate is in the tree, and that the tree is append-only.</p>
<p>Sunlight tiles are files containing 256 elements each, either hashes at a certain tree â€œheightâ€ or certificates (or pre-certificates) at the leaf level. Russ Cox has a great explanation <a href="https://research.swtch.com/tlog#tiling_a_log">of how tiles work on his blog</a>, or you can read <a href="https://c2sp.org/sunlight#merkle-tree">the relevant section of the Sunlight specification</a>. Even Trillian, the current implementation of CT we run, <a href="https://github.com/google/trillian/blob/master/docs/storage/storage.md">uses a subtree system</a> similar to these tiles as its internal storage.</p>
<p>Unlike the dynamic endpoints in previous CT APIs, serving a tree as tiles doesnâ€™t require any dynamic computation or request processing, so we can eliminate the need for API servers. Because the tiles are static, theyâ€™re efficiently cached, in contrast with CT APIs like get-proof-by-hash which have a different response for every certificate, so thereâ€™s no shared cache. The leaf tiles can also be stored compressed, saving even more storage!</p>
<p>The idea of exposing the log as a series of static tiles is motivated by our desire to scale out the read path horizontally and relatively inexpensively. We can directly expose tiles in cloud object storage like S3, use a caching CDN, or use a webserver and a filesystem.</p>
<p>Object or file storage is readily available, can scale up easily, and costs significantly less than databases from cloud providers. It seemed like the obvious path forward. In fact, we already have an S3-backed cache in front of our existing CT logs, which means we are currently storing our data twice.</p>
<h2 id="running-more-logs">Running More Logs</h2>
<p>The tiles API improves the read path, but we also wanted to simplify our architecture on the write path. With Trillian, we run a collection of nodes along with etcd for leader election to choose which will handle writing. This is somewhat complex, and we believe the CT ecosystem allows a different tradeoff.</p>
<p>The key realization is that Certificate Transparency is already a distributed system, with clients submitting certificates to multiple logs, and gracefully failing over from any unavailable ones to the others. Each individual logâ€™s write path doesnâ€™t require a highly available leader election system. A simple single-node writer can meet the 99% Service Level Objective of a CT log.</p>
<p>The single-node Sunlight architecture lets us run multiple independent logs with the same amount of computing power. This increases the systemâ€™s overall robustness, even if each individual log has lower potential uptime. No more leader election needed. We use a simple compare-and-swap mechanism to store checkpoints and prevent accidentally running two instances at once, which could result in a forked tree, but that has much less overhead than leader election.</p>
<h2 id="no-more-merge-delay">No More Merge Delay</h2>
<p>One of the goals of CT was to have limited latency for submission to the logs. A design feature called Merge Delay was added to support that. When submitting a certificate to a log, the log can return a Signed Certificate Timestamp (SCT) immediately, with a promise to include it in the log within the logâ€™s Maximum Merge Delay, conventionally 24 hours. While this seems like a good tradeoff to not slow down issuance, there have been multiple incidents and near-misses where a log stops operating with unmerged certificates, missing its maximum merge delay, and breaking that promise.</p>
<p>Sunlight takes a different approach, holding submissions while it batches and integrates certificates in the log, eliminating the merge delay. While this leads to a small latency increase, we think itâ€™s worthwhile to avoid one of the more common CT log failure cases.</p>
<p>It also lets us embed the final leaf index in an extension of our SCTs, bringing CT a step closer to direct client verification of Merkle tree proofs. The extension also makes it possible for clients to fetch the proof of log inclusion from the new static tile-based APIs, without requiring server-side lookup tables or databases.</p>
<h2 id="a-sunny-future">A Sunny Future</h2>
<p>Todayâ€™s announcement of Sunlight is just the beginning. Weâ€™ve released <a href="https://github.com/FiloSottile/sunlight">software</a> and a <a href="https://c2sp.org/sunlight">specification</a> for Sunlight, and have Sunlight CT logs running. Head to <a href="https://sunlight.dev/">sunlight.dev</a> to find resources to get started. We encourage CAs to start test submitting to <a href="https://letsencrypt.org/docs/ct-logs/#Sunlight">Letâ€™s Encryptâ€™s new Sunlight CT logs</a>, for CT Monitors and Auditors to add support for consuming Sunlight logs, and for the CT programs to consider trusting logs running on this new architecture. We hope Sunlight logs will be made usable for SCTs by the CT programs run by the browsers in the future, allowing CAs to rely on them to meet the browser CT logging requirements.</p>
<p>Weâ€™ve gotten positive feedback so far, with comments such as â€œGoogleâ€™s TrustFabric team, maintainers of Trillian, are supportive of this direction and the Sunlight spec. We have been working towards the same goal of cacheable tile-based logs for other ecosystems with <a href="https://github.com/transparency-dev/serverless-log">serverless tooling</a>, and will be folding this into Trillian and ctfe, along with adding support for the Sunlight API.â€</p>
<p>If you have feedback on the design, please join in the conversation on the <a href="https://groups.google.com/a/chromium.org/g/ct-policy">ct-policy mailing list</a>, or in the <a href="https://transparency-dev.slack.com/archives/C06PCS2P75Y">#sunlight</a> channel on the transparency-dev Slack (<a href="https://join.slack.com/t/transparency-dev/shared_invite/zt-27pkqo21d-okUFhur7YZ0rFoJVIOPznQ">invitation</a> to join).</p>
<p>Weâ€™d like to thank Chrome for supporting the development of Sunlight, and Amazon Web Services for their ongoing support for our CT log operation. If your organization monitors or values CT, please consider a financial gift of support. Learn more at <a href="https://www.abetterinternet.org/sponsor/">https://www.abetterinternet.org/sponsor/</a> or contact us at: <a href="mailto:sponsor@abetterinternet.org">sponsor@abetterinternet.org</a>.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin's techno scene added to Unesco intangible cultural heritage list (272 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</link>
            <guid>39713323</guid>
            <pubDate>Fri, 15 Mar 2024 08:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list">https://www.theguardian.com/world/2024/mar/15/berlins-techno-scene-added-to-unesco-intangible-cultural-heritage-list</a>, See on <a href="https://news.ycombinator.com/item?id=39713323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Germanyâ€™s culture ministry and Unesco commission have added Berlinâ€™s techno scene to the countryâ€™s list of intangible cultural heritage, in recognition of the sceneâ€™s contribution to the cultural identity of the city.</p><p>Berlinâ€™s Clubcommission, a network for Berlinâ€™s techno clubs and musicians, <a href="https://twitter.com/clubcommission/status/1767979957173580132" data-link-name="in body link">described</a> the move as â€œanother milestone for Berlin techno producers, artists, club operators and event organisersâ€.</p><p>Lutz Leichsenring, an executive member of Clubcommissionâ€™s board, <a href="https://www.dw.com/en/berlin-techno-added-to-unesco-cultural-heritage-list/a-68515354" data-link-name="in body link">told the German broadcaster DW</a>: â€œThe decision will help us ensure that club culture is recognised as a valuable sector worthy of protection and support.â€</p><p>For more than a decade there has been a campaign to have techno culture and music added to Germanyâ€™s list, spearheaded by Rave the Planet, a non-profit supporting electronic music culture.</p><p>â€œCongratulations to all the cultural creators who have shaped and contributed to Berlinâ€™s techno culture,â€ the group said in a statement on social media. â€œThis is a major milestone for the entire culture, and our joy is beyond words.â€</p><p>Rave the Planet submitted the application for techno to be included in the list in November 2022.</p><p>Intangible cultural heritage status is more commonly granted to more traditional cultural activities, such as Malawian Mwinoghe dancing or Slovakian bagpipe culture. The recent recognition on Unescoâ€™s list of intangible cultural heritage of Jamaican reggae and Indiaâ€™s huge Kumbh Mela festival, however, prompted techno community leaders in Berlin to campaign for their scene to be included in Germanyâ€™s register, which is separate to the Unesco list.</p><p>Techno is a fundamental part of the city, according to Peter Kirn, a Berlin-based DJ and music producer. In 2021 he told the Observer: â€œIn other cities, people wouldnâ€™t accept music thatâ€™s really hard or weird and full of synthesisers and really brutal, distorted drum machines. You canâ€™t play that at peak hour in a club, let alone over lunch. And here itâ€™s totally acceptable to play that over lunch.</p><p>â€œTechno has become a refuge for people who are marginalised, and thereâ€™s a natural attraction to Berlin as a place which is more permissive when you come from places that are less permissive.â€</p><p>The techno scene is one of six new entries on the intangible cultural heritage list in Germany; others include fruit wine and mountaineering. A parade in Bavaria known as the <em>Kirchseeoner Perchtenlauf</em>, where attenders dress as furry monsters, was also added to the list.</p></div><div><p><span data-dcr-style="bullet"></span> The headline and text of this article were amended on 15 March 2024. The techno scene has been added to the national intangible cultural heritage list compiled by the German commission for Unesco, not the global intangible cultural heritage list compiled by Unesco as an earlier version indicated.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[John Barnett before death "if anything happens, it's not suicide", claims friend (518 pts)]]></title>
            <link>https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</link>
            <guid>39712618</guid>
            <pubDate>Fri, 15 Mar 2024 06:32:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024">https://abcnews4.com/news/local/if-anything-happens-its-not-suicide-boeing-whistleblowers-prediction-before-death-south-carolina-abc-news-4-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39712618">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>John Barnett's family friend Jennifer doesn't think the Boeing whistleblower committed suicide in Charleston. In fact, she says he predicted what may happen to him days before he left for his deposition. March 14, 2024. (Provided-FILE, WCIV)</p><div id="js-Story-Content-0"><p><span>CHARLESTON COUNTY, S.C. (WCIV)  â€” </span>A close family friend of John Barnett said he predicted he might wind up dead and that a story could surface that he killed himself. </p><p>But at the time, he told her not to believe it. </p><p>"I know that he did not commit suicide," said Jennifer, a friend of Barnett's. "There's no way." </p><p>Jennifer said they talked about this exact scenario playing out. However, now, his words seem like a premonition he told her directly not to believe. </p><p>"I know John because his mom and my mom are best friends," Jennifer said. "Over the years, get-togethers, birthdays, celebrations and whatnot. We've all got together and talked." </p><p><strong><em>READ MORE:<a href="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024" target="_blank" title="https://abcnews4.com/news/local/mystery-lingers-around-boeing-whistleblower-death-at-charleston-hotel-charleston-county-john-barnett-boeing-news-lawsuit-abc-news-4-wciv-2024"> "Mystery lingers around Boeing whistleblower's death at Charleston hotel."</a></em></strong></p><p>When Jennifer needed help one day, Barnett came by to see her. They talked about his upcoming deposition in Charleston. Jennifer knew Barnett filed an extremely damaging complaint against Boeing. He said the aerospace giant retaliated against him when he blew the whistle on unsafe practices. <br></p><p>For more than 30 years, he was a quality manager. He'd recently retired and moved back to Louisiana to look after his mom. </p><p>"He wasn't concerned about safety because I asked him," Jennifer said. "I said, 'Aren't you scared?' And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'" </p><p>Jennifer added: "I know that he did not commit suicide. There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now." </p><p>Jennifer said she thinks somebody "didn't like what he had to say" and wanted to "shut him up" without it coming back to anyone. </p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024" target="_blank" title="https://abcnews4.com/news/local/john-was-a-brave-boeing-whistleblowers-lawyer-responds-to-news-of-his-death-john-barnett-boeing-news-abc-news-wciv-news-4-2024">"'John was brave': Boeing whistleblower's lawyer responds to news of his death."</a></em></strong></p><p>"That's why they made it look like a suicide," Jennifer said. </p><p>The last time Jennifer saw Barnett was at her father's funeral in late February. He was one of the pallbearers. Sometimes family and friends referred to him by his middle name â€“ Mitch. </p><p>"I think everybody is in disbelief and can't believe it," Jennifer said. "I don't care what they say, I know that Mitch didn't do that." </p><p>Just because Barnett is dead doesn't mean the case won't move forward. </p><p>His attorney said they're still prepared to go to trial in June. </p><p>News 4 reached out to Boeing following Barnett's death. They provided the following statement: </p><p>"We are saddened by Mr. Barnettâ€™s passing, and our thoughts are with his family and friends.â€<br></p><p><strong><em>READ MORE: <a href="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4" target="_blank" title="https://abcnews4.com/news/local/boeing-whistleblower-dies-in-charleston-charleston-county-coroners-office-confirms-south-carolina-boeing-news-abc-news-4">"Boeing whistleblower dies in Charleston, Charleston County Coroner's Office confirms."</a></em></strong></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard concluded that a dishonesty expert committed misconduct (202 pts)]]></title>
            <link>https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</link>
            <guid>39712173</guid>
            <pubDate>Fri, 15 Mar 2024 04:53:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct">https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct</a>, See on <a href="https://news.ycombinator.com/item?id=39712173">Hacker News</a></p>
Couldn't get https://www.chronicle.com/article/heres-the-unsealed-report-showing-how-harvard-concluded-that-a-dishonesty-expert-committed-misconduct: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[RSS was released 25 years ago today (103 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/RSS</link>
            <guid>39712025</guid>
            <pubDate>Fri, 15 Mar 2024 04:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/RSS">https://en.wikipedia.org/wiki/RSS</a>, See on <a href="https://news.ycombinator.com/item?id=39712025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en" dir="ltr" id="mw-content-text">




<table><caption>RSS</caption><tbody><tr><td colspan="2"><span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg" title="Feed Computer icon."><img alt="Feed Computer icon." src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/128px-Feed-icon.svg.png" decoding="async" width="128" height="128" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/192px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/256px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Filename_extension" title="Filename extension">Filename extension</a></th><td><p><kbd>.rss, .xml</kbd></p></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Media_type" title="Media type">Internet media&nbsp;type</a></th><td><code>application/rss+xml</code>&nbsp;(registration not finished)<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup></td></tr><tr><th scope="row">Developed&nbsp;by</th><td><a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a></td></tr><tr><th scope="row">Initial release</th><td>RSS 0.90 (Netscape), March&nbsp;15, 1999<span>; 25 years ago</span></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Software_release_life_cycle" title="Software release life cycle">Latest release</a></th><td><p>RSS 2.0 (version 2.0.11)<br>March&nbsp;30, 2009<span>; 14 years ago</span> </p></td></tr><tr><th scope="row">Type of format</th><td><a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">Web syndication</a></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Digital_container_format" title="Digital container format">Container&nbsp;for</a></th><td>Updates of a website and its related metadata (<a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a>)</td></tr><tr><th scope="row">Extended&nbsp;from</th><td><a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a></td></tr><tr><th scope="row"><span><a href="https://en.wikipedia.org/wiki/Open_file_format" title="Open file format">Open format</a>?</span></th><td>Yes</td></tr><tr><th scope="row">Website</th><td><span><a rel="nofollow" href="http://rssboard.org/rss-specification">rssboard<wbr>.org<wbr>/rss-specification</a></span></td></tr></tbody></table>
<p><b>RSS</b> (<b><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary</b> or <b>Really Simple Syndication</b>)<sup id="cite_ref-powers-2003-1_2-0"><a href="#cite_note-powers-2003-1-2">[2]</a></sup> is a <a href="https://en.wikipedia.org/wiki/Web_feed" title="Web feed">web feed</a><sup id="cite_ref-Netsc99_3-0"><a href="#cite_note-Netsc99-3">[3]</a></sup> that allows users and applications to access updates to websites in a <a href="https://en.wikipedia.org/wiki/Standardization" title="Standardization">standardized</a>, computer-readable format. Subscribing to RSS feeds can allow a user to keep track of many different websites in a single <a href="https://en.wikipedia.org/wiki/News_aggregator" title="News aggregator">news aggregator</a>, which constantly monitor sites for new content, removing the need for the user to manually check them. News aggregators (or "RSS readers") can be built into a <a href="https://en.wikipedia.org/wiki/Web_application" title="Web application">browser</a>, installed on a <a href="https://en.wikipedia.org/wiki/Application_software" title="Application software">desktop computer</a>, or installed on a <a href="https://en.wikipedia.org/wiki/Mobile_app" title="Mobile app">mobile device</a>.
</p><p>Websites usually use RSS feeds to publish frequently updated information, such as <a href="https://en.wikipedia.org/wiki/Blog" title="Blog">blog</a> entries, news headlines, episodes of audio and video series, or for distributing <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasts</a>. An RSS document (called "feed", "web feed",<sup id="cite_ref-GuardWF_4-0"><a href="#cite_note-GuardWF-4">[4]</a></sup> or "channel") includes full or summarized text, and <a href="https://en.wikipedia.org/wiki/Metadata" title="Metadata">metadata</a>, like publishing date and author's name. RSS formats are specified using a generic <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a> file.
</p><p>Although RSS formats have evolved from as early as March 1999,<sup id="cite_ref-Qstart_5-0"><a href="#cite_note-Qstart-5">[5]</a></sup> it was between 2005 and 2006 when RSS gained widespread use, and the ("<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>") icon was decided upon by several major web browsers. RSS feed data is presented to users using software called a news aggregator and the passing of content is called <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a>. Users subscribe to feeds either by entering a feed's <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" title="Uniform Resource Identifier">URI</a> into the reader or by clicking on the browser's <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">feed icon</a>. The RSS reader checks the user's feeds regularly for new information and can automatically download it, if that function is enabled.
</p>
<meta property="mw:PageProp/toc">
<h2><span id="History">History</span></h2>
<table role="presentation"><tbody><tr><td><p><span typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/42px-Ambox_current_red.svg.png" decoding="async" width="42" height="34" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/63px-Ambox_current_red.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/84px-Ambox_current_red.svg.png 2x" data-file-width="360" data-file-height="290"></span></span></p></td><td><p>This section needs to be <b>updated</b>.<span> Please help update this article to reflect recent events or newly available information.</span>  <span><i>(<span>October 2013</span>)</i></span></p></td></tr></tbody></table>

<p>The RSS formats were preceded by several attempts at <a href="https://en.wikipedia.org/wiki/Web_syndication" title="Web syndication">web syndication</a> that did not achieve widespread popularity. The basic idea of restructuring information about websites goes back to as early as 1995, when <a href="https://en.wikipedia.org/wiki/Ramanathan_V._Guha" title="Ramanathan V. Guha">Ramanathan V. Guha</a> and others in <a href="https://en.wikipedia.org/wiki/Apple_Inc." title="Apple Inc.">Apple</a>'s <a href="https://en.wikipedia.org/wiki/Apple_Advanced_Technology_Group" title="Apple Advanced Technology Group">Advanced Technology Group</a> developed the <a href="https://en.wikipedia.org/wiki/Meta_Content_Framework" title="Meta Content Framework">Meta Content Framework</a>.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Resource_Description_Framework" title="Resource Description Framework">RDF</a> Site Summary, the first version of RSS, was created by <a href="https://en.wikipedia.org/w/index.php?title=Dan_Libby&amp;action=edit&amp;redlink=1" title="Dan Libby (page does not exist)">Dan Libby</a> and Ramanathan V. Guha at <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape</a>. It was released in March 1999 for use on the My.Netscape.Com portal.<sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup> This version became known as RSS 0.9.<sup id="cite_ref-Qstart_5-1"><a href="#cite_note-Qstart-5">[5]</a></sup> In July 1999, Dan Libby of Netscape produced a new version, RSS 0.91,<sup id="cite_ref-Netsc99_3-1"><a href="#cite_note-Netsc99-3">[3]</a></sup> which simplified the format by removing RDF elements and incorporating elements from <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>'s news syndication format.<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> Libby also renamed the format from RDF to RSS <b>Rich Site Summary</b> and outlined further development of the format in a "futures document".<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>
</p><p>This would be Netscape's last participation in RSS development for eight years. As RSS was being embraced by web publishers who wanted their feeds to be used on My.Netscape.Com and other early RSS portals, Netscape dropped RSS support from My.Netscape.Com in April 2001 during new owner <a href="https://en.wikipedia.org/wiki/AOL" title="AOL">AOL</a>'s restructuring of the company, also removing documentation and tools that supported the format.<sup id="cite_ref-11"><a href="#cite_note-11">[11]</a></sup>
</p><p>Two parties emerged to fill the void, with neither Netscape's help nor approval: The <a href="https://en.wikipedia.org/wiki/RSS-DEV_Working_Group" title="RSS-DEV Working Group">RSS-DEV Working Group</a> and Dave Winer, whose <a href="https://en.wikipedia.org/wiki/UserLand_Software" title="UserLand Software">UserLand Software</a> had published some of the first publishing tools outside Netscape that could read and write RSS.
</p><p>Winer published a modified version of the RSS 0.91 specification on the UserLand website, covering how it was being used in his company's products, and claimed copyright to the document.<sup id="cite_ref-12"><a href="#cite_note-12">[12]</a></sup> A few months later, UserLand filed a U.S. trademark registration for RSS, but failed to respond to a <a href="https://en.wikipedia.org/wiki/United_States_Patent_and_Trademark_Office" title="United States Patent and Trademark Office">USPTO</a> trademark examiner's request and the request was rejected in December 2001.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>The RSS-DEV Working Group, a project whose members included <a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a>,<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup> Guha and representatives of <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly Media</a> and <a href="https://en.wikipedia.org/wiki/Moreover_Technologies" title="Moreover Technologies">Moreover</a>, produced RSS 1.0 in December 2000.<sup id="cite_ref-15"><a href="#cite_note-15">[15]</a></sup> This new version, which reclaimed the name RDF Site Summary from RSS 0.9, reintroduced support for RDF and added <a href="https://en.wikipedia.org/wiki/XML_namespace" title="XML namespace">XML namespaces</a> support, adopting elements from standard metadata vocabularies such as <a href="https://en.wikipedia.org/wiki/Dublin_Core" title="Dublin Core">Dublin Core</a>.
</p><p>In December 2000, Winer released RSS 0.92<sup id="cite_ref-16"><a href="#cite_note-16">[16]</a></sup>
a minor set of changes aside from the introduction of the enclosure element, which permitted audio files to be carried in RSS feeds and helped spark <a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">podcasting</a>. He also released drafts of RSS 0.93 and RSS 0.94 that were subsequently withdrawn.<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup>
</p><p>In September 2002, Winer released a major new version of the format, RSS 2.0, that redubbed its initials Really Simple Syndication. RSS 2.0 removed the <i>type</i> attribute added in the RSS 0.94 draft and added support for namespaces. To preserve backward compatibility with RSS 0.92, namespace support applies only to other content included within an RSS 2.0 feed, not the RSS 2.0 elements themselves.<sup id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup> (Although other standards such as <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> attempt to correct this limitation, RSS feeds are not aggregated with other content often enough to shift the popularity from RSS to other formats having full namespace support.)
</p><p>Because neither Winer nor the RSS-DEV Working Group had Netscape's involvement, they could not make an official claim on the RSS name or format. This has fueled ongoing controversy<sup>[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources"><span title="Statement needs to be more specific about the content to which it refers. (September 2016)">specify</span></a></i>]</sup> in the syndication development community as to which entity was the proper publisher of RSS.
</p><p>One product of that contentious debate was the creation of an alternative syndication format, Atom, that began in June 2003.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> The Atom syndication format, whose creation was in part motivated by a desire to get a clean start free of the issues surrounding RSS, has been adopted as <a href="https://en.wikipedia.org/wiki/IETF" title="IETF">IETF</a> Proposed Standard <a href="https://en.wikipedia.org/wiki/RFC_(identifier)" title="RFC (identifier)">RFC</a>&nbsp;<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc4287">4287</a>.
</p><p>In July 2003, Winer and UserLand Software assigned the copyright of the RSS 2.0 specification to Harvard's <a href="https://en.wikipedia.org/wiki/Berkman_Klein_Center_for_Internet_%26_Society" title="Berkman Klein Center for Internet &amp; Society">Berkman Klein Center for Internet &amp; Society</a>, where he had just begun a term as a visiting fellow.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> At the same time, Winer launched the <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a> with <a href="https://en.wikipedia.org/wiki/NetNewsWire" title="NetNewsWire">Brent Simmons</a> and <a href="https://en.wikipedia.org/wiki/Jon_Udell" title="Jon Udell">Jon Udell</a>, a group whose purpose was to maintain and publish the specification and answer questions about the format.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>
</p><p>In September 2004, Stephen Horlander created the now ubiquitous <a href="https://en.wikipedia.org/wiki/Web_feed#Feed_icon" title="Web feed">RSS icon</a> (<span typeof="mw:File"><a href="https://en.wikipedia.org/wiki/File:Feed-icon.svg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/16px-Feed-icon.svg.png" decoding="async" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/24px-Feed-icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/43/Feed-icon.svg/32px-Feed-icon.svg.png 2x" data-file-width="128" data-file-height="128"></a></span>) for use in the <a href="https://en.wikipedia.org/wiki/Mozilla" title="Mozilla">Mozilla</a> <a href="https://en.wikipedia.org/wiki/Firefox" title="Firefox">Firefox</a> <a href="https://en.wikipedia.org/wiki/Web_Browser" title="Web Browser">browser</a>.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup>
</p><p>In December 2005, the Microsoft Internet Explorer team and
<a href="https://en.wikipedia.org/wiki/Microsoft_Outlook" title="Microsoft Outlook">Microsoft Outlook</a> team<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> announced on their blogs that they were adopting Firefox's RSS icon. In February 2006, <a href="https://en.wikipedia.org/wiki/Opera_Software" title="Opera Software">Opera Software</a> followed suit.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup> This effectively made the orange square with white radio waves the industry standard for RSS and Atom feeds, replacing the large variety of icons and text that had been used previously to identify syndication data.
</p><p>In January 2006, <a href="https://en.wikipedia.org/wiki/Rogers_Cadenhead" title="Rogers Cadenhead">Rogers Cadenhead</a> relaunched the RSS Advisory Board without Dave Winer's participation, with a stated desire to continue the development of the RSS format and resolve ambiguities. In June 2007, the board revised their version of the specification to confirm that namespaces may extend core elements with namespace attributes, as Microsoft has done in Internet Explorer 7. According to their view, a difference of interpretation left publishers unsure of whether this was permitted or forbidden.
</p>
<h2><span id="Example">Example</span></h2>
<p>RSS is <a href="https://en.wikipedia.org/wiki/XML" title="XML">XML</a>-formatted plain text. The RSS format itself is relatively easy to read both by automated processes and by humans alike. An example feed could have contents such as the following:
</p>
<div dir="ltr"><pre><span></span><span>&lt;?xml version="1.0" encoding="UTF-8"&nbsp;?&gt;</span>
<span>&lt;rss</span><span> </span><span>version=</span><span>"2.0"</span><span>&gt;</span>
<span>&lt;channel&gt;</span>
<span> </span><span>&lt;title&gt;</span>RSS<span> </span>Title<span>&lt;/title&gt;</span>
<span> </span><span>&lt;description&gt;</span>This<span> </span>is<span> </span>an<span> </span>example<span> </span>of<span> </span>an<span> </span>RSS<span> </span>feed<span>&lt;/description&gt;</span>
<span> </span><span>&lt;link&gt;</span>http://www.example.com/main.html<span>&lt;/link&gt;</span>
<span> </span><span>&lt;copyright&gt;</span>2020<span> </span>Example.com<span> </span>All<span> </span>rights<span> </span>reserved<span>&lt;/copyright&gt;</span>
<span> </span><span>&lt;lastBuildDate&gt;</span>Mon,<span> </span>6<span> </span>Sep<span> </span>2010<span> </span>00:01:00<span> </span>+0000<span>&lt;/lastBuildDate&gt;</span>
<span> </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;ttl&gt;</span>1800<span>&lt;/ttl&gt;</span>

<span> </span><span>&lt;item&gt;</span>
<span>  </span><span>&lt;title&gt;</span>Example<span> </span>entry<span>&lt;/title&gt;</span>
<span>  </span><span>&lt;description&gt;</span>Here<span> </span>is<span> </span>some<span> </span>text<span> </span>containing<span> </span>an<span> </span>interesting<span> </span>description.<span>&lt;/description&gt;</span>
<span>  </span><span>&lt;link&gt;</span>http://www.example.com/blog/post/1<span>&lt;/link&gt;</span>
<span>  </span><span>&lt;guid</span><span> </span><span>isPermaLink=</span><span>"false"</span><span>&gt;</span>7bd204c6-1655-4c27-aeee-53f933c5395f<span>&lt;/guid&gt;</span>
<span>  </span><span>&lt;pubDate&gt;</span>Sun,<span> </span>6<span> </span>Sep<span> </span>2009<span> </span>16:20:00<span> </span>+0000<span>&lt;/pubDate&gt;</span>
<span> </span><span>&lt;/item&gt;</span>

<span>&lt;/channel&gt;</span>
<span>&lt;/rss&gt;</span>
</pre></div>
<h3><span id="Aggregators">Aggregators</span></h3>

<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Tiny_Tiny_RSS_English_Interface.png"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/330px-Tiny_Tiny_RSS_English_Interface.png" decoding="async" width="330" height="186" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/495px-Tiny_Tiny_RSS_English_Interface.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tiny_Tiny_RSS_English_Interface.png/660px-Tiny_Tiny_RSS_English_Interface.png 2x" data-file-width="1366" data-file-height="768"></a><figcaption>User interface of an RSS feed reader on a desktop computer</figcaption></figure>
<p>When retrieved, RSS reading software could use the XML structure to present a neat display to the end users. There are various news aggregator software for desktop and mobile devices, but RSS can also be built-in inside <a href="https://en.wikipedia.org/wiki/Web_browser" title="Web browser">web browsers</a> or <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> like <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Mozilla Thunderbird</a>.
</p>
<h2><span id="Variants">Variants</span></h2>
<p>There are several different versions of RSS, falling into two major branches (RDF and 2.*).
</p><p>The RDF (or RSS 1.*) branch includes the following versions:
</p>
<ul><li>RSS 0.90 was the original Netscape RSS version. This RSS was called <i>RDF Site Summary</i>, but was based on an early working draft of the RDF standard, and was not compatible with the final RDF Recommendation.</li>
<li>RSS 1.0 is an open format by the RSS-DEV Working Group, again standing for <i>RDF Site Summary</i>. RSS 1.0 is an RDF format like RSS 0.90, but not fully compatible with it, since 1.0 is based on the final RDF 1.0 Recommendation.</li>
<li>RSS 1.1 is also an open format and is intended to update and replace RSS 1.0. The specification is an independent draft not supported or endorsed in any way by the RSS-Dev Working Group or any other organization.</li></ul>
<p>The RSS 2.* branch (initially UserLand, now Harvard) includes the following versions:
</p>
<ul><li>RSS 0.91 is the simplified RSS version released by Netscape, and also the version number of the simplified version originally championed by Dave Winer from Userland Software. The Netscape version was now called <i>Rich Site Summary</i>; this was no longer an RDF format, but was relatively easy to use.</li>
<li>RSS 0.92 through 0.94 are expansions of the RSS 0.91 format, which are mostly compatible with each other and with Winer's version of RSS 0.91, but are not compatible with RSS 0.90.</li>
<li>RSS 2.0.1 has the internal version number 2.0. RSS 2.0.1 was proclaimed to be "frozen", but still updated shortly after release without changing the version number.  RSS now stood for <i>Really Simple Syndication</i>.  The major change in this version is an explicit extension mechanism using XML namespaces.<sup id="cite_ref-W3C_REC_XML_Namespace_25-0"><a href="#cite_note-W3C_REC_XML_Namespace-25">[25]</a></sup></li></ul>
<p>Later versions in each branch are <a href="https://en.wikipedia.org/wiki/Backward_compatibility" title="Backward compatibility">backward-compatible</a> with earlier versions (aside from non-conformant RDF syntax in 0.90), and both versions include properly documented extension mechanisms using XML Namespaces, either directly (in the 2.* branch) or through RDF (in the 1.* branch).  Most syndication software supports both branches. "The Myth of RSS Compatibility", an article written in 2004 by RSS critic and <a href="https://en.wikipedia.org/wiki/Atom_(standard)" title="Atom (standard)">Atom</a> advocate <a href="https://en.wikipedia.org/wiki/Mark_Pilgrim" title="Mark Pilgrim">Mark Pilgrim</a>, discusses RSS version compatibility issues in more detail.
</p><p>The extension mechanisms make it possible for each branch to copy innovations in the other. For example, the RSS 2.* branch was the first to support <a href="https://en.wikipedia.org/wiki/RSS_enclosure" title="RSS enclosure">enclosures</a>, making it the current leading choice for podcasting, and as of 2005 is the format supported for that use by <a href="https://en.wikipedia.org/wiki/ITunes" title="ITunes">iTunes</a> and other podcasting software; however, an enclosure extension is now available for the RSS 1.* branch, mod_enclosure.  Likewise, the RSS 2.* core specification does not support providing full-text in addition to a synopsis, but the RSS 1.* markup can be (and often is) used as an extension.  There are also several common outside extension packages available, e.g. one  from <a href="https://en.wikipedia.org/wiki/Microsoft" title="Microsoft">Microsoft</a> for use in <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> 7.
</p><p>The most serious compatibility problem is with HTML markup. Userland's RSS readerâ€”generally considered as the reference implementationâ€”did not originally filter out <a href="https://en.wikipedia.org/wiki/HTML" title="HTML">HTML</a> markup from feeds. As a result, publishers began placing HTML markup into the titles and descriptions of items in their RSS feeds. This behavior has become expected of readers, to the point of becoming a <a href="https://en.wikipedia.org/wiki/De_facto" title="De facto">de facto</a> standard.<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> Though there is still some inconsistency in how software handles this markup, particularly in titles. The RSS 2.0 specification was later updated to include examples of entity-encoded HTML; however, all prior plain text usages remain valid.
</p><p>As of January&nbsp;2007, tracking data from www.syndic8.com indicates that the three main versions of RSS in current use are 0.91, 1.0, and 2.0, constituting 13%, 17%, and 67% of worldwide RSS usage, respectively.<sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup> These figures, however, do not include usage of the rival web feed format Atom. As of August&nbsp;2008, the syndic8.com website is indexing 546,069 total feeds, of which 86,496 (16%) were some dialect of Atom and 438,102 were some dialect of RSS.<sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup>
</p>
<h2><span id="Modules">Modules</span></h2>
<p>The primary objective of all RSS modules is to extend the basic XML schema established for more robust syndication of content. This inherently allows for more diverse, yet standardized, transactions without modifying the core RSS specification.
</p><p>To accomplish this extension, a tightly controlled vocabulary (in the RSS world, "module"; in the XML world, "schema") is declared through an XML namespace to give names to concepts and relationships between those concepts.
</p><p>Some RSS 2.0 modules with established namespaces are:
</p>
<ul><li><a href="https://en.wikipedia.org/wiki/Media_RSS" title="Media RSS">Media RSS</a> (MRSS) 2.0 Module</li>
<li><a rel="nofollow" href="http://www.opensearch.org/Specifications/OpenSearch/1.1">OpenSearch RSS 2.0 Module</a></li></ul>
<h2><span id="Interoperability">Interoperability</span></h2>
<p>Although the number of items in an RSS channel is theoretically unlimited, some <a href="https://en.wikipedia.org/wiki/News_aggregators" title="News aggregators">news aggregators</a> do not support RSS files larger than 150KB. For example, applications that rely on the Common Feed List of <a href="https://en.wikipedia.org/wiki/Microsoft_Windows" title="Microsoft Windows">Windows</a> might handle such files as if they were corrupt, and not open them. <a href="https://en.wikipedia.org/wiki/Interoperability" title="Interoperability">Interoperability</a> can be maximized by keeping the file size under this limit.
</p><p><a href="https://en.wikipedia.org/wiki/Podcast" title="Podcast">Podcasts</a> are distributed using RSS. To listen to a podcast, a user adds the RSS feed to their podcast client, and the client can then list available episodes and download or stream them for listening or viewing. To be included in a podcast directory the feed must for each episode provide a title, description, artwork, category, language, and explicit rating. There are some services that specifically indexes and is a <a href="https://en.wikipedia.org/wiki/Search_engine" title="Search engine">search engine</a> for podcasts.<sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup>
</p><p>Some <a href="https://en.wikipedia.org/wiki/BitTorrent" title="BitTorrent">BitTorrent</a> clients support RSS. RSS feeds which provide links to .torrent files allow users to <a href="https://en.wikipedia.org/wiki/Broadcatching" title="Broadcatching">subscribe and automatically download</a> content as soon as it is published.
</p>
<h3></h3>

<p>Some services deliver RSS to an email inbox, sending updates from user's personal selection and schedules. Examples of such services include <a href="https://en.wikipedia.org/wiki/IFTTT" title="IFTTT">IFTTT</a>, <a href="https://en.wikipedia.org/wiki/Zapier" title="Zapier">Zapier</a> and others.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup> Conversely, some services deliver email to RSS readers.<sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> Further services like e. g. <a href="https://en.wikipedia.org/wiki/Gmane" title="Gmane">Gmane</a> allow to subscribe to feeds via <a href="https://en.wikipedia.org/wiki/NNTP" title="NNTP">NNTP</a>.
</p><p>It may be noted that <a href="https://en.wikipedia.org/wiki/Email_client" title="Email client">email clients</a> such as <a href="https://en.wikipedia.org/wiki/Mozilla_Thunderbird" title="Mozilla Thunderbird">Thunderbird</a> supports RSS natively.<sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p>
<h2></h2>
<p>Both RSS and <a href="https://en.wikipedia.org/wiki/Atom_(web_standard)" title="Atom (web standard)">Atom</a> are widely supported and are compatible with all major consumer feed readers. RSS gained wider use because of early feed reader support. Technically, Atom has several advantages: less restrictive licensing, <a href="https://en.wikipedia.org/wiki/Internet_Assigned_Numbers_Authority" title="Internet Assigned Numbers Authority">IANA</a>-registered <a href="https://en.wikipedia.org/wiki/MIME_type" title="MIME type">MIME type</a>, XML namespace, <a href="https://en.wikipedia.org/wiki/URI" title="URI">URI</a> support, <a href="https://en.wikipedia.org/wiki/RELAX_NG" title="RELAX NG">RELAX NG</a> support.<sup id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup>
</p><p>The following table shows RSS elements alongside Atom elements where they are equivalent.
</p><p>Note: the <a href="https://en.wikipedia.org/wiki/Asterisk" title="Asterisk">asterisk</a> character (*) indicates that an element must be provided (Atom elements "author" and "link" are only required under certain conditions).
</p>
<table>

<tbody><tr>
<th scope="col">RSS 2.0
</th>
<th scope="col">Atom 1.0
</th></tr>
<tr>
<td><code>author</code>
</td>
<td><code>author</code>*
</td></tr>
<tr>
<td><code>category</code>
</td>
<td><code>category</code>
</td></tr>
<tr>
<td><code>channel</code>
</td>
<td><code>feed</code>
</td></tr>
<tr>
<td><code>copyright</code>
</td>
<td><code>rights</code>
</td></tr>
<tr>
<td>â€”
</td>
<td><code>subtitle</code>
</td></tr>
<tr>
<td><code>description</code>*
</td>
<td><code>summary</code> and/or <code>content</code>
</td></tr>
<tr>
<td><code>generator</code>
</td>
<td><code>generator</code>
</td></tr>
<tr>
<td><code>guid</code>
</td>
<td><code>id</code>*
</td></tr>
<tr>
<td><code>image</code>
</td>
<td><code>logo</code>
</td></tr>
<tr>
<td><code>item</code>
</td>
<td><code>entry</code>
</td></tr>
<tr>
<td><code>lastBuildDate</code> (in <code>channel</code>)
</td>
<td><code>updated</code>*
</td></tr>
<tr>
<td><code>link</code>*
</td>
<td><code>link</code>*
</td></tr>
<tr>
<td><code>managingEditor</code>
</td>
<td><code>author</code> or <code>contributor</code>
</td></tr>
<tr>
<td><code>pubDate</code>
</td>
<td><code>published</code> (subelement of <code>entry</code>)
</td></tr>
<tr>
<td><code>title</code>*
</td>
<td><code>title</code>*
</td></tr>
<tr>
<td><code><a href="https://en.wikipedia.org/wiki/Time_to_live" title="Time to live">ttl</a></code>
</td>
<td>â€”
</td></tr></tbody></table>
<h2><span id="Current_usage">Current usage</span></h2>
<p>Several major sites such as <a href="https://en.wikipedia.org/wiki/Facebook" title="Facebook">Facebook</a> and <a href="https://en.wikipedia.org/wiki/Twitter" title="Twitter">Twitter</a> previously offered RSS feeds but have reduced or removed support. Additionally, widely used readers such as <a href="https://en.wikipedia.org/wiki/Shiira" title="Shiira">Shiira</a>, FeedDemon, and particularly <a href="https://en.wikipedia.org/wiki/Google_Reader" title="Google Reader">Google Reader</a>, have all been discontinued as of 2013, citing declining popularity in RSS.<sup id="cite_ref-ClosureAnnouncement_34-0"><a href="#cite_note-ClosureAnnouncement-34">[34]</a></sup> RSS support was removed in <a href="https://en.wikipedia.org/wiki/OS_X_Mountain_Lion" title="OS X Mountain Lion">OS X Mountain Lion</a>'s versions of <a href="https://en.wikipedia.org/wiki/Apple_Mail" title="Apple Mail">Mail</a> and <a href="https://en.wikipedia.org/wiki/Safari_(web_browser)" title="Safari (web browser)">Safari</a>, although the features were partially restored in Safari 8.<sup id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup> Mozilla removed RSS support from <a href="https://en.wikipedia.org/wiki/Mozilla_Firefox" title="Mozilla Firefox">Mozilla Firefox</a> version 64.0, joining <a href="https://en.wikipedia.org/wiki/Google_Chrome" title="Google Chrome">Google Chrome</a> and <a href="https://en.wikipedia.org/wiki/Microsoft_Edge" title="Microsoft Edge">Microsoft Edge</a> which do not include RSS support, thus leaving <a href="https://en.wikipedia.org/wiki/Internet_Explorer" title="Internet Explorer">Internet Explorer</a> as the last major browser to include RSS support by default.<sup id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup><sup id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup>
</p><p>Since the late 2010s there has been an uptick in RSS interest again. In 2018, <i><a href="https://en.wikipedia.org/wiki/Wired_(magazine)" title="Wired (magazine)">Wired</a></i> published an article named "It's Time for an RSS Revival", citing that RSS gives more control over content compared to algorithms and trackers from social media sites. At that time, <a href="https://en.wikipedia.org/wiki/Feedly" title="Feedly">Feedly</a> was the most popular RSS reader.<sup id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup> Chrome on <a href="https://en.wikipedia.org/wiki/Android_(operating_system)" title="Android (operating system)">Android</a> has added the ability to follow RSS feeds as of 2021.<sup id="cite_ref-40"><a href="#cite_note-40">[40]</a></sup>
</p>
<h2><span id="See_also">See also</span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/JSON_Feed" title="JSON Feed">JSON Feed</a></li>
<li><a href="https://en.wikipedia.org/wiki/Aaron_Swartz" title="Aaron Swartz">Aaron Swartz</a></li>
<li><a href="https://en.wikipedia.org/wiki/Comparison_of_feed_aggregators" title="Comparison of feed aggregators">Comparison of feed aggregators</a></li>
<li><a href="https://en.wikipedia.org/wiki/Data_portability" title="Data portability">Data portability</a></li>
<li><a href="https://en.wikipedia.org/wiki/FeedSync" title="FeedSync">FeedSync</a> previously Simple Sharing Extensions</li>
<li><a href="https://en.wikipedia.org/wiki/HAtom" title="HAtom">hAtom</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mashup_(web_application_hybrid)" title="Mashup (web application hybrid)">Mashup (web application hybrid)</a></li>
<li><a href="https://en.wikipedia.org/wiki/WebSub" title="WebSub">WebSub</a></li></ul>
<h2><span id="Notes">Notes</span></h2>
<div>
<ul><li><cite id="CITEREFPowers2003"><a href="https://en.wikipedia.org/wiki/Shelley_Powers" title="Shelley Powers">Powers, Shelley</a> (2003). <i>Practical RDF</i>. <a href="https://en.wikipedia.org/wiki/O%27Reilly_Media" title="O'Reilly Media">O'Reilly</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Practical+RDF&amp;rft.pub=O%27Reilly&amp;rft.date=2003&amp;rft.aulast=Powers&amp;rft.aufirst=Shelley&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></li></ul>
</div>
<h2><span id="References">References</span></h2>
<div>
<ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite><a rel="nofollow" href="https://tools.ietf.org/id/draft-nottingham-rss-media-type-00">"The application/rss+xml Media Type"</a>. Network Working Group. May 22, 2006. <a rel="nofollow" href="https://web.archive.org/web/20220614140253/https://tools.ietf.org/id/draft-nottingham-rss-media-type-00.txt">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>August 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+application%2Frss%2Bxml+Media+Type&amp;rft.pub=Network+Working+Group&amp;rft.date=2006-05-22&amp;rft_id=https%3A%2F%2Ftools.ietf.org%2Fid%2Fdraft-nottingham-rss-media-type-00&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-powers-2003-1-2"><span><b><a href="#cite_ref-powers-2003-1_2-0">^</a></b></span> <span><a href="#CITEREFPowers2003">Powers 2003</a>, p.&nbsp;10: "Another very common use of RDF/XML is in a version of RSS called RSS 1.0 or RDF/RSS. The meaning of the RSS abbreviation has changed over the years, but the basic premise behind it is to provide an XML-formatted feed consisting of an abstract of content and a link to a document containing the full content. When Netscape originally created the first implementation of an RSS specification, RSS stood for RDF Site Summary, and the plan was to use RDF/XML. When the company released, instead, a non-RDF XML version of the specification, RSS stood for Rich Site Summary. Recently, there has been increased activity with RSS, and two paths are emerging: one considers RSS to stand for Really Simple Syndication, a simple XML solution (promoted as RSS 2.0 by Dave Winer at Userland), and one returns RSS to its original roots of RDF Site Summary (RSS 1.0 by the RSS 1.0 Development group)."</span>
</li>
<li id="cite_note-Netsc99-3"><span>^ <a href="#cite_ref-Netsc99_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Netsc99_3-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFLibby,_Dan1999">Libby, Dan (July 10, 1999). <a rel="nofollow" href="https://web.archive.org/web/20001204093600/http://my.netscape.com/publish/formats/rss-spec-0.91.html">"RSS 0.91 Spec, revision 3"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape ttem</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/formats/rss-spec-0.91.html">the original</a> on December 4, 2000<span>. Retrieved <span>February 14,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91+Spec%2C+revision+3&amp;rft.pub=Netscape+ttem&amp;rft.date=1999-07-10&amp;rft.au=Libby%2C+Dan&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fformats%2Frss-spec-0.91.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-GuardWF-4"><span><b><a href="#cite_ref-GuardWF_4-0">^</a></b></span> <span>"Web feeds | RSS | The Guardian | guardian.co.uk",
  <i>The Guardian</i>, London, 2008, webpage:
  <a rel="nofollow" href="https://www.theguardian.com/help/feeds">GuardianUK-webfeeds</a>. <a rel="nofollow" href="https://web.archive.org/web/20171215111443/https://www.theguardian.com/help/feeds">Archived</a> December 15, 2017, at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>.</span>
</li>
<li id="cite_note-Qstart-5"><span>^ <a href="#cite_ref-Qstart_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Qstart_5-1"><sup><i><b>b</b></i></sup></a></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001208063100/http://my.netscape.com/publish/help/quickstart.html">"My Netscape Network: Quick Start"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/quickstart.html">the original</a> on December 8, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=My+Netscape+Network%3A+Quick+Start&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Fquickstart.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><cite id="CITEREFLash,_Alex1997">Lash, Alex (October 3, 1997). <a rel="nofollow" href="https://web.archive.org/web/20110809151456/http://news.cnet.com/2100-1001-203893.html">"W3C takes first step toward RDF spec"</a>. Archived from <a rel="nofollow" href="http://news.cnet.com/2100-1001-203893.html">the original</a> on August 9, 2011<span>. Retrieved <span>February 16,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=W3C+takes+first+step+toward+RDF+spec&amp;rft.date=1997-10-03&amp;rft.au=Lash%2C+Alex&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2F2100-1001-203893.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><cite id="CITEREFHines1999">Hines, Matt (March 15, 1999). "Netscape Broadens Portal Content Strategy". <i>Newsbytes</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Newsbytes&amp;rft.atitle=Netscape+Broadens+Portal+Content+Strategy&amp;rft.date=1999-03-15&amp;rft.aulast=Hines&amp;rft.aufirst=Matt&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+History&amp;rft.date=2007-06-07&amp;rft.au=RSS+Advisory+Board&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Frss-history&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20001204123600/http://my.netscape.com/publish/help/futures.html">"MNN Future Directions"</a>. <a href="https://en.wikipedia.org/wiki/Netscape" title="Netscape">Netscape Communications</a>. Archived from <a rel="nofollow" href="http://my.netscape.com/publish/help/futures.html">the original</a> on December 4, 2000<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=MNN+Future+Directions&amp;rft.pub=Netscape+Communications&amp;rft_id=http%3A%2F%2Fmy.netscape.com%2Fpublish%2Fhelp%2Ffutures.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-11"><span><b><a href="#cite_ref-11">^</a></b></span> <span><cite id="CITEREFAndrew_King2003">Andrew King (April 13, 2003). <a rel="nofollow" href="https://web.archive.org/web/20070119031128/http://www.webreference.com/authoring/languages/xml/rss/1/">"The Evolution of RSS"</a>. Archived from <a rel="nofollow" href="http://www.webreference.com/authoring/languages/xml/rss/1/">the original</a> on January 19, 2007<span>. Retrieved <span>January 17,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Evolution+of+RSS&amp;rft.date=2003-04-13&amp;rft.au=Andrew+King&amp;rft_id=http%3A%2F%2Fwww.webreference.com%2Fauthoring%2Flanguages%2Fxml%2Frss%2F1%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-12"><span><b><a href="#cite_ref-12">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (June 4, 2000). <a rel="nofollow" href="https://web.archive.org/web/20061110001520/http://backend.userland.com/rss091#copyrightAndDisclaimer">"RSS 0.91: Copyright and Disclaimer"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss091#copyrightAndDisclaimer">the original</a> on November 10, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.91%3A+Copyright+and+Disclaimer&amp;rft.pub=UserLand+Software&amp;rft.date=2000-06-04&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss091%23copyrightAndDisclaimer&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span><cite id="CITEREFU.S._Patent_&amp;_Trademark_Office">U.S. Patent &amp; Trademark Office. <a rel="nofollow" href="http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">"<span></span>'RSS' Trademark Latest Status Info"</a>. <a rel="nofollow" href="https://web.archive.org/web/20070816233807/http://tarr.uspto.gov/servlet/tarr?regser=serial&amp;entry=78025336">Archived</a> from the original on August 16, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%27RSS%27+Trademark+Latest+Status+Info&amp;rft.au=U.S.+Patent+%26+Trademark+Office&amp;rft_id=http%3A%2F%2Ftarr.uspto.gov%2Fservlet%2Ftarr%3Fregser%3Dserial%26entry%3D78025336&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">"RSS Creator Aaron Swartz Dead at 26"</a>. <i>Harvard Magazine</i>. January 14, 2013. <a rel="nofollow" href="https://web.archive.org/web/20210629135531/https://www.harvardmagazine.com/2013/01/rss-creator-aaron-swartz-dead-at-26">Archived</a> from the original on June 29, 2021<span>. Retrieved <span>June 29,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Harvard+Magazine&amp;rft.atitle=RSS+Creator+Aaron+Swartz+Dead+at+26&amp;rft.date=2013-01-14&amp;rft_id=https%3A%2F%2Fwww.harvardmagazine.com%2F2013%2F01%2Frss-creator-aaron-swartz-dead-at-26&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-15"><span><b><a href="#cite_ref-15">^</a></b></span> <span><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RDF+Site+Summary+%28RSS%29+1.0&amp;rft.date=2000-12-09&amp;rft.au=RSS-DEV+Working+Group&amp;rft_id=http%3A%2F%2Fweb.resource.org%2Frss%2F1.0%2Fspec&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-16"><span><b><a href="#cite_ref-16">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2000">Winer, Dave (December 25, 2000). <a rel="nofollow" href="https://web.archive.org/web/20110131184230/http://backend.userland.com/rss092">"RSS 0.92 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss092">the original</a> on January 31, 2011<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.92+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2000-12-25&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss092&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFWiner,_Dave2001">Winer, Dave (April 20, 2001). <a rel="nofollow" href="https://web.archive.org/web/20061102171227/http://backend.userland.com/rss093">"RSS 0.93 Specification"</a>. UserLand Software. Archived from <a rel="nofollow" href="http://backend.userland.com/rss093">the original</a> on November 2, 2006<span>. Retrieved <span>October 31,</span> 2006</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+0.93+Specification&amp;rft.pub=UserLand+Software&amp;rft.date=2001-04-20&amp;rft.au=Winer%2C+Dave&amp;rft_id=http%3A%2F%2Fbackend.userland.com%2Frss093&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-18"><span><b><a href="#cite_ref-18">^</a></b></span> <span><cite id="CITEREFHarvard_Law2007">Harvard Law (April 14, 2007). <a rel="nofollow" href="http://cyber.law.harvard.edu/rss/toplevelNamespace.html">"Top-level namespaces"</a>. <a rel="nofollow" href="https://web.archive.org/web/20110605164517/http://cyber.law.harvard.edu/rss/toplevelNamespace.html">Archived</a> from the original on June 5, 2011<span>. Retrieved <span>August 3,</span> 2009</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Top-level+namespaces&amp;rft.date=2007-04-14&amp;rft.au=Harvard+Law&amp;rft_id=http%3A%2F%2Fcyber.law.harvard.edu%2Frss%2FtoplevelNamespace.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><cite id="CITEREFFesta2003">Festa, Paul (August 4, 2003). <a rel="nofollow" href="http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">"Dispute exposes bitter power struggle behind Web logs"</a>. news.cnet.com. <a rel="nofollow" href="https://web.archive.org/web/20090806234534/http://news.cnet.com/Battle-of-the-blog/2009-1032_3-5059006.html">Archived</a> from the original on August 6, 2009<span>. Retrieved <span>August 6,</span> 2008</span>. <q>The conflict centers on something called Really Simple Syndication (RSS), a technology widely used to syndicate blogs and other Web content. The dispute pits Harvard Law School fellow Dave Winer, the blogging pioneer who is the key gatekeeper of RSS, against advocates of a different format.</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Dispute+exposes+bitter+power+struggle+behind+Web+logs&amp;rft.pub=news.cnet.com&amp;rft.date=2003-08-04&amp;rft.aulast=Festa&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fnews.cnet.com%2FBattle-of-the-blog%2F2009-1032_3-5059006.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.rssboard.org/advisory-board-notes">"Advisory Board Notes"</a>. <a href="https://en.wikipedia.org/wiki/RSS_Advisory_Board" title="RSS Advisory Board">RSS Advisory Board</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070927051743/http://www.rssboard.org/advisory-board-notes">Archived</a> from the original on September 27, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Advisory+Board+Notes&amp;rft.pub=RSS+Advisory+Board&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.rssboard.org%2Fadvisory-board-notes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.scripting.com/2003/07/18.html#rss20News">"RSS 2.0 News"</a>. <i>Scripting News</i>. <a href="https://en.wikipedia.org/wiki/Dave_Winer" title="Dave Winer">Dave Winer</a>. July 18, 2003. <a rel="nofollow" href="https://web.archive.org/web/20070822014007/http://www.scripting.com/2003/07/18.html#rss20News">Archived</a> from the original on August 22, 2007<span>. Retrieved <span>September 4,</span> 2007</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scripting+News&amp;rft.atitle=RSS+2.0+News&amp;rft.date=2003-07-18&amp;rft_id=http%3A%2F%2Fwww.scripting.com%2F2003%2F07%2F18.html%23rss20News&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">"2004-09-26 Branch builds"</a>. <i>The Burning Edge</i>. September 26, 2004. <a rel="nofollow" href="https://web.archive.org/web/20141009071447/http://www.squarefree.com/burningedge/2004/09/26/2004-09-26-branch-builds/">Archived</a> from the original on October 9, 2014<span>. Retrieved <span>October 6,</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Burning+Edge&amp;rft.atitle=2004-09-26+Branch+builds&amp;rft.date=2004-09-26&amp;rft_id=http%3A%2F%2Fwww.squarefree.com%2Fburningedge%2F2004%2F09%2F26%2F2004-09-26-branch-builds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>"<a rel="nofollow" href="https://web.archive.org/web/20051217102644/http://blogs.msdn.com/michael_affronti/archive/2005/12/15/504316.aspx">RSS icon goodness</a>", blog post by Michael A. Affronti of Microsoft (Outlook Program Manager), December 15, 2005</span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span><cite id="CITEREFtrond2006">trond (February 16, 2006). <a rel="nofollow" href="https://web.archive.org/web/20100417170259/http://my.opera.com/desktopteam/blog/show.dml/146296">"Making love to the new feed icon"</a>. Opera Desktop Team. Archived from <a rel="nofollow" href="http://my.opera.com/desktopteam/blog/show.dml/146296">the original</a> on April 17, 2010<span>. Retrieved <span>July 4,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Making+love+to+the+new+feed+icon&amp;rft.pub=Opera+Desktop+Team&amp;rft.date=2006-02-16&amp;rft.au=trond&amp;rft_id=http%3A%2F%2Fmy.opera.com%2Fdesktopteam%2Fblog%2Fshow.dml%2F146296&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-W3C_REC_XML_Namespace-25"><span><b><a href="#cite_ref-W3C_REC_XML_Namespace_25-0">^</a></b></span> <span><cite><a rel="nofollow" href="http://www.w3.org/TR/REC-xml-names/">"Namespaces in XML 1.0"</a> (2nd&nbsp;ed.). W3C. August 16, 2006. <a rel="nofollow" href="https://web.archive.org/web/20110316043909/http://www.w3.org/TR/REC-xml-names/">Archived</a> from the original on March 16, 2011<span>. Retrieved <span>May 22,</span> 2008</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Namespaces+in+XML+1.0&amp;rft.edition=2nd&amp;rft.pub=W3C&amp;rft.date=2006-08-16&amp;rft_id=http%3A%2F%2Fwww.w3.org%2FTR%2FREC-xml-names%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.w3.org/2001/10/glance/doc/howto.html">"W3C RSS 1.0 News Feed Creation How-To"</a>. <i>www.w3.org</i>. <a rel="nofollow" href="https://web.archive.org/web/20220614140126/https://www.w3.org/2001/10/glance/doc/howto.html">Archived</a> from the original on June 14, 2022<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.w3.org&amp;rft.atitle=W3C+RSS+1.0+News+Feed+Creation+How-To&amp;rft_id=https%3A%2F%2Fwww.w3.org%2F2001%2F10%2Fglance%2Fdoc%2Fhowto.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFHolzner">Holzner, Steven. <a rel="nofollow" href="http://www.peachpit.com/articles/article.aspx?p=674690">"Peachpit article"</a>. Peachpit article. <a rel="nofollow" href="https://web.archive.org/web/20111109173320/http://www.peachpit.com/articles/article.aspx?p=674690">Archived</a> from the original on November 9, 2011<span>. Retrieved <span>December 11,</span> 2010</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Peachpit+article&amp;rft.pub=Peachpit+article&amp;rft.aulast=Holzner&amp;rft.aufirst=Steven&amp;rft_id=http%3A%2F%2Fwww.peachpit.com%2Farticles%2Farticle.aspx%3Fp%3D674690&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><cite><a rel="nofollow" href="https://web.archive.org/web/20020803040757/http://www.syndic8.com/stats.php?Section=feeds#tabtable">"Syndic8 stats table"</a>. Syndic8.com. Archived from <a rel="nofollow" href="http://www.syndic8.com/stats.php?Section=feeds#tabtable">the original</a> on August 3, 2002<span>. Retrieved <span>August 12,</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Syndic8+stats+table&amp;rft.pub=Syndic8.com&amp;rft_id=http%3A%2F%2Fwww.syndic8.com%2Fstats.php%3FSection%3Dfeeds%23tabtable&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite><a rel="nofollow" href="https://lifehacker.com/the-best-podcast-search-engine-1818560337">"The Best Podcast Search Engine"</a>. <i>Lifehacker</i>. September 20, 2017. <a rel="nofollow" href="https://web.archive.org/web/20201129195032/https://lifehacker.com/the-best-podcast-search-engine-1818560337">Archived</a> from the original on November 29, 2020<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lifehacker&amp;rft.atitle=The+Best+Podcast+Search+Engine&amp;rft.date=2017-09-20&amp;rft_id=https%3A%2F%2Flifehacker.com%2Fthe-best-podcast-search-engine-1818560337&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite><a rel="nofollow" href="https://blogtrottr.com/">"Free realtime RSS and Atom feed to email service. Get your favourite blogs, feeds, and news delivered to your inbox"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170128081150/http://blogtrottr.com/">Archived</a> from the original on January 28, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Free+realtime+RSS+and+Atom+feed+to+email+service.+Get+your+favourite+blogs%2C+feeds%2C+and+news+delivered+to+your+inbox.&amp;rft_id=https%3A%2F%2Fblogtrottr.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite><a rel="nofollow" href="https://rss.com/">"RSS Feed Reader, your tool for saving time and money at RSS.com"</a>. <a rel="nofollow" href="https://web.archive.org/web/20170125224151/https://www.rss.com/">Archived</a> from the original on January 25, 2017<span>. Retrieved <span>January 26,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=RSS+Feed+Reader%2C+your+tool+for+saving+time+and+money+at+RSS.com&amp;rft_id=https%3A%2F%2Frss.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">"How to use Thunderbird to get RSS feeds! Here's How it Works"</a>. October 17, 2018. <a rel="nofollow" href="https://web.archive.org/web/20210413005112/https://www.uslsoftware.com/how-to-use-thunderbird-to-get-rss-feeds/">Archived</a> from the original on April 13, 2021<span>. Retrieved <span>February 5,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+to+use+Thunderbird+to+get+RSS+feeds%21+Here%27s+How+it+Works&amp;rft.date=2018-10-17&amp;rft_id=https%3A%2F%2Fwww.uslsoftware.com%2Fhow-to-use-thunderbird-to-get-rss-feeds%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-33"><span><b><a href="#cite_ref-33">^</a></b></span> <span><cite id="CITEREFLeslie_Sikos2011">Leslie Sikos (2011). <a rel="nofollow" href="https://web.archive.org/web/20150402152305/http://www.masteringhtml5css3.com/"><i>Web standards â€“ Mastering HTML5, CSS3, and XML</i></a>. <a href="https://en.wikipedia.org/wiki/Apress" title="Apress">Apress</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-4302-4041-9" title="Special:BookSources/978-1-4302-4041-9"><bdi>978-1-4302-4041-9</bdi></a>. Archived from <a rel="nofollow" href="http://www.masteringhtml5css3.com/">the original</a> on April 2, 2015<span>. Retrieved <span>June 14,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Web+standards+%E2%80%93+Mastering+HTML5%2C+CSS3%2C+and+XML&amp;rft.pub=Apress&amp;rft.date=2011&amp;rft.isbn=978-1-4302-4041-9&amp;rft.au=Leslie+Sikos&amp;rft_id=http%3A%2F%2Fwww.masteringhtml5css3.com&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-ClosureAnnouncement-34"><span><b><a href="#cite_ref-ClosureAnnouncement_34-0">^</a></b></span> <span><cite id="CITEREFHÃ¶lzle">HÃ¶lzle, Urs. <a rel="nofollow" href="http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">"A second spring of cleaning"</a>. googleblog.blogspot.com. <a rel="nofollow" href="https://web.archive.org/web/20130314045128/http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">Archived</a> from the original on March 14, 2013<span>. Retrieved <span>March 14,</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+second+spring+of+cleaning&amp;rft.pub=googleblog.blogspot.com&amp;rft.aulast=H%C3%B6lzle&amp;rft.aufirst=Urs&amp;rft_id=http%3A%2F%2Fgoogleblog.blogspot.com%2F2013%2F03%2Fa-second-spring-of-cleaning.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>

<li id="cite_note-36"><span><b><a href="#cite_ref-36">^</a></b></span> <span><cite><a rel="nofollow" href="http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">"Subscribe to RSS Feeds in Safari for OS X Yosemite"</a>. OSX Daily. November 3, 2014. <a rel="nofollow" href="https://web.archive.org/web/20150121185232/http://osxdaily.com/2014/11/03/subscribe-rss-feeds-safari-os-x/">Archived</a> from the original on January 21, 2015<span>. Retrieved <span>January 24,</span> 2015</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Subscribe+to+RSS+Feeds+in+Safari+for+OS+X+Yosemite&amp;rft.pub=OSX+Daily&amp;rft.date=2014-11-03&amp;rft_id=http%3A%2F%2Fosxdaily.com%2F2014%2F11%2F03%2Fsubscribe-rss-feeds-safari-os-x%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-37"><span><b><a href="#cite_ref-37">^</a></b></span> <span><cite id="CITEREFCimpanu2018">Cimpanu, Catalin (July 26, 2018). <a rel="nofollow" href="https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">"Mozilla to Remove Support for Built-In Feed Reader From Firefox"</a>. <i>BleepingComputer</i>. <a rel="nofollow" href="https://web.archive.org/web/20180726144716/https://www.bleepingcomputer.com/news/software/mozilla-to-remove-support-for-built-in-feed-reader-from-firefox/">Archived</a> from the original on July 26, 2018<span>. Retrieved <span>July 26,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BleepingComputer&amp;rft.atitle=Mozilla+to+Remove+Support+for+Built-In+Feed+Reader+From+Firefox&amp;rft.date=2018-07-26&amp;rft.aulast=Cimpanu&amp;rft.aufirst=Catalin&amp;rft_id=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsoftware%2Fmozilla-to-remove-support-for-built-in-feed-reader-from-firefox%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-38"><span><b><a href="#cite_ref-38">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">"Firefox 64.0, See All New Features, Updates and Fixes"</a>. <i>Mozilla</i>. December 11, 2018. <a rel="nofollow" href="https://web.archive.org/web/20181211143259/https://www.mozilla.org/en-US/firefox/64.0/releasenotes/">Archived</a> from the original on December 11, 2018<span>. Retrieved <span>December 12,</span> 2018</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Mozilla&amp;rft.atitle=Firefox+64.0%2C+See+All+New+Features%2C+Updates+and+Fixes&amp;rft.date=2018-12-11&amp;rft_id=https%3A%2F%2Fwww.mozilla.org%2Fen-US%2Ffirefox%2F64.0%2Freleasenotes%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-39"><span><b><a href="#cite_ref-39">^</a></b></span> <span><cite id="CITEREFBarrett2018">Barrett, Brian (March 30, 2018). <a rel="nofollow" href="https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">"It's Time for an RSS Revival"</a>. <i>Wired</i>. <a rel="nofollow" href="https://web.archive.org/web/20210812114050/https://www.wired.com/story/rss-readers-feedly-inoreader-old-reader/">Archived</a> from the original on August 12, 2021<span>. Retrieved <span>July 26,</span> 2021</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft.atitle=It%27s+Time+for+an+RSS+Revival&amp;rft.date=2018-03-30&amp;rft.aulast=Barrett&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwww.wired.com%2Fstory%2Frss-readers-feedly-inoreader-old-reader%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
<li id="cite_note-40"><span><b><a href="#cite_ref-40">^</a></b></span> <span><cite id="CITEREFCampbell2021">Campbell, Ian Carlos (October 8, 2021). <a rel="nofollow" href="https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">"Google Reader is still defunct, but now you can 'follow' RSS feeds in Chrome on Android"</a>. <i>The Verge</i>. <a rel="nofollow" href="https://web.archive.org/web/20220605165318/https://www.theverge.com/2021/10/8/22716813/google-chrome-follow-button-rss-reader">Archived</a> from the original on June 5, 2022<span>. Retrieved <span>June 19,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Verge&amp;rft.atitle=Google+Reader+is+still+defunct%2C+but+now+you+can+%27follow%27+RSS+feeds+in+Chrome+on+Android&amp;rft.date=2021-10-08&amp;rft.aulast=Campbell&amp;rft.aufirst=Ian+Carlos&amp;rft_id=https%3A%2F%2Fwww.theverge.com%2F2021%2F10%2F8%2F22716813%2Fgoogle-chrome-follow-button-rss-reader&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARSS"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span></h2>

<ul><li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-0">RSS 0.90 Specification</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-0-9-1-netscape">RSS 0.91 Specification</a></li>
<li><a rel="nofollow" href="http://web.resource.org/rss/1.0/">RSS 1.0 Specifications</a></li>
<li><a rel="nofollow" href="http://www.rssboard.org/rss-specification">RSS 2.0 Specification</a></li>
<li><a rel="nofollow" href="https://web.archive.org/web/20110718034619/http://diveintomark.org/archives/2002/09/06/history_of_the_rss_fork">History of the RSS Fork</a> (Mark Pilgrim)</li>
<li><a rel="nofollow" href="https://www.xul.fr/en-xml-rss.html">Building an RSS feed</a> Tutorial with example</li></ul>





<!-- 
NewPP limit report
Parsed by mwâ€web.eqiad.mainâ€7d644d6d99â€qsftd
Cached time: 20240315110540
Cache expiry: 2592000
Reduced expiry: false
Complications: [varyâ€revisionâ€sha1, showâ€toc]
CPU time usage: 0.817 seconds
Real time usage: 1.060 seconds
Preprocessor visited node count: 4966/1000000
Postâ€expand include size: 166970/2097152 bytes
Template argument size: 5945/2097152 bytes
Highest expansion depth: 24/100
Expensive parser function count: 17/500
Unstrip recursion depth: 1/20
Unstrip postâ€expand size: 148753/5000000 bytes
Lua time usage: 0.512/10.000 seconds
Lua memory usage: 8675186/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  915.847      1 -total
 27.41%  251.036      1 Template:Reflist
 16.24%  148.715     34 Template:Cite_web
 10.77%   98.630      9 Template:Navbox
  8.56%   78.396      1 Template:Web_syndication
  8.45%   77.375      1 Template:Infobox_file_format
  8.43%   77.162      1 Template:Short_description
  8.05%   73.749      1 Template:Infobox
  7.57%   69.316     13 Template:Main_other
  6.33%   58.011      2 Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:93489-0!canonical and timestamp 20240315110540 and revision id 1210081177. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postgres is eating the database world (227 pts)]]></title>
            <link>https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</link>
            <guid>39711863</guid>
            <pubDate>Fri, 15 Mar 2024 03:43:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4">https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4</a>, See on <a href="https://news.ycombinator.com/item?id=39711863">Hacker News</a></p>
Couldn't get https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4: Error: Request failed with status code 429]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro: What we got wrong at Oculus that Apple got right (314 pts)]]></title>
            <link>https://hugo.blog/2024/03/11/vision-pro/</link>
            <guid>39711725</guid>
            <pubDate>Fri, 15 Mar 2024 03:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugo.blog/2024/03/11/vision-pro/">https://hugo.blog/2024/03/11/vision-pro/</a>, See on <a href="https://news.ycombinator.com/item?id=39711725">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><em>by <a href="https://twitter.com/hbarra" target="_blank" rel="noreferrer noopener">Hugo Barra</a></em> (former Head of Oculus at Meta)</p>



<p>Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product. Inspired by my dear friend <a href="https://ma.tt/category/birthday/" target="_blank" rel="noreferrer noopener">Matt Mullenwegâ€™s 40th post</a>, I decided to put pen to paper.</p>



<p>This started as blog post and became an essay before too long, so Iâ€™ve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest â€” peppered with my own â€˜takesâ€™. Iâ€™ve tried to stick to original thoughts for the most part and link to what others have said where applicable.</p>



<p>Some of the topics I touch on:</p>



<ul>
<li>Why I believe Vision Pro may be an over-engineered â€œdevkitâ€</li>



<li>The genius &amp; audacity behind some of Appleâ€™s hardware decisions</li>



<li><em>Gaze &amp; pinch</em> is an incredible UI superpower and major industry ah-ha moment</li>



<li>Why the Vision Pro software/content story is so dull and unimaginative</li>



<li>Why most people wonâ€™t use Vision Pro for watching TV/movies</li>



<li>Appleâ€™s bet in immersive video is a total game-changer for <em>Live Sports</em></li>



<li>Why I returned my Vision Proâ€¦ and my Top 10 wishlist to reconsider</li>



<li>Appleâ€™s VR debut is the best thing that ever happened to Oculus/Meta</li>



<li>My unsolicited product advice to Meta for <em>Quest Pro 2</em> and beyond</li>
</ul>



<h2><strong>The Apple Vision Pro is the Northstar the VR industry needed, whether we admit it or not</strong></h2>



<p>Iâ€™ve been a VR enthusiast for most of my adult life, from working as an intern at <a href="https://www.roadtovr.com/end-of-an-era-disneyquest-first-vr-attraction-set-to-close/" target="_blank" rel="noreferrer noopener">Disney Quest VR</a> in the 1990s, to being an early backer of the <a href="https://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-game">Oculus Rift DK1 on Kickstarter</a> in 2013, to leading the Oculus VR/AR team at Meta from 2017 to 2020 (and getting to work alongside VR legends like John Carmack, Brendan Iribe and Jason Rubin), and always testing every VR product or experience I can get my hands on.</p>



<p>Back in my Oculus days, I used to semi-seriously joke with our team (and usually got a lot of heat for it!) that the best thing that could ever happen to us was having Apple enter the VR industry and become a direct competitor to Oculus. Iâ€™ve always believed that strong competition pushes a team to do their best work in any industry. This became clear to me especially after living for nearly 10 years at the center of the iOS/Android battle of ecosystems where each side made the other infinitely better by constantly raising the bar on UX, features, performance, developer APIs etc, and seeing each side respond by not only fast following but usually also improving on what the other had released. (And this definitely went both ways: iOS copied Android as much as Android copied iOS).</p>



<p>But in the case of VR at Oculus, we also never really felt like the world had a Northstar that could truly capture human hearts and minds, and without that it would be impossible to transition VR from being a niche gamer tech to the incredible spatial computing paradigm that we always thought it potentially represented (which I still very much believe in). Apple could <em>really </em>help us if they cared about VR.</p>



<p>The Vision Pro launch has more or less done exactly what I had always hoped for, which is to build a huge wave of awareness and curiosity that elevates the spatial computing ecosystem and could ultimately lead to mass-market consumer demand and a lot more developer interest that VR has ever had. Now itâ€™s up to the industry to create enough user value and demonstrate whether this is in fact the future of computing.</p>



<h2><strong>The Vision Proâ€™s <em>instant magic</em> comes down to just: (1) an unprecedented new level of presence in VR, and (2) a new UI superpower using <strong>gaze &amp; pinch</strong></strong></h2>



<p>Using Vision Pro is an instantly magical and intuitive experience â€” whether or not youâ€™ve used other VR headsets â€” purely because of Appleâ€™s unrelenting focus on delivering two specific capabilities that speak to our humanity:</p>



<p><strong>1) Feeling present and connected to your physical world</strong>: thanks to a high-fidelity passthrough (â€œmixed realityâ€) experience with very low latency, excellent distortion correction (<em>much</em> better than Quest 3), and sufficiently high resolution that allows you to even see your phone/computer screen through the passthrough cameras (i.e. without taking your headset off).</p>



<p>Even though there are major gaps left to be filled in future versions of the Vision Pro hardware (which Iâ€™ll get into later), this level of connection with the real world â€” or â€œpresenceâ€ as VR folks like to call it â€” is something that no other VR headset has ever come even close to delivering and so far was only remotely possible with AR headsets (ex: HoloLens and Magic Leap) which feature physically transparent displays but have their own significant limitations in many other areas. Appleâ€™s implementation of Optic ID as an overlay on top of live passthrough is a beautiful design decision that only enhances this sense of presence.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Pro high-fidelity passthrough experience parallels <strong>Appleâ€™s introduction of the iPhoneâ€™s original <em>retina display</em></strong>, which set a new experience bar and gold standard in mobile display fidelity. While much remains to be improved in the Vision Pro passthrough experience, Apple is unquestionably setting a new standard for all future headsets (by any vendor) that VR passthrough must be good enough to closely resemble reality.</mark></p>
</blockquote>



<p><strong>2) Having a new UI superpower with gaze &amp; pinch</strong>, thanks to a very precise eye tracking system (with 2 dedicated cameras per eye) embedded into the lenses, coupled with a wide-field-of-view hand tracking system that can â€œseeâ€ a finger pinch even with your hands are down or resting on your lap. Because it works so effortlessly for the user, it really feels like having a new â€œlaser visionâ€ superpower.</p>



<p>The hardware needed to track eyes and hands in VR has been around for over a decade, and itâ€™s Apple unique ability to bring everything together in a magical way that makes this UI superpower the most important achievement of the entire Vision Pro product, without a shadow of doubt.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>The Vision Proâ€™s new â€œgaze + pinchâ€ input modality is <strong>the VR equivalent of the iPhoneâ€™s capacitive multi-touch gestures</strong>. Introduced by Apple with the first iPhone launch nearly 17 years ago, multi-touch instantly became a new standard that changed computing forever. â€œGaze + pinchâ€ is so groundbreaking that itâ€™s an instant defacto standard for VR interaction that future VR headsets be forced to adopt sooner or later. Itâ€™s also going to be a huge developer unlock that leads to gaze-based interaction ideas that will blow our minds. </mark></p>
</blockquote>



<h2>Hardware</h2>



<h2><strong>Vision Pro is a meticulously over-engineered â€œdevkitâ€ that is <em>far too heavy</em> <em>to have product-market fit</em> but good enough to seed curiosity into the world</strong></h2>



<p>The Oculus VR story began with the 2013 launch of <em><a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_1" target="_blank" rel="noreferrer noopener">Oculus Rift DK1</a></em> (short for â€œdevkit v1â€ or â€œdevelopment kit v1â€). This was a headset launched by the original Oculus startup team â€” years before it was acquired by Facebook â€” with the explicit goal of seeding developer interest well before a commercial release. Given that VR was a non-existing market then, releasing a devkit was the correct and necessary strategy <em>there and then</em> for a startup to start building a content library as well as momentum among enthusiasts ahead of launching a consumer product. The team released a <a href="https://en.wikipedia.org/wiki/Oculus_Rift#Development_Kit_2" target="_blank" rel="noreferrer noopener"><em>DK2</em></a> about a year later in 2014, and finally launched the first Oculus Rift consumer headset in 2015.</p>







<figure><img data-attachment-id="208" data-permalink="https://hugo.blog/2024/03/11/vision-pro/palmer-luckey-oculus-dk1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png" data-orig-size="1500,844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="palmer-luckey-oculus-dk1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/palmer-luckey-oculus-dk1.png 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Oculus co-founder Palmer Luckey wearing the original Oculus Rift DK1 released in 2013 </figcaption></figure>



<p>When I joined Facebook to lead the Oculus team in 2017 after the acquisition, one of the many battles I found myself in the middle of almost immediately was the â€œdevkit warâ€. The Oculus teamâ€™s DK1 and DK2 legacy was so strong that it was not uncommon to hear arguments in product meetings pushing for us to launch VR headsets still in prototype stage as â€œdevkitsâ€ to end users. Since Oculus was no longer a startup â€” and had the resources to both extensively test prototypes without launching them as products <em>and </em>run extensive pre-launch developer programs â€” it no longer made sense for Oculus devkits to exist. This stance often didnâ€™t make me very popular amongst some of my Oculus OG colleagues.</p>



<p>Fast forward to 2024. After the Vision Pro launch, the VR hardware enthusiast community (including Oculus OG folks Iâ€™m still in touch with) quickly arrived at the conclusion that Apple really played it safe in the design of this first VR product by over-engineering it. For starters, Vision Pro ships with more sensors than whatâ€™s likely necessary to deliver Appleâ€™s intended experience. This is typical in a first-generation product thatâ€™s been under development for so many years. It makes Vision Pro start to feel like a devkit.</p>







<figure><img data-attachment-id="148" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_sensors/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png" data-orig-size="804,486" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_sensors" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" width="804" height="486" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=804" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png 804w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_sensors.png?w=768 768w" sizes="(max-width: 804px) 100vw, 804px"><figcaption>A sensor party: 6 tracking cameras, 2 passthrough cameras, 2 depth sensors<br>(plus 4 eye-tracking cameras not shown)</figcaption></figure>



<p>Hereâ€™s a quick comparison with existing VR headsets:</p>







<figure><table><thead><tr><th>Sensor</th><th>Vision Pro</th><th>Meta <br>Quest 3</th><th>Meta <br>Quest Pro</th></tr></thead><tbody><tr><td>Environment passthrough cameras</td><td>2</td><td>2</td><td>1</td></tr><tr><td>World tracking cameras</td><td>6</td><td>4</td><td>6</td></tr><tr><td>Depth sensors</td><td>2</td><td>1</td><td>â€“</td></tr><tr><td>Eye tracking cameras</td><td>4</td><td>â€“</td><td>2</td></tr><tr><td><strong>Total</strong></td><td><strong>14</strong></td><td><strong>7</strong></td><td><strong>9</strong></td></tr></tbody></table><figcaption>Side-by-side comparison with the sensor stacks of other VR headsets</figcaption></figure>



<p>This over-specâ€™ing is unsurprising and characteristic of a v1 product where its creator wants to ensure it survives the hardest tests early users will no doubt want to put the product through. Itâ€™s also a way for Apple to see how far developers will push the productâ€™s capabilities, as Apple is no doubt relying on that community to produce the majority of software/content magic for this new type of computer, as theyâ€™ve previously done with every other device class.</p>



<p>Appleâ€™s decision to over-spec the Vision Pro does, however, lead to the inevitable consequence of a headset weighing above 600g â€” heavier than most other VR headsets in the market to date â€” that <strong>makes it difficult for most people to wear it for more than 30-45 minutes at a time without suffering a lot of discomfort</strong>. Most of the discomfort comes in the form of pressure against the userâ€™s face and the back of the personâ€™s head.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Because of its heavy weight, Vision Pro has inevitably landed in the world as a high-quality â€œdevkitâ€ designed to capture everyoneâ€™s curiosity, hearts &amp; minds with its magic (especially through the voice of enthusiastic tech influencers) while being realistically focused on developers as its primary audience. In other words, the Vision Pro is a devkit that helps prepare the world to receive a more mainstream Apple VR headset that could have product-market fit in 1 or 2 generations.</mark></p>
</blockquote>



<p>All things considered, I do believe Appleâ€™s calculus was correct in prioritizing launching a first-generation product with fewer experience and design compromises at the expense of user comfort. And while many people have argued Apple could have avoided this major comfort issue by redistributing weight or using lighter materials, those attempts would have come at the expense of beauty and design. (Iâ€™ll come back to the weight issue shortly.)</p>



<p>With this in mind, itâ€™s easy to understand two particularly important decisions Apple made for the Vision Pro launch:</p>



<ul>
<li><strong>Designing an incredible in-store Vision Pro demo experience</strong>, with the primary goal of getting as many people as possible to experience the magic of VR through Appleâ€™s lenses â€” most of whom have no intention to even consider a $4,000 purchase. The demo is only secondarily focused on actually selling Vision Pro headsets.</li>



<li><strong>Launching an iconic woven strap that photographs beautifully</strong> even though this strap simply isnâ€™t comfortable enough for the vast majority of head shapes. Itâ€™s easy to conclude that this decision paid off because nearly every bit of media coverage (including and especially third-party reviews on YouTube) uses the woven strap despite the fact that itâ€™s less comfortable than the dual loop strap thatâ€™s â€œhidden in the boxâ€.</li>
</ul>



<h2><strong>The existence of Vision Pro in 2024 is entirely a function of Apple managing to ship a <em>first-of-its-kind</em> ultra <strong>high-resolution display</strong></strong></h2>



<p>One of our biggest product positioning struggles within the Oculus VR team from the very beginning â€” especially when trying to convince reviewers â€” was always related to having <em>underwhelming displays</em>. Every single Oculus headset that ever shipped (including the latest Quest 3) has suffered from resolution/pixelation issues varying from â€œterribleâ€ to â€œpretty badâ€. Itâ€™s like weâ€™re living in the VR-equivalent world of VGA computer monitors.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>In order For Apple to make a huge splash entering the VR market â€” a category thatâ€™s been around the consumer world for nearly 10 years â€” they needed to launch a product that was unambiguously better than anything that had ever existed. The obvious way to do that was to <strong>attack the Achilles heel of all existing headsets and reinvent the VR display</strong>, and thatâ€™s exactly what Apple did with the Vision Pro.</mark></p>
</blockquote>



<p>Vision Pro is the first VR headset that offers good enough resolution and visual acuity with little semblance of a <a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener"><em>screen door effect</em></a> or pixelation artifacts. This level of presence and fidelity could only be made possible with an ultra high-res display, and itâ€™s 100% clear that achieving an first-of-its-kind level of display quality was the internal launch bar for Vision Pro at Apple.</p>



<p>Appleâ€™s relentless and uncompromising hardware insanity is largely what made it possible for such a high-res display to exist in a VR headset, and itâ€™s clear that this product couldnâ€™t possibly have launched much sooner than 2024 for one simple limiting factor â€” the maturity of micro-OLED displays plus the existence of power-efficient chipsets that can deliver the heavy compute required to drive this kind of display (i.e. the M2).</p>



<p>Micro-OLED displays differ from any other previous consumer display technology because they are manufactured on top of a silicon substrate (similar to how semiconductor chips are made). To put the insanity of micro-OLED displays in perspective, the <strong>Vision Pro panel has a 7.4x higher pixel density than the latest iPhone and nearly 3x the Quest 3</strong>:</p>







<figure><table><thead><tr><th>Feature</th><th>Vision Pro</th><th>Bigscreen Beyond</th><th>Quest 3</th><th>iPhone 15 Pro Max</th></tr></thead><tbody><tr><td>Display Type</td><td>Micro-OLED</td><td>Micro-OLED</td><td>LCD</td><td>OLED</td></tr><tr><td>Resolution <br>(pixels per eye)</td><td>3660 x 3200</td><td>2560 x 2560</td><td>2064 x 2208</td><td>2796 x 1290</td></tr><tr><td>Total Pixels</td><td>23 million</td><td>13 million</td><td>9 million</td><td>3.6 million</td></tr><tr><td>Pixels Per Inch (PPI)</td><td>3386</td><td>(unknown)</td><td>1218</td><td>460</td></tr><tr><td>Pixels Per Degree (PPD)</td><td>34</td><td>32</td><td>25</td><td>94 <br>(at 1 foot distance)</td></tr></tbody></table></figure>



<p><em>(See the appendix of this essay for a quick explanation of <strong>Pixels Per Degree </strong>or <strong>PPD</strong>)</em></p>



<p>The folks at iFixit created this stunning GIF using a scientific microscope to compare the pixel size of the Vision Pro display â€” which measures 7.5 Î¼m, the size of a human red blood cell â€” with the pixel size of the latest iPad and iPhone displays:</p>







<figure><img data-attachment-id="54" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-display/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif" data-orig-size="940,520" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-display" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" width="940" height="520" src="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=940" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif 940w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/avp-display.gif?w=768 768w" sizes="(max-width: 940px) 100vw, 940px"><figcaption>Source: <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">iFixit</a></figcaption></figure>



<p>The Apple Vision Proâ€™s micro-OLED display has created a lot of chatter in my hardware supply chain world, with lots of companies â€” predominantly smartphone OEMs â€” quickly racing to try and build a product that can deliver a similar experience to Vision Pro. Apple has secured a 1-year exclusive with <a href="https://www.sony-semicon.com/en/products/microdisplay/oled.html" target="_blank" rel="noreferrer noopener">Sony Semiconductor Solutions Group</a> and its second supplier <a href="https://www.seeya-tech.com/en/" target="_blank" rel="noreferrer noopener">SeeYA Technology</a>. There are also rumors Apple is dropping Sony as a display supplier and replacing it with <a href="https://www.boe.com/en/Enterprise/VR_AR">BOE</a> (whose <a href="https://www.boe.com/en/Enterprise/VR_AR">website</a> says a panel equivalent to Vision Pro is at â€œsampleâ€ stage).</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>I fully expect the <a href="https://www.prnewswire.com/news-releases/lg-and-meta-forge-collaboration-with-meta-to-accelerate-xr-business-302073794.html" target="_blank" rel="noreferrer noopener">recently announced Meta/LG partnership</a> to be all about creating a supply chain advantage for Meta so they can race a <em>Quest Pro 2 </em>product into market that can compete with Vision Pro with LG putting some skin in the game to lower the street price of the headset.</mark></p>
</blockquote>



<p>(P.S. For anyone who wants to dig into more details of the Vision Pro display and pass-through system, I highly recommend <a href="https://www.ifixit.com/News/90409/vision-pro-teardown-part-2-whats-the-display-resolution" target="_blank" rel="noreferrer noopener">this article from iFixit</a> and <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">this article from Karl Guttag</a>, whoâ€™s an amazingly talented expert in display devices).</p>



<h2><strong>Apple made the Vision Pro display <em>intentionally blurry</em> in order to hide pixelation artifacts and make graphics appear smoother</strong></h2>



<p>There is a very good reason Apple has not used the word <em>retina</em> anywhere in their marketing materials for Vision Pro. Itâ€™s the simple fact that Vision Proâ€™s display does not pass the retina test â€” which is a <em>resolution high enough that the human eye can no longer discern individual pixels</em>. The Vision Pro display is nowhere near retina quality for a VR headset (see appendix for details) and yet <strong>our eyes cannot see individual pixels when looking at it</strong>. What gives?</p>



<p>During the first few days using Vision Pro, there was something that kept calling my attention but which I struggled to get my arms (or eyes) around. Everything my eyes saw in the headset felt a bit softer than I expected, and I initially attributed this to the seemingly refreshing absence of any <em><a href="https://pimax.com/what-is-the-screen-door-effect-in-vr/" target="_blank" rel="noreferrer noopener">screen door effect</a></em> â€” a pixelation artifact that has essentially doomed all VR headsets created up until now.</p>



<p>Well, as it turns out, the incredible <a href="https://kguttag.com/about-karl-guttag/" target="_blank" rel="noreferrer noopener">Karl Guttag</a> ran a meticulous <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/" target="_blank" rel="noreferrer noopener">photographic analysis of the Vision Pro display</a> and came to a curious and possibly disturbing conclusion: <strong>Apple <em>intentionally calibrated the Vision Pro display slightly out of focus</em> to make pixels a bit blurry and hide the screen door effect â€œin plain sightâ€</strong>.</p>



<p>This image from Karlâ€™s blog explains this well by comparing Vision Pro and Quest 3 displays side by side at a close enough distance where itâ€™s possible to see individual pixels and clearly see the intentional blur that was added to the Vision Pro display:</p>







<figure><img data-attachment-id="179" data-permalink="https://hugo.blog/2024/03/11/vision-pro/avp-vs-mq3-close-up-crop-copy/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp" data-orig-size="471,519" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="avp-vs-mq3-close-up-crop-copy" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" loading="lazy" width="471" height="519" src="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=471" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp 471w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=136 136w, https://hugobarracom.files.wordpress.com/2024/03/avp-vs-mq3-close-up-crop-copy.webp?w=272 272w" sizes="(max-width: 471px) 100vw, 471px"><figcaption>Extreme close-up comparison between Vision Pro (AVP) and Quest 3 (MQ3) displays (Source: <a href="https://kguttag.com/2024/03/01/apple-vision-pros-optics-blurrier-lower-contrast-than-meta-quest-3/">KGOnTech</a>)</figcaption></figure>



<p>What Karl concluded is that even though Quest 3 has a much lower display resolution than Vision Pro (1,218 PPI vs. 3,386 PPI), Quest 3 appears objectively crisper especially when showing high-contrast graphics. In other words, Quest 3 is squeezing the highest possible resolution out of its display at the expense of a â€œharsher lookâ€ while Apple is giving up some of the Vision Proâ€™s display resolution in order to achieve a â€œsofter lookâ€. Karl may disagree with my conclusion on this point:</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong> Intentionally making the Vision Pro optics blurry is a clever move by Apple because it results in way smoother graphics across the board by hiding the screen door effect (which in practice means that you wonâ€™t see pixelation artifacts). This is also where Appleâ€™s â€œtasteâ€ comes in, essentially resulting in the Vision Pro display being tuned to have a unique, softer, and more refined aesthetic than Quest 3 (or any other VR headsets). This is certainly a refreshing approach to designing VR hardware.</mark></p>
</blockquote>



<p>With this design decision, Apple is no doubt giving up a bit of the Vision Pro displayâ€™s high pixel resolution in order to achieve overall smoother graphics. You are definitely losing some text crispness in order to gain a higher perception of quality for images, video and 3D animations. This is a big benefit of starting with an ultra high-resolution micro-OLED display â€” Apple had enough pixels to work with that they could afford to make this trade-off. <strong>This is the kind of thing that our hardcore VR engineers at Oculus would have fought against to the end of the world, and I doubt we could have ever shipped a â€œblurred headsetâ€, LOL!</strong></p>



<h2><strong>Sadly, the Vision Pro display suffers from <em>significant motion blur &amp; image quality issues</em> that render passthrough mode unusable for longer periods</strong></h2>



<p>While Appleâ€™s decision to make individual pixels blurry on the Vision Pro display was extremely clever, the headset unfortunately suffers from a completely different type of blur thatâ€™s extremely problematic for the overall experience.</p>



<p>From the very first time I put on my Vision Pro, I noticed <strong>a lot of motion blur in passthrough</strong> mode even in excellent ambient lighting conditions and a still noticeable amount even when viewing immersive content. While my immediate instinct was to think that all VR headsets have that kind of motion blur and itâ€™s just more noticeable on Vision Pro, a side-by-side comparison with Quest 3 quickly proved itâ€™s significantly more serious on Vision Pro. This is particularly surprising considering that the passthrough cameras and display are both running at 90 hertz.</p>



<p>Since none of the initial Vision Pro reviews pointed out this issue, I ended up even calling Apple support to find out if this might be a known problem or possibly even a hardware defect. But then more in-depth reviews began pointing out the same problem (I highly recommend <a href="https://youtu.be/eOH33sWgds8?si=TTWCd8-UqX-D9-Eg" target="_blank" rel="noreferrer noopener">this review by Snazzy Labs</a>).</p>



<p>Motion blur in passthrough mode ended up being one of the many reasons why I decided to return my Vision Pro, because itâ€™s just uncomfortable, leads to unnecessary eye strain, and really gets in the way of anyone using the headset for longer periods of time in passthrough mode.</p>



<p>There are other noticeable issues as well which affect passthrough mode, including <strong>very little dynamic range, incorrect white balance in most indoor use cases, and signs of edge distortion and chromatic aberration</strong>. Some of these might be addressed by software updates, but I expect most will not as they probably are limitations of the hardware stack.</p>



<h2><strong>The Vision Pro packs <em>a lot more computing power </em>than most people might realize â€” the M2 + R1 combination puts it at the level of a MacBook Pro</strong></h2>



<p>Any standalone VR headset is basically a 2-in-1 system: a regular â€œcomputingâ€ computer and a spatial computer bundled together.</p>



<ul>
<li><strong>A <em>regular computer</em> in charge of running applications and performing general computation</strong>: this is everything that happens on your smartphone, tablet or notebook, including running the OS, executing applications across CPU/GPU loads, and doing computation work in the background.</li>



<li><strong>A <em>spatial computer</em> in charge of the environment</strong>: it keeps track of the whole environment, tracks your hands &amp; eyes, and ensures that everything â€” your surroundings, the OS system UI, and your apps â€” gets rendered in the right physical place in space and updated at 90 to 120 times per second while your head and body are moving around.</li>
</ul>



<p>These two â€œcomputersâ€ must operate together without missing a beat â€” any latency above 20 milliseconds becomes quickly noticeable in VR and will often translate very quickly into user perception of unresponsiveness or jankiness, which can cause discomfort, eye strain or even dizziness for many people.</p>



<p>Enter the Vision Pro dual-chip design:</p>







<blockquote>
<p>â€œ<em>A unique dualâ€‘chip design enables the spatial experiences on Apple Vision Pro. The powerful M2 chip simultaneously runs visionOS, executes advanced computer vision algorithms, and delivers stunning graphics, all with incredible efficiency. And the brand-new R1 chip is specifically dedicated to process input from the cameras, sensors, and microphones, streaming images to the displays within 12 milliseconds â€” for a virtually lag-free, real-time view of the world.</em>â€œ</p>
<cite>Apple Vision Pro website</cite></blockquote>



<p>The Vision Pro ships with the same M2 chip as the 2022 iPad Pro (or 2022 MacBook Air) alongside the new R1 chip which handles the massive amount of data coming from the 20+ tracking cameras and depth sensors (sensor fusion). Whatâ€™s interesting to note is that Vision Pro actually does perform largely like an iPad Pro in benchmark tests that push CPU and GPU to their limits in both single-core and multi-core scenarios (see chart below).â€‚</p>







<figure><img data-attachment-id="83" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_geekbench/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png" data-orig-size="1542,1416" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_geekbench" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" loading="lazy" width="1024" height="940" src="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/vision-pro_geekbench.png 1542w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Source: <a href="https://www.pcmag.com/reviews/apple-vision-pro" target="_blank" rel="noreferrer noopener">PC Magazine</a></figcaption></figure>



<p>This is more impressive than it seems, and demonstrates that the R1 chip is doing a very significant amount of heavy lifting â€” essentially the vast majority the spatial computing workload â€” leaving a lot of compute room for the M2 chip to deliver the same level of performance as if it was just running inside an iPad Pro. </p>



<p>By all accounts so far, the R1 chip appears to be of a fairly similar package size as the M2 chip (though built using a specialized architecture), which puts the Vision Pro well ahead of any current generation iPad or MacBook Air, and likely more on par with a MacBook Pro from a silicon performance perspective. Definitely an impressive achievement by the Apple Silicon team.</p>



<p>This also begs the questionâ€¦ what if you could completely offload the Vision Proâ€™s compute to another Apple device?</p>



<h2><strong>Appleâ€™s decision to use a tethered pack <em>will</em> <em>enable future Vision headsets to be much lighter</em> by offloading compute to an iPhone, iPad or MacBook </strong></h2>



<p>One of the most controversial aspects of Vision Pro is the fact that it sports a tethered battery pack, differently from all other commercially available standalone VR headsets. Many people have heavily criticized Apple for this decision because of the inconvenience of the â€œhangingâ€ external battery.</p>



<p>I agree with Palmer Luckey (<a href="https://youtu.be/S-sD2FTjSaw?si=AQUTyJria4TKmSbc&amp;t=1437" target="_blank" rel="noreferrer noopener">from his recent interview with Peter Diamandis</a>) that this was a necessary short-term decision on Appleâ€™s part given the reality of the hardware shipping inside the Vision Pro, but more importantly it was a very intentional long-term decision, which Iâ€™ll explain.</p>



<p>As I mentioned earlier, the Vision is a meticulously over-engineered computer with a very significant collection of power-hungry components:</p>



<ul>
<li>2x laptop-class processors (the R1 chip is almost the same size as the M2, which is the same processor shipping on MacBooks)</li>



<li>2x very bright micro-OLED displays with high pixel density</li>



<li>1x auxiliary EyeSight display</li>



<li>12x cameras and other sensors</li>



<li>2x blower fans</li>



<li>2x speakers</li>
</ul>



<p>As Quin from Snazzy Labs carefully explains <a href="https://youtu.be/eOH33sWgds8?si=M_-SlaOYK5k-1SPP&amp;t=988" target="_blank" rel="noreferrer noopener">in his excellent review</a>, the Vision Pro likely draws as much as <strong>40 watts of power</strong>, which is more than most MacBook laptops. This also means it has a power supply with the potential of generating a lot of heat. So, in addition to transferring the battery weight out of the headset, the decision to move to a tethered pack also keeps a huge heat source safely away from your head.</p>







<blockquote>
<p><mark><strong>MY TAKE: </strong>All that said, the long-term strategic reason for having an external battery pack is to set expectations with Vision Pro users that there will <em>always</em> be an external box connected to the headset. In future Vision headsets, Apple should be able to comfortably start moving a lot of electronics off the headset, possibly shaving off as much half of the weight over a few generations and <strong>target around 300g</strong>. This also opens an extremely interesting path for Apple in a few years to <strong>use an iPhone, iPad or MacBook as the tethered computer driving the headset</strong>, which would dramatically simplify the headset.</mark> </p>
</blockquote>



<p>Interestingly, there is a tethered VR headset in the market today that demonstrates this desirable end state. Itâ€™s the <a href="https://www.bigscreenvr.com/" target="_blank" rel="noreferrer noopener"><strong>Bigscreen Beyond</strong></a>, the worldâ€™s smallest PC VR headset (i.e. needs to be tethered to a computer) that is lighter than even most ski goggles at 127 grams. Bigscreenâ€™s ability to build this product is in many ways a bit of cheating since the headset was stripped of all sensors (no external cameras or eye tracking), but its existence nonetheless plays an important role in letting us experience what the future holds and where Appleâ€™s sights are focused.</p>







<figure><img data-attachment-id="152" data-permalink="https://hugo.blog/2024/03/11/vision-pro/carmack_bigscreen-beyond/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg" data-orig-size="1600,1183" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="carmack_bigscreen-beyond" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" loading="lazy" width="1024" height="757" src="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/carmack_bigscreen-beyond.jpeg 1600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>John Carmack wearing the Bigscreen Beyond VR headset, <br>which weighs 127g (vs. the Vision Proâ€™s 600g)</figcaption></figure>



<h2>Software</h2>



<h2><strong>The Vision Pro software story is a <em>bold antithesis of VR</em> â€” and the lack of exciting AR apps at launch paints the product into an empty corner</strong></h2>



<p><em>â€œWelcome to the era of spatial computingâ€</em> is Appleâ€™s leading slogan for Vision Pro and, and as expected by everyone in the VR industry, Apple is going all-in on AR (augmented reality) to deliver on this proposition. The company has gone out of their way to actively ignore everything that VR has been know for over the last decade.</p>



<p>At the center of Appleâ€™s marketing for Vision Pro is <em>â€œkeeping users connected to their surroundings and other peopleâ€</em>. Reading in-between these lines, itâ€™s not hard to see that Apple is taking an anti-VR stance that borderline accuses Metaâ€™s approach to VR of promoting human isolation while positioning Vision Pro as the antithesis of that.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> Appleâ€™s anti-VR stance is a risky move because it negates most of the traditional immersive content that has made the VR medium popular until now, and at least for now is painting Vision Pro into an empty corner. This reminds me of Appleâ€™s broad </mark><mark>stance on privacy â€” built to be in complete opposition to Meta/Google â€” which has put them in a tight spot by severely limiting their options and restricting innovation in the age of Gen AI. </mark></p>
</blockquote>



<p><strong>There are no fully immersive games in the Vision Pro app store</strong>, whereas easily &gt;90% of the Oculus Quest catalog is made of immersive VR games. Instead of leveraging the existing community of high-quality immersive VR content developers, Apple is focusing all of its energy exclusively on AR use cases that play to the companyâ€™s ecosystem strengths â€” iOS apps and MacOS productivity â€” which Iâ€™ll dive into over the next few sections.</p>



<p>The launch roster of 3D AR apps &amp; games is a tremendous disappointment â€” in both quality and quantity â€” and mostly includes a few simple casual games, some of which are originally 2D games hastily converted into 3D art. The fact that <em>ARKit</em> has been available for so many years on iPad and iPhone (despite its limited success) should have made it possible for Apple to easily round up developers into building a sufficient number of exciting and impressive AR titles for Vision Pro. Instead, <strong>weâ€™re seeing an initial lack of developer excitement for the category that should have been the <em>most defining and inspiring category </em>on Vision Pro.</strong></p>



<p>Ironically, Meta made almost exactly the same mistake launching Quest Pro in 2022. That headset shipped with close to zero AR apps despite their emphasis in the launch messaging on â€œfull-color mixed realityâ€.</p>







<blockquote>
<p><mark><strong>MY TAKE</strong>: This may be the first device category where Appleâ€™s â€œbuild it and they will comeâ€ approach to creating developer traction may simply not work as previously. It will be many years (and possibly even more than a decade) before there are tens of millions of active Vision Pro users willing to pay for spatial AR apps. Apple will need to take a page out of the Oculus playbook and actively motivate developers financially to develop for Vision Pro.</mark></p>
</blockquote>



<h2><strong>Vision Proâ€™s positioning as <strong>a </strong><em><strong>productivity &amp; movie watching â€œbig scree</strong>nâ€</em></strong> <strong>is dull &amp; unimaginative <strong>but Apple is unashamedly owning it</strong> </strong></h2>



<p>With a weak and limited launch roster of AR apps that doesnâ€™t include a single flagship 3D app or game, Apple had to focus the entire positioning for Vision Pro at launch almost entirely on how it plugs into the existing Apple ecosystem of 2D apps.</p>



<p>In the usual Apple-style product marketing, the launch messaging for Vision Pro is very explicitly codified in the <a href="https://www.apple.com/apple-vision-pro/" target="_blank" rel="noreferrer noopener">product webpage</a> and every single marketing asset is consistent with it. How Apple has chosen to sequence their product messaging matters just as much as the messages themselves. <strong>Vision Pro is 60% about 2D productivity and 40% about watching media/movies on a big screen</strong>:</p>







<figure><table><tbody><tr><td><strong>Use case</strong></td><td><strong>Apple Slogans</strong></td></tr><tr><td>Productivity</td><td><em>â€œFree your desktop. And your apps will follow.â€</em><p><em>â€œHow to work&nbsp;in allâ€‘new ways.â€</em></p></td></tr><tr><td>Media</td><td><em>â€œThe ultimate theater. Wherever you are. </em><p>â€œ<em>An immersive way to experience entertainment.â€</em></p></td></tr></tbody></table></figure>



<p>On a side note, <em>FaceTime with Persona avatars</em> and <em>spatial photos &amp; videos</em> are also pushed as core pillars in the Vision Pro product messaging, but theyâ€™re clearly just ancillary use cases to support marketing. Though too small to matter for now, they may (and for Appleâ€™s sake, hopefully will) end up playing a much bigger role in the future.</p>







<blockquote>
<p><mark><strong>MY TAKE:</strong> The Vision Pro launch is a significant missed opportunity, with Apple â€œwelcoming us into the era of spatial computingâ€ with a software and services stack that is practically only focused on 2D use cases. Though the in-store demos paint an exciting future, the experience delivered by Apple at launch is dull and unimaginative at best.</mark></p>
</blockquote>



<p>Putting aside my criticism to Appleâ€™s focus for the Vision Pro at launch, the next few sections will offer a deep dive into my thoughts and opinions on the software and experience enabling <em>Productivity</em> and <em>Media</em> use cases. </p>



<div>
<h2><strong>The Vision Pro desperately wants to be <em>the â€œfuture of work</em>â€ and pick up where Meta Quest Pro completely dropped the ball, butâ€¦</strong></h2>



<p>One of our strongest thesis from the early Oculus days was always about VR playing a defining role in the â€œfuture of workâ€, from running 2D apps in massive virtual displays to having native 3D apps that would make it a lot easier to work and collaborate with others on a project.</p>



<p>When Meta announced the Quest Pro in 2022, much of its marketing hype was in fact around the <a href="https://www.youtube.com/watch?v=eYjU9mV7-6g" target="_blank" rel="noreferrer noopener"><strong>Workrooms app</strong></a> (at the time led by my incredibly talented friend Mike LeBeau). The app allows you use your Mac from within VR, with a lot of attention to details needed to make it truly possible to work in VR for several hours, including support for up to 3 virtual monitors and the ability to see your physical keyboard in passthrough or replace it with a fully 3D rendered tracked twin.</p>



<figure><img data-attachment-id="196" data-permalink="https://hugo.blog/2024/03/11/vision-pro/quest-pro_workrooms-2/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png" data-orig-size="719,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="quest-pro_workrooms" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" loading="lazy" width="719" height="479" src="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=719" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png 719w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/quest-pro_workrooms.png?w=300 300w" sizes="(max-width: 719px) 100vw, 719px"><figcaption>Image from the Quest Pro launch marketing</figcaption></figure>



<p>Quest Pro was designed with the goal of being a lot more comfortable than other VR headsets so that people could wear it for longer periods of time. While this was a well-intended attempt, the product had a major flaw which made it less than a â€œminimum viable productâ€ and simply did not justify a price tag well above $1,000. The display resolution â€” at 22 PPD (pixels per degree) â€” was too low and vastly insufficient to unlock â€œworking in VRâ€ because of poor text readability. This shortcoming (in addition to very poor quality passthrough) was so massive that it rendered the product practically irrelevant at launch â€” I ended up returning my unit within 24 hours of first use.</p>



<p><strong><em>Can Vision Pro deliver where Quest Pro (and Quest 3) have failed?</em></strong></p>



<p>In order to really put Vision Pro to the test in real-life scenarios, I spent well over 100 hours trying to deploy as many of my own productivity workflows as I could, including about 1/3 of the work in this essay. Iâ€™ll share my conclusions over the next couple of sections.</p>



<p>First off, before diving into the value proposition of Vision Pro as a work/productivity computer, I needed to clearly frame my â€œjobs to be doneâ€ as specifically as possible. When Iâ€™m in â€œwork modeâ€ â€” whether doing actual professional work or just life management stuff â€” I have three distinct workstations that I use back and forth (aside from my smartphone, which I wonâ€™t include here):</p>



<ul>
<li><strong>Office workstation</strong> <strong>| Mac Pro with 2x Apple XDR 6K displays:</strong> my highest productivity setup because it gives me access to everything I need in a single view and enables zero-hurdle multi-tasking; itâ€™s basically my gold standard for any task or project no matter how complex with the highest speed &amp; quality</li>



<li><strong>Laptop | MacBook Pro 16-inch:</strong> medium-high productivity setup with a sufficiently large retina-quality display that still enables complex tasks with good enough multi-tasking though I do feel noticeably less productive; it requires a backpack to carry when going outside of home/office</li>



<li><strong>Tablet | iPad Pro 11-inch with keyboard:</strong> a low-medium productivity setup good for focused single-app work with extremely limited multi-tasking (ex. email, writing that doesnâ€™t require research, some life planning) but still better than using my phone; one great advantage is that I can carry this â€œmini computerâ€ more easily than a laptop without really needing a backpack</li>
</ul>



<p>Hereâ€™s a table summarizing these workstations:</p>



<figure><table><tbody><tr><td><strong>Device</strong></td><td><strong>Number of Pixels</strong></td><td><strong>Number of simult. windows</strong></td><td><strong>Ideal for</strong></td><td><strong>Ergonomics</strong></td><td><strong>Portability</strong></td><td><strong>Cost</strong></td></tr><tr><td>Mac Pro + 2x XDR displays</td><td>XXLarge<br><em>(2Ã—20 million)</em></td><td>8+</td><td>Any creative project with lots of multitasking</td><td>Ideal</td><td>â€“</td><td>&gt;$10,000</td></tr><tr><td>MacBook Pro 16-inch</td><td>Large<br><em>(7.7 million)</em></td><td>2-4</td><td>Most creative projects with limited multitasking</td><td>Good</td><td>Medium</td><td>$3,000</td></tr><tr><td>iPad Pro 11-inch + keyboard case</td><td>Medium<br><em>(4 million)</em></td><td>1</td><td>Full email, simple editing</td><td>Not great</td><td>High</td><td>$1,200</td></tr></tbody></table></figure>



<p><strong>I then asked myself: </strong>Could I see myself using Vision Pro as a productivity device instead of (or in conjunction with) any of my existing workstations?</p>



<p>These are the specific questions I set off to answer (from the lowest to highest bar):</p>



<ul>
<li>Can Vision Pro be a complete alternative to my Tablet Workstation so that I could carry it around instead of an iPad Pro?</li>



<li>Can Vision Pro enhance my Laptop Workstation enough that it feels like having a â€œvirtual XDR displayâ€ or two?</li>



<li>Could Vision Pro ever be better than ALL of my workstations at least for some productivity tasks? <strong><em>â‡’ This is what excites me the most!</em></strong></li>
</ul>
</div>



<div>
<h2><strong><span>Productivity Thesis #1</span>: Vision Pro as an iPad Pro replacement </strong></h2>



<blockquote>
<p><strong>Status:</strong> âŒ NOT READY <em>(but itâ€™s promising!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro aspires to become your â€œspatial iPad Proâ€ with really good potential for much better multi-tasking (than an iPad) and the ability to do focused work anywhere, but thereâ€™s simply too much usability friction and too many important apps missing for that to be a reality today (or likely in the next 1-2 years).</mark></p>
</blockquote>



<p>The Vision Pro is conveniently designed by Apple to immediately fit right into the existing Apple ecosystem as a (rather expensive) alternative to an iPad Pro. The headset has identical compute (same M2 chip) to an iPad Pro and conveniently supports iPad apps running natively. In fact, itâ€™s easy to claim the Vision Pro should in principle be better than an iPad Pro because you can run multiple iPad apps side-by-side in full screen mode, which would overcome one of the biggest productivity limitations of iPads â€” poor multitasking.</p>



<p>However, in reality this claim really doesnâ€™t hold true at all (at least not yet) for a few important limitations of Vision Pro at launch:</p>



<ul>
<li><strong>Many iPad apps donâ€™t work well </strong>(or at all) on Vision Pro despite <a href="https://developer.apple.com/help/app-store-connect/manage-your-apps-availability/manage-availability-of-iphone-and-ipad-apps-on-apple-vision-pro/" target="_blank" rel="noreferrer noopener">Apple automatically opting in developers</a>. There is substantial friction and instability navigating inside productivity apps given that theyâ€™re designed for a multi-touch UI (ex: some iPad gestures donâ€™t exist in Vision Pro, and some touch targets are too small). A lot of apps will require some effort by their developers to work well enough.</li>



<li><strong>Most productivity apps are still missing from the App Store</strong> (likely for the reason above), which would leave large holes in most peopleâ€™s workflows. For example, the most important missing apps for my own workflows include Chrome, Gmail, GDocs/Sheets/Slides, Asana.</li>



<li><strong>Text input is still quite buggy</strong> which adds more friction to any productivity workflow. Cursor placement, text selection and editing are super error prone. Dictation doesnâ€™t stream results as you speak.</li>



<li><strong>You must carry a keyboard and a trackpad</strong> (mice are <span>not</span> supported) for the vast majority of your iPad-class productivity workflows on Vision Pro, which could be an added inconvenience (compared to carrying an iPad with a keyboard case or even a laptop). Editing documents, spreadsheets or presentations without those is virtually impossible.</li>



<li><strong>There is no reliable workspace persistency</strong> which adds even more friction â€” you are forced to re-open apps, and then re-size and reposition windows almost every time. The capabilities we all want (which Apple should be able to ship soon if they want to) are (i) persistent workspaces, (ii) location-specific workspaces, and (iii) a spatial computing equivalent of Mission Control.</li>
</ul>



<p>All that said, these limitations can all be addressed by Apple and the potential of Vision Pro as an iPad Pro replacement really is there. Even though the iPad Pro has nearly twice the PPD (pixels per degree) as the Vision Pro, text readability of iPad apps on Vision Pro is good enough for you to run 3 or 4 side-by-side apps plus a number of ambient widgets.</p>



<p>I also really believe thereâ€™s a large enough white canvas for lots of Apple-style innovation and magic around letting users configure and manage their workspace with a combination of 2D panels and virtual 3D objects. The potential is really significant as long as Apple really empowers developers to innovate here (try <em>Nicholas Jitkoffâ€™s <a href="https://www.widget.vision/" target="_blank" rel="noreferrer noopener">widget.vision</a></em> to see some great early examples â€” the NY Times front page widget is my favorite). </p>



<p><strong>Call me crazy, but I personally could get quite excited by the idea of a â€œspatial iPad Proâ€</strong> if I was able to actually get all of my iPad apps on the Vision Pro, and if Apple addresses all of the issues causing friction in my workflows. The reason why is simply that of focus â€“ to be able to really â€œdial down realityâ€ and tune into the work wherever I am, without carrying my laptop with me but still having some degree of multitasking available.</p>



<h2><strong><span>Productivity Thesis #2</span>: Vision Pro as a MacBook virtual external monitor </strong></h2>



<blockquote>
<p><strong>Status:</strong> âœ… ALMOST READY <em>(needs some bug fixing!)</em></p>



<p><mark><strong>MY TAKE: </strong>The Vision Pro is a few software bug fixes away from being a suitable virtual-equivalent to an external monitor similar to a 27-inch Apple Studio Display that makes it easy to work immersively in VR using all your existing MacOS apps and workflows on a huge screen (but donâ€™t expect an Apple XDR 6K experience!).</mark></p>
</blockquote>



<p>One of Vision Proâ€™s best pieces of pure software/experience magic is the ability to seamlessly connect to a MacBook by simply looking at the computer while wearing the headset. This is a simple improvement to the traditional AirPlay UI that creates a profound sense of seamlessness which VR has always lacked.</p>



<p>Before diving into this thesis, Iâ€™ll establish that the Vision Pro will <em>never</em> become a suitable alternative for my office workstation with dual Apple XDR 6K displays. At 32 inches per-monitor and a total of 40 million pixels (with pixel density of 218 PPI and angular resolution &gt;100 PPD) and without a weight around my head, it is simply not a bar I would hold a VR headset against today or at any point in the future.</p>



<p>The more interesting question to focus on is whether Vision Pro could even begin to look like a suitable replacement for one or more 27-inch Apple Studio Displays (or equivalent). The short answer today is that Vision Pro can indeed come close (or very close) to that, but there are some important limitations that Apple needs to address to make this a relatively frictionless use case:</p>



<ul>
<li><strong>Lack of dual (or triple) monitor support</strong> is a huge bummer despite the fact that there are decent reasons for it (largely related to requiring a lot of local Wi-Fi bandwidth). Even though Vision Pro has a relatively narrow field of view that still feels like looking through binoculars, if I could get two or three virtual monitors out of a MacBook Air for example, things would start to look more interesting.</li>



<li><strong>Inconsistent keyboard and trackpad behaviors</strong> makes it very hard to switch back and forth between iPad/Vision apps and the Mac virtual display. I constantly find myself looking for my cursor, seeing the virtual keyboard pop up onscreen when clearly I donâ€™t need it (if Iâ€™m using a physical keyboard), not to mention that I cannot use my beloved Logitech MX mouse.</li>



<li><strong>There is no reliable workspace persistency</strong> which is exactly the same issue I discussed when talking about the iPad Pro use case in the previous section. This should be an easy fix.</li>



<li><strong>Eye tracking doesnâ€™t work in MacOS</strong> which not only leads to inconsistent input modalities as I said above but also just feels like a huge missed opportunity to offer a magical capability that MacOS has never seen before. This is not a low-hanging bug fix, but I donâ€™t see it as a huge technical leap for Apple if the MacOS team wants to address it.</li>



<li><strong>MacOS apps are â€œstuckâ€ inside the virtual monitor</strong> instead of being allowed to move around the entire space. This is another missed opportunity to deliver a truly spatial/immersive experience with Vision Pro, though significantly more complicated for Apple to address and would require really careful engineering by both MacOS and visionOS teams.</li>
</ul>



<p>Many of the issues I highlighted above are straightforward software challenges that Apple is well equipped to address and would make a world of difference. I suspect itâ€™s a matter of dealing with the usual internal politics/collaboration challenges getting the MacOS team to dedicate the necessary resources to address bugs and feature requests from the visionOS team.</p>



<p>The bottom line for me is that <strong>we can see a relatively near future where carrying a MacBook Air and a Vision Pro in your backpack could give you a reasonably good workstation</strong>, one that delivers enough benefits in the form of productivity gains that you might be willing to wear a headset for a few hours in a cafÃ©, on an airplane, or even on your couch at home. (This perspective is of course made in complete absence of value-for-money considerations).</p>



<p>Unsurprisingly, this is Appleâ€™s strongest hand with the Vision Pro launch as itâ€™s 100% controlled by them and uniquely leverages the existing Apple ecosystem. Yes, itâ€™s a very uninspiring and unimaginative use case, but it might be powerful enough for Apple to move a lot of headsets.</p>



<h2><strong>Watching movies in Vision Pro is great at first but most people will stop doing it after the initial novelty excitement wears off</strong></h2>



<p>Watching TV/movies in virtual reality seemed like such an incredibly compelling idea that we (the Oculus team at Meta/Facebook) built an entire product around that idea â€” <em>Oculus Go</em>. Launched in 2018, Oculus Go was the biggest product failure Iâ€™ve ever been associated with for the simple reason that it had extremely low retention despite strong partnerships with Netflix and YouTube. <strong>Most users who bought Oculus Go completely abandoned the headset after a few weeks.</strong> The full story is much more nuanced (including the fact that the Oculus Go failure got us on the path to Oculus Quest very quickly), but it taught us an important lesson.</p>



<p>The lesson we learned is that watching traditional (rectilinear) TV or movies in VR feels incredibly compelling at first, but the novelty wears off for most people after a few weeks. The reasons are:</p>



<ul>
<li><strong>Itâ€™s just not physically comfortable</strong> compared to watching TV or movies on an TV, tablet, or laptop, primarily because of the pressure on your head and face, plus the fact that you canâ€™t comfortably sit in any position or lie down while wearing with the headset</li>



<li><strong>Thereâ€™s a lot of friction </strong>to start watching a video in a VR headset if youâ€™re not already in VR â€” frequently a lot more steps required (especially finding and putting on the headset) and a more cumbersome navigation UI compared to our other devices</li>



<li><strong>Itâ€™s socially isolating and lonely</strong> to watch videos in VR, which will be a deal breaker for many people (although definitely not all)</li>
</ul>



<p>Back in the Oculus Go days, we concluded rather quickly that media consumption in VR is simply not a core â€œdaily driverâ€ pillar but more an ancillary use case that adds some value to other core pillars (such as productivity or gaming).</p>



<p>Vision Pro does bring more to the table with a much better display than previous VR headsets which can create magical movie experiences on occasion. For instance, watching an animated Disney or Pixar movie in 3D is absolutely stunning. But the essential product-market fit challenge remains:  </p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> VR is simply not a medium people will gravitate towards for watching 2D media on a regular basis. Adding to this all of the Vision Proâ€™s </mark><mark>comfort and friction issues, most people who get excited about watching media in the headset will eventually find themselves going back to their TV, tablet or laptop as their primary devices for video.</mark></p>
</blockquote>



<p>Watching 3D movies on Vision Pro is a fun entertainment experience, but these videos are â€œboxedâ€ and donâ€™t feel anything like witnessing real life. With the Vision Pro, Apple launched its new <em>Apple Immersive</em> video format, which opens the door for a new class of entertainment.</p>



<h2><strong>Apple Immersive Video opens a new world of possibilities for media in VR â€” but its <em>hyperrealism</em> may bring an unexpected <em>uncanny valley challenge</em></strong></h2>



<p>One of the big original bets we made at Facebook/Meta with the launch of <em>Oculus Go</em> in 2018 was that immersive 180-degree video would attract a massive amount of consumer interest and that this would somehow trigger a chain reaction in the world of entertainment. We were able to secure partnerships with a small number of media companies who had become specialized in capturing VR video early on, and we were off to the races.</p>



<p>Our initial excitement cooled off quickly. VR180 video quality on Oculus Go was decent but flat, washed out and far from amazing mostly due to low resolution. These videos didnâ€™t create a true sense of presence, of feeling transported to another reality. And most of the content was of one-off nature, with no real franchises that would have people coming back for more (with the exception of sports, which failed initially for other reasons that Iâ€™ll come back to later).</p>



<p>Within a year, the Oculus team pivoted to VR gaming and stopped investing in immersive video almost completely.</p>



<p>In 2020, Apple acquired <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener"><strong>NextVR</strong></a>, one of the small but highly respected companies we had been working with as they were edging into bankruptcy (we passed on the acquisition at Meta/Oculus). NextVR had spent over a decade building and perfecting VR 180 camera technology and production pipelines for broadcast-quality video. <a href="https://www.youtube.com/@Nextvr" target="_blank" rel="noreferrer noopener">The NextVR YouTube channel</a> is still live and provides amazing examples of what became possible with their technology <em>(make sure to pan around using your mouse/finger while watching videos in their YT channel)</em>.</p>



<figure><img data-attachment-id="102" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_camera/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png" data-orig-size="1200,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_camera" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" loading="lazy" width="1024" height="682" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_camera.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The latest publicly displayed NextVR 180-3D camera in 2018 (Source: <a href="https://www.theverge.com/2018/1/8/16862048/nextvr-six-degrees-of-freedom-augmented-reality-events-ces-2018" target="_blank" rel="noreferrer noopener">The Verge</a>)</figcaption></figure>



<p>The NextVR acquisition is what led to the incredibly <strong>Apple Immersive </strong>video format, which enables capture of <em>3D video in 180 degrees in 8K resolution at 90 frames per second</em>, an absolute juggernaut format with 8 times the number of pixels of a regular 4K video. <strong>The best way to think of the new <em>Apple Immersive</em> video format is kind of like a new IMAX-3D</strong>, but the real magic is the fact that itâ€™s projected inside an imaginary 180-degree sphere (horizontally <em>and</em> vertically) that takes over your entire field of view.</p>



<p>Vision Pro is the first VR headset that enables playback of 180-degree 3D video at what feels to the eyes like 4K quality. At launch, there are four Apple TV short films on Vision Pro shot in Apple Immersive video format. My absolute favorite of these films â€” <em>Adventure</em> â€” is a jaw-dropping cinematic piece that is likely to win its share of movie awards. Experiencing the Norwegian fjords with this level of immersion is completely breathtaking, so much so that it might be my favorite experience so far in Vision Pro. I have never felt transported to another place in this manner in any experience Iâ€™ve ever had, anywhere, period.</p>



<figure><img data-attachment-id="112" data-permalink="https://hugo.blog/2024/03/11/vision-pro/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post-jpg-slideshow_large/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg" data-orig-size="1960,1104" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/011623_immersive_originals_storytellers_apple_vision_big_image_03_big_image_post.jpg.slideshow_large.jpg 1960w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Adventure</strong> is one of the short films shot in Apple Immersive format released with the Vision Pro launch</figcaption></figure>



<p>My second favorite Apple Immersive video â€” <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> â€” is a super fun and intimate concert that really makes you feel what presence in VR could be like with another human. While itâ€™ll be fun for nearly everyone to see Alicia Keys in a close-up VR performance, it may not be nearly as happy and inspiring to see a human in close proximity in other situations.</p>



<blockquote>
<p><mark><strong>MY TAKE:</strong> The super high-fidelity <mark>Apple Immersive </mark>video format will run into an unexpected and significant â€œuncanny valleyâ€ challenge as a consequence of its <em>hyperrealism</em>. Seeing someone right in such close proximity to you and in such high fidelity may feel cool to one person but will feel uncomfortable or overwhelming to others. Less so in a scene like this intimate music concert or sports game, but probably a lot more so in dramatic storytelling and other types of more realistic films.</mark></p>
</blockquote>



<p>Back in the Oculus days, we used to run experiments to try and really understand which lines could not be crossed in VR content to avoid people feeling overwhelmed or even unsafe. One of the findings in these experiments was that too much realism and fidelity could be one of the things that crosses a line. In other words, <em>hyperrealism</em> could quickly drag people into the <em>uncanny valley</em>, one of two places we always want to avoid in VR (the other place is motion sickness).</p>



<figure><img data-attachment-id="139" data-permalink="https://hugo.blog/2024/03/11/vision-pro/alicia/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="alicia" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/alicia.jpeg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Many people will feel themselves crossing the uncanny valley while<br>watching <em><a href="https://www.youtube.com/watch?v=d555q5vaYns" target="_blank" rel="noreferrer noopener">Alicia Keys: Rehearsal Room</a></em> on Vision Pro</figcaption></figure>



<p>Navigating this creative challenge will take time and a lot of experimentation on Appleâ€™s side, and theyâ€™re the one company in the world we can trust to have the level of sensitivity and artistry for this journey, not to mention the ability to hire the best of the best talent. Practically, it probably means we can expect to see beautiful experiential films in Apple Immersive format exploring topics such as beautiful landscapes, wildlife, travel and music, but are less likely to see deep human storytelling with people in close proximity to the camera (which is typical of nearly all traditional filmmaking).</p>



<p>Luckily for Apple, there is one category where hyperrealism is much less likely to be an issue especially for hardcore fans â€” <strong><em>Live Sports</em></strong>.</p>



<h2><b><em>Live sports</em> will be Appleâ€™s secret weapon to sell a huge number of Vision Pro headsets to hardcore fans â€” but itâ€™s going to be a long &amp; expensive journey</b></h2>



<p>One of the original Oculus Go 30-second TV commercials featured an <a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">NBA courtside banter between Adam Levine and Jonah Hill</a> wearing the Oculus headset while watching a live game together in VR (each sitting in their own physical living room):</p>



<figure><img data-attachment-id="121" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_nba/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png" data-orig-size="3476,1844" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_nba" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" loading="lazy" width="1024" height="543" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_nba.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Screenshot from an original <em>Oculus Go</em> TV commercial (<a href="https://www.youtube.com/watch?v=SUdJt_3i0Us" target="_blank" rel="noreferrer noopener">full video</a>)</figcaption></figure>



<p>This TV commercial did extremely well, drove a significant amount of Oculus Go sales (after all, that headset only cost $199) and definitely showed that we were on to something potentially quite powerful with hardcore sports fans. But as I explained in the previous section, we did not manage to bring it to reality in a way that would meet expectations.</p>



<p>In the end, our team at Oculus completely failed to realize the opportunity of redefining the sports audience experience through VR for a number of reasons, but primarily because we just didnâ€™t have the patience to develop that market. We were unable to build the necessary industry support with sports leagues and broadcast right holders initially, so we stopped trying and the VR sports segment nearly died. There are small efforts on Quest today such as <a href="https://xtadiumvr.com/" target="_blank" rel="noreferrer noopener"><em>Xtadium</em></a> and <a href="https://www.oculus.com/vr/6525150070834263/" target="_blank" rel="noreferrer noopener"><em>Meta Horizons</em></a>, but the quality of the experience and the limited live content make it all too insignificant to matter. To date, nobody ever really tried hard enough to create this market.</p>



<p>Apple has the opportunity to completely change this, for a few reasons:</p>



<ul>
<li><strong>Apple Immersive on Vision Pro is a transformative experience</strong> in terms of video quality and its ability to deliver a real sense of presence. Watching a game in high-resolution VR has the <em>potential</em> to be legitimately better than a regular 4K TV broadcast by enabling hardcore fans to feel much closer to the action.</li>



<li><strong>Apple has VR broadcast expertise</strong> with its acquisition of NextVR, and could have been painstakingly building a robust production pipeline for live 8K video, which is a tall technical challenge requiring non-trivial investment and specialized talent.</li>



<li><strong>Apple is already active in the sports broadcast rights world</strong> through their existing MLS license and several other rumored conversations that may lead to Apple buying a lot more broadcast rights to continue to strengthen Apple TV (ex. English Premier League, Formula 1).</li>
</ul>



<p>The first place where Apple will likely explore using Apple Immersive and Vision Pro for a live broadcast is Major League Soccer in the US. Their recent announcement is a strong indicator this is likely coming in late 2024 or early 2025 (to continue building momentum for Vision Pro):</p>



<blockquote>
<p><em>Coming soon, all Apple Vision Pro users can experience the best of the 2023 MLS Cup Playoffs with <strong>the first-ever sports film captured in Apple Immersive Video</strong>. Viewers will feel every heart-pounding moment in 8K 3D with a 180-degree field of view and Spatial Audio that transports them to each match. </em></p>
<cite><a href="https://www.apple.com/newsroom/2024/02/2024-mls-season-kicks-off-today-exclusively-on-mls-season-pass-on-apple-tv/" target="_blank" rel="noreferrer noopener">Apple Press Release â€“ February 2024</a></cite></blockquote>



<figure><img data-attachment-id="110" data-permalink="https://hugo.blog/2024/03/11/vision-pro/nextvr_mls/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp" data-orig-size="1500,750" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nextvr_mls" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" loading="lazy" width="1024" height="512" src="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/nextvr_mls.webp 1500w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Apple will likely use MLS as a testing ground for developing Apple Immersive live broadcasts</figcaption></figure>



<p>Beyond MLS (where Apple already has a long-term agreement and the ability to basically do anything), it will take a significant amount of time and money for Apple to strike the necessary agreements with the main sports leagues (NBA, NFL, MLB, Premier League etc) to enable this kind of immersive broadcast experience. That said, this is likely only a matter of time, as the opportunity to rethink audience sports is large enough that it would matter a lot even to a multi-trillion dollar company like Apple.</p>



<blockquote>
<p><mark><strong>MY TAKE: </strong>Just to put things in perspective, prices of tickets for watching live sports (in the actual venue) have been going steadily up and are now in the $100s even for average to bad seats, with premium tickets easily going into the $1000s (<a href="https://www.cbsnews.com/news/how-much-super-bowl-2024-tickets-prices/" target="_blank" rel="noreferrer noopener"><strong>the cheapest SuperBowl ticket in 2024 was around $2,000 at face value</strong></a>). The business case for a high-quality immersive â€œcourtsideâ€ experience on Vision Pro is almost unquestionably very strong.</mark></p>
</blockquote>



<p>There are two major aspects Apple will have to nail in order to successfully monetize this opportunity, both of which will require a lot of design, engineering and experimentation:</p>



<ul>
<li><strong>Live sports are very social</strong>, which means Apple will have to invest heavily in delivering a co-watching experience that works equally well for people who are physically in the same room or virtually co-located, and which feels as natural as casually watching a game sitting on the couch with your family or at a bar with your friends.</li>



<li><strong>The experience bar will be very high</strong>, which means Apple will have to really customize every aspect of the experience to the nature of each sports to make it better than watching a game on a large 4K television â€” including camera angles, special replays, birds-eye visualizations, analysis overlays, game stats etc.</li>
</ul>



<p>This is a massive canvas for innovation, and it will take several generations of Vision Pro to get there. Iâ€™m optimistic and, speaking from the position of having been part of a team that really tried to go after this opportunity, I really believe this is one of those things where â€œit takes an Appleâ€ to change the game (pun very much intended!).</p>



<figure><img data-attachment-id="137" data-permalink="https://hugo.blog/2024/03/11/vision-pro/xtadium_tennis/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png" data-orig-size="1024,591" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="xtadium_tennis" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" loading="lazy" width="1024" height="591" src="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png 1024w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/xtadium_tennis.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Watching live tennis on Xtadium app on Quest: <br>multiple cameras to choose from <em>plus</em> simultaneous TV broadcast on giant virtual floating screen</figcaption></figure>



<figure><img data-attachment-id="142" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_golf/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_golf" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" loading="lazy" width="1024" height="576" src="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=300 300w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp?w=768 768w, https://hugobarracom.files.wordpress.com/2024/02/visionpro_golf.webp 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>PGA app on Vision Pro: <br>birds-eye view of a 3D model of the course and ability to track shots of a recorded prior match</figcaption></figure>



<figure><img data-attachment-id="174" data-permalink="https://hugo.blog/2024/03/11/vision-pro/visionpro_f1/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png" data-orig-size="3448,1910" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="visionpro_f1" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" loading="lazy" width="1024" height="567" src="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=2048 2048w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/visionpro_f1.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Concept of a Formula 1 mixed reality broadcast by viz artist John LePore (Source: <a href="https://www.youtube.com/watch?v=y9FpgxNeWJk" target="_blank" rel="noreferrer noopener">YouTube</a>):<br>birds-eye track view, main broadcast on giant floating screen, multi-camera access, live telemetry</figcaption></figure>



<h2>Conclusions<strong> </strong></h2>



<h2><strong>Why I returned my Vision Pro, and my wish list for what Apple could do to fix &amp; improve the product</strong> </h2>



<p>As a â€œproduct guyâ€, I usually force myself to behave like a real consumer making real trade-offs as much as I possibly can. I believe that always putting myself in the userâ€™s shoes is an important part of what I do not just for my own products but also for products built by other people. I admit Vision Pro is the ultimate tech toy, but since Iâ€™m not an active developer I canâ€™t justify the $4,049.78 price tag (512GB model + California sales tax) simply for keeping up with the VR market, so I returned my Vision Pro for a full refund inside the 14-day return window.</p>



<figure><img data-attachment-id="201" data-permalink="https://hugo.blog/2024/03/11/vision-pro/vision-pro_return/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png" data-orig-size="2018,1046" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vision-pro_return" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" loading="lazy" width="1024" height="530" src="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=1024 1024w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=300 300w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png?w=768 768w, https://hugobarracom.files.wordpress.com/2024/03/vision-pro_return.png 2018w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>In Appleâ€™s journey of <em>product-market fit</em> in VR, the Vision Pro has a long way to be able to deliver true retention. Appleâ€™s high-risk decision to completely exclude immersive VR games from the Vision Pro app store â€” <em>plus</em> their inexplicable failure to create exciting momentum by not having high-quality AR apps at launch â€” donâ€™t leave them with many options to deliver user value in the near term to non-developers.</p>



<p>The only low-hanging fruit is to make productivity really good, which despite being incredibly unimaginative and dull, should be one of Appleâ€™s biggest focus in the next iterations of visionOS. I donâ€™t discard the possibility of once again owning a 1st-gen Vision Pro in the future once Apple addresses all of the friction issues I uncovered and shared above. </p>



<p>During the 2 weeks of my Vision Pro experience, I accumulated a very long list of bug fixes and feature requests based on 2 weeks of Vision Pro usage. Iâ€™m going to share my Top 10 here:</p>



<ol>
<li><strong>Make productivity use cases frictionless </strong>first of all by closing the gap with developers to bring essential iPad apps to Vision Pro // fix text input &amp; editing and make it seamless // add support for 2 (and ideally 3) MacOS remote displays // add workspace window persistency // build â€œspatial Mission Controlâ€ and enforce a minimal recommended focal distance</li>



<li><strong>Have developers build amazing AR games</strong> and do everything possible to set a really high quality bar &amp; reward their creativity // add SharePlay support with <em>Personas</em> and really push for multi-player support, enabling people to be <em>and</em> play together.</li>



<li><strong>Improve passthrough mode</strong> to the extent that the hardware sensor stack allows, ideally reducing motion blur, improving white balance, and making seeing your hands more seamless (when viewing immersive content)</li>



<li><strong>Create workspace spatial persistency</strong> and allow me to configure different rooms in my home or office in such a way that Vision Pro always remembers my room-specific configurations</li>



<li><strong>Make 3D widgets &amp; objects first-class citizens </strong>in visionOS and enable people to decorate their homes &amp; offices persistently</li>



<li><strong>Let people bring their iPhone into VR</strong> by simply looking at the device (like the MacOS virtual display feature) and then getting a floating panel that they can place anywhere in their space â€” this will work wonders in reducing FOMO while in VR</li>



<li><strong>Add a <em>Guest mode</em> so anyone can give the Apple in-store demo</strong> and make it possible for Vision Pro users to â€œspread the loveâ€ â€” thereâ€™s nothing more magical than giving someone their first VR demo</li>



<li><strong>Add Persona support to SharePlay </strong>for watching video to so people can actually feel like theyâ€™re together â€” VR has a bad reputation of loneliness &amp; isolation, so making VR social must be a priority even though few people will use social features at first. There arenâ€™t enough people with Vision Pro for this to be a practical use case today, but itâ€™s important for Apple to set the right tone.</li>



<li><strong>Launch tons of beautiful environments</strong> ideally with a steady frequency, taking a page out of the Apple TV screensaver playbook â€” and include beautiful indoor environments as well (not just landscapes)</li>



<li><strong>(Lastlyâ€¦) Find a way to let people play immersive VR games</strong> by implementing <a href="https://en.wikipedia.org/wiki/OpenXR" target="_blank" rel="noreferrer noopener">OpenXR</a> support, forming a partnership with <em>SteamVR</em> or simply opening up visionOS a bit to allow VR developers and enthusiasts to build compatibility themselves</li>
</ol>
</div>



<h2><strong>ONE MORE THING: (1) Why Metaâ€™s Android moment is finally here, (2) My unsolicited product advice for <em>Quest Pro 2</em> and beyond</strong></h2>



<p>As I said at the beginning of this essay, while working at Meta/Oculus I used to semi-seriously joke that the best thing that could ever happen to us was having Apple enter the VR industry. One of the main reasons for me to say this was that I knew Apple would do the best job of any company making people really <em>want</em> VR through its unparalleled brand, design and marketing. Oculus co-founder Palmer Luckey puts it best:</p>







<blockquote>
<p><em>â€œVR will become something everyone wants before it becomes something everyone can afford.â€</em></p>
<cite><a href="https://twitter.com/palmerluckey/status/679907802962214912" target="_blank" rel="noreferrer noopener">Palmer Luckey, 2015 tweet</a></cite></blockquote>



<p>For Meta, the Vision Pro launch is the best marketing tool for Quest VR that the company could have dreamed of but could have never achieved on its own, for a few reasons:</p>



<ul>
<li><strong>It elevates VR to a level of mainstream consumer curiosity</strong> and breaks away from gamer and VR enthusiast niches; in media coverage alone the Vision Pro probably had 1,000x more reach than any Oculus/Quest launch in history</li>



<li><strong>It sets a new experience gold standard for VR</strong> especially by pushing the existing boundaries in display resolution and creating a new paradigm of â€œUI magicâ€ with gaze &amp; pinch which may be an instant defacto standard</li>



<li><strong>It establishes a pricing envelope</strong> that enables Meta to break away from the $500 price point that Quest has been stuck in, and specifically allows them to ship a Quest Pro 2 headset priced at a $1,000 to $1,500 (but likely not higher) without being completely rejected by consumers</li>



<li><strong>It creates a formidable competitor</strong> for Meta teams to maniacally chase and will almost certainly force the company to move with a much greater sense of urgency internally (which would be a great outcome as friends on the inside are constantly complaining Meta Reality Labs moves too slowly)</li>
</ul>



<p><strong><em>What should Meta do in response to the Vision Pro launch?</em></strong></p>



<p>In order to really seize this moment and opportunity created by the Vision Pro launch, Meta needs to ensure it ships a VR headset by mid-2025 that both builds on the new experience gold standard created by the Vision Pro <em>and </em>is objectively a better product across as many dimensions as possible. It is imperative for Meta not to repeat the inexplicable debacle that was the <em>Quest Pro</em> launch in 2022.</p>



<p>I put together my own <strong>Top 10</strong> <strong>wish list for <em>Quest Pro 2</em></strong>:</p>



<ol>
<li><strong>Double down investment in micro-OLED</strong> as itâ€™s likely the only way to achieve display resolution at or near Vision Pro; I suspect this may be exactly what the <a href="https://www.roadtovr.com/meta-lg-xr-partnership/" target="_blank" rel="noreferrer noopener">recently announced LG partnership</a> is about</li>



<li><strong>Build an ergonomic headset</strong> <strong>that can be worn for 2-4 hours</strong> without causing any major discomfort issues; ideally offering two battery options: (1) a head-strap with a built-in battery in the back of the head, and (2) a wired pack (like the Vision Pro) that moves the battery off the head and reduces the headset weight to below 500 grams while increasing energy capacity. </li>



<li><strong>Deliver better passthrough than Vision Pro</strong> by dramatically improving Quest 3â€™s latency and distortion correction and improving upon all of the Vision Pro passthrough issues â€” ensure no perceivable motion blur, high dynamic range, accurate white balance</li>



<li><strong>Take Appleâ€™s gaze+pinch UI to the next level</strong> by productizing all of the amazing research on hand tracking done at Meta (ex: Rob Wangâ€™s super talented group) to enable fine-grained gestures such as scrolling and D-pad selection by detecting small finger movements solely via camera input (this is <em>not</em> the CTRL Labs stackâ€¦ thatâ€™s for the future)</li>



<li><strong>Partner with Microsoft to make Windows computers 1st class citizens</strong> in Quest Pro 2 and enable advanced desktop productivity use cases that go well beyond virtual monitors (ex: make it possible to take any window and place it in space)</li>



<li><strong>Launch Android 2D tablet apps natively on Quest</strong> to match the Vision Pro iPad compatibility library either by partnering with Google to license <em>Play Store</em> (which <a href="https://www.theinformation.com/articles/meta-rebuffed-google-proposal-for-a-vr-and-ar-tie-up">seems unlikely these days</a> though I still believe Ash Jhaveri and Hiroshi could work together to pull it off) or just build a curated tablet app store directly (which we had considered in the past at Oculus but passed on)</li>



<li><strong>Launch human-like avatars with Quest Pro 2</strong> by productizing Metaâ€™s mind-blowing <a href="https://www.uploadvr.com/meta-codec-avatars-might-be-coming-to-quest/" target="_blank" rel="noreferrer noopener">Codec Avatars technology</a>, likely one of the VR research areas that has received the most R&amp;D dollars for the last 7+ years, used by <a href="https://youtu.be/MVYrJJNdrEg?si=EIZJ_Op4xRY9PD-h&amp;t=605" target="_blank" rel="noreferrer noopener">Lex Friedman in his interview with Mark Zuckerberg in late 2023</a></li>



<li><strong>Launch high-definition room scanning and unlock teleportation</strong> using technology that has existed within Oculus Research for several years now; it is time for Meta to make this future a reality where people can be remote but <em>feel</em> truly present by visiting each otherâ€™s home, office or favorite place</li>
</ol>



<h2>Appendix: Other Fun Things</h2>



<h2><strong>Despite all of its hardware insanity the Vision Pro display is still a far cry from a VR retina display (and may never get there)</strong></h2>



<p>As noted above the Vision Pro display delivers insane pixel density at over 3,000 PPI (compared to 500 PPI for the highest resolution smartphones), but because the panel is so close to our eyes, it still doesnâ€™t come even close to the resolution it would need to have in order to qualify as a <em><a href="https://en.wikipedia.org/wiki/Retina_display">retina display</a></em>.</p>



<p>A retina display, by <a href="https://www.kybervision.com/Blog/files/AppleRetinaDisplay.html" target="_blank" rel="noreferrer noopener">Appleâ€™s definition</a>, is a display with a high enough resolution that the human eye cannot resolve individual pixels. Because different devices are used at varying distances from the eye, there is no single PPI (pixels per inch) standard for retina across all device categories. Instead, itâ€™s useful to look at PPD (pixels per degree), which is a measure of angular resolution independent of viewing distance, or specifically the number of horizontal pixels per degree of <em>viewing angle</em>. See <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">this article from SimulaVR</a> for an excellent explanation, and hereâ€™s an image from that article:</p>







<figure><img data-attachment-id="74" data-permalink="https://hugo.blog/2024/03/11/vision-pro/ppd/" data-orig-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png" data-orig-size="460,286" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ppd" data-image-description="" data-image-caption="" data-medium-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300" data-large-file="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" loading="lazy" width="460" height="286" src="https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=460" alt="" srcset="https://hugobarracom.files.wordpress.com/2024/02/ppd.png 460w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=150 150w, https://hugobarracom.files.wordpress.com/2024/02/ppd.png?w=300 300w" sizes="(max-width: 460px) 100vw, 460px"><figcaption>Source: <a href="https://simulavr.com/blog/ppd-optics/" target="_blank" rel="noreferrer noopener">SimularVR</a></figcaption></figure>



<p>A human eye with 20/20 vision has a resolution of 60 PPD. This means very specifically that we can resolve 60 pixels per each 1 degree of viewing angle (or 1 pixel per arc minute, which is 1/60th of a degree). The Vision Pro has an angular resolution of 34, which is 1/3 more than the Meta Quest 3 but still very far away from the 60 PPD weâ€™d need for a retina-quality display.</p>



<p>Achieving angular resolution anywhere near 60 PPD in a VR headset is likely not possible with any technology in the mid-term horizon.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TextSnatcher: Copy text from images, for the Linux Desktop (298 pts)]]></title>
            <link>https://github.com/RajSolai/TextSnatcher</link>
            <guid>39711621</guid>
            <pubDate>Fri, 15 Mar 2024 02:57:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/RajSolai/TextSnatcher">https://github.com/RajSolai/TextSnatcher</a>, See on <a href="https://news.ycombinator.com/item?id=39711621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://wiki.gnome.org/Projects/Vala" rel="nofollow"><img src="https://camo.githubusercontent.com/b7b96bad0bedef8e14b37aa18f8559eb09fae075833a1a640ae2d3af86a6d49f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d616465253230576974682d56616c612532302d413536444532" alt="Vala Programming language" data-canonical-src="https://img.shields.io/badge/Made%20With-Vala%20-A56DE2"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg"><img src="https://github.com/RajSolai/TextSnatcher/actions/workflows/flatpak-build.yml/badge.svg" alt="Flatpak Build workflow"></a></p>
<div dir="auto">
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/RajSolai/TextSnatcher/blob/master/data/icons/com.github.rajsolai.textsnatcher.svg"><img src="https://github.com/RajSolai/TextSnatcher/raw/master/data/icons/com.github.rajsolai.textsnatcher.svg" height="110px"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">TextSnatcher</h2><a id="user-content-textsnatcher" aria-label="Permalink: TextSnatcher" href="#textsnatcher"></a></p>
<p dir="auto">Copy Text from Images with ease, Perform OCR operations in seconds.</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img alt="TextSnatcher OCR App for Linux" src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"></a><br>
<a href="https://www.producthunt.com/posts/textsnatcher?utm_source=badge-featured&amp;utm_medium=badge&amp;utm_souce=badge-textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/461f8889a0b1219ad9051d64231614176a85e2eb03b4b06a8770180fd8831e2b/68747470733a2f2f6170692e70726f6475637468756e742e636f6d2f776964676574732f656d6265642d696d6167652f76312f66656174757265642e7376673f706f73745f69643d333434343031267468656d653d6c69676874" alt="TextSnatcher - How to copy text from images, answer is TextSnatcher | Product Hunt" width="250" height="54" data-canonical-src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=344401&amp;theme=light"></a>
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<div dir="auto">
  <p><a href="https://flathub.org/apps/details/com.github.rajsolai.textsnatcher" rel="nofollow"><img width="240" alt="Download on Flathub" src="https://camo.githubusercontent.com/1a343de10fc46d1a887a0640899212590da30f4d415ce2caf85e4462bd95a3b8/68747470733a2f2f666c61746875622e6f72672f6173736574732f6261646765732f666c61746875622d62616467652d692d656e2e706e67" data-canonical-src="https://flathub.org/assets/badges/flathub-badge-i-en.png"></a></p><p dir="auto"><a href="https://appcenter.elementary.io/com.github.rajsolai.textsnatcher" rel="nofollow"><img src="https://camo.githubusercontent.com/02fb9c2fbb1ea0024a489c44798c28f84ba7149a8d144a20a8a5a28debd1e293/68747470733a2f2f61707063656e7465722e656c656d656e746172792e696f2f62616467652e737667" alt="Get it on AppCenter" data-canonical-src="https://appcenter.elementary.io/badge.svg"></a></p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Multiple Language Support.</li>
<li>Copy Text from images with a Drag.</li>
<li>Drag over any Image and Paste.</li>
<li>Fast and Easy to Use.</li>
<li>This application uses the Tesseract OCR 4.x for the character
recognition.</li>
<li>Read more about <a href="https://tesseract-ocr.github.io/tessdoc/Home.html" rel="nofollow">Tesseract</a> and Star â­ï¸ <a href="https://github.com/tesseract-ocr/tesseract">Tesseract-Project</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screencasts</h2><a id="user-content-screencasts" aria-label="Permalink: Screencasts" href="#screencasts"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-eng.mp4">textsnatcher-eng.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921719-228485ba-0d37-4b01-864e-63a2792248b5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MTktMjI4NDg1YmEtMGQzNy00YjAxLTg2NGUtNjNhMjc5MjI0OGI1Lm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ4MmZjZDQ2ZmVlODU3MDY5NTg2ZDI2YTkzOGNjZjY1OGFmZmE4MTA3ZTAzNDAwNTg3N2Y4ZTY3N2MyZGQ3YzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.j0LbbGRiW-lVKdyCCb2HQo0TYTj_CfpHqm5TSWM-tqo" controls="controls" muted="muted">

  </video>
</details>

<details open="">
  <summary>
    
    <span aria-label="Video description textsnatcher-tamil.mp4">textsnatcher-tamil.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" data-canonical-src="https://private-user-images.githubusercontent.com/54436424/152921736-c9567c9d-0afa-4c09-8706-6b2a1b6b635a.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA0ODYzMDUsIm5iZiI6MTcxMDQ4NjAwNSwicGF0aCI6Ii81NDQzNjQyNC8xNTI5MjE3MzYtYzk1NjdjOWQtMGFmYS00YzA5LTg3MDYtNmIyYTFiNmI2MzVhLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzE1VDA3MDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA4ZTFhZTUyN2RkOGU4YTI4OWMzMTUxZDc2MjkxYzFjZjU4NTQ5NmNlMzU2ODU3YjIyMDc0MGNjNzA4OGE4NjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.eeFrGifG7N1bcXi_wZRrMhFlRHV9v-gTz0_INWoCg1A" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-default.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png"><img src="https://raw.githubusercontent.com/RajSolai/TextSnatcher/master/data/screenshots/snap-dark.png" alt="TextSnatcher OCR App for Linux"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support Me</h2><a id="user-content-support-me" aria-label="Permalink: Support Me" href="#support-me"></a></p>
<p dir="auto"><a href="https://www.buymeacoffee.com/rajsolai" rel="nofollow"><img src="https://camo.githubusercontent.com/cace41b0afc90c68d0207e2bd809ee121f9ff4f72ac032e8ced972aee7adbb23/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f76322f64656661756c742d79656c6c6f772e706e67" alt="Buy Me A Coffee" data-canonical-src="https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png"></a></p>
<p dir="auto"><a href="https://ko-fi.com/R6R7ABG0F" rel="nofollow"><img src="https://camo.githubusercontent.com/ce32b4940b9ebf361cfd346ba0582815846406854cd2f701c11a85cb21eaa939/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Social Media Posts</h2><a id="user-content-social-media-posts" aria-label="Permalink: Social Media Posts" href="#social-media-posts"></a></p>
<p dir="auto"><a href="https://www.linkedin.com/posts/solai085_linux-commentbelow-apple-activity-6826408004519374848-wxsw" rel="nofollow">LinkedIn Post on Why I created TextSnatcher</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dependencies</h2><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">Ensure you have these dependencies installed</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Runtime Dependency</h3><a id="user-content-runtime-dependency" aria-label="Permalink: Runtime Dependency" href="#runtime-dependency"></a></p>
<ul dir="auto">
<li>scrot</li>
<li>tesseract-ocr</li>
<li>tesseract language data
<a href="https://archlinux.org/packages/community/x86_64/tesseract" rel="nofollow">arch repos</a>
<a href="https://packages.debian.org/search?keywords=tesseract-ocr" rel="nofollow">debian repos</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Buildtime Dependency</h3><a id="user-content-buildtime-dependency" aria-label="Permalink: Buildtime Dependency" href="#buildtime-dependency"></a></p>
<ul dir="auto">
<li>granite</li>
<li>gtk+-3.0</li>
<li>gobject-2.0</li>
<li>gdk-pixbuf-2.0</li>
<li>libhandy-1</li>
<li>libportal-0.5</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install, build and run</h2><a id="user-content-install-build-and-run" aria-label="Permalink: Install, build and run" href="#install-build-and-run"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# clone repository
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
# cd to dir
cd TextSnatcher
# run meson
meson build --prefix=/usr
# cd to build, build and test
cd build
sudo ninja install &amp;&amp; com.github.rajsolai.textsnatcher"><pre><span><span>#</span> clone repository</span>
git clone https://github.com/RajSolai/TextSnatcher.git TextSnatcher
<span><span>#</span> cd to dir</span>
<span>cd</span> TextSnatcher
<span><span>#</span> run meson</span>
meson build --prefix=/usr
<span><span>#</span> cd to build, build and test</span>
<span>cd</span> build
sudo ninja install <span>&amp;&amp;</span> com.github.rajsolai.textsnatcher</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inspirations</h2><a id="user-content-inspirations" aria-label="Permalink: Inspirations" href="#inspirations"></a></p>
<ul dir="auto">
<li>ReadMe: <a href="https://github.com/alainm23/planner">https://github.com/alainm23/planner</a></li>
<li>Application Structure: <a href="https://github.com/alcadica/develop">https://github.com/alcadica/develop</a></li>
<li>TextSniper (MacOS Application)</li>
</ul>
<p dir="auto">Made with â¤ï¸ for Linux</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York Disbars Infamous Copyright Troll (392 pts)]]></title>
            <link>https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/</link>
            <guid>39710455</guid>
            <pubDate>Thu, 14 Mar 2024 23:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/">https://abovethelaw.com/2024/03/new-york-disbars-infamous-copyright-troll/</a>, See on <a href="https://news.ycombinator.com/item?id=39710455">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-post-content">
<p><img src="https://abovethelaw.com/uploads/2021/03/iStock-484137638-300x200.jpg" alt="Dictionary Series â€“ Ethics" width="300" height="200">For years, Richard Liebowitz ran a very successful operation mostly sending threatening letters to companies claiming that they had infringed upon copyrights held by his photographer clients. Under the best of circumstances itâ€™s a niche practice area thatâ€™sâ€¦ <em>kinda shady</em>. But Liebowitz gained a degree of infamy across a number of matters for high-profile missteps in cases that sparked the ire of federal judges. Now, finally, <a href="https://nycourts.gov/reporter/3dseries/2024/2024_01309.htm" target="_blank" rel="noopener">New York has disbarred him</a>.</p>
<p>Liebowitz wasnâ€™t alone in the copyright trolling practice. A number of entities scour the internet looking for photographs that they can claim are â€œunlicensedâ€ and demanding thousands of dollars to settle the matter knowing that between statutory damages for copyright infringement and the cost of litigation, most companies will just pay it. Many times, the photo in question actually is legally licensed through an agency like Getty Images, but the plaintiff photographer has, for whatever reason, pulled the image since the license was granted.</p>
<p>This runs the risk that some plaintiff might do this on purpose hoping to catch some legal licenseholder unawares and bank on the target just settling to avoid bringing any lawyers into the situation. Which is why, for example,&nbsp;a judge in one case cited by the disbarment opinion ordered Liebowitz â€œproduce to the defendant records sufficient to show the royalty paid the last three times that the picture at issue was licensed, and the number of times the picture was licensed in the last five years; if the picture was never licensed, the plaintiff was to certify that fact as part of the plaintiffâ€™s production.â€ In this case, Liebowitz â€œdid not timely produce the required royalty information to the defendantâ€ per the disbarment opinion.</p>
<p>Though most of the opinion describes more fundamental case management problems. From a case brought in 2017:</p>
<blockquote><p>The respondent stated under penalty of perjury that he did not and had never made a settlement demand in this matter. In fact, the respondent had sent the defendantâ€™s counsel an email in which the respondent proposed settling the matter for the sum of $25,000.</p></blockquote>
<p>And another case brought in 2017:</p>
<blockquote><p>On January 13, 2018, the respondent submitted a letter (hereinafter the January 13, 2018 letter) to the District Court, requesting an adjournment of the pretrial conference scheduled for January 19, 2018, and stating that the defendant â€œhad yet to respond to the complaintâ€ and that the plaintiff intended to file a motion for a default judgment. Judge Cote granted the request and ordered the motion for entry of default due on January 26, 2018.</p>
<p>The respondentâ€™s statement in his January 13, 2018 letter that the defendant â€œhad yet to respond to the complaintâ€ was false and misleading, and the respondent knew that it was false and misleading when he made it. The January 13, 2018 letter failed to advise the court of the months-long history of communication between the parties, beginning in July 2017, as mentioned above.</p></blockquote>
<p>From yet another matter:</p>
<blockquote><p>The plaintiff admitted in a deposition and in other documents that the Photograph had been previously published on numerous occasions. To prevent the defendants from learning that the plaintiff did not hold a valid registration, the respondent stonewalled the defendantsâ€™ requests for documents and information. The respondent also failed to comply with an order by Magistrate Judge Debra Freeman to obtain and produce Copyright Office documents to demonstrate a valid registration. After it came to light that the Photograph was not registered, and despite the record stating otherwise, the respondent argued, without evidence, that the lack of registration was merely a mistake.</p></blockquote>
<p>If thereâ€™s a lesson to take away from these and the many, many more examples included in the opinion, itâ€™s that copyright trolling outfits are largely unprepared for someone to push back on their demands. Firing off demand letters, memorializing boilerplate licensing agreements, and collecting cash is a tidy business model right up until a firm has to juggle hearings and discovery requests and experts and â€œnot committing perjury.â€</p>
<p>But perhaps the most bizarre story involves Liebowitz missing an April 12, 2019 hearing, explaining that his grandfather had passed. When Judge Seibel directed Liebowitz under penalty of contempt to furnish evidence or documentation regarding the date of his grandfatherâ€™s death, Liebowitz shot back that the order â€œlikely constitutes a usurpation of judicial authority or a breach of judicial decorum.â€</p>
<blockquote><p>On November 7, 2019, the respondent retained counsel to represent him in the contempt proceedings, and on November 11, 2019, the respondent sent a letter to Judge Seibel admitting that he failed to carry out his responsibilities to the District Court and to his adversary. The respondent also admitted that his grandfather died on April 9, 2019, and was buried that same day.</p></blockquote>
<p>Just. Wow. You know, â€œmy grandfather died this weekâ€ is something you can tell a court <em>before</em> a hearing and theyâ€™ll probably grant it. <a href="https://abovethelaw.com/2023/05/pregnant-dont-plan-on-practicing-before-ohios-supreme-court-any-time-soon/" target="_blank" rel="noopener">Itâ€™s not like youâ€™re asking to give birth or anything</a>. But to lie about it to the court and then keep doubling down isâ€¦ a choice.</p>
<p>And a poor one as it turns out:</p>
<blockquote><p>ORDERED that pursuant to 22 NYCRR 1240.13, the respondent, Richard P. Liebowitz, a suspended attorney, is disbarred, effective immediately, and his name is stricken from the roll of attorneys and counselors-at-lawâ€¦.</p></blockquote>
<p><a href="https://nycourts.gov/reporter/3dseries/2024/2024_01309.htm" target="_blank" rel="noopener">Matter of Liebowitz</a> [New York Courts]</p>
<hr>
<p><img src="https://abovethelaw.com/uploads/2016/11/Headshot-300x200.jpg" alt="Headshot" width="189" height="126"><strong><em><a href="http://abovethelaw.com/author/joe-patrice/" target="_blank" rel="noopener">Joe Patrice</a>&nbsp;is a senior editor at Above the Law and co-host of <a href="http://legaltalknetwork.com/podcasts/thinking-like-a-lawyer/" target="_blank" rel="noopener">Thinking Like A Lawyer</a>. Feel free to&nbsp;<a href="mailto:joepatrice@abovethelaw.com">email</a> any tips, questions, or comments. Follow him on&nbsp;<a href="https://twitter.com/josephpatrice" target="_blank" rel="noopener">Twitter</a>&nbsp;if youâ€™re interested in law, politics, and a healthy dose of college sports news. Joe also serves as a <a href="https://www.rpnexecsearch.com/josephpatrice" target="_blank" rel="noopener">Managing Director at RPN Executive Search</a>.</em></strong></p>
<p><a href="https://bit.ly/406FC2u" target="_blank">
<img src="https://abovethelaw.com/uploads/2023/06/Practice_Banner_600x250_D.png" alt="CRM Banner">
</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Getty Makes Nearly 88,000 Art Images Free to Use However You Like (402 pts)]]></title>
            <link>https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html</link>
            <guid>39710454</guid>
            <pubDate>Thu, 14 Mar 2024 23:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html">https://www.openculture.com/2024/03/the-getty-makes-nearly-88000-art-images-free-to-use-however-you-like.html</a>, See on <a href="https://news.ycombinator.com/item?id=39710454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg" alt="" width="1024" height="805" srcset="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-360x283.jpg 360w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-240x189.jpg 240w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-768x604.jpg 768w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-360x283.jpg 360w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-240x189.jpg 240w, https://cdn8.openculture.com/2024/03/07214151/e5d29650-11f8-4897-9540-54a9dd65b04f_1024-768x604.jpg 768w"></p>
<p>Since the J. Paul GetÂ­ty MuseÂ­um <a href="https://www.openculture.com/2013/08/the-getty-puts-4600-art-images-into-the-public-domain.html">launched its Open ConÂ­tent proÂ­gram back in 2013</a>, weâ€™ve been feaÂ­turÂ­ing their efforts to&nbsp;make their vast colÂ­lecÂ­tion of culÂ­turÂ­al artiÂ­facts freely accesÂ­siÂ­ble online. Theyâ€™ve released not just digÂ­iÂ­tized works of art, but also a great many <a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">art hisÂ­toÂ­ry texts</a> and <a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">art books in genÂ­erÂ­al</a>. Just this week, they announced an expanÂ­sion of access to their digÂ­iÂ­tal archive, in that theyâ€™ve made nearÂ­ly 88,000 images free to downÂ­load on their <a href="https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-r/" target="_blank" rel="noopener" data-saferedirecturl="https://www.google.com/url?q=https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-r/&amp;source=gmail&amp;ust=1709952228434000&amp;usg=AOvVaw3D3T98bAbjSN30Dt8L1t4o">Open ConÂ­tent</a>&nbsp;dataÂ­base under&nbsp;<a href="https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-y/" target="_blank" rel="noopener" data-saferedirecturl="https://www.google.com/url?q=https://newsletters.getty.edu/t/t-l-eyhxty-tltjujtrih-y/&amp;source=gmail&amp;ust=1709952228434000&amp;usg=AOvVaw22gWg7B6gaFWK4wfpyrGsj">CreÂ­ative ComÂ­mons Zero (CC0)</a>. That means â€œyou can copy, modÂ­iÂ­fy, disÂ­tribÂ­ute and perÂ­form the work, even for comÂ­merÂ­cial purÂ­posÂ­es, all withÂ­out askÂ­ing perÂ­misÂ­sion.â€</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg" alt="" width="802" height="1023" srcset="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg 802w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-282x360.jpg 282w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-188x240.jpg 188w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-768x980.jpg 768w" sizes="(max-width: 802px) 100vw, 802px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802.jpg 802w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-282x360.jpg 282w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-188x240.jpg 188w, https://cdn8.openculture.com/2024/03/07214152/e065fdf7-798e-47cd-ac4c-182a1995b94a_802-768x980.jpg 768w"></p>
<p>The GetÂ­ty sugÂ­gests that you â€œadd a print of your favorite Dutch still life to your gallery wall or creÂ­ate a showÂ­er curÂ­tain using the <a href="https://www.getty.edu/art/collection/object/103JNH"><em>IrisÂ­es</em></a> by Van Gogh.â€ But if you <a href="https://www.getty.edu/art/collection/search?open_content=true">search the open conÂ­tent in their archive yourÂ­self</a>, you can sureÂ­ly get much more creÂ­ative than that.</p>


<p>The porÂ­talâ€™s interÂ­face lets you search by creÂ­ation date (with a timeÂ­line graph stretchÂ­ing back to the year 6000 BC), mediÂ­um (from agate and alabaster to woodÂ­cut and zinc), object type (includÂ­ing paintÂ­ings, phoÂ­tographs, and sculpÂ­tures, of course, but also akroÂ­teÂ­ria, horse trapÂ­pings, and tweezÂ­ers), and culÂ­ture. The selecÂ­tion reflects the wide manÂ­date of the GetÂ­tyâ€™s colÂ­lecÂ­tion, which encomÂ­passÂ­es as many of the civÂ­iÂ­lizaÂ­tions of the world as it does the eras of human hisÂ­toÂ­ry.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg" alt="" width="1024" height="534" srcset="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-360x188.jpg 360w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-240x125.jpg 240w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-768x401.jpg 768w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-470x246.jpg 470w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-484x252.jpg 484w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg" data-srcset="https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024.jpg 1024w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-360x188.jpg 360w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-240x125.jpg 240w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-768x401.jpg 768w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-470x246.jpg 470w, https://cdn8.openculture.com/2024/03/07214154/265db2bc-e1c0-47d4-9991-db1e1da02c6d_1024-484x252.jpg 484w"></p>
<p>In the <a href="https://www.getty.edu/art/collection/search?open_content=true">GetÂ­tyâ€™s open-conÂ­tent archive</a>, youâ€™ll find&nbsp;ancient sculpÂ­ture from <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=greek+sculpture">Greece</a>, <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=roman+sculpture">Rome</a> and many othÂ­er parts of the world besides; a fragÂ­menÂ­tary oinochoe (that is, a wine jug) from third-cenÂ­tuÂ­ry-BC PtoleÂ­maÂ­ic Egypt; <span>lavÂ­ishÂ­ly illuÂ­miÂ­natÂ­ed </span><a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=book+of+hours">medieval books of hours</a><span> (of the kind </span><a href="https://www.openculture.com/2023/04/discover-the-medieval-illuminated-manuscript-les-tres-riches-heures-du-duc-de-berry.html">preÂ­viÂ­ousÂ­ly feaÂ­tured here on Open CulÂ­ture</a><span>); works by such innoÂ­vÂ­aÂ­tive French painters as <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=manet">Ã‰douard Manet</a> and <a href="https://www.getty.edu/art/collection/search?open_content=true&amp;q=degas">Edgar Degas</a></span>; the stereoÂ­scopÂ­ic phoÂ­togÂ­raÂ­phy of <a href="https://www.getty.edu/art/collection/person/104VP5">CarÂ­leton H. Graves</a>, who in the late nineÂ­teenth and earÂ­ly twenÂ­tiÂ­eth cenÂ­tuÂ­ry capÂ­tured places from DenÂ­mark and PalesÂ­tine, to Japan and Korea; the darÂ­ing abstracÂ­tions of artists like <a href="https://www.getty.edu/art/collection/object/10411Q">Hannes Maria Flach</a>, <a href="https://www.getty.edu/art/collection/object/106F7N">JaromÃ­r Funke</a>, and <a href="https://www.getty.edu/art/collection/object/106GE7">FranÂ­cis BruguiÃ¨re</a>. But what you do with them is, of course, entireÂ­ly up to you. Enter <a href="https://www.getty.edu/art/collection/search?open_content=true">the colÂ­lecÂ­tion here</a>.</p>
<p><strong>RelatÂ­ed conÂ­tent:</strong></p>
<p><a href="https://www.openculture.com/2019/01/the-getty-digital-archive-expands-to-135000-images.html#google_vignette">The GetÂ­ty DigÂ­iÂ­tal Archive Expands to 135,000 Free Images: DownÂ­load High ResÂ­oÂ­luÂ­tion Scans of PaintÂ­ings, SculpÂ­tures, PhoÂ­tographs &amp; Much Much More</a></p>
<p><a href="https://www.openculture.com/2021/03/a-search-engine-for-finding-free-public-domain-images-from-world-class-museums.html">A Search Engine for FindÂ­ing Free, PubÂ­lic Domain Images from World-Class MuseÂ­ums</a></p>
<p><a href="https://www.openculture.com/2016/06/enjoy-100000-free-art-art-history-texts-courtesy-of-the-getty-research-portal.html">100,000 Free Art HisÂ­toÂ­ry Texts Now AvailÂ­able Online Thanks to the GetÂ­ty Research PorÂ­tal</a></p>
<p><a href="https://www.openculture.com/2021/06/download-great-works-of-art-from-40-museums-worldwide.html">DownÂ­load Great Works of Art from 40+ MuseÂ­ums WorldÂ­wide: Explore Artvee, the New Art Search Engine</a></p>
<p><a href="https://www.openculture.com/2023/04/the-smithsonian-puts-4-5-million-high-res-images-online.html">The SmithÂ­sonÂ­ian Puts 4.5 MilÂ­lion High-Res Images Online and Into the PubÂ­lic Domain, MakÂ­ing Them Free to Use</a></p>
<p><a href="https://www.openculture.com/2018/12/download-325-free-art-books-getty-museum.html">DownÂ­load Over 325 Free Art Books From the GetÂ­ty MuseÂ­um</a></p>
<p><em>Based in Seoul,&nbsp;</em><em><a href="http://blog.colinmarshall.org/">ColÂ­in</a></em><em><a href="http://blog.colinmarshall.org/">&nbsp;M</a></em><em><a href="http://blog.colinmarshall.org/">a</a></em><em><a href="http://blog.colinmarshall.org/">rshall</a>&nbsp;writes and broadÂ­cas</em><em>ts on cities, lanÂ­guage, and culÂ­ture. His projects include the SubÂ­stack newsletÂ­ter</em>&nbsp;<a href="https://colinmarshall.substack.com/">Books on Cities</a>,<em>&nbsp;the book&nbsp;</em>The StateÂ­less City: a Walk through 21st-CenÂ­tuÂ­ry Los AngeÂ­les&nbsp;<em>and the video series&nbsp;</em><a href="https://vimeo.com/channels/thecityincinema" rel="nofollow">The City in CinÂ­eÂ­ma</a><em>. FolÂ­low him on TwitÂ­ter at&nbsp;<a href="https://twitter.com/#%21/colinmarshall">@colinm</a></em><em><a href="https://twitter.com/#%21/colinmarshall">a</a></em><em><a href="https://twitter.com/#%21/colinmarshall">rshall</a>&nbsp;or on&nbsp;<a href="https://www.facebook.com/colinmarshallessayist">FaceÂ­book</a>.</em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Class Action Against General Motors LLC, OnStar LLC, LexisNexis Risk Solutions [pdf] (467 pts)]]></title>
            <link>https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf</link>
            <guid>39709991</guid>
            <pubDate>Thu, 14 Mar 2024 23:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf">https://static01.nyt.com/newsgraphics/documenttools/0a813fc8e0ac1b6c/6c03d310-full.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39709991">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Oregon Outback is now the largest Dark Sky Sanctuary in the world (274 pts)]]></title>
            <link>https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html</link>
            <guid>39709981</guid>
            <pubDate>Thu, 14 Mar 2024 22:59:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html">https://www.hereisoregon.com/experiences/2024/03/oregon-outback-is-now-the-largest-dark-sky-sanctuary-in-the-world.html</a>, See on <a href="https://news.ycombinator.com/item?id=39709981">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><figure><picture><source srcset="https://www.hereisoregon.com/resizer/_HagYJqVYuY9wOMkujjALuG74VA=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 992px)"><source srcset="https://www.hereisoregon.com/resizer/xjmp4FlBSk0CP8atOehfv-UyIJE=/1024x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 768px)"><source srcset="https://www.hereisoregon.com/resizer/-25zd8lcRPlJpxKkTBgvGEVim2Q=/768x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" media="screen and (min-width: 0px)"><img alt="stars and milky way over a small wooden cabin at night" src="https://www.hereisoregon.com/resizer/_HagYJqVYuY9wOMkujjALuG74VA=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/TQPJJ43OFNE5VNRSKKPGHDIBPM.jpeg" width="1440" height="0" loading="lazy"></picture><figcaption><p><span>Night sky over Summer Lake Hot Springs, outside of Paisley, Oregon, a few days before the height of the Perseid meteor shower. </span>(Samantha Swindler/Samantha Swindler/ The Oregonian)</p></figcaption></figure><p>A 2.5 million-acre swath of southern Oregon has been named the largest Dark Sky Sanctuary in the world.</p><p>The region, which on Monday was officially named the Oregon Outback International Dark Sky Sanctuary, comprises the southeastern half of Lake County, including <a href="https://www.oregonlive.com/travel/2017/07/a_rugged_high_desert_adventure.html">Hart Mountain</a>, <a href="https://www.oregonlive.com/environment/2022/01/oregons-lake-abert-is-in-deep-trouble-the-state-shut-down-its-effort-to-figure-out-why.html">Lake Abert</a> and <a href="https://www.oregonlive.com/living/2023/09/at-summer-lake-hot-springs-soaking-out-under-the-stars-is-the-golden-time.html">Summer Lake</a>. Future plans include expanding the sanctuary to 11.4 million acres across Harney and Malheur counties.</p><p>The designation was given by <a href="https://darksky.org/news/outback-dark-international-dark-sky-sanctuary/">DarkSky International</a>, an organization dedicated to protecting the nighttime environment and preserving dark skies through environmentally responsible outdoor lighting. The project is the work of the <a href="https://www.southernoregon.org/industry/oregon-outback-dark-sky-network/">Oregon Dark Sky Network</a>, an ad-hoc group of state, local and federal officials, private individuals, business owners and tourism agencies.</p><p>Travel Southern Oregon, which is a member of the network, celebrated the designation in a news release Monday.</p><p>â€œThis four-year collaboration brings together so many of the elements we try to achieve in regenerative tourism,â€ Bob Hackett, executive director of Travel Southern Oregon, said. â€œIt not only elevates the destination experience for visitors to Lake County and opens up opportunities for local businesses, but it also helps agencies and residents steward their lands in ways that celebrate a legacy of starry night skies for generations to come.â€</p><p>Oregon already has two destinations with official DarkSky International designations: Prineville Reservoir State Park, which in 2021 <a href="https://www.oregonlive.com/travel/2021/05/prineville-reservoir-certified-as-oregons-first-dark-sky-park.html">became a Dark Sky Park</a>, and Sunriver Nature Center &amp; Observatory, which was <a href="https://darksky.org/news/sunriver-first-idsp-in-oregon/">named a Dark Sky Place</a> in 2020.</p><figure><picture><source srcset="https://www.hereisoregon.com/resizer/J8hUmJNk1nmt058kbhEAJbymmtI=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 992px)"><source srcset="https://www.hereisoregon.com/resizer/ms0PNDHrqjILVovr83settcHlTA=/1024x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 768px)"><source srcset="https://www.hereisoregon.com/resizer/0gZWWnb5JgsA6m6qfP2bu19q5Oo=/768x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" media="screen and (min-width: 0px)"><img src="https://www.hereisoregon.com/resizer/J8hUmJNk1nmt058kbhEAJbymmtI=/1440x0/filters:format(jpg):quality(70)/cloudfront-us-east-1.images.arcpublishing.com/advancelocal/KJLYG2Y3EJAFNI6OUVSSVQXG3A.jpg" width="1440" height="0" loading="lazy"></picture><figcaption><p>(Mark Graves/The Oregonian)</p></figcaption></figure><p>The Oregon Outback International Dark Sky Sanctuary is now the largest of 19 Dark Sky Sanctuaries, which are spread out across five continents. At 2.5 million acres, the Oregon sanctuary is larger than Minnesotaâ€™s 1 million-acre Boundary Waters Canoe Area Wilderness, which was designated as a Dark Sky Sanctuary in 2020.</p><p>The expansion of the Oregon Outback International Dark Sky Sanctuary seems inevitable, with only a few local approvals and lighting changes needed to make it happen, DarkSky International said. Most land in the region is either privately property or public lands managed by the Bureau of Land Management. The largest city in the area is Lakeview, home to fewer than 2,500 people.</p><p>Stargazers know southern and southeast Oregon as home to some of the best places to watch meteor showers and other astronomical events. Dark, clear skies are ideal for anyone hoping to peer into the cosmos, whether with a telescope or the naked eye.</p><p>Amber Harrison, program manager for DarkSky International, said in a news release Monday that the organization is already looking forward to the second phase of the Oregon Outback project, the big expansion, which would be the first landscape-scale sanctuary of its kind.</p><p>â€œCongratulations to the Oregon Outback Dark Sky Network team for achieving a monumental milestone in our journey towards preserving the night,â€ Harrison said. â€œYour dedication and collaboration have made Phase 1 of the Oregon Outback International Dark Sky Sanctuary a reality, showcasing the power of collective action in safeguarding night sky protections.â€</p><p>--<a href="https://www.oregonlive.com/user/jameshale/posts.html">Jamie Hale</a> covers travel and the outdoors and co-hosts the <a href="https://podcasts.apple.com/us/podcast/peak-northwest/id1486961693">Peak Northwest podcast</a>. Reach him at 503-294-4077, <a href="mailto:jhale@oregonian.com">jhale@oregonian.com</a> or <a href="http://www.twitter.com/halejamesb">@HaleJamesB</a>.</p><p>Our journalism needs your support. Subscribe today to <a href="https://www.oregonlive.com/subscribe/">OregonLive.com</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What I learned from looking at 900 most popular open source AI tools (328 pts)]]></title>
            <link>https://huyenchip.com/2024/03/14/ai-oss.html</link>
            <guid>39709912</guid>
            <pubDate>Thu, 14 Mar 2024 22:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huyenchip.com/2024/03/14/ai-oss.html">https://huyenchip.com/2024/03/14/ai-oss.html</a>, See on <a href="https://news.ycombinator.com/item?id=39709912">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Four years ago, I did an analysis of the <a href="https://huyenchip.com/2020/06/22/mlops.html">open source ML ecosystem</a>. Since then, the landscape has changed, so I revisited the topic. This time, I focused exclusively on the stack around foundation models.</p>

<p>The full list of open source AI repos is hosted at <a href="https://huyenchip.com/llama-police">llama-police</a>. The list is updated every 6 hours.</p>

<hr>
<p><b>Table of contents</b><br>
<a href="#data">Data</a><br>
â€¦. <a href="#add_missing_repos">How to add missing repos</a><br>
<a href="#the_new_ai_stack">The New AI Stack</a><br>
â€¦. <a href="#ai_stack_over_time">AI stack over time</a><br>
â€¦â€¦.. <a href="#applications">Applications</a><br>
â€¦â€¦.. <a href="#ai_engineering">AI engineering</a><br>
â€¦â€¦.. <a href="#model_development">Model development</a><br>
â€¦â€¦.. <a href="#infrastructure">Infrastructure</a><br>
<a href="#open_source_ai_developers">Open source AI developers</a><br>
â€¦. <a href="#the_rise_of_one_person_companies">One-person billion-dollar companies</a><br>
â€¦. <a href="#1_million_commits">1 million commits</a><br>
<a href="#the_growing_china_open_source_ecosystem">The growing Chinaâ€™s open source ecosystem</a><br>
<a href="#live_fast_die_young">Live fast, die young</a><br>
<a href="#my_personal_favorite_ideas">My personal favorite ideas</a><br>
<a href="#conclusion">Conclusion</a><br></p>

<hr>


<h2 id="data">Data</h2>

<p>I searched GitHub using the keywords <code>gpt</code>, <code>llm</code>, and <code>generative ai</code>. If AI feels so overwhelming right now, itâ€™s because it is. There are 118K results for <code>gpt</code> alone.</p>

<p>To make my life easier, I limited my search to the repos with at least 500 stars. There were 590 results for <code>llm</code>, 531 for <code>gpt</code>, and 38 for <code>generative ai</code>. I also occasionally checked GitHub trending and social media for new repos.</p>

<p>After MANY hours, I found 896 repos. Of these, 51 are tutorials (e.g. <a href="https://github.com/dair-ai/Prompt-Engineering-Guide">dair-ai/Prompt-Engineering-Guide</a>) and aggregated lists (e.g. <a href="https://github.com/f/awesome-chatgpt-prompts">f/awesome-chatgpt-prompts</a>). While these tutorials and lists are helpful, Iâ€™m more interested in software. I still include them in the final list, but the analysis is done with the 845 software repositories.</p>

<p>It was a painful but rewarding process. It gave me a much better understanding of what people are working on, how incredibly collaborative the open source community is, and just how much Chinaâ€™s open source ecosystem diverges from the Western one.</p>

<h3 id="add_missing_repos">Add missing repos</h3>

<p>I undoubtedly missed a ton of repos. You can submit the missing repos <a href="https://forms.gle/1ijNSnizgWQaVYK16">here</a>. The list will be automatically updated every day.</p>

<p>Feel free to submit the repos with less than 500 stars. Iâ€™ll continue tracking them and add them to the list when they reach 500 stars!</p>

<h2 id="the_new_ai_stack">The New AI Stack</h2>

<p>I think of the AI stack as consisting of 4 layers: infrastructure, model development, application development, and applications.</p>

<center>
    <figure>
    <img alt="Generative AI Stack" src="https://huyenchip.com/assets/pics/ai-oss/1-ai-stack.png">
    </figure>
</center>


<ol>
  <li>
    <p><strong>Infrastructure</strong></p>

    <p>At the bottom is the stack is infrastructure, which includes toolings for serving (<a href="https://github.com/vllm-project/vllm">vllm</a>, <a href="https://github.com/triton-inference-server/server">NVIDIAâ€™s Triton</a>), compute management (<a href="https://github.com/skypilot-org/skypilot">skypilot</a>), vector search and database (<a href="https://github.com/facebookresearch/faiss">faiss</a>, <a href="https://milvus.io/">milvus</a>, <a href="https://github.com/qdrant/qdrant">qdrant</a>, <a href="https://github.com/lancedb/lancedb">lancedb</a>), â€¦.</p>
  </li>
  <li>
    <p><strong>Model development</strong></p>

    <p>This layer provides toolings for developing models, including frameworks for modeling &amp; training (transformers, pytorch, DeepSpeed), inference optimization (ggml, openai/triton), dataset engineering, evaluation, â€¦.. Anything that involves changing a modelâ€™s weights happens in this layer, including finetuning.</p>
  </li>
  <li>
    <p><strong>Application development</strong>
 With readily available models, anyone can develop applications on top of them. This is the layer that has seen the most actions in the last 2 years and is still rapidly evolving. This layer is also known as AI engineering.</p>

    <p>Application development involves prompt engineering, RAG, AI interface, â€¦</p>
  </li>
  <li>
    <p><strong>Applications</strong></p>

    <p>There are many open sourced applications built on top of existing models. The most popular types of applications are coding, workflow automation, information aggregation, â€¦</p>
  </li>
</ol>

<p>Outside of these 4 layers, I also have another category, <strong>Model repos</strong>, that are created by companies and researchers to share the code associated with their models. Examples of repos in this category are <code>CompVis/stable-diffusion</code>, <code>openai/whisper</code>, and <code>facebookresearch/llama</code>.</p>

<h3 id="ai_stack_over_time">AI stack over time</h3>
<p>I plotted the cumulative number of repos in each category month-over-month. There was an explosion of new toolings in 2023, after the introduction of Stable Diffusion and ChatGPT. The curve seems to flatten in September 2023 because of three potential reasons.</p>

<ol>
  <li>I only include repos with at least 500 stars in my analysis, and it takes time for repos to gather these many stars.</li>
  <li>Most low-hanging fruits have been picked. What is left takes more effort to build, hence fewer people can build them.</li>
  <li>People have realized that itâ€™s hard to be competitive in the generative AI space, so the excitement has calmed down. Anecdotally, in early 2023, all AI conversations I had with companies centered around gen AI, but the recent conversations are more grounded. Several even brought up scikit-learn. Iâ€™d like to revisit this in a few months to verify if itâ€™s true.</li>
</ol>

<center>
    <figure>
    <img alt="Generative AI Stack Over Time" src="https://huyenchip.com/assets/pics/ai-oss/2-ai-timeline.png">
    </figure>
</center>


<p>In 2023, the layers that saw the highest increases were the applications and application development layers. The infrastructure layer saw a little bit of growth, but it was far from the level of growth seen in other layers.</p>

<h4 id="applications">Applications</h4>

<p>Not surprisingly, the most popular types of applications are coding, bots (e.g. role-playing, WhatsApp bots, Slack bots), and information aggregation (e.g. â€œletâ€™s connect this to our Slack and ask it to summarize the messages each dayâ€).</p>

<center>
    <figure>
    <img alt="Breakdown of popular AI applications" src="https://huyenchip.com/assets/pics/ai-oss/3-ai-applications.png">
    </figure>
</center>


<h4 id="ai_engineering">AI engineering</h4>

<p>2023 was the year of AI engineering. Since many of them are similar, itâ€™s hard to categorize the tools. I currently put them into the following categories: prompt engineering, AI interface, Agent, and AI engineering (AIE) framework.</p>

<p><strong>Prompt engineering</strong> goes way beyond fiddling with prompts to cover things like constrained sampling (structured outputs), long-term memory management, prompt testing &amp; evaluation, etc.</p>

<center>
    <figure>
    <img alt="A list of prompt engineering tools" src="https://huyenchip.com/assets/pics/ai-oss/4-prompt-engineering.png">
    </figure>
</center>


<p><strong>AI interface</strong> provides an interface for your end users to interact with your AI application. This is the category Iâ€™m the most excited about. Some of the interfaces that are gaining popularity are:</p>

<ul>
  <li>Web and desktop apps.</li>
  <li>Browser extensions that let users quickly query AI models while browsing.</li>
  <li>Bots via chat apps like Slack, Discord, WeChat, and WhatsApp.</li>
  <li>Plugins that let developers embed AI applications to applications like VSCode, Shopify, and Microsoft Offices. The plugin approach is common for AI applications that can use tools to complete complex tasks (agents).</li>
</ul>

<p><strong>AIE framework</strong> is a catch-all term for all platforms that help you develop AI applications. Many of them are built around RAG, but many also provide other toolings such as monitoring, evaluation, etc.</p>

<p><strong>Agent</strong> is a weird category, as many agent toolings are just sophisticated prompt engineering with potentially constrained generation (e.g. the model can only output the predetermined action) and plugin integration (e.g. to let the agent use tools).</p>

<center>
    <figure>
    <img alt="AI engineering stack over time" src="https://huyenchip.com/assets/pics/ai-oss/5-ai-engineering.png">
    </figure>
</center>


<h4 id="model_development">Model development</h4>

<p>Pre-ChatGPT, the AI stack was dominated by model development. Model developmentâ€™s biggest growth in 2023 came from increasing interest in inference optimization, evaluation, and parameter-efficient finetuning (which is grouped under Modeling &amp; training).</p>

<p>Inference optimization has always been important, but the scale of foundation models today makes it crucial for latency and cost. The core approaches for optimization remain the same (quantization, low-ranked factorization, pruning, distillation), but many new techniques have been developed especially for the transformer architecture and the new generation of hardware. For example, in 2020, 16-bit quantization was considered state-of-the-art. Today, weâ€™re seeing <a href="https://arxiv.org/abs/2212.09720">2-bit quantization</a> and <a href="https://arxiv.org/abs/2402.17764">even lower than 2-bit</a>.</p>

<p>Similarly, evaluation has always been essential, but with many people today treating models as blackboxes, evaluation has become even more so. There are many new evaluation benchmarks and evaluation methods, such as comparative evaluation (see <a href="https://huyenchip.com/2024/02/28/predictive-human-preference.html#correctness_of_chatbot_arena_ranking">Chatbot Arena</a>) and AI-as-a-judge.</p>

<center>
    <figure>
    <img alt="Model Development Stack Over Time" src="https://huyenchip.com/assets/pics/ai-oss/6-model-development.png">
    </figure>
</center>


<h4 id="infrastructure">Infrastructure</h4>

<p>Infrastructure is about managing data, compute, and toolings for serving, monitoring, and other platform work. Despite all the changes that generative AI brought, the open source AI infrastructure layer remained more or less the same. This could also be because infrastructure products are typically not open sourced.</p>

<p>The newest category in this layer is vector database with companies like Qdrant, Pinecone, and LanceDB. However, many argue this shouldnâ€™t be a category at all. Vector search has been around for a long time. Instead of building new databases just for vector search, existing database companies like DataStax and Redis are bringing vector search into where the data already is.</p>

<h2 id="open_source_ai_developers">Open source AI developers</h2>

<p>Open source software, like many things, follows the long tail distribution. A handful of accounts control a large portion of the repos.</p>

<h3 id="the_rise_of_one_person_companies">One-person billion-dollar companies</h3>
<p>845 repos are hosted on 594 unique GitHub accounts. There are 20 accounts with at least 4 repos. These top 20 accounts host 195 of the repos, or 23% of all the repos on the list. These 195 repos have gained a total of 1,650,000 stars.</p>

<center>
    <figure>
    <img alt="Most active GitHub accounts" src="https://huyenchip.com/assets/pics/ai-oss/7-top-accounts.png">
    </figure>
</center>


<p>On Github, an account can be either an organization or an individual. 19/20 of the top accounts are organizations. Of those, 3 belong to Google: <code>google-research</code>, <code>google</code>, <code>tensorflow</code>.</p>

<p>The only individual account in these top 20 accounts is lucidrains. Among the top 20 accounts with the most number of stars (counting only gen AI repos), 4 are individual accounts:</p>

<ul>
  <li><a href="https://github.com/lucidrains">lucidrains</a> (Phil Wang): who can implement state-of-the-art models insanely fast.</li>
  <li><a href="https://github.com/ggerganov">ggerganov</a> (Georgi Gerganov): an optimization god who comes from a physics background.</li>
  <li><a href="https://github.com/lllyasviel">Illyasviel</a> (Lyumin Zhang): creator of Foocus and ControlNet whoâ€™s currently a Stanford PhD.</li>
  <li><a href="https://github.com/xtekky">xtekky</a>: a full-stack developer who created gpt4free.</li>
</ul>

<center>
    <figure>
    <img alt="Most active GitHub accounts" src="https://huyenchip.com/assets/pics/ai-oss/8-top-accounts-stars.png">
    </figure>
</center>


<p>Unsurprisingly, the lower we go in the stack, the harder it is for individuals to build. Software in the infrastructure layer is the least likely to be started and hosted by individual accounts, whereas more than half of the applications are hosted by individuals.</p>

<center>
    <figure>
    <img alt="Can you do this alone?" src="https://huyenchip.com/assets/pics/ai-oss/9-indie.png">
    </figure>
</center>


<p>Applications started by individuals, on average, have gained more stars than applications started by organizations. Several people have speculated that weâ€™ll see many very valuable one-person companies (see <a href="https://fortune.com/2024/02/04/sam-altman-one-person-unicorn-silicon-valley-founder-myth/">Sam Altmanâ€™s interview</a> and <a href="https://www.reddit.com/r/ChatGPT/comments/1ajwj5z/one_person_billion_dollar_company/">Reddit discussion</a>). I think they might be right.</p>

<center>
    <figure>
    <img alt="Can you do this alone?" src="https://huyenchip.com/assets/pics/ai-oss/10-indie-stars.png">
    </figure>
</center>


<h3 id="1_million_commits">1 million commits</h3>

<p>Over 20,000 developers have contributed to these 845 repos. In total, theyâ€™ve made almost a million contributions!</p>

<p>Among them, the 50 most active developers have made over 100,000 commits, averaging over 2,000 commits each. See the full list of the top 50 most active open source developers <a href="https://huyenchip.com/llama-devs">here</a>.</p>

<center>
    <figure>
    <img alt="Most active open source developers" src="https://huyenchip.com/assets/pics/ai-oss/11-devs.png">
    </figure>
</center>


<h2 id="the_growing_china_open_source_ecosystem">The growing China's open source ecosystem</h2>

<p>Itâ€™s been known for a long time that Chinaâ€™s AI ecosystem has diverged from the US (I also mentioned that in a <a href="https://huyenchip.com/2020/12/27/real-time-machine-learning.html#mlops_china_vs_us">2020 blog post</a>). At that time, I was under the impression that GitHub wasnâ€™t widely used in China, and my view back then was perhaps colored by Chinaâ€™s 2013 ban on GitHub.</p>

<p>However, this impression is no longer true. There are many, many popular AI repos on GitHub targeting Chinese audiences, such that their descriptions are written in Chinese. There are repos for models developed for Chinese or Chinese + English, such as <a href="https://github.com/QwenLM/Qwen">Qwen</a>, <a href="https://github.com/THUDM/ChatGLM3">ChatGLM3</a>, <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">Chinese-LLaMA</a>.</p>

<p>While in the US, many research labs have moved away from the RNN architecture for language models, the RNN-based model family <a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a> is still popular.</p>

<p>There are also AI engineering tools providing ways to integrate AI models into products popular in China like WeChat, QQ, DingTalk, etc. Many popular prompt engineering tools also have mirrors in Chinese.</p>

<p>Among the top 20 accounts on GitHub, 6 originated in China:</p>

<ol>
  <li><a href="https://github.com/THUDM">THUDM</a>: Knowledge Engineering Group (KEG) &amp; Data Mining at Tsinghua University.</li>
  <li><a href="https://github.com/OpenGVLab">OpenGVLab</a>: General Vision team of Shanghai AI Laboratory</li>
  <li><a href="https://github.com/OpenBMB">OpenBMB</a>: Open Lab for Big Model Base, founded by ModelBest &amp; the NLP group at Tsinghua University.</li>
  <li><a href="https://github.com/InternLM">InternLM</a>: from Shanghai AI Laboratory.</li>
  <li><a href="https://github.com/open-mmlab">OpenMMLab</a>: from The Chinese University of Hong Kong.</li>
  <li><a href="https://github.com/QwenLM">QwenLM</a>: Alibabaâ€™s AI lab, which publishes the Qwen model family.</li>
</ol>

<h2 id="live_fast_die_young">Live fast, die young</h2>

<p>One pattern that I saw last year is that many repos quickly gained a massive amount of eyeballs, then quickly died down. Some of my friends call this the â€œhype curveâ€. Out of these 845 repos with at least 500 GitHub stars, 158 repos (18.8%) havenâ€™t gained any new stars in the last 24 hours, and 37 repos (4.5%) havenâ€™t gained any new stars in the last week.</p>

<p>Here are examples of the growth trajectory of two of such repos compared to the growth curve of two more sustained software. Even though these two examples shown here are no longer used, I think they were valuable in showing the community what was possible, and it was cool that the authors were able to get things out so fast.</p>

<center>
    <figure>
    <img alt="Hype curve" src="https://huyenchip.com/assets/pics/ai-oss/12-hype-curve.png">
    </figure>
</center>


<h2 id="my_personal_favorite_ideas">My personal favorite ideas</h2>

<p>So many cool ideas are being developed by the community. Here are some of my favorites.</p>

<ul>
  <li>Batch inference optimization: <a href="https://github.com/FMInference/FlexGen">FlexGen</a>, <a href="https://github.com/ggerganov/llama.cpp/pull/1375">llama.cpp</a></li>
  <li>Faster decoder with techniques such as <a href="https://github.com/FasterDecoding/Medusa">Medusa</a>, <a href="https://github.com/hao-ai-lab/LookaheadDecoding">LookaheadDecoding</a></li>
  <li>Model merging: <a href="https://github.com/cg123/mergekit">mergekit</a></li>
  <li>Constrained sampling: <a href="https://github.com/outlines-dev">outlines</a>, <a href="https://github.com/guidance-ai/guidance">guidance</a>, <a href="https://github.com/sgl-project/sglang">SGLang</a></li>
  <li>Seemingly niche tools that solve one problem really well, such as <a href="https://github.com/arogozhnikov/einops">einops</a> and <a href="https://github.com/huggingface/safetensors">safetensors</a>.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Even though I included only 845 repos in my analysis, I went through several thousands of repos. I found this helpful for me to get a big-picture view of the seemingly overwhelming AI ecosystem. I hope the <a href="https://huyenchip.com/llama-police">list</a> is useful for you too. Please do let me know what repos Iâ€™m missing, and Iâ€™ll add them to the list!</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Buys DarwinAI Ahead of Major Generative AI Updates Coming in iOS 18 (115 pts)]]></title>
            <link>https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/</link>
            <guid>39709835</guid>
            <pubDate>Thu, 14 Mar 2024 22:43:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/">https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/</a>, See on <a href="https://news.ycombinator.com/item?id=39709835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/03/14/apple-acquires-darwinai/"><p>Apple acquired Canada-based company DarwinAI earlier this year to build out its AI team, reports <em><a href="https://www.bloomberg.com/news/articles/2024-03-14/apple-aapl-buys-canadian-ai-startup-darwinai-as-part-of-race-to-add-features">Bloomberg</a></em>. DarwinAI created AI technology for inspecting components during the manufacturing process, and it also had a focus on making smaller and more efficient AI systems.</p>
<p><img src="https://images.macrumors.com/t/KnV8VAc6R5Dd_3U1Esmw774Zr2U=/400x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy" srcset="https://images.macrumors.com/t/KnV8VAc6R5Dd_3U1Esmw774Zr2U=/400x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy 400w,https://images.macrumors.com/t/j8j2WKQb8lEd30rj8l8Z1wR25JE=/800x0/article-new/2022/03/hey-siri-banner-apple.jpg?lossy 800w,https://images.macrumors.com/t/WnxAtULV_HFlWOr5EliFjsbm-OU=/1600x0/article-new/2022/03/hey-siri-banner-apple.jpg 1600w,https://images.macrumors.com/t/X4xoV100u9gMqO-t4CP7rdkoQvE=/2500x0/filters:no_upscale()/article-new/2022/03/hey-siri-banner-apple.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="hey siri banner apple" width="1200" height="630"><br>DarwinAI's website and social media accounts have been taken offline following Apple's purchase. Dozens of former DarwinAI companies have now joined Apple's artificial intelligence division. AI researcher Alexander Wong, who helped build DarwinAI, is now a director in Apple's AI group.</p>
<p>Apple confirmed the acquisition with the statement that it typically gives when questioned about purchases. "Apple buys smaller technology companies from time to time" but does not discuss its purpose or plans.</p>
<p>In an effort to catch up with Microsoft, Google, and others in the AI market, Apple is working hard to build artificial intelligence features for its next-generation <a href="https://www.macrumors.com/roundup/ios-18/">iOS 18</a> and macOS 15 operating systems.</p>
<p>If Apple wants to be able to rival Microsoft's Bing, OpenAI's ChatGPT, and other generative AI offerings, it will need to integrate generative AI into a range of products. Apple is testing large language models, and AI features are said to be coming to <a href="https://www.macrumors.com/guide/siri/">Siri</a>, Shortcuts, Messages, <a href="https://www.macrumors.com/guide/apple-music/">Apple Music</a>, and more.</p>
<p>Apple is aiming to have AI features run on-device for privacy reasons, and DarwinAI's efforts to make smaller AI systems could be of use to further that endeavor.</p>
<p>Apple CEO <a href="https://www.macrumors.com/guide/tim-cook/">Tim Cook</a> <a href="https://www.macrumors.com/2024/02/28/tim-cook-apple-generative-ai-break-new-ground/">has promised</a> that Apple will "break new ground" in generative AI in 2024. "We believe it will unlock transformative opportunities for our users," said Cook.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/03/11/iphone-16-pro-expected-later-this-year/">iPhone 16 Pro Expected Later This Year With These 10 New Features</a></h3><p>While the iPhone 16 Pro and iPhone 16 Pro Max are still around six months away from launching, there are already many rumors about the devices. Below, we have recapped new features and changes expected so far. These are some of the key changes rumored for the iPhone 16 Pro models as of March 2024:Larger displays: The iPhone 16 Pro and iPhone 16 Pro Max will be equipped with larger 6.3-inch...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/iphone-se-4-expected-to-depreciate-heavily/">iPhone SE 4 Expected to Depreciate Heavily</a></h3><p>Resale value trends suggest the iPhone SE 4 may not hold its value as well as Apple's flagship models, according to SellCell. According to the report, Apple's iPhone SE models have historically depreciated much more rapidly than the company's more premium offerings. The third-generation iPhone SE, which launched in March 2022, experienced a significant drop in resale value, losing 42.6%...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/11/2024-ipad-pro-key-rumors/">2024 iPad Pro: Key Rumors to Be Aware of Ahead of Announcement</a></h3><p>Apple's next-generation iPad Pro models are expected to be announced in a matter of weeks, so what can customers expect from the highly anticipated new machines? The 2022 iPad Pro was a minor update that added the M2 chip, Apple Pencil hover, and specification upgrades like Wi-Fi 6E and Bluetooth 5.3 connectivity. The iPad Pro as a whole has generally only seen relatively small updates in...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/14/apple-wallet-app-ids-slow-adoption/">Apple Said iPhone Driver's Licenses Would Expand to These 8 U.S. Statesâ€¦ Two Years Ago</a></h3><p>In just four U.S. states, residents can add their driver's license or ID to the Apple Wallet app on the iPhone and Apple Watch, providing a convenient and contactless way to display proof of identity or age at select airports, businesses, and venues. Adoption of the feature has been slow since Apple first announced it in September 2021, with IDs in the Wallet app only available in Arizona,...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/11/apple-preparing-ios-17-4-1/">Apple Preparing iOS 17.4.1 Update for iPhone</a></h3><p>Apple appears to be internally testing iOS 17.4.1 for the iPhone, based on evidence of the software update in our website's logs this week. Our logs have revealed the existence of several iOS 17 versions before Apple released them, ranging from iOS 17.0.3 to iOS 17.3.1. iOS 17.4.1 should be a minor update that addresses software bugs and/or security vulnerabilities. It is unclear when...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/apple-announces-app-downloads-from-websites/">Apple Announces Ability to Download iPhone Apps From Websites in EU</a></h3><p>Apple today announced three further changes for developers in the European Union, allowing them to distribute apps directly from webpages, choose how to design in-app promotions, and more. Apple last week enabled alternative app stores in the EU in iOS 17.4, allowing third-party app stores to offer a catalog of other developers' apps as well as the marketplace developer's own apps. As of...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/14/apple-acquires-darwinai/">Apple Buys DarwinAI Ahead of Major Generative AI Updates Coming in iOS 18</a></h3><p>Thursday March 14, 2024 10:27 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple acquired Canada-based company DarwinAI earlier this year to build out its AI team, reports Bloomberg. DarwinAI created AI technology for inspecting components during the manufacturing process, and it also had a focus on making smaller and more efficient AI systems. DarwinAI's website and social media accounts have been taken offline following Apple's purchase. Dozens of former DarwinAI ...</p></div><div><h3><a href="https://www.macrumors.com/2024/03/12/m3-macbook-air-vs-macbook-pro/">Video Comparison: M3 MacBook Air vs. M3 MacBook Pro</a></h3><p>Tuesday March 12, 2024 10:42 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>With the refresh of the MacBook Air models in March, Apple now has M3 versions of the 13-inch MacBook Air, 14-inch MacBook Pro, and 15-inch MacBook Air, all with the same chip inside. For those trying to decide between the MacBook Pro and the MacBook Air, we did a comparison video to highlight what you're getting with each machine. Subscribe to the MacRumors YouTube channel for more videos. ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Genetically engineering koji mold to create a meat alternative (103 pts)]]></title>
            <link>https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/</link>
            <guid>39709827</guid>
            <pubDate>Thu, 14 Mar 2024 22:43:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/">https://newscenter.lbl.gov/2024/03/14/its-hearty-its-meaty-its-mold/</a>, See on <a href="https://news.ycombinator.com/item?id=39709827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      
<lbl-container wrapper-size="sm">
  <lbl-rich-text>
    <p>With animal-free dairy products and convincing vegetarian meat substitutes already on the market, itâ€™s easy to see how biotechnology can change the food industry. Advances in genetic engineering are allowing us to harness microorganisms to produce cruelty-free products that are healthy for consumers and healthier for the environment.</p>
<p>One of the most promising sources of innovative foods is fungi â€“ a diverse kingdom of organisms that naturally produce a huge range of tasty and nutritious proteins, fats, antioxidants, and flavor molecules. Chef-turned-bioengineer <a href="https://www.jbei.org/science-or-alchemy-transforming-the-future-of-food/" target="_blank" rel="noopener">Vayu Hill-Maini</a>, an affiliate in the Biosciences Area at Lawrence Berkeley National Laboratory (Berkeley Lab), is exploring the many possibilities for new tastes and textures that can be made from modifying the genes already present in fungi.</p>
<p>â€œI think itâ€™s a fundamental aspect of synthetic biology that weâ€™re benefiting from organisms that have evolved to be really good at certain things,â€ said Hill-Maini, who is a postdoctoral researcher at UC Berkeley in the lab of bioengineering expert <a href="https://keaslinglab.lbl.gov/jay-keasling/">Jay Keasling</a>. â€œWhat weâ€™re trying to do is to look at what is the fungus making and try to kind of unlock and enhance it. And I think thatâ€™s an important angle that we donâ€™t need to introduce genes from wildly different species. Weâ€™re investigating how we can stitch things together and unlock whatâ€™s already there.â€</p>
<p>In their recent paper, <a href="https://www.nature.com/articles/s41467-024-46314-8" target="_blank" rel="noopener">published today in <em>Nature Communications</em></a>, Hill-Maini and colleagues at UC Berkeley, the Joint BioEnergy Institute, and the Novo Nordisk Foundation Center for Biosustainability studied a multicellular fungus called <em>Aspergillus oryzae</em>, also known as koji mold, that has been used in East Asia to ferment starches into sake, soy sauce, and miso for centuries. First, the team used CRISPR-Cas9 to develop a gene editing system that can make consistent and reproducible changes to the koji mold genome. Once they had established a toolkit of edits, they applied their system to make modifications that elevate the mold as a food source. First, Hill-Maini focused on boosting the moldâ€™s production of heme â€“ an iron-based molecule which is found in many lifeforms but is most abundant in animal tissue, giving meat its color and distinctive flavor. (A synthetically produced plant-derived heme is also what gives the Impossible Burger its meat-duping properties.) Next, the team punched up production of ergothioneine, an antioxidant only found in fungi that is associated with cardiovascular health benefits.</p>
<p>After these changes, the once-white fungi grew red. With minimal preparation â€“ removing excess water and grinding â€“ the harvested fungi could be shaped into a patty, then fried into a tempting-looking burger.</p>
<div id="attachment_54355"><p><a href="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg"><img decoding="async" aria-describedby="caption-attachment-54355" src="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg" alt="A small, rounded reddish mass that looks like a meat patty sitting in a frying pan beaded with oil" width="1347" height="1500" srcset="https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage.jpg 1347w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-768x855.jpg 768w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-330x367.jpg 330w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/2024-02-02_featuredimage-890x991.jpg 890w" sizes="(max-width: 1347px) 100vw, 1347px"></a></p><p id="caption-attachment-54355">The koji mold patty after frying. (Credit: Vayu Hill-Maini)</p></div>
<p>Hill-Mainiâ€™s next objective is to make the fungi even more appealing by tuning the genes that control the moldâ€™s texture. â€œWe think that thereâ€™s a lot of room to explore texture by varying the fiber-like morphology of the cells. So, we might be able to program the structure of the lot fibers to be longer which would give a more meat-like experience. And then we can think about boosting lipid composition for mouth feel and further nutrition,â€ said Hill-Maini, who was a Fellow of the Miller Institute for Basic Research in Science at UC Berkeley during the study. â€œIâ€™m really excited about how can we further look at the fungus and, you know, tinker with its structure and metabolism for food.â€</p>
<p>Though this work is just the beginning of the journey to tap into fungal genomes to create new foods, it showcases the huge potential of these organisms to serve as easy-to-grow protein sources that avoid the complex ingredients lists of current meat substitutes&nbsp; and the cost barriers and technical <a href="https://www.forbes.com/sites/chloesorvino/2023/06/27/everything-you-need-to-know-about-lab-grown-meat-now-that-its-here/?sh=7f49d298f8a5">difficulties hindering the launch of cultured meat</a>. Additionally, the teamâ€™s gene editing toolkit is a big leap forward for the field of synthetic biology as a whole. Currently, a great variety of biomanufactured goods are made by engineered bacteria and yeast, the single-celled cousins of mushrooms and mold. Yet despite humanityâ€™s long history of domesticating fungi to eat directly or to make staples like miso, multicellular fungi have not yet been harnessed as engineered cellular factories to the same extent because their genomes are far more complex, and have adaptations that make gene editing a challenge. The CRISPR-Cas9 toolkit developed in this paper lays the foundation to easily edit koji mold and its many relatives.</p>
  </lbl-rich-text>
</lbl-container>







<lbl-container wrapper-size="sm">
  <lbl-rich-text>
    <p>â€œThese organisms have been used for centuries to produce food, and they are incredibly efficient at converting carbon into a wide variety of complex molecules, including many that would be almost impossible to produce using a classic host like brewerâ€™s yeast or <em>E. coli</em>,â€ said senior author <a href="https://keaslinglab.lbl.gov/jay-keasling/">Jay Keasling</a>, who is a senior scientist at Berkeley Lab and a professor at UC Berkeley. â€œBy unlocking koji mold through the development of these tools, we are unlocking the potential of a huge new group of hosts that we can use to make foods, valuable chemicals, energy-dense biofuels, and medicines. Itâ€™s a thrilling new avenue for biomanufacturing.â€</p>
<div id="attachment_54375"><p><a href="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-54375" src="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg" alt="A young man in a long sleeve red top sitting outside on a picnic bench. He is wearing lab goggles and holding a fork. On the table in front of him is a plate containing a petri dish filled with fuzzy mold." width="360" height="539" srcset="https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013.jpg 1002w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-768x1150.jpg 768w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-330x494.jpg 330w, https://newscenter.lbl.gov/wp-content/uploads/2024/03/XBD-202403-036-013-890x1332.jpg 890w" sizes="(max-width: 360px) 100vw, 360px"></a></p><p id="caption-attachment-54375">Vayu Hill-Maini is excited to see engineered fungi advance to new food products. Seen here on the JBEI balcony. (Credit: Marilyn Sargent/Berkeley Lab)</p></div>
<p>Given his culinary background, Hill-Maini is keen to ensure that the next generation of fungi-based products are not only palatable, but truly desirable to customers, including those with sophisticated tastes. In a separate study, he and Keasling collaborated with chefs at Alchemist, a two-Michelin-starred restaurant in Copenhagen, to play with the culinary potential of another multicellular fungus, <em>Neurospora intermedia. </em>This fungus is traditionally used in Indonesia to produce a staple food called oncom by fermenting the waste products left over from making other foods, such as tofu. Intrigued by its ability to convert leftovers into a protein-rich food, the scientists and chefs studied the fungus in the Alchemist test kitchen. They discovered <em>N. intermedia </em>produces and excretes many enzymes as it grows. When grown on starchy rice, the fungi produces an enzyme that liquifies the rice and makes it intensely sweet. â€œWe developed a process with just three ingredients â€“ rice, water, and fungus â€“ to make a beautiful, striking orange-colored porridge,â€ said Hill-Maini. â€œThat became a new dish on the tasting menu that utilizes fungal chemistry and color in a dessert. And I think that what it really shows is that thereâ€™s opportunity to bridge the laboratory and the kitchen.â€</p>
<p>Hill-Mainiâ€™s work on the gene editing research described in this article is supported by the Miller Institute at UC Berkeley. Keaslingâ€™s lab is supported by the Novo Nordisk Foundation. Both received additional support from the Department of Energy (DOE) Office of Science. The Joint BioEnergy Institute is a DOE Bioenergy Research Center managed by Berkeley Lab.</p>
<p># # #</p>
<p>Lawrence Berkeley National Laboratory (Berkeley Lab) is committed to delivering solutions for humankind through research in clean energy, a healthy planet, and discovery science. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Researchers from around the world rely on the Labâ€™s world-class scientific facilities for their own pioneering research. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energyâ€™s Office of Science.</p>
<p>DOEâ€™s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit&nbsp;<a href="http://energy.gov/science">energy.gov/science</a>.</p>
  </lbl-rich-text>
</lbl-container>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ban on same-sex marriage unconstitutional, Sapporo High Court rules (114 pts)]]></title>
            <link>https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/</link>
            <guid>39709657</guid>
            <pubDate>Thu, 14 Mar 2024 22:24:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/">https://www.japantimes.co.jp/news/2024/03/14/japan/crime-legal/same-sex-marriage-ruling/</a>, See on <a href="https://news.ycombinator.com/item?id=39709657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jtarticle">
                                      <p>The Sapporo High Court on Thursday ruled that a ban on same-sex marriage is unconstitutional, with strong phrasing <s> </s>that is expected to pressure the government and lawmakers for action.</p><p>It is the first time a high court has handed down a ruling that said Japan's ban on same-sex marriage is unconstitutional.</p><p>The high court judgment followed a similar ruling at the Tokyo District Court earlier on Thursday, which said that the ban on same-sex marriage is in a â€œstate of unconstitutionalityâ€ due to the lack of legal protections for same-sex couples. The Tokyo court, however, stopped short of issuing a stronger â€œunconstitutionalâ€ verdict.</p>
                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of data privacy company Onerep.com founded dozens of people-search firms (531 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/</link>
            <guid>39709089</guid>
            <pubDate>Thu, 14 Mar 2024 21:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/">https://krebsonsecurity.com/2024/03/ceo-of-data-privacy-company-onerep-com-founded-dozens-of-people-search-firms/</a>, See on <a href="https://news.ycombinator.com/item?id=39709089">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>The data privacy company <strong>Onerep.com</strong> bills itself as a Virginia-based service for helping people remove their personal information from almost 200 people-search websites. However, an investigation into the history of onerep.com finds this company is operating out of Belarus and Cyprus, and that its founder has launched dozens of people-search services over the years.</p>
<p>Onerepâ€™s â€œProtectâ€ service starts at $8.33 per month for individuals and $15/mo for families, and promises to remove your personal information from nearly 200 people-search sites. Onerep also markets its service to companies seeking to offer their employees the ability to have their data continuously removed from people-search sites.</p>
<div id="attachment_66754"><p><img aria-describedby="caption-attachment-66754" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-permanente.png" alt="" width="250" height="436"></p><p id="caption-attachment-66754">A testimonial on onerep.com.</p></div>
<p>Customer case studies published on onerep.com state that it struck a deal to offer the service to employees of <strong>Permanente Medicine</strong>, which represents the doctors within the health insurance giant <strong>Kaiser Permanente</strong>. Onerep also says it has made inroads among police departments in the United States.</p>
<p>But a review of Onerepâ€™s domain registration records and that of its founder reveal a different side to this company. Onerep.com says its founder and CEO is <strong>Dimitri Shelest</strong> from Minsk, Belarus, as does <a href="https://www.linkedin.com/in/dimitri-shelest-183626174/" target="_blank" rel="noopener">Shelestâ€™s profile on LinkedIn</a>. Historic registration records indexed by <a href="https://www.domaintools.com/" target="_blank" rel="noopener">DomainTools.com</a> say Mr. Shelest was a registrant of onerep.com who used the email address <strong>dmitrcox2@gmail.com</strong>.</p>
<p>A search in the data breach tracking service <a href="https://constella.ai/" target="_blank" rel="noopener">Constella Intelligence</a> for the name Dimitri Shelest brings up the email address <strong>dimitri.shelest@onerep.com</strong>. Constella also finds that Dimitri Shelest from Belarus used the email address <strong>d.sh@nuwber.com</strong>, and the Belarus phone number <strong>+375-292-702786</strong>.</p>
<p>Nuwber.com is a people search service whose employees all appear to be from Belarus, and it is one of dozens of people-search companies that Onerep claims to target with its data-removal service. Onerep.comâ€™s website disavows any relationship to Nuwber.com, stating quite clearly, â€œPlease note that OneRep is not associated with Nuwber.com.â€</p>
<p>However, there is an abundance of evidence suggesting Mr. Shelest is in fact the founder of Nuwber. Constella found that Minsk telephone number (375-292-702786) has been used multiple times in connection with the email address <strong>dmitrcox@gmail.com</strong>. Recall that Onerep.comâ€™s domain registration records in 2018 list the email address dmitrcox2@gmail.com.</p>
<p>It appears Mr. Shelest sought to reinvent his online identity in 2015 by adding a â€œ2â€ to his email address. A search on the Belarus phone number tied to Nuwber.com shows up in the domain records for <strong>askmachine.org</strong>, and DomainTools says this domain is tied to both dmitrcox@gmail.com and dmitrcox2@gmail.com.</p>
<div id="attachment_66783"><p><img aria-describedby="caption-attachment-66783" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest.png" alt="" width="749" height="439" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest.png 1275w, https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest-768x450.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/onerep-shellest-782x458.png 782w" sizes="(max-width: 749px) 100vw, 749px"></p><p id="caption-attachment-66783">Onerep.com CEO and founder Dimitri Shelest, as pictured on the â€œaboutâ€ page of onerep.com.</p></div>
<p>A search in DomainTools for the email address dmitrcox@gmail.com shows it is associated with the registration of at least 179 domain names, including dozens of mostly now-defunct people-search companies targeting citizens of Argentina, Brazil, Canada, Denmark, France, Germany, Hong Kong, Israel, Italy, Japan, Latvia and Mexico, among others.</p>
<p>Those include <a href="https://web.archive.org/web/20160324064030/https://nuwber.fr/" target="_blank" rel="noopener"><strong>nuwber.fr</strong></a>, a site registered in 2016 which was identical to <a href="https://web.archive.org/web/20160616212526/https://nuwber.com/" target="_blank" rel="noopener">the homepage of Nuwber.com at the time</a>. DomainTools shows the same email and Belarus phone number are in historic registration records for <a href="https://web.archive.org/web/20170709170147/https://nuwber.at/" target="_blank" rel="noopener">nuwber.at</a>, <a href="https://web.archive.org/web/20160804065746/https://nuwber.ch/" target="_blank" rel="noopener">nuwber.ch</a>, and <a href="https://web.archive.org/web/20160806212859/https://nuwber.dk/" target="_blank" rel="noopener">nuwber.dk</a> (all domains linked here are to their cached copies at archive.org, where available).</p>
<div id="attachment_66755"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66755" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png" alt="" width="749" height="477" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com.png 1436w, https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com-768x489.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/nuwber-dot-com-782x498.png 782w" sizes="(max-width: 749px) 100vw, 749px"></a></p><p id="caption-attachment-66755">Nuwber.com, circa 2015. Image: Archive.org.</p></div>
<p>A review of historic WHOIS records for onerep.com show it was registered for many years to a resident of Sioux Falls, SD for a completely unrelated site. But around Sept. 2015 the domain switched from the registrar GoDaddy.com to eNom, and the registration records were hidden behind privacy protection services. DomainTools indicates around this time onerep.com started using domain name servers from DNS provider constellix.com.&nbsp;Likewise, Nuwber.com first appeared in late 2015, was also registered through eNom, and also started using constellix.com for DNS at nearly the same time.</p>
<p>Listed <a href="https://www.linkedin.com/in/dzmitrybukuyazau/" target="_blank" rel="noopener">on LinkedIn</a> as a former product manager at OneRep.com between 2015 and 2018 is <strong>Dimitri Bukuyazau</strong>, who says their hometown is Warsaw, Poland. While this LinkedIn profile (linkedin.com/in/<strong>dzmitry</strong>bukuyazau) does not mention Nuwber, a search on this name in Google turns up <a href="https://web.archive.org/web/20200906155823/https://www.privacyduck.com/comparisons/privacyduck-vs-onerep-com-the-eastern-european-privacy-company/" target="_blank" rel="noopener">a 2017 blog post from privacyduck.com</a>, which laid out a number of reasons to support a conclusion that OneRep and Nuwber.com were the same company.</p>
<p>â€œAny people search profiles containing your Personally Identifiable Information that were on Nuwber.com were also mirrored identically on OneRep.com, down to the relativesâ€™ names and address histories,â€ Privacyduck.com wrote. The post continued:</p>
<blockquote><p>â€œBoth sites offered the same immediate opt-out process. Both sites had the same generic contact and support structure. They were â€“ and remain â€“ the same company (even PissedConsumer.com advocates this fact: https://nuwber.pissedconsumer.com/nuwber-and-onerep-20160707878520.html).â€</p>
<p>â€œThings changed in early 2016 when OneRep.com began offering privacy removal services right alongside their own open displays of your personal information. At this point when you found yourself on Nuwber.com OR OneRep.com, you would be provided with the option of opting-out your data on their site for free â€“ but also be highly encouraged to pay them to remove it from a slew of other sites (and part of that payment was removing you from their own site, Nuwber.com, as a benefit of their service).â€</p></blockquote>
<p>Reached via LinkedIn, Mr. Bukuyazau declined to answer questions, such as whether he ever worked at Nuwber.com. However, Constella Intelligence finds two interesting email addresses for employees at nuwber.com: d.bu@nuwber.com, and d.bu+figure-eight.com@nuwber.com, which was registered under the name â€œ<strong>Dzmitry</strong>.â€</p>
<p>PrivacyDuckâ€™s claims about how onerep.com appeared and behaved in the early days are not readily verifiable because the domain onerep.com has been completely excluded from the Wayback Machine at archive.org. The Wayback Machine will honor such requests if they come directly from the owner of the domain in question.</p>
<p>Still, Mr. Shelestâ€™s name, phone number and email also appear in the domain registration records for a truly dizzying number of country-specific people-search services, including <strong>pplcrwlr.in</strong>, <a href="https://web.archive.org/web/20150205232653/http://pplcrwlr.fr/" target="_blank" rel="noopener">pplcrwlr.fr</a>, <a href="https://web.archive.org/web/20150206043953/http://pplcrwlr.dk/" target="_blank" rel="noopener">pplcrwlr.dk</a>, <a href="https://web.archive.org/web/20150206005352/http://pplcrwlr.jp/" target="_blank" rel="noopener">pplcrwlr.jp</a>, <strong>peeepl.br.com</strong>, <strong>peeepl.in</strong>, <a href="https://web.archive.org/web/20130307194029/http://peeepl.it/" target="_blank" rel="noopener">peeepl.it</a> and <a href="https://web.archive.org/web/20140320054902/http://peeepl.co.uk/" target="_blank" rel="noopener">peeepl.co.uk</a>.</p>
<p>The same details appear in the WHOIS registration records for the now-defunct people-search sites <a href="https://web.archive.org/web/20140208014115/http://waatpp.de/" target="_blank" rel="noopener">waatpp.de</a>, <a href="https://web.archive.org/web/20150205232901/http://waatp1.fr/" target="_blank" rel="noopener">waatp1.fr</a>, <a href="https://web.archive.org/web/20120112131753/http://azersab.com/" target="_blank" rel="noopener">azersab.com</a>, and <a href="https://web.archive.org/web/20120507115825/http://ahavoila.com/" target="_blank" rel="noopener">ahavoila.com</a>, a people-search service for French citizens.<span id="more-66752"></span></p>
<div id="attachment_66756"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66756" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/waatp-de-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66756">The German people-search site waatp.de.</p></div>
<p>A search on the email address dmitrcox@gmail.com suggests Mr. Shelest was previously involved in rather aggressive email marketing campaigns. In 2010, an anonymous source <a href="https://krebsonsecurity.com/2010/09/spam-affialite-program-spamit-com-to-close/" target="_blank" rel="noopener">leaked to KrebsOnSecurity</a> the financial and organizational records of <a href="https://krebsonsecurity.com/tag/spamit/" target="_blank" rel="noopener"><strong>Spamit</strong></a>, which at the time was easily the largest Russian-language pharmacy spam affiliate program in the world.</p>
<p>Spamit paid spammers a hefty commission every time someone bought male enhancement drugs from any of their spam-advertised websites. Mr. Shelestâ€™s email address stood out because immediately after the Spamit database was leaked, KrebsOnSecurity searched all of the Spamit affiliate email addresses to determine if any of them corresponded to social media accounts at <strong>Facebook.com</strong> (at the time, Facebook allowed users to search profiles by email address).</p>
<p>That mapping, which was done mainly by generous graduate students at my alma mater <strong>George Mason University</strong>, revealed that dmitrcox@gmail.com was used by a Spamit affiliate, albeit not a very profitable one. That same <a href="https://www.facebook.com/dmitry.shelest" target="_blank" rel="noopener">Facebook profile for Mr. Shelest</a> is still active, and it says he is married and living in Minsk (last update: 2021).</p>
<div id="attachment_66757"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66757" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/peeepl.it_-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66757">The Italian people-search website peeepl.it.</p></div>
<p>Scrolling down Mr. Shelestâ€™s Facebook page to posts made more than ten years ago show him liking the Facebook profile pages for a large number of other people-search sites, including <a href="https://web.archive.org/web/20120215164504/http://findita.com/" target="_blank" rel="noopener">findita.com</a>, <a href="https://web.archive.org/web/20120611000147/http://findmedo.com/" target="_blank" rel="noopener">findmedo.com</a>, <a href="https://web.archive.org/web/20120202191444/http://folkscan.com/" target="_blank" rel="noopener">folkscan.com</a>, <a href="https://web.archive.org/web/20120204021306/http://huntize.com/" target="_blank" rel="noopener">huntize.com</a>, <a href="https://web.archive.org/web/20120201003010/http://ifindy.com/" target="_blank" rel="noopener">ifindy.com</a>, <a href="https://web.archive.org/web/20130209191213/http://jupery.com/" target="_blank" rel="noopener">jupery.com</a>, <a href="https://web.archive.org/web/20120214040919/http://look2man.com/" target="_blank" rel="noopener">look2man.com</a>, <a href="https://web.archive.org/web/20130318231852/http://lookerun.com/" target="_blank" rel="noopener">lookerun.com</a>, <a href="https://web.archive.org/web/20120217181822/http://manyp.com/" target="_blank" rel="noopener">manyp.com</a>, peepull.com, <a href="https://web.archive.org/web/20130425195446/http://perserch.com/" target="_blank" rel="noopener">perserch.com</a>, <a href="https://web.archive.org/web/20140104023637/http://persuer.com/" target="_blank" rel="noopener">persuer.com</a>, <a href="https://web.archive.org/web/20121001072151/http://pervent.com/" target="_blank" rel="noopener">pervent.com</a>, <a href="https://web.archive.org/web/20130116123555/http://piplenter.com/" target="_blank" rel="noopener">piplenter.com</a>, <a href="https://web.archive.org/web/20120215174246/http://piplfind.com/" target="_blank" rel="noopener">piplfind.com</a>, <a href="https://web.archive.org/web/20130302034909/http://piplscan.com/" target="_blank" rel="noopener">piplscan.com</a>, <a href="https://web.archive.org/web/20110210162153/http://popopke.com/" target="_blank" rel="noopener">popopke.com</a>, <a href="https://web.archive.org/web/20120210030201/http://pplsorce.com/" target="_blank" rel="noopener">pplsorce.com</a>, <a href="https://web.archive.org/web/20130215180627/http://qimeo.com/" target="_blank" rel="noopener">qimeo.com</a>, <a href="https://web.archive.org/web/20120125085744/http://scoutu2.com/" target="_blank" rel="noopener">scoutu2.com</a>, <a href="https://web.archive.org/web/20120215092423/http://search64.com/" target="_blank" rel="noopener">search64.com</a>, <a href="https://web.archive.org/web/20120204035144/http://searchay.com/" target="_blank" rel="noopener">searchay.com</a>, <a href="https://web.archive.org/web/20120215082510/http://seekmi.com/" target="_blank" rel="noopener">seekmi.com</a>, <a href="https://web.archive.org/web/20120502175915/http://selfabc.com/" target="_blank" rel="noopener">selfabc.com</a>, <a href="https://web.archive.org/web/20120211043052/http://socsee.com/" target="_blank" rel="noopener">socsee.com</a>, <a href="https://web.archive.org/web/20130116150616/http://srching.com/" target="_blank" rel="noopener">srching.com</a>, <a href="https://web.archive.org/web/20120204100134/http://toolooks.com/" target="_blank" rel="noopener">toolooks.com</a>, <a href="https://web.archive.org/web/20120706142742/http://upearch.com/" target="_blank" rel="noopener">upearch.com</a>, <a href="https://web.archive.org/web/20120306122418/http://webmeek.com/" target="_blank" rel="noopener">webmeek.com</a>, and many country-code variations of <a href="https://web.archive.org/web/20131103141811/http://viadin.ca/" target="_blank" rel="noopener">viadin.ca</a> (e.g. <a href="https://web.archive.org/web/20130427065532/http://viadin.hk/" target="_blank" rel="noopener">viadin.hk</a>, <a href="https://web.archive.org/web/20130805064112/http://viadin.com/" target="_blank" rel="noopener">viadin.com</a> and <a href="https://web.archive.org/web/20130428035516/http://viadin.de/" target="_blank" rel="noopener">viadin.de</a>).</p>
<div id="attachment_66758"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66758" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/popoke-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></a></p><p id="caption-attachment-66758">The people-search website popopke.com.</p></div>
<p>Domaintools.com finds that all of the domains mentioned in the last paragraph were registered to the email address dmitrcox@gmail.com.</p>
<p>Mr. Shelest has not responded to multiple requests for comment. KrebsOnSecurity also sought comment from onerep.com, which likewise has not responded to inquiries about its founderâ€™s many apparent conflicts of interest. In any event, these practices would seem to contradict the goal Onerep has stated on its site: â€œWe believe that no one should compromise personal online security and get a profit from it.â€</p>
<div id="attachment_66759"><p><a href="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png" target="_blank" rel="noopener"><img aria-describedby="caption-attachment-66759" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png" alt="" width="748" height="319" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-768x327.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-1536x654.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/findmedo-782x333.png 782w" sizes="(max-width: 748px) 100vw, 748px"></a></p><p id="caption-attachment-66759">The people-search website findmedo.com.</p></div>
<p><strong>Max Anderson</strong> is chief growth officer at <a href="https://www.360privacy.io/" rel="noopener" target="_blank">360 Privacy</a>, a legitimate privacy company that works to keep its clientsâ€™ data off of more than 400 data broker and people-search sites. Anderson said it is concerning to see a direct link between between a data removal service and data broker websites. </p>
<p>â€œI would consider it unethical to run a company that sells peopleâ€™s information, and then charge those same people to have their information removed,â€ Anderson said. </p>
<p>Last week, KrebsOnSecurity published <a href="https://krebsonsecurity.com/2024/03/a-close-up-look-at-the-consumer-data-broker-radaris/" target="_blank" rel="noopener">an analysis of the people-search data broker giant <strong>Radaris</strong></a>, whose consumer profiles are deep enough to rival those of far more guarded data broker resources available to U.S. police departments and other law enforcement personnel.</p>
<p>That story revealed that the co-founders of Radaris are two native Russian brothers who operate multiple Russian-language dating services and affiliate programs. It also appears many of the Radaris foundersâ€™ businesses have ties to a California marketing firm that works with a Russian state-run media conglomerate currently sanctioned by the U.S. government.</p>
<p><img decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/03/search64.png" alt="" width="750" height="345" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/03/search64.png 1880w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-768x353.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-1536x707.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2024/03/search64-782x360.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p>
<p>KrebsOnSecurity will continue investigating the history of various consumer data brokers and people-search providers. If any readers have inside knowledge of this industry or key players within it, please consider reaching out to krebsonsecurity at gmail.com.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Officially Raises Minimum Broadband Metric from 25Mbps to 100Mbps (464 pts)]]></title>
            <link>https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps</link>
            <guid>39708957</guid>
            <pubDate>Thu, 14 Mar 2024 21:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps">https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps</a>, See on <a href="https://news.ycombinator.com/item?id=39708957">Hacker News</a></p>
Couldn't get https://www.pcmag.com/news/fcc-officially-raises-minimum-broadband-metric-from-25mbps-to-100mbps: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[More powerful Go execution traces (276 pts)]]></title>
            <link>https://go.dev/blog/execution-traces-2024</link>
            <guid>39708591</guid>
            <pubDate>Thu, 14 Mar 2024 20:30:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://go.dev/blog/execution-traces-2024">https://go.dev/blog/execution-traces-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39708591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-slug="/blog/execution-traces-2024">
    
    <h2><a href="https://go.dev/blog/">The Go Blog</a></h2>
    

    
      
      
      
      <p>The <a href="https://go.dev/pkg/runtime/trace">runtime/trace</a> package contains a powerful tool for understanding and
troubleshooting Go programs.
The functionality within allows one to produce a trace of each goroutineâ€™s execution over some
time period.
With the <a href="https://go.dev/pkg/cmd/trace"><code>go tool trace</code> command</a> (or the excellent open source
<a href="https://gotraceui.dev/" rel="noreferrer" target="_blank">gotraceui tool</a>), one may then visualize and explore the data within these
traces.</p>
<p>The magic of a trace is that it can easily reveal things about a program that are hard to see in
other ways.
For example, a concurrency bottleneck where lots of goroutines block on the same channel might be
quite difficult to see in a CPU profile, because thereâ€™s no execution to sample.
But in an execution trace, the <em>lack</em> of execution will show up with amazing clarity, and the stack
traces of blocked goroutines will quickly point at the culprit.</p>
<p><img src="https://go.dev/blog/execution-traces-2024/gotooltrace.png" alt="">
</p>
<p>Go developers are even able to instrument their own programs with <a href="https://go.dev/pkg/runtime/trace#Task">tasks</a>,
<a href="https://go.dev/pkg/runtime/trace#WithRegion">regions</a>, and <a href="https://go.dev/pkg/runtime/trace#Log">logs</a> that
they can use to correlate their higher-level concerns with lower-level execution details.</p>
<h2 id="issues">Issues</h2>
<p>Unfortunately, the wealth of information in execution traces can often be out of reach.
Four big issues with traces have historically gotten in the way.</p>
<ul>
<li>Traces had high overheads.</li>
<li>Traces didnâ€™t scale well, and could become too big to analyze.</li>
<li>It was often unclear when to start tracing to capture a specific bad behavior.</li>
<li>Only the most adventurous gophers could programmatically analyze traces, given the lack of a
public package for parsing and interpreting execution traces.</li>
</ul>
<p>If youâ€™ve used traces in the last few years, youâ€™ve likely been frustrated by one or more of these
problems.
But weâ€™re excited to share that over the last two Go releases weâ€™ve made big progress in all four
of these areas.</p>
<h2 id="low-overhead-tracing">Low-overhead tracing</h2>
<p>Prior to Go 1.21, the run-time overhead of tracing was somewhere between 10â€“20% CPU for many
applications, which limits tracing to situational usage, rather than continuous usage like CPU
profiling.
It turned out that much of the cost of tracing came down to tracebacks.
Many events produced by the runtime have stack traces attached, which are invaluable to actually
identifying what goroutines where doing at key moments in their execution.</p>
<p>Thanks to work by Felix GeisendÃ¶rfer and Nick Ripley on optimizing the efficiency of tracebacks,
the run-time CPU overhead of execution traces has been cut dramatically, down to 1â€“2% for many
applications.
You can read more about the work done here in <a href="https://blog.felixge.de/reducing-gos-execution-tracer-overhead-with-frame-pointer-unwinding/" rel="noreferrer" target="_blank">Felixâ€™s great blog
post</a>
on the topic.</p>
<h2 id="scalable-traces">Scalable traces</h2>
<p>The trace format and its events were designed around relatively efficient emission, but required
tooling to parse and keep around the state of the entirety of a trace.
A few hundred MiB trace could require several GiB of RAM to analyze!</p>
<p>This issue is unfortunately fundamental to how traces are generated.
To keep run-time overheads low, all events are written to the equivalent of thread-local buffers.
But this means events appear out of their true order, and the burden is placed on the trace
tooling to figure out what really happened.</p>
<p>The key insight to making traces scale while keeping overheads low was to occasionally split the
trace being generated.
Each split point would behave a bit like simultaneously disabling and reenabling tracing in one
go.
All the trace data so far would represent a complete and self-contained trace, while the new trace
data would seamlessly pick up from where it left off.</p>
<p>As you might imagine, fixing this required <a href="https://go.dev/issue/60773">rethinking and rewriting a lot of the foundation of
the trace implementation</a> in the runtime.
Weâ€™re happy to say that the work landed in Go 1.22 and is now generally available.
<a href="https://go.dev/doc/go1.22#runtime/trace">A lot of nice improvements</a> came with the rewrite, including some
improvements to the <a href="https://go.dev/doc/go1.22#trace"><code>go tool trace</code> command</a> as well.
The gritty details are all in the <a href="https://github.com/golang/proposal/blob/master/design/60773-execution-tracer-overhaul.md" rel="noreferrer" target="_blank">design
document</a>,
if youâ€™re curious.</p>
<p>(Note: <code>go tool trace</code> still loads the full trace into memory, but <a href="https://go.dev/issue/65315">removing this
limitation</a> for traces produced by Go 1.22+ programs is now feasible.)</p>
<h2 id="flight-recording">Flight recording</h2>
<p>Suppose you work on a web service and an RPC took a very long time.
You couldnâ€™t start tracing at the point you knew the RPC was already taking a while, because the
root cause of the slow request already happened and wasnâ€™t recorded.</p>
<p>Thereâ€™s a technique that can help with this called flight recording, which you may already be
familiar with from other programming environments.
The insight with flight recording is to have tracing on continuously and always keep the most
recent trace data around, just in case.
Then, once something interesting happens, the program can just write out whatever it has!</p>
<p>Before traces could be split, this was pretty much a non-starter.
But because continuous tracing is now viable thanks to low overheads, and the fact that the runtime
can now split traces any time it needs, it turns out it was straightforward to implement flight
recording.</p>
<p>As a result, weâ€™re happy to announce a flight recorder experiment, available in the
<a href="https://go.dev/pkg/golang.org/x/exp/trace#FlightRecorder">golang.org/x/exp/trace package</a>.</p>
<p>Please try it out!
Below is an example that sets up flight recording to capture a long HTTP request to get you started.</p>
<div>
<pre>    
    fr := trace.NewFlightRecorder()
    fr.Start()

    
    var once sync.Once
    http.HandleFunc("/my-endpoint", func(w http.ResponseWriter, r *http.Request) {
        start := time.Now()

        
        doWork(w, r)

        
        if time.Since(start) &gt; 300*time.Millisecond {
            
            once.Do(func() {
                
                var b bytes.Buffer
                _, err = fr.WriteTo(&amp;b)
                if err != nil {
                    log.Print(err)
                    return
                }
                
                if err := os.WriteFile("trace.out", b.Bytes(), 0o755); err != nil {
                    log.Print(err)
                    return
                }
            })
        }
    })
    log.Fatal(http.ListenAndServe(":8080", nil))
</pre>
</div>
<p>If you have any feedback, positive or negative, please share it to the <a href="https://go.dev/issue/63185">proposal
issue</a>!</p>
<h2 id="trace-reader-api">Trace reader API</h2>
<p>Along with the trace implementation rewrite came an effort to clean up the other trace internals,
like <code>go tool trace</code>.
This spawned an attempt to create a trace reader API that was good enough to share and that could
make traces more accessible.</p>
<p>Just like the flight recorder, weâ€™re happy to announce that we also have an experimental trace reader
API that weâ€™d like to share.
Itâ€™s available in the <a href="https://go.dev/pkg/golang.org/x/exp/trace#Reader">same package as the flight recorder,
golang.org/x/exp/trace</a>.</p>
<p>We think itâ€™s good enough to start building things on top of, so please try it out!
Below is an example that measures the proportion of goroutine block events that blocked to wait on
the network.</p>
<div>
<pre>    
    r, err := trace.NewReader(os.Stdin)
    if err != nil {
        log.Fatal(err)
    }

    var blocked int
    var blockedOnNetwork int
    for {
        
        ev, err := r.ReadEvent()
        if err == io.EOF {
            break
        } else if err != nil {
            log.Fatal(err)
        }

        
        if ev.Kind() == trace.EventStateTransition {
            st := ev.StateTransition()
            if st.Resource.Kind == trace.ResourceGoroutine {
                id := st.Resource.Goroutine()
                from, to := st.GoroutineTransition()

                
                if from.Executing() &amp;&amp; to == trace.GoWaiting {
                    blocked++
                    if strings.Contains(st.Reason, "network") {
                        blockedOnNetwork++
                    }
                }
            }
        }
    }
    
    p := 100 * float64(blockedOnNetwork) / float64(blocked)
    fmt.Printf("%2.3f%% instances of goroutines blocking were to block on the network\n", p)
</pre>
</div>
<p>And just like the flight recorder, thereâ€™s a <a href="https://go.dev/issue/62627">proposal issue</a> that would
be a great place to leave feedback!</p>
<p>Weâ€™d like to quickly call out Dominik Honnef as someone who tried it out early, provided great
feedback, and has contributed support for older trace versions to the API.</p>
<h2 id="thank-you">Thank you!</h2>
<p>This work was completed, in no small part, thanks to the help of the those in the <a href="https://go.dev/issue/57175">diagnostics
working group</a>, started over a year ago as a collaboration between stakeholders from
across the Go community, and open to the public.</p>
<p>Weâ€™d like to take a moment to thank those community members who have attended the diagnostic
meetings regularly over the last year: Felix GeisendÃ¶rfer, Nick Ripley, Rhys Hiltner, Dominik
Honnef, Bryan Boreham, thepudds.</p>
<p>The discussions, feedback, and work you all put in have been instrumental to getting us to where we
are today.
Thank you!</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pornhub Blocked in Texas (103 pts)]]></title>
            <link>https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/</link>
            <guid>39708347</guid>
            <pubDate>Thu, 14 Mar 2024 20:03:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/">https://variety.com/2024/digital/news/pornhub-texas-blocked-age-verification-law-1235942280/</a>, See on <a href="https://news.ycombinator.com/item?id=39708347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
	<a href="https://variety.com/t/pornhub/" id="auto-tag_pornhub" data-tag="pornhub">Pornhub</a> and other affiliated adult websites have blocked access to users in Texas, amid a legal battle with the Lone Star Stateâ€™s attorney general over an age-verification law.</p>



<p>
	Last week, a federal appeals court <a href="https://www.ca5.uscourts.gov/opinions/pub/23/23-50627-CV0.pdf" target="_blank" rel="noreferrer noopener nofollow">upheld a Texas law</a> requiring pornography sites to institute age-verification measures to ensure only adults 18 and older are able to access them, while it also struck down a part of the law requiring porn sites to <a href="https://variety.com/2023/digital/news/pornhubs-texas-age-verification-law-violates-first-amendment-ruling-1235709902/">display â€œhealth warningsâ€</a> about the content. That came after a previous federal judgeâ€™s ruling that <a href="https://variety.com/2023/digital/news/pornhubs-texas-age-verification-law-violates-first-amendment-ruling-1235709902/">the Texas law violated the U.S. Constitutionâ€™s&nbsp;First Amendment</a>&nbsp;prohibition against free-speech restrictions. The Texas Attorney Generalâ€™s Office immediately appealed that decision.<a rel="noreferrer noopener nofollow" href="https://www.kargo.com/privacy" target="_blank"></a>

	</p>




<p>
	A new message displayed Thursday to users with internet addresses in Texas on Pornhub (and other sites operated by parent company <a href="https://variety.com/t/aylo/" id="auto-tag_aylo" data-tag="aylo">Aylo</a>) explained that it was disabling access to comply with the law, as first <a href="https://www.houstonchronicle.com/politics/texas/article/pornhub-blocked-texas-age-verification-19021482.php" target="_blank" rel="noreferrer noopener nofollow">reported</a> by the Houston Chronicle.</p>



<p>
	â€œAs you may know, your elected officials in Texas are requiring us to verify your age before allowing you access to our website. Not only does this impinge on the rights of adults to access protected speech, it fails strict scrutiny by employing the least effective and yet also most restrictive means of accomplishing Texasâ€™s stated purpose of allegedly protecting minors,â€ the message reads in part.

</p>



<p>
	The Pornhub sitesâ€™ message continued, â€œUntil the real solution is offered, we have made the difficult decision to completely disable access to our website in Texas. In doing so, we are complying with the law, as we always do, but hope that governments around the world will implement laws that actually protect the safety and security of users.â€</p>



<p>
	According to the message, â€œWhile safety and compliance are at the forefront of our mission, providing identification every time you want to visit an adult platform is not an effective solution for protecting users online, and in fact, will put minors and your privacy at risk.â€</p>



<p>
	Pornhub called the Texas age-verification â€œineffective, haphazard and dangerousâ€ and asserted that it will drive users â€œfrom those few websites which comply, to the thousands of websites, with far fewer safety measures in place, which do not comply. Very few sites are able to compare to the robust Trust and Safety measures we currently have in place. To protect minors and user privacy, any legislation must be enforced against all platforms offering adult content.â€</p>



<p>
	The Texas law was signed by Republican Gov. Greg Abbott in June 2023. The legislation, Texas H.B. 11811, was scheduled to go into effect on Sept. 1, but it was on hold after the lawsuit filed by the Free Speech Coalition (a group that includes Pornhubâ€™s parent company) resulted in a preliminary injunction staying its enforcement. The law applies to online publishers whose content is more than one-third â€œsexual material harmful to minorsâ€ and requires them to verify the age of all visitors using a government-issued ID or â€œpublic or private transactional data.â€

	</p>




<p>
	Reached for comment, the company provided a statement from Alex Kekesi, VP of brand and community at Aylo, which said in part: â€œThis is not the end. We are reviewing options and consulting with our legal teamâ€¦ We will continue to fight for our industry and the performers that legally earn a living, and we will continue to appeal through all available judicial recourse to recognize that this law is unconstitutional.â€</p>



<p>
	Keksi also said in the statement that Aylo â€œhas publicly supported age verification of users for years, but we believe that any law to this effect must ensure minors do not access content intended for adults and preserve user safety and privacy. We believe that the real solution for protecting minors and adults alike is to verify usersâ€™ ages at the point of access â€” the usersâ€™ devices â€” and to deny or permit access to age-restricted materials and websites based on that verification.â€</p>



<p>
	Pornhub and the companyâ€™s network of other sites are also blocked or restricted in at least seven other U.S. states â€” Arkansas, Louisiana, Mississippi, Montana, North Carolina, Virginia and Utah â€” which have adopted similar laws.</p>



<p>
	<a href="https://variety.com/2023/digital/news/pornhub-parent-name-change-aylo-adult-entertainment-1235700312/">Aylo is owned by Canadian private-equity firm Ethical Capital Partners</a>, which acquired Pornhubâ€™s predecessor company MindGeek for undisclosed financial terms last year. ECP has said it would focus on building the companyâ€™s â€œtrust and safetyâ€ and to make it â€œthe internet leader in fighting illegal online content.â€</p>



	<h3>
		<a href="https://variety.com/vip/playstation-state-of-play-advantage-over-xbox-kojima-1235893959/"></a>	</h3>
















</div></div>]]></description>
        </item>
    </channel>
</rss>