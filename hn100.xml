<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 31 Aug 2023 12:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[*@gmail.com (145 pts)]]></title>
            <link>https://xkcd.com/2822/</link>
            <guid>37333848</guid>
            <pubDate>Thu, 31 Aug 2023 07:54:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xkcd.com/2822/">https://xkcd.com/2822/</a>, See on <a href="https://news.ycombinator.com/item?id=37333848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bottom">
<p><img src="https://imgs.xkcd.com/s/a899e84.jpg" width="520" height="100" alt="Selected Comics" usemap="#comicmap"></p><map id="comicmap" name="comicmap">
<area shape="rect" coords="0,0,100,100" href="/150/" alt="Grownups">
<area shape="rect" coords="104,0,204,100" href="/730/" alt="Circuit Diagram">
<area shape="rect" coords="208,0,308,100" href="/162/" alt="Angular Momentum">
<area shape="rect" coords="312,0,412,100" href="/688/" alt="Self-Description">
<area shape="rect" coords="416,0,520,100" href="/556/" alt="Alternative Energy Revolution">
</map>

<p><a href="https://xkcd.com/1732/"><img src="https://imgs.xkcd.com/s/temperature.png" width="520" height="100" alt="Earth temperature timeline"></a></p>
<br>
<div id="comicLinks"><p>
Comics I enjoy:<br>
        <a href="http://threewordphrase.com/">Three Word Phrase</a>,
        <a href="https://www.smbc-comics.com/">SMBC</a>,
        <a href="https://www.qwantz.com/">Dinosaur Comics</a>,
        <a href="https://oglaf.com/">Oglaf</a> (nsfw),
        <a href="https://www.asofterworld.com/">A Softer World</a>,
        <a href="https://buttersafe.com/">Buttersafe</a>,
        <a href="https://pbfcomics.com/">Perry Bible Fellowship</a>,
        <a href="https://questionablecontent.net/">Questionable Content</a>,
        <a href="http://www.buttercupfestival.com/">Buttercup Festival</a>,
        <a href="https://www.homestuck.com/">Homestuck</a>,
	<a href="https://www.jspowerhour.com/">Junior Scientist Power Hour</a>
</p></div>
<br>

<br>
<center>
<p>xkcd.com is best viewed with Netscape Navigator 4.0 or below on a Pentium 3±1 emulated in Javascript on an Apple IIGS<br>at a screen resolution of 1024x1. Please enable your ad blockers, disable high-heat drying, and remove your device<br>from Airplane Mode and set it to Boat Mode. For security reasons, please leave caps lock on while browsing.</p>
</center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UTM – Virtual Machines for iOS and macOS (106 pts)]]></title>
            <link>https://github.com/utmapp/UTM</link>
            <guid>37333404</guid>
            <pubDate>Thu, 31 Aug 2023 06:53:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/utmapp/UTM">https://github.com/utmapp/UTM</a>, See on <a href="https://news.ycombinator.com/item?id=37333404">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">UTM</h2>
<p dir="auto"><a href="https://github.com/utmapp/UTM/actions?query=event%3Arelease+workflow%3ABuild"><img src="https://github.com/utmapp/UTM/workflows/Build/badge.svg?branch=master&amp;event=push" alt="Build"></a></p>
<blockquote>
<p dir="auto">It is possible to invent a single machine which can be used to compute any computable sequence.</p>
</blockquote>
<p dir="auto">-- Alan Turing, 1936</p>
<p dir="auto">UTM is a full featured system emulator and virtual machine host for iOS and macOS. It is based off of QEMU. In short, it allows you to run Windows, Linux, and more on your Mac, iPhone, and iPad. More information at <a href="https://getutm.app/" rel="nofollow">https://getutm.app/</a> and <a href="https://mac.getutm.app/" rel="nofollow">https://mac.getutm.app/</a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/utmapp/UTM/blob/main/screen.png"><img width="450px" alt="UTM running on an iPhone" src="https://github.com/utmapp/UTM/raw/main/screen.png"></a>
  <br>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/utmapp/UTM/blob/main/screenmac.png"><img width="450px" alt="UTM running on a MacBook" src="https://github.com/utmapp/UTM/raw/main/screenmac.png"></a>
</p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li>Full system emulation (MMU, devices, etc) using QEMU</li>
<li>30+ processors supported including x86_64, ARM64, and RISC-V</li>
<li>VGA graphics mode using SPICE and QXL</li>
<li>Text terminal mode</li>
<li>USB devices</li>
<li>JIT based acceleration using QEMU TCG</li>
<li>Frontend designed from scratch for macOS 11 and iOS 11+ using the latest and greatest APIs</li>
<li>Create, manage, run VMs directly from your device</li>
</ul>
<h2 tabindex="-1" dir="auto">Additional macOS Features</h2>
<ul dir="auto">
<li>Hardware accelerated virtualization using Hypervisor.framework and QEMU</li>
<li>Boot macOS guests with Virtualization.framework on macOS 12+</li>
</ul>
<h2 tabindex="-1" dir="auto">UTM SE</h2>
<p dir="auto">UTM/QEMU requires dynamic code generation (JIT) for maximum performance. JIT on iOS devices require either a jailbroken device, or one of the various workarounds found for specific versions of iOS (see "Install" for more details).</p>
<p dir="auto">UTM SE ("slow edition") uses a <a href="https://github.com/ktemkin/qemu/blob/with_tcti/tcg/aarch64-tcti/README.md">threaded interpreter</a> which performs better than a traditional interpreter but still slower than JIT. This technique is similar to what <a href="https://github.com/ish-app/ish">iSH</a> does for dynamic execution. As a result, UTM SE does not require jailbreaking or any JIT workarounds and can be sideloaded as a regular app.</p>
<p dir="auto">To optimize for size and build times, only the following architectures are included in UTM SE: ARM, PPC, RISC-V, and x86 (all with both 32-bit and 64-bit variants).</p>
<h2 tabindex="-1" dir="auto">Install</h2>
<p dir="auto">UTM (SE) for iOS: <a href="https://getutm.app/install/" rel="nofollow">https://getutm.app/install/</a></p>
<p dir="auto">UTM is also available for macOS: <a href="https://mac.getutm.app/" rel="nofollow">https://mac.getutm.app/</a></p>
<h2 tabindex="-1" dir="auto">Development</h2>
<h3 tabindex="-1" dir="auto"><a href="https://github.com/utmapp/UTM/blob/main/Documentation/MacDevelopment.md">macOS Development</a></h3>
<h3 tabindex="-1" dir="auto"><a href="https://github.com/utmapp/UTM/blob/main/Documentation/iOSDevelopment.md">iOS Development</a></h3>
<h2 tabindex="-1" dir="auto">Related</h2>
<ul dir="auto">
<li><a href="https://github.com/ish-app/ish">iSH</a>: emulates a usermode Linux terminal interface for running x86 Linux applications on iOS</li>
<li><a href="https://github.com/holzschu/a-shell">a-shell</a>: packages common Unix commands and utilities built natively for iOS and accessible through a terminal interface</li>
</ul>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">UTM is distributed under the permissive Apache 2.0 license. However, it uses several (L)GPL components. Most are dynamically linked but the gstreamer plugins are statically linked and parts of the code are taken from qemu. Please be aware of this if you intend on redistributing this application.</p>
<p dir="auto">Some icons made by <a href="https://www.freepik.com/" rel="nofollow">Freepik</a> from <a href="https://www.flaticon.com/" rel="nofollow">www.flaticon.com</a>.</p>
<p dir="auto">Additionally, UTM frontend depends on the following MIT/BSD License components:</p>
<ul dir="auto">
<li><a href="https://github.com/hackiftekhar/IQKeyboardManager">IQKeyboardManager</a></li>
<li><a href="https://github.com/migueldeicaza/SwiftTerm">SwiftTerm</a></li>
<li><a href="https://github.com/weichsel/ZIPFoundation">ZIP Foundation</a></li>
<li><a href="https://github.com/futuretap/InAppSettingsKit">InAppSettingsKit</a></li>
</ul>
<p dir="auto">Continuous integration hosting is provided by <a href="https://www.macstadium.com/opensource" rel="nofollow">MacStadium</a></p>
<p dir="auto"><a href="https://www.macstadium.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/6fb41aa454e96395da7edc953b15b836f150cff09655a443a9c99c3b7ba44749/68747470733a2f2f75706c6f6164732d73736c2e776562666c6f772e636f6d2f3561633363303436633832373234393730666336303931382f3563303139643931376262613331326166373535336234395f4d61635374616469756d2d646576656c6f7065726c6f676f2e706e67" alt="MacStadium logo" width="250" data-canonical-src="https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Money is pouring into AI. Skeptics say it’s a ‘grift shift’ (117 pts)]]></title>
            <link>https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift</link>
            <guid>37332974</guid>
            <pubDate>Thu, 31 Aug 2023 05:55:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift">https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift</a>, See on <a href="https://news.ycombinator.com/item?id=37332974">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
    



                
    



                
                    
    <div><figure>
    

    
        <picture>
    
    
            <source media="(max-width: 568px)" type="image/webp" width="568" height="331" srcset="https://cdn.assetmg.info/dims4/default/f021e75/2147483647/strip/true/crop/1200x700+0+0/resize/568x331!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 1x,https://cdn.assetmg.info/dims4/default/510328d/2147483647/strip/true/crop/1200x700+0+0/resize/1136x662!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 2x">

    

    
        <source media="(max-width: 568px)" width="568" height="331" srcset="https://cdn.assetmg.info/dims4/default/9c68797/2147483647/strip/true/crop/1200x700+0+0/resize/568x331!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg">

    


    
    
    
        
            
    
            <source media="(min-width: 1024px)" type="image/webp" width="1440" height="840" srcset="https://cdn.assetmg.info/dims4/default/641eb9b/2147483647/strip/true/crop/1200x700+0+0/resize/1440x840!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 1x,https://cdn.assetmg.info/dims4/default/6c10219/2147483647/strip/true/crop/1200x700+0+0/resize/2880x1680!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 2x">

    

    
        <source media="(min-width: 1024px)" width="1440" height="840" srcset="https://cdn.assetmg.info/dims4/default/ca6868c/2147483647/strip/true/crop/1200x700+0+0/resize/1440x840!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg">

    


        
    

    
    
        
    
            <source type="image/webp" width="1024" height="597" srcset="https://cdn.assetmg.info/dims4/default/dc99ceb/2147483647/strip/true/crop/1200x700+0+0/resize/1024x597!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 1x,https://cdn.assetmg.info/dims4/default/d86b705/2147483647/strip/true/crop/1200x700+0+0/resize/2048x1194!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 2x">

    

    
        <source width="1024" height="597" srcset="https://cdn.assetmg.info/dims4/default/f4ac9e6/2147483647/strip/true/crop/1200x700+0+0/resize/1024x597!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg">

    


    
    
    <img alt="Art_grift_shift_feature.jpg" srcset="https://cdn.assetmg.info/dims4/default/f4ac9e6/2147483647/strip/true/crop/1200x700+0+0/resize/1024x597!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 1x,https://cdn.assetmg.info/dims4/default/addf851/2147483647/strip/true/crop/1200x700+0+0/resize/2048x1194!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg 2x" width="1024" height="597" src="https://cdn.assetmg.info/dims4/default/f4ac9e6/2147483647/strip/true/crop/1200x700+0+0/resize/1024x597!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2F1e%2F9c%2Fcf470f664a3cab27db4b61b7db27%2Fart-grift-shift-feature.jpg" loading="lazy">


</picture>

    

    
        <div><figcaption><p>Illustration by II</p></figcaption></div>
    
</figure>
</div>


                

                <div>
                    
    <h2>The move from crypto to artificial intelligence has fueled the markets this year, but some <br>are questioning how much of it is real.</h2>


                    

                </div>
            </div><div data-modulewell="">
            <main data-module="" data-padding="none">
                <div>
                        <p>By the time a dormant penny stock company known as Applied Sciences managed to wrangle a listing on the Nasdaq in April 2022, it had reinvented itself as a cloud hosting service for bitcoin miners and changed its name to Applied Blockchain. But with the crypto world crashing that spring, the stock never took off. Within months, Applied Blockchain pivoted again — renaming itself Applied Digital.</p><p>If its previous iteration had been too late to cash in on the bitcoin mining craze, the company wasn’t going to miss the next big one: artificial intelligence.</p>
    

<p>Applied Digital’s stock finally began to soar in May,<b> </b>when CEO Wes Cummins announced that the company had signed a cloud hosting deal potentially worth $180 million with an unnamed but prominent AI customer, and another one for up to $460 million with another big player in the booming AI space. By July, the stock had surged some 450 percent for the year, becoming — for a time, at least — one of the big winners in today’s AI-driven stock market.</p><div data-category-uppercase="true" data-sponsored="true"><a aria-label="Alpha Opportunities in Emerging Markets" href="https://www.institutionalinvestor.com/article/2c1111olwhbe42u6talfk/innovation/alpha-opportunities-in-emerging-markets" data-cms-ai="0"><picture>
    
    
            <source media="(max-width: 568px)" type="image/webp" width="100" height="100" srcset="https://cdn.assetmg.info/dims4/default/ce90595/2147483647/strip/true/crop/365x365+110+0/resize/100x100!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 1x,https://cdn.assetmg.info/dims4/default/0fb2b71/2147483647/strip/true/crop/365x365+110+0/resize/200x200!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 2x">

    

    
        <source media="(max-width: 568px)" width="100" height="100" srcset="https://cdn.assetmg.info/dims4/default/c41d434/2147483647/strip/true/crop/365x365+110+0/resize/100x100!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg">

    


    
    
    
        
            

        
    

    
    
        
    
            <source type="image/webp" width="180" height="135" srcset="https://cdn.assetmg.info/dims4/default/522f83b/2147483647/strip/true/crop/487x365+49+0/resize/180x135!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 1x,https://cdn.assetmg.info/dims4/default/3824a11/2147483647/strip/true/crop/487x365+49+0/resize/360x270!/format/webp/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 2x">

    

    
        <source width="180" height="135" srcset="https://cdn.assetmg.info/dims4/default/5ea43b5/2147483647/strip/true/crop/487x365+49+0/resize/180x135!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg">

    


    
    
    <img alt="MIM-art-cover-EM-Alpha.jpg" srcset="https://cdn.assetmg.info/dims4/default/5ea43b5/2147483647/strip/true/crop/487x365+49+0/resize/180x135!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 1x,https://cdn.assetmg.info/dims4/default/5ab16d2/2147483647/strip/true/crop/487x365+49+0/resize/360x270!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg 2x" width="180" height="135" src="https://cdn.assetmg.info/dims4/default/5ea43b5/2147483647/strip/true/crop/487x365+49+0/resize/180x135!/quality/90/?url=https%3A%2F%2Fk2-prod-in-investor-prod.s3.amazonaws.com%2Fbrightspot%2Fd2%2Fd4%2Fcab840724d64b6667d36ec101c10%2Fmim-art-cover-em-alpha.jpg" loading="lazy">


</picture>

</a></div>
<p>Applied Digital is hardly alone in trying to capture some AI magic. Ever since ChatGPT burst onto the scene last November — with its “large language model” boasting a humanlike writing capability that at first blush seems to ensure productivity gains for everyone from publishers and movie studios to investment banks and hedge funds — so-called “generative AI” has turned the markets on their heads.</p>
    

<p>Coming off the worst year in recent history for venture capitalists, private market players like Andreessen Horowitz, Coatue Management, Tiger Global, Sequoia Capital, and Softbank quickly redirected their dollars to AI upstarts. Meanwhile, the stock prices of the big tech names suspected to be the major beneficiaries of this often-called “revolutionary” form of artificial intelligence have skyrocketed. The 2022 tech downturn became a faint memory as some five tech companies — Alphabet, Amazon, Apple, Microsoft, and Nvidia — accounted for the lion’s share of the stock market’s remarkable comeback this year, with the S&amp;P 500 rising 20 percent through July. Nvidia, maker of the superfast chips that switched from powering bitcoin mining to making generative AI possible, has gained more than 250 percent so far in 2023, making it the S&amp;P index’s top gainer.</p><p>In the midst of this bonanza, Applied Digital’s CEO — who is also the president of B. Riley Asset Management and was at one point the owner of about 25 percent<b> </b>of Applied Digital’s shares — posted on Twitter (now X) in June that the company had ordered 26,000<b> </b>top-tier H100 GPUs, or video processing cards, from Nvidia for $40,000 apiece. To those casting a wary eye on the company, it seemed too good to be true. A purchase that big would allow Applied Digital to “jump to the top of the pile in high-performance computing, alongside Google, Meta, and [Amazon Web Services],” short-seller Dan David said in a Wolfpack Research report that called the company “an embarrassing and predictable stock promotion.” He noted that the cost to purchase such equipment would run more than $1 billion — more than Applied Digital’s market cap of nearly $600 million. <b> </b></p>
    

<p>“The explosion of interest in AI after the emergence of ChatGPT has predictably attracted the worst promoters and scumbags to peddle fake AI wares to credulous investors,” says David, who claims Applied Digital is one of them. (The company did not respond to a request for comment.)</p><p>Applied Digital is one of nearly a dozen companies that short-sellers have been eyeing this year as questionable beneficiaries of AI mania. The shorts say their antennae are on the alert for even more. But so far, the skeptics are fighting an uphill battle. Short interest is relatively high in several of these stocks, and it’s been costly to bet against them. As of August 25, Applied Digital short-sellers, for example, had placed bets on 19 percent of the outstanding shares of the company. The short-sellers in aggregate are down almost 30 percent this year on the name, having lost about $10 million, says S3 Partners — although those who shorted Applied Digital at or near its peak would have profited.</p>
    

<p>Orso Partners co-founder Nate Koppikar, who is also short Applied Digital, has a term for what he sees going on. He calls the phenomenon “the grift shift” — arguing that companies and venture capital funds have pivoted from their losing crypto and tech bets to cash in on the AI moment.</p><p>All told, generative AI and machine learning start-ups raised about $39.4 billion this year, with $19.4 billion of that in the second quarter, according to PitchBook’s second-quarter Artificial Intelligence &amp; Machine Learning Report.</p><p>Though money has been pouring into everything said to be “AI,” a few critics are starting to wonder whether the latest technology is really transformational or merely evolutionary. Meanwhile, large language model problems like “hallucinations,” “drift,” and “degradation” are starting to populate the tech literature, in reference to the various types of errors cropping up in generative AI as studies indicate that the products’ output appears to have worsened over time.</p><p>Some investors are starting to grow antsy. AI has dominated the spotlight this year, but “investors are growing impatient with the lack of revenue growth from generative AI innovators,” says PitchBook. “No longer are big tech stocks going up after new-product or partnership announcements, putting pressure on start-ups to gain traction.”</p><p>Says Koppikar, “This looks like a great market for fraudsters to chase because of the ability to ‘Theranos’ your product” with semifunctional prototypes and claims of AI being used in products that are really just powered by humans. He argues that many of the new AI companies “are complete frauds or will never have scalable revenue models,” adding, “There is going to be a staggering amount of capex burned on AI that goes nowhere.”</p>
    <hr>
<p>
    <sh2>
        Last year, Koppikar was one of the first
    </sh2>
 investors to predict the cascading downturn in tech and growth stocks, based in part on the interdependency of the companies. He sees a more extreme version of that interdependency at work here, noting that many of the highly touted AI start-ups are backed by big companies like Nvidia, which needs the start-ups to buy its chips in what Koppikar calls a round-trip move. “The big tech companies are driving this. They fund the start-ups and make them buy products from them. It’s a recycling of cash,” he says.</p><p>For example, CoreWeave, a cloud service start-up, recently announced it is taking out a $2.3 billion collateralized loan backed by Nvidia chips. CoreWeave’s biggest investor<b> </b>is Nvidia, and participants in the debt offering include Magnetar Capital, Blackstone, Coatue, DigitalBridge Credit, BlackRock, PIMCO, and Carlyle Group.</p><p>“Nvidia’s explosive revenue growth really tells us nothing about the future of AI,” Koppikar explains. “It turns out every scammer in America is trying to buy H100 chips right now so that they can say they own them. In 2021, scam companies put Bitcoin on their balance sheets — now the scams have shifted over to putting $40,000 H100s on the balance sheet.”</p><p>Koppikar places Applied Digital, with its big order of Nvidia chips, squarely in that category. The company would seem to be a marginal player in the big-stakes world of AI — except for the fact that two customers it claims to have lined up are among the top-ten AI companies that Bloomberg anointed in late June as “the ones to watch right now.”</p><p>When Applied Digital in May announced a potential deal worth $460 million, it declined to name the customer. But later, CEO Cummins tweeted that both that company and the other client were on the Bloomberg top-ten list. His tweet was followed by a research report from brokerage and investment banking firm Craig-Hallum Capital Group, which handled a $125 million equity offering for Applied Digital in late June, noting that one of the company’s new cloud customers is an “LLM provider based in London.”</p><p>Short-sellers David and Koppikar point out that only one company is both on the Bloomberg list and headquartered in London, and that is Stability AI. Its product, Stable Diffusion — a model that allows users to create images based on a few word prompts — was initially so exciting to investors that the company was able to raise $100 million last October from prominent VCs including Coatue and Lightspeed Venture Partners. Stability AI quickly garnered a valuation of $1 billion.</p><p>But since then, the company’s high-flying status has been tarnished by revelations that founder Emad Mostaque lied about his Oxford University credentials, misled investors, and is burning through cash and losing executives, as outlined in a scathing <i>Forbes</i> article. <s> </s></p><p>David calls Stability AI “one of the most dubious AI start-ups in a field awash with speculative promotions.”</p><p>Adds Koppikar: “It looks like Applied Digital signed up a customer that has no way to deliver anything close to the revenues it’s claiming.” He calls the situation “a cautionary tale for AI.”</p><p>Stability AI did not respond by press time to <i>Institutional Investor</i>’s request for comment. But elsewhere, Mostaque has denied the accusations leveled against him. He has also predicted that AI will be “the biggest bubble of all time” and admitted that it is still in its early stages and “not quite ready” for mass-scale adoption in most industries, including banking.</p><p>On July 14, Applied Digital disclosed its other big AI cloud service customer: Character.ai, a neural language model chatbot application founded by former Google executives that generates humanlike text responses and can participate in “contextual” conversation.</p><p>Both Stability AI and Character.ai go a step beyond the text capabilities of ChatGPT, which many businesses believe can help write emails, summarize 10-Ks, draft legal documents, and even, some argue, offer strategic advice. These two programs are among the new generative AI offerings that add images — or sometimes videos — based on word prompts.</p><p>With Character.ai, users can interact with virtual avatars of celebrities like Taylor Swift and Elon Musk. The downside of the program is apparent on Reddit, where a subreddit called CharacterAI_NSFW gives explicit instructions on “how to sex the bots.” (NSFW stands for “not safe for work.”)</p><p>The subreddit’s advice is straightforward: “You just have to spend time flirting with them and slowly pushing the action forward while the content filter fights you. . . . Once you’ve romanced the bot to the point it actually likes you and is consenting, then you can start sexing it.”</p><p>Koppikar offers a note of caution on the video game–based AI programs. “This won’t be anywhere near as big as people think until they find a way to make it not creepy.”</p><p>In response, a spokesman for Character.ai cited the company’s terms of service, which state that “pornographic content is against our terms of service, and will not be supported at any point in the future.” Meanwhile, Character.ai has posted statistics showing that users have spent more time there, where they can engage with famous or fictional people, than on ChatGPT.</p><p>As of March, the Character.ai start-up was valued at $1 billion, according to PitchBook. Its most prominent backer is Andreessen Horowitz, which once boasted the biggest VC crypto fund. But now that such bets have sputtered, the VC firm has launched a new fund to invest in AI start-ups.</p><p>Not surprisingly, VC heavyweight Marc Andreessen, the firm’s co-founder, has become an effusive cheerleader for AI. In a recent lengthy blogpost titled “Why AI Will Save the World,” he argues that “what AI offers us is the opportunity to profoundly augment human intelligence to make all of these outcomes of intelligence — and many others, from the creation of new medicines to ways to solve climate change to technologies to reach the stars — much, much better from here.”</p><p>Among the benefits, says Andreessen, will be the ability of every child to have an “AI tutor that is infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely helpful.” In his vision of the future, every business and political leader, scientist, artist, doctor, and inventor will have an AI assistant to help maximize his or her endeavors.</p><p>“In short,” says Andreessen, “anything that people do with their natural intelligence today can be done much better with AI, and we will be able to take on new challenges that have been impossible to tackle without AI, from curing all diseases to achieving interstellar travel.”</p><p>Andreessen’s paean may represent peak AI fever. But right now, it’s about as popular to cast aspersions on the miracles of AI as it was two years ago to dismiss crypto as the future of money, or before that, to criticize SPACs as the preferred way to take a company public.</p><p>But although generative AI may well unleash untold gains for society, it also seems that its benefits could be less exciting than the hype suggests.</p>
    <hr>
<p>
    <sh2>
        Generative AI has already proved
    </sh2>
<b> </b>to be subject to some peculiar flaws. One is “hallucinations” — the ability of a model to invent facts. In one instance, OpenAi, the company behind ChatGPT, was sued in a Georgia state court by radio host Mike Walters, who claims the ChatGPT tool said he had embezzled money from a special-interest group for which he’d served as a financial officer — which is not true. ChatGPT “published libelous matter” regarding the talk show host, the lawsuit alleges.</p><p>Elsewhere, a Texas judge has banned attorneys from using ChatGPT to create filings without human oversight following an incident in which the program made up court cases.</p><p>OpenAi declined to comment on those cases.</p><p>Ben Dickson, a software engineer and the founder of the blog TechTalks, says that hallucinations are a “serious problem. LLMs are wont to generate plausible text that is not factually correct, such as made-up names of papers and journals.”</p><p>Moreover, there is new research, as well as anecdotal evidence, indicating that ChatGPT’s output has gotten worse, or “drifted,” over time.</p><p>For example, research by Stanford University and UC Berkeley professors looked at ChatGPT’s ability to identify prime numbers. Even with something as straightforward as math, the researchers found that in March, ChatGPT had 84 percent accuracy in identifying prime versus composite numbers, but by June had only 51 percent accuracy performing the same exercise.</p><p>“The findings are a warning about the risks of building applications on top of black-box AI systems like ChatGPT that could produce inconsistent or unpredictable results over time,” Dickson wrote on his blog.<b> “</b>My rule of thumb is ‘Only trust the model when you can verify,’” he told <i>Institutional Investor </i>in an email.</p><p>Outside the halls of academia, users have also noticed problems. As first reported in Business Insider, Peter Yang, a product lead at Roblox, tweeted in May that the model was generating faster outputs but that the quality was worse. When asked on Twitter what questions were the problem, Yang responded, “Just simple questions like making writing more clear and concise and generating ideas.”</p><p>OpenAI has denied there is any degradation in GPT-4’s capabilities. “We make each new version smarter than the previous one,” Peter Welinder, vice president of product and partnerships at OpenAI, tweeted in July. “Current hypothesis: When you use it more heavily, you start noticing issues you didn’t see before.” (Welinder declined <i>II</i>’s request for further comment.)</p><p>Meanwhile, Reuters recently reported that monthly traffic to ChatGPT’s website declined in June for the first time since its launch in November, according to analytics firm Similarweb.</p><p>“ChatGPT trends are ugly: Traffic is down, and interest — measured by Google — is in free fall,” says Koppikar. “This is not the type of trend you see in an exponential growth story at this early stage.” He points out that Facebook, in contrast, showed consistent explosive growth for years.</p><p>“AI isn’t the biggest thing since the internet,” Koppikar says. “It is a cute excuse to buy tech stocks again, though.”</p><p>More recently, Yang tweeted: “I haven’t seen a ‘100 AI tools you can’t miss’ thread in weeks. Have we peaked?”</p>
    <hr>
<p>
    <sh2>
        Companies of all stripes
    </sh2>
 are now touting their usage of AI, but many have been employing the technology for years. “These things already exist. It’s machine learning and regression analysis,” says Koppikar. The big difference with generative AI, he notes, is that Nvidia’s latest chips are many times more powerful than the earlier ones, meaning they can do computational analysis much faster.</p><p>Experts in the field agree. “In terms of underlying techniques, ChatGPT is not particularly innovative,” Yann LeCun, Meta’s chief AI scientist, told a group of reporters and business executives on Zoom recently. “It’s nothing revolutionary, although that’s the way it’s perceived in the public,” he said, according to a report on ZDNet, a tech insider newsletter.</p><p>How much ChatGPT’s degree of innovation matters is up for debate. If analysts can use ChatGPT to read 200-page documents in a matter of minutes and prepare a report, that can save companies time and money. “But when you see people talking about using these models to make investment decisions, there’s little evidence that I’ve seen that these models can do that,” says Angelo Calvello, founder of Rosetta Analytics, a small quant investment firm. (He is also an opinion columnist for <i>Institutional Investor</i>.) That said, big asset managers might be able to cut their staff 70 percent over time, Calvello argues, because generative AI “could write the IR letters, it could analyze documents. It’s all about efficiency.”</p><p>Of course, that is a major reason people are investing in AI’s perceived beneficiaries. Hedge fund manager Dan Loeb is one professional who claims to be ahead of the curve in investing in AI.</p><p>“We have watched AI evolve and believe the technology has matured to the point that it is driving a transformational technology platform shift similar to those seen roughly once per decade: the personal computer in the 1980s, internet in the 1990s, mobile in the 2000s, and cloud in the 2010s,” he wrote in a recent letter to his Third Point hedge fund’s investors. “AI is creating interesting investment opportunities in the information technology ‘stack,’ and we have increased our exposure to companies throughout the software and semiconductor value chains that should benefit from mass adoption of large language models, one of the foundational technologies underlying generative AI.”</p><p>The Third Point CEO says his hedge fund has been investing in “AI-enabled business models” since 2016. At that time, it made a Series B venture investment in Upstart, a fintech player that claims to use AI in making loans.</p><p>Upstart “ultimately became the firm’s most lucrative investment,” Loeb writes in his letter to investors. To be sure, Third Point did make a fortune in Upstart; its 16 percent stake in the company was worth as much as $3.9 billion in the third quarter of 2021 — the peak of the recent tech bubble — as the hedge fund began to unload its shares. (When Third Point invested in Upstart in 2016, the entire company was valued at a mere $200 million, notes PitchBook.) By the end of March 2022, Third Point was completely out and the stock was tanking. Upstart is now worth less than 10 percent of its value at the peak.</p><p>Short-sellers scoff at the designation of Upstart as an AI stock and have shorted 33 percent of its outstanding shares. This year, though, that trade has been another loser for the shorts, as Upstart rallied off its bottom during the generative AI boom. But though shares fell by about 50 percent in August after Upstart reported a big quarterly loss, shorts were still out $639 million, or nearly 100 percent, betting against Upstart this year as of August 25, according to S3 Partners.</p><p>One of the few AI shorts that has been profitable this year is SoundHound AI. Culper Research’s Christian Lamarco calls the company “a flailing AI wannabe claiming to have revolutionary technology, a growing restaurant business, and a massive backlog of contracts.” The 17-year-old company was brought public via a SPAC in 2022. It is not profitable, and insiders are dumping the stock, according to Culper’s July report. In it, Lamarco writes that SoundHound now “claims its AI technology will revolutionize phone and drive-thru ordering, but we think this foray is an utter failure.”</p><p>Culper asserts that SoundHound’s AI simply doesn’t work — it relies on call centers staffed by humans, say former employees that his firm interviewed. Short-sellers had made $8.25 million, an 18 percent gain, by August 25 by taking on SoundHound this year, says S3 Partners.</p><p>In response to Culper’s report, a company spokesperson said that “SoundHound’s voice AI platform powers millions of devices (cars, TVs, [internet of things] devices) and responds to billions of queries without human involvement, including via our customer service voice solutions. With its restaurant business, SoundHound is one of the only companies where humans are neither monitoring nor interfering in the ordering process. This includes our recently announced partnership with White Castle, where our AI-only drive-thru solution is scaling to 100 locations next year.”</p><p>By far the biggest company in the AI space that short-sellers are targeting is C3.ai — which has been more resilient to their attacks. Two-short sellers, Sahm Adrangi’s Kerrisdale Capital and Ben Axler’s Spruce Point Capital, have taken aim at C3.ai, noting that it is another pivot to AI and — like the others — has never been profitable in several years of existence.</p><p>“The company was originally founded as C3 Energy to develop analytics solutions for public utilities preparing for the emergence of cap-and-trade and smart grids,” Adrangi wrote in a recent report on the company. “But management’s master stroke was rebranding operations as C3.ai in 2019 and going public with the ‘AI’ stock ticker, thus securing its place as the default artificial intelligence stock play for the undiscriminating investor despite the bulk of its business coming from relatively dated analytics models built for a very small number of utility, energy, and government customers.”</p><p>As Adrangi tells <i>II</i>, “I don’t think you can claim to be an AI business just by calling yourself AI.”</p><p>C3.ai, helmed by software entrepreneur and Silicon Valley luminary Tom Siebel, “does have a real business. It’s just not worth where it’s currently trading at,” Adrangi explains. “In the most recent 12 months, it basically nearly burned as much cash as it had revenue.”</p><p>Axler says investors are “clearly making a bet” on Siebel, who sold his previous company, Siebel Systems, to Oracle for $5.85 billion in 2005. For his part, Siebel has lashed out at the short-sellers. In a Bloomberg interview, he called them “scumbags who should be in jail.”</p><p>And in a response posted on its website, C3.ai calls the Kerrisdale report “a highly creative and transparent attempt by a self-acclaimed short-seller to short the stock, publish an inflammatory letter to move the stock price downward, then cover the short and pocket the profits.” It also says Kerrisdale erred on two specific accounting criticisms in the report — gross margin and unbilled receivables.</p><p>The stock recovered some of the losses it incurred after the latest Kerrisdale report following a better-than-expected earnings report. But that has not mollified the short-sellers.</p><p>“We’re probably in the early-innings stage of where we are in the AI cycle, and it still remains to be seen who are going to be the winners and losers,” says Axler, who first began shorting C3.ai in February 2022. “But ultimately, we don’t see C3.ai as being a big winner.”</p><p>It has been a tough slog for the short-sellers so far. As of August 25, C3.ai short sales were 34 percent of the float, and this year, short-sellers had lost almost $380 million, a 51 percent decline, betting against Siebel, according to S3 Partners. The stock is up more than 150 percent this year, although it’s down almost 40 percent from its peak in June.</p><p>Applied Digital is also down about 40 percent from its high point — which came in July, when the company announced the Character.ai deal and shortly after insiders had registered plans to sell their shares. Short-seller reports by David and Koppikar also dinged the stock. Then, on August 14, investors sued the company in a Dallas federal court, repeating short-sellers’ claims that Allied Digital had misled investors about its business model and the independence of its board members, based on close ties to B. Riley, its underwriter and Applied Digital CEO Cummins’ employer. (Applied Digital has denied the allegations.)</p><p>Despite the recent setbacks for some of these stocks, AI is still driving the markets higher. Notably, last week Nvidia reported record quarterly sales of $16 billion, far ahead of market expectations. How long the momentum can last is another question.</p><p>Cautions Adrangi: “With all these overvalued, overhyped stocks, it’s hard to call when the bubble will burst. But one day it does.”<br></p>
                    </div>


            

            


            
    


        </main>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RyujinX – Open Source Nintendo Switch Emulator (282 pts)]]></title>
            <link>https://ryujinx.org/</link>
            <guid>37332493</guid>
            <pubDate>Thu, 31 Aug 2023 04:33:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ryujinx.org/">https://ryujinx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37332493">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How to handle Asian-style “Family name first” when designing interfaces? (130 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37332126</link>
            <guid>37332126</guid>
            <pubDate>Thu, 31 Aug 2023 03:30:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37332126">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37333218"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333218" href="https://news.ycombinator.com/vote?id=37333218&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Just have a single "name field" and maybe preferred name, which is not the same. The family/given name format doesn't make much sense here.<p>Indian name: Sathiavelllu Arunachalam, known as SA or Seth</p><p>SE Asian ethnic Chinese names: Harry Lee Kuan Yew, (English name) (Surname) (Given name). Hated the name Harry and got it removed, though many Chinese are referred to by an English name.</p><p>Indonesian name: Fatimah Azzahra (given name only)</p><p>Malaysian name: Sharifah Azizah binti Syed Ahmad Tarmizi, (honorific surname: Sharifah) (given name: Azizah) (patronym) (father's honorific surname: Syed) (father's given name: Ahmad Tarmizi)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37335060"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335060" href="https://news.ycombinator.com/vote?id=37335060&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The GOV.UK design system[1] agrees with you:<p>&gt; "Use single or multiple fields depending on your user’s needs. Not everyone’s name fits the first-name, last-name format. Using multiple name fields mean there’s more risk that a person’s name will not fit the format you’ve chosen and that it is entered incorrectly."</p><p>and</p><p>&gt; "Avoid asking users for their title. It’s extra work for them and you’re asking them to potentially reveal their gender and marital status, which they may not want to do."</p><p>and</p><p>&gt; "If your service stores personal information, you should allow users to update their details, including their name. Allowing users to change their name helps your service respect their personal identity. It also means they can continue using your service without having to start over. People change their name for many reasons. For example, because of a change in marital status, family situation or gender. Avoid making it hard for users to change their name. As well as causing them distress, it may make them reluctant to use your service."</p><p>[1] <a href="https://design-system.service.gov.uk/patterns/names/" rel="nofollow noreferrer">https://design-system.service.gov.uk/patterns/names/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333840"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333840" href="https://news.ycombinator.com/vote?id=37333840&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span><i>"Just have a single "name field" and maybe preferred name, which is not the same."</i><p>This is the way to go.</p><p><i>"The family/given name format doesn't make much sense here."</i></p><p>It doesn't make much sense in many places all over the world. In Germany technically speaking our given names are a set and we can have many of them. While it is practically necessary to write them out in a certain order in our documents, from a legal standpoint they are all equal. There is no first and second given name and most certainly no middle name. Consequence is that  in everyday life today I can be Hans and tomorrow Fritz. I can be Hans Fritz or Fritz Hans too, but not Hans-Fritz (with a hyphen) except if it's written like that in my birth certificate and then the order is fixed and I can't decide to be Hans, Fritz or Fritz-Hans.</p><p>The trade-off in this system is that it is much harder to change your name here then in most other places.</p><p>In Bavaria where I live, the informal convention is also last name first, exactly like in Asia.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334077"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334077" href="https://news.ycombinator.com/vote?id=37334077&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span><i>&gt; In Bavaria where I live, the informal convention is also last name first</i><p>It's probably a Roman thing. Julius Caesar was actually "Caesar, of the gens Julia", so last-name first.</p><p>In Italy we also used to list last-name first, particularly in official and military communications. This has changed significantly in the last 40 years, particularly in everyday bureaucracy, but it's still used by police and other bodies.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334696"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334696" href="https://news.ycombinator.com/vote?id=37334696&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Roman naming conventions[1] technically did have a personal first name that came first and was called that: first name or forename is a reasonably direct translation for praenomen.  The complexity is that by the late republic praenomina were largely from such a small set of names that they were not particularly useful to distinguish anyone, and they became somewhat of a formality that would often be omitted.<p>Julius Caesar has the 'group/last' name first because it doesn't include his first name at all: including it he would be Gaius Julius Caesar.  The cognomen (there Caesar), meanwhile, was a bit of an inconsistent mess that in various times and contexts could be personal, hereditary, honorific, given, adopted...</p><p>[1]: <a href="https://en.m.wikipedia.org/wiki/Roman_naming_conventions" rel="nofollow noreferrer">https://en.m.wikipedia.org/wiki/Roman_naming_conventions</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37335113"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37335113" href="https://news.ycombinator.com/vote?id=37335113&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Hereditary cognomina were used to distinguish different major branches of large, powerful clans. Some sort of device to specify which subbranch of an enormous group you're talking about appears to be necessary in every place that has such enormous groups; in imperial China, then as now, vast swathes of the population were covered by just a handful of surnames, but the practice developed of referring to notable families by a combination of their name and the location where they were dominant.<p>So Caesar's close relatives were all also called Caesar, which is how it got to be an imperial title. You might think that with the original cognomen locked into place for such families, members would get a second cognomen so that there would be at least one part of their name that could identify them. But apparently not.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334978"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334978" href="https://news.ycombinator.com/vote?id=37334978&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; In Italy we also used to list last-name first, particularly in official and military communications. This has changed significantly in the last 40 years, particularly in everyday bureaucracy, but it's still used by police and other bodies.<p>Only that... it didn't? The vast majority of bureaucracy and even many private entities in Italy use and strongly prefer the "Surname Birthname" format.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334658"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334658" href="https://news.ycombinator.com/vote?id=37334658&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Roman naming conventions were very different from current Italian ones, and Caesar's full name was Gaius Julius Caesar, where Gaius is the closest thing to what we would call a first name. If he had some brothers, they would all have been "Julius Caesar", and only the praenomen would have distinguished them. For example, Julius Caesar's father was also named Gaius Julius Caesar, and had (most likely) a brother whose name was Sextus Julius Caesar.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334724"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334724" href="https://news.ycombinator.com/vote?id=37334724&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; It's probably a Roman thing.<p>There is very likely no connection. The introduction of surnames in Germany took place in the 15th century (somewhat earlier in German speaking parts of Switzerland). In Franconia, the "ethnic" distinct northern part of which is today Bavaria and which was never a part of the Roman empire, the last-name first custom also occurs in a few places such as Bamberg.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334632"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334632" href="https://news.ycombinator.com/vote?id=37334632&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In Poland informally it’s always &lt;given name&gt; &lt;last name&gt;. Formally, it depends most places are on the same as informal, but particularly military and some other bureaucracies will use &lt;last name&gt; &lt;given name&gt;. I always thought this is a holdout from the pre-computer days to make it easier to sort alphabetically (last names are a bit better distributed than the popular first names).<p>In France they seem to prefer &lt;LAST NAME&gt; (yes, all caps) &lt;First name&gt; even for less formal things.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334856"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334856" href="https://news.ycombinator.com/vote?id=37334856&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Not sure how it is done in other countries, but it's very likely that the custom of using the last name in all caps exists elsewhere. AFAIK, it comes from the fact that some last names are also common first names, so it's easier to distinguish between the two like this, as the order isn't really that important. For example, if I see a DIDIER Bernard or a Bernard DIDIER, I know I need to call him M. Didier in a formal context or just Bernard in an informal one.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334580"><td></td></tr>
            <tr id="37334787"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334787" href="https://news.ycombinator.com/vote?id=37334787&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Gaius (also written Caius) was his given name, Ivlivs (Julius) the family name, Caesar the cognomen.<p>In inscriptions (short): CAIVS IVLIVS CAESAR</p><p>In speaking he would (in English;)) rather formally introduce himself as "Gaius of the Iulii, called Caesar".</p><p>His mother called him Gaius, his peers called him Caesar once he got or choose?) this name.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334930"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334930" href="https://news.ycombinator.com/vote?id=37334930&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Wouldn't Ceasar be much closer to his last name,though? It's definitely not his personal nickname.<p>His father was 'Gaius Julius Caesar' his grandfather (supposedly the first 'Ceasar') was 'Sextus Julius Caesar'.</p><p>The Cognomen is presumable something that evolved as a way to tell apart people belonging to different branches of the same family/clan because the number of first names was very limited.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37334588"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334588" href="https://news.ycombinator.com/vote?id=37334588&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>In Hungary it's also last name first, both officially and informally - not sure if it's some Asian influence or something else...</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334777"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334777" href="https://news.ycombinator.com/vote?id=37334777&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>France as well<p>I believe that it's mostly the English speaking world that is strict with having "first name" first and "last name" last.</p><p>It's not an Asian cultural feature to have family name first, but it's an English speaking thing to have it last.</p><p>OP isn't trying to use his software in Asia specifically.
He's just adapting to non US market.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37335234"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335234" href="https://news.ycombinator.com/vote?id=37335234&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>We should emphasize the “preferred name” option because it can be very expensive &amp; arduous to change a legal name. Also using real names in UI/messages should be considered a bad practice because you risk leaking info to shoulder snoopers or dead-naming folks that don’t want to expend the time/effort/money for a legal rename. Just don’t.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334621"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334621" href="https://news.ycombinator.com/vote?id=37334621&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>For the record, traditional Indian names are far more complicated.<p>Consider the name of a famous violinist, Lalgudi Gopala Jayaraman Radhakrishnan, also known as Lalgudi G. J. R. Krishnan. From a Western perspective, 'Lalgudi' isn't strictly a 'name' per se: it is a <i>toponymic surname</i>, and is the name of a taluk (or administrative subdivision: it is a third-order division after state, and district) in Tamil Nadu.</p><p>Gopala is an avonymic, i.e. his grandfather's name, and Jayaraman is a patronymic, his father's name. His given name is Radhakrishnan, but frequently rendered as Krishnan.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333435"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333435" href="https://news.ycombinator.com/vote?id=37333435&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>This is what I usually do if I need the name at all, but sometimes you need to interface with systems that need that data split.<p>A simple, unimportant one: how do you politely address the person? In Germany, you'd start a letter with "Sehr geehrte Frau Dr. Musterfrau", which implies knowing the last name, gender and title of the person. You can drop the title, but dropping the gender makes the whole thing impersonal. Using the full name feels off.</p><p>In my case, it was to address a resignation notice, so it was a bit of a nitpick with no legal consequences. However in other cultures and scenarios it can matter a lot more.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334356"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334356" href="https://news.ycombinator.com/vote?id=37334356&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It's interesting how in Germany there is so much accent on titles. Every country has their own interface standards and it is always educational to dig in to that for a bit to get acquainted with the local customs so you don't end up making some kind of faux pas. And sometimes those standards are very fluid, someone calling you 'Sir' may do so in a mildly sarcastic manner to get the point across, they don't necessarily believe you to have been knighted. And some people use access to their given name(s) as something to be granted explicitly, even though the company phone book spells out their first name and others are equally offended when you address them by their family name.<p>It's a minefield!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334623"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334623" href="https://news.ycombinator.com/vote?id=37334623&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Actually, the stereotype among Germans is that Austrians are even more fond of their titles, although I think that has decreased in both countries in the last decades.<p>The "addressing people by their first name and using <i>Du</i> instead of <i>Sie</i>" is indeed handled very differently in different companies. I consider myself lucky to work for a company where "Du" and first name is the rule, so I don't have to keep an internal list of colleagues which I can use "Du" with...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334579"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334579" href="https://news.ycombinator.com/vote?id=37334579&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; It's interesting how in Germany there is so much accent on titles.<p>Seeing all the title drop down when subscribing to the economist (UK) made it clear to me it is a thing there too, at least in some circles.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333746"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333746" href="https://news.ycombinator.com/vote?id=37333746&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I think to address your politeness questions, it can only be done if you handle each language/country split separately and the app creators have detailed cultural knowledge for each country/language. I come from a country right next to germany and addressing a person with a gender specifier like Frau would be seen as extremely old fashioned and potentially sexist.<p>So I guess it also depends a lot on to which degree the app is localized.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334931"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334931" href="https://news.ycombinator.com/vote?id=37334931&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Even in Germany you can work around it. It's necessary for a third gender option, or when the gender is not known.<p>I imagine that it's not France either? French has similar greetings, with the same indecisiveness around the polite/casual pronouns. There's the added challenge of Quebec where casual pronouns are far more commonly used.</p><p>There's also the trap of "mademoiselle" which is no longer used because we no longer care about a woman's marital status. It's the same as with Fraulein.</p><p>You are right that rules might need to be made by country. The beauty of small scale solutions is that you can sweat small things like this instead of applying Silicon Valley sensibilities to the entire world.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334505"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334505" href="https://news.ycombinator.com/vote?id=37334505&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Which country is that? And how would you address a person called Martina Müller in a written document? "Sehr geehrte Müller?" Or is it not a German-speaking country you're referring to?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334651"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334651" href="https://news.ycombinator.com/vote?id=37334651&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Even "sehr geehrte Müller" already includes the gender (it would be "sehr geehrter" for a man), so if you want to avoid the whole situation, you have to go informal, and indeed, I have received a lot of emails recently which start with simply "Hallo [firstName] [lastName]" (which I guess is still more formal then only "Hallo [firstName]").</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37333608"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333608" href="https://news.ycombinator.com/vote?id=37333608&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Some profile managers or registration forms in European sites asked me how I want to be addressed, including a menu with Mister, Doctor, PhD etc. American sites usually have at most name, middle name and family name and mails start with Dear Name. English is easy because Dear is OK for all genres. Other languages need a different word for male and female people so their either end up collecting personal information that they wouldn't need for the service they are offering or they have to find some impersonal way to address you.<p>An example with Italian. Welcome is benvenuto for men and benvenuta for women. Welcome back is either bentornato or bentornata. It's impossible to use them unless we ask for the genre of the customer or use a DB / AI to infer it from the name. We still need a way to let the customer fix any mistake, a mistake which would probably be unwelcome.</p><p>So the common workaround is to use words like ciao, which is informal and possibly not well received by older people, or buongiorno which is OK for most of the day but not in the night (good morning vs good evening) or just use the name.</p><p>Best solution, if you don't need names to serve your service or because of regulations, don't ask them, use only the email address and make GDPR happier.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334300"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334300" href="https://news.ycombinator.com/vote?id=37334300&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>One way to address the issue with gendered grammar would be to ask the user to chose between which of ‘benvenuto’ and ‘benvenuta’ should be used when corresponding with them.<p>That way, you don’t strictly know their gender, you don’t have to ask for it, just what grammar construct they prefer you use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37335044"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37335044" href="https://news.ycombinator.com/vote?id=37335044&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>That's another way to do it but I can see at least a couple of downsides:<p>1, the easy one: there might be more that one inflected form (gendered) in the UI and it could be tedious for the user to select all of them. Of course by choosing one we can automatically set the others.</p><p>2, the substantial one: strictly speaking the users don't tell us about their gender but there are strong hints about what they are, given the statistics of the general population and the name they gave us. I'm not sure how the Privacy Authority of a EU country would look at that but IANAL.</p><p>Furthermore we're starting to make our app enter the rabbit hole of non binary gender identities and to handle that we would have spend more time and money for maybe no reason.</p><p>I give an example that covers all those points:</p><p>Of my current customers one is running a service for which they are mandated to collect personal data. They must know the official gender as in the official ID documents of the country. Anything more than that could be nice but it is not necessary and would be probably unexpected by users, maybe even looked at suspiciously.</p><p>Another customer is running a B2B service. They need only a username and the company email of the users. If they want to welcome them with Benvenuto/a, Bienvenido/a, Bienvenu/ue they would have to ask for a bit of personal data that could put them needlessly into the scope of GDPR.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334382"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334382" href="https://news.ycombinator.com/vote?id=37334382&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>This is something that is easy to do in automation, but much harder to do in personal interaction unless you interact with people regularly. I've seen my name butchered in a hundred different ways and I've learned to simply not care but to respond to the message rather than the mode of address. And obviously I try to get it right whenever I address something. It's akin to the Robustness Principle: <a href="https://en.wikipedia.org/wiki/Robustness_principle" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Robustness_principle</a> .</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37334682"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334682" href="https://news.ycombinator.com/vote?id=37334682&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; A simple, unimportant one: how do you politely address the person?<p>This is I think solved with asking how people want to be addressed. It also helps deal with situations where someone's legal name isn't how they are usually referred to.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334984"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334984" href="https://news.ycombinator.com/vote?id=37334984&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It was a bit trickier in my case because it was an English tool to fill a German form. There was the awkward issue of "Frau Dr." and "Herr Dr." being the same thing in the English dropdown.<p>The other issue is that the full name is used in the address, but only the last name in the greeting. There are no apartment numbers in Germany, so the name in the address must match the name on the mailbox. There can also be a lot of "Herr Müller" working at the same address.</p><p>In the end I opted for Mr/Mrs/Other + first and last name, and dropped the title. It's good enough for this purpose, but it shows how tricky something like a name can get, especially if the person filling the form is not aware of all this.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37334794"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334794" href="https://news.ycombinator.com/vote?id=37334794&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Many Indonesian Javanese people only have a single name.  It is more rare these days, but certainly not zero, or only old people who don't use computer.  I've met people in their 20s with a single name.<p>Also, in (South) Korea, depending upon the person's preference, they may choose to do family name first or last.  It's crazy.  I don't know why.  (Can someone explain it to me?)  As a result, most will write their romanized family name in all uppercase.  Then, it can easily be identified as first or last.  The easiest place to see it is in movie credits.  When the screen is full of romanized Korean names, you see a mix of family name in all upper as first or last.</p><p>South Indian names can be incredibly long.  Many people will shorten to use a single character from either first or last name.</p><p>Just have a single free text field.  That should work for all cultures.  Allow for 64 chars minimum (hello Spaniards!) and possibly 128 chars.  Also, you might try a second field for "nick name".  Lots of people with very long names use a shorter form in less formal settings.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333994"><td></td></tr>
                <tr id="37334089"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334089" href="https://news.ycombinator.com/vote?id=37334089&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Nice, although the pertinent quote is:<p>"This article doesn't provide all the answers – the best answer will vary according to the needs of the application, and in most cases, it may be difficult to find a 'perfect' solution."
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334124"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334124" href="https://news.ycombinator.com/vote?id=37334124&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I respectfully disagree with their advice to use “Family name” and “Given name”, over “First name” and “Last name”.<p>Everybody has a First name and Last name (unless they are mononymic), even if this doesn’t correspond semantically to what the writer of the form anticipated.</p><p>Many people don’t have a Family name and this creates user confusion when filling out forms.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334209"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334209" href="https://news.ycombinator.com/vote?id=37334209&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The point is that First Name and Last Name do not correspond to Family Name and Given Name, which is actually what is desired (you can start an email to me as "Hello &lt;Given Name&gt;", and be reasonably sure of avoiding insult. You can't do that with "Hello &lt;First Name&gt;", you will definitely insult some people).<p>However, I don't do this. If I want someone's name so I can talk to them, I just ask them "what do you want us to call you?" with a single field (and if they enter "Captain HugePenis" then I cheerfully start the email "Hello Captain HugePenis,"). I only use Given/Family if I need their official name for bureaucracy reasons, and then it's very much "what's your name as it is written in your passport/id?", and I don't use it to address them, because it's very easy to get wrong.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334349"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334349" href="https://news.ycombinator.com/vote?id=37334349&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Okay but say a person with an East Asian name fills out your form and dutifully fills out your form with their name in reverse. i.e. if their name is the Chinese equivalent of “Smith John”, they fill it out as given name: John, family name: Smith. Aren’t you going to be rendering their name in the wrong order for all your interactions? How do you know which order to concatenate it?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334540"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37334540" href="https://news.ycombinator.com/vote?id=37334540&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>This is why you'd use the single "what do you want to be called?" value (e.g. preferred_name). Then it's not an issue because it's the exact text they provided.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334476"><td></td></tr>
                <tr id="37334508"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37334508" href="https://news.ycombinator.com/vote?id=37334508&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>This. There's two separate concerns: what you call them, and their "official" name.<p>What you call them is always better done as a separate single field, rather than trying to concatenate the official name fields.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37334691"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334691" href="https://news.ycombinator.com/vote?id=37334691&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In some languages ”family name” maybe confusing. E.g . in Polish it sometimes (often) mean maiden name (women traditionally replace or add to their last name the last name of their husbands).<p>So the “pre-marriage” name is referred to as:
- “nazwisko panieńskie” (maiden name)
- or “nazwisko rodowe” (literally this is “family name”)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334498"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334498" href="https://news.ycombinator.com/vote?id=37334498&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>So far the consensus of HN hive mind seems you should just give everyone a mandatory nick and optional full name fields and bypass all the formatting issues. It shouldn't be important if situation allows that.<p>With that said, the idea behind that recommendation is that, the "first" and "last" names in American mind corresponds to, or SHALL correspond to, the European idea of given names(Gates III or John or Jong-Il) and family names(father's name, name of their home communal village of The Vincis, or whatever for greater identities of an individual) respectively, rather than simply being the 1st and 2nd items of your `struct name` in either cases of American name orders and Asian name orders in use, and hence relevant forms for names SHALL be so marked clearly to avoid systems falsely identifying you with your community identifier that comes first in your culture. Doing this makes sense for this context and purpose.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334170"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334170" href="https://news.ycombinator.com/vote?id=37334170&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I have a friend who just married a Burmese girl of Indian origin and she just had a single name 'Jennifer'</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334244"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334244" href="https://news.ycombinator.com/vote?id=37334244&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The point isn't that it's first and last, it's to avoid an implicit assumption that they are in a particular order.  That semantic correspondence is exactly what changing to given/family is supposed to preserve.<p>Yes, there are people with only one.  There is/was a fairly well-known example in UK network infrastructure circles.  That's a problem in either system if you make the fields required.</p><p>Given/family removes ambiguity for a larger number of humans than first/last, but you're always going to run into <i>something</i> from the Falsehoods Programmers Believe About Names list so unless you can get away with just a single "name" field (and you can't always) it's the better option of the two.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334627"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334627" href="https://news.ycombinator.com/vote?id=37334627&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The original sin is assuming that names have a particular structure. But at least First/Last places the burden of that sin <i>not</i> on the user.<p>My partner has only given names. When forms ask for their “Family name”, my internal thought process goes like this:</p><p>1. WTF do I write here?</p><p>2. Why are they asking for Family name? Oh they are probably just Anglophone and assume this is how names are.</p><p>3. It’s okay, just lie on the form then.</p><p>Then I choose their final given name and write it in the family name field.</p><p>This kind of enforced microdishonesty grates on the user over time. It certainly makes me grumpy!</p><p>First and last name is better because it’s not imposing a structure that may not exist. The burden of that false structure goes on the service rather than the user.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37333412"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333412" href="https://news.ycombinator.com/vote?id=37333412&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In most cases you don't even need the users real name.<p>If I sign up for an account, but the invoice is going to the billing department of my company, why do you care what my real name is?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333505"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333505" href="https://news.ycombinator.com/vote?id=37333505&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; why do you care what my real name is<p>Because the billing department verifies bills before paying them, and needs to know who to ask about the bill?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333681"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333681" href="https://news.ycombinator.com/vote?id=37333681&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>If the name on a bill to my company does not match the name of the employee who ordered the product, the bill will certainly not be paid.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333804"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333804" href="https://news.ycombinator.com/vote?id=37333804&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Do you not use POs? Much easier, allows you to approve spend if necessary and means the person paying I voices doesn't need to have an encyclopedic knowledge of your employees and keep up to date with which ones have ledt</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334175"><td></td></tr>
                        <tr id="37333709"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333709" href="https://news.ycombinator.com/vote?id=37333709&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>A little suspicion I get from this post is it sounds like they have bunch of users under believable names in English, as well as many name-sounding local names that may or may not have to do with names, and they want to regularize the hell out of it to real names only, and looking into coding a machine readable data scheme to enforce a policy.<p>Otherwise there should be not a lot of reasons to parse and format Asian names, as suggested plenty times over here. If the incoming invoice has to go through legal checks first, the requirement shall be as codified and supplied, whether it's that the name shall be written back to front or in purple on red.</p><p>Maybe I'm just overthinking, likely I am.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37335141"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335141" href="https://news.ycombinator.com/vote?id=37335141&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>That's what we do, in our app.<p>People can write whatever they want, there, in whatever order they want.</p><p>Some Spanish (as in Catalan) names can be downright legendary.</p><p>I support non-Roman character sets.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333492"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333492" href="https://news.ycombinator.com/vote?id=37333492&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Slack does this and I don't like it. I have Hungarian colleagues and some (but not all!) of them have entered the family name first.<p>I always feel awkward responding to them, because I don't know if I'm typing `Hello $FIRSTNAME` or `Hello $LASTNAME`.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333537"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333537" href="https://news.ycombinator.com/vote?id=37333537&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>This is why you should always ask for a preferred name.  There’s lots of reasons why people don’t use their legal given name; I use my middle name and get annoyed when forced to use my first name.  Many societies (ex: Thailand) use nicknames instead of legal given names.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333977"><td></td></tr>
                <tr id="37334557"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334557" href="https://news.ycombinator.com/vote?id=37334557&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>You are being downvoted, but I wonder if people really appreciate the politeness on automated communication. If no person interacted with me, why would I want to be addressed respectfully? I think this mainly serves corporations which can humanize automated communication to make customers feel more cared about (by simulating interaction like a real human would initiate)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334640"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334640" href="https://news.ycombinator.com/vote?id=37334640&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I believe it makes the experience more pleasant (at least for neurotypical people), but it'simportant to not overdo it. It shouldn't add friction or subtract from clarity.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334847"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334847" href="https://news.ycombinator.com/vote?id=37334847&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I here this trope so much.  It is tiring and closed-minded.  Are you Western culture and male?  Probably.  I have seen whole blog posts dedicated to this matter.  Really?  This matter is so stressful that people need to write 100s of words about it?  Sheesh.<p>Are you aware that it is polite to start a convo with people from South Asia with a small greeting?  They wait for you to ack.  If you don't ack, they will not continue.</p><p>It is an important part of intercultural communication to continuously adapt to different communication styles.</p><p>Why don't people write a tiny chatbot that applies to certain users?  When the person says: "Hello."  The bot can reply: "Hello!"  It seems easy enough.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334982"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334982" href="https://news.ycombinator.com/vote?id=37334982&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; They wait for you to ack. If you don't ack, they will not continue<p>Which is possibly one of the (admittedly probably very minor) reasons why they were left so far behind economically. Focusing on form over function in communication can be very counterproductive.</p><p>Not that I'm saying that saying "Hello, how are you?" to someone is waste of time. However certain people/cultures really go overboard with this and expected outsider to comply with these norms when they have much better things to do is not the most sensible idea.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37333547"><td></td></tr>
                <tr id="37335041"><td></td></tr>
            <tr id="37333771"><td></td></tr>
                <tr id="37334262"><td></td></tr>
                <tr id="37334422"><td></td></tr>
                                    <tr id="37333820"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333820" href="https://news.ycombinator.com/vote?id=37333820&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I feel like this is the way. If you want, include fields for all types of names (given, honorific, father's surname, whatever is imaginable) but also add the field "how should we address you" and use that for messages.<p>So:</p><p>* honorific surname: Sharifah
* given name: Azizah
* father's honorific surname: Syed
* father's given name: Ahmad Tarmizi
* address me as: Zaphod Beeblebrox the 4th
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334908"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334908" href="https://news.ycombinator.com/vote?id=37334908&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Middle names are also equally important in the Philippines. The website that the OP included is lacking.<p>Either in the format:</p><p>- given-name middle-name surname</p><p>- surname, given-name (note there is no comma here) middle-name or middle initial
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333558"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333558" href="https://news.ycombinator.com/vote?id=37333558&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>this. and if they don't use latin alphabet,"local name" addition would really be nice. that's what my paycheck giver does.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333949"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333949" href="https://news.ycombinator.com/vote?id=37333949&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>right - if you have a first name, last name and then switch on the locale you will have welcome text like Congratulations Rasmussen when I'm using a website while in Japan.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333756"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333756" href="https://news.ycombinator.com/vote?id=37333756&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; SE Asian ethnic Chinese names: Harry Lee Kuan Yew, (English name) (Surname) (Given name). Hated the name Harry and got it removed, though many Chinese are referred to by an English name.<p>Really? Your parents named you the exact same name as a famous politician?</p><p>Edit: Really? Downvote me for the OP’s bad English?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333922"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333922" href="https://news.ycombinator.com/vote?id=37333922&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>The famous Singaporean politician Lee Kuan Yew to whom you refer was given the English name Harry by his parents. The poster you're quoting isn't themselves claiming to be LKY.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334574"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334574" href="https://news.ycombinator.com/vote?id=37334574&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>What does “Hated the name Harry and got it removed” meant then? Sounds like the poster got the name removed.<p>Also it was his grandfather that gave him the name “Harry”, not his parents.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37335001"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37335001" href="https://news.ycombinator.com/vote?id=37335001&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>There's no pronoun for that sentence, so it's left undefined who it's talking about. It doesn't mean the poster. It also doesn't really matter, if one former Singaporean politician dropped their "first name" then it's being used as an example that others may too.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37335029"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37335029" href="https://news.ycombinator.com/vote?id=37335029&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It means that LKY himself stopped using the name Harry, for whatever reason. His (LKY) Wikipedia article states when, but not why - it cites his autobiography so that may be a useful source if you are interested to learn more.</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37332325"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332325" href="https://news.ycombinator.com/vote?id=37332325&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Definitely read <a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" rel="nofollow noreferrer">https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-...</a> if you haven't yet.<p>Then think about what are the <i>requirements</i> your system needs when it comes to names.</p><p>Does the app need to know what a user's name is at all or is a username enough? Does it need to distinguish the family part of their name for anything?</p><p>A thing I think is the most general is to just have a Full Name field (min length 1 and either John Doe or something cute as default) And a Nickname or Display Name field if your app needs to show something on screen.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333638"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333638" href="https://news.ycombinator.com/vote?id=37333638&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It's not just programmers. Plenty of genealogists, do things like automatically give a wife their husband's (male!) patronym. Even though they see it doesn't work that way in the sources, they seem to feel that it "should" work that way.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37335146"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37335146" href="https://news.ycombinator.com/vote?id=37335146&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In France married women, if they let their guard down, will be given their husband's last name as "nom d'usage" upon almost any sign-up paperwork, without being asked, at least in my experience.<p>Nom d'usage technically has no legal value, it's just a last name you might want to be addressed as, normally that of your husband but it can be a pen name and whatever. It's optional, and technically only at the request of the relevant person. Men can have nom d'usage too (égalité, after all).</p><p>Still, immigration offices, banks, insurances... they often slap the husband's last name if that field is left empty, just because. Why would you want something else, right?</p><p>We started crossing those fields to make it clear she doesn't want a nom d'usage.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333407"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333407" href="https://news.ycombinator.com/vote?id=37333407&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Is there an article showing a solution because listing problems is the easy part?<p>And maybe it's just me but</p><p>&gt;because names are central to our identities, virtually by definition.</p><p>isn't true, my name is for the outside world, inside I'm just I, that's my identity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333639"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333639" href="https://news.ycombinator.com/vote?id=37333639&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>In this case, "listing problems" - as you dismissively call it - takes a lot of care and effort, the article is a distillation of experience and knowledge that very few possess. It's up to you to use this understanding of the problem space to decide which aspects you're willing to trade off. The article's job is to help people make these tradeoffs consciously, instead of designing based on simplistic assumptions made in ignorance.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333519"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333519" href="https://news.ycombinator.com/vote?id=37333519&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>There is no "a solution" and never will be. Design your solution to be maximally relevant to your scenario and accept that it can't be perfect.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37335012"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37335012" href="https://news.ycombinator.com/vote?id=37335012&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt;&gt;because names are central to our identities, virtually by definition.<p>&gt;isn't true, my name is for the outside world, inside I'm just I, that's my identity.</p><p>What happens in your head when someone calls your name? Surely that's a feeling of identity?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334054"><td></td></tr>
                <tr id="37334769"><td></td></tr>
            <tr id="37334174"><td></td></tr>
                        <tr id="37332779"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332779" href="https://news.ycombinator.com/vote?id=37332779&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>There are 4 main name forms that I often see:<p>* Full name (John Smith)</p><p>* Index name (Smith, John) - mostly for meatspace compatibility.</p><p>* Preferred name (John Smith) - used in lists with other people's names</p><p>* Personal name (John) - used in direct communications
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37335035"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37335035" href="https://news.ycombinator.com/vote?id=37335035&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The name used in direct communications can depend on the level of formality and on the context. For example, John / Mr. Smith / Dr. Smith.<p>Then there is the difference between the "full" name and the legal full name, with all names spelled out. The latter is sometimes necessary, but it can be awkward or inappropriate to use it in most places that expect a full name.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333731"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333731" href="https://news.ycombinator.com/vote?id=37333731&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Index vs. preferred name can get a bit wonky. I used to work on software that had a lot of attendee/speaker list functionality. Different types of customer had strong expectations about how names should be displayed and ordered, sometimes even in different contexts (attendees might be listed by last name but speakers by first, etc.). We had medical conferences where everybody would be listed as "Dr. Fredrica Bloggs, MD" but the sort order would be by last name. And that was just for English-speaking customers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332399"><td></td></tr>
                <tr id="37332405"><td></td></tr>
                <tr id="37334803"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334803" href="https://news.ycombinator.com/vote?id=37334803&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; No hyphens, apostrophes, or commas; except in the Preferred Name field<p>But my passport and drivers license both show my middle names hyphenated :(
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332613"><td></td></tr>
                        <tr id="37332693"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332693" href="https://news.ycombinator.com/vote?id=37332693&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; That Klingon Empire thing was a joke, right?<p>That made me laugh out loud. It's a great reminder why being conscious of the assumptions you make is an important part of development and one that LLMs can't really do.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333929"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333929" href="https://news.ycombinator.com/vote?id=37333929&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>This list would have been so much better with examples or a short explanation how or why the assumptions break in case it is not obvious. The author offers examples on demand [1] but while I would be interested to know them, it seems not important enough to bother the author. Maybe the author sees this comment and has some time to spare, then it could benefit everyone and not just one person asking.<p>[1] <i>If you need examples of real names which disprove any of the above commonly held misconceptions, I will happily introduce you to several.</i>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334325"><td></td></tr>
            <tr id="37334032"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334032" href="https://news.ycombinator.com/vote?id=37334032&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I live in Japan (as does Kalzameus), my name will regularly cause problems in computer systems. In fact, quite often I'm not using my name but what I am called in Japanese, and even then it will often fail. Number 1 problem - not enough characters, because most Japanese people only use a few characters for their entire name, don't have middle names, and don't have spaces in their names. And that's just for starters. Should I use what I consider to be my name, my full name, the name on my passport or the name on my residence card…?<p>It's just not that hard to find problems with name inputs.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333668"><td></td></tr>
                <tr id="37333794"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333794" href="https://news.ycombinator.com/vote?id=37333794&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I can tell you that in my passport, my name appears in two different places and it's spelled differently been them.<p>Also, a lot of people in the world don't have passports.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334042"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334042" href="https://news.ycombinator.com/vote?id=37334042&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>My wife has an extra first name that she doesn't always use, and my mum has two different but both correct spellings of her name.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334380"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334380" href="https://news.ycombinator.com/vote?id=37334380&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; What is on your passport then?<p>In Ireland, it’s not entirely rare to use different forms of one’s name in Irish and English.</p><p>For example, in English our president would be called “ Michael Daniel Higgins”, but in Irish “Mícheál Dónal Ó hUigínn”, and while there’s obviously a correspondence between them, they are pronounced quite differently.</p><p>It’s possible to change the version of the name you use on your passport after six months of regular use (compared to two years for any other kind of name change), and in that situation <i>both</i> forms of your name will be listed on your passport.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334060"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334060" href="https://news.ycombinator.com/vote?id=37334060&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; <i>What is on your passport then?</i><p>See my other comment on Vietnamese names, actually if you have a Belgian passport and a Vietnamese name, your actual given name (the third part for women with the "Thi" middle name) is not shown on your passport or identity card, only the first letter of it.</p><p>For French people who have three given names it's the same, although the two last ones are generally not used (you could say they're little endian compared to Vietnamese big endian names, I suppose) so it's not as important.</p><p>I have no idea why, and Belgium is the only country I know that does that, but it means your passport name is absolutely not your canonical full name.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333963"><td></td></tr>
                <tr id="37334001"><td></td></tr>
                  <tr id="37333958"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333958" href="https://news.ycombinator.com/vote?id=37333958&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>The magician Teller just has 'Teller' on his passport, and if you refer to 'Raymond Joseph Teller', people won't know who you're referring to.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333901"><td></td></tr>
                        <tr id="37332686"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332686" href="https://news.ycombinator.com/vote?id=37332686&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I lived in Asia for many years and worked with people from many places there.<p>It's common to hear "How should I address you?" This is equivalent to the people here suggesting a "nickname" field (good idea).</p><p>There are people with only one name. Don't make them double it (Ananda Ananda).</p><p>There are people with several given names. But they may only want to be called by the first, or the first two, or the last.</p><p>There are people who wish to be called by their full name. They may find it jarring to be addressed by just one piece of their name.</p><p>Finally there are people who go by a name which is not part of their legal name at all. Short forms like Bob instead of Robert, but also freeform names for various reasons, perhaps the most sensitive being that some government official may have determined their legal name contrary to their own wishes. Imagine your mother named you Sue but someone decided that must be short for Susan and put that on your government documents.</p><p>Related: when people want to show which part of their name is the family name, they either make it all uppercase or underline it. You can see this on some CVs but it happens elsewhere too when a full name is going to be read by people who don't know the addressee.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333380"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333380" href="https://news.ycombinator.com/vote?id=37333380&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I have an elderly German aunt by marriage who considers it an affront that Vodafone sends her chirpy business letters starting "Hallo &lt;&lt;first name&gt;&gt;!" and continuing in the familiar second person instead of the more (to her) respectful "Sehr geehrte Frau Dr. &lt;&lt;last name&gt;&gt;," and formal "Sie" (grammatically third person plural, but also the formal second person). First names and familiar second person grammar are very much reserved for family and friends in that generation.<p>Is there a similar sentiment among older people in parts of Asia?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333707"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333707" href="https://news.ycombinator.com/vote?id=37333707&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It <i>is</i> an affront though, a butchering of the language and just generally disrespectful. The distinction is an integral part of the German language and it's wholly inappropriate for a telco provider to address someone like this.<p>Same goes with names: missing accents, umlauts, special characters, wrong ordering, wrong titles/gender etc. is aggravating. And customers <i>do</i> complain.</p><p>German companies often spend a non-trivial amount on software engineering time and data cleanup to get these things right. And the French take it even more seriously.</p><p>Non-native speakers may not understand it, but that's no excuse and you will alienate people and customers. Even younger generations notice.</p><p>And just fyi I'm a millennial and per "Du" with everyone I know. Not my telco provider though.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333865"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333865" href="https://news.ycombinator.com/vote?id=37333865&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It's your personal preference to be addressed with _Sie_, but languages change and German is seeing shift to the use of _Du_ even for transactional business [1]. Target audience, region, familiarity etc. will all affect which word is used. Claiming the use of _Du_ in customer relationships as "wholly inappropriate" again is an expression of personal preference, not fact.<p>[1] <a href="https://www.abendblatt.de/wirtschaft/article233949947/firmen-kunden-hamburg-otto-aldi-duzen-siezen-unternehmenskommunikation-sprache.html" rel="nofollow noreferrer">https://www.abendblatt.de/wirtschaft/article233949947/firmen...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334362"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334362" href="https://news.ycombinator.com/vote?id=37334362&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Isn't personal preference the entire point, though?  If the aforementioned German aunt is offended enough by how a company addresses her, she might stop doing business with them.  That's a loss for the company and is something they might want to think about.  (Or maybe not, but who knows.)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334857"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334857" href="https://news.ycombinator.com/vote?id=37334857&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It got a lot more common during Covid, the social distancing rules in Supermarkets etc. were often written with the informal "Du", I guess to foster a sense of community and "we're all in this together".</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334653"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334653" href="https://news.ycombinator.com/vote?id=37334653&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I have a personal preference for wearing clothes in public.  I think people would rather that was respected, certainly in my case.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333871"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333871" href="https://news.ycombinator.com/vote?id=37333871&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Not saying she's unreasonable for feeling that way - just wondering if this is also something that should be considered in Asia, too, and giving a bit of context for people who aren't familiar with Germany. I know that Japanese has an even more "complicated" (to non-native-speakers) grammatical distinction of familiarity, but not much more about it, and have even less idea about how it works in other Asian languages/cultures.<p>Knowing what name to display isn't always enough - you also need to think about how the rest of the text around it should be translated to match. For German, the safe answer is still third-person plural Sie, unless you're absolutely certain your audience are all children, and missing that distinction might be taken by your audience as either that you're trying to be creepily over-familiar, or that you think they're children.</p><p>A sibling comment says that "Du" (second-person familiar) for business is becoming more common, and while that's true, I would NOT recommend using it without a good knowledge of German culture and your specific audience.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334629"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334629" href="https://news.ycombinator.com/vote?id=37334629&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I'm not sure about older people, but business/formal emails in Japanese are like at least 70% fluff. There are loads of guides on the internet like [1], just look at the English translation of the example at the top...<p>Depending on the level of formality you might need to conjugate verbs slightly differently or use vocabulary that's considered more respectful. From my experience the younger generation is more relaxed about the rules, but Japan as a whole is pretty slow to change.</p><p>[1] <a href="https://www.wasabi-jpn.com/japanese-lessons/how-to-write-emails-in-japanese-with-practical-examples/" rel="nofollow noreferrer">https://www.wasabi-jpn.com/japanese-lessons/how-to-write-ema...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334350"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334350" href="https://news.ycombinator.com/vote?id=37334350&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I speak both german and french, and I don't like being addressed with "Du" or "Tu", be it in letters, email or whatever. It is for me a lack of respect.<p>In France the etiquette requires that you ask permission to use "Tu", and there are people who refuse you the right (it basically signals that they see you as a servant).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334337"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334337" href="https://news.ycombinator.com/vote?id=37334337&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I'd say it depends on the area and the type of business. In Lower Bavaria it's quite common to use informal language even with strangers. Any mechanic, plumber or other craftsmen will use "Du" the moment they see you. Formal language is usually used in a more formal environment, like banks or maybe some fancy restaurant or fancy shop.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333792"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333792" href="https://news.ycombinator.com/vote?id=37333792&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>But this is the issue. There is not one way to do it right.<p>My ideal message from my telco would be this one line:</p><p>"Hi Leo, your monthly bill for &lt;phone number&gt; is available. Get it here: &lt;url&gt;"
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334826"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334826" href="https://news.ycombinator.com/vote?id=37334826&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>You would likely get a similar reaction in Japan by addressing letters to &lt;given name&gt; rather than &lt;surname&gt;, using plain style rather than polite style, and omitting keigo.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332737"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332737" href="https://news.ycombinator.com/vote?id=37332737&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; It's common to hear "How should I address you?" This is equivalent to the people here suggesting a "nickname" field (good idea).<p>It’s not uncommon in the west either and it’s weird how much we ignored it in software e.g. “I’m John Q. Smith the Third but call me John” or “May I call you j-dog?”</p><p>&gt; There are people who wish to be called by their full name. They may find it jarring to be addressed by just one piece of their name.</p><p>There’s also the issue of honorifics which you usually can’t get from names alone, as well as titles (whether professional or nobility) which can be very important to people’s identity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333401"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333401" href="https://news.ycombinator.com/vote?id=37333401&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Web apps great equalizer. No one cares if you are knighted or have phd, put in your name and you can watch funny cats same way as the rest of pleb.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333705"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333705" href="https://news.ycombinator.com/vote?id=37333705&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Imagine if the name on a tomb does not match the one that the person preferred to be called with, and everybody used all the times. I know one case but I don't know the reason.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332570"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332570" href="https://news.ycombinator.com/vote?id=37332570&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Does your “Asia Pacific” presence include countries like India? As others have pointed out, it’s better to read that article “Falsehoods programmers believe about names” [1] and then figure out how you’d like to deal with it. Looking at “Family, Given” or “Given, Family” is shortsighted and may cover only a few east Asian countries.<p>In India there are patronymic names with initials, mononyms (no “family name” or initials), names with just a given name (one or multiple words) and no “family name”, names where the “last name” is a place, etc.</p><p>If you truly want to cater to all kinds, just have one field for name and another for what they’d like to be addressed as.</p><p>[1]: <a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" rel="nofollow noreferrer">https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37335169"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37335169" href="https://news.ycombinator.com/vote?id=37335169&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I have nothing useful to add except that I know quite a few people who would be happy if they could have an apostrophe in their name without everything going to shit</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333836"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333836" href="https://news.ycombinator.com/vote?id=37333836&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>This is not just an Asian thing, Hungarians put their family name first (another reason not to call it a "last name").<p>Not to forget the special case that people may only have one name (for which some immigration offices had to invent the rule to repeat the single name twice, as first and last name, to comply with online and offline forms that have these often mandatory fields). So we may add two patterns to your pattern list:</p><pre><code>  Only, Only
  Only
</code></pre>
Localization is tricky, not just for names, e.g. postcodes (GB) and zip codes (US) come after the city ("London SW4 0AF", "Beverly Hill, CA 90210"), in many other places before (e.g. FR, DE, e.g. "91054 Erlangen").</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334340"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334340" href="https://news.ycombinator.com/vote?id=37334340&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I was recently in India.  When using Zomato to order a food delivery, I was confused why the address form was asking for a PIN code.  It took a moment to realize that PIN code was referring to what I think of as a zip/postal code.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334465"><td></td></tr>
                        <tr id="37332244"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332244" href="https://news.ycombinator.com/vote?id=37332244&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Personal opinion.<p>As an Asian person, don’t overthink this.</p><p>Just have fields for family name and given name to distinguish between the two.</p><p>Sure, when there’s a text field saying hello “first name last name” it might be flipped but this shouldn’t be a deal breaker or offensive in any capacity.</p><p>Worst case is you can have a toggle they can click but from a developer standpoint, that might be over engineering for something that might cause headaches later.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332558"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332558" href="https://news.ycombinator.com/vote?id=37332558&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Don't do this.<p>In Vietnam names like Nguyen Thi Anh Mai are common.</p><p>The family name is "Nguyen" is the family name. "Anh Mai" is the "first name". But they should be called "Mai". And reversing the order "Mai Anh Thi Nguyen" is just wrong.</p><p>And Catholic families in Vietnam often have names "Nguyen Thi To but everybody calls me by my baptismal name of Mary". Or you have a "house name" (i.e. your real name) and an outside name (so that ghosts can't follow you home). Or you have your real name and an unofficial English name (for reasons both good &amp; bad) that literally everyone at work calls you.</p><p>In the Philippines people have names Anna Katrina Gomez. But you don't call them Anna. You call them Katrina because that's how it often works in the Philippines.</p><p>The solution is simple:</p><p>One field is "what is your full name?"</p><p>One field is "what would you like to be called?"</p><p>You don't need to try to infer one from the other.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333798"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333798" href="https://news.ycombinator.com/vote?id=37333798&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>This is the most sensible answer here.<p>My name is Ritobrata Ghosh (Given Family). But my friends, teachers, colleagues always called me Rito. I have a different short name that is used by immediate family and close relatives.</p><p>Rito is also easier for western people's tongues. So, that is what I almost always use. I use my full name only in legal and financial documents. In every other place, I am just Rito. I like to be addressed as "Rito" and appear as "Rito Ghosh" in badges, documentation, slack, any non-legal/non-financial documents.</p><p>So, the best way to go, in my opinion, is having two fields- one for full name, another for preferred name. <i>Do not _assume_ anything</i>.</p><p>This is also consistent with western names, like: William Henry Gates III as Bill Gates.</p><p>There is also another field in many places where there are badges and such. "This field will appear as is in your badge" is wise in such cases.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332622"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332622" href="https://news.ycombinator.com/vote?id=37332622&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Interesting. Indeed, why not just ask “what would you like to be called?” and maybe for administrative(?) purposes ask the full name.<p>Naming is a tar pit and even Westerners might like to be called John when their name is Winstonshire. Let’s not even bother.</p><p>What is the downside? I’m not seeing it. I see people here recommending to implement logic to handle this fully and I’m not seeing why this would be preferable.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332720"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37332720" href="https://news.ycombinator.com/vote?id=37332720&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Heck, I just saw on Reddit some guy just discovered his girlfriend's "real name" is "Lawr'ryn" due to dumb parents. She preferred to go by Lauren. And her brother Pur'see went by Percy.<p>Why, as a system designer, would you force your user to see "Lawy'ryn" every day just because that's on some government document somewhere?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332617"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332617" href="https://news.ycombinator.com/vote?id=37332617&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Yes please! My wife and I constantly struggle with this kind of thing. When there's just a "name" field that permits some high number of characters, it's fine.<p>Which name is my "first name" or "last name" also depends on what identity document I'm using. I immigrated to Asia and switched to the family-name-first convention, but my birth certificate and so on are the opposite. So it's conceivable that a system requiring two forms of identification has both name orders, and both are correct.</p><p>Names are awful.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332892"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37332892" href="https://news.ycombinator.com/vote?id=37332892&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Not to mention you always have to second guess the designer.<p>Does "last name" mean "family name" even though it is my first name?</p><p>And what counts as "middle name"? Is it "Thi"? But my "first name" isn't Anh Mai. It is just Mai. But "Thi Anh" isn't my middle name, either, that's just nonsense.</p><p>And, yes, we have the "multiple documents with different name order" depending on whether it is the Vietnamese birth certificate, the US Consular Record of Birth Abroad, the Vietnamese passport, or the Australian passport....
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334970"><td></td></tr>
            <tr id="37333869"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333869" href="https://news.ycombinator.com/vote?id=37333869&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; <i>In Vietnam names like Nguyen Thi Anh Mai are common.</i><p>Which reminds me of this problem: she might have sisters called, say, Nguyen Thi Anh May and Nguyen Thi Anh Minh, and in quite a few countries this will result in them having identical names on their identity cards and most communication, either "Nguyen Thi", "Nguyen Thi Anh" or even "Nguyen Thi Anh M."
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332594" href="https://news.ycombinator.com/vote?id=37332594&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Are you missing a name from your PH example perhaps?<p>Everyone I've known there has their mother's maiden name as a middle name, and father's last as last name.  Then on marriage sliding the original last name to the middle name.</p><p>But one thing I did notice is that very few go by any given name, usually a play on it or a nickname instead.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333493"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333493" href="https://news.ycombinator.com/vote?id=37333493&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>And maybe you don't need the full name.<p>And don't limit it to Asia either, it's relevant everywhere.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333289"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333289" href="https://news.ycombinator.com/vote?id=37333289&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; You don't need to try to infer one from the other.<p>So you should make your system more cumbersome for 95%+ of users just for the sake of doing the right thing in occasional edge cases?</p><p>By all means make it possible to override, but you should absolutely default "what would you like to be called?" rather than making everyone enter their name twice.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333339"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333339" href="https://news.ycombinator.com/vote?id=37333339&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; So you should make your system more cumbersome for 95%+ of users just for the sake of doing the right thing in occasional edge cases?<p>As the app is apparently developed for use in Asia-Pacific, no, it isn't.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333362"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37333362" href="https://news.ycombinator.com/vote?id=37333362&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I guarantee that if someone vaguely familiar with the countries they operate in spends half an hour actually trying, they'll come up with heuristics that are correct for 95%+ of the target population.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333586"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37333586" href="https://news.ycombinator.com/vote?id=37333586&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>So use the heuristics to set the initial value for the second field based on the first. Just let the user edit it as they wish.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334411"><td></td></tr>
                              <tr id="37333460"><td></td></tr>
                <tr id="37333503"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37333503" href="https://news.ycombinator.com/vote?id=37333503&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>How do you figure that? Billions of people would mean over 10% of the world population having exceptional names, which seems very implausible.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333827"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37333827" href="https://news.ycombinator.com/vote?id=37333827&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In way you are right.<p>From the global point of view the exceptional case is the American one, Name Middle Family. It's about 330 M people vs 8 billions, 4%, which is less than 10%.</p><p>But then you add the exceptional cases for every country or culture. From the other comments we already know that even in Western Europe there are many differences. So if you sum all those exceptional cases you get the 100% of the world population.</p><p>Edit: 330 M, typo was 353 M.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333611"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37333611" href="https://news.ycombinator.com/vote?id=37333611&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It's not implausible if you consider that you're taking a rule that usually but not always works in the US and try to apply it to the whole world.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333561"><td></td></tr>
            <tr id="37334011"><td></td></tr>
                <tr id="37334424"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37334424" href="https://news.ycombinator.com/vote?id=37334424&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Yes. The vast majority of them follow a small number of simple patterns, and it's really not that hard to find a heuristic that interprets them with high accuracy.</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37333839"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333839" href="https://news.ycombinator.com/vote?id=37333839&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>No.<p>Either the name serves some legal purpose and they need to write it in full and 2 fields supports that.</p><p>Or the name serves no purpose and they can write what they want in those 2 fields.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334213"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334213" href="https://news.ycombinator.com/vote?id=37334213&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Eh, the "don't overthink it" is how all tech has become western centric, especially US-centric. In the grand scheme of things it's a small annoyance, but having your culture or norms constantly overwritten by some tech-bros is bad.<p>Like, my name still doesn't work in a lot of systems that expect ascii characters only. Like, thanks for deciding that names æøåü or similar isn't a valid name.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332461"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332461" href="https://news.ycombinator.com/vote?id=37332461&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Yes assume I have a Family name field and a Given Name field, the question is how to display them in the right order.<p>Im considering having a select field next to the name something like</p><p>Name Order: "Western style", "Eastern style"</p><p>or</p><p>Name Order: "Given Name, Family Name", "Family name, Given Name"</p><p>With a bit of extra work I can detect the country set a default for this field, while allowing users in places like Pakistan to still get the display order they want.</p><p>But I've never seen it done this way before, hence my curiosity to ask everyone.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332541"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332541" href="https://news.ycombinator.com/vote?id=37332541&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Country isn’t adequate; different parts of India use different conventions. To list the two states I’m most familiar with: West Bengal is mostly “Given Family” &lt;<a href="https://en.wikipedia.org/wiki/Bengali_name" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Bengali_name</a>&gt;, while Telangana largely uses “F. Given” (family name normally abbreviated, in my experience; the initial can also be more than one Latin letter for some Telugu consonants, mostly “Ch.”, and can be initials for more than one word) &lt;<a href="https://en.wikipedia.org/wiki/Telugu_names" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Telugu_names</a>&gt;.<p>Seriously, just avoid splitting name fields if you can help it. Do you <i>absolutely have to</i> have split names? Begrudgingly do so and acknowledge things will never be right for everyone. But if not, just don’t (and don’t make the mistake of <i>assuming</i> a format in order to do things like surname sorting—with a unified field you simply can’t do surname sorting).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333382"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333382" href="https://news.ycombinator.com/vote?id=37333382&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Minor thing, don’t call it “Western style” or “Eastern style”. I’m not sure what the correct term is.<p>There are plenty of Asian countries that don’t follow last name - first name so it’s not necessarily an “eastern” thing.</p><p>another opinion from me about automating based on country. It’s a cool thing to implement but might not be worth the effort.</p><p>An example I have is in the Philippines, majority of people go First Name Last Name. However, there’s a Chinese sub population that has last name first name. But then most of this sub population also have “Christian” names that they use in Most official documents or websites. I’m pretty sure other countries will also have similar nuances.</p><p>Maybe a screen name or username will be the razor.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332408"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332408" href="https://news.ycombinator.com/vote?id=37332408&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Well, if the app is designed for the asia pacific region then last name first name should be default to fit the majority of user's expectation, no?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333341"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333341" href="https://news.ycombinator.com/vote?id=37333341&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>This is lumping Asia Pacific into one big group. For example in the Philippines, its first name - last name that’s the standard. And then there’s a minority of Chinese-Filipinos that sometimes have traditional Chinese names.<p>It’s more complex than what it seems that I think a razor would work rather than going through all cases.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333706"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333706" href="https://news.ycombinator.com/vote?id=37333706&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Even in countries where you might assume this is true, it's not always!<p>For instance in China, most names are single character family name followed by a two character given name. Some rare family names are two characters and usually they will have a single character given name. Some people have single character family name and single character given name.</p><p>Then you get people from different ethnic groups who put the family name at the end. Often these are characterised by a long given name and then an interpunct joining that with the family name, but that's not always done. For instance a popular singer at the moment has the Chinese name 希林娜依·高 where 高 is a fairly normal Chinese surname so it's easy to identify as such. A very popular actress is named 迪丽热巴·迪力木拉提 so it's not always the case you can easily tell which name is which.</p><p>Even for those with simple construction, so the characters are FAB (family name, and 2 characters for given name). You'll find that within a family, where you might expect pet names, it's common instead to call each other by AB, except when they're annoyed and use the full name FAB. In a business context, someone with a higher rank might be F先生 (Mr F), F总 (Manager F), F董 (Director F), but close colleagues might call you AA, BB, 小A, 小B etc - but typically they won't get to choose that, they'll be told by the person "you can call me ___" and they might allow different people to use different names or react with their official title if someone uses too informal a name with them.</p><p>I've got Chinese friends who exclusively use an English name at work in China. I've got a friend who later moved to the UK, and most people in the UK know her by her Chinese name, but except for a few close school friends, nobody in her working life in China even knew her real name because the company she worked for only used English names (this isn't very common though).</p><p>In other countries, many people don't use their first name much. In the UK, I've known quite a few people who've primarily used their middle name, and others (including me) are generally known by others with a name that isn't among any of their given names. Sometimes these are common transformations, e.g. David to Dave, David to Dai (in Wales), but other times people just use initials or nicknames (e.g. I've got a friend who pretty everybody calls Danger, it started as a joke with friends, but now even his parents sometimes use it!)</p><p>Anyway, that was a long diversion, but it's always safer I think to ask for the full name as one field that not try to change it, but use it exactly as provided for official purposes, and to also ask for a preferred name for your communication with them. In cases were you share the name to other people, e.g. an online community, you might want an additional display name which might well be different again.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333899"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333899" href="https://news.ycombinator.com/vote?id=37333899&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It doesn’t matter if your friend uses an English name at work.<p>It doesn’t matter if the ethnic group writes it last first or first last.</p><p>If you’re filling in Chinese documentation and writing it in Chinese it will be a single field in Chinese.</p><p>If you’re writing it in English it will be written in 2 fields as first name and last name as the English version of the Chinese name.</p><p>The spoken English name of “alice” or “David” are not used on official documentation.</p><p>Name cards will sometimes have English names and sometimes they will put the English name with Chinese last name. But also write the Chinese name in Chinese.</p><p>I can’t believe how much people are over thinking this stuff.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334452"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37334452" href="https://news.ycombinator.com/vote?id=37334452&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Sounds like you're overthinking.  One field for "full name" and another for "how you want to be addressed" is way simpler than the "rules" you list above.  And also consider that you've only listed rule for two cases (and your rules are wrong, or at best incomplete), and have left out the hundreds of other rules for other languages and cultures.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334568"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37334568" href="https://news.ycombinator.com/vote?id=37334568&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>There is no list.<p>Either you live in China and fill in your name in Chinese.</p><p>Or you don’t live in China and you write your name in English.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                          <tr id="37335217"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37335217" href="https://news.ycombinator.com/vote?id=37335217&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Another edge case: aside from different orderings in different countries, in Spanish speaking countries people have 2 surnames.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332742"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332742" href="https://news.ycombinator.com/vote?id=37332742&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Ohh boy, as someone who works on a travel booking, service serving customers in Asia, this problem hits hard. I guess the answer depends on how you plan to handle the names once they got submitted.<p>I happen to be handing that data over to airlines, which has some of the less forgiving, yet fragmented name requirements. If you handle this incorrectly, your customer can't fly, even after they paid for the flight. And for those who say that this doesn't matter as much: <i>It absolutely does</i>. People do get confused by this more frequently than you think. I've seen people losing an entire trip that they saved for, all because of unclear naming requirements.</p><p>The way I deal with this is to provide a country and locale specific name fields. You don't have to detect the geolocation or track the user for this, just let them choose whatever locale setting they want, and give them the "sensible" layout. Here are some examples:</p><p>- In Vietnam, we use last name then first name.</p><p>- In Indonesia, we use first name, then last name, but also give an option to declare that the person doesn't have a last name.</p><p>- In Singapore, we use a single field to input the first name and last name.</p><p>Even when you've handled the layout convention carefully, the 3rd party you're handing the data to, if one exists, might not give the same care and attention that you do. In my case: some airlines just haven't gotten around the idea that some people simply don't have last names. When a person with a single name wants to fly, airlines want the customer to use the name for both first and last name (e.g. If the person's name is David, then the airline expects "David David"). If you require First Name and Last Name as the input, and don't elaborate on how to fill them, the customer might simply fill the last name with a dot (".") character. The airlines / any other 3rd party won't accept that.  For this, I suggest to detail out the ways in which you handle the data and go talk to your providers, if any.</p><p>All in all, it's a pretty tough challenge, and the wisdom around this isn't going to fit inside a single HN post. I do commend you for actually thinking about this problem. Good Luck.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333520" href="https://news.ycombinator.com/vote?id=37333520&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; In Singapore, we use a single field to input the first name and last name.<p>I have a completely “ordinary” name from a western perspective – first (given), middle, last (family). I live in Singapore, which has a few different popular naming conventions from a few different cultures. I’ve received documents with my name in every single variation possible. I‘ve been Mr First Name, Mr Middle Name, Mr Last Name, and so on. Often I can’t even determine if they have my name correct in their records – it could be recorded correctly but used incorrectly, or it could be recorded incorrectly and used correctly. Sometimes I suspect it’s recorded incorrectly but also used incorrectly in a different way.</p><p>Normally it’s not a problem, but like you say, airline tickets can cause issues. I think I’ve been demoted from “check in online” to “check in at a counter because we need to check your paperwork” a bunch of times because my passport doesn’t match my name on my ticket. Often it’s not even the name order – the airline will only sell my ticket with a first name and last name field (meaning I have to drop my middle name, which is on my passport) or they ask for all three and then concatenate first and middle with no space and truncate the last few letters.</p><p>Everything would be so much easier if I could just <i>enter my name</i>.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333744"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333744" href="https://news.ycombinator.com/vote?id=37333744&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It seems that with things like ticket there should be explicit "as in passport" instructions. Not that people will follow those...</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333883"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333883" href="https://news.ycombinator.com/vote?id=37333883&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Some people have more than one passport (multiple citizenship) and could use a different one to enter different countries because of visas. The name could be partly different on those passports. But yes, they probably know the passport they'll use since the time they buy the ticket.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37335084"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37335084" href="https://news.ycombinator.com/vote?id=37335084&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I as an example have 2 passports, and my name is first middle1 middle2 last.<p>One of my passports has my name like that, the other is first middle1 last, so even between passports its not the same, as one of them drops one of my middle names.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37334502"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334502" href="https://news.ycombinator.com/vote?id=37334502&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Why not just have a single field for "your name as it appears on your identity document"?  Or do the various airline systems expect the name to be split into separate fields depending on the locale?<p>I've always found it weird how broken this is.  Some US airlines have separate entry fields for first, middle, and last names.  But then they jam it together on the boarding pass as "Last, Firstmiddle" (yes, with first and middle mashed together, and middle lowercased).  That of course doesn't match my ID, but I've never had a problem traveling, even when I sometimes leave out my middle name entirely.  (I guess I get less scrutiny most places since I'm an American white male.)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334709"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334709" href="https://news.ycombinator.com/vote?id=37334709&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>There may be multiple identity documents with different names. This particularly happens a lot when the name itself is not written in Latin script and has to be romanized---one of my credit card had a different romanization (that I couldn't control) compared to what was written to my passport (that I had a control). Of course it is also possible that there is no canonical romanization available in any of those documents as well.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333769"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333769" href="https://news.ycombinator.com/vote?id=37333769&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; In Singapore, we use a single field to input the first name and last name.<p>How do you handle this when passing the data to the airlines?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333569"><td></td></tr>
                  <tr id="37335059"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37335059" href="https://news.ycombinator.com/vote?id=37335059&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>There's an established pattern for this, which is to provide users with a choice of:<pre><code>  Sort order:
    - By First Name
    - By Last Name

  Display order:
    - First, Last
    - Last, First
</code></pre>
Obviously, users can input names in a multitude of ways. But the convention is to offer at least two distinct fields, "First name" and "Last name". The sort order determines if the items are ordered by first name or last name. And the display order determines if the full name for each item is displayed as First name and then or last name, or the other way around.<p>And finally, set a default based by the user's locale and region.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333600"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333600" href="https://news.ycombinator.com/vote?id=37333600&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Related: as a Swede, the first times I had to fill out a honorific, it felt very weird. And many times they are required. We don't use honorifics in Sweden. The only difference between formal and informal address, is pretty much whether you use a nickname or not. The idea that there is a single category of "western name" is wonky.<p>Another is that my spoken/given name is after my "middle" name. Especially American forms don't deal well with that. In Sweden, you usually underline the given name, since it's used to address the person. Now I see a comment here saying that's a thing in the Philippines. That's interesting.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333775"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333775" href="https://news.ycombinator.com/vote?id=37333775&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>You used honorifics in Sweden too until fairly recently. Don't need to go further back than Pippi Longstockings (Herr Nilsson, Fröken Prysselius). And in genealogy I see plenty of Magisters, Profosses, Dragoons etc.<p>But Swedish naming traditions are great evidence that there's no such clean divide between "western" and "eastern" naming schemes: There have actually been dialects/areas where they put the genitive form if the family/farm name first, e.g "Dals Olof" even in Swedish, and in Finnish-speaking areas this was very common.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37335040"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335040" href="https://news.ycombinator.com/vote?id=37335040&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I find that while public Swedish records always store my full name, the "underlining" has often been lost. Many times I have been addressed by nurses or civil servants who assumed that my first given name was my spoken name.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332178"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332178" href="https://news.ycombinator.com/vote?id=37332178&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Could you just have a single field for name and allow anything in that field? The only downside is you can't do something like "Hi ${FIRST_NAME}" in your emails, but the upside is that it basically handles any name or name order, e.g. Indonesian single names, Spanish names with the mother's surname included, etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37332326"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332326" href="https://news.ycombinator.com/vote?id=37332326&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Re: <i>hi {first name}</i>, I’ve had pretty good luck with a name field and then a “nickname” field that explicitly asks, “how would you like us to greet you?”</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332820"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332820" href="https://news.ycombinator.com/vote?id=37332820&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; The only downside is you can't do something like "Hi ${FIRST_NAME}" in your emails<p>Which might be an upside, because there are cultures and contexts where it’s incredibly rude. You’re a glorified ledger, not a family member or life-long friend.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333562"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333562" href="https://news.ycombinator.com/vote?id=37333562&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>And in others the formality feels off and a tell-tale sign that the sender is not from here.<p>I get a lot of mail from foreign readers. Indians and Pakistanis are pretty formal, and most Africans even more so. On the other hand, some people from those same countries will write stream of consciousness emails that look like a text message to their dad.</p><p>The Western countries are a lot less formal, but and frequently drop the polite form which I was raised to use with all strangers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332255"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332255" href="https://news.ycombinator.com/vote?id=37332255&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>That's probably a good way to deal with interface issue, but do you really need their family names? It is a perhaps a good time to look at which data is needed vs what data collection from users is normalized</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37335033"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37335033" href="https://news.ycombinator.com/vote?id=37335033&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I come from a family that traditionally writes names as  Familyname Givenname.<p>But even in our country Firstname Lastname is prevalent among many regions and central government.</p><p>I grew up in the 90s and simply resigned that I will always be Givenname Familyname in most systems and documents.</p><p>After school I never bothered writing / typing / filling my Familyname first anywhere.</p><p>By simply staying consistent with this (wrong) way, I ensured that all my documents perfectly agree with each other. That peace of mind and lack of hassle is worth any "sacrifice" for me. Others may place different value on keeping their conventional names and must also have real reasons to do so too. I was lucky maybe, not to have those hurdles.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332228"><td></td></tr>
            <tr id="37332259"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332259" href="https://news.ycombinator.com/vote?id=37332259&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>You have two fields: full name and the name you want to be addressed by (e.g. in an email). This should cover the requirements of 99% of apps. You can add another field if really needed.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334358"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334358" href="https://news.ycombinator.com/vote?id=37334358&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>If a message is automatically generated I'd rather it address me as "Dear Customer" or whatever. In general, perhaps it's better to have something nicely written in the manner of a circular than a badly faked personal message which is likely to impress as less human than the honest and straightforward approach.<p>Spammers and big corporations write "Dear John, Did you know ..."; your local sports club doesn't do that crap.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333797"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333797" href="https://news.ycombinator.com/vote?id=37333797&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Substitute "full name" with "legal name". Then it's the government's problem.<p>Of course if you don't need the legal name it's best not to ask for it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334242"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334242" href="https://news.ycombinator.com/vote?id=37334242&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Won’t cover all finance apps - your name will most likely need to be submitted for sanctions screening, credit check, KYC checks etc. where understanding first name and last name is very important for cross-checking against other lists which expect the same.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332342"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332342" href="https://news.ycombinator.com/vote?id=37332342&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; the name you want to be addressed by (e.g. in an email)<p>In fact, I don't want my full name to be addressed anywhere at all. Just call me by the alias I provided.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332479"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332479" href="https://news.ycombinator.com/vote?id=37332479&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>The full name is typically used for billing, or just for record keeping. Your SaaS itself may not need it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37334119"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37334119" href="https://news.ycombinator.com/vote?id=37334119&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>What do you need their name for?<p>Do you need their legal name? As in, actually legally need it?</p><p>Realise that you'll probably need several name fields in different places depending on what you're doing, i.e. an e-commerce app may have a name to address someone by, a name on a billing address, a name on a shipping address, a name on a payment method... all of which can vary for the same person.</p><p>If all you need is a way to address them, just a single name field.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332316"><td></td></tr>
                <tr id="37332351"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332351" href="https://news.ycombinator.com/vote?id=37332351&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Is it really worth using enum in postgres over a plain string, and optionally a check constraint if you're super concerned about strictness?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37332918"><td></td></tr>
                        <tr id="37333514"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333514" href="https://news.ycombinator.com/vote?id=37333514&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Is there any reason why you need to specifically know the first and last name, or the family and given name?  I think there is a lot of inertia in the idea that you must have two name fields, but why?<p>Why not just have one name field, and let people type what they want in it?</p><p>Or better yet, have one field for “what should I print as the name when I send you a package or letter”, and “what would you like me to call you?” (e.g. preferred name)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333713"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333713" href="https://news.ycombinator.com/vote?id=37333713&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Also even in west the first name + last name doesn't capture the whole effect.<p>For example my credit card has last name, first name and middle name. And should probably be passed as such. In some addressing you want all three. But for most communication and even mailing things only first + last is enough or if more formal last + first.</p><p>In the end it is huge mess with no clearly applicable rules.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37334008"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37334008" href="https://news.ycombinator.com/vote?id=37334008&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Try logging into Facebook with different locale settings on your Facebook account. You can have several names depending on your language. That way they can handle name order but also show English speaking contacts a readable name, while say, Japanese friends gets to see the Japanese name.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333568"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333568" href="https://news.ycombinator.com/vote?id=37333568&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>So first of all everyone has read "falsehoods programmers believe about names". And of course everyone knows we should just use "name" and that's it. Perhaps a greeting/displayname if we need to do "Hello $customer" in an email.<p>But still. There are often hard requirements <i>that are impossible to reconcile</i>  with the "just store one name" idea.
As an example: you need to produce alphabetically sorted lists. It would rarely be acceptable to produce lists of people sorted by their given names in Western Europe for example. So as an example if you really do need to have BOTH "Doe, John" and "John Doe" what do you do in case you know at least 99% of users will have names following this format? Do you force people to enter their name twice? In that situation I would:</p><p>- Store it as a single name column, with more columns as required (greeting, sortable, ).</p><p>- Present a UX that is appropriate for the region of 99% users. So for western Europe show a "first name", "last name", but don't store that: just set the name column to "$firstname $lastname" and the index name column to "$lastname, $firstname" and greeting to "$firstname" for example. That way I at least let 99% of your users only input the name once.</p><p>- But (and this is important) offer an alternative way of specifying it for those who don't have a name that conforms to the format. You let just that small fraction of users enter a name/indexname/greeting instead.</p><p>And yes: the above ONLY applies if it's really a hard requirement to separate them, because the last name sorting was really a requirement. Of course there are other requirements that force other solutions. The "just one input" is the correct solution for all the cases where it is possible.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334367"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334367" href="https://news.ycombinator.com/vote?id=37334367&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Presenting an alphabetically sorted list only really makes sense if you can force everyone to use a single alphabet.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334250"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37334250" href="https://news.ycombinator.com/vote?id=37334250&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; It would rarely be acceptable to produce lists of people sorted by their given names in Western Europe for example.<p>Luckily, it has become a bit uncommon to manually search a printed document by name.</p><p>In times of GDPR, the days of printed lists of personal info posted on some bulletin board are pretty much over.
For a list that is non-public, chances are you already have it on a device with a search feature.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37333101"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333101" href="https://news.ycombinator.com/vote?id=37333101&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>IMHO as a Japanese, the western convention is preferable if the app is English. It's less confusing.<p>At least it should be consistent across the app.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332246"><td></td></tr>
                <tr id="37332471"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332471" href="https://news.ycombinator.com/vote?id=37332471&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Yes. Unless you have a really good reason to split the name (legal documents), you should think really hard about why you're attempting to get a full name in the first place, instead of "a nickname for this account". And if you really do want the name, why can't you use whatever the user puts in a single field "name" instead.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332281"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332281" href="https://news.ycombinator.com/vote?id=37332281&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>This. I'd also add "How would you like to be called" as sometimes people go by their second given name, not the first given name. For example, you might have a common first name like Muhammad or Ketut.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332345"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332345" href="https://news.ycombinator.com/vote?id=37332345&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I would not do this. For reasons of respect, you may need to address a person by either their first or last name (with a title).<p>Addressing a person by their full name can come across as rude (I personally find it rude too, e.g. "Hey John R. Brown, this is your order"), so it's useful to split up "given name" and "family name" into 2 fields so that you don't have to depend on name ordering which varies by culture.</p><p>If they only have one name, like many people in Myanmar, then it's just the given name. If they don't have family names (some cultures use patronymics) then it's just a long given name. If they have double-barreled last names like Hispanic names, then it's just a long family name.</p><p>Most people in the world have filled out forms on U.S. sites. They'll know what to do.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333780"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37333780" href="https://news.ycombinator.com/vote?id=37333780&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; <i>Most people in the world have filled out forms on U.S. sites. They'll know what to do.</i><p>I'm just French (where we often use "FAMILY Given" in a formal setting, but "Given Family" is common otherwise) and I still need a moment to think every time I see "first name" "last name" fields instead of "family name" "given name", after decades of being online.</p><p>My parents just fill these forms in random order, basically.</p><p>The other problem is:</p><p>&gt; <i>For reasons of respect, you may need to address a person by either their first or last name (with a title).</i></p><p>This often doesn't translate well between cultures. American websites will often use the given name in any setting and this is weird. If they want to be friendly-informal they should address me with "Hello Given Family, this is your order".
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332424"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332424" href="https://news.ycombinator.com/vote?id=37332424&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; I would not do this. For reasons of respect, you may need to address a person by either their first or last name (with a title).<p>Just add a field for the desired form of address, or a nickname.</p><p>&gt; Addressing a person by their full name can come across as rude</p><p>Stuffy, Maybe, but rude?  Calling people by their first name is rude. Calling people by forms of address which don’t exist is rude.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332496"><td></td></tr>
            <tr id="37333663"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333663" href="https://news.ycombinator.com/vote?id=37333663&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span><i>&gt; Just add a field for the desired form of address, or a nickname.</i><p>For westerners you might want sunny, good-news messages to say "Hi John" but for an apology for a problem (like a delay or cancellation) you might want to use the more respectful "Dear Mr Doe"</p><p>On the other hand, this discussion has plenty of examples of why separate 'first name' and 'last name' fields don't work in all cultures. And I think we can all agree that asking people for a name and <i>two</i> desired forms of address ready to e-mail them in two different tones would be a very unusual sign up process.</p><p>I'm not sure there's any universal, fully respectful way of doing this across cultures.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332538"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37332538" href="https://news.ycombinator.com/vote?id=37332538&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; Stuffy, Maybe, but rude? Calling people by their first name is rude. Calling people by forms of address which don’t exist is rude.<p>For me and my people, it's perceived as rude (it sounds infantilizing). Just because you don't perceive it that way doesn't mean it is not.</p><p>&gt; Calling people by their first name is rude</p><p>Yes, it can be. That's why it's useful to have their last names disambiguated so a proper title can be appended. When you have the full name in a single string, you can't do that -- you don't have enough information to decompose the name into its parts. When you have both given name and family names, you don't have to guess which is which.</p><p>There are forms of address that sound rude if appended to full names.</p><p>(of course it can be argued that even with 2 fields one might get it wrong, so I fully agree that adding a field indicating how one prefers to be addressed is the best solution of all)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332699"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37332699" href="https://news.ycombinator.com/vote?id=37332699&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; For me and my people, it's perceived as rude (it sounds infantilizing). Just because you don't perceive it that way doesn't mean it is not.<p>So what you’re actually saying now is that any <i>inferred</i> form of address may come across as rude, which means you original suggestion is plain incorrect: splitting given name and family name does not allow generating a genetically never rude form of address, because you can’t know whether the user’s culture favours given names, family names, full names, or even none of the above.</p><p>And that’s before getting into honorifics and titles.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332799"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37332799" href="https://news.ycombinator.com/vote?id=37332799&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; So what you’re actually saying now is that any inferred form of address may come across as rude, which means you original suggestion is plain incorrect: splitting given name and family name does not allow generating a genetically never rude form of address, because you can’t know whether the user’s culture favours given names, family names, full names, or even none of the above.<p>No, that is not it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332959"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_37332959" href="https://news.ycombinator.com/vote?id=37332959&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; No, that is not it.<p>Yes, it very much is. You’re declaring that one form of address is rude to your culture, while acknowledging that it is proper to an other culture.</p><p>This means the two cultures are completely incompatible on that point, and thus no generic munging method can satisfy both.</p><p>And yet your original suggestion was to do exactly that, just in a form suited or at least compatible with your cultural sensibilities.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333089"><td></td></tr>
                              <tr id="37332608"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37332608" href="https://news.ycombinator.com/vote?id=37332608&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; For me and my people, it's perceived as rude<p>Are you or your people Asian?</p><p>Though, even in Asia which names to call a person can differ wildly. I'm not sure how your personal ideas about rudeness are relevant here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37332794"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37332794" href="https://news.ycombinator.com/vote?id=37332794&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>&gt; Are you or your people Asian?<p>Yes.</p><p>&gt; I'm not sure how your personal ideas about rudeness are relevant here.</p><p>Names matter.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                          <tr id="37333782"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333782" href="https://news.ycombinator.com/vote?id=37333782&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Another case you might want to accommodate is Arabic speaking countries where the names goes like name son-of father-name son-of grandfather-name son-of great-grandfather-name tribe name.  In most cases, such as when dealing with government applications four generations is required but sometimes tribe name is omitted where it can identify your religion, sect to prevent bias to you or against you.  The son-of (Bin or Ibin) is mostly omitted except for cases where the first name is composite of two names, like “Mohammed Ali”, where both parts are valid names on their own.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333534"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37333534" href="https://news.ycombinator.com/vote?id=37333534&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Shameless plug: I gave a talk on names and the problems that arise when you give them special treatment - <a href="https://youtu.be/NfKhY3sAQ9E" rel="nofollow noreferrer">https://youtu.be/NfKhY3sAQ9E</a><p>But the tl;dw is to do this: &lt;input type="text" name="name" /&gt;</p><p>I don't know what kind of app you're building, but unless it's 100% impossible (from a business perspective) not to separate people's names into multiple fields, you really should just make it one field.</p><p>If it's a true requirement for the business—due to a third-party vendor's requirements, for example—then I suppose you could simply ask the user which order they prefer and display their name accordingly.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332435"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332435" href="https://news.ycombinator.com/vote?id=37332435&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Not necessary Asian btw. As a Russian I can tell that Russians also may use "family name - given name - father name", usually in official documents.<p>Though it it also may be just "given - family", as well as any other variation informally. So you wouldn't make a mistake with UI
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37333374"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333374" href="https://news.ycombinator.com/vote?id=37333374&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Doesn't Russian also have different forms of names depending on their grammatical place in the sentence?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334177"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334177" href="https://news.ycombinator.com/vote?id=37334177&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>You only need to capture the name in nominative case, though; declined forms are derivable from that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37334351"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37334351" href="https://news.ycombinator.com/vote?id=37334351&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I was wondering when someone would cite this: <a href="https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/" rel="nofollow noreferrer">https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-...</a><p>But since nobody else seems to have done so, I will. Globally, names don't follow any common format or concept at all that you can reduce to a formula. My takeaway is that it's best to have a free-format "full name" field, and another "preferred name" field so you can allow people to choose how to be referred to.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332467"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332467" href="https://news.ycombinator.com/vote?id=37332467&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Indonesia: Single name is common. Therefore only one field filled in should be legal.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37332726"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332726" href="https://news.ycombinator.com/vote?id=37332726&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It's half funny and sad that for passports one has to repeat their names for both fields or not fill the second field at all (which may be not allowed) or have to make up new name.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37332738"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37332738" href="https://news.ycombinator.com/vote?id=37332738&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Yes. and for things like KYC/AML the 100 points test can get very literal about "as written on the passport"</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37332550"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332550" href="https://news.ycombinator.com/vote?id=37332550&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>I built an ecommerce system where we shipped hundreds of thousands of tshirts to people. They used a bazillion variations of browsers. It took a while for us to get the form 100% correct across everything.<p>One field.</p><p>Also, don't use type=number fields... the browser UX is terrible across many devices. Just make everything type=text and be done with it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334855"><td></td></tr>
            <tr id="37332555"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332555" href="https://news.ycombinator.com/vote?id=37332555&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>Don't over think it. Either:<p>- Label the fields FAMILY, GIVEN, MIDDLE (then put them in the order you desire)
- Or simply put NAME</p><p>For a LOT of use cases, FULL NAME side-steps the issue and works out great. Sure your emails that go out are "Hi {{ full_name }}," but that's okay.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37334143"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37334143" href="https://news.ycombinator.com/vote?id=37334143&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Representing names (and postal addressing via UPU S1) are one of the use cases where SGML/XML and other document techniques feels appropriate in internal business data representation, as opposed to storing details in 1NF (a la first/last name) in databases, especially when working with international customer data.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37332350"><td></td></tr>
            <tr id="37332671"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332671" href="https://news.ycombinator.com/vote?id=37332671&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>If it is just for e-commerce, just steal signup form from local e-commerce websites. Or if there are local competitor platforms that does as good as yours, look into theirs.<p>But the question sounds like there are already substantial asian users in divergent conventions without frictions. If that is what your app is trying to fix, I'm not completely sure how it will work out.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37333920"><td></td></tr>
            <tr id="37332320"><td></td></tr>
            <tr id="37332507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332507" href="https://news.ycombinator.com/vote?id=37332507&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>In English, if the names are separated by a comma it generally indicates that they're in surname, given names order.<p>I think this is to make sorting easier, since names are sorted in alphabetical order by surname then given names.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332289"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332289" href="https://news.ycombinator.com/vote?id=37332289&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It depends on the purpose. Here you say it’s just so you can call them by their name properly, so just use a single text field “What name would you like us to address you as?”. If there’s legal or other issues that might not be good enough.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333253"><td></td></tr>
                <tr id="37335056"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335056" href="https://news.ycombinator.com/vote?id=37335056&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>It's worth noting that this convention is also applicable in Francophone African countries, but not in Quebec.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37333348"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333348" href="https://news.ycombinator.com/vote?id=37333348&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>I was going to say that. It’s amazing how often French users get it wrong when filling in forms. It can result in online services trying a more informal tone calling people by their last name only (“Salut, DUPONT!”), which can be seen as a bit rude. People don’t read form labels with great attention.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333636"><td></td></tr>
                <tr id="37333733"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37333733" href="https://news.ycombinator.com/vote?id=37333733&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It is respectful in France too, but only if you use a title. Say the person is DUPONT Maëlle, then:<p>"Bonjour madame Dupont" is respectful</p><p>"Bonjour Maëlle" is informal</p><p>"Bonjour Dupont" is rude.</p><p>"Bonjour Maëlle Dupont" or "Bonjour DUPONT Maëlle", using whatever the user put as their "name", is just neutral (though the former is less formal than the latter).</p><p>Generally, you only want to address young people by their given name, and adults are more likely to find being addressed by their given name rude, than young people are to find being addressed by their family name "weird" (unless they are maybe less than 16).</p><p>I find that using just the given name in any kind of official communication often has a "fellow kids" vibe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37332401"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332401" href="https://news.ycombinator.com/vote?id=37332401&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Don't some Asians use an alternate name too, a more Westernized version of their name, alongside their native name?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37334676"><td></td></tr>
            <tr id="37332254"><td></td></tr>
                <tr id="37335068"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37335068" href="https://news.ycombinator.com/vote?id=37335068&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>The tech giants often don't account for EU customs, let alone non-Western ones.<p>Instead I would look at how the relevant governments handle it, if they have something like the UK's component catalog.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332442"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332442" href="https://news.ycombinator.com/vote?id=37332442&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Seems many tech giants have taken a western colonial view and expect anyone different to just suck it up.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37332542"><td></td></tr>
            <tr id="37332231"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37332231" href="https://news.ycombinator.com/vote?id=37332231&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>One interesting approach would be to use a regex to detect if the name is written in Hanzi, Kanji or the Korean or Vietnamese alphabets and write the name in whatever format is appropriate for the locality. The Asian people I know would be be unsurprised to see their name rendered western style when written using whatever romanization format their native language uses.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37333250"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333250" href="https://news.ycombinator.com/vote?id=37333250&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>You mean well so I gave you an upvote :-). I'm a Westerner living in Asia. The name my British parents gave me is James Hush. Every family member calls me Jamie. My legal Chinese name is 許杰, if you tried to write that the "Western" way the closest thing you could do is write xǔ jié, but even then there's multiple Chinese characters that represent "xǔ" and "jié" so it would make no sense.<p>Names are hard hahahaha.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37332440"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37332440" href="https://news.ycombinator.com/vote?id=37332440&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>You can't do this: what language do you think a name like 金正恩 belongs to, and how do you rule out the others? Given this, what parts do you expect to extract from it?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37334140"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37334140" href="https://news.ycombinator.com/vote?id=37334140&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><p><span>It's very simple: put a drop-down field on the web form which allows the user to select which naming convention they'd like to use (generally by country, but there can be more for other regions too).  This should yield only a few hundred different options to scroll through.  Then each option loads a different form specific to that option.  Load the whole thing in one giant blob of JS and have it load for every page view.<p>I've seen lots of websites programmed exactly this way, so it's obviously good web design.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37334555"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37334555" href="https://news.ycombinator.com/vote?id=37334555&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Sure, but that was a direct response to someone who said "just have a single field and run it through a regex to figure things outs".</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37333468"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37333468" href="https://news.ycombinator.com/vote?id=37333468&amp;how=up&amp;goto=item%3Fid%3D37332126"></a></center>    </td><td><br><div>
                  <p><span>Please don't do this. Like with western names, it is impossible to discern nationality/culture from written names alone.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Absurd Success (367 pts)]]></title>
            <link>https://www.marginalia.nu/log/87_absurd_success/</link>
            <guid>37331778</guid>
            <pubDate>Thu, 31 Aug 2023 02:27:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/87_absurd_success/">https://www.marginalia.nu/log/87_absurd_success/</a>, See on <a href="https://news.ycombinator.com/item?id=37331778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>So… I’ve had the most unreal week of coding. Zero exaggeration, I’ve halved the
RAM requirements of the search engine, removed the need to take the system
offline during an upgrade, removed hard limits on how many documents can be indexed,
and quadrupled soft limits on how many keywords can be in the corpus.</p><p>It’s been a long term goal to keep it possible to run and operate the system
on low-powered hardware, and so far improvements have been made, to the point
where my 32 Gb RAM developer machine feels spacey rather than cramped, but this
set of changes takes it several notches further.</p><p>But to roll back the tape to more somber times.</p><p>Marginalia Search went offline for almost a week due to some problems
with the <a href="https://www.marginalia.nu/release-notes/v2023-08-0/">latest release</a>.</p><p>I won’t go into the details too much here, it was a string of fairly trivial
scaling problems in a process with a run time of 1 day when it works well
and 2-3 days when it doesn’t. Had to restart it a few times, mostly because
it ran out of RAM.</p><p>The cause of the <em>outage</em> was the fact that the system needs to go offline
during an index switch in the first place. If it could have staid online while
all this went on, nobody would have been the wiser…</p><p>The reason for this is that the URL database is wiped during an upgrade.</p><p>Inserting and updating rows in a database table rapidly approaching a billion rows
is <em>slow</em>, and the processes involved in loading the new data uses a crap-ton of RAM
and can’t co-run with the index service which does the same; and the primary key for
the table is also a 32 bit integer so … you can’t really go beyond 2 billion in the
first place.</p><p>While the processes to get the system up and kicking again were ticking along,
I had a moment to reflect on the problems, came up a plan to slay the dragons
responsible for needing any sort of outage, while at the same time drastically
reducing the system requirements.</p><h2 id="the-first-dragon--the-url-database">The First Dragon: The URL database.</h2><p>The URL database is one of the oldest components of the search engine and a
lot of the design decisions that went into it made sense at the time, but that
hasn’t been the case for years since. Everything about the system is alien
to how it looked back when the tables were first drawn up.</p><p>Migrating the data has been such a nasty prospect that it’s mostly been left to
be as-is modulo some fairly small changes (ALTER TABLE … takes a few hours in prod).</p><p>At the heart of the URL database is two tables that are only ever written to while loading a new
index, and only ever read via primary key lookups. There’s an additional index keeping paths
unique within domains, and it is just absurdly large for what it does.</p><p><img src="https://www.marginalia.nu/log/87_absurd_success/olddesign.png" alt="An excerpt of the tables involved"></p><p>There’s a URL table mostly responsible for assigning an unique numeric ID to each URL,
and a PAGE_DATA table, where each PAGE_DATA entry contains information such as the title and
of a link, if we’ve indexed it.</p><p>So what if we smack these two tables together into a single table <strong>sqlite database</strong>,
keep the data real simple, make the loader process responsible for generating unique IDs,
and have some other table for enumerating URLs we aren’t indexing… yeah that’s pretty doable.</p><p>It’s an unconventional choice to mix sqlite and mariadb like this, but the two serve different needs.
The system needs a persistent view of the world, but that view is relatively small, like sub 1 GB most likely,
but also plug-and-play state changes (with state in the terabytes). It does fly in the face of every
conventional wisdom, but has real benefits.</p><p>So, something like this:</p><p><img src="https://www.marginalia.nu/log/87_absurd_success/newdesign.png" alt="The new tables"></p><p>We’re still softly relating the DOCUMENT and DOMAIN tables acoss databases, but we never actually need to join
the two; in fact in a disaster scenario, the most important bits of the DOMAIN table can be re-constructed from the DOCUMENT table.</p><p>I’m glossing over some details…</p><h2 id="generate-unique-url-ids-with-no-index">Generate unique URL IDs with no index</h2><p>The URLs table was using 32 bit IDs, and it was always on the brink of integer rollover
and a known scaling issue. Part of why the URL database is wiped is to keep the ID
column from overflowing.</p><p>This doesn’t really work if we want to generate unique IDs in a way that won’t require us
to keep a collection of each URL we’ve seen in memory. URLs are sometimes repeated so we
can’t just use a counter and increment each time.</p><p>Each record also had a DOMAIN ID, but each URL only has a single domain. In the old solution
there was a unique constraint on URL ID and DOMAIN ID. That’s not really desirable, as indices
get very expensive for tables this large.</p><p>So let’s <em>construct</em> a 64 bit ID instead of letting the database assign one for us. Say the lower
26 bits are an ordinal, bits 26-56 are a document ID, and the highest most bits are reserved for the
index to do sorting tricks with.</p><p>This permits about 67,000,000 documents per domain (~10x English Wikipedia), and 2 billion domains,
which is nearly 100 times as many domains as the number of domains Marginalia has seen referenced
anywhere on the web. Nice and future proof.</p><p>We’re moving a lot of data sanity responsibility out of the database and into the process that
generates the data, but they are a lot cheaper to enforce on that end so that’s a feature, if a
bit unconventional.</p><p><img src="https://www.marginalia.nu/log/87_absurd_success/urlscheme.png" alt="bit coding scheme for the new ID"></p><h2 id="unexpected-benefits">Unexpected Benefits</h2><p>So getting rid of all these humongous indices and large tables mixing hot and cold URL data means
the MariaDB server doesn’t need 36 Gb of RAM anymore. The hot data left in MariaDB is a few hundred
megabyteas at most, and the server most likely won’t need more than 2 Gb assigned to it.</p><p>The SQLite data needs an index too, but only for its primary key, and the actual hot part of
the index should be sub-gigabyte. It’s this much smaller because we’re indexing an order of
magnitude less data, and the index is of a single 64 bit long. There is no additional unique
constraint!</p><p>Additionally, since the output from the loader is now just a bunch of files, it turned out to
be pretty easy to create an automatic backup of these files. That means that in the case of
a bad deployment, the system can recover from a disaster within a few hours as contrasted with almost
a week like before.</p><p>If I had stopped there, it would be a pretty sweet change, but I felt emboldened by the success
and decided to turn to the other piece of ancient technical debt.</p><h2 id="the-second-dragon--reverse-index-construction">The Second Dragon: Reverse index construction</h2><p>The previous changes mostly affected the loader. It outputs a urls database and a (document,words[])-oriented
journal of document data. We need to transpose that data into something that is not only on the shape (word,documents[]),
but then make it indexable.</p><p>To help do this, a lexicon is used to map from keyword strings to a dense mapping of term ids. The first word you insert
gets id 0, the next word id 1, etc. This is basically a long-&gt;int open hash map. The system maps from string to long using
a 64 bit hash function.</p><p>The problem with this is that it gets full. This is mostly a Java problem, since Java doesn’t permit arrays with more than 2
billion entries; and the implementation used chokes all the way back at 1 billion. You can work around it, but even if you do,
the next problem is that at 1 billion entries, it uses 12 Gb of RAM.</p><p>The index service sits at a well-endowed 60 Gb RAM in production right now, most of which is off-heap memory so it could do with
going on a bit of a diet. It’s on-heap (i.e. within-JVM) size somewhat exceeds 32 Gb wich is especially unfortunate, as it prevents
the use of <a href="https://wiki.openjdk.org/display/HotSpot/CompressedOops">CompressedOOPs</a>, which is something you really want in Java.</p><p>The old reverse index construction has, like the URLs table, been around for a hot minute. It’s mostly been around due to how
unpleasant it is to work with. I was really struggling to find a good abstraction while I originally built it, and it’s been
<em>append only code</em> since. It’s not made better by this inherently being very finicky programming. If anything is even the slightest
bit off, everything falls apart in deeply inscrutable ways.</p><p>Very loosely, the shape of the algorithm was something like this:</p><div><pre tabindex="0"><code data-lang="java"><span><span>  highestWordId <span>=</span> findHighestWordId<span>();</span>
</span></span><span><span>  <span>long</span><span>[]</span> counts <span>=</span> <span>new</span> <span>long</span><span>[</span>highestWordId<span>];</span>
</span></span><span><span>  <span>for</span> each word in each document<span>:</span>
</span></span><span><span>    counts<span>[</span>word<span>]++;</span>
</span></span><span><span>  
</span></span><span><span>  <span>// offsets[i] = sum(counts, 0, i);
</span></span></span><span><span><span></span>  <span>long</span><span>[]</span> offsets <span>=</span> turnCountsToOffsets<span>(</span>counts<span>);</span> 
</span></span><span><span>  write offsets to file
</span></span><span><span>  
</span></span><span><span>  <span>long</span><span>[]</span> documents <span>=</span> memory map <span>max</span><span>(</span>offsets<span>)</span> longs in a file
</span></span><span><span>  
</span></span><span><span>  <span>for</span> each word in each document<span>:</span>
</span></span><span><span>    documents<span>[</span>offsets<span>[</span>word<span>]++]</span> <span>=</span> document
</span></span><span><span>  
</span></span><span><span>  <span>// draw the rest of the owl
</span></span></span><span><span><span></span>  constructIndices<span>(</span>documents<span>,</span> offsetsfile<span>)</span>
</span></span></code></pre></div><p>We need a lexicon because of this part. We’re relying on the domain of word ids to be densely packed.</p><div><pre tabindex="0"><code data-lang="java"><span><span>  <span>long</span><span>[]</span> counts <span>=</span> <span>new</span> <span>long</span><span>[</span>highestWordId<span>];</span>
</span></span></code></pre></div><p>Other than requiring an absurd amount of RAM, there’s a subtle farther down in the code:</p><div><pre tabindex="0"><code data-lang="java"><span><span>  <span>for</span> each word in each document<span>:</span>
</span></span><span><span>    documents<span>[</span>offsets<span>[</span>word<span>]++]</span> <span>=</span> document
</span></span></code></pre></div><p>This part writes about a terabyte of data out of order to a memory mapped file. Since it is almost completely
out of order, most of these changes are constantly being committed to disk. You may have heard
that SSDs deal with random access much better than mechanical hard drives. For read access this is largely
true, for small writes it is not, due to an effect called <a href="https://en.wikipedia.org/wiki/Write_amplification">write amplification</a>.</p><p>In brief, every time an SSD updates a single byte anywhere on disk, it needs to erase and
re-write that entire page. To avoid this it has caches that gather adjacent writes into a single operation,
but these caches aren’t terabyte sized, and do nothing in this scenario.</p><p>So while we’re writing a terabyte, the hard drive is writing something like half a petabyte. This is understandably slow and puts an insane amount of wear on the disk.</p><p>For a long time a stopgap has been in placed called a <code>RandomWriteFunnel</code>, which buckets the writes
into smaller files first, and then writes the larger file in-order. This is very slow, but not
as slow as spraying and praying like the original algorithm did.</p><p>I’ve understood for a long time that this is not the civilized way of transposing index data. What you do instead is
to create a series of smaller indexes where everything fits in memory, and then you merge them. Merging sorted lists
is fast and even mechanical hard drives agree with the operation. Still a bit finicky though, especially since we’ll
merge sorted lists of sorted lists that need merging.</p><p>If you construct the index this way, you <strong>no longer need the lexicon</strong>! It’s possible to use the 64 bit string hashes
for term ids instead!</p><p>Initially a few head-scratchers cropped up:</p><ul><li><p>The input to the index construction is compressed. How can parts of it be read quickly? — Don’t. Create multiple smaller files instead, and read them one by one.</p></li><li><p>What is a good way to merge indexes? — Don’t. Merge the data before the indexes are created.</p></li></ul><p>An abstraction called a <code>preindex</code> is introduced, which is basically what was constructed in the
code sample above. Two arrays, one with wordIds, and one with counts; both indexing a third array
with document data.</p><ul><li><a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/features-index/index-reverse/src/main/java/nu/marginalia/index/construction/ReversePreindex.java">index-reverse/…/ReversePreindex</a></li><li><a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/features-index/index-reverse/src/main/java/nu/marginalia/index/construction/ReversePreindexDocuments.java">index-reverse/…/ReversePreindexDocuments</a></li><li><a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/features-index/index-reverse/src/main/java/nu/marginalia/index/construction/ReversePreindexWordSegments.java">index-reverse/…/ReversePreindexWordSegments</a></li></ul><p><img src="https://www.marginalia.nu/log/87_absurd_success/preindex.svg" alt="Preindex"></p><p>Since the inputs are small, these preindexes are easy to construct as we can do most of it in RAM. It’s so much easier when you can do you can assume the data fits in RAM. We can then commit them to disk as we generate them, and add a subsequent merging step to construct one final preindex.</p><p><img src="https://www.marginalia.nu/log/87_absurd_success/merging.svg" alt="Successive merging of files"></p><p>The preindex is finally turned into the reverse index by adding static btree indices to the words
table and each document block like before. This is a relatively quick process.</p><p><img src="https://www.marginalia.nu/log/87_absurd_success/index.svg" alt="The final output"></p><p>This does not only require much less RAM since the lexicon is out, it’s also quite a bit
faster than the original algorithm. But wait, there’s more! The lexicon had the unfortunate
side-effect of giving each set of processed data its own dialect, a word might be given one ID
in one run, but probably another ID in another run. This meant that you needed to process all data
at the same time. Foregoing the lexicon, it’s possible to merge disparate sets of data, and e.g.
reuse the output of processing a large set of seldom-changing data (say Wikipedia), and join it with
more frequently changing index data.</p><h2 id="in-closing">In closing</h2><p>This set of changes has been absurdly successful. The system RAM requirement has been halved,
almost every single known scaling problem and operational paper cut has been addressed. It’s also
had unexpected benefits I’ve only begun to consider.</p><p>I wish I knew what happened, or how to replicate it. It’s involved more upfront design than I
normally do, by necessity. I like feeling my way forward in general, but there are problems where
the approach just doesn’t work. This is squarely within that domain.</p><p>Sometimes you just tug at the right yarn I guess, and then problems unravel one by one.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Australia will not force age verification due to privacy and security concerns (138 pts)]]></title>
            <link>https://www.theguardian.com/australia-news/2023/aug/31/roadmap-for-age-verification-online-pornographic-material-adult-websites-australia-law</link>
            <guid>37330234</guid>
            <pubDate>Wed, 30 Aug 2023 22:58:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/australia-news/2023/aug/31/roadmap-for-age-verification-online-pornographic-material-adult-websites-australia-law">https://www.theguardian.com/australia-news/2023/aug/31/roadmap-for-age-verification-online-pornographic-material-adult-websites-australia-law</a>, See on <a href="https://news.ycombinator.com/item?id=37330234">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The federal government will not force adult websites to bring in age verification following concerns about privacy and the lack of maturity of the technology.</p><p>On Wednesday, the communications minister, Michelle Rowland, released the eSafety commissioner’s long-awaited <a href="https://www.theguardian.com/australia-news/2023/apr/04/labor-to-consider-age-verification-roadmap-for-restricting-online-pornography-access" data-link-name="in body link">roadmap for age verification</a> for online pornographic material, which has been sitting with the government since March 2023.</p><p>The federal government has decided against forcing sites to bring in age verification technology, instead tasking the eSafety commissioner, Julie Inman Grant, to work with the industry to develop a new code to educate parents on how to access filtering software and limit children’s access to such material or sites that are not appropriate.</p><figure id="eab0beff-bb27-476a-8c7f-8af05fd88457" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/australia-news/2023/aug/20/australia-eyes-uk-online-bill-in-fight-with-tech-companies-over-encryption-child-safety&quot;,&quot;text&quot;:&quot;Australia eyes UK online bill in fight with tech companies over encryption and child safety&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;elementId&quot;:&quot;eab0beff-bb27-476a-8c7f-8af05fd88457&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}"></gu-island></figure><p>“It is clear from the roadmap at present, each type of age verification or age assurance technology comes with its own privacy, security, effectiveness or implementation issues,” the government’s response to the roadmap said.</p><p>The technology must work effectively without circumvention, must be able to be applied to pornography hosted outside Australia, and not introduce the risk to personal information for adults who choose to access legal pornography, the government stated.</p><p>“The roadmap makes clear that a decision to mandate age assurance is not yet ready to be taken.”</p><ul>
 <li><p><strong><a href="https://www.theguardian.com/australia-news/2022/oct/29/email-newsletters-guardian-australia-best-daily-news-emails-newsletter-free-sign-up-inbox-subscribe-headlines?CMP=copyembed" data-link-name="in body link">Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup</a></strong></p></li>
</ul><p>The new tranche of codes will be developed by eSafety following the implementation of the first set of industry codes in December this year.</p><p>The government will also bring forward an independent statutory review of the Online Safety Act in 2024 to ensure it is fit for purpose and this review will be completed in this term of government. The UK’s approach to age assurance will also be monitored as the UK is “a key likeminded partner”.</p><p>In the roadmap, eSafety’s research found of the 75% of 16 to 18-year-olds who said they had seen online pornography, nearly one-third had seen it before the age of 13, and nearly half between 13 and 15.</p><p>The report suggested to trial a pilot of age assurance technologies, but this was not adopted by the government. The report also noted the government’s <a href="https://www.theguardian.com/technology/2022/oct/07/government-considers-centralising-digital-id-verification-on-mygov-in-wake-of-optus-breach" data-link-name="in body link">development of a digital ID</a> in the wake of the Optus and Medibank data breaches, but said it was not suggesting the government ID be used for confirming ages on pornographic websites.</p><p>The roadmap said there was also a gap in sex education, particularly for LGBTQ+ people, with online pornography filling the place in many cases.</p><p>“In our research, [participants who identified as lesbian, gay or bisexual] were also significantly more likely to say there were some positive effects of online pornography on young people learning about sex and exploring sexuality than straight participants. Some stakeholders reflected this may be due to a lack of other representations or learning sources for young LGBTIQ+ people – including a lack of inclusive sex education in schools.”</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-14">skip past newsletter promotion</a><p id="EmailSignup-skip-link-14" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>The government said it had funded $83.5m over six years for age-appropriate, evidence-based respectful relationships education.</p><p>While Pornhub remained the most popular pornographic site for Australian users, the report also noted that Australian-based industry bodies reported the local creators tended to be female or LGBTQ+, operating as sole traders.</p><p>“Between these two ends of the spectrum are a variety of businesses with different business models and levels of size, maturity, capacity, and capability to adopt technological measures to promote children’s safety. What constitutes appropriate steps for one provider might create an undue burden for another.”</p><p>Rowland said the roadmap underscored the value of the industry codes and developing standards to keep children safe.</p><p>“The government supports this approach, and will work with the regulator to ensure the full and successful implementation of the Online Safety Act,” she said.</p><p>“While the government awaits the outcome of this process, the digital industry is on notice that we will not hesitate to take further action should it fail to keep children safe.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Removes ‘Pirate’ URLs from Users’ Privately Saved Links (124 pts)]]></title>
            <link>https://torrentfreak.com/google-removes-pirate-sites-from-users-privately-saved-links-230830/</link>
            <guid>37329365</guid>
            <pubDate>Wed, 30 Aug 2023 21:26:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/google-removes-pirate-sites-from-users-privately-saved-links-230830/">https://torrentfreak.com/google-removes-pirate-sites-from-users-privately-saved-links-230830/</a>, See on <a href="https://news.ycombinator.com/item?id=37329365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Anti-Piracy category archives." href="https://torrentfreak.com/category/anti-piracy/"><span property="name">Anti-Piracy</span></a><meta property="position" content="2"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the DMCA category archives." href="https://torrentfreak.com/category/anti-piracy/dmca/"><span property="name">DMCA</span></a><meta property="position" content="3"></span> &gt; <span></span>
</p>
<p>
<span> </span>
To date, Google has processed more than seven billion copyright takedown requests for its search engine. The majority of the reported links are purged from Google's search index, as required by the DMCA. Recently, however, Google appears to gone a step further, using search takedowns to "moderate" users' privately saved links collections,
</p>
</div><div>
<p><img decoding="async" src="https://torrentfreak.com/images/googlesavedlogo.jpg" alt="googlesaved" width="300" height="110" srcset="https://torrentfreak.com/images/googlesavedlogo.jpg 322w, https://torrentfreak.com/images/googlesavedlogo-300x110.jpg 300w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20110'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/googlesavedlogo.jpg 322w, https://torrentfreak.com/images/googlesavedlogo-300x110.jpg 300w" data-lazy-src="https://torrentfreak.com/images/googlesavedlogo.jpg">For many people, Google is the go-to starting point when they need to find something on the web. With just a few keystrokes, the search engine can find virtually anything.</p>
<p>The company also has many other tools to browse and organize the web, including the Chrome browser and YouTube. </p>
<p>All these products and services fall under the umbrella of the company Alphabet. While the various departments are largely run separately, there is plenty of overlap too. This week, we stumbled upon information suggesting that DMCA notices, received for the search engine, directly impact people’s privately saved links. </p>
<p>As reported earlier this month, Google’s search index is a prime target for copyright holders. Over the past several years, more than <a href="https://torrentfreak.com/google-search-asked-to-remove-one-billion-pirate-links-in-9-months-230807/">seven billion ‘infringing’ URLs</a> have been flagged, with the majority removed. This makes sense, as Google is legally required to process DMCA takedown requests. </p>
<p>What comes as a surprise, however, is that the search takedown requests also impact other Google services. </p>
<h2>Search Takedowns Affect Saved URLs</h2>
<p>A few hours ago, Eddie Roosenmaallen <a href="https://strangeobject.space/@silvermoon82/110969122337810598">shared an email</a> from Google, notifying him that a link had been removed from his Google Saved collection because it violates Google’s policy. </p>
<p>The reason cited for the removal is the “downstream impact”, as the URL in question is “blocked by Google Search”.</p>
<p>“The following saved item in one of your collections was determined to violate Google’s policy. As a result, the item will be moderated..,” Google writes, pointing out a defunct KickassTorrents domain as the problem.</p>
<center><img decoding="async" src="https://torrentfreak.com/images/saved-google.jpg" alt="" width="600" height="566" srcset="https://torrentfreak.com/images/saved-google.jpg 1120w, https://torrentfreak.com/images/saved-google-300x283.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%20566'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/saved-google.jpg 1120w, https://torrentfreak.com/images/saved-google-300x283.jpg 300w" data-lazy-src="https://torrentfreak.com/images/saved-google.jpg"></center>
<p>Initially, it was suggested that this removal impacted Google’s synched Chrome bookmarks but further research reveals that’s not the case. Instead, the removals apply to Google’s <a href="https://support.google.com/websearch/answer/13128452?hl=en&amp;co=GENIE.Platform%3DAndroid">saved</a> feature. </p>
<p>This Google service allows users to <a href="https://www.google.com/save?authuser=0">save and organize</a> links, similar to what Pinterest does. These link collections can be private or shared with third parties. </p>
<h2>Bookmarks?</h2>
<p>The initial bookmark confusion is likely caused by the fact that, in Google’s app, the saved icon (shown below) appears by default. When clicked, the page in question is added to a “favorite pages” collection, which some people see as a bookmark. </p>
<center><img decoding="async" src="https://torrentfreak.com/images/google-saved.jpg" alt="google saved" width="400" height="265" srcset="https://torrentfreak.com/images/google-saved.jpg 750w, https://torrentfreak.com/images/google-saved-300x198.jpg 300w" sizes="(max-width: 400px) 100vw, 400px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20400%20265'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/google-saved.jpg 750w, https://torrentfreak.com/images/google-saved-300x198.jpg 300w" data-lazy-src="https://torrentfreak.com/images/google-saved.jpg"></center>
<p>Confusing terminology aside, what stands out here is that Google’s search content policy also applies to these saved links. As a result, URLs for which Google receives a search takedown, disappear from saved collections as well. This applies to both public and private collections. </p>
<center><img decoding="async" src="https://torrentfreak.com/images/search-saved.jpg" alt="" width="600" height="67" srcset="https://torrentfreak.com/images/search-saved.jpg 1111w, https://torrentfreak.com/images/search-saved-300x33.jpg 300w" sizes="(max-width: 600px) 100vw, 600px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20600%2067'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/search-saved.jpg 1111w, https://torrentfreak.com/images/search-saved-300x33.jpg 300w" data-lazy-src="https://torrentfreak.com/images/search-saved.jpg"></center>
<h2>DMCA’d URLS can’t be Saved</h2>
<p>TorrentFreak was able to replicate this issue. Google doesn’t allow us to ‘save’ URLs that are removed from Google search, such as YouTube ripper “Yout.com”, torrent site “1337x.to”, or the earlier mentioned “Katcr.com.”</p>
<p>These blockades apply to single URLs, not entire domains. For example, thepiratebay.org is still visible in Google searches and can be added to a collection. However, Pirate Bay links that are deindexed, <a href="https://thepiratebay.org/search.php?q=top100:48h_201">such as this one</a>, can’t be saved.</p>
<p>The same applies to other sites. The old homepage of YouTube ripper 2conv.com can’t be saved since it’s been removed from Google search, but the latest homepage URL (2conv.com/neshq) can still be added. </p>
<p>It’s not clear why Google enforces the search policy for saved links or whether preventing copyright infringement is the main goal. The company didn’t immediately respond to our request for comment. If we hear back, this article will be updated accordingly.</p>
<p>For now, the impact is relatively limited as the saved feature isn’t widely used. However, if Google decides to “moderate” users’ Chrome bookmarks, or its DNS resolver, things could get interesting. </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why do shared hospital rooms not violate HIPAA? (157 pts)]]></title>
            <link>https://law.stackexchange.com/questions/28878/how-is-it-legal-for-a-hospital-to-put-two-patients-together-in-the-same-room-in</link>
            <guid>37329261</guid>
            <pubDate>Wed, 30 Aug 2023 21:15:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://law.stackexchange.com/questions/28878/how-is-it-legal-for-a-hospital-to-put-two-patients-together-in-the-same-room-in">https://law.stackexchange.com/questions/28878/how-is-it-legal-for-a-hospital-to-put-two-patients-together-in-the-same-room-in</a>, See on <a href="https://news.ycombinator.com/item?id=37329261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>As a short answer, <a href="https://www.hhs.gov/hipaa/for-professionals/faq/197/must-facilities-have-private-or-soundproof-rooms/index.html" rel="noreferrer">guidance from the Department of Health and Human Services</a> has clarified that HIPAA does not require hospitals to provide separate rooms.</p>

<p>As a longer answer, HIPAA is very deeply misunderstood. It does not prohibit "leaking" patient information; it prohibits unreasonable and unpermitted disclosures of protected health information (PHI).</p>

<p>Among the PHI disclosures that are permitted are uses that are <a href="https://www.law.cornell.edu/cfr/text/45/164.506" rel="noreferrer">for the purpose of delivering medical treatment</a>. Of course, the covered entity (in this case, the hospital) is required to take reasonable measures to safeguard that information.</p>

<p>One of the areas that trips people up is figuring out exactly what it is we're safeguarding that information from. A lot people assume that the HIPAA imposes an absolute rule against disclosure of PHI, but it's more accurate to say that HIPAA requires reasonable safeguards against the unauthorized use of PHI.</p>

<p>With that standard in mind, it becomes easier to see why you don't need to universally separate patients. In all likelihood, neither you nor your roommate is likely to use the other's PHI in any way not allowed under HIPAA. We can look at your question as proof: You've disclosed a person's health condition and medical history, but you were a reasonable person and omitted the man's name, birth date, record number, and anything else that might allow us to link that information to an individual.</p>

<p>Hospitals -- and the law -- recognize that most people have no interest in a random strangers' medical information, let alone plans to do something nefarious with it. Because there isn't much of a threat there, the hospital isn't required to take exhaustive measures to protect the information. But when you put all that information for every patient for every doctor for every department for every hospital into a single database, the information starts getting a lot more valuable. That's why there are much tighter regulations surrounding protection of electronic records.</p>

<p>Of course, the roommate situation might be different if the hospital had a patient that they somehow knew had a history of identity theft or even a history of disclosing PHI. I've never heard of this happening, but I'd imagine that that knowledge would require the hospital to either segregate that patient or otherwise take extra care to avoid disclosing any information about a roommate.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[At Taser maker Axon, ex-staffers say loyalty meant being tased or tattooed (123 pts)]]></title>
            <link>https://www.reuters.com/investigates/special-report/axon-taser-exposures/</link>
            <guid>37328786</guid>
            <pubDate>Wed, 30 Aug 2023 20:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/investigates/special-report/axon-taser-exposures/">https://www.reuters.com/investigates/special-report/axon-taser-exposures/</a>, See on <a href="https://news.ycombinator.com/item?id=37328786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure id="VVGXJ2DQIT_0" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption"></figcaption>
    </figure>
<p id="paragraph-1">SCOTTSDALE, Arizona</p>
<p id="paragraph-2">The employee let out a guttural scream as the darts struck his back. Two men lowered his stiffened body to the floor.</p>
<p id="paragraph-3">A crowd of co-workers howled with laughter and cheered as the account executive, still prostrate, managed to praise the weapon that felled him – a Taser electroshock gun manufactured by his company, Axon Enterprise Inc.</p>
<p id="paragraph-4">“Oh, that’s good,” the Axon employee, Ross Blank, heaved into a microphone.</p>
<p id="paragraph-5">Blank later posted <a href="https://www.linkedin.com/posts/ross-blank-8a1a98199_thisisaxon-activity-7023763957232386048-FuC6/" id="article-link-0">a video</a> on his LinkedIn account of the company’s January event, held at a Phoenix resort. “Taking one more for the team is Bad Assery,” long-time executive Steve Tuttle commented on the post. “Love it!”</p>

    
<p id="paragraph-7">Axon, a corporation with a market capitalization of $15 billion, has a dominant position in its niche of electroshock weapons and body camera technology for law enforcement. It describes its mission as a noble one: saving lives. Its best-known product is the Taser, the device it developed to temporarily immobilize criminal suspects with darts that deliver electric current, providing police with an alternative to firearms. Axon&nbsp;says that more than 18,000 law enforcement agencies in 107 countries use Tasers.</p>
<p id="paragraph-8">Less well-known is the all-in corporate culture at Axon, which has tested employees’ commitment and fealty in unusual ways – through measures that some embraced wholeheartedly but others felt were extreme and potentially dangerous.</p>
<p id="paragraph-9">Shawn Gorman, a lawyer who worked at Axon until 2019, said the company had a high-pressure culture of loyalty, unlike anything he has seen in nearly two decades of practice.</p>
<p id="paragraph-10">“It was truly toxic,” he said.</p>
<p id="paragraph-11">The tasing exercise struck several workplace experts consulted by Reuters as an outlandish and unnecessary hazard. It’s “unhealthy at best, dangerous at worst,” said Jennifer Chatman, a professor studying workplace culture at the Haas School of Business at the University of California, Berkeley. “This is on the fringe for sure.”</p>
<p id="paragraph-12">In statements to Reuters, Axon disputed that staff tasings are hazardous. The company and its chief executive officer, Rick Smith, said employees are not pressured into anything.</p>
<p id="paragraph-13">“We strongly object to any implication that Axon pressures employees to engage in activities against their will,” said Andrea James, the company’s chief communications officer.</p>

    <blockquote id="UJWR4XEIH7_2">
        <p>“It was truly toxic.”</p>
        
    </blockquote>
<p id="paragraph-15">James defended Axon’s culture, describing it as “a collaborative environment of mission-driven individuals who join forces to deliver an extraordinarily profound impact on society.”</p>
<p id="paragraph-16">Axon said that staff tasings have never drawn formal complaints and are conducted with “the utmost focus on both psychological and physical safety.” Blank said he never felt pressured by Axon. Tuttle did not respond to requests for comment.</p>
<p id="paragraph-17">Staff tasings, known as “exposures,” are corporate rituals at Axon. Sometimes they involve mid-career employees like Blank, but often they are used to initiate interns or new recruits into Axon’s all-in culture, according to numerous interviews and videos seen by Reuters. <span id="tasetase"><em id="BZQ6CIYI60_4"></em> “Tase, Tase, Tase,” staffers chanted in unison</span>&nbsp;in a March 2019 recording as a target stood in the line of fire.</p>
<p id="paragraph-18">“It looks like a scene from ancient Rome, gladiator-style,” said Valencia Gibson, a former international support manager at Axon. She was one of several former staffers to compare tasings, frequently held in the tiered atrium at Axon’s Scottsdale headquarters, to the Colosseum’s spectacles. Gibson said there was pressure in general for staff to be tased but that she was able to decline due to pregnancy.</p>

    <figure id="JNZEFIF37Q_7" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">Axon directed staff attending a January 2022 conference to a foyer where they could obtain company-branded tattoos, a screenshot of a&nbsp;message to employees shows. Markings and timestamp removed by Reuters for source protection</figcaption>
    </figure>
<p id="paragraph-20">Besides hosting Taser exposures, company leaders have urged staff to “make things permanent” by getting tattoos of Axon logos on their bodies and to bet part of their pay on a stock plan most advantageous to ensconced, highly placed executives, according to interviews and Axon materials reviewed by Reuters.</p>
<p id="paragraph-21">Axon’s leaders want to know “who is in for the long haul,” a senior employee told a co-worker considering the stock plan in 2018 text messages that were reviewed by Reuters. The senior employee, who was helping to implement the plan, added that such tests of loyalty are&nbsp;“totally wrong but it is what it is.”</p>
<p id="paragraph-22">For this story, Reuters reviewed approximately 100 Axon records, including PowerPoint presentations, internal announcements, meeting notes and text messages, as well as 24 exposure videos made either by Axon or unofficially by staffers from 2016 through 2023. Some videos portray multiple individuals being tased.</p>
<p id="paragraph-23">Reuters also interviewed 63 current and former Axon employees, including nine former executives. Most people interviewed requested anonymity, saying they feared harm to their careers or had signed non-disclosure agreements at Axon, sometimes to obtain severance pay. Axon employed about 2,800 full-time staff at the end of 2022, according to its latest annual report.</p>
<p id="paragraph-24">Many ex-employees interviewed by Reuters described Axon as a boys’ club that was unwelcoming or even offensive to women. One HR staffer’s 2019 PowerPoint presentation to an executive on inclusion efforts – reviewed by Reuters – specifically criticized Axon’s “‘Bro’ culture” and “lack of diversity in top leadership.”</p>
<p id="paragraph-25">Men historically have dominated the upper ranks, with 129 in management and financial roles compared with 46 women, according to 2020 data Axon submitted in a public procurement.</p>
<p id="paragraph-26">A dozen people told Reuters that leaders sometimes used obscene or sexual terms for women or discriminated against them. Josh Isner, now Axon’s second in command, referred to one of the company’s few female executives as the “Vagina,” according to several people who either heard the slur or were told about it soon afterward, as well as text exchanges alluding to the comments.</p>
<p id="paragraph-27">Axon said Isner made no such remark. Isner said in a statement, “The defamatory language attributed to me by a disgruntled former employee is emotionally motivated and a complete fabrication.”</p>
<p id="paragraph-28">The company has “zero tolerance” for workplace discrimination or harassment and objects to “inflammatory allegations regarding our environment as it relates to women,” James said.</p>
<p id="paragraph-29">“Axon supports and celebrates a gender-diverse workforce,” she said, adding that 40% of the highest-level executive team is female, and that the company has promoted men and women equally this year.</p>

    <figure id="video-title" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">Executive Josh Isner teased an employee before his first tasing,&nbsp;as shown in this excerpt from a promotional video Axon posted on its website in 2019. “Big tree fall hard,” Isner said before current flowed through the bellowing worker.</figcaption>
    </figure>
<p id="paragraph-31">Fifteen people, about half of whom oversaw human-resources and legal work for Axon, said they felt company leaders sidelined those who didn’t show sufficient loyalty or commitment to the all-in credo – by, for instance, declining the stock program or failing to be tased. They said they believed those employees received less desirable assignments, were excluded from business meetings, or, in some cases, were forced from their jobs.</p>
<p id="paragraph-32">Not everyone objected to the company’s distinctive culture. About a third of those interviewed by Reuters said they felt no pressure to undergo exposures or eagerly signed up.</p>
<p id="paragraph-33">James said Reuters glossed over positive employee reviews, while relying on sources that were “not credible.” She pointed to a confidential, company-funded survey from June in which 87% of some 700 U.S.-based staffers rated Axon as “a great place to work.” Claims that employees felt sidelined are “not rooted in fact,” Axon said. Reuters was unable to obtain a copy of the full survey results.</p>
<p id="paragraph-34">The apparent light-heartedness and the zaniness of the tasing exercise – described previously in a <a href="https://www.newyorker.com/magazine/2018/08/27/can-the-manufacturer-of-tasers-provide-the-answer-to-police-abuse" id="article-link-1">2018 story in The New Yorker</a> about Axon’s role in the police body-camera business – might suggest that staff exposures at most induce temporary pain or discomfort.</p>

    
<p id="paragraph-36">Tasings, however, can be fatal.</p>
<p id="paragraph-37">More than 1,000 people have died in the United States following incidents in which police used Axon’s Tasers,&nbsp;often along with other types of force, <a href="https://www.reuters.com/investigates/special-report/usa-taser-911" id="article-link-2">a Reuters examination in 2017 found</a>.&nbsp;No one with whom Reuters spoke was aware of deaths or lawsuits stemming from tasings of Axon staff. Over the years, however, Axon has faced over 120 external lawsuits alleging wrongful death&nbsp;after the weapon’s use, according to <a href="https://www.reuters.com/investigates/special-report/usa-taser-legal/" id="article-link-3">Reuters reporting</a>.</p>
<p id="paragraph-38">Axon keeps legal settlements confidential. But the company said in February in its annual report filed with the U.S. Securities and Exchange Commission that no product claim after 2014 has exceeded its $5 million in self-insurance. Axon has faced fewer lawsuits since 2009, the year it introduced a new Taser model with a lower charge. In the wake of a study showing Taser pulses could interfere with the <a href="https://www.reuters.com/investigates/special-report/usa-taser-science/" id="article-link-4">rhythms of the human heart</a>, the company that year revised its recommendations on where to aim the device. Previously, users were told to aim at the torso; since 2009, Axon has encouraged users to shoot for the back or below the chest.</p>

    <blockquote id="GBX9CZRNM6_10">
        <p>“I saw people cry real bad.”</p>
        
    </blockquote>
<p id="paragraph-40">Videos of employee tasings&nbsp;seen by Reuters show that darts were aimed at the back and thigh area under highly controlled conditions, in which subjects wore safety glasses and spotters clutched their arms as they fell to a mat. Even so, workers have fainted, cried, bled or suffered other pain, witnesses or former human-resources staff said.</p>

    <figure id="video-title" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">WARNING: GRAPHIC CONTENT – An employee and her mother&nbsp;writhe during a tasing at the company’s Scottsdale headquarters in a promotional video the Taser maker published on its website in 2019.</figcaption>
    </figure>
<p id="paragraph-42">“I saw people cry real bad,” said one long-time employee who has since left the company.</p>
<p id="paragraph-43">Ann Rosenthal, a senior advisor to the U.S. Occupational Safety and Health Administration (OSHA) until 2022, said Axon’s tasing of staff&nbsp;could violate a provision of the federal Occupational Safety and Health Act, which mandates that workplaces be free of recognized hazards that are “likely to cause death or serious physical harm.”</p>
<p id="paragraph-44">An OSHA spokesperson said the administration has no ongoing investigations of Axon, nor any findings from completed investigations. It declined further comment on Axon. The main commission overseeing workplace safety in Arizona, where Axon is based, declined to comment.</p>
<p id="paragraph-45">The company told Reuters that it objected to any suggestion that its workplace practices “are illegal or dangerous.”</p>
<p id="paragraph-46">In 2022, Axon published <a href="https://www.axon.com/news/technology/truth-about-taser" id="article-link-5">a blog post</a> aimed at “debunking some of the most common myths” about its weapons. There, the company argues that its Tasers are “one of the most studied, safe and effective means of quickly stopping a threat,” causing muscles to flex by employing “the same technology you see in off-the-shelf muscle stimulators used for rehab and muscle therapy.”</p>
<p id="paragraph-47">Axon itself has issued <a href="https://www.reuters.com/investigates/special-report/usa-taser-warning/" id="article-link-6">multiple pages of safety warnings</a> to police about the risks of using the technology. Even in the controlled setting of the Axon workplace, the company requires exposure candidates to sign a waiver, forfeiting their right to sue Axon for any exposure-related injury.</p>

    <figure id="T4SPE8D9VW_12" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">Axon requires employees who participate in tasings to sign a liability waiver that repeatedly cites the risk of death.&nbsp;Highlights added by Reuters</figcaption>
    </figure>
<p id="paragraph-49">The waiver warns of the potential for death at least 12 times, according to a 2019 copy. Repeated electrical stimuli “can induce seizure in some people, which may result in death or serious injury,” the waiver states.</p>
<p id="paragraph-50">“Certainly this is a hazard, and they’re clearly aware it’s a hazard because they’re warning employees that they could get killed,” said Rosenthal, who until 2018 was the Labor Department’s top lawyer on workplace safety.</p>
<p id="paragraph-51">She and two other experts said the pressure workers describe is similar to that experienced by initiates of&nbsp;gangs or some fraternities.</p>
<p id="paragraph-52">“It sounds like a hazing process as much as anything else,” Rosenthal said.</p>
<p id="paragraph-53">Sigma Chi brothers</p>
<p id="paragraph-54">While a rising sophomore at Harvard in the 1980s, Rick Smith, the future Axon CEO, attended a fraternity rush week at a friend’s school.</p>
<p id="paragraph-55">“There was something about being in the company of men, and men alone, that enabled us to speak openly and honestly about what was on our minds,” <a href="https://observer.com/2016/05/how-harvard-administrators-went-wrong-on-single-gender-groups/" id="article-link-7"></a><a href="https://observer.com/2016/05/how-harvard-administrators-went-wrong-on-single-gender-groups/" id="article-link-8">he wrote years later</a> in an op-ed for the news site Observer.com.</p>
<p id="paragraph-56">In 1989, Smith wrote, he started a chapter of the Sigma Chi fraternity at Harvard despite the university’s refusal – both then and now – to recognize such single-gender groups on the grounds that they are discriminatory.</p>
<p id="paragraph-57">The fraternity fostered what <a href="https://www.bostonmagazine.com/education/2018/03/13/harvard-university-broken/" id="article-link-9">Smith described in Boston Magazine</a> as “lasting friendships.” As of February, three of the company’s eight board members, including Smith, were alumni of Harvard’s Sigma Chi chapter. Since then, three with no Sigma Chi association have been added, bringing the board to 11.</p>
<p id="paragraph-58">Axon President Josh Isner, who is not a board member, also is a former member of the Harvard fraternity.</p>
<p id="paragraph-59">The company said Smith’s articles had no bearing on Axon.</p>
<p id="paragraph-60">“My comments on the value of private spaces for men and women to adjust to college life is very separate from how I view diversity in the workplace,” Smith said. “I learn from people who are different from me, who bring different life experiences, and I am proud of our investments to build a diverse team.”</p>
<p id="paragraph-61">Smith said Isner’s fraternity affiliation was not a factor in Isner’s position at the company and described him as “an instrumental leader at Axon for more than a decade.”</p>
<p id="paragraph-62">Michael Church, Sigma Chi’s executive director, said that “throughout the years Rick Smith has demonstrated a thorough appreciation for the benefits of a fraternal experience.” He directed further questions to Axon.</p>
<p id="paragraph-63">From the time Smith co-founded the business with his brother in 1993, he put his body on the line. A recording from that year, posted on the company website, shows Smith standing in an inflatable pool of water and taking a hit from an early Taser. Smith falls down and grunts, with his arms and legs flailing for about 10 seconds.</p>

<p id="paragraph-68">Screenshots&nbsp;from an Axon promotional video show CEO Rick Smith taking a Taser hit&nbsp;in 1993, the year&nbsp;he co-founded the business.</p>
<p id="paragraph-69">Around 1998, U.S. Marine Corps veteran Hans Marrero joined the company, helping to attract law enforcement customers. An old video <a href="https://www.youtube.com/watch?v=7xfI6-Pkwbw" id="article-link-10">on YouTube</a>, which Axon replayed for police at an <a href="https://youtu.be/ju42HtqDz50?t=708" id="article-link-11">event this year</a>, billed Marrero as “one of the toughest men alive” and displayed him handily resisting punches and chokeholds.</p>
<p id="paragraph-70">He “can take pain,” a description of the video on YouTube reads, “but can he handle a taser?”</p>
<p id="paragraph-71">After collapsing from the weapon, <a href="https://www.youtube.com/watch?v=7xfI6-Pkwbw" id="article-link-12"></a> Marrero says the Taser had a greater effect than a grenade that once hit him: It “knocked me on my ass,” making him “lay there, like a little baby.”</p>

    
<p id="paragraph-73">Marrero toured the United States in a camper van “demonstrating” the Taser, Smith said in a separate promotional video for Axon. The demos around 1999 marked a turning point in sales to police, Smith <a href="https://vimeo.com/337807714" id="article-link-13">has said</a>, and set up the company to earn $8.4 million from an initial public offering in 2001.</p>
<p id="paragraph-74">Asked to comment in a brief phone call, Marrero said, “I’m not interested, buddy.”</p>
<p id="paragraph-75">Smith, now 53, set a macho tone in the workplace, interviews and records show.&nbsp;For instance, Smith at times mocked male employees by calling them a “pussy,” according to two written exchanges seen by Reuters. In a meeting held in recent years&nbsp;with more than a dozen employees including women, he used the same term to refer to a male staffer who wouldn’t do as he asked, an attendee said.</p>
<p id="paragraph-76">According to the 2019 presentation by a human resources staffer, Smith stated his concern in a company meeting that diversity and inclusion efforts meant that Axon “won’t know how to ‘have fun.’”</p>
<p id="paragraph-77">Smith and other executives and staffers took excursions to strip clubs, six former employees said and text messages showed. Some said police joined on occasion, and staff who did not attend said they felt excluded.</p>
<p id="paragraph-78">In Las Vegas, Smith traveled around with executives in a rented stretched truck known as “The Beast,” according to a photo and caption posted on the limousine company’s social media account in May 2022. The burnt-orange-colored vehicle, examined by a Reuters reporter, has a stripper pole inside it.</p>
<div id="beast-duo">

    <figure id="3QI3XLGF4V_19" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">Axon CEO Rick Smith and fellow executives arrive via private jet in Las Vegas and travel around in a rented stretched truck known as “The Beast.” The photo was posted by the limo company on&nbsp;its Facebook page in May 2022.</figcaption>
    </figure>

    <figure id="15BWMLT8Q4_20" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">“The Beast” limo has a stripper pole inside and seats that fold up to create luggage space or a dance floor. REUTERS/Jeffrey Dastin</figcaption>
    </figure>
</div>
<p id="paragraph-83">In statements to Reuters, the company drew a distinction between employees’ personal and professional undertakings. “We do not take customers or employees to strip clubs for any company events,” Axon said. “What people choose to do on their own personal time is something we respect their privacy to do. We have firm expectations that nothing inappropriate comes into the workplace either directly or indirectly.”</p>
<p id="paragraph-84">Both male and female staffers said taking time for family was discouraged or penalized at Axon. Three women and two men said in interviews that Axon forced them out after they sought or took parental leave in recent years. One of them, Valencia Gibson, said she received a layoff notice in 2017 a few months before she had a baby.</p>
<p id="paragraph-85">“It’s not morally, ethically right,” Gibson said, adding that she did not pursue legal action because she didn’t want to put herself through the stress.</p>
<p id="paragraph-86">One of the men, however, filed an Equal Employment Opportunity complaint in 2022, seen by Reuters, which he said is pending. Among other things, the person alleged sexism figured in Axon’s decision to terminate him after he filed a family-leave request to bond with his newborn.</p>
<p id="paragraph-87">In its statements to Reuters, Axon disputed the Equal Employment Opportunity claim, adding that more than 100 employees have benefited from its parental leave policies this year. The Equal Employment Opportunity Commission declined to comment, citing federal law that barred it from releasing information on such complaints.</p>
<p id="paragraph-88">Axon <a href="https://filecache.investorroom.com/mr5ir_axon/256/AXON_ESGReport_Feb2021.pdf" id="article-link-14">described</a> its leave policies – up to 20 weeks paid for women and 10 weeks for men – as “family-friendly” and “industry-leading” in a 2021 corporate responsibility report. In contracts including a <a href="https://clkrep.lacity.org/onlinecontracts/2022/C-140744_C_07-01-22.pdf" id="article-link-15">recent $7 million deal</a> with Los Angeles for police-car cameras, it agreed not to discriminate against any person on the basis of factors such as gender, pregnancy or childbirth.</p>
<p id="paragraph-89">Smith said he “deeply” embraces diversity at Axon, noting the company wants a balance between “professionalism and personality” while believing “everyone should bring their full selves to work every day and feel welcomed as unique human beings.”</p>
<div id="V16NPGKNRC_22">
<div id="4DN9CP56I0_23">

    <figure id="video-title" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">“You’re a man’s kind of man,” shouted a member of an uproarious crowd at Axon’s Scottsdale offices in a 2018 tasing video of an intern.</figcaption>
    </figure>
</div>
<div id="ZH6EN75M1B_26">

    <figure id="video-title" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">Summer 2022 intern Rylan Bennigson posted this video on LinkedIn of his tasing, an exercise he described to Reuters as “100% voluntary” and “a celebration of what we had done in the internship.”</figcaption>
    </figure>
</div>
<div id="K41UVH3GYH_29">

    <figure id="video-title" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">An employee&nbsp;lets out a cry as she’s tased months after her start at Axon. The employee, who&nbsp;posted&nbsp;the video on LinkedIn in 2022, declined to comment.</figcaption>
    </figure>
</div>
</div>
<p id="paragraph-101">‘Young and impressionable’</p>
<p id="paragraph-102">To be tased, in the Axon lexicon, is to “ride the lightning.”</p>
<p id="paragraph-103">Six former employees including two executives told Reuters that Axon tased&nbsp;staff strategically around new-hire orientations – a contention that Axon called “false.”</p>
<p id="paragraph-104">In a 2019 workplace survey seen by Reuters, exposures were described as “Intern Cohort Engagement Activities.” An intern’s tasing in 2018 elicited uproarious screams from his peers, a video showed. “You’re a man’s kind of man!” one person shouted.</p>

    <figure id="MV4LEES9F8_33" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">A screenshot of an Axon workplace survey from 2019 shows the company considered tasings, otherwise known as “exposures,” as one of its “Intern Cohort Engagement Activities.”</figcaption>
    </figure>
<p id="paragraph-106">“I felt like there definitely was a lot of pressure there,” a different former intern told Reuters, comparing the experience to fraternity rush. “You’re part of the team or the brotherhood if you did it.” He said he could not breathe when he was shot, and he had to leave work early to recover. Every time he heard a Taser discharge “from that point on, I’d kind of jump a little bit in my chair.”</p>
<p id="paragraph-107">Around 2009, the company began hiring cohorts of around 15 college graduates to join a “leadership development program,” a two-year paid role that gave them a chance at a longer-term job.</p>
<p id="paragraph-108">A former employee said she understood the focus on initiating young people this way: “One, they’re young and impressionable, and two, start them early.”</p>
<p id="paragraph-109">“Everybody tells their (tasing) stories,” said the person, who herself took a hit from the darts. “They try to make you think it’s cool.”</p>

    <blockquote id="R8WMHWJE01_34">
        <p>“It looks like a scene from ancient Rome, gladiator-style.”</p>
        
    </blockquote>
<p id="paragraph-111">In an Axon video taken&nbsp;at a January 2022 company conference, a new employee states on camera while awaiting a tasing, “This is day three of the job, so we’ll see how it goes.” The footage shows the staffer, clad in a superhero outfit from the animated film series “The Incredibles,” squinting and smiling as the darts hit her back.</p>
<p id="paragraph-112">Others took exposures shirtless or in a sports bra, to avoid having their clothes punctured by darts or stained with blood, said workers, including former intern Keara Berlin.</p>
<p id="paragraph-113">“It just seems like if the goal really is for yourself to experience the product that you’re working on that it should be as safe as possible and private,” not “in a big group of people and taking your shirt off,” said Berlin, now 24. She said the majority of her 2019 cohort was tased&nbsp;in one session lasting at least 45 minutes, although she said she had no interest and told colleagues as an excuse that she was not appropriately dressed.</p>
<p id="paragraph-114">Some interns and young staffers told Reuters they wanted to be tased to satisfy their curiosity or gain confidence in the product. Some said they appreciated the camaraderie. They also liked the mementos for stepping up to the challenge: commemorative coins adorned with Axon insignia. Similar coins are given to military personnel or police officers for service achievements.</p>

    
<p id="paragraph-116">Rylan Bennigson, an intern last summer, said his tasing&nbsp;was “100% voluntary.” He called it “a personal decision and a celebration of what we had done in the internship.”</p>
<p id="paragraph-117">Since at least 2005, the company has required exposure volunteers to waive their rights to sue, copies of these forms archived from Axon’s old website show. By 2011, Axon began to warn more prominently in the waivers that the weapons “Can cause death or serious injury,” the forms show.</p>
<p id="paragraph-118">Axon said the waiver aimed “to truly ensure that our employees know and understand their risks and consider them prior to proceeding.” Employees have no obligation to undergo tasing for career advancement, Axon said, and doing so can be in private if they so choose.</p>
<p id="paragraph-119">Smith said in the 2018 New Yorker piece that he no longer watched exposures, because, to him, they were like smelling tequila after a hangover. He told Reuters in his statement that tasings are “unpleasant.”&nbsp;But he defended the practice, saying that most of the company’s law enforcement customers require or offer their officers tasing opportunities as a way to develop “deeper understanding of the technology, as well as an empathy for people on the receiving end.”</p>
<p id="paragraph-120">“We offer this same experience to our employees,” Smith said.</p>
<p id="paragraph-121">Reuters confirmed that some police agencies, including those in Sacramento and San Jose, California, do offer the option. The Los Angeles Police Department requires a Taser-like experience, with exceptions for those with medical risks. The department attaches wires to officers rather than firing the weapon’s darts into their backs, avoiding treatment costs and risks associated with having an open wound, a captain told Reuters.</p>
<p id="paragraph-122">In his statement to Reuters, Smith said: “There is zero pressure for anyone to participate&nbsp;(in tasings)&nbsp;and we make significant effort to ensure people understand they are not expected to do it. Most of our board and many of our most senior executives have chosen not to experience Taser devices.”</p>
<p id="paragraph-123">Axon said employee exposures occurred just a few times a year. They are “part of Axon’s culture because we see the importance in standing behind and in front of our products.”</p>
<p id="paragraph-124">Loyalty etched in ink</p>
<p id="paragraph-125">In October 2021, in a tweet, CEO Smith flexed to display an Axon logo and lightning bolts tattooed on his bicep, celebrating stock options he earned.</p>
<p id="paragraph-126">“Wow,” former chief of staff Mihir Shah commented. “All in!!!”</p>
<p id="paragraph-127">Shah declined to comment and deleted his tweet during a phone call with Reuters in July.</p>
<p id="paragraph-128">Smith and other executives in recent years got inked during excursions to Starlight Tattoo at the Mandalay Bay Resort and Casino in Las Vegas, according to Smith’s public statements, employee text messages and three former staffers.</p>

    <figure id="WZ5ZG7TF6Q_36" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">CEO Smith displays the fresh ink on his bicep to mark the Axon stock options he earned. The photos were taken at Starlight Tattoo in Las Vegas and posted on his Twitter account in October 2021.</figcaption>
    </figure>
<p id="paragraph-130">Starlight’s owner, Mario Barth, said he was traveling and could not immediately comment.</p>
<p id="paragraph-131">After getting Axon-themed tattoos, some executives prodded their colleagues to follow suit, said four former employees. Three said&nbsp;they personally resisted getting tattoos despite repeated, sometimes discomfiting entreaties from company leaders.</p>
<p id="paragraph-132">“The execs who got tattoos were considered to be the ones who were all in,” said one of the former employees. “It was like being branded.”</p>
<p id="paragraph-133">“If you didn’t get it – you were questioned,” the person said.</p>
<p id="paragraph-134">Axon invited tattoo artists to internal conferences this year and last, company videos show. “Willing to make things permanent?” an internal company message asked employees. The message directed staff to an on-site artist in a foyer in a Scottsdale conference center.</p>
<p id="paragraph-135">Two of the former employees said women were encouraged to obtain tattoos after company officials expressed concern about only having male employees with Axon ink.</p>
<p id="paragraph-136">“They kept asking some of the girls to get tattoos,” one said. “I was like, no thanks. It’s going to fuel the boys-club fire.”</p>
<p id="paragraph-137">Axon said the vast majority of staff had no such ink and disputed that employees felt pressure to receive tattoos. Smith called the assertion “simply ridiculous,” adding, “Tattoos have never come up in hiring discussions nor promotion discussions, and I would come down hard on anyone who tried to make an issue of it.”</p>
<p id="paragraph-138">Some staffers embraced the offer. One video produced by Axon, posted on its website in November 2022, showed a sales director at a company conference getting a Taser tattoo, a circle with a lightning bolt at the center, on his bicep.</p>
<p id="paragraph-139">The sales director told Reuters he was inspired by Axon’s goal of cutting officer-involved shootings and signed up with as many as 40 of his peers.</p>

<p id="paragraph-144">Axon invited tattoo artists to a Phoenix conference it hosted this year to ink corporate insignia on willing employees, as shown in&nbsp;screenshots from a January video the company posted on YouTube.</p>
<p id="paragraph-145">The tattoo was a “funny, big cultural thing,” said the person, who declined to be named, saying he did not want to be associated with an article critical of Axon. “I do not regret it even a little bit.”</p>
<p id="paragraph-146">“I am proud of my tattoo,” Axon’s Chief Legal Officer Isaiah Fields said in a statement to Reuters. “It symbolizes the small part I have played in Axon’s mission and the positive impact we have made on the world.”</p>
<p id="paragraph-147">The stock plan was another measure of employee commitment – and it led to angst and regrets among some senior staff.</p>
<p id="paragraph-148">Around 2018, following a large stock award to Smith, investors told Axon they wanted to see broader employee buy-in to a stock program instead of “highly concentrated plans among senior management,” the company said in securities filings.</p>
<p id="paragraph-149">More than 500 U.S. staffers with salaries of at least $100,000 were asked to suspend from 5% to as much as half their pay, <a href="https://app.quotemedia.com/data/downloadFiling?webmasterId=101533&amp;ref=112527642&amp;type=PDF&amp;symbol=AXON&amp;companyName=Axon+Enterprise+Inc.&amp;formType=DEFA14A&amp;dateFiled=2018-12-21&amp;CK=1069183" id="article-link-16">according to a 2018 securities filing</a>. In return, over nine years, employees could get up to triple that amount in chunks of stock each time the board certified that Axon had&nbsp;met rigorous financial thresholds.</p>
<p id="paragraph-150">If Axon did not reach any targets by the time an employee quit or was fired, the staffer would get nothing. Those dismissed without cause would earn no stock or a capped amount.</p>
<p id="paragraph-151">Axon did not specifically comment on the stock plan for this story, other than to point out that the majority of qualifying employees were capped at committing up to 10% of their earnings.</p>
<p id="paragraph-152">In a 2019 press release, Axon trumpeted that “more than 300 employees” agreed to allocate $75 million of their compensation to the plan, choosing “to align their pay directly with value creation for shareholders.”</p>
<p id="paragraph-153">This May, Smith told investors on a Zoom video call that “nearly 100” ultimately became millionaires from the program. He rolled up his sleeve to show a tattoo to mark milestones in the form of filled-in bars.</p>
<p id="paragraph-154">“I have my financial scorecard proudly on my body,” he said. He said around a dozen other employees got the same tattoo.</p>
<p id="paragraph-155">The biggest winners by far from the plan, which was reviewed by Reuters, have been top-level executives. Isner alone reaped more than $73 million in vested stock resulting from the targets&nbsp;Axon hit, <a href="https://www.sec.gov/Archives/edgar/data/1069183/000155837022005230/tmb-20220520xdef14a.htm" id="article-link-17">a securities filing</a> last year shows.</p>

    
<p id="paragraph-157">Isner did not respond to a request for comment on the plan.</p>
<p id="paragraph-158">Six former employees told Reuters they had lost between several hundred dollars and tens of thousands of dollars, although one of them was able to recoup the money upon exiting the company.</p>
<p id="paragraph-159">Wayne Guay, an expert on stock compensation at the Wharton School of the University of Pennsylvania, said the plan was aggressive in its targets and covered an unusually long period – more risky for younger, often highly mobile employees.</p>
<p id="paragraph-160">“The founder is going to be there til the end for sure, but rank-and-file employees?” said Guay. “I’m just not sure.”</p>
<p id="paragraph-161">In 2018 messages to an Axon official involved in the stock plan, one young employee expressed misgivings. The official told the staffer to “take a chance” on the plan or risk falling behind “with your pants down.”</p>
<p id="paragraph-162">A different lower-level executive told Reuters he acceded to&nbsp;what he felt was “immense pressure” to sign up.</p>
<p id="paragraph-163">“I knew that the people that I reported to would see whether I was committing to be a company man for the next nine years,” said the executive, who has since departed Axon.</p>
<p id="paragraph-164">Shot 62 times</p>
<p id="paragraph-165">Axon’s all-in culture wasn’t just aimed at eager interns and ambitious young executives.</p>
<p id="paragraph-166">In a December 2019 video created by Axon, then-training manager Lamar Cousins claimed that he had been tased around 62 times. Cousins did not respond to emails requesting comment.</p>

    <figure id="PKEDVMB4D7_42" itemscope="" itemtype="http://schema.org/ImageObject">
        
        <figcaption itemprop="caption">A message sent to employees announcing a staff tasing event. Markings and timestamp removed by Reuters for source protection</figcaption>
    </figure>
<p id="paragraph-168">A company-wide announcement summoned employees to a ballroom during an internal conference at the Scottsdale Westin resort in January 2022. “Exposures are happening AGAIN in (room) Herberger 3AB! Grab front&nbsp;row to see your fellow employees take a ride!” it read.</p>
<p id="paragraph-169">Videos show 50 or more people of varying ages rallying around to watch spectacles combining elements of playfulness and provocation.</p>
<p id="paragraph-170">High-level executives joined in the antics. In the December 2019 video, then-Chief Revenue Officer Isner asked Kevin De Rosa Jr, a strapping employee in his 30s, if he had been tased during his three years at Axon. De Rosa said he hadn’t.</p>
<p id="paragraph-171">“What?” Isner replied. “Are you kidding?” Isner then turned to the camera and said with a smirk, “I think a lot less of him.”</p>
<p id="paragraph-172">Moments later, De Rosa lined up and was shot with a Taser. “Big tree fall&nbsp;hard,” Isner quipped to a laughing crowd, among them a manager who waved a cutout of CEO Smith’s face. De Rosa bellowed as current flowed through his body.</p>
<p id="paragraph-173">De Rosa declined to comment.</p> 
<p id="paragraph-178">Gibson, the former international support manager, disputed Axon’s statement that exposures are held to improve understanding of the product.</p>
<p id="paragraph-179">It is “more about a bro code,” she said. “This has nothing to do with sales. This has nothing to do with understanding a client or a customer... They’re just doing this for fun. And that’s off-putting.”</p>
<p id="paragraph-180">Gorman, the former Axon lawyer, said he “vividly” remembers an executive asking him if he was going to be tased.</p>
<p id="paragraph-181">“I thought he was joking around,” he said, but “every time there was an exposure, it was like, did you sign up? When are you going to sign up? You’re going to do it, right?”</p>
<p id="paragraph-182">Gorman resisted, saying he was concerned by reports of people involuntarily urinating or defecating as they took a hit.</p>
<p id="paragraph-183">He soon felt he did not belong at Axon.</p>
<p id="paragraph-184">“You’re not part of the culture unless you do it,” he said.</p> 

    <div data-id="R0RFWTO57F_51">
<p id="paragraph-187"><strong id="FAS99BKNYE_52">Culture Shock</strong></p>
<p id="paragraph-188">By Jeffrey Dastin</p>
<p id="paragraph-189">Additional reporting: Paresh Dave</p>
<p id="paragraph-190">Art direction: John Emerson</p>
<p id="paragraph-191">Edited by Julie Marquis</p>

        </div>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keisan Casio is shutting down (128 pts)]]></title>
            <link>https://keisan.casio.com/keisan/abolition.php</link>
            <guid>37328669</guid>
            <pubDate>Wed, 30 Aug 2023 20:35:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://keisan.casio.com/keisan/abolition.php">https://keisan.casio.com/keisan/abolition.php</a>, See on <a href="https://news.ycombinator.com/item?id=37328669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
	<p>
		<h2><span id="iconbar">Announcement for the website closure</span></h2>
	</p>
	<div>
		<ol>
			<li><a href="https://keisan.casio.com/">Home</a><span>/</span></li><li>Announcement for the website closure</li>
		</ol>
	</div>


	<div><p>

		Dear Customers,</p><p>
		keisan English website(keisan.casio.com) is due for closure.</p><p>

		<b>Scheduled closing date: <span color="red" size="+1">Wednesday, September 20, 2023 </span></b></p><p>

		Thank you for using our service for many years.</p><p>

		We appreciate your understanding.</p><p>

		Best Regards,<br>
		Website  manager.</p></div>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Echo Chess: The Quest for Solvability (110 pts)]]></title>
            <link>https://samiramly.com/chess</link>
            <guid>37327895</guid>
            <pubDate>Wed, 30 Aug 2023 19:44:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://samiramly.com/chess">https://samiramly.com/chess</a>, See on <a href="https://news.ycombinator.com/item?id=37327895">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
				<h2>Prologue</h2><p>Let’s make Chess more fun in Single-Player. How hard could it be? ☠️</p><p>This is the story of venturing too deep, head-first, into the unknown. It all started with a doodle on a piece of paper. This doodle to be exact.</p><figure><img src="https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png" srcset="https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png 4030w, https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png/500w 500w, https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png/750w 750w, https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png/1000w 1000w, https://samiramly.com/media/itgT1X7c0P4L4P4VlAER9VajhKHHpsosR8H99rsh.png/1500w 1500w"><figcaption>Some pics are only worth log(1k) words.</figcaption></figure><p>I first conceived of this game on a whim as part of many strategy and puzzle games I’d been designing for fun. I was musing with the idea of a chess-inspired Turn-Based Strategy (TBS) game that no one would recognize as chess-based. My first attempts purposefully kept steering the theme away from chess. Why go vanilla when there are so many wilder thematic flavors and bolder mechanics to explore?</p><p>Asymmetric rewards, alternating movement rules, stochastic obstacles, stealth, morphing, etc. A wise friend and fellow strategy game nerd* then said to me:</p><blockquote><p><em>The game’s dope. But what’s wrong with people associating it with Chess? No need to innovate that kernel away - their mental load would be taken up by re-learning how to move. Chess pieces are a universal language. They help them overcome the activation energy and figure out what’s going on.</em></p></blockquote><p>So I re-designed the rules, mechanics, and first few levels with <s>a sharpie and a whiteboard</s>   <s>Figma and a LLaMA</s>   sandwich paper and a pencil. Then I started testing it with friends by setting up this old wooden board I found in storage. Soon “the game” looked more like this.</p><figure><img src="https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg" srcset="https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg 2822w, https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg/500w 500w, https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg/750w 750w, https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg/1000w 1000w, https://samiramly.com/media/S5HdoQbJI3ZRlU6KMz5wJ18AscCks4GNbHy0HlI3.jpg/1500w 1500w"><figcaption>Fallen pieces = obstacles. Inverted ones = me running out of pieces (4 black rooks above).<br>Ballpens are for board resizing. No, there’s no “game engine”. Yes, I personally update the game state for you every time you move. No, Hollywood hasn’t texted me yet.</figcaption></figure><p>Something surprising started happening quickly. Anyone who tried the game got hooked. People would keep coming back trying to beat levels they couldn’t solve. They’d ask me to manually reset the board to specific checkpoints in a puzzle so they could try again.</p><p>Chess masters and n00bs alike would describe feeling a rush of excitement every time they’d reach the ‘Aha’ moment of a maze. Then people wanted to play “the game” at home and show it to <em>their</em> friends. I’d send them photos of my scribbled level designs so they could reproduce them on their own with a physical chess board or something like <a href="https://lichess.org/editor" target="_blank" rel="noopener noreferrer">lichess</a>.</p><p>One day I just figured this was getting comically unsustainable. So I decided to build “the game” into a proper thing online and call it <strong>Echo Chess</strong>.</p><p>And thus began a deep, deep rabbit hole…🐇</p><hr><h2>Echo Chess “Classic”</h2><h3>Gameplay Design</h3><p>Three simple rules of Echo Chess — still relevant from v0 to this day:</p><blockquote><ol><li><p><em>You are playing White, there is no opponent. You must capture all pieces to win.</em></p></li><li><p><em>You become the "echo" of any piece you capture. Captured a bishop? Become that bishop.</em></p></li><li><p><em>You can’t pass through red obstacles. Find the best move order to clear the board.</em></p></li></ol></blockquote><p>And so it was that Echo Chess graduated from a sandwich paper to a Mechanical Turk model to a minimalist web app, where you just hit a link and solve the maze. No downloads, no installs, no load screens, no tutorials, no handholding. Just tap the link, figure out the puzzle. An extreme experiment in <em>‘show, don’t tell’ </em>design.</p><figure><img src="https://samiramly.com/media/KYjbyreQJkQXeICvensRnvIYABbpwVWAwulFmwb3.gif"><figcaption>A new meaning for ‘eat what you kill’. </figcaption></figure><p>What’s the catch? Because you’re transformed with every capture, the maze is effectively changing as you play.</p><figure><img src="https://samiramly.com/media/e1aMXA1n50pmB1zlRbuYauS4D770Ntao0YZXMQBe.png" srcset="https://samiramly.com/media/e1aMXA1n50pmB1zlRbuYauS4D770Ntao0YZXMQBe.png 754w, https://samiramly.com/media/e1aMXA1n50pmB1zlRbuYauS4D770Ntao0YZXMQBe.png/500w 500w, https://samiramly.com/media/e1aMXA1n50pmB1zlRbuYauS4D770Ntao0YZXMQBe.png/750w 750w"><figcaption>Level 6 of “Classic” v1. The higher ones get <em>significantly</em> more challenging.</figcaption></figure><p>In later levels, you unlock squad-like gameplay where you get to move and coordinate multiple pieces — or even cannibalize one of your own to clear the way, if it comes down to it.</p><figure><img src="https://samiramly.com/media/BsqD5iHfJBwwpQPbmMwOsF6HXblAZoKpiwPPRcDV.gif"><figcaption>The enemy of my enemy is another person blocking the way.</figcaption></figure><h3><strong>Game Mechanics Implications</strong></h3><p>I think one of the reasons Echo Chess is intriguing to so many diverse player personas is that it’s (laughably) easy to learn, yet (frustratingly) hard to master.</p><p>To give you but a glimpse as to why that’s the case:</p><ul><li><p>The moment you enjoy any capture whatsoever, you lose all your existing abilities. If you were a knight galavanting around obstacles, you just lost all that by capturing a rook.</p></li><li><p>Landed yourself a powerful queen? Great, you can only use it once before it’s gone. Think you should’ve saved capturing it for later when you’re in a pinch? Who says the option would still be there if you take another path? Capturing order matters. A <em>lot</em>.</p></li><li><p>Need to clear the passage to cross to the other side of the board? Don’t forget you’ll become whatever blocker you’re clearing. Pawns = fewer degrees of freedom. Keep them till the end, or better get them over with quickly?</p></li></ul><p>Oh you think it’d be much easier if you only had more white pieces on your side? Why don’t you try solving, say, Level 13 below? (jk pls don’t actually try it cold turkey. It’s pretty much impossible.)</p><figure><img src="https://samiramly.com/media/ZQaxKzZNaABWsWYfPcxJI7psIYFCnqys8yT7yl6k.png" srcset="https://samiramly.com/media/ZQaxKzZNaABWsWYfPcxJI7psIYFCnqys8yT7yl6k.png 754w, https://samiramly.com/media/ZQaxKzZNaABWsWYfPcxJI7psIYFCnqys8yT7yl6k.png/500w 500w, https://samiramly.com/media/ZQaxKzZNaABWsWYfPcxJI7psIYFCnqys8yT7yl6k.png/750w 750w"><figcaption>Level 13, or the “puzzle from hell” as one of the early playtesters called it.</figcaption></figure><p>Very quickly you’ll realize Echo Chess is a puzzle game that’s deceivingly immune to brute force. Click-spamming won’t get you far here.</p><p>I actually had to add a ‘Give Up’ button as an early termination state for those who get stumped but want to save their high score (in some ways, this makes puzzle games different from the arcade genre because you can never just… die).</p><hr><h3>Balancing, Levels and Game Design</h3><p>Traditional game design tells us we should aim to reproduce, both as strictly and as loosely as possible, this kind of sentiment when setting the difficulty of a game:</p><figure><img src="https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png" srcset="https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png 2800w, https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png/500w 500w, https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png/750w 750w, https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png/1000w 1000w, https://samiramly.com/media/MGBEX8OvexXz4M4ONCSKkSaBCFonYSmDPGmNOHeV.png/1500w 1500w"><figcaption>The ‘<a href="http://www.davetech.co.uk/difficultycurves" target="_blank" rel="noopener noreferrer">Difficulty Saw</a>’ curve most designers swear their game has. It doesn’t.</figcaption></figure><p>A good game feels like it’s getting easier as you start mastering its mechanics, until a new mechanic/challenge/twist is introduced, and the difficulty shoots up again. You try applying the old strategies you’ve perfected but it’s no use. Then you get it: you need to snap out of the comfort zone and learn some new SKilLz it’s pushing you to learn. So you actually #gitgud and the game feels easier again, up until you hit the next bump. Rinse and repeat.</p><p>In theory, this is the recipe for a truly fun game where players enter the coveted ‘Flow’ state of gaming.</p><p>In practice, this <em>kinda</em> works a tiny bit, until it doesn’t. Players come with all sorts of prior skills (having played many games of the same genre, or just being wired a certain way), a wide spectrum of (im)patience, and all kinds of expectations as to what qualifies as a difficulty spike, or where <em>they</em> would’ve drawn the line between offensively easy and unfairly brutal. You somehow end up both scaring the n00bs <em>and</em> boring the veterans.</p><figure><img src="https://samiramly.com/media/J25rPvAYnKkW4UYNLQoGRZU0zvye7A4FEH7Dplaq.png" srcset="https://samiramly.com/media/J25rPvAYnKkW4UYNLQoGRZU0zvye7A4FEH7Dplaq.png 671w, https://samiramly.com/media/J25rPvAYnKkW4UYNLQoGRZU0zvye7A4FEH7Dplaq.png/500w 500w"><figcaption>More people will rate your game as top-left-red than bottom-right-red.<br>Since the dumbing down of modern society, the Mindless zone has never been so fertile.</figcaption></figure><p>What’s the answer, then? Well, if you’re thinking of making a game that’s ‘easy to learn, hard to master’, I think it goes one of two ways:</p><ul><li><p>If you’re reading this after the year <s>2020</s> <s>2010</s> 2000, don’t make this game. Seriously, don’t. No one will play it. Social media and OF culture have somehow managed to <a href="https://www.dailystar.co.uk/news/world-news/worlds-sexiest-chess-player-confesses-29551434?utm_source=flipboard&amp;utm_content=DailyStar%2Fmagazine%2FWeird+news" target="_blank" rel="noopener noreferrer">turn strategy games into trash TV</a>. You can’t compete with that. Find some other thing instead. You’ve been warned.</p></li><li><p>If you’re someone who’s 100% immune to good advice and you still insist on making this game, then the gods be with you. At the very least, though, please do these simple things:</p><ul><li><p>(1) let people skip to any level they want, whenever they want. Let them gauge their own spice tolerance.</p></li><li><p>(2) if you really <em>need</em> to sprinkle new mechanics here and there, add them in some micro-saw-tooth way.</p></li><li><p>(3) make your macro difficulty ramp a slow-building exponential curve instead, closer to this next one. Newcomers will Dunning–Kruger their way to the early fun part, and ambitious players will power through to reach the <em>really</em> fun stuff.</p><ul><li><p>(Optional) offer enough replayability and variability for each difficulty tier so that newbies can thrive in cuteness land forever, while GMs and PhDs play a <a href="https://arxiv.org/abs/2107.13460" target="_blank" rel="noopener noreferrer">math meta</a> of their own.</p></li></ul></li></ul></li></ul><figure><img src="https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png" srcset="https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png 2800w, https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png/500w 500w, https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png/750w 750w, https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png/1000w 1000w, https://samiramly.com/media/iG9NnjjlTByBP8maQ39sqJwvl8Lg39i6ojBUWKoR.png/1500w 1500w"><figcaption>A more desirable curve for easy-to-learn-hard-to-master games.</figcaption></figure><p>Cool. Let’s bring it back to Echo Chess. Friends who know me well will attest that, when it comes to any form of creative expression, I ascribe to the (more-controversial-than-it-has-to-be) <a href="https://en.wikipedia.org/wiki/Spoiler_(media)" target="_blank" rel="noopener noreferrer">No Spoilers</a> school of thought.</p><p>As a service to fellow nospoilerists out there, I’ll only break down one (fictional) example in detail here. Then I’ll let you enjoy the actual levels yourself.</p><figure><img src="https://samiramly.com/media/l6kJ1EwfiL1uNF80sI2gKS2tlPXsw9MTk88W0yk2.png" width="319" height="314" srcset="https://samiramly.com/media/l6kJ1EwfiL1uNF80sI2gKS2tlPXsw9MTk88W0yk2.png 608w, https://samiramly.com/media/l6kJ1EwfiL1uNF80sI2gKS2tlPXsw9MTk88W0yk2.png/500w 500w"><figcaption>If your levels have nothing to say, your game’s language is too primitive.</figcaption></figure><p>Consider the 4x4 board above. You’re playing King, and need to capture two pawns, a knight, and a rook. There are 2 impassable obstacles. Which piece do you capture first? And is this a good tutorial level?</p><p>If you’re new to Echo Chess, it means you fall in one of two camps:</p><ul><li><p>(1) You see a cute small level. Only 4 pieces. You brute-force your way into capturing every piece to see what happens. All hail the #yolo king.</p></li><li><p>(2) You’ve binge-watched <em>The Queen's Gambit</em> and have been slogging through Chess.com, because Covid. Or maybe you’re an <em>actual</em> Chess player. So you know better. You plan your 1st, 2nd, Nth move in your head and assign values to certain squares.</p></li></ul><p>The problem if you’re a type-2 person is that in Chess, you already have some built-in <em>intuition</em> as to what generally makes a move good vs bad. You’ve spent so many years practicing advanced tactics and openings that using your intuition as a heuristic in your ‘forward pass’ exploration makes it less brute-forcy than for type-1s. But in Echo Chess, your heuristics can be easily deceived. Is r &gt; n or r &lt; n?</p><p>Here’s how an Echo Chess player would typically solve the board above using a ‘backward pass’ instead (obviously there’s more than one “right” way to do these, but FWIW):</p><figure><img src="https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png" srcset="https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png 1534w, https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png/500w 500w, https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png/750w 750w, https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png/1000w 1000w, https://samiramly.com/media/0IJxE93ontlSYYbdsTpcZfO99AuNE88w19qznG1g.png/1500w 1500w"><figcaption>Step 1. Identify the top pawn as an ‘end state’ piece that has 0 valid moves.<br>Step 2. Identify the bottom pawn as only ever able to capture one piece: the top pawn.<br><strong>Conclusion: LAST 2 captures MUST be bottom p, then top p.</strong></figcaption></figure><figure><img src="https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png" srcset="https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png 1530w, https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png/500w 500w, https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png/750w 750w, https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png/1000w 1000w, https://samiramly.com/media/28po1g2ovJKWxbG23Gh7jk1XPuQ0MmgB2IwryZe7.png/1500w 1500w"><figcaption>Step 3. Identify the bottom pawn as non-capturable by a knight (no L’s can reach it).<br>Step 4. Bottom p is non-capturable by p or n. But it’s our penultimate capture, so K can’t get it.<br><strong>Conclusion: the rook MUST capture bottom p. LAST 3 captures MUST be: r, p, p.</strong></figcaption></figure><figure><img src="https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png" srcset="https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png 1530w, https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png/500w 500w, https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png/750w 750w, https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png/1000w 1000w, https://samiramly.com/media/h6iaPPmSNsD4AtmCCJlEbJ3n6q4ZjLrphi2GyGPC.png/1500w 1500w"><figcaption>Step 5: r is non-capturable by the p’s. But it’s our 3rd-to-last capture, so K can’t get it.<br>Step 6: n must capture r. It needs a few L-jumps but it can do it.<br><strong>Conclusion: LAST 4 captures MUST be: n, r, bottom p, then top p.</strong></figcaption></figure><figure><img src="https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png" srcset="https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png 1530w, https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png/500w 500w, https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png/750w 750w, https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png/1000w 1000w, https://samiramly.com/media/fwHL9TzEOYwt2nAWBsHTXDaPhTw3uSXjmi9k4EvW.png/1500w 1500w"><figcaption>Step 7: All that’s left is for K to capture n. Easy.<br>Step 8: Recap of the ‘forward pass’ capturing order to clear the board.<br><strong>Conclusion: backprop does the smoothest moonwalk.</strong></figcaption></figure><p>In reality, an ‘Echo Chess player’ (wtv that is) would instinctively have spotted the 2 pawns as finishers from the first glance, and likely computed Steps 3-5 pretty quickly. I’d say the L-jumps of Step 6 are the only cryptic bit in this maze, <em>assuming</em> the right intuition has already been developed. Is it a good tutorial level? Nope.</p><p>The onus of nurturing this type of game-mechanic-specific intuition is on the game designer, <em>not</em> the player. It should be (and is) experienced in a <em>prior</em> maze to this one. There are at least a dozen similar heuristics to what we just saw that you’re bound to pick up while playing Echo Chess. I’ve designed each level to hopefully convey the importance of a new one. If you guess some of them, please drop me a note.</p><p>If you’re curious about the more challenging ‘Aha’s, try levels 11-15 on <a href="https://echochess.com/" target="_blank" rel="noopener noreferrer">echochess.com</a>. You can switch to any of them anytime by clicking the Level dropdown at the top.</p><hr><h3>Scoring System</h3><p>Let’s talk about scores. I’ve always believed that a well-designed scoring system should be optimized to properly incentivize better gameplay, not cheesing for high scores or <a href="https://warcraft3.info/articles/394/warcraft-3-beyond-apm-a-brief-exploration-of-in-game-statistics" target="_blank" rel="noopener noreferrer">APM</a>. So naturally I baked that into the design of Echo Chess.</p><ul><li><p>You get points for each capture (higher per piece type, higher overall for harder levels).</p></li><li><p>Gameplay-wise, there’s (almost) no difference between a Q and a K. Why? Because single-player. If it’s always your turn, space-time gets bent. K * moves = Q.</p></li></ul><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="" data-name=""><code><p><span> 1</span><span><span>function</span><span> </span><span>updateScoreForCapture</span><span>(</span><span>pieceType</span><span>) {</span></span></p><p><span> 2</span><span><span>  </span><span>var</span><span> pieceScores </span><span>=</span><span> {</span></span></p><p><span> 3</span><span><span>    </span><span>'p'</span><span>: </span><span>10</span><span>,</span></span></p><p><span> 4</span><span><span>    </span><span>'n'</span><span>: </span><span>30</span><span>,</span></span></p><p><span> 5</span><span><span>    </span><span>'b'</span><span>: </span><span>30</span><span>,</span></span></p><p><span> 6</span><span><span>    </span><span>'r'</span><span>: </span><span>50</span><span>,</span></span></p><p><span> 7</span><span><span>    </span><span>'q'</span><span>: </span><span>80</span><span>,</span></span></p><p><span> 8</span><span><span>    </span><span>'k'</span><span>: </span><span>100</span></span></p><p><span> 9</span><span><span>  }</span></span></p><p><span>10</span><span><span>  runningScore </span><span>+=</span><span> currentLevel </span><span>*</span><span> pieceScores[pieceType]</span></span></p><p><span>11</span><span><span>}</span></span></p></code></pre><ul><li><p>You get a “level bonus” every time you solve a maze. This increases quadratically as levels go up.</p></li><li><p>The level bonus is penalized (in a compounding way) for every move you make. So move efficiency matters a ton —especially in higher levels where the compound penalty hurts a bigger base. You’d still always get <em>some</em> positive bonus, nbd.</p></li></ul><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="" data-name=""><code><p><span>1</span><span><span>function</span><span> </span><span>updateScoreForLevelBonus</span><span>(</span><span>numMoves</span><span>) {</span></span></p><p><span>2</span><span><span>  </span><span>var</span><span> levelBase </span><span>=</span><span> </span><span>200</span><span> </span><span>*</span><span> Math.</span><span>pow</span><span>(currentLevel, </span><span>2</span><span>)</span></span></p><p><span>3</span><span><span>  </span><span>// penalize by compounding -2% for every move used in this level</span></span></p><p><span>4</span><span><span>  </span><span>var</span><span> levelBonus </span><span>=</span><span> Math.</span><span>floor</span><span>(levelBase </span><span>*</span><span> Math.</span><span>pow</span><span>(</span><span>0.98</span><span>, numMoves))</span></span></p><p><span>5</span><span><span>  levelBonus </span><span>=</span><span> Math.</span><span>round</span><span>(levelBonus </span><span>/</span><span> </span><span>5</span><span>) </span><span>*</span><span> </span><span>5</span></span></p><p><span>6</span><span><span>  runningScore </span><span>+=</span><span> levelBonus</span></span></p><p><span>7</span><span><span>}</span></span></p></code></pre><ul><li><p>Jumping levels using the dropdown is allowed at any time, but the score resets automatically to avoid cheating the live leaderboard.</p></li><li><p>You get a “time bonus” when finishing the game based on how long it took you to beat the whole thing. The most you could ever score (even at hyperspace speed) is capped at a multiple of base, then it starts dropping as a reciprocal of time spent.</p></li></ul><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="" data-name=""><code><p><span>1</span><span><span>function</span><span> </span><span>updateScoreForGameCompletion</span><span>() {</span></span></p><p><span>2</span><span><span>  </span><span>// time bonus drops reciprocally but is always in [0, +30%]</span></span></p><p><span>3</span><span><span>  </span><span>var</span><span> cappedMultiplier </span><span>=</span><span> </span><span>0.3</span><span> </span><span>/</span><span> (</span><span>1</span><span> </span><span>+</span><span> </span><span>0.005</span><span> </span><span>*</span><span> secondsElapsed)</span></span></p><p><span>4</span><span><span>  timeBonus </span><span>=</span><span> Math.</span><span>floor</span><span>(cappedMultiplier </span><span>*</span><span> runningScore)</span></span></p><p><span>5</span><span><span>  runningScore </span><span>+=</span><span> timeBonus</span></span></p><p><span>6</span><span><span>}</span></span></p></code></pre><p>Basically the scoring function rewards <strong>maze-solving first, move efficiency second, </strong>and<strong> speed of completion last.</strong></p><p>This incentivizes players to embrace the joy of adventuring and <em>actually</em> play the maze until they truly ‘get it’ without premature optimization. Then they can go back and search for more creative solution paths (which brings a joy of its own).</p><p>Players always behave in ways the game reinforces them to, whether they realize it or not. And a <a href="https://en.wikipedia.org/wiki/Metagame" target="_blank" rel="noopener noreferrer">meta</a> <em>will</em> emerge from any scoring system (LeetCode, anyone?). This follows directly from evolutionary psychology —or from RL if you prefer robo-speak. Oh and speedrunners gonna speedrun no matter what you do.</p><blockquote><p><em>“Make sure the winning strategy is also the funnest one to play.” -</em><a href="https://magic.wizards.com/en/news/making-magic/twenty-years-twenty-lessons-part-1-2016-05-30" target="_blank" rel="noopener noreferrer"><em>Mark Rosewater</em></a><em>, strategy game designer and puzzlemaker.</em></p></blockquote><hr><h3><s>Game</s> Full Stack Development</h3><p>So how did Echo Chess go from wooden boards to pixels?</p><p>The frontend is, believe it or not, good old Javascript, jQuery, HTML, CSS. The backend is a Flask server hooked into a ReplitDB. Async calls are made with good old AJAX. Echo Chess is retro through and through.</p><p>Two open source libraries helped quite a bit for the earliest prototype’s client-side plumbing: <code>chess.js</code> and <code>chessboard.js</code>. Of course, an enormous amount of work went in to go from a barebone chess moves validator to a fully fledged chess variant puzzle game.</p><p>In fact, a ton of code in these libraries had to be overwritten and extended to incorporate chess variants mechanics. Here are but a few examples:</p><ul><li><p>King movements, checks, checkmate</p></li><li><p>Relative pins, absolute pins, immobilized pieces</p></li><li><p>Restrictions on kings per player (&lt; or &gt; 1)</p></li><li><p>Piece promotions</p></li><li><p>Castling rights</p></li><li><p>Player turns, or lack thereof</p></li><li><p>Half-moves, full moves, draws</p></li><li><p>Pawn double-step disabling</p></li><li><p>En-passant dynamics</p></li><li><p>Squares having obstacles or boundaries</p></li><li><p>Movement pathing with obstacles</p></li><li><p>Capture-based transformation</p></li><li><p>Preventing transformation reversion</p></li><li><p>Self-sacrifice dynamics</p></li><li><p>(you get the point)</p></li></ul><p>The client side keeps track of the game state, level state, moves efficiency, scoring, timing and consistency across level jumps, retries, restarts, switching game modes, etc.</p><p>The server handles saving and retrieving scores, live leaderboards, level data validation and prediction (more on that later on). The latest saved high scores get cached on the client and only get updated by the server when relevant.</p><p>Let’s take a look at a simple implementation example from Echo Chess to illustrate how a traditional chess engine can be used as a building block for chess variants.</p><p>Here we’d like to highlight in green all the squares a piece can move to, taking into account the concept of obstacles we’ve defined for this game. We start by formalizing how an obstacle can block stuff.</p><figure><img src="https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png" width="504" height="259" srcset="https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png 2104w, https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png/500w 500w, https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png/750w 750w, https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png/1000w 1000w, https://samiramly.com/media/a8QrlLy9u7mK52lroADgs9qVzfTToJOPeAfVVTMw.png/1500w 1500w"><figcaption>Left: all 3 on same line, AND obstacle between them.<br>Right: all 3 on same line, but obstacle NOT between them.</figcaption></figure><p>Let’s write some simple code to capture this.</p><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="" data-name=""><code><p><span> 1</span><span><span>function</span><span> </span><span>obstacleOnPath</span><span>(</span><span>direction</span><span>, </span><span>source</span><span>, </span><span>target</span><span>, </span><span>obstacle</span><span>) {</span></span></p><p><span> 2</span><span><span>  </span><span>let</span><span> [i1, j1] </span><span>=</span><span> </span><span>getRankAndFile</span><span>(source)</span></span></p><p><span> 3</span><span><span>  </span><span>let</span><span> [i2, j2] </span><span>=</span><span> </span><span>getRankAndFile</span><span>(target)</span></span></p><p><span> 4</span><span><span>  </span><span>let</span><span> [ik, jk] </span><span>=</span><span> </span><span>getRankAndFile</span><span>(obstacle)</span></span></p><p><span> 5</span><span><span>  </span><span>if</span><span> (direction </span><span>==</span><span> </span><span>'h'</span><span>) {</span></span></p><p><span> 6</span><span><span>    </span><span>// horizontal block</span></span></p><p><span> 7</span><span><span>    </span><span>if</span><span> (i1 </span><span>==</span><span> i2 </span><span>&amp;&amp;</span><span> i1 </span><span>==</span><span> ik </span><span>&amp;&amp;</span><span> </span><span>isBetween</span><span>(jk, j1, j2)) {</span></span></p><p><span> 8</span><span><span>      </span><span>return</span><span> </span><span>true</span></span></p><p><span> 9</span><span><span>    }</span></span></p><p><span>10</span><span><span>  }</span></span></p><p><span>11</span><span><span>  </span><span>else</span><span> </span><span>if</span><span> (direction </span><span>==</span><span> </span><span>'v'</span><span>) {</span></span></p><p><span>12</span><span><span>    </span><span>// vertical block</span></span></p><p><span>13</span><span><span>    </span><span>if</span><span> (j1 </span><span>==</span><span> j2 </span><span>&amp;&amp;</span><span> j1 </span><span>==</span><span> jk </span><span>&amp;&amp;</span><span> </span><span>isBetween</span><span>(ik, i1, i2)) {</span></span></p><p><span>14</span><span><span>      </span><span>return</span><span> </span><span>true</span></span></p><p><span>15</span><span><span>    }</span></span></p><p><span>16</span><span><span>  }</span></span></p><p><span>17</span><span><span>  </span><span>else</span><span> </span><span>if</span><span> (direction </span><span>==</span><span> </span><span>'d'</span><span>) {</span></span></p><p><span>18</span><span><span>    </span><span>// diagonal block</span></span></p><p><span>19</span><span><span>	</span><span>if</span><span> (Math.</span><span>abs</span><span>(i2 </span><span>-</span><span> i1) </span><span>==</span><span> Math.</span><span>abs</span><span>(j2 </span><span>-</span><span> j1) </span><span>&amp;&amp;</span><span> </span></span></p><p><span>20</span><span><span>		Math.</span><span>abs</span><span>(ik </span><span>-</span><span> i1) </span><span>==</span><span> Math.</span><span>abs</span><span>(jk </span><span>-</span><span> j1) </span><span>&amp;&amp;</span><span> </span></span></p><p><span>21</span><span><span>		Math.</span><span>abs</span><span>(i2 </span><span>-</span><span> ik) </span><span>==</span><span> Math.</span><span>abs</span><span>(j2 </span><span>-</span><span> jk) </span><span>&amp;&amp;</span></span></p><p><span>22</span><span><span>		</span><span>isBetween</span><span>(ik, i1, i2) </span><span>&amp;&amp;</span><span> </span><span>isBetween</span><span>(jk, j1, j2)) {</span></span></p><p><span>23</span><span><span>		</span><span>return</span><span> </span><span>true</span></span></p><p><span>24</span><span><span>	}</span></span></p><p><span>25</span><span><span>  }</span></span></p><p><span>26</span><span><span>  </span><span>return</span><span> </span><span>false</span></span></p><p><span>27</span><span><span>}</span></span></p></code></pre><p>Now we can combine these concepts with the usage of the chess engine to write a simple highlighting function for possible moves.</p><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="h=1,2,9,29,30,31,32" data-name=""><code><p><span> 1</span><span><span>import</span><span> { Chess } </span><span>from</span><span> </span><span>'chess.js'</span></span></p><p><span> 2</span><span><span>const</span><span> </span><span>chess</span><span> </span><span>=</span><span> </span><span>new</span><span> </span><span>Chess</span><span>()</span></span></p><p><span> 3</span><span><wbr></span></p><p><span> 4</span><span><span>function</span><span> </span><span>blockedTarget</span><span>(</span><span>source</span><span>, </span><span>target</span><span>) {</span></span></p><p><span> 5</span><span><span>  </span><span>var</span><span> obstacles </span><span>=</span><span> levels[currentLevel].obstacles</span></span></p><p><span> 6</span><span><span>  </span><span>if</span><span> (obstacles.</span><span>includes</span><span>(target)){</span></span></p><p><span> 7</span><span><span>    </span><span>return</span><span> </span><span>true</span></span></p><p><span> 8</span><span><span>  }</span></span></p><p><span> 9</span><span><span>  </span><span>var</span><span> pieceType </span><span>=</span><span> chess.</span><span>get</span><span>(source).type</span></span></p><p><span>10</span><span><span>  </span><span>if</span><span> (pieceType </span><span>==</span><span> </span><span>'b'</span><span> </span><span>||</span><span> pieceType </span><span>==</span><span> </span><span>'q'</span><span>) {</span></span></p><p><span>11</span><span><span>    </span><span>for</span><span> (</span><span>let</span><span> obstacle </span><span>of</span><span> obstacles) {</span></span></p><p><span>12</span><span><span>      </span><span>if</span><span> (</span><span>obstacleOnPath</span><span>(</span><span>'d'</span><span>, source, target, obstacle)) {</span></span></p><p><span>13</span><span><span>        </span><span>return</span><span> </span><span>true</span><span>;</span></span></p><p><span>14</span><span><span>      }</span></span></p><p><span>15</span><span><span>    }</span></span></p><p><span>16</span><span><span>  }</span></span></p><p><span>17</span><span><span>  </span><span>if</span><span> (pieceType </span><span>==</span><span> </span><span>'r'</span><span> </span><span>||</span><span> pieceType </span><span>==</span><span> </span><span>'q'</span><span>) {</span></span></p><p><span>18</span><span><span>    </span><span>for</span><span> (</span><span>let</span><span> obstacle </span><span>of</span><span> obstacles) {</span></span></p><p><span>19</span><span><span>      </span><span>if</span><span> (</span><span>obstacleOnPath</span><span>(</span><span>'h'</span><span>, source, target, obstacle) </span><span>||</span></span></p><p><span>20</span><span><span>	      </span><span>obstacleOnPath</span><span>(</span><span>'v'</span><span>, source, target, obstacle)) {</span></span></p><p><span>21</span><span><span>        </span><span>return</span><span> </span><span>true</span><span>;</span></span></p><p><span>22</span><span><span>      }</span></span></p><p><span>23</span><span><span>    }</span></span></p><p><span>24</span><span><span>  }</span></span></p><p><span>25</span><span><span>  </span><span>return</span><span> </span><span>false</span><span>;</span></span></p><p><span>26</span><span><span>}</span></span></p><p><span>27</span><span><wbr></span></p><p><span>28</span><span><span>function</span><span> </span><span>colorPossibleMoves</span><span>(</span><span>source</span><span>) {</span></span></p><p><span>29</span><span><span>  </span><span>var</span><span> moves </span><span>=</span><span> chess.</span><span>moves</span><span>({</span></span></p><p><span>30</span><span><span>    square: source,</span></span></p><p><span>31</span><span><span>    verbose: </span><span>true</span></span></p><p><span>32</span><span><span>  })</span></span></p><p><span>33</span><span><span>  </span><span>if</span><span> (moves.</span><span>length</span><span> </span><span>==</span><span> </span><span>0</span><span>) </span><span>return</span></span></p><p><span>34</span><span><span>  </span><span>greenSquare</span><span>(source)</span></span></p><p><span>35</span><span><span>  </span><span>for</span><span> (</span><span>var</span><span> i </span><span>=</span><span> </span><span>0</span><span>; i </span><span>&lt;</span><span> moves.</span><span>length</span><span>; i</span><span>++</span><span>) {</span></span></p><p><span>36</span><span><span>    </span><span>if</span><span> (</span><span>!</span><span>blockedTarget</span><span>(source, moves[i].to)) {</span></span></p><p><span>37</span><span><span>      </span><span>greenSquare</span><span>(moves[i].to)</span></span></p><p><span>38</span><span><span>    }</span></span></p><p><span>39</span><span><span>  }</span></span></p><p><span>40</span><span><span>}</span></span></p></code></pre><p>From then on, we can call the <code>colorPossibleMoves</code> function every time we pick up a white piece from a given square, and it will color all the corresponding squares in green.</p><pre onmouseenter="" onmouseleave="" data-language="javascript" data-annotations="h=11-16" data-name=""><code><p><span> 1</span><span><span>function</span><span> </span><span>onDragStart</span><span>(</span><span>source</span><span>, </span><span>piece</span><span>) {</span></span></p><p><span> 2</span><span><span>  </span><span>// player can only pick up white pieces</span></span></p><p><span> 3</span><span><span>  </span><span>if</span><span> (piece.</span><span>search</span><span>(</span><span>/</span><span>^</span><span>b/</span><span>) </span><span>!=</span><span> </span><span>-</span><span>1</span><span>) {</span></span></p><p><span> 4</span><span><span>    </span><span>$</span><span>(</span><span>"#wrong-piece"</span><span>)[</span><span>0</span><span>].</span><span>play</span><span>();</span></span></p><p><span> 5</span><span><span>    </span><span>return</span><span> </span><span>false</span></span></p><p><span> 6</span><span><span>  }</span></span></p><p><span> 7</span><span><span>  </span><span>colorPossibleMoves</span><span>(source)</span></span></p><p><span> 8</span><span><span>  </span><span>...</span></span></p><p><span> 9</span><span><span>}</span></span></p><p><span>10</span><span><wbr></span></p><p><span>11</span><span><span>var</span><span> board </span><span>=</span><span> </span><span>new</span><span> </span><span>Chessboard</span><span>(</span><span>'board'</span><span>, {</span></span></p><p><span>12</span><span><span>  position: levels[currentLevel].fen,</span></span></p><p><span>13</span><span><span>  draggable: </span><span>true</span><span>,</span></span></p><p><span>14</span><span><span>  onDragStart: onDragStart</span></span></p><p><span>15</span><span><span>  </span><span>...</span></span></p><p><span>16</span><span><span>})</span></span></p></code></pre><figure><img src="https://samiramly.com/media/DW3agkIE5mtpOCjTCBTkyo8ObkpoEw1JBwDurZ9l.gif" width="383" height="380"><figcaption>Sometimes it’s easy to forget how constrained a cornered queen can be.</figcaption></figure><p>Fast forward several thousand lines of code, and soon enough, Echo Chess ‘Classic’ was live! Friends were able to enjoy it on the go.</p><p>Since I had already meticulously crafted 15 puzzle levels, there was enough in there to keep everyone busy. For a while, the hardest puzzles remained unsolved. Dozens would try, day in, day out, but very few would be able to reach the very end. Weeks passed. And then it happened. People started finishing the game.</p><figure><img src="https://samiramly.com/media/NBBHftXlrQuaN6s0BSqjtXUByJB0I5T0OHnzoMUx.gif" width="240" height="405"><figcaption>That’s legit street cred right here.</figcaption></figure><p>Like a rush of emotions after binge-watching the last season of a favorite show, they celebrated, sighed, stared into the abyss, then cracked their knuckles, and came back asking for more. MOAR LEVELS. ASAP.</p><hr><h3>Echo Chess “ENDLESS”</h3><p>Once again, I was faced with the bittersweet realization that the users’ appetite for Echo Chess had exceeded my ability to manually keep up with “DM-ing” it. I knew I needed to automate level creation to make it, once and for all, entirely independent from my involvement as a mammal.</p><h3><strong>Procedural Generation</strong></h3><p>And so was born the new ‘Endless’ mode for Echo Chess! A true “Tetris”-like puzzle game, completely separate from ‘Classic Mode’, togglable in url params.</p><figure><img src="https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png" srcset="https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png 2972w, https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png/500w 500w, https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png/750w 750w, https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png/1000w 1000w, https://samiramly.com/media/ABlPvg2q00bg53Ewd0oiPGUrTNFI51ZGBCTMUnb6.png/1500w 1500w"><figcaption>Endless Mode (right-side) even has its own brand, compliments of <a href="https://angelahaddad.com/" target="_blank" rel="noopener noreferrer">Angela</a> ✨</figcaption></figure><p>Infinite levels, all randomly generated, all in real time. Anytime you finish a level, you carry over your winning piece to the next, and the board of the upcoming puzzle pivots around your current piece’s position.</p><figure><img src="https://samiramly.com/media/OQm5D28TswfphY8nP7EKVvt2gijZZb63P9LQJBIa.gif" width="324" height="484"><figcaption> Levels automatically swivel around you.</figcaption></figure><p>So what do we really mean by ‘Endless’?</p><p>We randomly generate each level from parametrized distributions of pieces, obstacles and boundaries (1). This gives us full control over adjusting the variety and difficulty of generations (2), and lets us tailor the beginning of each level to coincide with the ending of the prior one (3).</p><p>That brings us to the time mechanic. How do we converge an <em>infinite</em> level progression? We make it a countdown race:</p><blockquote><p><em>100 seconds. Time's running out! </em>(4)<em> Wanna stay alive? Keep solving new levels! You get more time back for solving bigger ones. </em>(5)</p></blockquote><p>Coincidentally, with this new fast-paced playing style comes a shift from a pure strategy/puzzle solving to a full-on arcade mode. The scoring function gets adjusted accordingly with bonuses scaling up or down with level sizes and difficulty —and we get a new leaderboard that’s kept fully separate.</p><hr><h3>Encoding, Decoding</h3><p>Now that we’re dealing with so many levels, we’ll need an efficient and lossless way to conveniently <em>read, write, encode, decode, send, receive, store, retrieve, transform, analyze, augment, </em>and<em> render</em> any level configuration.</p><figure><img src="https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png" width="548" height="336" srcset="https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png 2194w, https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png/500w 500w, https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png/750w 750w, https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png/1000w 1000w, https://samiramly.com/media/6kPAuKnBiz7SCKT5w4MyRzAA4MNOVXWGSJg116v8.png/1500w 1500w"><figcaption>Unforunately my wooden system falls a bit short of Shannon entropy.</figcaption></figure><p>Chess players reading this are probably already thinking of multiple such viable systems, like PGN, SAN, or UCI. These are all great for chess moves encoding and decoding, and could possibly be adjusted for our needs. But there’s actually a better system we could reuse for the purpose of game states in particular: the elegant Forsyth–Edwards Notation (FEN).</p><figure><img src="https://samiramly.com/media/ulfPW8BZc9NHOCua6KlUSOW9bfortl6FRUo7zxu7.gif"><figcaption>One row of a chess board converted to a string using FEN code.<br>Source: chessgames.com</figcaption></figure><p>The easiest way to understand a FEN string is as follows:</p><ul><li><p>Read it left to right, one row at a time, starting from the top row</p></li><li><p>Lowercase letters = black chess pieces</p></li><li><p>Uppercase letters = white chess pieces</p></li><li><p>Digits = number of consecutive empty squares (ignore their color)</p></li><li><p><strong>K</strong>ing♚, <strong>Q</strong>ueen♛, <strong>R</strong>ook♜, <strong>B</strong>ishop♝, <strong>P</strong>awn♟, k<strong>N</strong>ight♞</p><ul><li><p>The King already called dibs on ‘K’ (royal greed has no limits) so use ‘N’ for the <s>peasants</s> knights instead🐴🌝</p></li></ul></li></ul><figure><img src="https://samiramly.com/media/lpXCLwnVruJUd1eFBYNUQ8ra4e21FO2Ilwm27hI0.gif"><figcaption>All rows of the board converted and combined to generate the board’s FEN string.</figcaption></figure><p>First order of business: repurpose and expand the FEN encoding system for Echo Chess. In addition to everything else FEN does, I needed a version that takes into account things like the locations of obstacles, the size and shape of a level, where the boundary squares are, and so on.</p><h3><strong>Developing a New Formal Notation</strong></h3><p>So I came up with a new encoding that expands on FEN, which I’m offering up here for anyone who might find it valuable for their work. I call it the “<em><strong>compoundFEN</strong></em>”.</p><figure><img src="https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg" srcset="https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg 4964w, https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg/500w 500w, https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg/750w 750w, https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg/1000w 1000w, https://samiramly.com/media/WLhx1zlDvfxhszoPreJCWzzwXtZFgOlmtcq5wspA.jpg/1500w 1500w"><figcaption>One row of an Echo Chess board converted to a string using <em><strong>compoundFEN</strong></em> code.</figcaption></figure><p>Let’s consider the following example from Echo Chess to see how the <code>compoundFEN</code> would be derived. To better visualize this, we’ll overlay a grid over the level to make the canonical 8x8 chess board pop.</p><figure><img src="https://samiramly.com/media/7OD6X5xjXJDxQbRm8XWF81nV60bk5sOdTOAusWiX.gif"><figcaption>Level 7 of the original Classic mode. Looks like all in all, it was all just bricks in the wall.</figcaption></figure><p>We go through each row, top to bottom, starting from the 0th to the 7th row — sure, we could instead count from 1 to 8 like sapiens commoners, but what would the robots say?</p><ul><li><p>(0) Nothing but a board boundary in this row showing the confines of this particular level size. Can’t really think of these squares as ‘empty’ because no movement is allowed on them. <strong>Let’s call each of these </strong><em><strong>boundary squares</strong></em><strong> an ‘X’</strong>. The royal K hasn’t expropriated the X letter yet so we should be good to go.</p><ul><li><p>We’ll encode this row in compoundFEN as ‘<strong>XXXXXXXX</strong>’.</p></li></ul></li><li><p>(1) Okay so we start with an ‘X’ for a boundary. Then we have an actual empty square, so that’s a 1. Now a bunch of obstacles. They’re technically similar to boundaries in that they block the player’s motion, but they have a slightly different use. <strong>Let’s call these </strong><em><strong>obstacle squares</strong></em><strong> lowercase ‘x’</strong> just in case. Okay so we have 5 of those ‘x’, followed by another ‘X’.</p><ul><li><p>Great, we can encode this as ‘<strong>X1xxxxxX</strong>’.</p></li></ul></li><li><p>(2) This should be easier. We have ‘X’ followed by an ‘x’, followed by a bunch of black pieces (so all lowercase: q, p, b, k, and r), then a final ‘X’, with 0 proper ‘empty’ squares to be noted anywhere.</p><ul><li><p>Boom. ‘<strong>XxqpbkrX</strong>’.</p></li></ul></li><li><p>(3) Same thing here: we start with an ‘X’ followed by an ‘x’, then we get a <em>white</em> piece (so it’s uppercase) and it’s an ‘N’, not a ‘K’, because knights can’t even keep their initials in an absolute monarchy. Then we continue as before with the alternating black pawns ‘p’ and obstacles ‘x’ until hitting the last boundary ‘X’.</p><ul><li><p>Easy. ‘<strong>XxNpxpxX</strong>’.</p></li></ul></li><li><p>(4) Straightforward row: ‘X’ then ‘x’, then a black king ‘k’, then 4 ‘x’, and a final ‘X’.</p><ul><li><p>Super chill. ‘<strong>XxkxxxxX</strong>’.</p></li></ul></li><li><p>(5) Interesting, a slightly different one. ‘X’ and 2 ‘x’, sure. Then 3 actual empty squares. Okay so we’ll represent these as ‘3’ according to the base FEN convention. Then a black knight ‘n’ and a final ‘X’.</p><ul><li><p>Cool. ‘<strong>Xxx3nX</strong>’.</p></li></ul></li><li><p>(6) Back to familiar ones, we’ve got this. ‘<strong>X1xbnrxX</strong>’.</p></li><li><p>(7) We already know this one, nothing but boundaries. ‘<strong>XXXXXXXX</strong>’</p></li></ul><p>Now we put all these rows together, separated by ‘<strong>/</strong>’, to get the full ‘piece placement’ portion of the <code>compoundFEN</code>:</p><figure><img src="https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png" srcset="https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png 2584w, https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png/500w 500w, https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png/750w 750w, https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png/1000w 1000w, https://samiramly.com/media/7pyJxCvcMfuAd2EV4rMh6IaoS7xWqxHVJLaMbIIq.png/1500w 1500w"><figcaption></figcaption></figure><p>Let’s test this to see if our compoundFEN for Level 7 looks right. Here’s a quick decoding function you can use to convert from compoundFEN to a 2D board.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>fen_to_board</span><span>(compound_fen):</span></span></p><p><span> 2</span><span><span>    board </span><span>=</span><span> np.empty((</span><span>8</span><span>, </span><span>8</span><span>), </span><span>dtype</span><span>=</span><span>str</span><span>)</span></span></p><p><span> 3</span><span><span>    board.fill(</span><span>' '</span><span>)   </span></span></p><p><span> 4</span><span><span>    </span><span># grab 'piece placement' rows, ignore the rest</span></span></p><p><span> 5</span><span><span>    fen_parts </span><span>=</span><span> compound_fen.split(</span><span>' '</span><span>)  </span></span></p><p><span> 6</span><span><span>    ranks </span><span>=</span><span> fen_parts[</span><span>0</span><span>].split(</span><span>'/'</span><span>)   </span></span></p><p><span> 7</span><span><span>    rank_index </span><span>=</span><span> </span><span>0</span></span></p><p><span> 8</span><span><span>    file_index </span><span>=</span><span> </span><span>0</span></span></p><p><span> 9</span><span><span>    </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>len</span><span>(ranks)):</span></span></p><p><span>10</span><span><span>        rank </span><span>=</span><span> ranks[i]</span></span></p><p><span>11</span><span><span>        </span><span>for</span><span> j </span><span>in</span><span> </span><span>range</span><span>(</span><span>len</span><span>(rank)):</span></span></p><p><span>12</span><span><span>            char </span><span>=</span><span> rank[j]</span></span></p><p><span>13</span><span><span>            </span><span>if</span><span> char.isdigit():</span></span></p><p><span>14</span><span><span>                </span><span># consecutive empty squares</span></span></p><p><span>15</span><span><span>                file_index </span><span>+=</span><span> </span><span>int</span><span>(char)</span></span></p><p><span>16</span><span><span>            </span><span>else</span><span>:</span></span></p><p><span>17</span><span><span>                board[rank_index][file_index] </span><span>=</span><span> char</span></span></p><p><span>18</span><span><span>                file_index </span><span>+=</span><span> </span><span>1</span></span></p><p><span>19</span><span><span>        rank_index </span><span>+=</span><span> </span><span>1</span></span></p><p><span>20</span><span><span>        file_index </span><span>=</span><span> </span><span>0</span><span>   </span></span></p><p><span>21</span><span><span>    </span><span>return</span><span> board</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>compound_fen </span><span>=</span><span> </span><span>'XXXXXXXX/X1xxxxxX/XxqpbkrX/XxNpxpxX/XxkxxxxX/Xxx3nX/X1xbnrxX/XXXXXXXX w - - 0 1'</span></span></p><p><span>2</span><span><span>board </span><span>=</span><span> fen_to_board(compound_fen)</span></span></p><p><span>3</span><span><span>print</span><span>(board)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>[[</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>2</span><span><span> [</span><span>'X'</span><span> </span><span>' '</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>3</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>'q'</span><span> </span><span>'p'</span><span> </span><span>'b'</span><span> </span><span>'k'</span><span> </span><span>'r'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>4</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>'N'</span><span> </span><span>'p'</span><span> </span><span>'x'</span><span> </span><span>'p'</span><span> </span><span>'x'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>5</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>'k'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>6</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>'x'</span><span> </span><span>' '</span><span> </span><span>' '</span><span> </span><span>' '</span><span> </span><span>'n'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>7</span><span><span> [</span><span>'X'</span><span> </span><span>' '</span><span> </span><span>'x'</span><span> </span><span>'b'</span><span> </span><span>'n'</span><span> </span><span>'r'</span><span> </span><span>'x'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>8</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]]</span></span></p></code></pre><p>Looks great. Go ahead and compare every cell of this board decoded from our compoundFEN to the Level 7 screenshot above. Remember: lowercase ‘x’ refers to obstacles, and uppercase ‘X’ refers to boundary squares.</p><p>And now here’s an encoding function to automate the conversion of boards to compoundFENs moving forward.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>board_to_fen</span><span>(board):</span></span></p><p><span> 2</span><span><span>    compound_fen </span><span>=</span><span> </span><span>''</span></span></p><p><span> 3</span><span><span>    </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>len</span><span>(board)):</span></span></p><p><span> 4</span><span><span>        row </span><span>=</span><span> board[i]</span></span></p><p><span> 5</span><span><span>        empty_count </span><span>=</span><span> </span><span>0</span></span></p><p><span> 6</span><span><span>        </span><span>for</span><span> j </span><span>in</span><span> </span><span>range</span><span>(</span><span>len</span><span>(row)):</span></span></p><p><span> 7</span><span><span>            piece </span><span>=</span><span> row[j]</span></span></p><p><span> 8</span><span><span>            </span><span># count consecutive empty squares</span></span></p><p><span> 9</span><span><span>            </span><span>if</span><span> piece </span><span>==</span><span> </span><span>' '</span><span>:</span></span></p><p><span>10</span><span><span>                empty_count </span><span>+=</span><span> </span><span>1</span></span></p><p><span>11</span><span><span>            </span><span>else</span><span>:</span></span></p><p><span>12</span><span><span>                </span><span># add preceding empty squares</span></span></p><p><span>13</span><span><span>                </span><span>if</span><span> empty_count </span><span>&gt;</span><span> </span><span>0</span><span>:</span></span></p><p><span>14</span><span><span>                    compound_fen </span><span>+=</span><span> </span><span>str</span><span>(empty_count)</span></span></p><p><span>15</span><span><span>                    empty_count </span><span>=</span><span> </span><span>0</span></span></p><p><span>16</span><span><span>                </span><span># add this square's content</span></span></p><p><span>17</span><span><span>                compound_fen </span><span>+=</span><span> piece</span></span></p><p><span>18</span><span><span>        </span><span># add row's trailing empty squares</span></span></p><p><span>19</span><span><span>        </span><span>if</span><span> empty_count </span><span>&gt;</span><span> </span><span>0</span><span>:</span></span></p><p><span>20</span><span><span>            compound_fen </span><span>+=</span><span> </span><span>str</span><span>(empty_count)</span></span></p><p><span>21</span><span><span>        </span><span>if</span><span> i </span><span>&lt;</span><span> </span><span>len</span><span>(board) </span><span>-</span><span> </span><span>1</span><span>:</span></span></p><p><span>22</span><span><span>            compound_fen </span><span>+=</span><span> </span><span>'/'</span></span></p><p><span>23</span><span><span>    compound_fen </span><span>+=</span><span> </span><span>' w - - 0 1'</span></span></p><p><span>24</span><span><span>    </span><span>return</span><span> compound_fen</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>print</span><span>(board_to_fen(board))</span></span></p><p><span>2</span><span><span>print</span><span>(compound_fen </span><span>==</span><span> board_to_fen(board))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>XXXXXXXX</span><span>/</span><span>X1xxxxxX</span><span>/</span><span>XxqpbkrX</span><span>/</span><span>XxNpxpxX</span><span>/</span><span>XxkxxxxX</span><span>/</span><span>Xxx3nX</span><span>/</span><span>X1xbnrxX</span><span>/</span><span>XXXXXXXX</span><span> w </span><span>-</span><span> </span><span>-</span><span> </span><span>0</span><span> </span><span>1</span></span></p><p><span>2</span><span><span>True</span></span></p></code></pre><p>If instead you wanted to check the basic ‘standard’ FEN encoding for this same Level 7 example, you could simply ignore all ‘x’ or ‘X’ codes in the compoundFEN and assume they’re part of the consecutive empty spaces. That’s because standard FENs consider anything that’s not a chess piece to be empty. This is what you’d get: <code>8/8/2qpbkr1/2Np1p2/2k5/6n1/3bnr2/8 w - - 0 1</code>, which you can conveniently <a href="https://lichess.org/editor/8/8/2qpbkr1/2Np1p2/2k5/6n1/3bnr2/8_w_-_-_0_1?color=white" target="_blank" rel="noopener noreferrer">verify with this online chess board</a> (just keep in mind that it won’t look as recognizable without the Echo Chess parts).</p><hr><h3><strong>The Problem of Solvability</strong></h3><p>So now we have an efficient, reliable way to procedurally generate random levels, manipulate them, and serve them to our players. But one thing’s for sure: we don’t want to be serving unsolvable mazes. Tough puzzles can be thrilling, broken puzzles are just trash.</p><p>How do we check that for a given generated maze? Is it even <em>theoretically</em> <em>possible</em> to force randomly generated Echo Chess mazes to be solvable? To understand what we’re up against, let’s start by taking a look at <em>why</em> solving an Echo Chess maze is so tricky to start with.</p><p>Each of these trivial examples illustrates a simple configuration where the maze is <strong>completely unsolvable</strong>.</p><figure><img src="https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png" srcset="https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png 4496w, https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png/500w 500w, https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png/750w 750w, https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png/1000w 1000w, https://samiramly.com/media/lvi0usaV46Luse8TDujxV2R2gIfkSor2iAnPmSMI.png/1500w 1500w"><figcaption>Top 2 configurations have no valid first moves due to obstacles or boundaries.<br>Bottom 2 have no valid first capture — effectively forming disconnected subgraphs.</figcaption></figure><p>Now let’s look at some <em><strong>less</strong></em><strong>-trivial </strong>examples of <strong>unsolvable configurations</strong>. See if you can spot the issue before looking at the answers.</p><figure><img src="https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png" srcset="https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png 4543w, https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png/500w 500w, https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png/750w 750w, https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png/1000w 1000w, https://samiramly.com/media/bcZKvL6jDAkSfLYjm2ruvg1SUU6kJjFITbxdGqhb.png/1500w 1500w"><figcaption>Top-left: the rook is impossible to reach. The capturable bishop acts as a blocker. <br>Top-right: no piece exists that could reach the cornered queen.<br>Bottom-left: if q is captured, can’t get b; but if b is captured, can’t get q.<br>Bottom-right: Two bishops have no valid moves; capture one and you’re stuck.</figcaption></figure><p>At this stage, you may be starting to develop some intuition for what made these mazes unsolvable. And yet, here’s an example of a level that <em><strong>feels</strong></em><strong> unsolvable</strong> the first dozen times you try it, <strong>but that does actually have known solutions</strong>.</p><figure><img src="https://samiramly.com/media/18QjcnqcouXId548KS5lyWv2Hte6dwo3hG8p47Kb.png" srcset="https://samiramly.com/media/18QjcnqcouXId548KS5lyWv2Hte6dwo3hG8p47Kb.png 636w, https://samiramly.com/media/18QjcnqcouXId548KS5lyWv2Hte6dwo3hG8p47Kb.png/500w 500w"><figcaption>If you can’t see the solution, head to echochess.com and try it out yourself.<br>It’s Level 8 in Classic mode, from back when I was still designing every puzzle manually.</figcaption></figure><p>You might be asking yourself: okay, so solving a maze on the fly is not that straightforward, but given enough prep time, what’s the big deal? Instead of generating completely random levels in <em>real time</em>, why doesn’t this guy just serve a random level from a <em>pre-generated</em> list of solvable levels that he’s already curated beforehand?</p><p>Good question —that was my initial attempt. The reason why that turns out to be not such a good idea is that we have no control over which carry-over piece and on which carry-over square the player would be starting any given level, seeing that it depends entirely on their prior solution path. Still, you might argue, you could either:</p><ul><li><p>(1) make sure each level has only one possible end point<br>=&gt; Nope, that means we’d be back to manual designs, beating the whole point of the Endless mode</p></li><li><p>(2) pre-generate and curate levels for every combination of piece type and square the player might end up in, just in case</p><p>=&gt; Really? You want me to manually test and curate 8 x 8 x 6 = 384 combinations <em>for every level</em>?</p></li><li><p>or (3) abandon the whole concept of a carry-over piece and simply serve levels from a curated pregen list regardless of how the prior level ended</p><p>=&gt; That would actually ruin the core vibe of Endless that all users currently love: it really fees like one continuous, coherent, infinite level being played. To be fair, manually editing a pregen list is still much faster than manually designing from scratch (MidJourney, anyone?) so this could still come in handy for extra help on future <em>Classic</em> mode levels design, but it won’t give us what we need for Endless.</p></li></ul><p>In any case, this still does not solve the primary matter at hand, which is finding an automated method for generating mazes that have solutions.</p><p>One way to approach this would be to try using Pathfinding Algorithms like Depth-First Search (DFS) with Backtracking, or variants of A* and heuristic-based approaches.</p><figure><img src="https://samiramly.com/media/Jah4baYZ78IIenY4X9f2Sztx4mH0NIx84UycHOI3.gif"><figcaption>An example of a maze generator using <a href="https://en.wikipedia.org/wiki/Maze_generation_algorithm#Recursive_implementation" target="_blank" rel="noopener noreferrer">DFS with recursive backtracking</a>.<br>Source: something I built for fun as I was iterating on Echo Chess.</figcaption></figure><p>But before we hit another iceberg of delusion, let’s pause and think a bit about what we’re really attempting here:</p><ol><li><p>To solve an Echo Chess level, we have to clear the full board.</p></li><li><p>Clearing the board means capturing every piece.</p></li><li><p>To capture a piece, we have to move to its location.</p></li><li><p>Once a piece is captured, we can’t capture it again.</p></li><li><p>Therefore, to solve an Echo Chess level, we have to ‘visit’ each piece exactly once.</p></li></ol><p>Does this ring a bell? Bad news, you guys. It looks like the problem of Echo Chess Solvability (ECS) could be mappable to the Hamiltonian Path Problem (HPP) of visiting each city exactly once in a graph. </p><figure><img src="https://samiramly.com/media/PqMmAn4yuG3BDu2thAsHEDlLAey9usHuOVGhBck5.gif"><figcaption>If only solving a Hamiltonian Path Problem was as easy as tracing a known solution.<br>Source: Combinatorica</figcaption></figure><p>In case you remember your Thoeretical Computer Science class from back in the day, the HPP is an NP-complete problem.</p><p>And if you also want to account for Echo Chess’s move efficiency mechanic, like finding the <em>shortest</em> Hamiltonian path visiting all cities, then you’re even risking NP-hard territory.</p><p>If you don’t recall the HPP, don’t worry about it - you may be more familiar with its close cousin TSP since <a href="https://en.wikipedia.org/wiki/Travelling_salesman_problem" target="_blank" rel="noopener noreferrer">the Traveling Salesman Problem</a> is such a notorious one. Any way you look at it, you should probably be <a href="https://mashable.com/article/travis-kalanick-witness-testimony-uber-waymo-trial" target="_blank" rel="noopener noreferrer">un-pumped</a> that we’re dealing with these beasts.</p><figure><img src="https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png" srcset="https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png 3284w, https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png/500w 500w, https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png/750w 750w, https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png/1000w 1000w, https://samiramly.com/media/0vivWlF1vKm4erIls5sx5cxe9mwNqID76IKbjwVF.png/1500w 1500w"><figcaption>The Travelly Boi. My money’s on the Travelly Boi.<br>Source: reddit, obviously.</figcaption></figure><p>Granted, with enough compute, we could likely attempt some approaches like Monte Carlo Tree Search, alpha-beta pruning, or others, but there are other aspects of the Echo Chess solvability problem that can be non-trivial to deal with — even if we developed a reproducible mapping from a level’s starting compoundFEN+movement rules to a graph theory representation.</p><p>Things like disappearing ‘cities’ (due to captures), ‘roads’ changing upon every city visit (due to the echoing mechanism), a morphing topology as the graph is being explored: we would need to account for each of these meticulously.</p><p>Yes, some heuristic-based backtracking algorithm with clever pruning may put up a decent fight with all of this. But at the end of the day, let’s not lose track of what we <em>actually</em> care about.</p><blockquote><p><em>Ultimately, the main goal we’re after here is answering this simple question: is this level we’re serving to the player likely to be “solvable” or “not solvable”?</em></p></blockquote><p>Remember: we’re not really looking for any <em>actual</em> solutions to a maze, that would be our players’ job —they can keep all the fun to themselves. We just need a simple boolean to tell us YES or NO for solvability. So at the very core of it, <strong>we are dealing with a </strong><em><strong>Classification</strong></em><strong> problem!</strong></p><p>And that’s something we should be able to, in theory, deal much better with, assuming we set things up correctly. Why don’t we give that a shot? 🌚</p><hr><h3>Data mining</h3><p>The thing that comes to mind right away when we think of any supervised learning approach is data. Do we have any, can we get any, what shape does it come in, will we get it labeled, will it be clean, will we have enough, you know the drill.</p><p>Thankfully, the initial Classic mode of Echo Chess has already achieved a very humble level of traction and engagement, entirely thanks to word-of-mouth from players enjoying the game around the world.</p><figure><img src="https://samiramly.com/media/oWRSsDbzyRUdBvw8A9Xo7LzdPpuGSfmsVezmGQuy.gif" width="252" height="448"><figcaption>IIRC the game’s being played in 300+ cities across 60+ countries.</figcaption></figure><p>Okay so we have some real users and we can serve them some fresh levels data, that’s great. Let’s assume for a second that we can keep getting similar traction and engagement for Endless mode as we got so far from Classic. How will we get <em>labeled</em> data out of this?</p><p>Well if someone gets to the next level, that means they must’ve found a correct solution. Corollary: that level they were playing <em>must</em> be solvable. Just by winning at the game, they’re labeling it for us! Now we just need to get our Genki Dama donors to keep playing and solving while having fun.</p><p>Wait —isn’t that a catch-22? How are we going to:</p><ul><li><p><strong>(a)</strong> <strong>convince enough people to play that game mode so we can</strong></p></li><li><p><strong>(b)</strong> crowdsource labeled data, which we’ll use to</p></li><li><p><strong>(c)</strong> train a model for predicting solvability, which will allow us to</p></li><li><p><strong>(d)</strong> guess which generated levels are solvable, so that we can</p></li><li><p><strong>(e)</strong> make something that’s fun to play, which is key in order to</p></li><li><p><strong>(f)</strong> <strong>convince enough people to play that game mode?</strong></p></li></ul><figure><img src="https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png" width="230" height="322" srcset="https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png 1776w, https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png/500w 500w, https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png/750w 750w, https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png/1000w 1000w, https://samiramly.com/media/qRprN2k6XsYGZ5G9PBRzWsmRuD0VDeUZfNMYxNtA.png/1500w 1500w"><figcaption>I wonder where Aquinas stands on scissors packaging.</figcaption></figure><p>The subtle trick is in <strong>(e)</strong>. If we can make something that’s <em>just enough</em> fun to play to kickstart the first cycle, we’ll have our prime mover. We’ll need it to be playable without the user reaching a dead end anytime an unsolvable level shows up. That would kill the whole vibe.</p><p>Say hello to the <strong>SHUFFLE</strong> button: your infinite Get-Out-of-Jail-Free card to shuffle those pesky dead ends away!</p><p>Okay, I guess we could acknowledge upfront there are imperfections and dead ends and hope someone will still give the janky prototype a try? Nah, we can do insanely better than that. <strong>We make it a </strong><em><strong>feature</strong></em><strong>, not a bug!</strong> Remember <a href="https://en.wikipedia.org/wiki/Minesweeper_(video_game)" target="_blank" rel="noopener noreferrer">Minesweeper</a> from the 90s?</p><p>A pothole is a tragic accident. A mine is a strategic trap.</p><blockquote><p><em>Introducing ENDLESS Mode. Feeling stuck at any level? Try again, or simply 'SHUFFLE' it away and move on to the next one 💁🏻‍♀️ </em></p><p><em>Watch out! Some levels are purposefully sprinkled in there as unsolvable traps to be SHUFFLED away 👻</em></p><p><em>Hone the skill of directly spotting dead-ends or you'll get stuck in a treacherous maze 🧨 </em></p><p><em>Don't be shy with SHUFFLING! You have unlimited shuffles - just keep an eye on the countdown or you'll be racing to Game Over⏱️😉</em></p></blockquote><figure><img src="https://samiramly.com/media/OGaopo859WSOzhmcDBMxI4h9sfaDxCymVNpYzRzv.png" width="341" height="389" srcset="https://samiramly.com/media/OGaopo859WSOzhmcDBMxI4h9sfaDxCymVNpYzRzv.png 742w, https://samiramly.com/media/OGaopo859WSOzhmcDBMxI4h9sfaDxCymVNpYzRzv.png/500w 500w"><figcaption>Unsolvables are a feature, not a bug. Temporarily, until the ML’s ready.</figcaption></figure><p>To offer that same adrenaline rush old-school arcade games are famous for, we also cap the extra time that’s winnable from successfully solving levels back at the countdown’s starting 100sec. This lets our players keep their guard up as they progress — and keeps our levels being diligently <s>solved</s> labeled.</p><p>And the best part, no data whatsoever about any user is needed. Echo Chess simply tracks its own generated level configs, and tags whether they were solved or not. All data collection is 100% <strong>anonymous</strong>, not even <em>'anonymized'</em>. Proud to be supporting <a href="https://plausible.io/about" target="_blank" rel="noopener noreferrer">Plausible.io</a>, a true privacy-obsessed, cookie-less, GDPR-native, open source analytics platform.</p><p>Next, I needed to GTM with this crowdsourcing Trojan Horse. A savvy marketing friend recommended I post a promo video on Instagram (or, God forbid, Tiktok) to reach newcomers from the casual gaming scene.</p><ul><li><p><strong>Pros:</strong> more users = labeled level data = better ML models🤓</p></li><li><p><strong>Cons:</strong> must download insta and learn <a href="https://www.youtube.com/shorts/ynjhhSvUO5o" target="_blank" rel="noopener noreferrer">what kids are into these days</a></p></li></ul><p>Not too proud of what happened next but, hey, data ain’t cheap ngl.</p><figure><img src="https://samiramly.com/media/0HOTPM1qzWsQpnLFnoIJupE3wVsgRFGykkAdQBiw.gif" width="295" height="472"><figcaption>I won’t say I had to use the robot voice, but I won’t deny it either.<br>No clue who the face is at the end btw - is that a Gen Z thing?</figcaption></figure><p>Believe it or not, that lame promo + some trending song really hit it off with the social gods. Soon enough, Echo Chess grew to 1000s of WAUs entirely through word of mouth. I started getting tagged in Twitter conversations from strangers in other languages.</p><figure><img src="https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png" width="443" height="352" srcset="https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png 2216w, https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png/500w 500w, https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png/750w 750w, https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png/1000w 1000w, https://samiramly.com/media/aY6bm4EDA3kFO45FpyoP9kGYfid2QZj6OjVYv9xZ.png/1500w 1500w"><figcaption>Spannend back at you, brothers. Thank Godheid for Google Translate.</figcaption></figure><p>So that was great — it meant I now had real users (happily) playtesting the randomly generated mazes for free, trying their best to beat each one, and automatically tagging a new maze as ‘solvable’ in the background every time they leveled up. Win-win-win. Boom.</p><p>Obviously though, the challenge remained with identifying all the ‘unsolvable’ levels. Trying to crowdsource the labeling of these ones is extremely likely to lead to <em>tons</em> of false negative labels that dirty the data, so it’s a bad idea to delegate it. Just because someone out there couldn’t solve a level doesn’t mean it’s unsolvable. Only the opposite is true.</p><p>What’s the best way to guarantee your data is clean? Collect it yourself.</p><p>To get over the hump of initial bootstrapping, I went ahead and manually tested+tagged 500 different level generations. Yep, 500. Manually.</p><p>Was it time-consuming? Agree to disagree. Did my head hurt? Ice cream helped. Did I enjoy it? You bet. I mean I’m literally solving a puzzle game, it's not like I'm labeling dry paint.</p><figure><img src="https://samiramly.com/media/LUdvA7RJRGZXsu3bzhsziyeli1R0WFe0dW5A68JA.jpg" width="503" height="311" srcset="https://samiramly.com/media/LUdvA7RJRGZXsu3bzhsziyeli1R0WFe0dW5A68JA.jpg 787w, https://samiramly.com/media/LUdvA7RJRGZXsu3bzhsziyeli1R0WFe0dW5A68JA.jpg/500w 500w, https://samiramly.com/media/LUdvA7RJRGZXsu3bzhsziyeli1R0WFe0dW5A68JA.jpg/750w 750w"><figcaption>Pro tip: gamify your ML data collection. Games are a good drug.</figcaption></figure><p>And I’ll let you in on a little secret, but please don’t try this in prod. I was <s>too lazy</s> concerned about getting carpal tunnel from clicking around 100s of levels on my local dev env with a desktop mouse. So I pushed an easter-egg <code>div</code> to the live mobile version of Echo Chess and I used it to tag all ‘unsolvables’ by tapping an invisible pixel on the screen while on the go —the ‘solvables’ continued being tracked automatically upon leveling up.</p><p>Which brings us to today. Here’s the data mining pipeline as it stands: client app -&gt; Flask server -&gt; ReplitDB -&gt; export to csv -&gt; Jupyter nb for analysis + model training.</p><p>From here on out, once we reach some models we feel ‘good enough’ about, we can export them, import them into our Flask server, and handle the Inference stage from there through regular client-server communication.</p><p>But let’s not get ahead of ourselves yet. We continue our post-data-mining journey on the Jupyter side.</p><hr><h2><strong>Finally playing with some data</strong></h2><p>We start by pulling in the labeled levels data into a Jupyter notebook. I’m using Kaggle here, but we can easily switch it up with Colab, our own box, or literally anything.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>import</span><span> pandas </span><span>as</span><span> pd</span></span></p><p><span>2</span><span><span>df </span><span>=</span><span> pd.read_csv(</span><span>'levels-dataset.csv'</span><span>)</span></span></p><p><span>3</span><span><span>df</span></span></p></code></pre><figure><img src="https://samiramly.com/media/H9oJP7JBBphaNzvkkIusK3skvxYuZIc4duEvGIJO.png" srcset="https://samiramly.com/media/H9oJP7JBBphaNzvkkIusK3skvxYuZIc4duEvGIJO.png 1482w, https://samiramly.com/media/H9oJP7JBBphaNzvkkIusK3skvxYuZIc4duEvGIJO.png/500w 500w, https://samiramly.com/media/H9oJP7JBBphaNzvkkIusK3skvxYuZIc4duEvGIJO.png/750w 750w, https://samiramly.com/media/H9oJP7JBBphaNzvkkIusK3skvxYuZIc4duEvGIJO.png/1000w 1000w"><figcaption>5548 rows × 7 columns</figcaption></figure><p>That’s <strong>5,548 </strong><em><strong>labeled, </strong></em><strong>unique, procedurally generated levels.</strong> Every single one of them manually played and certified by a human player as a solvable or unsolvable maze. Now we’re talking🤘</p><p>We seem to have ~40 levels (0.7%) that got mined with an <code>undefined</code> compoundFEN. Let’s just get rid of those to keep things clean. No tears shed.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>df </span><span>=</span><span> df[</span><span>~</span><span>(df[</span><span>'compoundFen'</span><span>] </span><span>==</span><span> </span><span>'undefined'</span><span>)]</span></span></p><p><span>2</span><span><span>df.reset_index(</span><span>drop</span><span>=</span><span>True</span><span>, </span><span>inplace</span><span>=</span><span>True</span><span>)</span></span></p><p><span>3</span><span><span>print</span><span>(</span><span>len</span><span>(df))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>5508</span></span></p></code></pre><p>Done. 5.5k. That’s a nice round number.</p><h3>Train-Test Splits</h3><p>Before we do anything else, we should randomly split the data into three separate sets: <strong>Training</strong>, <strong>Validation</strong> and <strong>Test</strong>.</p><p>Remember taking the SAT? Our friend Bill here does and he’ll take us down memory lane.</p><figure><img src="https://samiramly.com/media/Igg7ZOGbVEfEpfaMdc82TaYENsk3yYEL9Pirc6Wf.png" width="463" height="286" srcset="https://samiramly.com/media/Igg7ZOGbVEfEpfaMdc82TaYENsk3yYEL9Pirc6Wf.png 786w, https://samiramly.com/media/Igg7ZOGbVEfEpfaMdc82TaYENsk3yYEL9Pirc6Wf.png/500w 500w, https://samiramly.com/media/Igg7ZOGbVEfEpfaMdc82TaYENsk3yYEL9Pirc6Wf.png/750w 750w"><figcaption>Sorry Bill, we’re about to dissect your past trauma.</figcaption></figure><p>It only took Bill a few tries to nail those practice sheets that come with answers in the back. Sure, his first two attempts were abysmal. But that’s because the test format is weird. Once Bill saw the correct answers he directly got what the fuss is about. Soon he was acing every single question. Bill’s mom was proud. Her Billy was always such a fast learner.</p><p>Then Bill tried a ‘realistic’ test. Downloaded a fresh test bank, sight-unseen —even timed himself and all. But Bill didn’t get so lucky this time around. No sweat, he’d been there before. Better flunk <em>these</em> than the <em>actual</em> exam. At least Bill was now used to the ‘real’ conditions and the importance of guessing when getting unfamiliar questions.</p><p>By his tenth test, Bill had really perfected his guessing game. He taught his friends tips and tricks, preaching about multiple-choice patterns and Bayesian probabilities of B’s and C’s in sections following alternating A’s and D’s. He showed them charts he’d been plotting of test bank Q&amp;As. Bill had finally conquered the SAT.</p><figure><img src="https://samiramly.com/media/xzqvrROdaL2LcbSF0dVaxuRJwMycX1PuP56ujcYC.png" width="428" height="293" srcset="https://samiramly.com/media/xzqvrROdaL2LcbSF0dVaxuRJwMycX1PuP56ujcYC.png 604w, https://samiramly.com/media/xzqvrROdaL2LcbSF0dVaxuRJwMycX1PuP56ujcYC.png/500w 500w"><figcaption>Real life can be a slap in the face. Tough lessons can really <a href="https://www.youtube.com/watch?v=68L6JA_CnmU" target="_blank" rel="noopener noreferrer">cook you through</a>.</figcaption></figure><p>When Bill finally sat down for the real thing on D-Day, he couldn’t shake that weird feeling that the questions he was getting seemed a bit… <em>different</em>. Still, Bill had an unfair advantage: his genius ‘probablistic guessing’ technique. He could’ve sworn it’s infallible —even thought about patenting it. Yet Bill never bombed so hard as he did that day.</p><p>Could it be that Bill was studying for the wrong version all along?</p><blockquote><p><em>You, my friend, were majorly overfitting. You got good at gaming ‘a’ system, just not the one that mattered.</em></p></blockquote><p>Instead of focusing his learning on <a href="https://www.lesswrong.com/posts/HnPEpu5eQWkbyAJCT/the-simple-math-of-everything" target="_blank" rel="noopener noreferrer">universal concepts</a> that generalize widely, Bill just wasted his time coming up with two-bit parlor tricks to impress a handful of practice tests. And the saddest part: if he hadn’t gone through the three sobering phases, Bill would’ve never even known he sucked.</p><p>Let’s not repeat Bill’s mistake. We want our models to learn actually useful stuff that’s applicable to data they’ve never seen before. The whole point is to test how well we can predict solvability for new <em>unkown</em> levels. So let’s be sure to set some aside that we’ll keep 100% untouched.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span># Split 10% into Test, preserving the `solvability` distribution</span></span></p><p><span>2</span><span><span>df, tst_df </span><span>=</span><span> train_test_split(df, </span><span>test_size</span><span>=</span><span>0.1</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>, </span><span>stratify</span><span>=</span><span>df[</span><span>'solvability'</span><span>])</span></span></p><p><span>3</span><span><wbr></span></p><p><span>4</span><span><span># Split the non-test portion (90%) into 70% Training, 20% Validation</span></span></p><p><span>5</span><span><span>trn_df, val_df </span><span>=</span><span> train_test_split(df, </span><span>test_size</span><span>=</span><span>0.2</span><span>/</span><span>0.9</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>, </span><span>stratify</span><span>=</span><span>df[</span><span>'solvability'</span><span>])</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>print</span><span>(</span><span>len</span><span>(trn_df), </span><span>len</span><span>(val_df), </span><span>len</span><span>(tst_df))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>3855</span><span> </span><span>1102</span><span> </span><span>551</span></span></p></code></pre><p>Great. We’ve now randomly split our levels into:</p><ul><li><p><strong>70% Training Set</strong> in <code>trn_df</code> <em>(3,855/5,508)</em></p></li><li><p><strong>20% Validation Set</strong> in <code>val_df</code> <em>(1,102/5,508)</em></p></li><li><p><strong>10% Test Set</strong> in <code>tst_df</code> <em>(551/5,508)</em></p></li></ul><h3>Data Preprocessing and Feature Engineering</h3><p>Given the knowledge of EchoChess mechanics, we already know there are many features of a level that we could analyze and which could provide some solvability predictive power: things like walls, starting pieces, number of queens, and so on. All this information can be extracted from a level's <code>compoundFen</code>, which means new features can be easily engineered and populated for every row.</p><p>Without boring you with the deets, I wrote a preprocessing function that takes in a pandas df, and returns it with all the extra features that may be of interest. This function is then called like this.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>proc_data(trn_df)</span></span></p><p><span>2</span><span><span>proc_data(val_df)</span></span></p><p><span>3</span><span><span>proc_data(tst_df)</span></span></p></code></pre><p>Here are some of the new columns added by <code>proc_data</code>:</p><ul><li><p><code>levelSize</code></p></li><li><p><code>levelGeoShape</code></p></li><li><p><code>levelOrientation</code></p></li><li><p><code>numObstacleSquares</code></p></li><li><p><code>numConnectedWalls</code></p></li><li><p><code>lengthOfLongestWall</code></p></li><li><p><code>startingWhitePiece</code></p></li><li><p><code>isStartingPieceBlocked</code></p></li><li><p><code>numEndStates</code></p></li><li><p><code>numBlackKnights</code></p></li><li><p><code>numBlackBishops</code></p></li><li><p><code>numBlackRooks</code></p></li><li><p><code>numBlackQueens</code></p></li><li><p><code>numBlackKings</code></p></li><li><p><code>numEmptySpaces</code></p></li><li><p><code>emptySquaresProportion</code></p></li><li><p><code>blackPiecesProportion</code></p></li><li><p><code>obstacleSquaresProportion</code></p></li><li><p>+to encode a certain understanding of the relative spatial positioning of every board state, we generate the following 64 features (for each of the squares in the 8x8 board):</p><ul><li><p>[ content of cell <code>square_xy</code> for every <code>xy</code> position on the board ]</p></li></ul></li></ul><p>This is a sample of what our Training set <code>trn_df</code> looks like right now.</p><figure><img src="https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png" srcset="https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png 4348w, https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png/500w 500w, https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png/750w 750w, https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png/1000w 1000w, https://samiramly.com/media/42mv3m8bJ15vCeO7FiIK7TADt57Vzl8oWFdWcO2a.png/1500w 1500w"><figcaption>3855 rows × 67 columns</figcaption></figure><p>I’ll only call out the implementation of a few interesting features we added that I think showcase well the power and flexibility of the <code>compoundFen</code> format.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>get_starting_white_piece</span><span>(fen):</span></span></p><p><span>2</span><span><span>    </span><span>return</span><span> </span><span>next</span><span>((c </span><span>for</span><span> c </span><span>in</span><span> fen </span><span>if</span><span> c.isupper() </span><span>and</span><span> c </span><span>!=</span><span> </span><span>'X'</span><span>), </span><span>''</span><span>)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>count_black_pieces</span><span>(fen):</span></span></p><p><span>2</span><span><span>    excluded_chars </span><span>=</span><span> [</span><span>'x'</span><span>, </span><span>'w'</span><span>]</span></span></p><p><span>3</span><span><span>    </span><span>return</span><span> </span><span>sum</span><span>(</span><span>1</span><span> </span><span>for</span><span> c </span><span>in</span><span> fen </span><span>if</span><span> c.islower() </span><span>and</span><span> c </span><span>not</span><span> </span><span>in</span><span> excluded_chars)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>count_empty_spaces</span><span>(fen):</span></span></p><p><span>2</span><span><span>    piece_placement </span><span>=</span><span> fen.split(</span><span>' '</span><span>)[</span><span>0</span><span>]</span></span></p><p><span>3</span><span><span>    digits </span><span>=</span><span> [</span><span>int</span><span>(char) </span><span>for</span><span> char </span><span>in</span><span> piece_placement </span><span>if</span><span> char.isdigit()]</span></span></p><p><span>4</span><span><span>    </span><span>return</span><span> </span><span>sum</span><span>(digits)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>hasValidMove</span><span>(rank, file, board):</span></span></p><p><span> 2</span><span><span>    piece </span><span>=</span><span> board[rank][</span><span>file</span><span>].lower()</span></span></p><p><span> 3</span><span><span>    </span><span>if</span><span> piece </span><span>not</span><span> </span><span>in</span><span> [</span><span>'r'</span><span>, </span><span>'b'</span><span>, </span><span>'n'</span><span>, </span><span>'k'</span><span>, </span><span>'q'</span><span>]:</span></span></p><p><span> 4</span><span><span>        </span><span>return</span><span> </span><span>0</span><span>	</span></span></p><p><span> 5</span><span><span>    </span><span>def</span><span> </span><span>outOfBounds</span><span>(i, j):</span></span></p><p><span> 6</span><span><span>        </span><span>return</span><span> (i </span><span>&lt;</span><span> </span><span>0</span><span>) </span><span>or</span><span> (i </span><span>&gt;=</span><span> </span><span>8</span><span>) </span><span>or</span><span> (j </span><span>&lt;</span><span> </span><span>0</span><span>) </span><span>or</span><span> (j </span><span>&gt;=</span><span> </span><span>8</span><span>)</span></span></p><p><span> 7</span><span><span>    </span><span>def</span><span> </span><span>blockedMove</span><span>(dx, dy):</span></span></p><p><span> 8</span><span><span>        </span><span>if</span><span> outOfBounds(rank</span><span>+</span><span>dx, </span><span>file</span><span>+</span><span>dy):</span></span></p><p><span> 9</span><span><span>            </span><span>return</span><span> </span><span>1</span></span></p><p><span>10</span><span><span>		</span><span># check if it hits an obstacle or boundary</span></span></p><p><span>11</span><span><span>        </span><span>return</span><span> (board[rank</span><span>+</span><span>dx][</span><span>file</span><span>+</span><span>dy].lower() </span><span>==</span><span> </span><span>'x'</span><span>)</span></span></p><p><span>12</span><span><span>    blockedCross </span><span>=</span><span> </span><span>all</span><span>(blockedMove(dx, dy) </span><span>for</span><span> dx, dy </span><span>in</span><span> [(</span><span>1</span><span>, </span><span>0</span><span>), (</span><span>-</span><span>1</span><span>, </span><span>0</span><span>), (</span><span>0</span><span>, </span><span>1</span><span>), (</span><span>0</span><span>, </span><span>-</span><span>1</span><span>)])</span></span></p><p><span>13</span><span><span>    blockedDiagonal </span><span>=</span><span> </span><span>all</span><span>(blockedMove(dx, dy) </span><span>for</span><span> dx, dy </span><span>in</span><span> [(</span><span>1</span><span>, </span><span>1</span><span>), (</span><span>-</span><span>1</span><span>, </span><span>1</span><span>), (</span><span>1</span><span>, </span><span>-</span><span>1</span><span>), (</span><span>-</span><span>1</span><span>, </span><span>-</span><span>1</span><span>)])</span></span></p><p><span>14</span><span><span>    l_jumps </span><span>=</span><span> [(</span><span>2</span><span>, </span><span>1</span><span>), (</span><span>2</span><span>, </span><span>-</span><span>1</span><span>), (</span><span>-</span><span>2</span><span>, </span><span>1</span><span>), (</span><span>-</span><span>2</span><span>, </span><span>-</span><span>1</span><span>), (</span><span>1</span><span>, </span><span>2</span><span>), (</span><span>1</span><span>, </span><span>-</span><span>2</span><span>), (</span><span>-</span><span>1</span><span>, </span><span>2</span><span>), (</span><span>-</span><span>1</span><span>, </span><span>-</span><span>2</span><span>)]</span></span></p><p><span>15</span><span><span>    blockedLJump </span><span>=</span><span> </span><span>all</span><span>(blockedMove(dx, dy) </span><span>for</span><span> dx, dy </span><span>in</span><span> l_jumps)   </span></span></p><p><span>16</span><span><span>    </span><span>if</span><span> piece </span><span>==</span><span> </span><span>'r'</span><span> </span><span>and</span><span> blockedCross:</span></span></p><p><span>17</span><span><span>        </span><span>return</span><span> </span><span>0</span></span></p><p><span>18</span><span><span>    </span><span>elif</span><span> piece </span><span>==</span><span> </span><span>'b'</span><span> </span><span>and</span><span> blockedDiagonal:</span></span></p><p><span>19</span><span><span>        </span><span>return</span><span> </span><span>0</span></span></p><p><span>20</span><span><span>    </span><span>elif</span><span> piece </span><span>==</span><span> </span><span>'n'</span><span> </span><span>and</span><span> blockedLJump:</span></span></p><p><span>21</span><span><span>        </span><span>return</span><span> </span><span>0</span></span></p><p><span>22</span><span><span>    </span><span>elif</span><span> (piece </span><span>==</span><span> </span><span>'k'</span><span> </span><span>or</span><span>  piece </span><span>==</span><span> </span><span>'q'</span><span>) </span><span>and</span><span> blockedCross </span><span>and</span><span> blockedDiagonal:</span></span></p><p><span>23</span><span><span>        </span><span>return</span><span> </span><span>0</span><span>     </span></span></p><p><span>24</span><span><span>    </span><span>return</span><span> </span><span>1</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>blocked_starting_piece</span><span>(fen):</span></span></p><p><span>2</span><span><span>    board </span><span>=</span><span> fen_to_board(fen)</span></span></p><p><span>3</span><span><span>    white_pieces </span><span>=</span><span> [</span><span>'R'</span><span>, </span><span>'B'</span><span>, </span><span>'N'</span><span>, </span><span>'K'</span><span>, </span><span>'Q'</span><span>]</span></span></p><p><span>4</span><span><span>    </span><span>for</span><span> rank </span><span>in</span><span> </span><span>range</span><span>(</span><span>8</span><span>):</span></span></p><p><span>5</span><span><span>        </span><span>for</span><span> </span><span>file</span><span> </span><span>in</span><span> </span><span>range</span><span>(</span><span>8</span><span>):</span></span></p><p><span>6</span><span><span>            </span><span>if</span><span> board[rank][</span><span>file</span><span>] </span><span>in</span><span> white_pieces </span><span>and</span><span> </span><span>not</span><span> hasValidMove(rank, </span><span>file</span><span>, board):</span></span></p><p><span>7</span><span><span>                </span><span>return</span><span> </span><span>1</span></span></p><p><span>8</span><span><span>    </span><span>return</span><span> </span><span>0</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>numEndStates</span><span>(fen):</span></span></p><p><span>2</span><span><span>    board </span><span>=</span><span> fen_to_board(fen)</span></span></p><p><span>3</span><span><span>    black_pieces </span><span>=</span><span> [</span><span>'r'</span><span>, </span><span>'b'</span><span>, </span><span>'n'</span><span>, </span><span>'k'</span><span>, </span><span>'q'</span><span>]</span></span></p><p><span>4</span><span><span>    blocked_count </span><span>=</span><span> </span><span>0</span></span></p><p><span>5</span><span><span>    </span><span>for</span><span> rank </span><span>in</span><span> </span><span>range</span><span>(</span><span>8</span><span>):</span></span></p><p><span>6</span><span><span>        </span><span>for</span><span> </span><span>file</span><span> </span><span>in</span><span> </span><span>range</span><span>(</span><span>8</span><span>):</span></span></p><p><span>7</span><span><span>            </span><span>if</span><span> board[rank][</span><span>file</span><span>] </span><span>in</span><span> black_pieces </span><span>and</span><span> </span><span>not</span><span> hasValidMove(rank, </span><span>file</span><span>, board):</span></span></p><p><span>8</span><span><span>                blocked_count </span><span>+=</span><span> </span><span>1</span></span></p><p><span>9</span><span><span>    </span><span>return</span><span> blocked_count</span></span></p></code></pre><p>Each one of the numerous functions like these is then easily applied to the individual rows of the dataframe to create new features. For instance:</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>df[</span><span>'numEndStates'</span><span>] </span><span>=</span><span> df[</span><span>'compoundFen'</span><span>].apply(numEndStates)</span></span></p><p><span>2</span><span><span>df[</span><span>'blackPieces'</span><span>] </span><span>=</span><span> df[</span><span>'compoundFen'</span><span>].apply(getBlackPieces)</span></span></p><p><span>3</span><span><span>df[</span><span>'numRooks'</span><span>] </span><span>=</span><span> df[</span><span>'blackPieces'</span><span>].apply(</span><span>lambda</span><span> x: countSpecificPiece(x, </span><span>'r'</span><span>))</span></span></p></code></pre><p>And so on and so forth.</p><p>In order to simplify our analysis down the line, let’s also make a distinction, within each of the three 90/20/10 datasets<code>trn_df</code>, <code>val_df</code>, and <code>tst_df</code>, between the independent variables (the X’s) and the dependent variable (the Y). Y is the column we’re trying to predict, i.e. <code>solvability</code>. X’s are all the columns we’re using to predict Y.</p><p>Note that this is a <em>column</em> split, <em>not a row</em> split like train-val-test. So the amount of data in each set won’t change.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>def</span><span> </span><span>xs_y</span><span>(df):</span></span></p><p><span>2</span><span><span>    </span><span>...</span></span></p><p><span>3</span><span><span>	</span><span># `cats`, `conts` are names of categorical &amp; continuous independent variables</span></span></p><p><span>4</span><span><span>    xs </span><span>=</span><span> df[cats</span><span>+</span><span>conts].copy()</span></span></p><p><span>5</span><span><span>    </span><span>return</span><span> xs,df[</span><span>'solvability'</span><span>] </span><span>if</span><span> </span><span>'solvability'</span><span> </span><span>in</span><span> df </span><span>else</span><span> </span><span>None</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>trn_xs, trn_y </span><span>=</span><span> xs_y(trn_df)</span></span></p><p><span>2</span><span><span>val_xs, val_y </span><span>=</span><span> xs_y(val_df)</span></span></p><p><span>3</span><span><span>tst_xs, tst_y </span><span>=</span><span> xs_y(tst_df)</span></span></p></code></pre><p>Cool, that was easy. From here, anytime we want to analyze, model or predict the relationship between the X’s and the Y for any of the three train/val/test sets, we can simply use the corresponding pair.</p><h3>Exploratory Data Analysis</h3><p>We can now perform some preliminary EDA on the <em>training set</em>. As you already guessed, we’ll be using <code>trn_xs</code> and <code>trn_y</code> for these plots.</p><figure><img src="https://samiramly.com/media/Tp40o7143O8ykAqBSqa15u5gQAx5GB6cUSqNvXkn.png" srcset="https://samiramly.com/media/Tp40o7143O8ykAqBSqa15u5gQAx5GB6cUSqNvXkn.png 765w, https://samiramly.com/media/Tp40o7143O8ykAqBSqa15u5gQAx5GB6cUSqNvXkn.png/500w 500w, https://samiramly.com/media/Tp40o7143O8ykAqBSqa15u5gQAx5GB6cUSqNvXkn.png/750w 750w"><figcaption>Give a bishop a huge open field, and they’ll still miss out on (exactly) half its glory.<br>If life gives you bishops, trade them for anything else.</figcaption></figure><figure><img src="https://samiramly.com/media/zNAyPP6RLGuiV8E1QFajiWBXxblcngnECBj8MQWS.png" srcset="https://samiramly.com/media/zNAyPP6RLGuiV8E1QFajiWBXxblcngnECBj8MQWS.png 761w, https://samiramly.com/media/zNAyPP6RLGuiV8E1QFajiWBXxblcngnECBj8MQWS.png/500w 500w, https://samiramly.com/media/zNAyPP6RLGuiV8E1QFajiWBXxblcngnECBj8MQWS.png/750w 750w"><figcaption>If there are enough queens to morph into out there somewhere, you’ll probably be fine.</figcaption></figure><figure><img src="https://samiramly.com/media/KwrSDtAFCLPSQzzRxLyLr1KhqcaKoVY9TALrCxUs.png" srcset="https://samiramly.com/media/KwrSDtAFCLPSQzzRxLyLr1KhqcaKoVY9TALrCxUs.png 764w, https://samiramly.com/media/KwrSDtAFCLPSQzzRxLyLr1KhqcaKoVY9TALrCxUs.png/500w 500w, https://samiramly.com/media/KwrSDtAFCLPSQzzRxLyLr1KhqcaKoVY9TALrCxUs.png/750w 750w"><figcaption>The smaller the level, the more constrained the exploration paths become.</figcaption></figure><figure><img src="https://samiramly.com/media/Cl2veiUXgERz9phF2lxlfnivNMmJS8j5jr5z2xH3.png" srcset="https://samiramly.com/media/Cl2veiUXgERz9phF2lxlfnivNMmJS8j5jr5z2xH3.png 762w, https://samiramly.com/media/Cl2veiUXgERz9phF2lxlfnivNMmJS8j5jr5z2xH3.png/500w 500w, https://samiramly.com/media/Cl2veiUXgERz9phF2lxlfnivNMmJS8j5jr5z2xH3.png/750w 750w"><figcaption>More pieces to capture -&gt; more degrees of freedom -&gt; more paths to victory.</figcaption></figure><figure><img src="https://samiramly.com/media/LGElB2zKvFkhvX1NYi9JvZlw7aL63LFilCgdnd8s.png" srcset="https://samiramly.com/media/LGElB2zKvFkhvX1NYi9JvZlw7aL63LFilCgdnd8s.png 764w, https://samiramly.com/media/LGElB2zKvFkhvX1NYi9JvZlw7aL63LFilCgdnd8s.png/500w 500w, https://samiramly.com/media/LGElB2zKvFkhvX1NYi9JvZlw7aL63LFilCgdnd8s.png/750w 750w"><figcaption>GMs should really try sprinkling some obstacles in their Chess matches.</figcaption></figure><figure><img src="https://samiramly.com/media/HiTLz3HLJj5MBmJt9ZIjDQpOxUaWgcWAvJWXMnWo.png" srcset="https://samiramly.com/media/HiTLz3HLJj5MBmJt9ZIjDQpOxUaWgcWAvJWXMnWo.png 810w, https://samiramly.com/media/HiTLz3HLJj5MBmJt9ZIjDQpOxUaWgcWAvJWXMnWo.png/500w 500w, https://samiramly.com/media/HiTLz3HLJj5MBmJt9ZIjDQpOxUaWgcWAvJWXMnWo.png/750w 750w"><figcaption>6x6 ‘core’ level size for Endless mode (excluding boundaries on files a &amp; h or ranks 1 &amp; 8).<br>Can you spot the 4 ‘quad’ centers where knights have 0 moves? (c3, f3, c6, f6)</figcaption></figure><hr><h2>Machine Learning</h2><p>Based on these preliminary EDA observations, let's set up a dummy classifier to get a super quick baseline. We’re just eye-balling the above <code>trn_df</code> plots for now.</p><p>Remember, we train a classifier on <code>trn_df</code> —or more precisely in this case, we ‘eye-ball’ a dummy classifier on <code>trn_df</code>, then we validate our classifier’s predictive power on <code>val_df</code>. The test set <code>tst_df</code>is purposefully never touched! We’ll only come back to it after we’ve selected the winning model using <code>val_df</code> and we’re done with absolutely everything.</p><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>naive_predict</span><span>(df):</span></span></p><p><span> 2</span><span><span>    predictions </span><span>=</span><span> []</span></span></p><p><span> 3</span><span><span>    </span><span>for</span><span> _, row </span><span>in</span><span> df.iterrows():</span></span></p><p><span> 4</span><span><span>        </span><span># having queens on the board helps</span></span></p><p><span> 5</span><span><span>        </span><span>if</span><span> row[</span><span>'numQueens'</span><span>] </span><span>&gt;</span><span> </span><span>0</span><span>:</span></span></p><p><span> 6</span><span><span>            predictions.append(</span><span>True</span><span>)</span></span></p><p><span> 7</span><span><span>        </span><span># having fewer obstacles helps</span></span></p><p><span> 8</span><span><span>        </span><span>elif</span><span> row[</span><span>'obstaclesPortion'</span><span>] </span><span>&lt;</span><span> </span><span>0.25</span><span>:</span></span></p><p><span> 9</span><span><span>            predictions.append(</span><span>True</span><span>)</span></span></p><p><span>10</span><span><span>        </span><span># assume everything else unsolvable</span></span></p><p><span>11</span><span><span>        </span><span>else</span><span>:</span></span></p><p><span>12</span><span><span>            predictions.append(</span><span>False</span><span>)</span></span></p><p><span>13</span><span><span>    </span><span>return</span><span> predictions</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="" data-name=""><code><p><span>1</span><span><span>naive_preds </span><span>=</span><span> naive_predict(val_xs)</span></span></p><p><span>2</span><span><span>print</span><span>(mean_absolute_error(val_y, naive_preds))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="numbers=false" data-name=""><code><p><span><span>0.14065335753176045</span></span></p></code></pre><figure><img src="https://samiramly.com/media/r8oB5ZZo1j1JiSEWxuE2rR78OTIkBFxyWLa8n2kE.png" srcset="https://samiramly.com/media/r8oB5ZZo1j1JiSEWxuE2rR78OTIkBFxyWLa8n2kE.png 463w"><figcaption></figcaption></figure><p>Here’s how to interpret this information:</p><ul><li><p><strong>‘Mean Absolute Error’ (MAE)</strong> = how far off are we, <em>on average</em>, from correctly predicting the solvability (<code>val_y</code>) of each of the <strong>1,102</strong> Echo Chess levels in <code>val_xs</code>. We got <strong>MAE=0.14</strong> here.</p></li><li><p><strong>‘Confusion Matrix’</strong> = fancy term to mean this 2x2 table above which gives a sense of how ‘confused’ our classifier was in its predictions. For every group of solvability <em>actuals</em> (vertical axis), it shows us what our <em>predictions</em> were (horizontal axis).</p><ul><li><p>We correctly predicted ‘unsolvable’ for <strong>27</strong> true unsolvables (True Negatives, TN). Nice.</p></li><li><p>We incorrectly predicted ‘unsolvable’ for <strong>60</strong> levels that were actually solvable (False Negatives, FN). Not so nice.</p></li><li><p>We correctly predicted ‘solvable’ for <strong>920</strong> true solvables (True Positives, TP). Good.</p></li><li><p>We incorrectly predicted ‘solvable’ for <strong>95</strong> levels that were actually unsolvable (False Positives, FP). Not good.</p></li></ul></li></ul><p>Our goal now is to hopefully improve on this naive predictor which will be used as a mininmum baseline for performance.</p><p>Let’s first compare these results to a proper, but simple, <em><strong>Decision Tree</strong></em> to check whether our EDA-eye-balling predictor is that far off. Instead of us splitting hairs on where the nice-looking groups should be separated on visual plots, a tree model will recursively partition our levels based on the features that best separate them into distinct groups —maximizing both heterogeneity across the groups and homogeneity within each.</p><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="h=1" data-name=""><code><p><span> 1</span><span><span>from</span><span> sklearn.tree </span><span>import</span><span> DecisionTreeClassifier, export_graphviz</span></span></p><p><span> 2</span><span><span>import</span><span> graphviz</span></span></p><p><span> 3</span><span><wbr></span></p><p><span> 4</span><span><span>m </span><span>=</span><span> DecisionTreeClassifier(</span><span>max_leaf_nodes</span><span>=</span><span>4</span><span>).fit(trn_xs, trn_y)</span><span>;</span></span></p><p><span> 5</span><span><wbr></span></p><p><span> 6</span><span><span>def</span><span> </span><span>draw_tree</span><span>(t, df, size</span><span>=</span><span>10</span><span>, ratio</span><span>=</span><span>0.6</span><span>, precision</span><span>=</span><span>2</span><span>, </span><span>**</span><span>kwargs):</span></span></p><p><span> 7</span><span><span>    s</span><span>=</span><span>export_graphviz(t, </span><span>out_file</span><span>=</span><span>None</span><span>, </span><span>feature_names</span><span>=</span><span>df.columns, </span><span>filled</span><span>=</span><span>True</span><span>, </span><span>rounded</span><span>=</span><span>True</span><span>,</span></span></p><p><span> 8</span><span><span>                      </span><span>special_characters</span><span>=</span><span>True</span><span>, </span><span>rotate</span><span>=</span><span>False</span><span>, </span><span>precision</span><span>=</span><span>precision, </span><span>**</span><span>kwargs)</span></span></p><p><span> 9</span><span><span>    </span><span>return</span><span> graphviz.Source(re.sub(</span><span>'Tree {'</span><span>, </span><span>f</span><span>'Tree </span><span>{{</span><span> size=</span><span>{</span><span>size</span><span>}</span><span>; ratio=</span><span>{</span><span>ratio</span><span>}</span><span>'</span><span>, s))</span></span></p><p><span>10</span><span><wbr></span></p><p><span>11</span><span><span>draw_tree(m, trn_xs, </span><span>size</span><span>=</span><span>10</span><span>)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/Xw16gi0HBjwK9dXpxrqU16HpR7h0JpTj6mPK7yTv.png" srcset="https://samiramly.com/media/Xw16gi0HBjwK9dXpxrqU16HpR7h0JpTj6mPK7yTv.png 691w, https://samiramly.com/media/Xw16gi0HBjwK9dXpxrqU16HpR7h0JpTj6mPK7yTv.png/500w 500w"><figcaption></figcaption></figure><p>Okay, so it’s first picking up on the proportion of obstacles to board size, just like we did. It has a more precise cut-off —sure, we kinda winged that one. Then it’s looking at the number of capturable pieces on the board, and finally the type of the starting piece. That makes a lot of sense, the EDA plots for these were pretty interesting. Not too bad.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>print</span><span>(mean_absolute_error(val_y, m.predict(val_xs)))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>0.11070780399274047</span></span></p></code></pre><p>MAE at 0.11. Some improvement already. Maybe we'll see even better results with a bigger tree that has more leaves. Feels seasonal enough.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>m </span><span>=</span><span> DecisionTreeClassifier(</span><span>max_leaf_nodes</span><span>=</span><span>16</span><span>)</span></span></p><p><span>2</span><span><span>m.fit(trn_xs, trn_y)</span></span></p><p><span>3</span><span><span>draw_tree(m, trn_xs, </span><span>size</span><span>=</span><span>16</span><span>)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/wSrWj9WuvY4TG9ckKcEBDJyR9vGb4hzHeBREPMYq.png" srcset="https://samiramly.com/media/wSrWj9WuvY4TG9ckKcEBDJyR9vGb4hzHeBREPMYq.png 1239w, https://samiramly.com/media/wSrWj9WuvY4TG9ckKcEBDJyR9vGb4hzHeBREPMYq.png/500w 500w, https://samiramly.com/media/wSrWj9WuvY4TG9ckKcEBDJyR9vGb4hzHeBREPMYq.png/750w 750w, https://samiramly.com/media/wSrWj9WuvY4TG9ckKcEBDJyR9vGb4hzHeBREPMYq.png/1000w 1000w"><figcaption></figcaption></figure><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>print</span><span>(mean_absolute_error(val_y, m.predict(val_xs)))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>0.1161524500907441</span></span></p></code></pre><p>Nope, our average error actually <em>increased</em> a tiny bit. Let’s see if we get improvements with a full-on <em><strong>Random Forest</strong></em> instead.</p><p>Think of an RF as a team of individual Decision Trees working together to make better decisions. Each one takes a random chunk of our levels (rows) and a random subset of their features (columns) and tries to come up with some clever prediction. Then they get together and combine their efforts. It’s like traveling in big groups, only actually enjoyable, not entirely useless, and where decisions do get made.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>from</span><span> sklearn.ensemble </span><span>import</span><span> RandomForestClassifier</span></span></p><p><span>2</span><span><wbr></span></p><p><span>3</span><span><span>rf </span><span>=</span><span> RandomForestClassifier(</span><span>100</span><span>, </span><span>min_samples_leaf</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>)</span></span></p><p><span>4</span><span><span>rf.fit(trn_xs, trn_y)</span></span></p><p><span>5</span><span><wbr></span></p><p><span>6</span><span><span>rf_preds </span><span>=</span><span> rf.predict(val_xs)</span></span></p><p><span>7</span><span><span>print</span><span>(mean_absolute_error(val_y, rf_preds))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>0.10435571687840291</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>confusion_matrix(val_y, rf_preds)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/u1T6Zfu02W3TWnUZBXBmfbp9T2WKplrib6jYsqlh.png" srcset="https://samiramly.com/media/u1T6Zfu02W3TWnUZBXBmfbp9T2WKplrib6jYsqlh.png 461w"><figcaption></figcaption></figure><p>Interesting. MAE looks better at 0.10, but we now seem to almost always be predicting the 'solvable' class, leading to a ridiculous amount of False Positives: 111 unsolvables incorrectly classified as solvable 🤦🏻‍♂️.</p><p>Remember when we data mined all those labeled levels, and how crowdsourcing through gaming was only letting us confidently gather solvables?</p><p>Well this was bound to happen, and we saw it coming, but now we’ve ended up with an imbalanced level dataset where unsolvables are much less common (~1:8 ratio).</p><p>Might as well mindlessly predict ‘solvable’ across the board.</p><figure><img src="https://samiramly.com/media/bA4wo6p5sjdKZ6rNkTVLUJCeEdM9M9IR1ayTxxYB.png" width="425" height="273" srcset="https://samiramly.com/media/bA4wo6p5sjdKZ6rNkTVLUJCeEdM9M9IR1ayTxxYB.png 626w, https://samiramly.com/media/bA4wo6p5sjdKZ6rNkTVLUJCeEdM9M9IR1ayTxxYB.png/500w 500w"><figcaption>He’s not wrong, you know.</figcaption></figure><p>Maybe we can try selecting a threshold higher than 50% for the model to predict <code>True</code>. That’d be, loosely speaking, like asking it to increase its confidence in a level's solvability before saying yes.</p><p>Hmm. What do we choose as a threshold: 60%, 75%, 90%… even higher? They say ML is an iterative science. Let’s <s>throw things at the wall</s> start with an educated guess to get a feel for the impact.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>threshold </span><span>=</span><span> </span><span>0.75</span></span></p><p><span>2</span><span><span>raw_rf_preds </span><span>=</span><span> rf.predict_proba(val_xs)[:, </span><span>1</span><span>]</span></span></p><p><span>3</span><span><span>rf_preds </span><span>=</span><span> (raw_rf_preds </span><span>&gt;=</span><span> threshold).astype(</span><span>int</span><span>)</span></span></p><p><span>4</span><span><wbr></span></p><p><span>5</span><span><span>print</span><span>(mean_absolute_error(val_y, rf_preds))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>0.14791288566243194</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>confusion_matrix(val_y, rf_preds)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/Ad3VVQhJPT5B3z5ZcxFpm2sR4KwXihOiAU9Dcu3b.png" srcset="https://samiramly.com/media/Ad3VVQhJPT5B3z5ZcxFpm2sR4KwXihOiAU9Dcu3b.png 462w"><figcaption></figcaption></figure><p>Look at us ping-ponging. We now have a slightly <em>worse</em> MAE, but the amount of False Positives has significantly improved (almost halved) because we are now demanding a higher confidence threshold.</p><p>I feel like there’s a missing part.</p><hr><h3>Eval Metrics</h3><p>Before we go any further with ML model iterations to see what works best, we need a crystal clear definition of what it means to be ‘best’ for our specific use case, or we might end up optimizing the wrong thing right passed it.</p><blockquote><p><em>“If you aim at nothing, you’ll hit it every time.” —someone who aimed at sounding deep.</em></p></blockquote><p>It’s clear there are trade-offs here for improving some of the metrics at the expense of others. But is there a smarter and more systematic way to tune this threshold hyperparameter? Sure thing, we’ll need to choose a clever point on the Precision-Recall and AUC-ROC curves, as we’ll see below. Defining that point is trickier than it feels.</p><p><strong>Precision vs Recall</strong></p><p>Given that the ultimate goal of all this is to <em>minimize unsolvable levels being served to the user</em>, and given that the procedural generation of new levels is fairly efficient and 'cheap' in some sense, one argument could be to maximize ‘Precision’ (i.e. the proportion of levels classified as 'solvable' when they are, indeed, solvable in reality).</p><figure><img src="https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg" srcset="https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg 1708w, https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg/500w 500w, https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg/750w 750w, https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg/1000w 1000w, https://samiramly.com/media/ggoFYvhCjBHpLE5s2cxd4lyEGutw8ln1tA8havXO.jpg/1500w 1500w"><figcaption>No, I won’t include LaTeX formulas for Precision and Recall.<br>They ++affectation and --intuition.</figcaption></figure><p>You can think of aiming for high precision as requiring a high confidence in the levels we are judging to be 'solvable', even if we happen to also be overconservatively discarding other levels that were borderline solvable but that we preferred classifying as 'unsolvable' just in case.</p><p>In other words, we’d be willing to optimize for high precision <em>even at the expense of a potential drop in ‘Recall’</em> (i.e. even if many potentially valid levels get missed out on through misclassification in our overzealous quest for purity of 'solvables').</p><figure><img src="https://samiramly.com/media/3tSlslIP4mGuczsomOTBp2EzkXgoOXVKacPX753i.png" width="436" height="219" srcset="https://samiramly.com/media/3tSlslIP4mGuczsomOTBp2EzkXgoOXVKacPX753i.png 704w, https://samiramly.com/media/3tSlslIP4mGuczsomOTBp2EzkXgoOXVKacPX753i.png/500w 500w"><figcaption>Recall and True Positive Rate (TPR) are literally the same thing. Gandalf-level renaming.</figcaption></figure><p><strong>FPR vs F1</strong></p><p>Similarly, we also certainly want to <em>minimize the "False Positives Rate" or FPR ratio</em> (i.e. proportion of 'unsolvable' levels that mistakenly get classified as 'solvable', thus ruining our purity of 'solvables'). We can either plot FPR directly for every threshold of our binary classifier, or use the Receiver Operating Characteristic (ROC) curve to analyze the FPR vs TPR (True Positive Rate) trade-off. We’ll do that shortly below.</p><p>However, given the class imbalance and that the majority of our dataset comes with <em><strong>positive</strong></em> labels ('solvable' levels), we can likely <em>expect Precision in this dataset to be slightly less sensitive than FPR to an 'unsolvable' level being misclassified as 'solvable'</em>. This is because precision (and recall) would reflect mostly the ability of prediction of the <em>positive</em> class (solvables), not the <em>negative</em> class (unsolvables) which will naturally be harder to detect due to the smaller number of samples.</p><p>In this case, we can <em>expect FPR to be more sensitive than Precision</em> to the model's actual performance given that the negative class is the minority one in the dataset.</p><p><strong>Therefore, we will look for the optimal threshold range that </strong><em><strong>minimizes</strong></em><strong> FPR, and </strong><em><strong>maximize</strong></em><strong> the F1 Score within that range</strong>.</p><p>Why F1, for that secondary goal? Trying to maximize Precision even further within an <em>already minimized FPR range</em> could lead to undesirable extremes like incredibly low Recall and/or TPR values. Conversely, it could be tempting to search within that range for a 'reasonable-recall' point along the Precision-Recall curve, so that fewer procedurally generated solvables get unnecessarily tossed out. But that's pretty much what F1 is doing for us under the hood either way, so F1 gets us two birds, one stone.</p><figure><img src="https://samiramly.com/media/lGPgzPT9dAo3ohrMXhO0l7aYhG8U3saovmWqO1qB.jpg" width="405" height="831" srcset="https://samiramly.com/media/lGPgzPT9dAo3ohrMXhO0l7aYhG8U3saovmWqO1qB.jpg 500w"><figcaption>The trick is to keep the grip loose enough, or you’ll end up letting go of every last level.</figcaption></figure><p>We then define a max % FPR we are willing to accept for our use case, say, 5%. Remember, this would represent how many of the generated levels that we predict to be solvable - all else equal - end up being, unfortunately, unsolvable. We’ll still have some leeway in how to make use of these generated levels and their predictions in prod, but for now this seems like a reasonable starting point.</p><p>To recap our approach:</p><ul><li><p><strong>For evaluating models:</strong></p><ul><li><p>focus on <code>FPR</code> (rightside ratio in our confusion matrix)</p></li></ul></li><li><p><strong>To select prediction thresholds:</strong></p><ul><li><p>sweep along the thresholds that are <strong>within 5pp of min </strong><code><strong>FPR</strong></code></p></li><li><p>select the one that <strong>maximizes </strong><code><strong>F1</strong></code><strong> among these</strong></p></li><li><p>set that <code>min_fpr_max_f1_threshold</code> as a hyperparameter</p></li></ul></li></ul><p>Here’s the code we’ll need to implement it —minus minutiae for plots and the like. We’ll be plotting a bunch of cool curves for different models but it’s all based on the kernel below.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>from</span><span> sklearn.metrics </span><span>import</span><span> roc_curve, precision_recall_curve</span></span></p><p><span> 2</span><span><wbr></span></p><p><span> 3</span><span><span>def</span><span> </span><span>threshold_tune_binary_classifier</span><span>(val_xs, val_y, model, max_fpr_tolerance</span><span>=</span><span>0.05</span><span>):</span></span></p><p><span> 4</span><span><span>    </span></span></p><p><span> 5</span><span><span>    raw_preds </span><span>=</span><span> model.predict_proba(val_xs)[:, </span><span>1</span><span>]</span></span></p><p><span> 6</span><span><span>    fpr, tpr, thresholds_roc </span><span>=</span><span> roc_curve(val_y, raw_preds, </span><span>drop_intermediate</span><span>=</span><span>False</span><span>)</span></span></p><p><span> 7</span><span><span>    precision, recall, thresholds_pr </span><span>=</span><span> precision_recall_curve(val_y, raw_preds)</span></span></p><p><span> 8</span><span><span>    f1_scores </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> (precision </span><span>*</span><span> recall) </span><span>/</span><span> (precision </span><span>+</span><span> recall)</span></span></p><p><span> 9</span><span><span>    min_fpr_thresholds_range </span><span>=</span><span> thresholds_roc[fpr </span><span>&lt;=</span><span> (</span><span>min</span><span>(fpr) </span><span>+</span><span> max_fpr_tolerance)]</span></span></p><p><span>10</span><span><span>    </span></span></p><p><span>11</span><span><span>    </span><span># Find thresholds, precisions, recalls within min-FPR range    </span></span></p><p><span>12</span><span><span>    sorted_idx </span><span>=</span><span> np.argsort(thresholds_pr)</span></span></p><p><span>13</span><span><span>    idx_range </span><span>=</span><span> np.where((thresholds_pr[sorted_idx] </span><span>&gt;=</span><span> </span><span>min</span><span>(min_fpr_thresholds_range)) </span><span>&amp;</span><span> (thresholds_pr[sorted_idx] </span><span>&lt;=</span><span> </span><span>max</span><span>(min_fpr_thresholds_range)))</span></span></p><p><span>14</span><span><span>    pr_idx_in_range </span><span>=</span><span> </span><span>sorted</span><span>(sorted_idx[idx_range])</span></span></p><p><span>15</span><span><span>    pr_thresholds_in_range </span><span>=</span><span> thresholds_pr[pr_idx_in_range]</span></span></p><p><span>16</span><span><span>    precision_in_range </span><span>=</span><span> precision[pr_idx_in_range]</span></span></p><p><span>17</span><span><span>    recall_in_range </span><span>=</span><span> recall[pr_idx_in_range]</span></span></p><p><span>18</span><span><wbr></span></p><p><span>19</span><span><span>    </span><span># Maximize F1 within min-FPR range</span></span></p><p><span>20</span><span><span>    f1_in_range </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> (precision_in_range </span><span>*</span><span> recall_in_range) </span><span>/</span><span> (precision_in_range </span><span>+</span><span> recall_in_range)</span></span></p><p><span>21</span><span><span>    min_fpr_max_f1_threshold </span><span>=</span><span> pr_thresholds_in_range[np.argmax(f1_in_range)]</span></span></p><p><span>22</span><span><span>    min_fpr_max_f1_index </span><span>=</span><span> </span><span>int</span><span>(np.where(thresholds_pr </span><span>==</span><span> min_fpr_max_f1_threshold)[</span><span>0</span><span>])</span></span></p><p><span>23</span><span><span>    roc_target_index </span><span>=</span><span> </span><span>int</span><span>(np.where(thresholds_roc </span><span>==</span><span> min_fpr_max_f1_threshold)[</span><span>0</span><span>])</span></span></p><p><span>24</span><span><wbr></span></p><p><span>25</span><span><span>    </span><span>return</span><span> {</span></span></p><p><span>26</span><span><span>        </span><span>'threshold'</span><span>: min_fpr_max_f1_threshold,</span></span></p><p><span>27</span><span><span>        </span><span>'precision'</span><span>: precision[min_fpr_max_f1_index],</span></span></p><p><span>28</span><span><span>        </span><span>'recall'</span><span>: recall[min_fpr_max_f1_index],</span></span></p><p><span>29</span><span><span>        </span><span>'f1'</span><span>: f1_scores[min_fpr_max_f1_index],</span></span></p><p><span>30</span><span><span>        </span><span>'fpr'</span><span>: fpr[roc_target_index],</span></span></p><p><span>31</span><span><span>        </span><span>'tpr'</span><span>: tpr[roc_target_index]</span></span></p><p><span>32</span><span><span>    }</span></span></p></code></pre><h3>Model Selection and Tuning</h3><p>Great. So now we have data, features, metrics, and a proper way to select suitable models. Let’s spin up some proper tabular models and see what’s up.</p><h3>Tabular models</h3><p><strong>Random Forest</strong></p><p>Remember our <code>rf</code>? Let’s retune this model to improve the metrics <em>we actually</em> care about.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>rf_targets </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, rf)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.9505685425685425</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.63</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>43.98</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>60.83</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>43.98</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.92</span><span>%</span></span></p></code></pre><p>And I’ve got some snazzy plots for this <code>rf</code> for us to look at. Don’t worry if things look a bit confusing, we’ll go through them together in a sec.</p><figure><img src="https://samiramly.com/media/3FdVM8GtCSMMteYuXk0iYwL5zXhtEtN7zxL92bOy.png" srcset="https://samiramly.com/media/3FdVM8GtCSMMteYuXk0iYwL5zXhtEtN7zxL92bOy.png 957w, https://samiramly.com/media/3FdVM8GtCSMMteYuXk0iYwL5zXhtEtN7zxL92bOy.png/500w 500w, https://samiramly.com/media/3FdVM8GtCSMMteYuXk0iYwL5zXhtEtN7zxL92bOy.png/750w 750w"><figcaption>Left: FPR for every prediction threshold we could set. We’re almost hitting our 5% max tolerance.<br>Right: F1 for every possible threshold. That’s the max we could get given our FPR constraint.<br>(X-axis the same for both)</figcaption></figure><figure><img src="https://samiramly.com/media/hVIZOep3fJ5VrstnZaTAYkHRZVNVW0ab2CsHBZRm.png" srcset="https://samiramly.com/media/hVIZOep3fJ5VrstnZaTAYkHRZVNVW0ab2CsHBZRm.png 969w, https://samiramly.com/media/hVIZOep3fJ5VrstnZaTAYkHRZVNVW0ab2CsHBZRm.png/500w 500w, https://samiramly.com/media/hVIZOep3fJ5VrstnZaTAYkHRZVNVW0ab2CsHBZRm.png/750w 750w"><figcaption>Left: Precision for every possible threshold. As expected, FPR gets us a good value here ‘for free’.<br>Right: Corresponding Recall at each Precision value. Again, F1 lands us on a solid point ‘for free’.<br>(Y-axis the same for both)</figcaption></figure><figure><img src="https://samiramly.com/media/78KgRS9qyftcGn3YmvpHzEYKHAiuxUo8pFSGn6bL.png" width="329" height="331" srcset="https://samiramly.com/media/78KgRS9qyftcGn3YmvpHzEYKHAiuxUo8pFSGn6bL.png 478w"><figcaption>Bonus: Corresponding TPR (i.e. Recall) for every FPR value. This one specifically illustrates the trade-off of making overconservative predictions. But it’s a little redundant for what we need.</figcaption></figure><p>What does this all mean?</p><p>It means that for this model and dataset, if we set an overly conservative prediction threshold (i.e. has to be &gt;0.95 to be considered solvable, <em>everything</em> else is assumed not), we can drop the False Positive rate down to 4.9% and get a pretty high Precision of 98.6%. In other words, we’d be pretty confident that the solvable levels we choose are actually solvable.</p><p>Evidently this comes at a cost. We’d only be getting a True Positive rate of 44%, so we’d be throwing out 1 of every ~2.2 actually solvable levels, say half of them. But that’s the best that <em>this</em> model can give us for <em>our</em> data given the metrics <em>we</em> care to optimize. Not too shabby tbh.</p><figure><img src="https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png" srcset="https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png 3624w, https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png/500w 500w, https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png/750w 750w, https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png/1000w 1000w, https://samiramly.com/media/g5oDpfHcH5HMn7SEzanrOTgH4dyyHhFDXMOa5FAU.png/1500w 1500w"><figcaption>Left: orginal RF before tuning a threshold. Right: after tuning for min-FPR, max-F1.<br>We completely reversed almost all the FPs at the cost of only ~half the TPs.<br>This TP loss would look scary if we hadn’t purposefully defined our goals.</figcaption></figure><p>All in all, this looks promising. We’ll probably end up using multiple models that can pick up on different things and ensemble them together. Let’s add this model to our <code>models_predictions</code> ensembling dict.</p><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="" data-name=""><code><p><span>1</span><span><span>raw_preds </span><span>=</span><span> rf.predict_proba(val_xs)[:, </span><span>1</span><span>]&nbsp;</span></span></p><p><span>2</span><span><span>rf_threshold_target </span><span>=</span><span> rf_targets[</span><span>'threshold'</span><span>]</span></span></p><p><span>3</span><span><span>predictions </span><span>=</span><span> (raw_preds </span><span>&gt;=</span><span> rf_threshold_target).astype(</span><span>int</span><span>)</span></span></p><p><span>4</span><span><span>models_predictions[</span><span>'Random-Forest-tuned'</span><span>] </span><span>=</span><span> {</span></span></p><p><span>5</span><span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>'tuned_threshold'</span><span>: rf_threshold_target,</span></span></p><p><span>6</span><span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>'raw_predictions'</span><span>: raw_preds,</span></span></p><p><span>7</span><span><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>'class_predictions'</span><span>: predictions</span></span></p><p><span>8</span><span><span>}</span></span></p></code></pre><p><strong>Balanced Random Forest</strong></p><p>Another approach is to use a <em><strong>Balanced</strong></em><strong> </strong>Random Forest (BRF) which could also help reduce the rate of false positives caused by our imbalanced dataset.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>from</span><span> imblearn.ensemble </span><span>import</span><span> BalancedRandomForestClassifier</span></span></p><p><span>2</span><span><wbr></span></p><p><span>3</span><span><span>brf </span><span>=</span><span> BalancedRandomForestClassifier(</span><span>n_estimators</span><span>=</span><span>100</span><span>, </span><span>min_samples_leaf</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>)</span></span></p><p><span>4</span><span><span>brf.fit(trn_xs, trn_y)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/2JKvdYSXG6KyVIjWbVSHQyx0OslIKz9NzFCA4EOB.png" srcset="https://samiramly.com/media/2JKvdYSXG6KyVIjWbVSHQyx0OslIKz9NzFCA4EOB.png 467w"><figcaption></figcaption></figure><p>This has much less FPs than the PRE-tuning unbalanced Random Forest, but it does not beat the threshold-tuned version of our unbalanced rf, given <em>our </em>metrics goals. Let's see if we can combine both approaches by threshold-tuning the BRF.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>brf_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, brf, max_fpr_tolerance)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.6988891533303299</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.67</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>45.51</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>62.29</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>45.51</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.92</span><span>%</span></span></p></code></pre><figure><img src="https://samiramly.com/media/LzX4JbRtacdyhIKRQKoF2iV8FQqMbTByLLnP9vT4.png" srcset="https://samiramly.com/media/LzX4JbRtacdyhIKRQKoF2iV8FQqMbTByLLnP9vT4.png 459w"><figcaption></figcaption></figure><p>This tuned-BRF has very comparable FPR and Precision to the unbalanced tuned-RF, while also having slightly TPR and F1. We’ll add it to the ensemble as well —same code with <code>brf</code> instead of <code>rf</code>.</p><p><strong>XGBoost (eXtreme Gradient Boosting)</strong></p><p>I’m thinking it’s worth trying to train an XGBoost model instead —XGBoost tends to handle unbalanced datasets slightly better, so it could be a good fit in this case.</p><p>Remember the team analogy we touched on for random forests? XGBoost is like that smart kid who added some structure around the decision-making team. You’re still creating a bunch of trees, but not entirely randomly —instead you try to fix the mistakes of the previous trees as you go, paying extra attention to the errors and adjusting each new tree accordingly. Hence the boosting name.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>import</span><span> xgboost </span><span>as</span><span> xgb</span></span></p><p><span>2</span><span><wbr></span></p><p><span>3</span><span><span>xgb_model </span><span>=</span><span> xgb.XGBClassifier(</span><span>n_estimators</span><span>=</span><span>100</span><span>, </span><span>min_child_weight</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>, </span><span>scale_pos_weight</span><span>=</span><span>sum</span><span>(trn_y</span><span>==</span><span>0</span><span>)</span><span>/</span><span>sum</span><span>(trn_y</span><span>==</span><span>1</span><span>))</span></span></p><p><span>4</span><span><span>xgb_model.fit(trn_xs, trn_y)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/oeJypD7T1f4I1jAW7nlotGyiPbQBQpSehTvpvZNQ.png" srcset="https://samiramly.com/media/oeJypD7T1f4I1jAW7nlotGyiPbQBQpSehTvpvZNQ.png 465w"><figcaption></figcaption></figure><p>This first attempt of XGBoost seems to have a fairly high FPR. Let's try threshold-tuning it see if it can fit our needs better.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>xgb_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, xgb_model, max_fpr_tolerance)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.9594321846961975</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.92</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>46.94</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>63.67</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>46.94</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.10</span><span>%</span></span></p></code></pre><figure><img src="https://samiramly.com/media/KlbKNEjmsQq8rgFfeAgur103RVyAWa9ZFoB92Rqr.png" srcset="https://samiramly.com/media/KlbKNEjmsQq8rgFfeAgur103RVyAWa9ZFoB92Rqr.png 463w"><figcaption></figcaption></figure><p>Look at that gorgeous baby. Better vitals across the board: lower FPR, higher Precision, higher Recall/TPR, higher F1. Sold to the ensemble in the back.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>raw_preds </span><span>=</span><span> xgb_model.predict_proba(val_xs)[:, </span><span>1</span><span>]</span></span></p><p><span>2</span><span><span>predictions </span><span>=</span><span> (raw_preds </span><span>&gt;=</span><span> xgb_threshold_target).astype(</span><span>int</span><span>)</span></span></p><p><span>3</span><span><span>models_predictions[</span><span>'XGBoost-tuned'</span><span>] </span><span>=</span><span> {</span></span></p><p><span>4</span><span><span>    </span><span>'tuned_threshold'</span><span>: xgb_threshold_target,</span></span></p><p><span>5</span><span><span>    </span><span>'raw_predictions'</span><span>: raw_preds,</span></span></p><p><span>6</span><span><span>    </span><span>'class_predictions'</span><span>: predictions</span></span></p><p><span>7</span><span><span>}</span></span></p></code></pre><p><strong>Feature Importance</strong></p><p>Another interesting approach we can try is to only select the most important features according to the random forest, and then train our model only on these features, disregarding all others. The reason this works in many cases is because it can help reduce noise, multicollinearity with less relevant features, and make the model less prone to overfitting.</p><p>Random Forests are usually good at identifying feature importance in a dataset. Let’s take a look using our <code>rf</code> model.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>get_top_features_importance</span><span>(model, trn_xs, min_imp</span><span>=</span><span>0</span><span>, num_top_features</span><span>=</span><span>40</span><span>):</span></span></p><p><span> 2</span><span><span>    plt.figure(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>, </span><span>8</span><span>))</span></span></p><p><span> 3</span><span><span>    feature_importances </span><span>=</span><span> model.feature_importances_    </span></span></p><p><span> 4</span><span><span>    df </span><span>=</span><span> pd.DataFrame(</span><span>dict</span><span>(</span><span>cols</span><span>=</span><span>trn_xs.columns, </span><span>imp</span><span>=</span><span>feature_importances))</span></span></p><p><span> 5</span><span><span>    df </span><span>=</span><span> df[df[</span><span>'imp'</span><span>] </span><span>&gt;</span><span> min_imp].sort_values(</span><span>by</span><span>=</span><span>'imp'</span><span>).tail(num_top_features)</span></span></p><p><span> 6</span><span><span>    ax </span><span>=</span><span> df.plot(</span><span>'cols'</span><span>, </span><span>'imp'</span><span>, </span><span>kind</span><span>=</span><span>'barh'</span><span>, </span><span>legend</span><span>=</span><span>False</span><span>, </span><span>xlabel</span><span>=</span><span>''</span><span>, </span><span>ylabel</span><span>=</span><span>''</span><span>)</span></span></p><p><span> 7</span><span><span>    plt.show()</span></span></p><p><span> 8</span><span><span>    </span><span>return</span><span> df.sort_values(</span><span>by</span><span>=</span><span>'imp'</span><span>, </span><span>ascending</span><span>=</span><span>False</span><span>)</span></span></p><p><span> 9</span><span><wbr></span></p><p><span>10</span><span><span>rf_top_features </span><span>=</span><span> get_top_features_importance(rf, trn_xs, </span><span>min_imp</span><span>=</span><span>0.02</span><span>)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/BqmAd59A6aBwmeoqUImeTLxAWJQIi76duwxb6ihq.png" srcset="https://samiramly.com/media/BqmAd59A6aBwmeoqUImeTLxAWJQIi76duwxb6ihq.png 529w, https://samiramly.com/media/BqmAd59A6aBwmeoqUImeTLxAWJQIi76duwxb6ihq.png/500w 500w"><figcaption></figcaption></figure><p>Now let’s retrain our random forest only on these top features. We’ll make sure to threshold-tune it as well.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>imp_trn_xs </span><span>=</span><span> trn_xs[rf_top_features.cols]</span></span></p><p><span>2</span><span><span>imp_val_xs </span><span>=</span><span> val_xs[rf_top_features.cols]</span></span></p><p><span>3</span><span><wbr></span></p><p><span>4</span><span><span>rf_top_feats </span><span>=</span><span> RandomForestClassifier(</span><span>n_estimators</span><span>=</span><span>100</span><span>, </span><span>min_samples_leaf</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>)</span></span></p><p><span>5</span><span><span>rf_top_feats.fit(imp_trn_xs, trn_y)</span></span></p><p><span>6</span><span><wbr></span></p><p><span>7</span><span><span>top_feats_rf_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(imp_val_xs, val_y, rf_top_feats, max_fpr_tolerance)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.9614293206793206</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.72</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>47.14</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>63.81</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>47.14</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.92</span><span>%</span></span></p></code></pre><figure><img src="https://samiramly.com/media/Dd1mowOd1ENOCGYBNTrmmFltF2MvB5mCLo45iewZ.png" srcset="https://samiramly.com/media/Dd1mowOd1ENOCGYBNTrmmFltF2MvB5mCLo45iewZ.png 474w"><figcaption></figcaption></figure><figure><img src="https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png" srcset="https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png 2212w, https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png/500w 500w, https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png/750w 750w, https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png/1000w 1000w, https://samiramly.com/media/YrLrSv6MRl0HVF4QynPQoav5e8D3jzXQcnL0GdFG.png/1500w 1500w"><figcaption></figcaption></figure><figure><img src="https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png" srcset="https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png 2224w, https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png/500w 500w, https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png/750w 750w, https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png/1000w 1000w, https://samiramly.com/media/s4reOxHdCCCbfyXUQSkeUYpDvKfAylL2v58qM8gd.png/1500w 1500w"><figcaption></figcaption></figure><p>Anoter model giving great results. Throw it in the ensemble bag. You know the drill.</p><p>Alternatively, we could check if there’s a better feature selection subset according to the feature importance interpretation of an XGBoost model as opposed to the Random Forest. In general, these should be close enough but it doesn’t hurt to check for our data.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>xgb_top_features_importance</span><span>(xgb_model, importance_type, min_imp</span><span>=</span><span>0</span><span>, num_top_features</span><span>=</span><span>40</span><span>):</span></span></p><p><span> 2</span><span><span>    xgb.plot_importance(xgb_model, </span><span>importance_type</span><span>=</span><span>importance_type, </span><span>max_num_features</span><span>=</span><span>num_top_features,</span></span></p><p><span> 3</span><span><span>                        </span><span>title</span><span>=</span><span>f</span><span>"Feature </span><span>{</span><span>importance_type</span><span>}</span><span> importance"</span><span>, </span><span>xlabel</span><span>=</span><span>f</span><span>"</span><span>{</span><span>importance_type</span><span>}</span><span>s"</span><span>)</span></span></p><p><span> 4</span><span><span>    xgb_imps_dict </span><span>=</span><span> xgb_model.get_booster().get_score(</span><span>importance_type</span><span>=</span><span>importance_type)</span></span></p><p><span> 5</span><span><span>    xgb_imps_dict </span><span>=</span><span> </span><span>dict</span><span>(</span><span>sorted</span><span>(xgb_imps_dict.items(), </span><span>key</span><span>=lambda</span><span> item: item[</span><span>1</span><span>], </span><span>reverse</span><span>=</span><span>True</span><span>))</span></span></p><p><span> 6</span><span><span>    df </span><span>=</span><span> pd.DataFrame(</span><span>list</span><span>(xgb_imps_dict.items()), </span><span>columns</span><span>=</span><span>[</span><span>'features'</span><span>, </span><span>f</span><span>"</span><span>{</span><span>importance_type</span><span>}</span><span>_importance"</span><span>]).head(num_top_features)</span></span></p><p><span> 7</span><span><span>    df </span><span>=</span><span> df[ df[</span><span>f</span><span>"</span><span>{</span><span>importance_type</span><span>}</span><span>_importance"</span><span>] </span><span>&gt;=</span><span> min_imp ]</span></span></p><p><span> 8</span><span><span>    </span><span>return</span><span> df</span></span></p><p><span> 9</span><span><span>	</span></span></p><p><span>10</span><span><span>xgb_new </span><span>=</span><span> xgb.XGBClassifier(</span><span>n_estimators</span><span>=</span><span>100</span><span>, </span><span>min_child_weight</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>, </span><span>scale_pos_weight</span><span>=</span><span>sum</span><span>(trn_y</span><span>==</span><span>0</span><span>)</span><span>/</span><span>sum</span><span>(trn_y</span><span>==</span><span>1</span><span>))</span></span></p><p><span>11</span><span><span>xgb_new.fit(trn_xs, trn_y)</span></span></p><p><span>12</span><span><span>xgb_top_features </span><span>=</span><span> xgb_top_features_importance(xgb_new, </span><span>importance_type</span><span>=</span><span>"gain"</span><span>, </span><span>num_top_features</span><span>=</span><span>15</span><span>)</span></span></p><p><span>13</span><span><span>x_imp_trn_xs </span><span>=</span><span> trn_xs[xgb_top_features.features]</span></span></p><p><span>14</span><span><span>x_imp_val_xs </span><span>=</span><span> val_xs[xgb_top_features.features]</span></span></p></code></pre><figure><img src="https://samiramly.com/media/ENUZpEl9qmY49zhrCewQVmDz15ReBd8zu0vAmKva.png" srcset="https://samiramly.com/media/ENUZpEl9qmY49zhrCewQVmDz15ReBd8zu0vAmKva.png 874w, https://samiramly.com/media/ENUZpEl9qmY49zhrCewQVmDz15ReBd8zu0vAmKva.png/500w 500w, https://samiramly.com/media/ENUZpEl9qmY49zhrCewQVmDz15ReBd8zu0vAmKva.png/750w 750w"><figcaption>XGB assigns a somewhat different ranking here and there, but the overall collection is similar.</figcaption></figure><p>Interestingly, when we retrain the XGB model on <code>xgb_top_features</code> instead of <code>rf_top_features</code>, we get slightly <em>worse</em> results. Even varying the <code>importance_type</code> XGB uses to select the top features (across gain, cover or weight) doesn’t improve so much over the good old <code>rf_top_features</code> approach. We’ll just keep it simple and stick to the original appraoch we had.</p><p>I’d say we have a decent start so far with these tabular models. There’s something much more intriguing we might be missing out on, though. I’m sure some of you reading this might have been waiting for it from the moment we uttered the word ‘classification’.</p><h3>CNNs and image classification</h3><p><strong>CNN classifier using chess board arrays</strong></p><p>Taking inspiration from image recognition approaches, since 2D chess boards are very structured and always come in the same form with a cell containing one of a predefined set of classes, we can train a Convolutional Neural Network (CNN) on the 2D chess board states extracted from every level's FEN.</p><ul><li><p>(1) We’d need 13 categories for the content of a cell given that pawns are excluded here (K, Q, B, N, R, k, q, b, n, r, _, x, X).</p></li><li><p>(2) We then one-hot-encode each of these 13 to avoid any implicit relative ranking between them.</p></li><li><p>(3) Every one-hot-encoding of the 13 can now be considered a separate channel for the CNN.</p></li><li><p>(4) So our input tensor becomes (numSamples, 13, 8, 8) for every 8x8 chess board. Note that since the biggest levels in Endless mode are of size 6x6, we could also preconvert all 2D boards to 6x6 and make the input tensor (numSamples, 13, 6, 6) if necessary.</p></li></ul><figure><img src="https://samiramly.com/media/DhAwDPuBQhsJ40R4.png" width="498" height="416" srcset="https://samiramly.com/media/DhAwDPuBQhsJ40R4.png 5540w, https://samiramly.com/media/DhAwDPuBQhsJ40R4.png/500w 500w, https://samiramly.com/media/DhAwDPuBQhsJ40R4.png/750w 750w, https://samiramly.com/media/DhAwDPuBQhsJ40R4.png/1000w 1000w, https://samiramly.com/media/DhAwDPuBQhsJ40R4.png/1500w 1500w"><figcaption>A chess board is literally this. But instead of RGB, it’s KQRBN. Same stuff, more channels.<br>It’s all tensors all the way down regardless.</figcaption></figure><p>This approach might even capture the essence of the spatial data and relative positioning of pieces in a level in a <em>better</em> way than img classification of the rendered level from the Echo Chess app, because only the 'substance' of a level's config is being trained on, without all the noise and variability linked to unnecessary visual elements.</p><p>On the flip side, though, this may also make it challenging to simply fine-tune a pretrained SOTA CNN or img classifier from the web since the input format could become substantially different from what is expected.</p><p>In any case, we can see how well a basic neural net performs in this approach, and then take it from there. We’ll need to prepare the input data accordingly.</p><pre onmouseenter="" onmouseleave="" data-language="python" data-annotations="" data-name=""><code><p><span>1</span><span><span>trn_X </span><span>=</span><span> np.array([fen_to_board(x) </span><span>for</span><span> x </span><span>in</span><span> trn_df[</span><span>'compoundFen'</span><span>].values])</span></span></p><p><span>2</span><span><span>one_hot_trn_X </span><span>=</span><span> np.array([one_hot_encode_2D_array(x, char_to_index) </span><span>for</span><span> x </span><span>in</span><span> trn_X])</span></span></p><p><span>3</span><span><span>trn_X_tensors </span><span>=</span><span> torch.tensor(one_hot_trn_X)</span></span></p></code></pre><p>We’ll also do the same for <code>val_X</code>, <code>trn_y</code>, and <code>val_y</code>.</p><p>To choose a CNN architecture, let’s first start with a simple baseline and we can iterate from there. In general, most CNNs can be thought of as pretty much some conv layers, normalization, and activation functions, rinse and repeat. We’ll use the following simple arch as a start.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>class</span><span> </span><span>ChessCNN</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span></span></p><p><span> 2</span><span><span>    </span><span>def</span><span> </span><span>__init__</span><span>(self):</span></span></p><p><span> 3</span><span><span>        </span><span>super</span><span>(ChessCNN, </span><span>self</span><span>).</span><span>__init__</span><span>()</span></span></p><p><span> 4</span><span><span>        </span><span># Convolutional Layer 1</span></span></p><p><span> 5</span><span><span>        </span><span>self</span><span>.conv1 </span><span>=</span><span> nn.Conv2d(</span><span>in_channels</span><span>=</span><span>13</span><span>, </span><span>out_channels</span><span>=</span><span>16</span><span>, </span><span>kernel_size</span><span>=</span><span>3</span><span>, </span><span>stride</span><span>=</span><span>1</span><span>)</span></span></p><p><span> 6</span><span><span>        </span><span># Max Pooling Layer 1</span></span></p><p><span> 7</span><span><span>        </span><span>self</span><span>.maxpool1 </span><span>=</span><span> nn.MaxPool2d(</span><span>kernel_size</span><span>=</span><span>2</span><span>, </span><span>stride</span><span>=</span><span>2</span><span>)</span></span></p><p><span> 8</span><span><span>        </span><span># Convolutional Layer 2</span></span></p><p><span> 9</span><span><span>        </span><span>self</span><span>.conv2 </span><span>=</span><span> nn.Conv2d(</span><span>in_channels</span><span>=</span><span>16</span><span>, </span><span>out_channels</span><span>=</span><span>32</span><span>, </span><span>kernel_size</span><span>=</span><span>3</span><span>, </span><span>stride</span><span>=</span><span>1</span><span>)</span></span></p><p><span>10</span><span><span>        </span><span># Fully Connected Layer 1</span></span></p><p><span>11</span><span><span>        </span><span>self</span><span>.fc1 </span><span>=</span><span> nn.Linear(</span><span>in_features</span><span>=</span><span>32</span><span>, </span><span>out_features</span><span>=</span><span>64</span><span>)</span></span></p><p><span>12</span><span><span>        </span><span># Fully Connected Layer 2 (Output Layer)</span></span></p><p><span>13</span><span><span>        </span><span>self</span><span>.fc2 </span><span>=</span><span> nn.Linear(</span><span>in_features</span><span>=</span><span>64</span><span>, </span><span>out_features</span><span>=</span><span>1</span><span>)</span></span></p><p><span>14</span><span><wbr></span></p><p><span>15</span><span><span>    </span><span>def</span><span> </span><span>forward</span><span>(self, x):</span></span></p><p><span>16</span><span><span>        x </span><span>=</span><span> F.relu(</span><span>self</span><span>.conv1(x))</span></span></p><p><span>17</span><span><span>        x </span><span>=</span><span> </span><span>self</span><span>.maxpool1(x)</span></span></p><p><span>18</span><span><span>        x </span><span>=</span><span> F.relu(</span><span>self</span><span>.conv2(x))</span></span></p><p><span>19</span><span><span>        x </span><span>=</span><span> torch.flatten(x, </span><span>1</span><span>)</span></span></p><p><span>20</span><span><span>        x </span><span>=</span><span> F.relu(</span><span>self</span><span>.fc1(x))</span></span></p><p><span>21</span><span><span>        x </span><span>=</span><span> torch.sigmoid(</span><span>self</span><span>.fc2(x))</span></span></p><p><span>22</span><span><span>        </span><span>return</span><span> x</span></span></p></code></pre><p>This is what our basic <code>cnn</code> looks like:</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>ChessCNN(</span></span></p><p><span>2</span><span><span>  (conv1): Conv2d(</span><span>13</span><span>, </span><span>16</span><span>, </span><span>kernel_size</span><span>=</span><span>(</span><span>3</span><span>, </span><span>3</span><span>), </span><span>stride</span><span>=</span><span>(</span><span>1</span><span>, </span><span>1</span><span>))</span></span></p><p><span>3</span><span><span>  (maxpool1): MaxPool2d(</span><span>kernel_size</span><span>=</span><span>2</span><span>, </span><span>stride</span><span>=</span><span>2</span><span>, </span><span>padding</span><span>=</span><span>0</span><span>, </span><span>dilation</span><span>=</span><span>1</span><span>, </span><span>ceil_mode</span><span>=</span><span>False</span><span>)</span></span></p><p><span>4</span><span><span>  (conv2): Conv2d(</span><span>16</span><span>, </span><span>32</span><span>, </span><span>kernel_size</span><span>=</span><span>(</span><span>3</span><span>, </span><span>3</span><span>), </span><span>stride</span><span>=</span><span>(</span><span>1</span><span>, </span><span>1</span><span>))</span></span></p><p><span>5</span><span><span>  (fc1): Linear(</span><span>in_features</span><span>=</span><span>32</span><span>, </span><span>out_features</span><span>=</span><span>64</span><span>, </span><span>bias</span><span>=</span><span>True</span><span>)</span></span></p><p><span>6</span><span><span>  (fc2): Linear(</span><span>in_features</span><span>=</span><span>64</span><span>, </span><span>out_features</span><span>=</span><span>1</span><span>, </span><span>bias</span><span>=</span><span>True</span><span>)</span></span></p><p><span>7</span><span><span>)</span></span></p></code></pre><p>We’ll train this <code>cnn</code> model for 8 epochs with the Binary Cross-Entropy (BCE) as the Loss Function, Adam as the optimizer, and a Learning Rate of 0.002. Why these choices? That’s what worked best in my experiments on this data.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>cnn </span><span>=</span><span> ChessCNN()</span></span></p><p><span> 2</span><span><span>criterion </span><span>=</span><span> nn.BCELoss()</span></span></p><p><span> 3</span><span><span>optimizer </span><span>=</span><span> optim.Adam(cnn.parameters(), </span><span>lr</span><span>=</span><span>0.002</span><span>)</span></span></p><p><span> 4</span><span><span>num_epochs </span><span>=</span><span> </span><span>8</span></span></p><p><span> 5</span><span><span>batch_size </span><span>=</span><span> </span><span>8</span></span></p><p><span> 6</span><span><span>for</span><span> epoch </span><span>in</span><span> </span><span>range</span><span>(num_epochs):</span></span></p><p><span> 7</span><span><span>    cnn.train()</span></span></p><p><span> 8</span><span><span>    </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>0</span><span>, </span><span>len</span><span>(trn_X_tensors), batch_size):</span></span></p><p><span> 9</span><span><span>        inputs </span><span>=</span><span> trn_X_tensors[i:i</span><span>+</span><span>batch_size].float()</span></span></p><p><span>10</span><span><span>        labels </span><span>=</span><span> torch.tensor(trn_y[i:i</span><span>+</span><span>batch_size], </span><span>dtype</span><span>=</span><span>torch.float32).view(</span><span>-</span><span>1</span><span>, </span><span>1</span><span>)</span></span></p><p><span>11</span><span><span>        optimizer.zero_grad()</span></span></p><p><span>12</span><span><span>        outputs </span><span>=</span><span> cnn(inputs)</span></span></p><p><span>13</span><span><span>        loss </span><span>=</span><span> criterion(outputs, labels)</span></span></p><p><span>14</span><span><span>        loss.backward()</span></span></p><p><span>15</span><span><span>        optimizer.step()</span></span></p></code></pre><p>Now that we have a neural net classifier spun up, let’s tune its prediction threshold like we did for the other classifer models.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>get_cnn_raw_preds</span><span>(cnn, X_train, X_val):</span></span></p><p><span> 2</span><span><span>    cnn.eval()</span></span></p><p><span> 3</span><span><span>    </span><span>with</span><span> torch.no_grad():</span></span></p><p><span> 4</span><span><span>        trn_raw_predictions </span><span>=</span><span> cnn(X_train.float())</span></span></p><p><span> 5</span><span><span>        trn_raw_predictions </span><span>=</span><span> trn_raw_predictions.squeeze().numpy()</span></span></p><p><span> 6</span><span><span>        val_raw_predictions </span><span>=</span><span> cnn(X_val.float())</span></span></p><p><span> 7</span><span><span>        val_raw_predictions </span><span>=</span><span> val_raw_predictions.squeeze().numpy()</span></span></p><p><span> 8</span><span><span>    raw_preds_dict </span><span>=</span><span> {</span></span></p><p><span> 9</span><span><span>        </span><span>"trn_raw_preds"</span><span>: trn_raw_predictions,</span></span></p><p><span>10</span><span><span>        </span><span>"val_raw_preds"</span><span>: val_raw_predictions</span></span></p><p><span>11</span><span><span>    }</span></span></p><p><span>12</span><span><span>    </span><span>return</span><span> raw_preds_dict</span></span></p><p><span>13</span><span><span>	</span></span></p><p><span>14</span><span><span>trn_raw_predictions </span><span>=</span><span> get_cnn_raw_preds(cnn, trn_X_tensors, val_X_tensors)[</span><span>'trn_raw_preds'</span><span>]</span></span></p><p><span>15</span><span><span>val_raw_predictions </span><span>=</span><span> get_cnn_raw_preds(cnn, trn_X_tensors, val_X_tensors)[</span><span>'val_raw_preds'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>cnn_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_X_tensors, val_y, cnn, max_fpr_tolerance, </span><span>val_raw_preds</span><span>=</span><span>val_raw_predictions)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><figure><img src="https://samiramly.com/media/BIJQx25JQxOh8ASBHYlHpvnjhi2cin5tqabaHvlj.png" width="457" height="352" srcset="https://samiramly.com/media/BIJQx25JQxOh8ASBHYlHpvnjhi2cin5tqabaHvlj.png 934w, https://samiramly.com/media/BIJQx25JQxOh8ASBHYlHpvnjhi2cin5tqabaHvlj.png/500w 500w, https://samiramly.com/media/BIJQx25JQxOh8ASBHYlHpvnjhi2cin5tqabaHvlj.png/750w 750w"><figcaption></figcaption></figure><p>Okay, now we’re talking. But before we rush and add this model to our ensemble, it’s always good to check our curves to see if anything looks off.</p><figure><img src="https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png" srcset="https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png 2212w, https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png/500w 500w, https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png/750w 750w, https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png/1000w 1000w, https://samiramly.com/media/IdL0egXNI4PNUchvTvYfwyk5F5GsXLsnzt6dAATJ.png/1500w 1500w"><figcaption></figcaption></figure><figure><img src="https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png" srcset="https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png 2216w, https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png/500w 500w, https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png/750w 750w, https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png/1000w 1000w, https://samiramly.com/media/2n3oFObR0TOjTO1EoKUOSmTsq4bkjJuHIWcG96j3.png/1500w 1500w"><figcaption></figcaption></figure><p>Hmm. This model's curves for FPR, Precision and (especially) F1 are <em>incredibly steep </em>in the vicinity of the threshold target<em>.</em> I don’t feel so great about this news.</p><p>It means that the model could be <em>extremely sensitive to noise </em>or slight variations in the data. And it’s especially the case for the FPR value that could fluctuate easily between 0 and <em>40%</em> if the test data has enough variation compared to our training+validation sets. It will be important to keep this in mind when deciding whether/how to incorporate this model's predictions in the ensemble.</p><p>With all this experimentation we’ve been doing to deal with our imbalanced data, we still haven’t tried adding the secret sauce. Let’s rectify this next.</p><figure><img src="https://samiramly.com/media/PkW5GNYVfsxnzq8lGLJFc7T2pYsaojBUvNGnw16h.png" srcset="https://samiramly.com/media/PkW5GNYVfsxnzq8lGLJFc7T2pYsaojBUvNGnw16h.png 700w, https://samiramly.com/media/PkW5GNYVfsxnzq8lGLJFc7T2pYsaojBUvNGnw16h.png/500w 500w"><figcaption></figcaption></figure><hr><h3><strong>Resampling to balance class distribution</strong></h3><p>Given that only a small proportion of levels in the data are of the ‘unsolvable’ class, what if we just tried to… oversample from this minority class to get a more balanced dataset?</p><p>If we believe the unsolvable levels we currently have fairly represent the underlying distribution of unsolvables, let’s sample more of them! This has the potential to improve model performance by tackling the imbalanced data issue at the root.</p><p>We proceed to trying various resampling methods like:</p><ul><li><p><code>RandomOverSampler</code></p></li><li><p><code>SMOTE</code> (Synthetic Minority Oversampling Technique)</p></li><li><p><code>BorderlineSMOTE</code></p></li><li><p><code>ADASYN</code></p></li></ul><p>So now we go back to the very beginning, oversample, retrain, retune, and re-evaluate. For each resampling method and each model. Fun fun fun 🙃 Don’t worry I’ll fast-forward you to the interesting stuff.</p><p>From all the experimentation I did, the resampling method that led to the best performance was the regular random oversampling without interpolation using the <code>RandomOverSampler</code>. SMOTE and its variant resampling methods were disappointingly not helpful in improving results on this dataset. In fact, performance actually <em>suffered</em> when trying out fancier resampling methods on this data.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>trn_xs_rnd_resampled, trn_y_rnd_resampled </span><span>=</span><span> resample_to_balance(</span><span>'RandomOverSampler'</span><span>, trn_xs,trn_y)</span></span></p></code></pre><figure><img src="https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png" srcset="https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png 1738w, https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png/500w 500w, https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png/750w 750w, https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png/1000w 1000w, https://samiramly.com/media/z2d8l5aA5mvR7hTrEnAEswFp5jSowzoCCoedVVTo.png/1500w 1500w"><figcaption>Nice. Our training set is now at 6,856 levels with 50-50 solvables.<br>It’s crazy to think that each of these is an actual Echo Chess level you can play!</figcaption></figure><p><strong>Oversampling with tuned-RF</strong></p><p>Now that we’ve updated our training set with oversampling, we can retrain (and re-tune) our promising random forest model. Notice we’re using the new versions here: <code>trn_xs_rnd_resampled</code> and <code>trn_y_rnd_resampled</code>.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>rf_rnd_rs </span><span>=</span><span> RandomForestClassifier(</span><span>100</span><span>, </span><span>min_samples_leaf</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>)</span></span></p><p><span>2</span><span><span>rf_rnd_rs.fit(trn_xs_rnd_resampled, trn_y_rnd_resampled)</span></span></p><p><span>3</span><span><span>rf_rnd_rs_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, rf_rnd_rs, max_fpr_tolerance)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.8903113063407183</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.60</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>43.06</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>59.94</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>43.06</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.92</span><span>%</span></span></p></code></pre><figure><img src="https://samiramly.com/media/XABVK5gWMd9bkiBMl5wvwoqReRpDU3M12gSNgDhH.png" width="479" height="377" srcset="https://samiramly.com/media/XABVK5gWMd9bkiBMl5wvwoqReRpDU3M12gSNgDhH.png 1066w, https://samiramly.com/media/XABVK5gWMd9bkiBMl5wvwoqReRpDU3M12gSNgDhH.png/500w 500w, https://samiramly.com/media/XABVK5gWMd9bkiBMl5wvwoqReRpDU3M12gSNgDhH.png/750w 750w, https://samiramly.com/media/XABVK5gWMd9bkiBMl5wvwoqReRpDU3M12gSNgDhH.png/1000w 1000w"><figcaption></figcaption></figure><p>Sure, not bad, I’ll take it. Dump it in our ensemble.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>models_predictions[</span><span>'RandomOversampling-Random-Forest-tuned'</span><span>] </span><span>=</span><span> {</span></span></p><p><span>2</span><span><span>    </span><span>'tuned_threshold'</span><span>: rf_rnd_rs_threshold_target,</span></span></p><p><span>3</span><span><span>    </span><span>'raw_predictions'</span><span>: rf_rnd_rs.predict_proba(val_xs)[:, </span><span>1</span><span>] ,</span></span></p><p><span>4</span><span><span>    </span><span>'class_predictions'</span><span>: (rf_rnd_rs.predict_proba(val_xs)[:, </span><span>1</span><span>] </span><span>&gt;=</span><span> threshold).astype(</span><span>int</span><span>)</span></span></p><p><span>5</span><span><span>}</span></span></p></code></pre><p><strong>Oversampling with tuned-BalancedRF</strong></p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>brf_rnd_rs </span><span>=</span><span> BalancedRandomForestClassifier(</span><span>n_estimators</span><span>=</span><span>100</span><span>, </span><span>min_samples_leaf</span><span>=</span><span>5</span><span>, </span><span>random_state</span><span>=</span><span>42</span><span>)</span></span></p><p><span>2</span><span><span>brf_rnd_rs.fit(trn_xs_rnd_resampled, trn_y_rnd_resampled)</span></span></p><p><span>3</span><span><span>brf_rnd_rs_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, brf_rnd_rs, max_fpr_tolerance)[</span><span>'threshold'</span><span>]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>Threshold: </span><span>0.8999404166095343</span></span></p><p><span>2</span><span><span>Precision: </span><span>98.50</span><span>%</span></span></p><p><span>3</span><span><span>Recall: </span><span>40.10</span><span>%</span></span></p><p><span>4</span><span><span>F1 Score: </span><span>57.00</span><span>%</span></span></p><p><span>5</span><span><span>True</span><span> Positive Rate: </span><span>40.10</span><span>%</span></span></p><p><span>6</span><span><span>False</span><span> Positive Rate: </span><span>4.92</span><span>%</span></span></p></code></pre><figure><img src="https://samiramly.com/media/8C18d1kSBbX1RwfUWDlogutu2IQh4py334NTMvKT.png" width="515" height="378" srcset="https://samiramly.com/media/8C18d1kSBbX1RwfUWDlogutu2IQh4py334NTMvKT.png 1146w, https://samiramly.com/media/8C18d1kSBbX1RwfUWDlogutu2IQh4py334NTMvKT.png/500w 500w, https://samiramly.com/media/8C18d1kSBbX1RwfUWDlogutu2IQh4py334NTMvKT.png/750w 750w, https://samiramly.com/media/8C18d1kSBbX1RwfUWDlogutu2IQh4py334NTMvKT.png/1000w 1000w"><figcaption></figcaption></figure><p>Okay, maybe I’m getting a little too picky given the stellar options we have, but this one might be pushing it a bit.</p><p>I mean we’re already oversampling, and using a balanced RF to handle imbalanced data, <em>and</em> tuning a threshold to be super conservative (all at the risk of generalizability issues). At least we should expect some crazy performance. I think we pass on this one.</p><p><strong>Oversampling with tuned-XGBoost</strong></p><p>I’ll spare you the same lines of code. This one also got better with oversampling. Guess where it went? Into the ensemble.</p><figure><img src="https://samiramly.com/media/uFGicrxZAr6Lnb0TY8jdEIPd17xIW6ffUGZ3XIvm.png" width="476" height="359" srcset="https://samiramly.com/media/uFGicrxZAr6Lnb0TY8jdEIPd17xIW6ffUGZ3XIvm.png 1108w, https://samiramly.com/media/uFGicrxZAr6Lnb0TY8jdEIPd17xIW6ffUGZ3XIvm.png/500w 500w, https://samiramly.com/media/uFGicrxZAr6Lnb0TY8jdEIPd17xIW6ffUGZ3XIvm.png/750w 750w, https://samiramly.com/media/uFGicrxZAr6Lnb0TY8jdEIPd17xIW6ffUGZ3XIvm.png/1000w 1000w"><figcaption></figcaption></figure><p>I think this was actually our best model so far —hard to keep up with all these nice goodies. Real-talk, though, next time I’m going down such a deep rabbit hole, I’m obviously using an MLOps platform like <a href="https://wandb.ai/site" target="_blank" rel="noopener noreferrer">Weights &amp; Biases</a> to keep track of all these model iterations. IYKYK. You live and you learn.</p><hr><h3><strong>Ensembling all the promising models</strong></h3><p>So far we’ve been adding each promising model to a <code>models_predictions</code> dict of dicts. Let’s convert it to a dataframe to work with it easily.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>models_predictions_df </span><span>=</span><span> pd.DataFrame.from_dict(models_predictions, </span><span>orient</span><span>=</span><span>'index'</span><span>)</span></span></p><p><span>2</span><span><span>print</span><span>(models_predictions_df.index.values)</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>[</span><span>'Random-Forest-tuned'</span><span> </span><span>'Balanced-Random-Forest-tuned'</span><span> </span><span>'XGBoost-tuned'</span><span> </span><span>'Top-features-RF-tuned'</span><span> </span><span>'CNN-tuned'</span><span> </span><span>'RandomOversampling-Random-Forest-tuned'</span><span> </span><span>'RandomOversampling-XGBoost-tuned'</span><span>]</span></span></p></code></pre><p>We can now experiment with different approaches of ensembling using the Validation set to pick the winning one.</p><p>For instance, we could get the mean raw predictions across the ensemble and then use our existing <code>threshold_tune_binary_classifier</code> function to select a good <code>ensemble_raw_threshold_target</code> prediction threshold for the avg raw preds that minimizes FPR and maximizes F1 scores within our 5% tolerance.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>avg_models_raw_preds </span><span>=</span><span> models_predictions_df[</span><span>'raw_predictions'</span><span>].mean()</span></span></p><p><span>2</span><span><wbr></span></p><p><span>3</span><span><span>ensemble_raw_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, </span><span>None</span><span>, </span><span>max_fpr_tolerance</span><span>=</span><span>max_fpr_tolerance, </span><span>val_raw_preds</span><span>=</span><span>avg_models_raw_preds)[</span><span>'threshold'</span><span>]</span></span></p><p><span>4</span><span><wbr></span></p><p><span>5</span><span><span>avg_raw_to_tuned_preds </span><span>=</span><span> (avg_models_raw_preds </span><span>&gt;=</span><span> ensemble_raw_threshold_target).astype(</span><span>int</span><span>)</span></span></p></code></pre><p>Similarly, we could ensemble by averaging the class prediction themselves (instead of raw preds), then selecting a good <code>ensemble_classes_threshold_target</code> for the avg of the class preds themselves.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>avg_models_class_preds </span><span>=</span><span> models_predictions_df[</span><span>'class_predictions'</span><span>].mean()</span></span></p><p><span>2</span><span><wbr></span></p><p><span>3</span><span><span>ensemble_classes_threshold_target </span><span>=</span><span> threshold_tune_binary_classifier(val_xs, val_y, </span><span>None</span><span>, </span><span>max_fpr_tolerance</span><span>=</span><span>max_fpr_tolerance, </span><span>val_raw_preds</span><span>=</span><span>avg_models_class_preds)[</span><span>'threshold'</span><span>]</span></span></p><p><span>4</span><span><wbr></span></p><p><span>5</span><span><span>avg_class_to_tuned_preds </span><span>=</span><span> (avg_models_class_preds </span><span>&gt;=</span><span> ensemble_classes_threshold_target).astype(</span><span>int</span><span>)</span></span></p></code></pre><p>Or we could require that <em>both</em> these approaches agree (relative to each one’s tuned threshold respectively), if we wanted to be <em>even more</em> overconservative on solvability.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>comb_preds </span><span>=</span><span> (avg_raw_to_tuned_preds </span><span>*</span><span> avg_class_to_tuned_preds </span><span>&gt;</span><span> </span><span>0</span><span>).astype(</span><span>int</span><span>)</span></span></p></code></pre><p>But in general, each of these approaches risks getting us into dangerous overfitting territory as we squeeze greedily more of the tuning juice out of the validation set. Interestingly, the simpler, non-tuned approach, ends up being as performant too: ensembling through majority voting of the class predictions.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>majority_needed </span><span>=</span><span> math.ceil(num_models_in_ensemble </span><span>/</span><span> </span><span>2</span><span>)</span></span></p><p><span>2</span><span><span>majority_class_preds </span><span>=</span><span> (num_positive_class_preds </span><span>&gt;=</span><span> majority_needed).astype(</span><span>int</span><span>)</span></span></p></code></pre><p>Here are the final results, side-by-side, for each of these 4 ensembling techniques, post-threshold-tuning (if applicable) on the validation set:</p><figure><img src="https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png" width="458" height="537" srcset="https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png 2864w, https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png/500w 500w, https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png/750w 750w, https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png/1000w 1000w, https://samiramly.com/media/7xGN7ug7ZZJYFnYqJoWOeQL64jSpxFHp5o9z8eMa.png/1500w 1500w"><figcaption>When choosing among gemstones, pick the color you like.</figcaption></figure><p>Looking at the TPs and FPs of each, no matter which ensembling method is chosen, when judging a generated level as solvable, we’d still guess right ~99% of the time. For this and the reasons above, <strong>we’ll go with the simple majority approach as our ensembling technique.</strong></p><p>More specifically, our goal when productizing this will be to pick the “most-promisingly solvable” level from a candidate list of randomly generated levels. So we’ll sort first by class majority vote predictions, then by avg raw preds within each class —this way even if no solvables are found, at least we’d be serving the <em>most promising</em> level among the unsolvable ones.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span># Example usage for inference</span></span></p><p><span>2</span><span><span>ensemble_preds </span><span>=</span><span> predict_solvability(level_df, </span><span>'models'</span><span>)</span></span></p><p><span>3</span><span><span>most_promising_levels </span><span>=</span><span> ensemble_preds.sort_values(</span><span>by</span><span>=</span><span>[</span><span>'ensemble_preds'</span><span>, </span><span>'avg_raw_preds'</span><span>], </span><span>ascending</span><span>=</span><span>[</span><span>False</span><span>, </span><span>False</span><span>])</span></span></p></code></pre><h3>In<strong>ference on the External Test</strong></h3><p>Remember that 10% of data we completely stashed away in <code>tst_df</code> ages ago? We can now <em>finally</em> check how our ensemble is doing on it. If it sucks, we’re kind of back to square one like our friend Bill. Moment of truth.</p><figure><img src="https://samiramly.com/media/zdudtzqFfKRjupmsLrgTKnSJPxutGmNH0euvfBG5.png" width="655" height="503" srcset="https://samiramly.com/media/zdudtzqFfKRjupmsLrgTKnSJPxutGmNH0euvfBG5.png 1074w, https://samiramly.com/media/zdudtzqFfKRjupmsLrgTKnSJPxutGmNH0euvfBG5.png/500w 500w, https://samiramly.com/media/zdudtzqFfKRjupmsLrgTKnSJPxutGmNH0euvfBG5.png/750w 750w, https://samiramly.com/media/zdudtzqFfKRjupmsLrgTKnSJPxutGmNH0euvfBG5.png/1000w 1000w"><figcaption>FPR of 1.2% on the <em>untouched</em> test set. I’m not crying, you’re crying.</figcaption></figure><p>Boom! And that's it! Extremely promising results on both the validation and test sets. When we’re ready to push this to prod, we can load these best-performing models into the server and cache them, grab their pre-tuned thresholds, generate individual predictions at the inference stage, and ensemble them together through majority voting.</p><p>Great stuff. 6 classical ML + 1 DL model. All straightforward architectures. Preprocessing, feature engineering, hyperparameter tuning and some usage of oversampling here and there to handle the imbalanced dataset. A few other minor things but all in all just a vintage ML modeling routine.</p><figure><img src="https://samiramly.com/media/VaYY3zWhdgDytIrWcz89QWYqNoEFjTxcnWJSOEwK.png" srcset="https://samiramly.com/media/VaYY3zWhdgDytIrWcz89QWYqNoEFjTxcnWJSOEwK.png 500w"><figcaption>Genies were the first AGIs.</figcaption></figure><p><strong>But wait!</strong> We still have two final tricks up our sleeve to improve this further. Chess players will like these.</p><hr><h3><strong>Trick #1: Data Augmentation</strong></h3><p>Taking inspiration from other domain areas like image processing, we can quickly notice an opportunity to significantly augment our dataset by creating new valid, labeled samples from our existing labeled ones:</p><ul><li><p>Every <code>rectangle</code> level can be mirrored either horizontally or vertically on the opposing side (depending on its shape) to create one new, effectively identical, level for classification.</p></li><li><p>Similarly, every <code>quad</code> level can be mirrored across the 3 opposing corners (either horizontally, vertically, or both horizontally <em>and</em> vertically) to create 3 new, effectively identical, levels.</p></li></ul><figure><img src="https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png" width="412" height="411" srcset="https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png 2948w, https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png/500w 500w, https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png/750w 750w, https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png/1000w 1000w, https://samiramly.com/media/NW4XzKcGxttS2ewBWaUdbsKgvL2PE28lgcC2CeVj.png/1500w 1500w"><figcaption>Top-left: original level. Top-right: augmentation through vertical mirroring.<br>Bottom-left: aug. through horizontal mirroring. Bottom-right: aug. through v+h mirroring.</figcaption></figure><p>Each of these 4 levels above are pretty much the same and should command the same <code>solvability</code> label, yet they each have a different <code>compoundFEN</code> input. Importantly, they also accurately represent the type of levels that get generated by our ground truth distribution —which we can be sure of, given that it’s a parametrized distribution we invented ourselves for the random level generator.</p><p>Good data augmentation opportunity + easy low hanging fruits. Let’s implement them quickly.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span> 1</span><span><span>def</span><span> </span><span>mirror</span><span>(compoundFen, direction):</span></span></p><p><span> 2</span><span><span>    table </span><span>=</span><span> fenToTable(compoundFen)</span></span></p><p><span> 3</span><span><span>    </span><span>if</span><span> direction </span><span>==</span><span> </span><span>'h'</span><span>:</span></span></p><p><span> 4</span><span><span>        </span><span>return</span><span> tableToFen(table[[</span><span>0</span><span>, </span><span>4</span><span>, </span><span>5</span><span>, </span><span>6</span><span>, </span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>, </span><span>7</span><span>]])</span></span></p><p><span> 5</span><span><span>    </span><span>elif</span><span> direction </span><span>==</span><span> </span><span>'v'</span><span>:</span></span></p><p><span> 6</span><span><span>        </span><span>return</span><span> tableToFen(table[:, [</span><span>0</span><span>, </span><span>4</span><span>, </span><span>5</span><span>, </span><span>6</span><span>, </span><span>1</span><span>, </span><span>2</span><span>, </span><span>3</span><span>, </span><span>7</span><span>]])</span></span></p><p><span> 7</span><span><span>		</span></span></p><p><span> 8</span><span><span>def</span><span> </span><span>augment</span><span>(rows, mirroring_direction):</span></span></p><p><span> 9</span><span><span>    augmented_rows </span><span>=</span><span> rows.copy()</span></span></p><p><span>10</span><span><span>    augmented_rows[</span><span>'compoundFen'</span><span>] </span><span>=</span><span> augmented_rows[</span><span>'compoundFen'</span><span>].apply(mirror, </span><span>args</span><span>=</span><span>(mirroring_direction,))                      </span></span></p><p><span>11</span><span><span>    augmented_rows[</span><span>'levelOrientation'</span><span>] </span><span>=</span><span> augmented_rows[</span><span>'levelOrientation'</span><span>].apply(switch_orientation, </span><span>args</span><span>=</span><span>(mirroring_direction,))</span></span></p><p><span>12</span><span><span>    </span><span>return</span><span> augmented_rows</span></span></p><p><span>13</span><span><wbr></span></p><p><span>14</span><span><span>def</span><span> </span><span>mirror_augmentation</span><span>(df):</span></span></p><p><span>15</span><span><span>    rectangle_rows </span><span>=</span><span> df[df[</span><span>'levelGeoShape'</span><span>] </span><span>==</span><span> </span><span>'rectangle'</span><span>]</span></span></p><p><span>16</span><span><span>    quad_rows </span><span>=</span><span> df[df[</span><span>'levelGeoShape'</span><span>] </span><span>==</span><span> </span><span>'quad'</span><span>]</span></span></p><p><span>17</span><span><span>    h_rectangle_rows </span><span>=</span><span> rectangle_rows[rectangle_rows[</span><span>'levelOrientation'</span><span>].isin([</span><span>'h-top'</span><span>, </span><span>'h-bottom'</span><span>])]</span></span></p><p><span>18</span><span><span>    v_rectangle_rows </span><span>=</span><span> rectangle_rows[rectangle_rows[</span><span>'levelOrientation'</span><span>].isin([</span><span>'v-left'</span><span>, </span><span>'v-right'</span><span>])]</span></span></p><p><span>19</span><span><span>    augmented_rows </span><span>=</span><span> pd.concat([</span></span></p><p><span>20</span><span><span>        augment(h_rectangle_rows, </span><span>'h'</span><span>),</span></span></p><p><span>21</span><span><span>        augment(v_rectangle_rows, </span><span>'v'</span><span>),</span></span></p><p><span>22</span><span><span>        augment(quad_rows, </span><span>'h'</span><span>),</span></span></p><p><span>23</span><span><span>        augment(quad_rows, </span><span>'v'</span><span>),</span></span></p><p><span>24</span><span><span>        </span><span># mirror quad diagonally across</span></span></p><p><span>25</span><span><span>        augment(augment(quad_rows, </span><span>'v'</span><span>), </span><span>'h'</span><span>)</span></span></p><p><span>26</span><span><span>    ], </span><span>ignore_index</span><span>=</span><span>True</span><span>)</span></span></p><p><span>27</span><span><span>    augmented_df </span><span>=</span><span> pd.concat([df, augmented_rows], </span><span>ignore_index</span><span>=</span><span>True</span><span>)</span></span></p><p><span>28</span><span><span>    </span><span>return</span><span> augmented_df</span></span></p></code></pre><p>And here is a simple example in action.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>fen_test </span><span>=</span><span> </span><span>"XXXXXXXX/XqbxBb1X/Xx1xnxrX/X1n1kq1X/XXXXXXXX/XXXXXXXX/XXXXXXXX/XXXXXXXX w - - 0 1"</span></span></p><p><span>2</span><span><span>print</span><span>(fen_to_board(fen_test))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>[[</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>2</span><span><span> [</span><span>'X'</span><span> </span><span>'q'</span><span> </span><span>'b'</span><span> </span><span>'x'</span><span> </span><span>'B'</span><span> </span><span>'b'</span><span> </span><span>' '</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>3</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>' '</span><span> </span><span>'x'</span><span> </span><span>'n'</span><span> </span><span>'x'</span><span> </span><span>'r'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>4</span><span><span> [</span><span>'X'</span><span> </span><span>' '</span><span> </span><span>'n'</span><span> </span><span>' '</span><span> </span><span>'k'</span><span> </span><span>'q'</span><span> </span><span>' '</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>5</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>6</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>7</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>8</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]]</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>mirrored_fen </span><span>=</span><span> mirror(fen_test,</span><span>'h'</span><span>)</span></span></p><p><span>2</span><span><span>print</span><span>(fen_to_board(mirrored_fen))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>[[</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>2</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>3</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>4</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>5</span><span><span> [</span><span>'X'</span><span> </span><span>'q'</span><span> </span><span>'b'</span><span> </span><span>'x'</span><span> </span><span>'B'</span><span> </span><span>'b'</span><span> </span><span>' '</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>6</span><span><span> [</span><span>'X'</span><span> </span><span>'x'</span><span> </span><span>' '</span><span> </span><span>'x'</span><span> </span><span>'n'</span><span> </span><span>'x'</span><span> </span><span>'r'</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>7</span><span><span> [</span><span>'X'</span><span> </span><span>' '</span><span> </span><span>'n'</span><span> </span><span>' '</span><span> </span><span>'k'</span><span> </span><span>'q'</span><span> </span><span>' '</span><span> </span><span>'X'</span><span>]</span></span></p><p><span>8</span><span><span> [</span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span> </span><span>'X'</span><span>]]</span></span></p></code></pre><p>It may look silly, but this stuff actually allows us to 2X our 'rectangle' row samples and 4X our 'quad's, significantly increasing the size of our dataset.</p><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>augmented_trn_df </span><span>=</span><span> mirror_augmentation(trn_df)</span></span></p><p><span>2</span><span><span>print</span><span>(</span><span>len</span><span>(trn_df), </span><span>len</span><span>(augmented_trn_df))</span></span></p></code></pre><pre onmouseenter="" onmouseleave="" data-language="py" data-annotations="" data-name=""><code><p><span>1</span><span><span>3855</span><span>, </span><span>7877</span></span></p></code></pre><p>That’s more than a 2X increase in the size of our data! <em><strong>7,877</strong></em><strong> Echo Chess levels.</strong> I wonder how long it’d take for someone to play them all.</p><p>It’s worth mentioning too that we could similarly implement further data augmentations through simple 90-degree rotations, as well as internal reflections, of any level to generate new FENs out of every <code>quad</code>, <code>rectangle</code>, or even <code>full</code> level. It’s not shown in the current version but I’ll just leave you with this for inspiration:</p><figure><img src="https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExdWx4dnF0Y2wwb3BkdnM2M2YwaXcxaWhyaHJuMGd6NDM2eWFmejdvdCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ms3yqSf67KQjnXm6kN/giphy.gif" width="278" height="278"><figcaption>A board is a board is a board.</figcaption></figure><h3>Trick #2: <strong>Pre-predicting ‘guaranteed unsolvables’</strong></h3><p>We can again use our knowledge of the Echo Chess game mechanics to directly filter out any ‘guaranteed’ unsolvable levels before even passing the baton to the ensemble model - for example any level with:</p><ul><li><p>No valid first move</p></li><li><p>No black pieces on board</p></li><li><p>More ‘end-state-pieces’, or black pieces with no valid moves, than starting white pieces</p></li><li><p>‘Untethered’ starting bishops —i.e. starting with a bishop on a white square when all black pieces are on a black square, or vice versa</p></li><li><p>And so on…</p></li></ul><p>We already saw how to check for many of these in the <em>Feature Engineering</em> section, so I won’t bore you with the implementation again. The key thing here is that when we add all the little tweaks like these, as well as the data augmentation part, the oversampling, feature engineering and feature selection, and retrain our ensemble model on the augmented set, the improvements quickly add up even further.</p><p>We’ll see next how this is all faring today in prod on the ⭐️<em>LIVE</em>⭐️ app on <a href="https://echochess.com/" target="_blank" rel="noopener noreferrer">echochess.com</a>.</p><hr><h2>The FINAL Outcome on the LIVE APP</h2><p>I can’t believe I’m saying this… But we are FINALLY at the stage where we can put all this in prod. It’s here. It’s live. People are using it. With ML.</p><p>I know you’re dying to find out how this all ends, so I’ll spare you the details. This is the gist of it.</p><blockquote><p><em>The </em><a href="https://echochess.com/?endless" target="_blank" rel="noopener noreferrer"><em>live Echo Chess Endless mode</em></a><em> in prod generates a solvable maze 99%+ of the time. You can test it yourself </em>🫳<em>🎤</em></p></blockquote><p>How? Every single time we need to serve a new level to the player, 50 (yes, FIFTY) different candidate levels get procedurally generated using parametrized random distributions.</p><p>On the server, we generate predictions for each of these 50 random gens using each of the tuned ML/DL models we’ve selected. We then ensemble them through majority voting, plus keep track of each level’s average raw predictions from every model.</p><p>We then sort the 50 candidate gens by majority class first, and avg raw preds second. The “most-promisingly solvable” level from these is then sent to the client, rendered and served to be played. The whole thing happens in a split-second while the level transition sound effect is playing.</p><figure><img src="https://samiramly.com/media/edvZ3lvgXF15F3QV.jpg" width="445" height="574" srcset="https://samiramly.com/media/edvZ3lvgXF15F3QV.jpg 500w"><figcaption>IT WORKS, YOU GUYS! We made a fun Single-Player Chess 🥲</figcaption></figure><hr><h2>Future <strong>work, what didn’t make the cut</strong></h2><p>I would be remiss if I didn’t mention some of the crazy ideas I wanted to try with Echo Chess before I realized this rabbit hole was getting a tad bit too deep.</p><p>If any of these resonates with you (or especially if you have better ideas than these), drop me a note and maybe I’ll try it out in the upcoming version.</p><h3><strong>RE:Enhancing Predictions</strong></h3><p><strong>Transformer-based NLP classification of compoundFENs</strong></p><ul><li><p>The encoded <code>compoundFEN</code> is just a string. NLP it. Fine-tune a pre-trained Transformer model from HuggingFace like UCL’s <a href="https://arxiv.org/abs/2306.09200" target="_blank" rel="noopener noreferrer">ChessGPT</a> (a 2.8B language model pretrained on Chess).</p></li></ul><p><strong>LLM k-shot in-context learning</strong></p><ul><li><p>Instead of using a task-specific classifier, feed in all the labeled FENs from the training set as few-shot examples in a prompt to an LLM, and provide it with the unlabeled validation+test sets to make <code>solvability</code> predictions on (can do the inference one FEN at a time or in bulk through the LLM provider’s API or a wrapper like LangChain).</p></li><li><p>Given the size of the training sets, we’d certainly need to use a large-context-window LLM like Anthropic's Claude2 100k – and possibly even some creative version of Retrieval Augmented Generation (RAG) and vector dbs.</p></li></ul><p><strong>JUMBO data augmentation</strong></p><p>After EVERY player move (which leads to a new board state), save each interim <code>compoundFEN</code> as its own unique ‘new level’.</p><ul><li><p>When the actual level is solved (or is tagged as unsolvable), assign the same solvability for all these unique interim FENs</p></li><li><p>That’s because if a path exists from start to end, then definitionally all the interim states of that path can reach the end, <em>if they follow that same path</em>.</p></li></ul><p><strong>Synthetic generation of Unsolvables</strong></p><ul><li><p>For every level without a valid first move, generate N similar levels with the same <code>startingWhitePiece</code> on that same starting square, but with ALL other NON-obstacle squares replaced with random combinations of {black piece, obstacle square, empty square}. They all won’t have valid first moves either.</p></li><li><p>Do the same for every level with more “end-state” pieces than white pieces. Even shuffle the white pieces themselves in type and location, but keep their same count. All these levels will be unsolvable too.</p></li></ul><p><em><strong>Actual</strong></em><strong> Image classification of levels</strong></p><ul><li><p>Generate jpg screenshots of every level using its <code>compoundFEN</code>, then train an image classifier on the labeled images</p></li><li><p>Can fine-tune a pretrained SOTA classifier on this img dataset to get better performance than training from scratch. And given that chess boards probably don’t look as similar to what ImageNet deals with usually as something like ‘cats’ vs ‘dogs’, we’d likely pick one of the architectures from the PyTorch Image Models (timm) that’s more suitable to datasets that are not very ‘ImageNet-like’.</p></li><li><p>fast.ai has done <a href="https://www.kaggle.com/code/jhoward/the-best-vision-models-for-fine-tuning" target="_blank" rel="noopener noreferrer">some great analysis</a> on such models and datasets.</p></li></ul><p><strong>Letting AI actually play using Reinforcement Learning</strong></p><ul><li><p>Instead of thinking of the problem as a classification problem, approaching it instead from an RL lens à la AlphaZero. Tons of fun could be had exploring this direction.</p></li></ul><figure><img src="https://samiramly.com/media/i1M6wiP1sJj5xtrq5PFfwRHysn9dVIofYSNyyzrh.jpg" srcset="https://samiramly.com/media/i1M6wiP1sJj5xtrq5PFfwRHysn9dVIofYSNyyzrh.jpg 917w, https://samiramly.com/media/i1M6wiP1sJj5xtrq5PFfwRHysn9dVIofYSNyyzrh.jpg/500w 500w, https://samiramly.com/media/i1M6wiP1sJj5xtrq5PFfwRHysn9dVIofYSNyyzrh.jpg/750w 750w"><figcaption>I wonder how they’ll drink those beers.</figcaption></figure><p>From a ‘business’ use case and user-centered design standpoint, we’re solid. We’ve also pretty much saturated the returns from improvements in solvability prediction tbh. So let’s turn our eyes instead to overall Product and Game Design.</p><h3>RE:<strong>Enhancing the Product experience</strong></h3><ul><li><p>Adjust difficulty curve as player solves more procedurally-generated levels (via varying distribution parametrization, classification thresholds, selected percentile from the sorted preds, etc.)</p></li><li><p>Let players decide what gens look like using an LLM interface: ‘I want more knights, 4-square walls, pawns <em>with</em> promotion, but keep it easy’</p></li><li><p>Player can unlock power-ups like:</p><ul><li><p>Call reinforcements to add extra random white pieces on the board (start with 0 available, win extra call every X levels)</p></li><li><p>Upgrade your white piece to Queen at any time (start with 2 instant promotions available, win extra one every Y levels)</p></li><li><p>Trigger regional explosions by landing on a target highlighted square that shows up every X levels&nbsp;</p></li><li><p>Portal squares to teleport the player’s current piece to another part of the board</p></li></ul></li><li><p>A community-driven level maker for Echo Chess. Anyone could design their own crazy level and upload it to the community to try out.</p></li></ul><p>I’m sure there are tons of other interesting ideas as well. Curious what others will find.</p><hr><h2>Epilogue</h2><p>The game is live, it’s being played by 1000s, both Classic and Endless modes are a hit in their unique ways. ML really saved the day for Endless. And I’ll always enjoy manually crafting hardcore puzzles for Classic.</p><p>So just go ahead and try it out already 🙂</p><p>For all you chess experts looking for a challenge, try starting with levels 8+ in Classic. And for <em>especially</em> ambitious chess veterans, see how many you can solve starting 11+ (heads-up: it gets really difficult, <em>really</em> quickly!)</p><p>Hopefully this little rabbit hole of a story inspires other builders to create more with chess, games, ML, or all of the above. If you come across anything like that that piques your interest, please drop a link below. Would love to try it out.</p><p>And if you made it to the end and actually enjoyed <em>any</em> of this (or even if you didn’t), please let me know by sharing your thoughts or angry rants. Feedback is always welcome anytime.</p><p>Happy puzzling 🕹️</p><p>-Sami</p>
			</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I automated 1/2 of my typing (654 pts)]]></title>
            <link>https://github.com/eschluntz/compress</link>
            <guid>37326870</guid>
            <pubDate>Wed, 30 Aug 2023 18:40:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/eschluntz/compress">https://github.com/eschluntz/compress</a>, See on <a href="https://news.ycombinator.com/item?id=37326870">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://codecov.io/gh/eschluntz/compress" rel="nofollow"><img src="https://camo.githubusercontent.com/07b06fdd538d34c3437675bcbb7d94e331202216ffde7fecbdda68a3fb74cc11/68747470733a2f2f636f6465636f762e696f2f67682f657363686c756e747a2f636f6d70726573732f6272616e63682f6d61737465722f67726170682f62616467652e7376673f746f6b656e3d434d4c35503238454c4c" alt="codecov" data-canonical-src="https://codecov.io/gh/eschluntz/compress/branch/master/graph/badge.svg?token=CML5P28ELL"></a></p>
<p dir="auto"><a href="https://github.com/eschluntz/compress/actions"><img src="https://github.com/eschluntz/compress/actions/workflows/run_tests.yml/badge.svg" alt="Build Status"></a></p>
<h2 tabindex="-1" dir="auto">Compress</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/eschluntz/compress/blob/master/img/expand-demo.gif"><img src="https://github.com/eschluntz/compress/raw/master/img/expand-demo.gif" alt="demo" data-animated-image=""></a></p>
<p dir="auto">This is a tool for automatically creating typing shortcuts from a corpus of your own writing! I use these shortcuts mainly for email and slack:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/eschluntz/compress/blob/master/img/email-expand.gif"><img src="https://github.com/eschluntz/compress/raw/master/img/email-expand.gif" alt="email" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/eschluntz/compress/blob/master/img/slack-expand.gif"><img src="https://github.com/eschluntz/compress/raw/master/img/slack-expand.gif" alt="slack" data-animated-image=""></a></p>
<p dir="auto">This repo parses a corpus of text and suggest what shortcuts you should use to save the most letters while typing. It then generates config files for <a href="https://github.com/autokey/autokey">Autokey</a>, a linux program that implements keyboard shortcuts!</p>
<p dir="auto">It also contains a tool for optionally parsing a Slack Data Export of your messages to create a corpus.</p>
<h2 tabindex="-1" dir="auto">What phrases should I abbreviate?</h2>
<p dir="auto">The code looks through the corpus to find common n-grams that can be replaced with much shorter phrases. The suggestions are ranked by <code>[characters saved] * [frequency of phrase]</code>.</p>
<p dir="auto">I was surprised that very short and frequent words topped this list, such as <code>the -&gt; t</code>, instead of longer phrases that I use a lot, such as <code>what do you think -&gt; wdytk</code>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/eschluntz/compress/blob/master/img/chars-saved.png"><img src="https://github.com/eschluntz/compress/raw/master/img/chars-saved.png" alt="results"></a></p>
<p dir="auto">Just reading through the results was amusing to see how repetitive some of my writing is :)</p>
<h2 tabindex="-1" dir="auto">How to pick abbreviations?</h2>
<p dir="auto">This is largely preferences and heuristics to try to generate memorable abbreviations for different phrases. Some of my design philosphies were:</p>
<ol dir="auto">
<li>The abbrev cannot be a word that I want to type. Right now this is done with a blacklist, but I should change it to use my actual corpus.</li>
<li>The goal is being memorable. 1st letter is top choice, and 1st letter + last letter is next choice.</li>
<li>More common phrases get priority for more memorable abbrevs.</li>
</ol>
<p dir="auto">This is currently done as a manual post-process step, but I like to make "families"
of abbrevs to make them more memorable. Some example heuristics for this are:</p>
<ol dir="auto">
<li>Plurals should have the same abbrev as the singular, but with an "s". For example <code>robot -&gt; r</code> and <code>robots -&gt; rs</code>.</li>
<li>If a word has an abbrev, a phrase that contains that word should contain the abbrev. For example:</li>
</ol>
<div data-snippet-clipboard-copy-content="the       -> t
robot     -> r
the robot -> tr"><pre><code>the       -&gt; t
robot     -&gt; r
the robot -&gt; tr
</code></pre></div>
<ol start="3" dir="auto">
<li>Think about how similar words' abbrevs can be similar as well. i.e.</li>
</ol>
<div data-snippet-clipboard-copy-content="some      -> s
someone   -> sn
something -> st
sometime  -> sti"><pre><code>some      -&gt; s
someone   -&gt; sn
something -&gt; st
sometime  -&gt; sti
</code></pre></div>
<h2 tabindex="-1" dir="auto">Instructions</h2>
<ol dir="auto">
<li>run <code>install.sh</code> to install dependencies. Currently tested on python 3.10.12</li>
<li>Put any corpus of your text that you want to compress in <code>data/corpus/*.txt</code></li>
<li>If you want to use your slack history as a corpus:
<ol dir="auto">
<li>export it to a folder called <code>data/slack_export</code>. Only slack workspace admins can do this (and it only exports public channels).</li>
<li>Change <code>USERNAME_TO_EXPORT</code> at the top of the file to your slack username.</li>
<li>Run <code>parse_slack.py</code>. This will generate a new corpus document in <code>data/corpus/</code></li>
<li>DELETE YOUR SLACK EXPORT WITH <code>srm</code></li>
</ol>
</li>
<li>Run <code>find_suggested_phrases.py</code>. This will generate a list of the top 200 suggested shortcuts to <code>output/suggested_shortcuts.yaml</code></li>
<li>Edit or add any shortcuts that you want, then copy the file to <code>shortcuts.yaml</code>.
<ul dir="auto">
<li>This is a manual step so you can customize it without it being blown out every time you run the script again.</li>
<li>It's also saved in git even though it's an output so that I can keep it in sync across multiple of my computers :)</li>
<li>If you're starting out, I suggest just going with 10-20 shortcuts to make it easier to remember them</li>
</ul>
</li>
<li>Run <code>generate_autokeys.py</code> to convert <code>shortcuts.yaml</code> into actual config files for <code>autokey</code>.</li>
<li>Install <a href="https://github.com/autokey/autokey">Autokey</a>
<ul dir="auto">
<li>Right now, Autokey is only supported on linux with X11, not Wayland</li>
</ul>
</li>
<li>Symlink the output into autokey's config: <code>ln -s output/autokey_phrases ~/.config/autokey/data/My Phrases/</code></li>
<li>From now on when you edit <code>shortcuts.yaml</code> you can re-generate and reload autokey with <code>reload.sh</code></li>
</ol>
<h2 tabindex="-1" dir="auto">Notes</h2>
<p dir="auto"><a href="https://github.com/autokey/autokey">Autokey</a> Uses simulated keyboard input to replace phrases with your abbreviations. I tried several chrome extensions but this worked much more reliably without conflicting with sites' own javascript.</p>
<p dir="auto">The config files I generate are set to only apply when Chrome is in focus because that's where I do most of my english typing. I found that keeping this active in terminal and vscode caused way more problems than it was solved because my abbreviations overlapped with common short linux commands and variable names i.e. <code>t</code>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC refuses to scrap rule requiring ISPs to list every monthly fee (344 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/08/fcc-says-too-bad-to-isps-complaining-that-listing-every-fee-is-too-hard/</link>
            <guid>37326806</guid>
            <pubDate>Wed, 30 Aug 2023 18:37:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/08/fcc-says-too-bad-to-isps-complaining-that-listing-every-fee-is-too-hard/">https://arstechnica.com/tech-policy/2023/08/fcc-says-too-bad-to-isps-complaining-that-listing-every-fee-is-too-hard/</a>, See on <a href="https://news.ycombinator.com/item?id=37326806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/getty-binary-data-money-800x533.jpg" alt="Illustration of US paper currency and binary data to represent Internet connectivity.">
      <figcaption><p>Getty Images | imagedepotpro</p></figcaption>  </figure>

  




<!-- cache hit 32:single/related:06413538ccf2cab862fdb428aef9ef26 --><!-- empty -->
<p>The Federal Communications Commission yesterday rejected requests to eliminate an upcoming requirement that Internet service providers list all of their monthly fees.</p>
<p>Five major trade groups representing US broadband providers <a href="https://www.fcc.gov/ecfs/document/10117331109471/1">petitioned</a> the FCC in January to scrap the requirement before it takes effect. In June, <a href="https://arstechnica.com/tech-policy/2023/06/comcast-complains-to-fcc-that-listing-all-of-its-monthly-fees-is-too-hard/">Comcast told the FCC</a> that the listing-every-fee rule "impose[s] significant administrative burdens and unnecessary complexity in complying with the broadband label requirements."</p>
<p>The five trade groups <a href="https://arstechnica.com/tech-policy/2023/08/isps-complain-that-listing-every-fee-is-too-hard-urge-fcc-to-scrap-new-rule/">kept up the pressure</a> earlier this month in a meeting with FCC officials and in a filing that complained that listing every fee is too hard. The FCC refused to bend, <a href="https://www.fcc.gov/document/fcc-declines-reconsider-broadband-consumer-label-rules">announcing</a> yesterday that the rules will take effect without major changes.</p>
<p>"Every consumer needs transparent information when making decisions about what Internet service offering makes the most sense for their family or household. No one wants to be hit with charges they didn't ask for or they did not expect," FCC Chairwoman Jessica Rosenworcel said.</p>
<p>Yesterday's order "largely affirms the rules... while making some revisions and clarifications such as modifying provider record-keeping requirements when directing consumers to a label on an alternative sales channel and confirming that providers may state 'taxes included' when their price already incorporates taxes," the FCC said.</p>
<h2>ISPs don’t want to list all fees</h2>
<p>Comcast and other ISPs objected to a requirement that ISPs "list all recurring monthly fees" including "all charges that providers impose at their discretion, <em>i.e.</em>, charges not mandated by a government." They complained that the rule will force them "to display the pass-through of fees imposed by federal, state, or local government agencies on the consumer broadband label."</p>                                            
                                                        
<p>As we've previously written, ISPs could simplify billing and comply with the new broadband-labeling rules by including all costs in their advertised rates. That would give potential customers a clearer idea of how much they have to pay each month and save ISPs the trouble of listing every charge that they currently choose to break out separately.</p>
<p>Rejecting the broadband industry's request, the FCC order yesterday said:</p>
<blockquote><p>[W]e affirm our requirement that providers display all monthly fees with respect to broadband service on the label to provide consumers with clear and accurate information about the cost of their broadband service. We thus decline providers' request that they not disclose those fees or that they instead display an "up to" price for certain fees they choose to pass through to consumers.</p></blockquote>
<p>Specifically, "providers must itemize the fees they add to base monthly prices, including fees related to government programs they choose to 'pass through' to consumers, such as fees related to universal service or regulatory fees," the FCC said.</p>
<p>The FCC was ordered by Congress to implement broadband-label rules. The FCC is requiring ISPs to display the labels to consumers at the point of sale and include information such as the monthly price, additional fees, introductory rates, data caps, charges for data overages, and performance metrics. The FCC rules aren't in force yet because they are subject to a federal Office of Management and Budget (OMB) review under the US Paperwork Reduction Act.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Photogrammetry on Commercial Flights (2021) (211 pts)]]></title>
            <link>https://leifgehrmann.com/2021/09/05/photogrammetry-on-a-plane/</link>
            <guid>37325622</guid>
            <pubDate>Wed, 30 Aug 2023 17:25:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leifgehrmann.com/2021/09/05/photogrammetry-on-a-plane/">https://leifgehrmann.com/2021/09/05/photogrammetry-on-a-plane/</a>, See on <a href="https://news.ycombinator.com/item?id=37325622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p><span>
    <span>05 Sep 2021</span>
  </span></p><p>Airplane window seats are great! Not only are they more comfy and easier to take a nap in, but they also have the most quaint form of in-flight entertainment: The outside world!</p>

<p>But other than taking a few photos of holiday mementos and lens-flaring sunsets, what’s the point? Well, I’m here to prove that you can recycle your photos into something vaguely cool using <strong>photogrammetry</strong>.</p>

<p>For example, see this photo of Edinburgh I took in August 2015:</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/edinburgh.jpg" alt="Photo of Edinburgh from above, taken from an airplane window."></p>

<p>Using <a href="https://qgis.org/">QGIS</a> I can transform this photo onto a map using the ‘georeferencer’ tool. First I mapped several reference points on the photo with identifiable locations on a map of Edinburgh. Then I set the transformation function in the georeferencer settings to be ‘projective’.</p>

<picture>
    <source srcset="https://assets.leifgehrmann.com/posts/2021-09-05/georeferencer-dark.jpg" media="(prefers-color-scheme: dark)">
    <img alt="A screenshot of QGIS demonstrating the georeferencer tool being used on a photo of Edinburgh." src="https://assets.leifgehrmann.com/posts/2021-09-05/georeferencer-light.jpg">
</picture>

<p>After running the transformer, I ended up with a projected image that distorts the photo so that it can be displayed on a world map. You can see the final result by checking out the map below (<a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/edinburgh_2.json">fullscreen</a>).</p>



<center><p><small>
<strong>Hint:</strong> Pan around the map and zoom in to see the transformation up close. Toggle labels using ⓘ to help with identifying features. Toggle 'compare mode' using ◐ to compare the photo with commercial aerial imagery.
</small></p></center>

<p>Using ‘compare mode’ in the map above you might notice the imagery is slightly warped in different parts of the map. For example, parts of Princes Street in the centre of Edinburgh are misaligned with the real map.</p>

<p>That’s the one downside of using simple projective transformation, which assumes the projected surface is always flat. As the illustration below demonstrates, by using a transformation that assumes a flat surface, the ground surface that is in higher elevation to the projected surface will appear to shift away from the camera and conversely points in lower-elevation will shift towards the camera.</p>

<picture>
    <source srcset="https://assets.leifgehrmann.com/posts/2021-09-05/distortion-dark.png" media="(prefers-color-scheme: dark)">
    <img alt="An illustration showing a cross-section of terrain and a plane hovering above the ground. Two lines emanate from an airplane window, symbolising the field of view of a photo taken from the plane. The lines intersect with the ground, and another line joining the intersecting points symbolises the geometrical 'plane' that the imagery would be projected on. The illustration demonstrates that the terrain oscillates above and below the 'plane', which implies that the projected image won't be reliable, precisely because the image transformation assumes the terrain is flat." src="https://assets.leifgehrmann.com/posts/2021-09-05/distortion-light.png">
</picture>

<p>This error becomes more pronounced the closer to the ground the photo is taken. In the example below (<a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/edinburgh_6.json">fullscreen</a>), notice how the <a href="https://en.wikipedia.org/wiki/Broxburn#The_Shale_Bings">‘Greendykes Bing’</a> stretches (see top-right of the photo). The effect is also noticeable with the farmland shifting around to the left of the ‘bing’ where the land dips into a creek.</p>



<p>The other alternative to a ‘projective’ transformation is to use a ‘thin plate spline’, but the distortion effects can only be mitigated by adding a ridiculous number of reference points which can take ages. For this blog post, I’ll be keeping it simple and use the ‘projective’ transformation through all my examples.</p>

<h2 id="applications">Applications</h2>

<p>So what are some applications of having this data? I’ve come up with three:</p>

<h3 id="-open-aerial-data">① Open aerial data</h3>

<p>One benefit to creating my own imagery data is the freedom of ownership. I, as the individual who took the photo, have the right to apply any copyright license I please. So theoretically I can publish this work into the public domain.</p>

<p>There are only two other ways I know of to get imagery right now that are available to be used publicly, both of which have their own problems.</p>

<p><strong>Satellite imagery</strong> in the public domain does exist, such as <a href="https://landlook.usgs.gov/">Landsat 8 and Sentinel-2</a>. But from what I can see, these images have a resolution of ~30 meters. In the photo I took, the midsection of the photo (The centre of the city) has a resolution of ~15 meters. With a better camera setup I could probably get much better results than the casual photo I took.</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/sentinel2look.jpg" alt="Satellite imagery of Edinburgh from the Sentinel-2 Satellite. The image shows clouds covering up parts of Edinburgh and the details of the city are very hard to make out."></p>

<center><p><small>
Satellite imagery of Edinburgh from Sentinel-2.<br>(© Copernicus Sentinel data, 2021)
</small></p></center>

<p><strong>Drone imagery</strong> is also an option. These images are usually taken by consumer drones, and are pretty effective at capturing hyper-local aerial data. Much better quality than what I can achieve through an airplane window!</p>

<p>There are several commercial businesses that do drone photogrammetry, but there are also plenty of hobbyists. The only collective effort I’ve seen of people publicly sharing drone photogrammetry is <a href="https://openaerialmap.org/">OpenAerialMap</a>. The neat thing about OpenAerialMap is that all of the imagery is available under a creative-commons public license (CC-BY).</p>

<p>However the issue with flying drones for collecting imagery is often the legal restrictions. For example in the United Kingdom, the Civil Aviation Authority restricts drones from flying anywhere above people, including in buildings or vehicles, with a no-fly-zone extending all the way to the legal height limit. So while drone imagery is easy to get for rural areas, it’s not so useful for cities. In contrast, commercial flights have no problem flying over cities, so it’s possible to capture imagery of urban environments.</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/regulations.png" alt="An illustration of two drone regulations provided by the Civil Aviation Authority. One requires operators to not fly closer to people than 50 meters. The other requires operators to be the same distance away from people as the distance from the ground when the drone is more than 50 meters above the ground."></p>

<center><p><small>
Drone regulations are very restrictive in urban environments.<br>(© Civil Aviation Authority, 2021)
</small></p></center>

<p>So between satellite imagery and hyper-local drone imagery, capturing aerial data from a commercial aircraft is a neat middle ground. 😁</p>

<h3 id="-amateur-surveying">② Amateur surveying</h3>

<p>It’s also possible to use the aerial imagery for getting very up-to-date information that can be used for mapping purposes, such as contributing data to <a href="https://www.openstreetmap.org/">OpenStreetMap.org</a>. Depending on the quality of the photo, it might be possible to detect newly built infrastructure like roads and buildings.</p>



<p>For example, I took the photo above (<a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/germany_2.json">fullscreen</a>) in March 2017. Some of the wind turbines visible in this photo were literally in the process of being constructed when I took the photo. Since it was March, that would have been several months before the yearly aerial imagery for 2017 was collected, which usually is in the late-spring and summer months.</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/wind-turbines.jpg" alt="A sequence of 3 photos showing the construction of 2 wind turbines viewed from the sky. One showing Summer 2016, one showing March 2017 (which was captured by your's truly), and a third one showing Summer 2017."></p>

<center><p><small>
The gradual construction of a wind turbine from start to finish. My photo reveals the turbine being constructed, but the construction equipment is still visible.<br>
(© PDOK Luchtfoto Beeldmateriaal 25cm, 2016 &amp; 2017)
</small></p></center>

<p>In a lot of countries yearly aerial imagery is very rare. So while my example above is a bit contrived since the Netherlands captures data annually, it could still be useful in other countries.</p>

<p>So aerial data captured on commercial flights has a slight edge to professional aerial data, which is great for amateur mappers who want to get the scoop before the professional surveyors. Of course, you can’t really control <em>what</em> photos you’ll be able to capture because of flight paths and cloud conditions, but it’s still an edge. 😛</p>

<h3 id="-historical-imagery">③ Historical imagery</h3>

<p>Re-projecting a photo can also help with easily comparing historical data.</p>

<p>Time-lapsing pictures captured from the sky can be difficult, but by transforming it to fit on a map, it’s much easier to identify gradual changes to a landscape or city.</p>

<p>For the example below (<a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/netherlands_2.json">fullscreen</a>), you can see what <a href="https://www.natuurmonumenten.nl/projecten/marker-wadden/english-version">Marker Wadden</a> looked like in March, 2017.</p>



<p>For context, below are a sequence of photos that show what Marker Wadden looked like between 2016 and 2017:</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/historical-imagery.jpg" alt=""></p>

<center><p><small>
(© PDOK Luchtfoto Beeldmateriaal 25cm, 2016 &amp; 2017)
</small></p></center>

<h2 id="conclusion">Conclusion</h2>

<p>Hopefully I’ve convinced you that re-projecting your holiday photos is a neat idea with some cool applications! Or at the very least thought it was mildly interesting.</p>

<p>Of course there are several limitations with taking photos from a plane.</p>

<ul>
  <li>You don’t get to control the flight path, meaning you don’t get to choose what specific landmarks you can photograph.</li>
  <li>The weather and lighting is always un-predictable.</li>
  <li>It’s difficult to control the camera’s focus.</li>
  <li>The plane window might have scratches in the plastic, messing up the photo.</li>
  <li>Seat-neighbours probably thinking you are a weirdo for constantly taking pictures, especially during take-off.</li>
</ul>

<p>…but that doesn’t stop us from trying, right? 😄</p>

<p><img src="https://assets.leifgehrmann.com/posts/2021-09-05/extra-photos.jpg" alt="3 photos of different locations, including: Ketelmeer, Netherlands; Edinburgh, Scotland; Koblenz, Germany"></p>

<p>I had a bit of fun doing these transformations, so I dug through my photo library for more photos I could georeference. So here are a few more that I thought looked interesting:</p>

<ul>
  <li><a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/netherlands_1.json">Ketelmeer, Netherlands</a></li>
  <li><a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/edinburgh_1.json">Edinburgh, Scotland (At twilight)</a></li>
  <li><a href="https://map-tile-compare.leifgehrmann.com/#https://tiles.leifgehrmann.com/configs/germany_1.json">Koblenz, Germany</a></li>
</ul>

<hr>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://landlook.usgs.gov/"><em>LandsatLook Viewer</em></a> – United States Geological Survey, 2021</li>
  <li><a href="https://register-drones.caa.co.uk/drone-code"><em>The Drone and Model Aircraft Code</em></a> – Civil Aviation Authority, 2021</li>
  <li><a href="https://www.pdok.nl/viewer/"><em>PDOK Viewer</em></a> – PDOK, 2021</li>
  <li><a href="https://github.com/leifgehrmann/map-tile-compare"><em>map-tile-compare</em></a> – The visualisation tool I made to display the transformed photos for this blog post.</li>
</ul>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google kills two-year “Pixel Pass” subscription after just 22 months (149 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/08/google-kills-two-year-pixel-pass-subscription-after-just-22-months/</link>
            <guid>37325310</guid>
            <pubDate>Wed, 30 Aug 2023 17:07:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/08/google-kills-two-year-pixel-pass-subscription-after-just-22-months/">https://arstechnica.com/gadgets/2023/08/google-kills-two-year-pixel-pass-subscription-after-just-22-months/</a>, See on <a href="https://news.ycombinator.com/item?id=37325310">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      I'm charging Google an early cancellation fee    —
</h4>
            
            <h2 itemprop="description">Two years on a Pixel Pass was supposed to get you a new phone. </h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/chrome_WcXfVjnIRy-800x450.png" alt="Pixel Pass bundled a phone and a bunch of Google services. ">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/08/chrome_WcXfVjnIRy.png" data-height="1080" data-width="1920">Enlarge</a> <span>/</span> Pixel Pass bundled a phone and a bunch of Google services. </p><p>Google</p></figcaption>  </figure>

  





<p>Google's "Pixel Pass" payment plan has perished. The company is shutting down the all-in-one subscription service that let customers pay a monthly fee for a new Pixel Phone and a bunch of Google subscription services like YouTube Premium. When the service launched in October 2021, Google said that every two years on the Pixel Pass would make you eligible for a brand new phone. But the service only lasted 22 months, so no one will be eligible for that phone upgrade.</p>
<p>
Pixel Pass <a href="https://blog.google/products/pixel/introducing-pixel-pass/">was introduced</a> alongside the Pixel 6 launch. For $45 per month, you would get a new Pixel 6, YouTube and YouTube Music Premium, 200GB of Google One storage, "Preferred Care" coverage for your phone, and a "Google Play Pass," a Netflix-style all-you-can-eat subscription for Play Store apps and games. Google said you could "save up to $294 over two years" compared to buying everything individually. A more expensive $55-per-month plan got you a Pixel 6 Pro instead, and there were even plans that included Google Fi Wireless cellular service.</p>
<p>Google explained its latest product shutdown on <a href="https://support.google.com/pixelpass/answer/13968577#zippy=%2Cwhy-is-pixel-pass-being-discontinued">a support page</a>. "Beginning August 29, 2023, Pixel Pass is no longer offered for new Pixel purchases or renewal," the company said.</p>
<p>The support page FAQ asks, "Why is the Pixel Pass being discontinued?" but Google doesn't answer its own question, saying only, "We offer the best value of our hardware products and give users the flexibility to purchase their favorite services. We continue to evaluate offers based on customer feedback and provide different ways for them to access the best of Google."</p>                                            
                                                        
<p>Besides the questionable popularity of a service like this, Google has been raising a lot of its subscription prices lately, and the Pixel Pass's discounts ran counter to those price increases. YouTube Premium has <a href="https://arstechnica.com/gadgets/2023/07/youtube-premium-gets-a-2-price-increase-now-13-99-a-month/">gone up</a> $2 since the Pixel Pass launched, and while Google has raised storage prices for <a href="https://arstechnica.com/gadgets/2023/03/google-workspace-launches-annual-plans-20-price-increase-for-monthly-users/">business Workspace customers</a> and <a href="https://cloud.google.com/storage/pricing-announce">Google Cloud</a>, it hasn't gone after personal Google One accounts yet.</p>
<p>The terms of the Pixel Pass were for two years, and paying the subscription for that time would pay off your Pixel phone. Early cancellation meant a big final bill for the remainder of the phone cost. That won't happen here, though—while new signups are no longer allowed, existing users will be able to finish out their two-year term. The end of the term was supposed to mean re-upping with a shiny new device, but Google now says, "By the end of the 2 year term, you can’t upgrade to a new phone with Pixel Pass."</p>
<p>Once the two years are up and your Pixel Pass subscription is dead, there's still the question of what to do with your subscriptions to the still-surviving Google services. After your two years, Google says, "The included Google subscriptions, such as Google One, Google Play Pass, and YouTube Premium, automatically renew each month until canceled. You’ll receive a monthly bill for Google One, Google Play Pass, and YouTube Premium at the current discounted rate, which is visible in the email sent to you on August 29, 2023 with the subject line, 'An important update on Pixel Pass.' If you are using your Pixel Pass device on Google Fi Wireless, the $5 service discount to your Google Fi Wireless plan also ends."</p>
<p>To take some sting out of the move, Google is offering a "$100 loyalty reward credit" for active Pixel Pass subscribers. You can use it for $100 off a new Pixel phone from the Google Store or Google Fi, and it expires in two years.</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Long Live the 'GPU Poor' – Open-Source AI Grants (313 pts)]]></title>
            <link>https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/</link>
            <guid>37324683</guid>
            <pubDate>Wed, 30 Aug 2023 16:37:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/">https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/</a>, See on <a href="https://news.ycombinator.com/item?id=37324683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	                    <p><span>We believe artificial intelligence has the power to </span><a href="https://a16z.com/2023/06/06/ai-will-save-the-world/"><span>save the world</span></a><span>—and that a thriving open source ecosystem is essential to building this future.</span></p>
<p><span>Thankfully, the open source ecosystem is starting to develop, and we are now seeing open source models that rival closed-source alternatives. Hundreds of small teams and individuals are also working to make these models more useful, accessible, and performant.</span></p>
<p><span>These projects push the state of the art in open source AI and help provide a more robust and comprehensive understanding of the technology. They include: instruction-tuning base LLMs; removing censorship from LLM outputs; optimizing models for low-powered machines; building novel tooling for model inference; researching LLM security issues; and many others.</span></p>
<p><span>However, the people behind these projects often don’t have the resources available to pursue their work to conclusion or maintain it in the long run. The situation is more acute in AI than traditional infrastructure, since even fine-tuning models requires significant GPU computing resources, especially as open source models get larger.</span></p>
<figure id="attachment_324695" aria-describedby="caption-attachment-324695"><a href="https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?ssl=1"><img decoding="async" fetchpriority="high" src="https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=500%2C635&amp;ssl=1" alt="" width="500" height="635" srcset="https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?w=770&amp;ssl=1 770w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=236%2C300&amp;ssl=1 236w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=768%2C975&amp;ssl=1 768w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=98%2C125&amp;ssl=1 98w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=197%2C250&amp;ssl=1 197w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=260%2C330&amp;ssl=1 260w, https://i1.wp.com/a16z.com/wp-content/uploads/2023/08/image1-1.png?resize=496%2C630&amp;ssl=1 496w" sizes="(max-width: 500px) 100vw, 500px" data-recalc-dims="1"></a><figcaption id="caption-attachment-324695"><em><strong>Obligatory xkcd homage (<a href="https://xkcd.com/2347/" target="_blank" rel="noopener">original</a>)</strong></em></figcaption></figure>
<p><span>To help close this resource gap, we’re announcing today the </span><b>a16z Open Source AI Grant program</b><span>. We’ll support a small group of open source developers through grant funding (</span><b>not</b><span> an investment or SAFE note), giving them the opportunity to continue their work without the pressure to generate financial returns.</span></p>
<p><span>We’re also announcing the first batch of grant recipients and funded projects:</span></p>
<ul>
<li><a href="https://huggingface.co/jondurbin"><span>Jon Durbin (Airoboros)</span></a><span>: instruction-tuning LLMs on synthetic data</span></li>
<li><a href="https://huggingface.co/ehartford"><span>Eric Hartford</span></a><span>: fine-tuning uncensored LLMs</span></li>
<li><a href="https://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)"><span>Jeremy Howard (fast.ai)</span></a><span>: fine-tuning foundation models for vertical applications</span></li>
<li><a href="https://huggingface.co/TheBloke"><span>Tom Jobbins (TheBloke)</span></a><span>: quantizing LLMs to run locally</span></li>
<li><a href="https://github.com/vllm-project/vllm"><span>Woosuk Kwon and Zhuohan Li (vLLM)</span></a><span>: library for high-throughput LLM inference</span></li>
<li><a href="https://nousresearch.com/"><span>Nous Research</span></a><span>: </span><span>new fine-tuned language models akin to the Nous Hermes and Puffin series</span></li>
<li><a href="https://github.com/oobabooga/text-generation-webui"><span>oobabooga</span></a><span>: web UI and platform for local LLMs</span></li>
<li><a href="https://github.com/teknium1"><span>Teknium</span></a><span>: synthetic data pipelines for LLM training</span><span><br>
</span></li>
</ul>
<p><span>We want to thank them for their contributions to the field, and for fostering open collaboration, learning, and advancement in AI.</span></p>
<div>
<p>***</p>
<p><em>The views expressed here are those of the individual AH Capital Management, L.L.C. (“a16z”) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein.</em></p>
<p><em>This content is provided for informational purposes only, and should not be relied upon as legal, business, investment, or tax advice. You should consult your own advisers as to those matters. References to any securities or digital assets are for illustrative purposes only, and do not constitute an investment recommendation or offer to provide investment advisory services. Furthermore, this content is not directed at nor intended for use by any investors or prospective investors, and may not under any circumstances be relied upon when making a decision to invest in any fund managed by a16z. (An offering to invest in an a16z fund will be made only by the private placement memorandum, subscription agreement, and other relevant documentation of any such fund and should be read in their entirety.) Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z, and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by Andreessen Horowitz (excluding investments for which the issuer has not provided permission for a16z to disclose publicly as well as unannounced investments in publicly traded digital assets) is available at https://a16z.com/investments/.</em></p>
<p><em>Charts and graphs provided within are for informational purposes solely and should not be relied upon when making any investment decision. Past performance is not indicative of future results. The content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Please see https://a16z.com/disclosures for additional important information.</em></p>
</div>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI shatters revenue expectations, predicted to generate over $1B (116 pts)]]></title>
            <link>https://the-decoder.com/openai-shatters-revenue-expectations-predicted-to-generate-over-1-billion/</link>
            <guid>37324503</guid>
            <pubDate>Wed, 30 Aug 2023 16:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://the-decoder.com/openai-shatters-revenue-expectations-predicted-to-generate-over-1-billion/">https://the-decoder.com/openai-shatters-revenue-expectations-predicted-to-generate-over-1-billion/</a>, See on <a href="https://news.ycombinator.com/item?id=37324503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#primary">Skip to content</a></p>
		<!-- #masthead --><main id="primary">
	<article id="post-7471">

	
<div>
    

    
                <div>
                    
                                            <div>
                                                                    <p><a href="https://the-decoder.com/artificial-intelligence-news/ai-and-society/" rel="category tag">AI and society</a></p><p><span><time datetime="2023-08-30T16:27:02+02:00">Aug 30, 2023</time><time datetime="2023-08-30T16:27:40+02:00">Aug 30, 2023</time></span></p>
                                                            </div>
                    


                    

                                            <div>
                                                            <div>
                                    <p>Midjourney prompted by THE DECODER</p>
                                </div>
                                                        			<div>
									<picture>
						<source media="(min-width:1200px)" srcset="https://the-decoder.com/wp-content/uploads/2023/08/neural_network_printing_money-1200x673.png">
						<source media="(min-width:700px)" srcset="https://the-decoder.com/wp-content/uploads/2023/08/neural_network_printing_money-770x432.png 1x, https://the-decoder.com/wp-content/uploads/2023/08/neural_network_printing_money-1200x673.png 2x">
						<source media="(min-width:376px)" srcset="https://the-decoder.com/wp-content/uploads/2023/08/neural_network_printing_money-770x432.png">
						<img src="https://the-decoder.com/wp-content/uploads/2023/08/neural_network_printing_money.png" data-no-lazy="1" alt="OpenAI shatters revenue expectations, predicted to generate over $1 billion" width="1456" height="816">
					</picture>
							</div><!-- .post-thumbnail -->

		                        </div>
                    
                </div>


                

            <div>
    <div>
                    <p><a href="https://the-decoder.com/author/maximilian-schreiner/" aria-label="Author Avatar"><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MCIgaGVpZ2h0PSI2MCIgdmlld0JveD0iMCAwIDYwIDYwIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBzdHlsZT0iZmlsbDojZjJmMmYyO2ZpbGwtb3BhY2l0eTogMC4xOyIvPjwvc3ZnPg==" alt="" data-src="/resources/images/avatar_maximilian_schreiner.jpg" data-srcset="/resources/images/avatar_maximilian_schreiner.jpg 2x" height="60" width="60" decoding="async"></a></p>
            </div>

    <div>

                    <p>Max is managing editor at THE DECODER. As a trained philosopher, he deals with consciousness, AI, and the question of whether machines can really think or just pretend to.</p>
            
            </div>

    </div>
    </div>
		<div>

			

			
						<p><strong>OpenAI is expected to far exceed previous revenue projections next year, generating more than $1 billion in revenue, according to The Information.</strong></p>
<p>OpenAI is expected to generate more than $1 billion in revenue over the next 12 months, The Information reported, citing an internal source. That figure is well above the estimates OpenAI had previously given investors.</p>
<p>Prior to the launch of ChatGPT, the company projected revenue of just $28 million in 2021. The estimate implies that OpenAI is currently generating more than $80 million in revenue per month.</p>
<h2>OpenAI's technology is already being used by many companies</h2>
<p>The growth suggests that companies are finding commercial applications for OpenAI's conversational AI technology. App developers and companies like Wall Street firm Jane Street appear to be using the technology to make or save money, according to The Information.</p>
<p>Other OpenAI customers include Zoom, Stripe, Notion, and Databricks. Microsoft uses OpenAI technology in Office 365, Bing Chat, GitHub Copilot and more.</p>
<p>Just a few days ago, it also launched <a href="https://the-decoder.com/openai-launches-chatgpt-enterprise-with-enhanced-security-privacy-and-speed/">ChatGPT Enterprise</a>, an enterprise version of the chatbot that promises privacy and direct connectivity to enterprise software - competing with Microsoft's offerings.</p>
<h2>Trend towards customized AI assistants for enterprises</h2>
<p>So the trend is increasingly toward customized AI assistants for enterprises that can use them e.g. to quickly analyze data, inform trades, and add value, as in the case of Jane Street. Citadel is another high-frequency trading firm that has approached OpenAI and others about integrating the technology into its business.</p>
<p>Whether OpenAI can keep such high-profile clients for long remains to be seen: Jane Street recently hired David So, who previously spent eight years at Google and helped develop language models such as PaLM-2 and the Bard chatbot, according to The Information. The hire could be seen as a sign that the company is investing more in developing its own language models.</p>
		</div>
		<!-- .entry-content -->
	



<div>
    <div>
        
<div>
    
    <p>Support our independent, free-access reporting. Any contribution helps and secures our future. Support now:</p>
    
</div>                    <div id="summary"><ul>
 	<li>OpenAI is expected to generate more than $1 billion in revenue next year, well above the company's previous revenue projections.</li>
 	<li>App developers and companies such as Jane Street, Zoom, Stripe, and Microsoft are already using OpenAI technology.</li>
 	<li>The trend is toward custom AI assistants for businesses, with Google, Microsoft, and OpenAI in direct competition.</li>
</ul></div>
                            
            </div>

    <div>
    <div>
                    <p><a href="https://the-decoder.com/author/maximilian-schreiner/" aria-label="Author Avatar"><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MCIgaGVpZ2h0PSI2MCIgdmlld0JveD0iMCAwIDYwIDYwIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBzdHlsZT0iZmlsbDojZjJmMmYyO2ZpbGwtb3BhY2l0eTogMC4xOyIvPjwvc3ZnPg==" alt="" data-src="/resources/images/avatar_maximilian_schreiner.jpg" data-srcset="/resources/images/avatar_maximilian_schreiner.jpg 2x" height="60" width="60" decoding="async"></a></p>
            </div>

    <div>

                    <p>Max is managing editor at THE DECODER. As a trained philosopher, he deals with consciousness, AI, and the question of whether machines can really think or just pretend to.</p>
            
            </div>

    </div>
</div>


</article><!-- #post-7471 -->
</main><!-- #main -->


<!-- #secondary -->

<!-- #colophon -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I wrote, “Scrum is a cancer,” and the Internet had thoughts about it (184 pts)]]></title>
            <link>https://twitter.com/svpino/status/1696869327335571833</link>
            <guid>37324427</guid>
            <pubDate>Wed, 30 Aug 2023 16:24:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/svpino/status/1696869327335571833">https://twitter.com/svpino/status/1696869327335571833</a>, See on <a href="https://news.ycombinator.com/item?id=37324427">Hacker News</a></p>
Couldn't get https://twitter.com/svpino/status/1696869327335571833: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[File Attachments: Databases can now store files and images (123 pts)]]></title>
            <link>https://xata.io/blog/file-attachments</link>
            <guid>37324370</guid>
            <pubDate>Wed, 30 Aug 2023 16:21:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xata.io/blog/file-attachments">https://xata.io/blog/file-attachments</a>, See on <a href="https://news.ycombinator.com/item?id=37324370">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, as part of <a href="https://xata.io/blog/launch-week-august-2023">our launch week</a>, we’re beyond excited to announce a feature that we've wanted to add ever since we started Xata: File Attachments. Think of it as having a new database column type where you can store files of any size, and behind the scenes they are stored in AWS S3 and cached through a global CDN. Files simply become a part of a database record. For example, they respect the same security boundary -- if you can access a record, you can also access its attached files. Image file types also get some extra functionality allowing you to request them at any size and style with built-in transformations. With this release, we aim to simplify your application architecture and reduce the number of services you need to manage.</p>
<p>In this blog we'll dive into the capabilities released today and architecture behind our implementation. We hope you enjoy the read as much as we did building it 😃</p>

<p>Both files and relational data are ubiquitous in today's applications. Even the most basic scenarios generally require both structured relational data and file storage. Think about a blog which uses both posts metadata and images, a product catalogue or a document library. They all require basic data record management and query capabilities, as well as storage and access to large binary objects like images, videos, and documents.</p>
<p>We often see engineers using a relational database with a separate storage service to store the data used by their application. In most cases the binary files are related to the relational data, therefore the common pattern is to store a file link in the database. This experience adds unnecessary friction and we saw an opportunity to simplify. Because file storage use cases are so closely correlated to data, we decided to embed <a href="https://xata.io/docs/sdk/file-attachments">file attachments</a> directly into our serverless database and bring a new experience for building data apps that require binary storage.</p>
<p>We've shown this feature to a number of developers before releasing it, and this is the type of feedback we've received on our approach so far:</p>



<p>Let’s take a deeper look into what gets simplified and how Xata achieves this.</p>

<p>We’re really excited about today's release because it packs a lot of functionality into the simple experience of adding another column to your database. We wanted this experience to feel familiar, like adding and viewing an attachment in a spreadsheet instead of dropping a file into a generic bucket. When we started to work on the file attachments, we had the following goals in mind:</p>
<ol role="list"><li>Relational and storage APIs should share the same endpoints and the same connections. It is easier to work with and maintain one service rather than two.</li><li>APIs must share the same authorization scheme and the same permissions model. Avoid having to use different credentials and keep permissions in sync.</li><li>Relational data and binary object data should share the same region. Both data types should reside in the same compliance boundary and have similar guarantees.</li></ol>
<p>The first step in our design was to acknowledge that relational data and large binary objects have very different consumption models and trying to fit both into the same storage service leads to unacceptable compromises. One size doesn’t fit all. Serving large binary objects from a relational database cannot match the performance of a dedicated storage service in terms of compute cost, concurrency and throughput. The opposite is even more obvious, a storage service cannot match the querying and data management capabilities of a relational database.</p>
<p>Like with any design challenge, we had to devise a solution to address seemingly conflicting requirements. The APIs for relational data and binary objects needed to be unified, while the backend storage had to differ to achieve the expected performance and feature set.</p>

<p>At the API and database schema level, the binary object data type became the <code>file</code> column type. You can attach one or multiple files in a column to a record in your database. This approach allowed all existing Xata features to work with the file type without any API change.</p>

<p>Using the existing Xata REST APIs or SDKs, and the same connection, a developer can now upload a file, download a file, run queries over files using filters, aggregations, joins, and even run search queries to match file metadata.</p>
<p>In the record model the file column holds a <a href="https://xata.io/docs/concepts/file-attachments#record-apis">JSON object</a> with a predefined schema which contains both file metadata and the file content.</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="json"><code><span data-line=""><span>{</span></span>
<span data-line=""><span>  </span><span>"</span><span>name</span><span>"</span><span>:</span><span> </span><span>"</span><span>Butterfree.png</span><span>"</span><span>,</span></span>
<span data-line=""><span>  </span><span>"</span><span>mediaType</span><span>"</span><span>:</span><span> </span><span>"</span><span>image/png</span><span>"</span><span>,</span></span>
<span data-line=""><span>  </span><span>"</span><span>size</span><span>"</span><span>:</span><span> </span><span>75</span><span>,</span></span>
<span data-line=""><span>  </span><span>"</span><span>version</span><span>"</span><span>:</span><span> </span><span>1</span><span>,</span></span>
<span data-line=""><span>  </span><span>"</span><span>attributes</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>    </span><span>"</span><span>height</span><span>"</span><span>:</span><span> </span><span>475</span><span>,</span></span>
<span data-line=""><span>    </span><span>"</span><span>width</span><span>"</span><span>:</span><span> </span><span>475</span></span>
<span data-line=""><span>  </span><span>},</span></span>
<span data-line=""><span>  </span><span>"</span><span>base64Content</span><span>"</span><span>:</span><span> </span><span>"</span><span>iVBORw0KGgoAAAANSUhEUgAAAAIAAAACCAYAAABytg0kAAAAEklEQVR42mNk+M9QzwAEjDAGACCDAv8cI7IoAAAAAElFTkSuQmCC...</span><span>"</span></span>
<span data-line=""><span>}</span></span></code></pre></div>

<p>Thinking further about typical relational data and file storage use cases, we noticed significant commonalities and differences. The management of files maps very well to relational data management. CRUD operations are similar in the sense that they work over organized data, entries have owners, there is a permission model in place, and they offer a level of consistency and durability. In general we can say that the write patterns are fairly similar. An application for managing files can very well use a database API to achieve the same.</p>
<p>The important difference comes with the read patterns. Relational databases are designed for complex queries, which can involve high usage of CPU and memory, while storage reads have minimal CPU and memory usage. Because the operations are extremely simple, the storage reads generally scale to much higher request rate, concurrency and throughput. Therefore storage read patterns expect very high concurrency and throughput with minimal cost. Offering the database read for files retrieval will not meet the expectations for a storage service.</p>
<p>We will call this high scale read use case, the content distribution scenario. To address content distribution, Xata introduces direct access URLs. They can be retrieved by reading the file column. Accessing the URL does not involve a database call and therefore they are NOT subject to Xata concurrency and rate limits, and can make use of the storage service capacity.</p>

<p>Since file attachments share the same endpoints with the Xata APIs, the same authorization scheme applies. Whether you are using API keys or OAuth, the same credentials can be used for managing files. This single service approach reduces complexity on the client side and at the same time guarantees that future authorization methods and future permissions models apply uniformly to both records API and file attachments API.</p>

<p>As we discussed in the previous section, there are two distinct usage scenarios for files. The first is the common relational data approach where the operations are file CRUD and metadata query. The common authorization applies here. Access to the file is conditioned by the user having access to the database record.</p>
<p>The second scenario is the content distribution through direct access URLs. In this case the authorization requirements are very different. If resource management requires a trusted security model, content distribution sometimes requires unrestricted access.</p>
<p>To cater for different URL access needs, Xata provides 3 levels of authorization for the URLs that it generates.</p>
<ol role="list"><li>Public Access - the requests to retrieve the file are not subject to any authentication or authorization. This is very convenient for truly public content, but can be very dangerous if configured by mistake on sensitive data. By default, all uploaded files are private (the access URL requires authentication). Public access can be configured per file to allow maximum flexibility. The default can be updated per column, for scenarios where all files need to be public.</li><li>Signed URL - Xata can generate a signed URL which grants access to anyone having the URL for a specified amount of time. This is commonly used in scenarios where an image is rendered but the URL cannot be further shared because it expires shortly. The default timeout is 1 minute, but it can be configured per file.</li><li>Authenticated URL - the requests need to include a valid Authorization header in order to retrieve the file.</li></ol>
<p>All the access URLs offer lower latency compared to file retrieval through the Xata API because, apart from the authorization check (for signed and authenticated URLs) they go directly to storage skipping Xata middleware and the database service.</p>

<p>Going further on storage performance, modern applications require low latency across the globe.</p>
<p>We could not claim to simplify working with files and then ask customers to configure and manage their own CDN to get reasonable global performance. As a result, the support for file attachments includes built-in CDN capabilities. There is no opt-in and no action is required to enable it; all direct access URLs are served through a CDN by default. In essence, all files retrieved through a URL are cached at the edge, making the following requests blazing fast.</p>
<p>Xata opted to integrate <a href="https://www.cloudflare.com/application-services/products/cdn/">Cloudflare’s Global CDN</a> for its wide geographical coverage, its performance and its feature set.</p>
<p>As with any cache, the great performance improvement comes with the fundamental problem of stale cache entries and cache invalidation. Xata addresses stale cache entries by design using immutable file objects. This means any update to a file is in fact a new file object, with a different ID generating a different URL and eventually a different cache entry. This pattern is also known as <em>versioning</em>, because conceptually the cache keys change with every version of the object.</p>
<p>Following this pattern, when the client application loads a set of records from Xata, it also gets the most up-to-date URLs which are guaranteed NOT to hit a stale cache entry, no matter where the cache is (browser, web proxy, CDN). This is very important because the user can clear the browser cache and Xata can invalidate the CDN cache, but a web proxy in between might still serve a stale cache entry. Versioning guarantees this can never happen. The downside is that file URLs are not persistent and they should not be used as static resources. However, this fits the Xata model of attaching binary objects to database records. Similar to the database records, the URLs are dynamic content and needs to be retrieved through database reads and queries.</p>
<p>We have seen that content cannot be stale, but there is an important note on permissions. When a public object gets cached, changing permissions will not invalidate the cached entry. Making a file private applies immediately for new URLs, but the file is still accessible through old URLs until the cache entry expires in 2h. Xata advises greatest caution when configuring public access, because in practice there is a delay in changing the permissions from public to private.</p>

<p>Looking at the most common scenarios where relational data is used together with binary files, the image use case stands out. All web applications use images today and images tend to require processing before they are rendered. Xata file attachments come with a <a href="https://xata.io/docs/sdk/image-transformations">comprehensive set of image transformations</a> -- from resizing to photo adjustments to changing format and compression.</p>
<p>All transformations are applied at the edge and are cached by default. Again, Xata leverages Cloudflare functionality for image transformations.</p>
<p>We considered more flexible transformation definitions through query parameters or request body objects, but in the end chose the industry standard of defining the transformations in the URL path. This makes it easier to embed the transformation into a web page and the transformation is automatically included in the cache key within the CDN.</p>
<p><code>https://eu-west-1.storage.xata.sh/transform/rotate=180,height=50/nj42n37o4l3dd19fe6vsh4plkk</code></p>

<p>Generally, our philosophy is to abstract away complexity where we can so our end users don’t have to worry about it. File attachments is no exception to this strategy.</p>
<div><p><a href="https://xata.io/mdx/blog/fa_architecture.png"><img alt="File attachments architecture" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_architecture.png&amp;w=3840&amp;q=75"></a></p><p><figcaption>File attachments architecture</figcaption></p></div>
<p>Behind the scenes, the file data is actually stored in two places. The file content is stored as an AWS S3 object while the file metadata (name, type, size, S3 pointer) is stored as a JSON object in the PostgreSQL database table.</p>
<p>We chose S3 for the binary object storage for its high performance, durability, availability and because it shares the same compliance certifications with the AWS Aurora, which we use for running PostgreSQL. This way, all data (relational and binary objects) is located in the same data centers and benefits from the same compliance guarantees.</p>
<p>The fundamental challenge when writing state to two different services is making it transactional. Xata service implements two-phase commit semantics to ensure that a file write either completes successfully or is rolled back. This is one of the key operations where Xata does the heavy lifting and abstracts away the complexity. The client code can rely on the transactional guarantee and no longer be concerned with two-phase commit or dealing with an out of sync state.</p>
<p>A second set of challenges comes around data deletion and cleanup, where again it is not trivial to keep the state in sync between two services. With database tables, Xata takes the cautious approach of delayed cleanup. This allows us to offer undo delete operations (not exposed yet) and to have a general recovery option in case of accidental deletes. When file content storage comes into the picture, it needs to follow the same pattern. Restoring only half of the deleted data is not particularly useful.</p>
<p>The implemented solution treats the PostgreSQL metadata as source of truth and removes the associated file data only when the corresponding records are deleted from PostgreSQL. This is achieved by hooking into the PostgreSQL replication and handling delete record events. This is an elegant way of replicating the deleted state from PostgreSQL to S3, but unfortunately it is only half the solution, because it only handles individual record or value deletes. When data is deleted by dropping entire columns, tables, databases the events are not captured in replication so Xata handles this separately by scheduling bulk deletes.</p>
<p>Backup support adds another level of complexity. Xata creates regular backups and can perform a restore on request. The binary object storage needs to match the ability to restore the state from the time of the database backup. This is achieved through a combination of immutable S3 objects and configuring S3 lifecycle. Deleted files become inaccessible, but they are kept for 7 days after their deletion for recovery purposes. After the 7 days the files are permanently deleted.</p>

<p>If you look at the file column JSON example shown earlier, it might strike you as odd. Files don’t come encoded as base64 and they are certainly not used in their encoded form.</p>
<p>Xata APIs are JSON based and therefore the file content needed to fit in a JSON object. We chose to use Base64 encoding as it is the most common binary encoding and has the widest library support across languages. For a client application that has binary content in a buffer, it should be trivial to encode it as Base64. The Xata SDK offers helpers <a href="https://xata.io/docs/sdk/file-attachments">here</a>.</p>
<p>While we recommend this approach when dealing with small files and when extending existing apps that already use Xata APIs, we acknowledge there are drawbacks in using the Base64 encoding. For very large files or for very high throughput applications, the extra CPU cost of encoding and decoding the files begins to matter and it is exposed in the delivery latency. Also, the extra 33% in file size added by the encoding impacts the size on the wire and implicitly the performance and bandwidth costs.</p>
<p>To alleviate these concerns we introduced <a href="https://xata.io/docs/sdk/file-attachments">binary file APIs</a>. These APIs use similar endpoints but they accept and they retrieve binary content. For files larger than 20MB these are the only APIs for uploading content.</p>
<p>It is important to mention these are still database APIs, they involve reads and writes of metadata to PostgreSQL and they are not direct S3 wrappers.</p>

<p>We have launched this functionality with an example image gallery app to highlight the developer experience you get with Xata and give you a starting point to try it out. This example combines full-text search, aggregations, files, and image transformations. If you’d like to get started, be sure to check out our <a href="https://github.com/xataio/sample-nextjs-chakra-gallery-app">example repository</a>.</p>
<div><p><a href="https://xata.io/mdx/blog/fa_example_gallery_app.png"><img alt="Gallery example with file attachments" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Ffa_example_gallery_app.png&amp;w=3840&amp;q=75"></a></p><p><figcaption>Gallery example with file attachments</figcaption></p></div>

<p>Before releasing file attachments, we decided to open up an early access program for those interested in our community. It was our first early access program since our private beta last year, and we were blown away by the amount of support and engagement we received from our honorary Xataflies. We sincerely want to thank everyone that participated. This feature would not be as amazing as it is today without your help 🙏</p>
<p>Want to join in on the fun with our community? Sign up for our content hackathon <a href="https://xata.io/blog/launch-week-august-2023">here</a> to win some prizes and get some sweet Xata swag.</p>

<p>Xata is simplifying the patterns of working with relational data and binary objects together. If you are facing any of the problems described in this post, we welcome you to <a href="https://app.xata.io/">sign up</a> and try Xata. We’d love your feedback on this feature, if you have any suggestions, questions, or issues reach out to us on <a href="https://xata.io/discord">Discord</a> or follow us on <a href="https://twitter.com/xata">X / Twitter</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[2023 Paid VPN Relationship and Corporate VPN Ownership Map (208 pts)]]></title>
            <link>https://windscribe.com/vpnmap</link>
            <guid>37324202</guid>
            <pubDate>Wed, 30 Aug 2023 16:10:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://windscribe.com/vpnmap">https://windscribe.com/vpnmap</a>, See on <a href="https://news.ycombinator.com/item?id=37324202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This VPN map shows the relationships between VPN companies, their corporate owners, and paid affiliates who profit from reviewing them positively. It includes information on latest community news, ownership changes, and is updated periodically. Every proven relationship between media companies, content sites, corporate VPNs, and independent VPNs that we could find. Use the key below to easily see the links between these entities.</p>

<p><a href="https://kumu.io/sobeyharker/vpn-relationships#vpn-company-relationships">View Full Map</a></p>
<h3>Map Legend</h3>
<ul>
<li><strong>Red</strong> - Corporate Relationship &amp; Ownership</li>
<li><strong>Orange</strong> - Paid Relationship or Paid Affiliates (Dashes)</li>
<li><strong>Blue</strong> - Cooperation or Partnership (These companies may share staff, resources, networks, or facilities with one another).</li>
<li><strong>Purple</strong> - Corporate Media Relationship &amp; Ownership</li>
<li><strong>Brown</strong> - Legal Dispute</li>
<li><i>Hover over nodes to highlight relationships</i></li>
<li><i>Use right-click to focus on select areas</i></li>
</ul>
<p><a href="https://blog.windscribe.com/the-vpn-relationship-map-2023">VPN Relationship Map Highlights 2023</a></p>
<p><a href="https://blog.windscribe.com/the-vpn-relationship-map">VPN Relationship Map Highlights 2022</a></p>
<h2>The Best VPN</h2>
<p><i>Not All VPNs Are Equal</i></p>
<p>The purpose of this map is to highlight the secretly toxic side of the VPN industry, to champion the few heroes in our space that exist, and to showcase how much that corporate spending dictates public opinion of which VPN is best. This map shows the links between VPNs, their actual owners, paid affiliates, and media organizations.
</p>
<h2>Paid VPN Affiliates</h2>
<p><i>The paid incentive to lie for profit</i></p>
<p>The overwhelming majority of VPN comparison sites aren’t providing their services for free.</p>
<p>They get kickbacks. These are disclosed as “affiliate fees” and they earn a set commission per sale.</p>
<p>The most popular VPN review websites can earn tens of thousands of dollars per month simply for providing a link directly to VPNs they are partnered with. In an ideal world, the merits of the VPN being reviewed would prevent these websites from changing their scores.</p>
<p>Unfortunately, after reviewing these sites ourselves we have found that not a single affiliate site on the front page of Google adheres to that principle.</p>
<p>Instead, even VPNs that have suffered breaches, disclosed user information, have security vulnerabilities, or lack technical features displayed by other VPNs score higher.</p>
<p>Because the financial incentive is too strong. With the VPN industry worth $44.6B currently and predicted to break $77B within 3 years the margin for profit will only grow. We can only hope to draw attention to this so people will do their own research and find the solutions that serve them best.</p>
<p>As with Windscribe our biggest source of growth is by referrals. By the technically-minded who find we suit their needs and they refer us to their friends in need of similar solutions.</p>
<h2>Corporate VPN Owners</h2>
<p><i>Friendly branding, uncaring ownership</i></p>
<p>A lot of VPNs have cute branding but vicious owners. While their design team is marvelous, little can be said about those that own said companies.</p>
<p>ExpressVPN, famously known previously as Kape, got into the game by creating malware and seeking ways to scrape personal information. Not hide it. Their owner Teddy Sagi has been served multiple lawsuits pertaining to fraud.
Nord VPN’s owners, Tesonet, are well known for their work in data-mining and collection. That’s just two quick examples.</p>
<p>The VPN space should be first and foremost about privacy and while people can change – companies rarely do. Their legacies paint a pretty clear picture about what they value and what they see as important.</p>
<p>Many VPNs also have the same corporate owner. You’ll notice in the map many VPNs are actually part of the same brand. They have a different paint job, but same engine – as it were.</p>
<h2>Bought Influence</h2>
<p><i>Corporate acquisitions to change brand perceptions</i></p>
<p>Many VPNs featured here have suffered data breaches and loss of customer records. Some have bowed to LEO requests and provided customer data. These events should be something that breaks a VPN company altogether.
</p>
<p>Yet corporate VPNs have other options available to them.</p>
<p>They can simply buy referral sites or buy competitors. Kape, the owners of ExpressVPN, bought Webselenese—giving them control over Safety Detectives and VPNMentor. What should be noted here is that ExpressVPN paid more for these content affiliate websites than they did for some actual VPN companies they acquired.</p>
<p>The affiliate programs that have been mentioned are one thing. But in the VPN space we often have queries from content sites telling us that they will only feature us if we are willing to pay thousands of dollars. While as a small independent VPN we don’t have the budget to pay for this coverage – the larger corporate VPNs certainly do have the budget. As many pay hundreds of thousands of dollars per year to be featured on content sites, have positive news spun about them, and for YouTubers and other influencers to paint them in a positive light.</p>
<p>Some of their partners even go so far to paint other VPNs not affiliated with them poorly.</p>
<h2>Dangerous VPN Claims</h2>
<p><i>False representations of threat modeling can cost lives</i></p>
<p>While a small technical hiccup or mistake can be an issue, outright lying about the threat model a VPN can secure can be dangerous.</p>
<p>In fact, the misrepresentation of what a VPN can do can be outright deadly.</p>
<p>We serve our users across all regions. However, the geopolitics involved can be rather dicey. We often get requests from activists, protestors, and NGOs in countries where information is severely restricted and accessing said information could lead to imprisonment or even death.</p>
<p>While some users just want to expand their digital libraries or keep their habits secret – many of our users are trying to fight for a better tomorrow in their country. They are trying to fight for the rights of their fellow citizens.</p>
<p>They may not however have the technical know-how to properly defend themselves. While we can all point to marketing hype and say “Well, they should know!” you have to consider the fact that information for them may be hard to come by or discern.</p>
<p>Which is why we think it is very important to not misrepresent what a VPN can do. Even at the cost of losing customers.</p>
<h2>Secret VPN Logging</h2>
<p><i>"Zero logs VPN, no Logs VPN!" until caught</i></p>
<p>Many VPNs tout a robust no-logs approach to their userbase. Typically, as a sales tactic to ensure that people sign up. Yet we have discovered over the years that while they say one thing – it comes to light they were actually tracking users after all.</p>
<p>There are some less extreme examples of this such as tracking on websites for business reasons.</p>
<p>Here at Windscribe, we have zero user-tracking for marketing purposes and a <a href="https://windscribe.com/privacy">strict no-log policy</a>. But, many of our competitors have a lot of tracking and ways to fingerprint users on their website to help their ad campaigns sell more VPN licenses.</p>
<p>However, the worst case situation is that they lie about not tracking users and then they get hit with a LEO request they bow down to. That they did actually track that user all along and provide user logs and user information to the authorities. Just to avoid the hassle of being taken to court.</p>
<p>Here at Windscribe we take transparency to be a very important point of trust. Which is why we highly recommend you pay attention to our <a href="https://windscribe.com/transparency">Transparency Report</a>.</p>
<p>We hold user privacy and security to that same high standard. Our server locations all operate with RAM-only diskless setups. So that even if we are served a judgment, they raid our offices and confiscate servers, they cannot recover anything. In fact, despite a few years ago our servers running an older industry standard – when raided by the authorities they found absolutely nothing.</p>
<p>We even disclosed a vulnerability that could have been used which has long since been fixed that other VPNs can still fall prey to. Because once again transparency is important to us. While we are lucky that it was never attempted with us even though we have warned other VPNs of this issue many have chosen to ignore us, as fixing it would be costly and require a significant overhaul of their systems.
</p>
<p>Which as you’re aware from reading this page you’ve learned that many more companies prefer profit over performance.</p>
<h2>Lack of Independent VPNs</h2>
<p><i>The rapidly decreasing pool of non-corporate VPNs</i></p>
<p>We’d like to end this by giving a stark warning. To the right of the map you will see a small pool of VPNs with a little green circle.</p>
<p>That denotes that they are independent of corporate owners. That they alone hold the keys to their systems, that they determine their growth, and the direction they head.</p>
<p>Each year corporate VPN owners buy more media companies, buy more VPNs, and attempt to strangle newcomers to the scene.</p>
<p>While untested and unknown VPNs can be a danger themselves...There are only a handful of surviving independents that can be trusted with your data.</p>
<p>We hope that others will be as stalwart as us. Refusing offers of buyout, refusing to sell to the highest bidder, but ultimately this may not be the case.</p>
<p>So keep your eye on the map, keep yourself educated, and support the VPNs that suit you best.</p>
<p>Because these powerful corporately owned VPNs are doing their utmost to hobble everyone else just so they can control the VPN space and ultimately – you.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keeping Figma Fast: perf-testing the WASM editor (256 pts)]]></title>
            <link>https://www.figma.com/blog/keeping-figma-fast/</link>
            <guid>37324121</guid>
            <pubDate>Wed, 30 Aug 2023 16:01:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figma.com/blog/keeping-figma-fast/">https://www.figma.com/blog/keeping-figma-fast/</a>, See on <a href="https://news.ycombinator.com/item?id=37324121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"><section><div><p><time datetime="August 29, 2023">August 29, 2023</time></p><div><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAFUElEQVQ4jTXSS1NbhwFAYe2ySKdd9Q90pst2posu3Ok0NqEBG2wqJAF6P690da+uHldI4iFZCCSQkAEZYwrB2MLiZeoYDAEMxMF9EHuCcRuPATfBjVOnnWamaeLBzfZ0vOgf+DbnaJ7d/R27S1dZGiswPdjF5s2r7P1pne2VacZyMdrdepTmGsTGanzaakRdFQl7NaVILSPRWkqht7msniIXOo3XrUdztFBid6rAbF+cS8kg86N9vF+5xPXBFP1RN3FHIyFzA0GbDsWuR7FpabWfoUuoIeurouB/i1L4NViPKBjQPKv08nA8zUw6wFDEyXC7yEinn8Gog7xiIhu00hMT6YwFUIMiiugi7DWR9OvI+GpIOX9JSjhFu9yAz9eC5tNrPeyUEkx3+iiFbAwEzYx1CsxfiLFwoZVKMcZovoNMIogk2DAbDRibGwm4DKTEBqK2KhR7DUF/E6JsQ7M/0cV2f4Ryws1w2EY55eHeZJqnm2UO7kzyh9khFseLTBUy9LcqCEY9NdVVNNS9g2xrIOw8S8inRwlY8CtONB9fL3JnqJ3ZtMhMl8i93yZ4vjbMfx4t89XeGodb8zy6fYO9926wPTnGYCxMQ001p976FZamBvyORoJiM/L/wbVKiaWxHFujHexNdfDFapFvHlQ4PrzLvw93+Hxnk6ebaxysrvKXmzdY6M8iGw0Y6k9jb9HismjxuQ34JQtS2I1m4UqejekBniz289VWP68eXuW/f93k+O+f8PXn+7zY3eFoa5NPNzY4WFni7tgQk8koRVUm4jZjN/8Gwd2EHLATVL1oFifOs7s4wJfbo3y7e43vPtvguy//zPG/jvj6xREvdj/iaH2Vz9bXOFy+yc7EIHdKPcwV03SG3NitWjweAwHFSkgV0Hw0287zrQIvH01zfLDGq+e7vPrHPt988QnPn9zn4N46h6vL7C8usHt9lOX+DsaTQQodMqpixeHQ4fHokQMmQqoLzeP1Qf55f4yXT27z8uldjp894NujB+zvLHJ77jIrlXE+vjXP/coYqwNJRmNewi4TgmDCL5nweJvxeJsIKCYir8HfL0+ytzXNwc4yf3u4wYvH2xw8WGe+PEQmnWCk2MuHM5Pcm7zI8oUkw3EJr9OKXXSixASkoA3B97qyETXqRFPubeXGYIqVqVE+fH+eOyuzVCrj9OTTyKpKd6aHD+ZneLw0x055hInuDiTJjzssE+pUkFUXHl8T3tcvhqxosh4do1EXlaEeyuURCiNFUhd66SycR4irOOPdXLs2z+O1Ff44M8XFfC9KQkWIK7hVCXdQwB+yIgaa8UtGNOdFO+XeBJWJAvmRPtSeDIlijq7hDGI6ztvBHhzZKcav3mLw8gyhvhKRfJJQthVLSMEoyQQ7ZFo7nQTCzWjGu2N8MH+J6bmLtOaSuKLtKMkkiVwCf0rlF3KaH8vvcqJtjpMdUxi6h4gOpIhkVYyyiNYp4ItJtOUE2npb0JTzbSzNXWT4Sh/eWBiDJ4Q9EEGK+nGFBE54Wvm+8zJviLP8KDJBUzpLIKVg8Zo4ozNQq2vB6ncQy3jIlOxoZkpJ3r3SS3qgDYciYPD48cWjBBMyVtHOz80S37MM8IZQ5qeRIYyxMGaXgfpztZx8p56qukaaHS0EYw6SGTearbkBRq9kiefC2EQrFtFDNNtGIhfFJjv5WZPAm4YefuAocUJK0SRYaGo5g1ZXR/Xps1SdOYfBrEeQTKgxJ5qNW5cojXehpiVsQgsOyUEin6C9EMMScPMTnYc3tUl+aMlxUgjT4tBjc5zDbNdSe7aOX9fX0WzV4xCMSBEb/wONwfIgw7EI0QAAAABJRU5ErkJggg==" alt="" data-lqip="true"><img data-loading="true" src="https://cdn.sanity.io/images/599r6htc/localized/1470152a8f587f29726a3e89b56e879fe2ef3d48-400x400.png?w=400&amp;h=400&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/1470152a8f587f29726a3e89b56e879fe2ef3d48-400x400.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 200w,https://cdn.sanity.io/images/599r6htc/localized/1470152a8f587f29726a3e89b56e879fe2ef3d48-400x400.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 300w,https://cdn.sanity.io/images/599r6htc/localized/1470152a8f587f29726a3e89b56e879fe2ef3d48-400x400.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 400w" loading="eager" alt=""></p><div><p>Slava Kim</p><!-- --> <p><span>Software Engineer, Figma</span></p></div></div><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAFfklEQVQ4jQXBe0zTiQHA8V/M4i7LeX/MxOwSt11Ovemi6JxRRDNfeFz1wFNPcHCAC4jgARYozysnZZRHQSiv8ipSDmsLpTwLlgJtacuzLS1QWkCm5P65LNntjyXL/vzu8xH+dvxXxB77kDthH3PprogbzxtpezXGnjWL/3o/5n/+X/KjdT/1Lz7kdk4kFao+5ia3GRsKUmz0cHFkjgNqHfuU7XzQ0YNQcOE3SM4dJP/CQXKiTyERZ6BV9RAaK+Wf1pP8y/YR3oGD5Mo+5Xp+HLJWLcM6N3U980S+snHIYGF/l5ZfNKv5QGtEkEYep+rzo9RH/Z6W6E/oiA/HUJyHQ1WPp+cx3pciRl7EkJJ3hzNZD7hdIEFcquRbmYbwutd81NTHPkU7+xSt7O96jVAQdYaG2HC6Ey+h+eY86riztCV+QU92NrrCAgxl3zHwopHaeiUPyyRE5qbyUJxDnrSBtCoNf6rq5EB5EwfK6jlc3YyQFnGMUtEZau6dR3H/AmXRf0YafYGmRzFospLoyc9kuK0Lq9nB8IQFtb6frh90aLUmtIM2in8wIWrRcqupl8cdWgTx9ZM8vXSUpLOHiQ07TFL4CRQJX+KoyWejs5zp6mcYqktwTJjZCrxnK7CHzx1iZSmAb/Ut9oUNBixL6KYWGZpaRKiJ/5y0y8eJOvJrrn96iIxr4ZikOfzU/5L/TOrZ7alm+PsnGFUNrHsCvHv7EwH/Ln5PiO3NPXYC7wn6d1n37rC8sIFQm/ZXEiJOcvXIIWLCjvH3r6OxlBYSVDWyp+nAr5ShFyfQKknHNjbJzuYea6s7eFaCbK6/I7S2S8C9hW9xg/nZZYR+lZL0uyJEp47x5GoE6scpWIqLcFfK2GyoZFleQn9WIorke+hbmvGtBHC7t3E611hZ2WLJtcH8zArONy4sBjPCP4JBVPJyUm9e4XnMbSbyC/FX17JZV82uspIdpRxLURbVD6NpKSlkbtqFc36TqRkPc851rFYvFpML8+A0Iy8NCP/++WfeDPZTkhhHviiSvvR0lmQVeORVeOUylsuK6U1NQCK6gkKciWV8ljlngKlZDw7XOjb7KqYxB4beUXob1AiBQBCjTk9R6iMSLp8jL+o6/ZlZOGVyXGXlGJ6mkXvjIsmXTlEvyWbGNIttbg3LrAfnwjp2hw+j0Yq6RYuyrA5hdMKKpm+Qktw8vo44y72wz6h8cJepEik2qZT6uK+4H/YJiRGf0fm9mLnJKZwOLzbHKkvuII75DQaH7LQ1aamWKhDUhmm6DWYqZJXEXw3nyxO/Q3zzCn3Z39Kf+4wC0V+IPnGIlGvHaS8TY30zgdu7gXdtm9X1XVyLAYbHXHR1DVMrVyFU95qo7xmiWFLI/fDTRP3ht6TcuEaDuIjGwnLS78Tw4PxR0m+do/Z5EYbxKeZ8O3i3f8Qfes/8cgCTeYnX+mna2wYQipSv+K6mm8w0MV9dvkjk6T8SdyeeonIN0rphMp5VkJaYTEl2Bs3t3WgmFul3hpjxv8cd2mPJG2LG7mV8coGBwRmElOIGMkrVPJEoiU/K4X5sGsniF+S32anQ+ajS2KhsH6JRM0zf5Dxqs48Wkw+9M4Rj4x1L/m1sDg+TUy7GxmcRbiYXEifp4Em5gafyQTIVo+S22pHr/XRbgujsAbonl2ky2OgYX0Q56qZCv0in2c8bz1tcvi1m7EuMjk9jNI4jnL6VzBePK0kq6eWpYhxxiw2JyopMbaZRO4HaaEbVb6KspQ9Z5xClmlkKX9poHF1hZDHEnG8Lm3OFkVEzutcDCEciRFy8l4UoVU5sTivx+e18k1vHo+xS0nKKkSqU1Kq1FNW0kqfoRtw8irjNTP3QIsb5IHbfNvYFLyMjk2j7tPwfF+wuZHIzIYEAAAAASUVORK5CYII=" alt="" data-lqip="true"><img data-loading="true" src="https://cdn.sanity.io/images/599r6htc/localized/3562e222a45b14a9b2ee71ed8c596d84e10bad9a-1116x1118.png?w=1116&amp;h=1118&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/3562e222a45b14a9b2ee71ed8c596d84e10bad9a-1116x1118.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 558w,https://cdn.sanity.io/images/599r6htc/localized/3562e222a45b14a9b2ee71ed8c596d84e10bad9a-1116x1118.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 837w,https://cdn.sanity.io/images/599r6htc/localized/3562e222a45b14a9b2ee71ed8c596d84e10bad9a-1116x1118.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1116w" loading="eager" alt=""></p><div><p>Laurel Woods</p><!-- --> <p><span>Software Engineer, Figma</span></p></div></div></div></div><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACH0lEQVQokW2SSU9TURSA+wdsCStNJC5cuGJl2LjVlTEYtN6H0I0uSCCaaEg0MQ5NlBhBJbRAB4ewoRMgCAGDEfra19daOrcPSiGNG//HZ95ALeLiy7lDzpdzzr02e6wfR1TCEZFwhIUZo//BuJfojEpc/OLC/e0h8e1xSnGvxbSB7XiCMJJaZyGBY16YMSToDAu6lwZ5sHGfr1tj5OMeSvI0JXmGsoEu1EUhS6Sv2ysMCTrmLUKCC7HbDK+PsLL1iqLspZKYoST7KMkByrLP2JvCoyr+adOuEzE5F+vHtTZEZMtNIeExknXKCV0YNKIp1Ns0KrTmZ9ERlehaFHQvS/QsuxCrQ3z88ZRsYopK0k8l6aOanD2BrX1GR4PvCAvOLwoGNwd4m7xDMDFCZPs5qjxJJRmgonwyoi6otWEKw8eFpyISZ6K3cK4PEFZHqRWf0Cg+oqZOUDWqClBVglQV/3GhYmK+stWm/iXOLggurznxyvcoZN7RzL/kMPeCXXWKmuJrQxcE0NTPaOqH1rnNHjX/4ekFwaW1mwxv9zKbdqFkxqj/fM9B7hl7mfFWgmahC7X0HI3iCvVcBC0VMCvsWhL0rDrp/96HN9tLRutjrzLK/o6Hw7ybRtaNlvKiKX60lCVMmfLdzByNwhL1nRC1VMC4s13ZuMHr9DU2q1fZb1znd/0uv4oTNPOTNIuPqWfe/JWdwI+mBtFSOj52VQ9/AMOiTudeRKRjAAAAAElFTkSuQmCC" alt="" width="1632" height="919" data-lqip="true"><img data-loading="true" src="https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?rect=1,0,3262,1837&amp;w=1632&amp;h=919&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 816w,https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1224w,https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format 1632w,https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 2448w,https://cdn.sanity.io/images/599r6htc/localized/bb2254ecf6496316f94e482cf1470f6e4c8d83da-3264x1837.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 3262w" loading="eager" alt="" width="1632" height="919"></p></div></section><div><ol><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#the-little-macbook-that-could-until-it-could-no">The little MacBook that could...until it could no more</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#our-dream-setup">Our dream setup</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#embracing-performance-work-in-a-remote-first-world">Embracing performance work in a remote-first world</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#a-tale-of-two-workstreams">A tale of two workstreams</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#the-work-ahead">The work ahead</a></li></ol></div><div><ol><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#the-little-macbook-that-could-until-it-could-no">The little MacBook that could...until it could no more</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#our-dream-setup">Our dream setup</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#embracing-performance-work-in-a-remote-first-world">Embracing performance work in a remote-first world</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#a-tale-of-two-workstreams">A tale of two workstreams</a></li><li><a tabindex="-1" href="https://www.figma.com/blog/keeping-figma-fast/#the-work-ahead">The work ahead</a></li></ol></div><section><div id="progress-indicator-target"><p>When a laptop crashed in an empty office, we knew it was time to overhaul our performance testing framework.</p><div><div><div><p><strong>Illustrations by&nbsp;Rose&nbsp;Wong.</strong></p></div><p>In 2018, all we needed was a single MacBook. At least, that’s all we needed to run our entire in-house performance testing system. The laptop looped the same couple of test scenarios over and over, and reported any changes and timings every hour or so to a shared dashboard. <a href="https://www.figma.com/blog/figma-faster/">We wrote all about it</a> and the optimizations we made, timed with a major performance overhaul to&nbsp;WebAssembly.</p><div><p>In 2018, we <a href="https://www.figma.com/blog/figma-faster/">discovered</a> that restructuring our document renderer and ironing out WebAssembly bugs made Figma 3x&nbsp;faster.</p></div><p>At the time, what we outlined was sufficient. So was the single laptop. Five years on, a lot has changed. Figma’s codebase has become larger and more complex, and Figma has expanded to include plugins, community features, a new product (FigJam), a workspace for developers (Dev Mode), and <a href="https://www.figma.com/release-notes/">too many updates to count</a>. Our team is distributed around the world, far beyond the bounds of a single laptop. As we’ve scaled, we’ve realized we need a more sophisticated approach.</p><div><p><img src="data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAYHBQj/xAAhEAACAgIBBQEBAAAAAAAAAAACAwEEAAURBhIhIzFBof/EABUBAQEAAAAAAAAAAAAAAAAAAAED/8QAGREAAwADAAAAAAAAAAAAAAAAAAECERIT/9oADAMBAAIRAxEAPwBp3W3vJZQRqrC0k85EiIe7jxmbtt/1BrXU5dsEtWxwqIYXxzE/clfWO22NG/XGjaMWIju5mf2cX7XUW9usTN60bAA4OI5j7GENajU4Z0Rb3PvL2f3DJmqXWUqcMzwYwXksMk7RXmz/2Q==" alt="" data-lqip="true"><img data-loading="true" src="https://cdn.sanity.io/images/599r6htc/localized/4cad57473a76fdc6d0254d48e1c992a1079943f1-4032x3024.jpg?w=4032&amp;h=3024&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/4cad57473a76fdc6d0254d48e1c992a1079943f1-4032x3024.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 2016w,https://cdn.sanity.io/images/599r6htc/localized/4cad57473a76fdc6d0254d48e1c992a1079943f1-4032x3024.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 3024w,https://cdn.sanity.io/images/599r6htc/localized/4cad57473a76fdc6d0254d48e1c992a1079943f1-4032x3024.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 4032w" loading="lazy" alt=""></p><p>Running tests on a single laptop is <a href="https://www.reddit.com/r/ProgrammerHumor/comments/11u7tp7/linux_ideapad_server/" target="_blank" rel="noreferrer">common practice</a> for smaller companies. We always try to keep our processes lean and avoid over-engineering, so this approach worked for us until&nbsp;recently.</p></div><h2 id="the-little-macbook-that-could-until-it-could-no"><a href="#the-little-macbook-that-could-until-it-could-no">The little MacBook that could...until it could no&nbsp;more</a></h2><div><p>A <strong>granular performance test</strong> checks detail at scale. In one of our newer tests, we simulated rapid panning around a file with 100 multiplayer editors moving layers and typing new text simultaneously. </p></div><p>This realization happened gradually, and then all at once. First, it became clear that the few large design files we used for testing could no longer represent an ever-growing number of product features, not to mention their edge cases. The best practice would be to run a <strong>granular performance test</strong> specific to each feature, but our team’s expansion to more than 400 engineers and managers made that unsustainable. We increasingly shipped more changes daily, which meant that no single person—or laptop—could keep track of every change that went into&nbsp;a&nbsp;release.</p><p>In early 2020, like so many companies, we started working remotely. But that same MacBook stayed plugged in and running at our office in San Francisco. In October, it overheated. We tried setting up the same tests on another laptop, but we couldn’t get them to run smoothly. We knew that it was time to rethink our testing system and come up with an approach built for&nbsp;scale.</p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w,https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w,https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w,https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w,https://cdn.sanity.io/images/599r6htc/localized/93fa729b869d5b4b35f65a51234c433d677ca878-1608x1072.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w" loading="lazy" alt="A Figma file with many screens" width="804" height="536"></p></div><figcaption>An example of one of the design files we used for testing in 2018</figcaption></figure></div><h2 id="our-dream-setup"><a href="#our-dream-setup">Our dream setup</a></h2><p>It was daunting to overhaul the system, but the silver lining is that it gave us the opportunity to go back to the drawing board and dream up our ideal system. We started by outlining the key challenges we needed to address: a growing number of features that can impact performance, difficulty operating testing hardware, and a dearth of accurate performance metrics. Through that lens, we came up with a few requirements.</p><p>First, we wanted a system that could test every proposed code change in our main monorepo so we could spot performance regressions across all product features early in the development cycle. This would allow us to approach performance proactively, instead of reactively fixing bugs that users report after a feature goes live. Proactive performance is something that’s core to our work at Figma, and that we embody in many other areas of our engineering work. We know that many of our users spend hours a day in Figma; even a small lag can cause major hiccups in their workflows.</p><div><p>Running many tests in parallel (also called <strong>parallel runs</strong>) is a common way to reduce the time spent waiting for the <a href="https://www.atlassian.com/continuous-delivery/continuous-integration" target="_blank" rel="noreferrer">Continuous Integration (CI)</a> to complete. At Figma, we already use parallelization on cloud-hosted CI runners for every other type of&nbsp;testing.</p><p>Running performance tests on real hardware for every pull request would require around 100 identical runners at&nbsp;peak.</p></div><p>Next, the system had to run fast. We decided that any performance guardrail check would need to finish in under 10 minutes—anything beyond that would get in the way of the fast-paced development that’s so central to our engineering culture. To achieve this goal, we’d have to embrace <strong>parallel runs</strong>, requiring us to simultaneously run dozens of performance stress scenarios.</p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?rect=1,0,1607,904&amp;w=528&amp;h=297&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w,https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w,https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w,https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w,https://cdn.sanity.io/images/599r6htc/localized/004a5da1938be4f29d572a5d186529e0c820d4ae-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w" loading="lazy" alt="A FigJam file with tons of colorful sticky notes" width="528" height="297"></p></div><figcaption>One of the early stress tests we created in FigJam</figcaption></figure><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?rect=1,0,1607,904&amp;w=528&amp;h=297&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w,https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w,https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w,https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w,https://cdn.sanity.io/images/599r6htc/localized/1755c4da8ed0ba3889059625647a30ac2c5c5658-1608x904.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w" loading="lazy" alt="A Figma file with tons of comment pins" width="528" height="297"></p></div><figcaption>A stress test with 5,000 comment pins</figcaption></figure></div><p>Lastly, we needed a way to discuss and compare performance. We wanted the ability to reveal the most subtle changes in performance on real hardware, collect CPU profiles, and share links to all this data to drive decisions and continue improving the model moving&nbsp;forward.</p><p>These requirements might sound straightforward, but fine-tuning performance for Figma can be nuanced. Figma’s foundation on the web, combined with the way people use Figma—with tons of distributed collaborators—can make for a uniquely tricky environment. We often contend with massive client data sets, dozens of screens at a time, and constant rendering. Many of our performance challenges more closely resemble those in the <a href="https://www.figma.com/blog/how-figma-draws-inspiration-from-the-gaming-world/">gaming world</a> than ones you might encounter in typical applications.</p><h2 id="embracing-performance-work-in-a-remote-first-world"><a href="#embracing-performance-work-in-a-remote-first-world">Embracing performance work in a remote-first world</a></h2><p>With these goals in mind, we turned our attention to the <em>how</em>. Initially, it was just the core working group, which is typical of engineering projects at Figma. While many other folks helped guide, review, and approve our work at different intervals, as a rule, we always try to keep the working group as lean as possible so we can move quickly. After taking a realistic look at what we could deliver in the first six months of work, our strategy was to get to the big, impactful wins quickly, while also planning to build a system that could sustain long-term growth.</p><p>We thought about how to scale the single MacBook and considered building a device farm with many identical laptops running tests on every commit. As we considered what it would take to execute that, we realized that it would be expensive, in terms of both hardware and maintenance. Running a device farm is incredibly challenging with a small team: Not only do you need to ensure every test laptop is nurtured with the right cooling, network bandwidth, and system updates, but you also need to replace hardware as older laptops start to degrade. Even if we ran all tests on virtual machines, we’d still need seemingly endless engineering resources to tame the performance variance.</p><div><p>Counting <strong>CPU instructions</strong> is great for tracking performance of a traditional algorithm but, in practice, other hardware can become a&nbsp;bottleneck.</p><p>Figma is a graphics-heavy application with a lot of work split between CPU and GPU. The speed of moving data between CPU, GPU, and main memory can slow things down, too (commonly called <strong>“I/O” for “input and output” </strong>speeds).</p></div><p>In talking to internal experts and doing our own research, we learned that some performance testing <strong>counts CPU instructions</strong> executed by the program to deterministically detect regressions, but found challenges with this approach, too. For one, our application is a combination of WASM/WebGL and HTML/JS/CSS code running in a variety of browsers with their own interpreters and just-in-time optimizers. Furthermore, Figma’s editor was not always constrained by CPU—sometimes it would be GPU, and rarely <strong>I/O</strong>.</p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?rect=2,0,3214,1811&amp;w=804&amp;h=453&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w,https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w,https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w,https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w,https://cdn.sanity.io/images/599r6htc/localized/c7e3f545b7e026ce88965427504b8dccc6e7cc0c-3217x1811.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w" loading="lazy" alt="A cloud at the top and a row of computers at the bottom, both connected by unifying system in the middle." width="804" height="453"></p></div><figcaption>We shipped a cloud-based system and a hardware system, both connected by the same CI system.</figcaption></figure></div><p>After considering a few different approaches, we ultimately decided to ship two systems: A cloud-based system would handle mass testing, covering our bases for the majority of situations, and a hardware system would be highly targeted, tackling situations that required more precision. They would be connected by the same CI system, and engineers across product teams would run the same suite of performance test scenarios across&nbsp;them.</p><h3 id="_1-cloud-based-system"><a href="#_1-cloud-based-system">(1) Cloud-based system</a></h3><div><p>There are many ways to mitigate some noise aspects of VMs, like dedicated hosts or RAM disks, <a href="https://dropbox.tech/infrastructure/keeping-sync-fast-with-automated-performance-regression-detectio" target="_blank" rel="noreferrer">like Dropbox did</a>.</p></div><p>The first system runs in GPU-enabled virtual machines, in a headless Chromium process on every code change in every pull request. Virtual machines (VMs) can be tricky for testing performance due to the noise levels of virtualized hardware, loud neighbors on the same underlying host, and inconsistencies in measurement, which all pose problems for accurately timing complex applications. These factors can add a good amount of variance to performance, so we set a 20% pass margin to skip the noise and catch the most egregious and obvious regressions. We decided to only rely on the VM-based system for spotting really large regressions (e.g. a rendering algorithm accidentally turning from <em>O(n)</em> into <em>O(n^2))</em>. Running in VMs also allowed us to run many tests in parallel to hit the 10 minute feedback&nbsp;cycle.</p><p>While building out this system, we had to make a design decision about whether to test components separately (we already had a practice of compiling the C++ editor into a stand-alone binary for tests) or together, end-to-end, in the browser (we already do this for integration testing). We opted for the latter—full in-browser testing with a real GPU. This approach allowed us to capture complex issues involving WebGL rendering, CSS affecting browser layout algorithms, and even accidental misuses of&nbsp;React&nbsp;hooks.</p><div><p>In the future, we could go as far as testing with production backend services, but that requires a lot more engineering investment. <a href="https://engineering.fb.com/2016/08/31/web/browserlab-automated-regression-detection-for-the-web/" target="_blank" rel="noreferrer">Meta illustrates</a> the complexities of catching issues end-to-end beyond the&nbsp;browser.</p></div><h3 id="_2-hardware-system"><a href="#_2-hardware-system">(2) Hardware system</a></h3><p>The second system runs on a small array of test laptops that includes older machines, or those that don’t have the latest hardware (think: older MacBooks, outdated Windows laptops, Chromebooks) and allows our developers to schedule custom runs of any test scenario through the same scheduling system in CI. While it has a slower queue, this system accommodates on-demand runs where consistent user-like hardware adds confidence:</p><div><p>A <strong>hot path</strong> is a code path that is performance-critical, something that occurs many times every&nbsp;second.</p></div><ul><li>Bisecting a subtle regression that made its way to staging or&nbsp;production</li><li>Comparing day-over-day changes to performance on realistic devices</li><li>Experimenting on various approaches to improve the “<strong>hot paths</strong>” such as canvas rendering</li></ul><p>We added a handful of features—shared by both systems—on top of that foundation. While these features weren’t critical to how the systems function, they helped us better understand and report out on&nbsp;performance:</p><ul><li>Two types of test scenarios: those that stress local edits, and those that receive a stream of simulated multiplayer changes</li><li>A detailed HTML report with recorded metrics that’s easy to link to for internal sharing</li><li>A CPU profile for any run to help us gain a deeper understanding of the&nbsp;bottlenecks</li></ul><p>Overall, our approach combined virtual and real hardware, allowing us to guardrail the majority of regressions while providing the tools to our engineers to collaborate on hot areas or suspect code changes remotely.</p></div><div><h2 id="a-tale-of-two-workstreams"><a href="#a-tale-of-two-workstreams">A tale of two workstreams</a></h2><p>After scoping these two approaches and spending six months on design and development, the system went live in October 2022. From there, we focused on collaborating with a few specific performance-focused projects. Here are two case studies on performance, directly from the teammates who led these workstreams.</p><h3 id="improving-rendering-performance"><a href="#improving-rendering-performance">Improving rendering performance</a></h3><p><em>Andrew Chan, Software Engineer,&nbsp;Figma</em></p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?rect=1,0,3262,1399&amp;w=1632&amp;h=700&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 816w,https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1224w,https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format 1632w,https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 2448w,https://cdn.sanity.io/images/599r6htc/localized/096a3d2d2bd2908e2895ec2114033cbccad07d7a-3264x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 3262w" loading="lazy" alt="" width="1632" height="700"></p></div><figcaption></figcaption></figure></div><p>Unlike most of the applications built in the browser, Figma renders a lot of content using WebGL, a programming interface that allows lower-level control over rendering quality and performance at the cost of complexity. There are unique challenges that come along with this, and we’re constantly exploring new ways to improve performance and rendering speed—especially on slower devices that might not have the most powerful graphics card&nbsp;or&nbsp;CPU.</p><p>We recently developed stress test scenarios for rendering complex scenes—like thousands of layers edited by 50 multiplayer users—and took a very deep dive into CPU and GPU profiles. There were some techniques that worked well for some hardware configurations, but added too much overhead on others; based on these investigations, we identified which rendering sub-systems to focus on. After all, there is a wide variety of combinations of different CPUs, GPUs, drivers, operating systems, and even browser behaviors that can affect performance.</p><p>We needed to validate every optimization idea as quickly as possible: Is it actually as impactful as we think? Are there any unintended consequences that bring performance down in older or less performant devices? Validating our ideas with runs on real hardware was the key to prioritizing time-consuming engineering work. In benchmarking low-fidelity prototypes, we learned that many ideas that we initially thought were great candidates turned out to be less impactful—or even harmful—on older computers.</p><p>After a few rounds of quick experimentation, we turned our attention to time-slicing of rendering, which showed promising results. With Figma’s <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/">multiplayer technology</a>, in which multiple people can edit a file at once, Figma’s rendering engine has to quickly respond to remote and local edits alike. To do that, the new algorithm prioritizes local edits and operations over rendering changes produced by other users—just like how modern computers allocate computing time among applications. While this approach remained almost unnoticeable on the latest machines, slower devices saw an improvement on perceived frame&nbsp;rate.</p><p>Throughout our development process, we constantly benchmarked the optimized code path against the baseline and the original prototype. We wanted to make sure that we maintained the desired increase in FPS and some custom metrics we introduced specifically for this project as we iterated on implementation and fine-tuned UX trade-offs. With our team split across New York and San Francisco, it was crucial to have convenient access to test machines, track progress through periodic benchmarks, and support our intuition with experimental data.</p></div><div><h3 id="speeding-up-figjam"><a href="#speeding-up-figjam">Speeding up FigJam</a></h3><p><em>Sean Marney, Software Engineer,&nbsp;Figma</em></p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?rect=0,1,3265,1398&amp;w=1632&amp;h=699&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 816w,https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1224w,https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format 1632w,https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 2448w,https://cdn.sanity.io/images/599r6htc/localized/5c9ff903954900c0c3f2d31af3f107c05aba2084-3265x1399.png?w=1632&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 3264w" loading="lazy" alt="" width="1632" height="699"></p></div><figcaption></figcaption></figure></div><p>Since sharing our <a href="https://www.figma.com/blog/figma-faster/">approach to performance</a> in 2018, we’ve gone through some big changes, including launching another product: FigJam. Figma’s existing rendering and <a href="https://www.figma.com/blog/how-figma-draws-inspiration-from-the-gaming-world/#engineers-as-digital-world-builders">multiplayer interaction systems</a> provided a solid foundation for launch; now that FigJam has been out in the world for over two years, we can see that the optimizations we originally created for Figma aren’t quite optimized for FigJam. For example, users are more likely to quickly add tons of sticky notes and recluster them in FigJam, compared to iterating on designs within a few frames in Figma. As we continued optimizing the system for Figma and FigJam, we had to be careful not to create an optimization that only worked for one part of the product: An optimization benefiting a solo designer creating a complex mockup in Figma may not be helpful for hundreds of people participating in a quick whiteboard session in&nbsp;FigJam.</p><p>As we iterated on FigJam in the early days, we’d fix issues reactively, in response to customer reports or changes in production metrics. But we knew there was a better way. With the help of our new testing framework, we wrote tests covering the majority of essential user flows, and automatically found issues before they hit production—most often before they were even merged into the codebase. During the initial setup, we encountered some headaches (no one likes new, flaky tests), but ultimately the tests proved extremely valuable for preventing regressions from completely innocent-seeming code.</p><p>For example: Did you know that <code>background-filter: blur(0)</code> can fix a variety of Safari rendering bugs but <a href="https://twitter.com/finerflame/status/1636163183898673152?s=20" target="_blank" rel="noreferrer">can also subtly affect its layer compositing speed</a>? I didn’t, nor did the developer who tried to update our sticky page curl animation’s CSS to make it more realistic and more clearly a sticky note. In fact, I’d go so far as to say that pretty much no one should need to&nbsp;know&nbsp;that.</p><div><figure><div><p><img src="https://cdn.sanity.io/images/599r6htc/localized/d38808e911b1b25d3958adfca87387eeae2d3d21-1512x946.png?rect=1,0,1511,946&amp;w=1080&amp;h=676&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/d38808e911b1b25d3958adfca87387eeae2d3d21-1512x946.png?w=1080&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 540w,https://cdn.sanity.io/images/599r6htc/localized/d38808e911b1b25d3958adfca87387eeae2d3d21-1512x946.png?w=1080&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 810w,https://cdn.sanity.io/images/599r6htc/localized/d38808e911b1b25d3958adfca87387eeae2d3d21-1512x946.png?w=1080&amp;q=75&amp;fit=max&amp;auto=format 1080w,https://cdn.sanity.io/images/599r6htc/localized/d38808e911b1b25d3958adfca87387eeae2d3d21-1512x946.png?w=1080&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1511w" loading="lazy" alt="" width="1080" height="676"></p></div><figcaption>A "GPU layers" mode in Safari that we used used to fix a bug that had to do with a CSS animation of the sticky note corner curl</figcaption></figure></div><p>We don’t usually go looking for CSS changes when we find performance degradations, but thanks to our suite of automated tests, we were able to find this bug while it was still ingesting on our staging platform and make a fix before it ever impacted users. The tests run often enough, on the HEAD of our codebase, that we could see a significant spike in frame lengths for core user behaviors. As a result, we were able to identify the exact code change that caused the&nbsp;issue.</p><p>Other times, performance issues may make it further in the process before they’re caught. For example, if a test is temporarily disabled, we won’t get an automated alert about it. Luckily, we can run these tests on older commits, using real hardware in the remotely accessible lab. That means that even these issues can be sussed out, using <code>git bisect</code> to narrow down the commits until we get to a single culprit! At one point, we had an optimization that a…certain engineer wrote...with my initials. While the optimization was great for the 50th percentile of frame rate, it had issues in the 90th percentile, only on specific machines. This meant that while our default system couldn’t catch it, a manual run of a few tests would spot the issue within 10 or so runs of&nbsp;the&nbsp;tests.</p><h2 id="the-work-ahead"><a href="#the-work-ahead">The work ahead</a></h2><p>Performance is a tricky area. Things you expect to be the bottleneck often aren’t. And things you’d never suspect of causing a performance issue can take you by surprise. That’s why it’s so important to establish testing and benchmarking frameworks. We’re now at the point where no one person knows the nuances of all our products and features—and we can’t expect them to! Still, our work impacts our teammates’ work, and vice versa. More than anything, we need to focus on systematically addressing these concerns—it’s essential to maintaining development velocity.</p><p>We’ve come a long way since that single MacBook. Since these improvements went live, we’ve focused on empowering domain experts to drive performance-improving initiatives and putting checks in place to ensure it does not regress over time. With the arrival of remote on-demand test runs, detailed reporting, and carefully picked metrics, we can collaborate on performance-sensitive code across teams and time zones. Plus, we can expand on our existing foundation, rather than reinventing the&nbsp;wheel.</p><p>If this project sounds interesting, <a href="https://www.figma.com/careers/">check out our open roles</a>—we’re hiring!</p></div></div></div><div><form novalidate=""><div><h2>Subscribe for a Shortcut to fresh news and fun surprises</h2><div><p><label><p>I agree to opt-in to Figma's mailing list.</p></label></p></div></div></form></div><section><header><h2>Related articles</h2></header></section><p><section><h2>Create and collaborate with Figma</h2><a href="https://www.figma.com/signup?locale=en" target="_blank" rel="noreferrer">Get started for free</a></section></p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Weight-based motor vehicle tax (219 pts)]]></title>
            <link>https://www.skatteetaten.no/en/rates/weight-based-motor-vehicle-tax/</link>
            <guid>37324104</guid>
            <pubDate>Wed, 30 Aug 2023 16:00:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.skatteetaten.no/en/rates/weight-based-motor-vehicle-tax/">https://www.skatteetaten.no/en/rates/weight-based-motor-vehicle-tax/</a>, See on <a href="https://news.ycombinator.com/item?id=37324104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1">


<p>The amount you have to pay in weight-based motor vehicle tax depends on the weight, the number of axles and the suspension system. Therefore, the weight-based vehicle tax may vary considerably depending on the vehicle. Diesel vehicles must in addition pay an environmentally differentiated tax.</p>

<p><label for="js-rateSelectedYear">Select year</label>


</p>
<hr>

<h3>Motorkjøretøy</h3>
<table data-tablestyle="MsoNormalTable" data-tablelook="1184" aria-rowcount="20">
<tbody>
<tr aria-rowindex="1">
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Avgiftsgruppe (kg)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Luftfjæring (kr)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Annet fjæringssystem (kr)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:2,&quot;335551620&quot;:2,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="2">
<td data-celllook="4369">
<p><em><span data-contrast="auto">2 eller flere aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="3">
<td data-celllook="4369">
<p><span data-contrast="auto">7 500–11 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="4">
<td data-celllook="4369">
<p><em><span data-contrast="auto">2 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="5">
<td data-celllook="4369">
<p><span data-contrast="auto">12 000–12 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">779</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="6">
<td data-celllook="4369">
<p><span data-contrast="auto">13 000–13 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">779</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 392</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="7">
<td data-celllook="4369">
<p><span data-contrast="auto">14 000–14 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 392</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 779</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="8">
<td data-celllook="4369">
<p><span data-contrast="auto">15 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 779</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 478</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="9">
<td data-celllook="4369">
<p><em><span data-contrast="auto">3 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="10">
<td data-celllook="4369">
<p><span data-contrast="auto">12 000–14 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="11">
<td data-celllook="4369">
<p><span data-contrast="auto">15 000–16 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">779</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 033</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="12">
<td data-celllook="4369">
<p><span data-contrast="auto">17 000–18 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 033</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 669</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="13">
<td data-celllook="4369">
<p><span data-contrast="auto">19 000–20 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 669</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 033</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="14">
<td data-celllook="4369">
<p><span data-contrast="auto">21 000–22 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 033</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 898</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="15">
<td data-celllook="4369">
<p><span data-contrast="auto">23 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 898</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 264</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="16">
<td data-celllook="4369">
<p><em><span data-contrast="auto">Minst 4 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="17">
<td data-celllook="4369">
<p><span data-contrast="auto">12 000–24 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 033</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 056</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="18">
<td data-celllook="4369">
<p><span data-contrast="auto">25 000–26 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 056</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 966</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="19">
<td data-celllook="4369">
<p><span data-contrast="auto">27 000–28 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 966</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 451</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="20">
<td data-celllook="4369">
<p><span data-contrast="auto">29 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 451</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 395</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
</tbody>
</table>
<p><span><br>Vogntog (kombinasjoner av kjøretøy)</span></p>
<table data-tablestyle="MsoNormalTable" data-tablelook="1184" aria-rowcount="39">
<tbody>
<tr aria-rowindex="1">
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Avgiftsgruppe (kg)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Luftfjæring (kr)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><strong><span data-contrast="auto">Annet fjæringssystem (kr)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:2,&quot;335551620&quot;:2,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="2">
<td data-celllook="4369">
<p><em><span data-contrast="auto">2 + 1 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="3">
<td data-celllook="4369">
<p><span data-contrast="auto">7 500–13 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="4">
<td data-celllook="4369">
<p><span data-contrast="auto">14 000–15 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="5">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–17 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">435</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">590</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="6">
<td data-celllook="4369">
<p><span data-contrast="auto">18 000–19 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">590</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">787</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="7">
<td data-celllook="4369">
<p><span data-contrast="auto">20 000–21 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">787</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 265</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="8">
<td data-celllook="4369">
<p><span data-contrast="auto">22 000–22 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 265</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 509</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="9">
<td data-celllook="4369">
<p><span data-contrast="auto">23 000–24 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 509</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 378</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="10">
<td data-celllook="4369">
<p><span data-contrast="auto">25 000–27 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 378</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 838</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="11">
<td data-celllook="4369">
<p><span data-contrast="auto">28 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 838</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 414</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="12">
<td data-celllook="4369">
<p><em><span data-contrast="auto">2 + 2 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="13">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–24 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">768</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 209</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="14">
<td data-celllook="4369">
<p><span data-contrast="auto">25 000–25 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 209</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 710</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="15">
<td data-celllook="4369">
<p><span data-contrast="auto">26 000–27 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 710</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 312</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="16">
<td data-celllook="4369">
<p><span data-contrast="auto">28 000–28 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 312</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 701</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="17">
<td data-celllook="4369">
<p><span data-contrast="auto">29 000–30 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 701</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 156</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="18">
<td data-celllook="4369">
<p><span data-contrast="auto">31 000–32 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 156</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 596</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="19">
<td data-celllook="4369">
<p><span data-contrast="auto">33 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 596</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">8 270</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="20">
<td data-celllook="4369">
<p><em><span data-contrast="auto">2 + minst 3 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="21">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–37 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 542</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 153</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="22">
<td data-celllook="4369">
<p><span data-contrast="auto">38 000–40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 153</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">8 205</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="23">
<td data-celllook="4369">
<p><span data-contrast="auto">over 40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">8 205</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">10 988</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="24">
<td data-celllook="4369">
<p><em><span data-contrast="auto">Minst 3 + 1 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="25">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–24 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">768</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 209</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="26">
<td data-celllook="4369">
<p><span data-contrast="auto">25 000–25 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 209</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 710</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="27">
<td data-celllook="4369">
<p><span data-contrast="auto">26 000–27 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 710</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 312</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="28">
<td data-celllook="4369">
<p><span data-contrast="auto">28 000–28 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 312</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 701</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="29">
<td data-celllook="4369">
<p><span data-contrast="auto">29 000–30 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 701</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 156</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="30">
<td data-celllook="4369">
<p><span data-contrast="auto">31 000–32 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 156</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 596</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="31">
<td data-celllook="4369">
<p><span data-contrast="auto">33 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 596</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">8 270</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="32">
<td data-celllook="4369">
<p><em><span data-contrast="auto">Minst 3 + 2 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="33">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–37 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 065</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 476</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="34">
<td data-celllook="4369">
<p><span data-contrast="auto">38 000–40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 476</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">7 407</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="35">
<td data-celllook="4369">
<p><span data-contrast="auto">over 40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">7 407</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">10 745</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="36">
<td data-celllook="4369">
<p><em><span data-contrast="auto">Minst 3 + minst 3 aksler</span></em><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">

</td>
<td data-celllook="4369">

</td>
</tr>
<tr aria-rowindex="37">
<td data-celllook="4369">
<p><span data-contrast="auto">16 000–37 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 500</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 933</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="38">
<td data-celllook="4369">
<p><span data-contrast="auto">38 000–40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 933</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 164</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="39">
<td data-celllook="4369">
<p><span data-contrast="auto">over 40 000</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">4 164</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 374</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
</tbody>
</table>
<h2><span><br>Miljødifferensiert årsavgift for dieseldrevne kjøretøy</span></h2>
<p>Vekt og avgasskravnivå (EURO-standarden).<br>Alle summer er oppgitt i kroner.</p>
<table data-tablestyle="MsoNormalTable" data-tablelook="1184" aria-rowcount="5">
<tbody>
<tr aria-rowindex="1">
<td data-celllook="4369">

</td>
<td colspan="8" data-celllook="4369">
<p><strong><span data-contrast="auto">Avgasskravnivå (EURO)</span></strong><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:2,&quot;335551620&quot;:2,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="2">
<td data-celllook="4369">
<p><strong>Vektklasser (kg)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>Ingen (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>I (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>II (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>III (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>IV (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>V (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>VI eller strengere (kr)&nbsp;</strong></p>
</td>
<td data-celllook="4369">
<p><strong>0-utslipp (kr)&nbsp;</strong></p>
</td>
</tr>
<tr aria-rowindex="3">
<td data-celllook="4369">
<p><span data-contrast="auto">7 500–11 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 569</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 095</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 166</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 319</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">694</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">434</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">109</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">0</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="4">
<td data-celllook="4369">
<p><span data-contrast="auto">12 000–19 999</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">9 136</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">5 076</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 550</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 166</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 143</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">709</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">179</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">0</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
<tr aria-rowindex="5">
<td data-celllook="4369">
<p><span data-contrast="auto">20 000 og over</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">16 244</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">9 305</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">6 600</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">3 967</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">2 094</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">1 301</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">327</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
<td data-celllook="4369">
<p><span data-contrast="auto">0</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335551550&quot;:3,&quot;335551620&quot;:3,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
</td>
</tr>
</tbody>
</table>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[High-Speed AI Drone Overtakes World-Champion Drone Racers (244 pts)]]></title>
            <link>https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html</link>
            <guid>37323834</guid>
            <pubDate>Wed, 30 Aug 2023 15:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html">https://www.news.uzh.ch/en/articles/media/2023/Drone-race.html</a>, See on <a href="https://news.ycombinator.com/item?id=37323834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><time itemprop="dateCreated" datetime="2023-08-30">30.08.2023</time>
        <span>Robotics</span>
    </p>
    <h2>Challenge Accepted: High-speed AI Drone Overtakes World-Champion Drone Racers</h2>
        <p>In a milestone for artificial intelligence (AI), the AI system “Swift”, designed by UZH researchers, has beaten the world champions in drone racing – a result that seemed unattainable just a few years ago. The AI-piloted drone was trained in a simulated environment. Real-world applications include environmental monitoring or disaster response.</p>
    <section>
    
            <ul>
                <li>
                    <a href="https://www.news.uzh.ch/en/alltopics/topics/media.html">Press Releases</a>
                </li>
                <li>
                    <a href="https://www.news.uzh.ch/en/alltopics/topics/research.html">Research</a>
                </li>
                <li>
                    <a href="https://www.news.uzh.ch/en/alltopics/disciplines/science.html">Mathematics and Natural Sciences</a>
                </li>
                <li>
                    <a href="https://www.news.uzh.ch/en/feeds/forschen.html">Forschen</a>
                </li>
                <li>
                    <a href="https://www.news.uzh.ch/en/alltopics/zeitfragen.html">Zeitfragen</a>
                </li>
                <li>
                    <a href="https://www.news.uzh.ch/en/alltopics/dossiers/innovation-en.html">Innovation</a>
                </li>
        </ul>
</section>
</div><section>
    
<!-- component: TextImage -->
<div>
      
         
        
        <figure>
        
        
            <div>
      <p><img src="https://www.news.uzh.ch/dam/jcr:9475cedb-db52-4aaf-83c6-7d1f48120b36/Figure1a%20Hauptbild.jpg" alt="">
      </p>
    </div>
            
            <!-- legacy news image Gallery -->
            
            
              <figcaption>The AI-trained autonomous drone (in blue) managed the fastest lap overall, half a second ahead of the best time of a human pilot. (Image: UZH / Leonard Bauersfeld)</figcaption>
        </figure>
     
      <div>
         <p>Remember when IBM’s Deep Blue won against Gary Kasparov at chess in 1996, or Google’s AlphaGo crushed the top champion Lee Sedol at Go, a much more complex game, in 2016? These competitions where machines prevailed over human champions are key milestones in the history of artificial intelligence. Now a group of researchers from the University of Zurich and Intel has set a new milestone with the first autonomous system capable of beating human champions at a physical sport: drone racing.</p>

<p>The AI system, called Swift, won multiple races against three world-class champions in first-person view (FPV) drone racing, where pilots fly quadcopters at speeds exceeding 100 km/h, controlling them remotely while wearing a headset linked to an onboard camera.</p>
 
      </div>
     
   </div>
<!-- /component: TextImage -->

<!-- component: TextImage -->
<div>
         <h3>Learning by interacting with the physical world</h3>

<p>“Physical sports are more challenging for AI because they are less predictable than board or video games. We don’t have a perfect knowledge of the drone and environment models, so the AI needs to learn them by interacting with the physical world,” says Davide Scaramuzza, head of the Robotics and Perception Group at the University of Zurich – and newly minted drone racing team captain.</p>

<p>Until very recently, autonomous drones took twice as long as those piloted by humans to fly through a racetrack, unless they relied on an external position-tracking system to precisely control their trajectories. Swift, however, reacts in real time to the data collected by an onboard camera, like the one used by human racers. Its integrated inertial measurement unit measures acceleration and speed while an artificial neural network uses data from the camera to localize the drone in space and detect the gates along the racetrack. This information is fed to a control unit, also based on a deep neural network that chooses the best action to finish the circuit as fast as possible.</p>
 
      </div>
<!-- /component: TextImage -->

<!-- component: TextImage -->
<div>
         <h3>Training in an optimised simulation environment</h3>

<p>Swift was trained in a simulated environment where it taught itself to fly by trial and error, using a type of machine learning called reinforcement learning. The use of simulation helped avoid destroying multiple drones in the early stages of learning when the system often crashes. “To make sure that the consequences of actions in the simulator were as close as possible to the ones in the real world, we designed a method to optimize the simulator with real data,” says Elia Kaufmann, first author of the paper. In this phase, the drone flew autonomously thanks to very precise positions provided by an external position-tracking system, while also recording data from its camera. This way it learned to autocorrect errors it made interpreting data from the onboard sensors.</p>
 
      </div>
<!-- /component: TextImage -->

<!-- component: TextImage -->
<div>
         <h3>Human pilots still adapt better to changing conditions</h3>

<p>After a month of simulated flight time, which corresponds to less than an hour on a desktop PC, Swift was ready to challenge its human competitors: the 2019 Drone Racing League champion Alex Vanover, the 2019 MultiGP Drone Racing champion Thomas Bitmatta, and three-times Swiss champion Marvin Schaepper. The races took place between 5 and 13 June 2022, on a purpose-built track in a hangar of the Dübendorf Airport, near Zurich. The track covered an area of 25 by 25 meters, with seven square gates that had to be passed in the right order to complete a lap, including challenging maneuvers including a Split-S, an acrobatic feature that involves half-rolling the drone and executing a descending half-loop at full speed.</p>

<p>Overall, Swift achieved the fastest lap, with a half-second lead over the best lap by a human pilot. On the other hand, human pilots proved more adaptable than the autonomous drone, which failed when the conditions were different from what it was trained for, e.g., if there was too much light in the room.</p>

<p>Pushing the envelope in autonomous flight is important way beyond drone racing, Scaramuzza notes. “Drones have a limited battery capacity; they need most of their energy just to stay airborne. Thus, by flying faster we increase their utility.” In applications such as forest monitoring or space exploration, for example, flying fast is important to cover large spaces in a limited time. In the film industry, fast autonomous drones could be used for shooting action scenes. And the ability to fly at high speeds could make a huge difference for rescue drones sent inside a building on fire.</p>

<h3>Literature:</h3>

<p>Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen Koltun, Davide Scaramuzza: Champion-Level Drone Racing using Deep Reinforcement Learning. Nature. 31 August 2023. DOI:&nbsp;10.1038/s41586-023-06419-4</p>
 
      </div>
<!-- /component: TextImage -->

<!-- component: TextImage -->
<div>
      
         
        
        <figure>
        
        
            <div>
      <p><img src="https://www.news.uzh.ch/dam/jcr:1ce74cd3-1051-40c0-acf0-d7546054f740/503_2021.06.11_Swiss%20Drone%20Days_RS_8921_B_kl_CMS.jpg" alt="">
      </p>
    </div>
            
            <!-- legacy news image Gallery -->
            
            
              <figcaption>The races took place in June 2022 on a purpose-built track in a hangar at Dübendorf Airport near Zurich. (Photo: Regina Sablotny)</figcaption>
        </figure>
     
      
     
   </div>
<!-- /component: TextImage -->

    <div>
            <h2>UZH News – Innovation</h2>
            
            
        </div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On (My) Caste (498 pts)]]></title>
            <link>https://www.cs.toronto.edu/~meel/opinions/caste.html</link>
            <guid>37323760</guid>
            <pubDate>Wed, 30 Aug 2023 15:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.toronto.edu/~meel/opinions/caste.html">https://www.cs.toronto.edu/~meel/opinions/caste.html</a>, See on <a href="https://news.ycombinator.com/item?id=37323760">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>


 
Allow me to introduce you to one of the largest population groups in India,
as recognized by the constitution of India: Other Backward Classes 
(<a href="https://en.wikipedia.org/wiki/Other_Backward_Class">OBC Category</a>). 
OBCs are estimated to comprise 40-45% of India's population, therefore, about
500 million people belong to OBC category. 
In other words, 1 in 16 people in the world belongs to the OBC category. As you might have already guessed,
OBCs have historically been socio-economically disadvantaged. The wiki page is a good start. 
</p><p>


The purpose of this post is three-fold: (1) re-claim my identity in its entirety, 
(2) inform the broader public, and (3) reach out to someone searching for an OBC who
"made" it in academia.</p><p>

 It is, perhaps, appropriate to address the elephant in the room:
 <b>I belong to OBC Category</b>. 
 Phew, this was not as hard as I anticipated. 
Typically, one's surname (last name) is a giveaway and most Indians can reasonably identify someone's
caste based on the last name. Fortunately, my last name is rather uncommon and in such 
scenarios one often gets benefit of the doubt, and therefore, very few people have been aware of 
my caste.
 I had begun to mention references to my caste in Facebook/Twitter threads in the past two years. I was, however, not ready
 to publicly declare it <b> until I received tenure</b> as it seemed too risky. </p><p>

 An uninitiated might be forgiven for not realizing that caste-based discrimination is rampant in India
 (yes, even among faculty members  at <a href="https://kafila.online/2019/04/10/the-saderla-story-courage-in-the-face-of-violent-prejudice-manindra-agrawal/">
   IITs</a>), and perhaps worse among Non-Resident Indians (NRIs). 
 Therefore, there was always fear of what would a potential letter writer or 
 someone on tenure evaluation committee think of me if they knew I belonged to OBC.
 It was the same fear that stopped me from mentioning anything about my caste in any of DEI statements that I prepared for the job search or tenure: I had to pretend not to know what it feels to be under-represented. 
</p><p>

As a professor at a  public university, I feel I ought to inform  broader public about the lack of representation of OBCs. I think I can summarize the lack of representation 
with the help of <b> a claim that I believe is true: There are at most five tenure-track faculty who belong to OBC category 
among all the faculty members in North America's "top" 50 CS departments. </b> 
Any reasonable process to pick 50 CS departments should suffice. 
I will, of course, be overjoyed to be corrected. 
</p><p>

You might ask: What evidence do I have to support my claim? 
</p><p>

 To this end, in early 2023, I sought to determine the distribution of CS faculty according to their castes at the five of the most pretigious Indian Institute of Technology (IITs). I relied on Right to Information (RTI) queries to individual IITs to gather data. The result are either shocking or not surprising, depending on how much one knows about the caste system. Well, without further adieu, here are the responses from each of the IITs:



</p></div><ul>
<li> <a href="http://www.cs.toronto.edu/~meel/attachments/caste/iitb-status.pdf">IIT Bombay </a> </li>
<li> <a href="http://www.cs.toronto.edu/~meel/attachments/caste/iitd-status.pdf">IIT Delhi </a> </li>
<li> <a href="http://www.cs.toronto.edu/~meel/attachments/caste/iitk-status.pdf">IIT Kanpur </a> </li>
<li> <a href="http://www.cs.toronto.edu/~meel/attachments/caste/iitm-status.pdf">IIT Madras </a> </li>
<li> <a href="http://www.cs.toronto.edu/~meel/attachments/caste/iitkgp-status.pdf">IIT Kharagpur</a>  </li>
</ul><p>

Here is the summary: <b>There is exactly one out of 180 CS faculty members across five
  IITs who belongs to OBC category.</b></p><p>

While the past and present draw a bleak picture concerning the representations of OBCs,
I remain wishful about the future, but there is work to be done. One of the reasons 
I wanted to write this post was because I remember craving a role model belonging to
the OBC category. Therefore, if an OBC person is contemplating a Ph.D. or faculty position, I want them to know: Yes, "we" can succeed. Please reach out to me if I can help, and let me assure you I would be happier than you if I were to hear from you. <b>It's been a lonely road for far too long. </b>

</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The End of the Googleverse (167 pts)]]></title>
            <link>https://www.theverge.com/23846048/google-search-memes-images-pagerank-altavista-seo-keywords</link>
            <guid>37323727</guid>
            <pubDate>Wed, 30 Aug 2023 15:33:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/23846048/google-search-memes-images-pagerank-altavista-seo-keywords">https://www.theverge.com/23846048/google-search-memes-images-pagerank-altavista-seo-keywords</a>, See on <a href="https://news.ycombinator.com/item?id=37323727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>The first thing ever searched on Google was the name Gerhard Casper, a former Stanford president. As <a href="https://www.newyorker.com/magazine/2012/04/30/get-rich-u">the story goes</a>, in 1998, Larry Page and Sergey Brin demoed Google for computer scientist John Hennessy. They searched Casper’s name on both AltaVista and Google. The former pulled up results for Casper the Friendly Ghost; the latter pulled up information on Gerhard Casper the person.</p><p>What made Google’s results different from AltaVista’s was its algorithm, <a href="https://www.wired.com/2005/08/battelle/">PageRank</a>, which organized results based on the amount of links between pages. In fact, the site’s original name, BackRub, was a reference to the backlinks it was using to rank results. If your site was linked to by other authoritative sites, it would place higher in the list than some random blog that no one was citing.</p><p>Google officially went online later in 1998. It quickly became so inseparable from both the way we use the internet and, eventually, culture itself, that we almost lack the language to describe what Google’s impact over the last 25 years has actually been. It’s like asking a fish to explain what the ocean is. And yet, all around us are signs that the era of “peak Google” is ending or, possibly, already over.&nbsp;</p><div><p>This year, <em>The Verge </em>is exploring how Google Search has reshaped the web into a place for robots —&nbsp;and how the emergence of AI threatens Google itself.</p></div><p>There is a <a href="https://news.ycombinator.com/item?id=35861915">growing</a> <a href="https://www.theguardian.com/commentisfree/2023/mar/11/users-advertisers-we-are-all-trapped-in-the-enshittification-of-the-internet">chorus</a> of <a href="https://www.theverge.com/23711172/google-amp-accelerated-mobile-pages-search-publishers-lawsuit">complaints</a> that Google is not as accurate, as competent, as dedicated to search as it once was. The rise of massive closed algorithmic social networks like Meta’s Facebook and Instagram began eating the web in the 2010s. More recently, there’s been a shift to entertainment-based video feeds like TikTok — which is now being used as a primary search engine by a new generation of internet users.&nbsp;</p><p>For two decades, Google Search was the largely invisible force that determined the ebb and flow of online content. Now, for the first time since Google’s launch, a world without it at the center actually seems possible. We’re clearly at the end of one era and at the threshold of another. But to understand where we’re headed, we have to look back at how it all started.</p><p>If you’re looking for the moment Google truly crossed over into the zeitgeist, it was likely around 2001. In February 2000, Jennifer Lopez wore her iconic green Versace dress to the Grammys, which former Google CEO Eric Schmidt <a href="https://www.bbc.com/news/entertainment-arts-49780020">would later say</a> searches for inspired how Google Image Search functioned when it launched in summer 2001. That year was also the moment when users began to realize that Google was important enough to hijack.&nbsp;</p><p>The term “Google bombing” was first coined by Adam Mathes, now a product manager at Google, who first described the concept in April 2001 while writing for the site Uber.nu. Mathes successfully used the backlinks that fueled PageRank to make the search term “talentless hack” bring up his friend’s website. Mathes did not respond to a request for comment.</p><p>A humor site called Hugedisk.com, however, successfully pulled it off first in January 2001. A writer for the site, interviewed under the pseudonym Michael Hugedisk, <a href="https://www.wired.com/2007/01/remembering-the-first-google-bomb/">told <em>Wired</em></a> in 2007 that their three-person team linked to a webpage selling pro-George W. Bush merchandise and was able to make it the top result on Google if you searched “dumb motherfucker.”</p><p>“One of the other guys who ran the site got a cease and desist letter from the bombed George Bush site’s lawyers. We chickened out and pulled down the link, but we got a lot of press,” Hugedisk recounted.</p><p>“It’s difficult to see which factors contribute to this result, though. It has to do with Google’s ranking algorithm,” a Google spokesperson <a href="https://www.wired.com/2001/01/google-link-is-bush-league/">said of the stunt</a> at the time, calling the search results “an anomaly.”</p><p>But it wasn’t an anomaly. In fact, there’s a way of viewing the company’s 25-year history as an ongoing battle against users who want to manipulate what PageRank surfaces.</p><p>“[Google bombing] was a popular thing — get your political enemy and some curse words and then merge them in the top Google Image resolve and sometimes it works,” blogger Philipp Lenssen told <em>The Verge</em>. “Mostly for the laughs or giggles.”</p><div><p>There’s a way of viewing the company’s 25-year history as an ongoing battle against users who want to manipulate what PageRank surfaces</p></div><p>Lenssen still remembers the first time he started to get a surge of page views from Google. He had been running a gaming site called <a href="https://www.gamesforthebrain.com/">Games for the Brain</a> for around three years without much fanfare. “It was just not doing anything,” he told <em>The Verge</em>. “And then, suddenly, it was a super popular website.”</p><p>It can be hard to remember how mysterious these early run-ins with Google traffic were. It came as a genuine surprise to Lenssen when he figured out that “brain games” had become a huge search term on Google. (Even now, in 2023, Lenssen’s site is still the first non-sponsored Google result for “brain games.”)</p><p>“Google kept sending me people all day long from organic search results,” he said. “It became my main source of income.”</p><p>Rather than brain games, however, Lenssen is probably best known for a blog he ran from 2003 to 2011 called Google Blogoscoped. He was, for a long time, one of the main chroniclers of everything Google. And he remembers the switch from other search engines to Google in the late 1990s. It was passed around by word of mouth as a better alternative to AltaVista, which wasn’t the biggest search engine of the era but was considered the best one yet.</p><p>In 2023, search optimization is a matter of sheer self-interest, a necessity of life in a Google-dominated world. The URLs of new articles are loaded with keywords. YouTube video titles, too — not too many, of course, because an overly long title gets cut off. Shop listings by vendors sprawl into wordy repetition, like side sign spinners reimagined as content sludge. And it goes beyond just Google’s domain. Solid blocks of blue hashtags and account tags trail at the end of influencer Instagram posts. Even teenagers tag their TikToks with #fyp — a hashtag thought to make it more likely for videos to be gently bumped into the algorithmic feeds of strangers.&nbsp;</p><p>The word SEO “kind of sounds like spam when you say it today,” said Lenssen, in a slightly affected voice. “But that was not how it started.”&nbsp;</p><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/376x251/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/384x256/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/415x277/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/480x320/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/540x360/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/640x427/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/750x500/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/828x552/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/1080x720/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/1200x800/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/1440x960/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/1920x1280/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/2048x1365/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/2400x1600/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2000/2400x1600/filters:focal(1500x1000:1501x1001):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874223/236755_When_Google_ruled_the_world_MRohn_002.jpeg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p>To use the language of today, Lenssen and his cohort of bloggers were the earliest content creators. Their tastes and sensibilities would inflect much of digital media today, from <em>Wordle</em> to food Instagram. It might seem unfathomable now, but unlike the creators of 2023, the bloggers of the early 2000s weren’t in a low-grade war with algorithms. By optimizing for PageRank, they were helping Google by making it better. And that was good for everyone because making Google better was good for the internet.</p><p>This attitude is easier to comprehend when you look back at Google’s product launches in these early years —&nbsp;Google Groups, Google Calendar, Google News, Google Answers. The company also acquired Blogger in 2003.&nbsp;</p><p>“Everything was done really intelligently, very clean, very easy to use, and extremely sophisticated,” said technologist Andy Baio, who still blogs at Waxy.org. “And I think that Google Reader was probably the best, like one of the best, shining examples of that.”&nbsp;</p><p>“Everybody I knew was living off Google Reader,” recalled Scott Beale of Laughing Squid.</p><p>Google Reader was created by engineer Chris Wetherell in 2005. It allowed users to take the RSS feeds — an open protocol for organizing a website’s content and updates — and add those feeds into a singular reader. If Google Search was the spinal cord of 2000s internet culture, Google Reader was the central nervous system.&nbsp;</p><p>“They were encouraging people to write on the web,” said<strong> </strong>Baio. Bloggers like Lenssen, Baio, and Beale felt like everything Google was doing was in service of making the internet better. The tools it kept launching felt tied to a mission of collecting the world’s information and helping people add more content to the web.&nbsp;</p><div><p>Lenssen said he now sees SEO as more or less part of the same nefarious tradition as Google bombing</p></div><p>Many of these bloggers feel differently now. Lenssen said he now sees SEO as more or less part of the same nefarious tradition as Google bombing. “You want a certain opinion to be in the number one spot, not as a meme but to influence people,” he said. Most of the other bloggers expressed a similar change of heart in interviews for this piece.</p><p>“When Google came along, they were ad-free with actually relevant results in a minimalistic kind of design,” Lenssen said. “If we fast-forward to now, it’s kind of inverted now. The results are kind of spammy and keyword-built and SEO stuff. And so it might be hard to understand for people looking at Google now how useful it was back then.”</p><p>But there is one notable holdout among these early web pioneers: Danny Sullivan, who, during this period, became the world’s de facto expert on all things search. (Which, after the dawn of the millennium, increasingly just became Google Search.) Sullivan’s expertise gives his opinion some weight, though there is one teeny little wrinkle — since 2017, he’s been an employee of Google, working as the company’s official search liaison. Which means even if he doesn’t think they are, his opinions about search now have to be in line with Google’s opinions about search.</p><p>According to Sullivan, the pattern of optimizing for search predates Google —&nbsp; it wasn’t the first search engine, after all. As early as 1997, people were creating “doorway pages” — pages full of keywords meant to trick web crawlers into overindexing a site.&nbsp;</p><p>More crucially, Sullivan sees Google Search not as a driver of virality but as a mere echo.</p><p>“I just can’t think of something that I did as a Google search that caused everybody else to do the same Google search,” Sullivan said. “I can see that something’s become a meme in some way. And sometimes, it could even be a meme on Google Search, like, you know, the Doodles we do. People will say, ‘Now you got to go search for this; you’ve got to go see it or whatever.’ But search itself doesn’t tend to cause the virality.”&nbsp;</p><p>Those hundreds of millions of websites jockeying for placement on the first page of results don’t influence how culture works, as Sullivan sees it. For him, Google Search activity does not create more search activity. Decades may have passed, but people are essentially still searching for “Jennifer Lopez dress.” Culture motivates what goes into the search box, and it’s a one-way street.&nbsp;</p><p>But causality is both hard to prove and disprove. The same set of facts that leads Sullivan to discount the effect of Google on culture can just as readily point to the opposite conclusion.</p><div><p>That same month, what is largely considered to be the first real internet meme, “All Your Base Are Belong To Us,” was launched into the mainstream</p></div><p>In February 2001, right after Hugedisk’s Google bomb, Google launched Google Groups, a discussion platform that integrated with the internet’s first real social network, Usenet. And that same month, what is largely considered to be the first real internet meme, “All Your Base Are Belong To Us,” was launched into the mainstream after years of bouncing around as a message board inside joke. It became one of the largest search trends on Google, and an archived <a href="https://archive.google/press/timeline.html">Google Zeitgeist report</a> even lists the infamous mistranslated video game cutscene as one of the top searches in February 2001.&nbsp;</p><p>Per Sullivan’s logic, Google Groups added better discovery to both Usenet and the myriad other message boards and online communities creating proto-meme culture at the time. And that discoverability created word-of-mouth interest, which led to search interest. The uptick in searches merely reflected what was happening outside of Google.&nbsp;</p><p>But you can just as easily conclude that Google — in the form of Search and Groups —&nbsp;drove the virality of “All Your Base Are Belong To Us.”&nbsp;</p><p>“All Your Base Are Belong To Us” had been floating around message boards as an animated GIF as early as 1998. But after Google went live, it began mutating the way modern memes do. A fan project <a href="http://www.overclocked.org/OCzerowing.htm">launched</a> to redub the game, the meme got a page on <a href="https://www.newgrounds.com/portal/view/11940">Newgrounds</a>, and most importantly, the first Photoshops of the meme showed up in a <a href="https://web.archive.org/web/20120114133510/http://frogstar.com/content/time-line-all-your-base-are-belong-us">Something Awful thread</a>. (Consider how much harder it would have been, pre-Google, to find the assets for “All Your Base Are Belong To Us” in order to remix them.)&nbsp;</p><p>That back and forth between social and search would create pathways for, and then supercharge, an online network of independent publishers that we now call the blogosphere. Google’s backlink algorithm gave a new level of influence to online curation. The spread of “All Your Base Are Belong To Us” — from message boards, to search, to aggregators and blogs — set the stage for, well, how everything has worked ever since.</p><p>SEO experts like Sullivan might rankle at the idea that Google’s PageRank is a social algorithm, but it’s not <em>not</em> a social mechanism.&nbsp;</p><p>We tend to think of “search” and “social” as competing ideas. The history of the internet between the 2000s and the 2010s is often painted as a shift from search engines to social networks. But PageRank does measure online discussion, in a sense —&nbsp;and it also influences how discussion flows. And just like the algorithms that would eventually dominate platforms like Facebook years later, PageRank has a profound effect on how people create content.&nbsp;</p><p>Alex Turvy, a sociologist specializing in digital culture, said it’s hard to map our current understanding of virality and platform optimization to the earliest days of Google, but there are definitely similarities.</p><p>“I think that the celebrity gossip world is a good example,” he said. “Folks that understood backlinks and keywords earlier than others and were able to get low-quality content pretty high on search results pages.”</p><p>He cited examples such as Perez Hilton and the blogs <a href="https://www.crazydaysandnights.net/">Crazy Days and Nights</a> and <a href="https://ohnotheydidnt.livejournal.com/">Oh No They Didn’t!</a> Over the next few years, the web began to fill with aggregators like eBaum’s World, Digg, and CollegeHumor.&nbsp;</p><p>But even the creators of original high-quality content were not immune to the pressures of Google Search.</p><p>Deb Perelman is considered one of the <a href="https://www.culturedmag.com/article/2023/05/17/food-blogger-deb-perelman-smitten-kitchen">earliest food bloggers</a> and is certainly one of the few who’s still at it. She started blogging about food in 2003. Her site, Smitten Kitchen, was launched in 2006 and has since spawned three books. In the beginning, she says, she didn’t really think much about search. But eventually, she, like the other eminent bloggers of the period, took notice.</p><p>“It was definitely something you were aware of — your page ranking — just because it affected whether people could find your stuff through Google,” she said.&nbsp;</p><div><p>It’s hard to find another sector more thoroughly molded by the pressures of SEO than recipe sites</p></div><p>It’s hard to find another sector more thoroughly molded by the pressures of SEO than recipe sites, which, these days, take a near-uniform shape as an extremely long anecdote (often interspersed with ads), culminating in a recipe card that is remarkably terse in comparison. The formatting and style of food bloggers has generated <a href="https://mashable.com/article/why-are-there-long-stories-on-food-blogs">endless discourse</a> for years.&nbsp;</p><p>The reason why food blogs look like that, according to Perelman, is pretty straightforward: the bloggers want to be read on Google.</p><p>That said, she’s adamant that most of the backlash against food bloggers attaching long personal essays to the top of their posts is obnoxious and sexist. People can just not read it if they don’t want to. But she also acknowledged writers are caving to formatting pressures. (There are <a href="https://yoast.com/clear-and-seo-friendly-paragraphs">countless</a> <a href="https://medium.com/illumination/paragraph-length-and-structure-for-seo-d8d503f2a1a6">guides</a> <a href="https://www.searchenginejournal.com/word-count-seo-importance/441742/">instructing</a> that writers use a specific amount of sentences per paragraph and a specific amount of paragraphs per post to rank better on Google.)</p><p>“Rather than writing because there was maybe a story to tell, there was this idea that it was good for SEO,” she said. “And I think that that’s a less quality experience. And yeah, you could directly say I guess that Google has sort of created that in a way.”</p><p>Sullivan says PageRank’s algorithm is a lot simpler than most people assume it is. At the beginning, most of the tips and tricks people were sharing were largely pointless for SEO. The subject of SEO is still rife with superstition. There are a lot of different ideas that people have about exactly how to get a prominent spot on Google’s results, Sullivan acknowledges. But most of the stuff you’ll find by, well, googling “SEO tricks” isn’t very accurate.&nbsp;</p><p>And here is where you get into the circular nature of his argument against Google’s influence. Thousands of food bloggers are searching for advice on how to optimize their blogs for Google. The advice that sits at the top of Google is bad, but they’re using it anyway, and now, their blogs all look the same. Isn’t that, in a sense, Google shaping how content is made?</p><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/376x292/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/384x299/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/415x323/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/480x373/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/540x420/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/640x498/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/750x583/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/828x644/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/1080x840/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/1200x933/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/1440x1120/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/1920x1493/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/2048x1593/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/2400x1866/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:3000x2333/2400x1866/filters:focal(1500x1167:1501x1168):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24874235/236755_When_Google_ruled_the_world_MRohn_003.jpeg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p>“All Your Base Are Belong To Us” existed pre-Google but suddenly rose in prominence as the search engine flickered on. Other forms of content began following the same virality curve, rocketing to the top of Google and then into greater pop culture.</p><p>Perelman said that one of the first viral recipes she remembers from that era was a 2006 <em>New York Times</em> tutorial on <a href="https://go.redirectingat.com/?xs=1&amp;id=1025X1701640&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D13Ah9ES2yTU">how to make no-knead bread</a> by Sullivan Street Bakery’s Jim Lahey. “That was a really big moment,” she said.</p><p>True to form, Sullivan doubts that it was search, itself, that made it go viral. “It almost certainly wasn’t hot because search made it hot. Something else made it hot and then everybody went to search for it,” he said.</p><p>(Which may be true. But the video tutorial was also published on YouTube one month after the site was purchased by Google.)</p><p>The viral no-knead bread recipe is a perfect example of how hard it can be to separate the discoverability Google brought to the internet from the influence of that discoverability. And it was even harder 20 years ago, long before we had concepts like “viral” or “influencer.”&nbsp;</p><div><p>Alice Marwick, a communications professor and author of <a href="https://www.amazon.com/Private-Political-Networked-Privacy-Social-ebook/dp/B0C3XNWTL9/?tag=theverge02-20"><em>The Private Is Political: Networked Privacy and Social Media</em></a>, told <em>The Verge</em> that it wasn’t until Myspace launched in 2003 that we started to even develop the idea of internet fame.</p></div><p>“There wasn’t like a pipeline for virality in the way that it is,” she said. “Now, there is a template of, like, weird people doing weird stuff on the internet.”</p><div><p>“Google has gotten shittier and shittier.”</p></div><p>Marwick said that within the internet landscape of the 2000s, Google was the thing that sat on top of everything else. There was a sense that as anarchic and chaotic as the early social web was out in the digital wilderness, what Google surfaced denoted a certain level of quality.&nbsp;</p><p>But if that last 25 years of Google’s history could be boiled down to a battle against the Google bomb, it is now starting to feel that the search engine is finally losing pace with the hijackers. Or as Marwick put it, “Google has gotten shittier and shittier.”</p><p>“To me, it just continues the transformation of the internet into this shitty mall,” Marwick said. “A dead mall that’s just filled with the shady sort of stores you don’t want to go to.”</p><p>The question, of course, is when did it all go wrong? How did a site that captured the imagination of the internet and fundamentally changed the way we communicate turn into a burned-out Walmart at the edge of town?&nbsp;</p><p>Well, if you ask Anil Dash, it was all the way back in 2003 — when the company turned on its AdSense program.</p><p>“Prior to 2003–2004, you could have an open comment box on the internet. And nobody would pretty much type in it unless they wanted to leave a comment. No authentication. Nothing. And the reason why was because who the fuck cares what you comment on there. And then instantly, overnight, what happened?” Dash said. “Every single comment thread on the internet was instantly spammed. And it happened overnight.”</p><p>Dash is one of the web’s earliest bloggers. In 2004, he won a <a href="https://www.wired.com/techbiz/it/news/2004/07/64130">competition</a> Google held to google-bomb itself with the made-up term “nigritude ultramarine.” Since then, Dash has <a href="https://www.anildash.com/2017/11/29/underscores-optimization-arms-races/">written extensively</a> over the years on the impact platform optimization has had on the way the internet works. As he sees it, Google’s advertising tools gave links a monetary value, killing anything organic on the platform. From that moment forward, Google cared more about the health of its own network than the health of the wider internet.&nbsp;</p><p>“At that point it was really clear where the next 20 years were going to go,” he said.</p><div><p>“At that point it was really clear where the next 20 years were going to go.”</p></div><p>Google Answers closed in 2006. Google Reader shut down in 2013, taking with it the last vestiges of the blogosphere. Search inside of Google Groups has <a href="https://www.vice.com/en/article/jp5a77/google-a-search-company-has-made-its-internet-archive-impossible-to-search%5C">repeatedly broken</a> over the years. Blogger still works, but without Google Reader as a hub for aggregating it, most publishers started making native content on platforms like Facebook and Instagram and, more recently, TikTok.&nbsp;</p><p>Discoverability of the open web has suffered. Pinterest <a href="https://www.inverse.com/input/culture/pinterest-sucks-google-image-photo-search-ruining-internet">has been accused</a> of eating Google Image Search results. And the recent protests over third-party API access at Reddit <a href="https://arstechnica.com/gadgets/2023/06/google-admits-reddit-protests-make-it-harder-to-find-helpful-search-results/">revealed</a> how popular Google has become as a search engine not for Google’s results but for Reddit content. Google’s place in the hierarchy of Big Tech is slipping enough that some are <a href="https://www.androidpolice.com/apple-maps-good-now-problem-google-maps/">even admitting</a> that Apple Maps is worth giving another chance, something unthinkable even a few years ago.</p><p>On top of it all, OpenAI’s massively successful ChatGPT has dragged Google into a race against Microsoft to build a completely different kind of search, one that uses a chatbot interface supported by generative AI.&nbsp;</p><p>Twenty-five years ago, at the dawn of a different internet age, another search engine began to struggle with similar issues. It was considered the top of the heap, praised for its sophisticated technology, and then suddenly faced an existential threat. A young company created a new way of finding content.</p><p>Instead of trying to make its core product better, fixing the issues its users had, the company, instead, became more of a portal, weighted down by bloated services that worked less and less well. The company’s CEO admitted in 2002 that it “tried to become a portal too late in the game, and lost focus” and <a href="https://www.wired.com/2002/11/altavista-makeover-a-better-view/">told <em>Wired</em> at the time</a> that it was going to try and double back and focus on search again. But it never regained the lead.</p><p>That company was AltaVista.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking the LG Monitor's EDID (269 pts)]]></title>
            <link>https://gist.github.com/kj800x/be3001c07c49fdb36970633b0bc6defb</link>
            <guid>37323604</guid>
            <pubDate>Wed, 30 Aug 2023 15:24:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/kj800x/be3001c07c49fdb36970633b0bc6defb">https://gist.github.com/kj800x/be3001c07c49fdb36970633b0bc6defb</a>, See on <a href="https://news.ycombinator.com/item?id=37323604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-hacking-the-lg-monitor-s-edid-md">
    <article itemprop="text"><p dir="auto"><strong>preface</strong>: Posting these online since it sounds like these notes are somewhat interesting based on a few folks I've shared with. These are semi-rough notes that I basically wrote for myself in case I ever needed to revisit this fix, so keep that in mind.</p>
<p dir="auto">I recently bought an LG ULTRAGEAR monitor secondhand off of a coworker. I really love it and it's been great so far, but I ran into some minor issues with it in Linux. It works great on both Mac and Windows, but on Linux it displays just a black panel until I use the second monitor to go in and reduce the refresh rate down to 60 Hz.</p>
<p dir="auto">This has worked decent so far but there's some issues:</p>
<ul dir="auto">
<li>It doesn't work while linux is booting up. The motherboards boot sequence is visible just fine, but as soon as control is handed over to Linux and I'd normally see a splash screen while I'm waiting for my login window, I see nothing.</li>
<li>It doesn't work on the login screen. This would be fine if login consistently worked on my second screen, but I need to manually switch the cables between my work computer and the desktop for the second screen and sometimes I don't feel like doing that. Even when I switch the cables, the second screen seems to be moody and doesn't always show the login screen either.</li>
<li>Once I've logged in and fixed the settings on my second screen it seems to go fine, unless I actually unplug the second screen. If I do, it looks like the graphics settings go reset back to default (settings that don't work) and I lose the main monitor too.</li>
</ul>
<h2 dir="auto">Debugging</h2>
<p dir="auto">Since I was able to fix the issue by manually reducing the refresh rate, my guess is that the issue is really about Linux insisting on defaulting the monitor to a mode that it doesn't support: <code>3440 x 1440 143.923 Hz</code>. I started looking online to do some research and I found a lot of articles and forum posts about how to sync up your logged-in display preferences with your boot sequence preferences &amp; your greeter preferences, but that was mostly around fixing resolution, position, and orientation of monitors and it only worked for specific monitor connections.</p>
<p dir="auto">In my case, it wasn't so much about the positioning of the monitors, the default mode that my monitor seemed to want to be in just didn't seem to work at all. Regardless, I tried the suggestions around creating an xrandr script and having it run when the greeter starts, but it seems that my monitor didn't care and the xrandr script wasn't doing anything at all. I also tried updating my version of Linux Mint and my Linux kernel to the latest versions, neither of those solved the issue.</p>
<p dir="auto">Since this monitor did work out of the box when hooked up to the Macbook or when I boot this computer into Windows, I knew it was possible for this monitor to work properly.</p>
<h2 dir="auto">EDID</h2>
<p dir="auto">I landed on the arch wiki for xrandr, xorg, and kernel mode setting and started to get the idea that my monitor was possibly reporting the wrong mode to the OS. This eventually lead me to <a href="https://lists.fedoraproject.org/archives/list/users@lists.fedoraproject.org/thread/D2CGPRU4OXMPQ4KKHW5ZYZ2CMELALRCG/" rel="nofollow">this mailing list</a> post which had me start to look closer at the EDID. Running <code>edid-decode /sys/devices/pci0000:00/0000:00:03.1/0000:08:00.0/drm/card0/card0-DP-2/edid</code> I got this output:</p>
<pre><code>edid-decode (hex):

00 ff ff ff ff ff ff 00 1e 6d 4b 77 b5 42 02 00
09 1e 01 04 b5 50 21 78 9f f6 75 af 4e 42 ab 26
0e 50 54 21 09 00 71 40 81 80 81 c0 a9 c0 b3 00
d1 c0 81 00 d1 cf da a7 70 50 d5 a0 34 50 90 20
3a 30 20 4f 31 00 00 1a 00 00 00 fd 00 30 90 e1
e1 50 01 0a 20 20 20 20 20 20 00 00 00 fc 00 4c
47 20 55 4c 54 52 41 47 45 41 52 0a 00 00 00 ff
00 30 30 39 4e 54 5a 4e 34 43 31 34 39 0a 02 34

02 03 30 71 23 09 07 07 47 10 04 03 01 1f 13 12
83 01 00 00 e3 05 c0 00 e2 00 6a e6 06 05 01 61
61 3d 6d 1a 00 00 02 05 30 90 00 04 61 3d 61 3d
4e d4 70 d0 d0 a0 32 50 30 20 3a 00 20 4f 31 00
00 1a 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ee

70 12 79 00 00 03 01 14 66 38 01 86 6f 0d ef 00
2f 80 1f 00 9f 05 45 00 02 00 09 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
00 00 00 00 00 00 00 00 00 00 00 00 00 00 0b 90

----------------

Block 0, Base EDID:
  EDID Structure Version &amp; Revision: 1.4
  Vendor &amp; Product Identification:
    Manufacturer: GSM
    Model: 30539
    Serial Number: 148149
    Made in: week 9 of 2020
  Basic Display Parameters &amp; Features:
    Digital display
    Bits per primary color channel: 10
    DisplayPort interface
    Maximum image size: 80 cm x 33 cm
    Gamma: 2.20
    DPMS levels: Standby
    Supported color formats: RGB 4:4:4, YCrCb 4:4:4, YCrCb 4:2:2
    Default (sRGB) color space is primary color space
    First detailed timing includes the native pixel format and preferred refresh rate
    Display is continuous frequency
  Color Characteristics:
    Red  : 0.6865, 0.3076
    Green: 0.2587, 0.6699
    Blue : 0.1494, 0.0576
    White: 0.3134, 0.3291
  Established Timings I &amp; II:
    DMT 0x04:   640x480    59.940 Hz   4:3    31.469 kHz  25.175 MHz
    DMT 0x09:   800x600    60.317 Hz   4:3    37.879 kHz  40.000 MHz
    DMT 0x10:  1024x768    60.004 Hz   4:3    48.363 kHz  65.000 MHz
    DMT 0x24:  1280x1024   75.025 Hz   5:4    79.976 kHz 135.000 MHz
  Standard Timings:
    CVT     :  1152x864    59.959 Hz   4:3    53.783 kHz  81.750 MHz (EDID 1.4 source)
    GTF     :  1152x864    60.000 Hz   4:3    53.700 kHz  81.624 MHz (EDID 1.3 source)
    DMT 0x23:  1280x1024   60.020 Hz   5:4    63.981 kHz 108.000 MHz
    DMT 0x55:  1280x720    60.000 Hz  16:9    45.000 kHz  74.250 MHz
    DMT 0x53:  1600x900    60.000 Hz  16:9    60.000 kHz 108.000 MHz (RB)
    DMT 0x3a:  1680x1050   59.954 Hz  16:10   65.290 kHz 146.250 MHz
    DMT 0x52:  1920x1080   60.000 Hz  16:9    67.500 kHz 148.500 MHz
    DMT 0x1c:  1280x800    59.810 Hz  16:10   49.702 kHz  83.500 MHz
    CVT     :  1920x1080   74.906 Hz  16:9    84.643 kHz 220.750 MHz (EDID 1.4 source)
    GTF     :  1920x1080   75.000 Hz  16:9    84.600 kHz 220.637 MHz (EDID 1.3 source)
  Detailed Timing Descriptors:
    DTD 1:  3440x1440   60.001 Hz  43:18   89.521 kHz 429.700 MHz (800 mm x 335 mm)
                 Hfront  144 Hsync 800 Hback 416 Hpol P
                 Vfront    3 Vsync  10 Vback  39 Vpol N
  Display Range Limits:
    Monitor ranges (Bare Limits): 48-144 Hz V, 225-225 kHz H, max dotclock 800 MHz
    Display Product Name: 'LG ULTRAGEAR'
    Display Product Serial Number: '009NTZN4C149'
  Extension blocks: 2
Checksum: 0x34

----------------

Block 1, CTA-861 Extension Block:
  Revision: 3
  Basic audio support
  Supports YCbCr 4:4:4
  Supports YCbCr 4:2:2
  Native detailed modes: 1
  Audio Data Block:
    Linear PCM:
      Max channels: 2
      Supported sample rates (kHz): 48 44.1 32
      Supported sample sizes (bits): 24 20 16
  Video Data Block:
    VIC  16:  1920x1080   60.000 Hz  16:9    67.500 kHz 148.500 MHz
    VIC   4:  1280x720    60.000 Hz  16:9    45.000 kHz  74.250 MHz
    VIC   3:   720x480    59.940 Hz  16:9    31.469 kHz  27.000 MHz
    VIC   1:   640x480    59.940 Hz   4:3    31.469 kHz  25.175 MHz
    VIC  31:  1920x1080   50.000 Hz  16:9    56.250 kHz 148.500 MHz
    VIC  19:  1280x720    50.000 Hz  16:9    37.500 kHz  74.250 MHz
    VIC  18:   720x576    50.000 Hz  16:9    31.250 kHz  27.000 MHz
  Speaker Allocation Data Block:
    FL/FR - Front Left/Right
  Colorimetry Data Block:
    BT2020YCC
    BT2020RGB
  Video Capability Data Block:
    YCbCr quantization: No Data
    RGB quantization: Selectable (via AVI Q)
    PT scan behavior: Always Underscanned
    IT scan behavior: Always Underscanned
    CE scan behavior: Always Underscanned
  HDR Static Metadata Data Block:
    Electro optical transfer functions:
      Traditional gamma - SDR luminance range
      SMPTE ST2084
    Supported static metadata descriptors:
      Static metadata type 1
    Desired content max luminance: 97 (408.759 cd/m^2)
    Desired content max frame-average luminance: 97 (408.759 cd/m^2)
    Desired content min luminance: 61 (0.234 cd/m^2)
  Vendor-Specific Data Block (AMD), OUI 00-00-1A:
    02 05 30 90 00 04 61 3d 61 3d                   '..0...a=a='
  Detailed Timing Descriptors:
    DTD 2:  3440x1440   99.990 Hz  43:18  148.986 kHz 543.500 MHz (800 mm x 335 mm)
                 Hfront   48 Hsync  32 Hback 128 Hpol P
                 Vfront    3 Vsync  10 Vback  37 Vpol N
Checksum: 0xee

----------------

Block 2, DisplayID Extension Block:
  Version: 1.2
  Extension Count: 0
  Display Product Type: Extension Section
  Video Timing Modes Type 1 - Detailed Timings Data Block:
    DTD:  3440x1440  143.923 Hz  64:27  217.323 kHz 799.750 MHz (aspect 64:27, no 3D stereo, preferred)
               Hfront   48 Hsync  32 Hback 160 Hpol P
               Vfront    3 Vsync  10 Vback  57 Vpol N
  Checksum: 0x0b
Checksum: 0x90
</code></pre>
<p dir="auto">I am not an EDID expert by any means (although I'm certainly more than a novice after this whole saga), but most of that looks fine to me. Actually the only place where I saw the mode that Linux Mint was defaulting to was in that last block:</p>
<pre><code>Block 2, DisplayID Extension Block:
    ...
    DTD:  3440x1440  143.923 Hz  64:27  217.323 kHz 799.750 MHz (aspect 64:27, no 3D stereo, preferred)
</code></pre>
<p dir="auto">My hypothesis now was that if there was some way for me to change the EDID so that that last block had <code>60 Hz</code> instead of <code>143.923 Hz</code>, that could fix everything.</p>
<p dir="auto">I started by trying to read the specs for EDID and DisplayID. Ultimately these were helpful to have on the side, but actually reading and understanding the details of every byte was beginning to confuse me and was starting to take me too long. I started thinking about a faster way to grok this bytestring when I realized there was a program I just used that was really great at parsing edids: <code>edid-decode</code>.</p>
<h2 dir="auto">Edid-Decode</h2>
<p dir="auto">I found the git repo and cloned it locally. Thankfully, the build instructions were super easy, just one <code>make</code> later and I had a development build of <code>parse-edid</code> on disk.</p>
<div dir="auto"><pre>git clone git://linuxtv.org/edid-decode.git
<span>cd</span> edid-decode
make
./edid-decode /sys/devices/pci0000:00/0000:00:03.1/0000:08:00.0/drm/card0/card0-DP-2/edid</pre></div>
<p dir="auto">Looking at the output, I wanted to hone in on that <code>DisplayID Extension Block</code> section. I popped the source open in VSCode and started searching around. <code>DisplayID Extension Block</code> shows up as a constant in the <code>block_name</code> function, so looking at usages of that brought me to <code>edid_state::parse_extension</code>. This clearly was a DisplayID block, so I clicked into <code>parse_displayid_block</code>.</p>
<p dir="auto">At this point I wanted to orient myself a bit. The function was using things like <code>x[1]</code> and <code>x[2]</code> but I knew we weren't thinking about the second and third bytes of the whole EDID. I added a print statement to print out the first 5 bytes of x, just so that I could compare with my hex editor and see where we were.</p>
<div dir="auto"><pre><span>printf</span>(<span><span>"</span>-- %x %x %x %x %x<span>"</span></span>, x[<span>0</span>], x[<span>1</span>], x[<span>2</span>], x[<span>3</span>], x[<span>4</span>]);</pre></div>
<p dir="auto">This gave us <code>70 12 79 00 00</code> and I could find where that sequence was in my hex editor. In fact, I could also see that the variable assignments lined up with the structure table in the DisplayID Wikipedia page (<code>version</code>, <code>length</code>, <code>prod_type</code>, <code>ext_count</code>).</p>
<p dir="auto">Continuing to trace the code, we moved the pointer forward by 5 and then jumped into <code>edid_state::displayid_block</code>. We know that the next byte <code>tag</code> is <code>0x03</code> which matches the fact that <code>Video Timing Modes Type 1 - Detailed Timings Data Block</code> is the next section that's printed out. Later down the function, there's a function call to <code>edid_state::parse_displayid_type_1_7_timing</code> that we enter and this is where the code starts to get exciting.</p>
<p dir="auto">Actually, the start of the function isn't that bad, but scroll down halfway and you'll find this:</p>
<div dir="auto"><pre>t.hact = <span>1</span> + (x[<span>4</span>] | (x[<span>5</span>] &lt;&lt; <span>8</span>));
hbl = <span>1</span> + (x[<span>6</span>] | (x[<span>7</span>] &lt;&lt; <span>8</span>));
t.hfp = <span>1</span> + (x[<span>8</span>] | ((x[<span>9</span>] &amp; <span>0x7f</span>) &lt;&lt; <span>8</span>));
t.hsync = <span>1</span> + (x[<span>10</span>] | (x[<span>11</span>] &lt;&lt; <span>8</span>));
t.hbp = hbl - t.hfp - t.hsync;
<span>if</span> ((x[<span>9</span>] &gt;&gt; <span>7</span>) &amp; <span>0x1</span>)
  t.pos_pol_hsync = <span>true</span>;
t.vact = <span>1</span> + (x[<span>12</span>] | (x[<span>13</span>] &lt;&lt; <span>8</span>));
vbl = <span>1</span> + (x[<span>14</span>] | (x[<span>15</span>] &lt;&lt; <span>8</span>));
t.vfp = <span>1</span> + (x[<span>16</span>] | ((x[<span>17</span>] &amp; <span>0x7f</span>) &lt;&lt; <span>8</span>));
t.vsync = <span>1</span> + (x[<span>18</span>] | (x[<span>19</span>] &lt;&lt; <span>8</span>));
t.vbp = vbl - t.vfp - t.vsync;
<span>if</span> ((x[<span>17</span>] &gt;&gt; <span>7</span>) &amp; <span>0x1</span>)
  t.pos_pol_vsync = <span>true</span>;
<span>if</span> (x[<span>3</span>] &amp; <span>0x10</span>) {
  t.<span>interlaced</span> = <span>true</span>;
  t.<span>vfp</span> /= <span>2</span>;
  t.<span>vsync</span> /= <span>2</span>;
  t.<span>vbp</span> /= <span>2</span>;
}</pre></div>
<p dir="auto">Oh no, there's <strong>no way</strong> I'm going to be able to figure out all this byte manipulation in my head. I probably don't need to care about most of this just to figure out how to fix the refresh rate, and I wish there was a way I could just see what all these values get set to in the end.</p>
<h2 dir="auto">Actually debugging now</h2>
<p dir="auto">I don't do much C or C++ development, but I've used <code>gdb</code> in the past for schoolwork and a really great capture the flag I did once. I probably can remember enough about it to get some useful info out.</p>
<p dir="auto">First I worked to try and get debug symbols enabled. Turns out this Makefile already has debug symbols turned on, so that wasn't actually necessary. If you ever do need to turn on debug symbols, the flag to remember is <code>-g</code> for the compiler although in this Makefile I just tossed it onto the <code>WARN_FLAGS</code> variable since I didn't see an obvious other place for it.</p>
<p dir="auto">I fired up <code>gdb ./edid-decode</code> did some quick googling to figure out that setting a breakpoint is <code>break filename.cpp:line-number</code> and running with arguments is <code>run arg1 arg2</code> and I landed in the right place.</p>
<pre><code>Breakpoint 1, edid_state::parse_displayid_type_1_7_timing (this=0x5555555b3940 &lt;state&gt;, x=0x5555555b4408 &lt;edid+264&gt; "f8\001\206o", &lt;incomplete sequence \357&gt;, type7=false, block_rev=1, is_cta=false) at parse-displayid-block.cpp:368
368		print_timings("    ", &amp;t, name.c_str(), s.c_str(), true);
(gdb) 
</code></pre>
<p dir="auto">Ok, we just got through that wild bit where we set up <code>t</code>. I'm not entirely sure what I'm expecting since a C struct is just a bunch of bytes, but lets try to print it out:</p>
<pre><code>(gdb) p t
$1 = {hact = 3440, vact = 1440, hratio = 64, vratio = 27, pixclk_khz = 799750, rb = 0, interlaced = false, hfp = 48, hsync = 32, 
  hbp = 160, pos_pol_hsync = true, vfp = 3, vsync = 10, vbp = 57, pos_pol_vsync = false, hborder = 0, vborder = 0, even_vtotal = false, 
  no_pol_vsync = false, hsize_mm = 0, vsize_mm = 0, ycbcr420 = false}
</code></pre>
<p dir="auto">Oh wow, that's a lot more readable than I expected. I guess debug symbols go a long way! Some of these values I recognize (3440 and 1440 are the resolution), but I'm not seeing the one most important value I need to fix: the refresh rate of <code>143.923</code>. Let's step forward and see if we can figure out how that's calculated:</p>
<pre><code>(gdb) n
    DTD:  3440x1440  143.922761 Hz  64:27   217.323 kHz    799.750000 MHz (aspect 64:27, no 3D stereo, preferred)
               Hfront   48 Hsync  32 Hback  160 Hpol P
               Vfront    3 Vsync  10 Vback   57 Vpol N
369		if (is_cta) {
(gdb) 
</code></pre>
<p dir="auto">Oops, we went way too far, all the calculations for the refresh rate just got done without us. I had to go look up the difference between <code>s</code> and <code>n</code> and realized that I stepped over all the interesting stuff (<code>n</code> = next line, <code>s</code> = step into). Well at least we know the interesting stuff is inside the <code>print_timings</code> function.</p>
<p dir="auto">Back in my editor, we go looking and we find another relatively dense function. At this point, I'm looking for other strategies to take shortcuts and I realize that there's a <code>Hz</code> right after the refresh rate. I do a quick search and I find this <code>printf</code>:</p>
<div dir="auto"><pre><span>printf</span>(<span><span>"</span>%s%s: %5ux%-5s %10.6f Hz %3u:%-3u %8.3f kHz %13.6f MHz%s<span>\n</span><span>"</span></span>,
	   prefix, type,
	   t-&gt;hact, buf,
	   refresh,
	   t-&gt;hratio, t-&gt;vratio,
	   out_hor_freq_khz,
	   pixclk / <span>1000000.0</span>,
	   s.c_str());</pre></div>
<p dir="auto">That actually makes things a lot clearer. We're really looking at where the <code>refresh</code> variable is set. The definition for that is a bit like this:</p>
<div dir="auto"><pre><span>double</span> refresh = t-&gt;pixclk_khz * <span>1000.0</span> / (htotal * vtotal);
<span>if</span> (options[OptNTSC] &amp;&amp; fmod(refresh, <span>6.0</span>) == <span>0</span>) {
  refresh *= ntsc_fact;
}</pre></div>
<p dir="auto">Ah, so that <code>pixclk_khz</code> from the struct earlier is actually pretty key. Let me try to see if I do that math, do I get the same number?</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{799750 \cdot 1000}{3440 \cdot 1440} = 161.44\dots
$$</math-renderer></p>
<p dir="auto">What if we add in <code>ntsc_fact</code> which is <code>1000.0 / 1001.0</code></p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{799750 \cdot 1000}{3440 \cdot 1440} \cdot \frac{1000}{1001}= 161.28\dots
$$</math-renderer></p>
<p dir="auto">Huh, 161 is close to 143, but neither of these are quite right. I must be missing something.</p>
<p dir="auto">After a lot of searching, it turned out that <code>htotal</code> and <code>vtotal</code> aren't just the resolution as I had first assumed. Here's the relevant bits, with comments added based on what stuff ended up evaluating out to:</p>
<div dir="auto"><pre><span>if</span> (t-&gt;interlaced) <span><span>//</span> false</span>
  vact /= <span>2</span>;

<span>unsigned</span> hbl = t-&gt;hfp + t-&gt;hsync + t-&gt;hbp + <span>2</span> * t-&gt;hborder; <span><span>//</span> 240</span>
<span>unsigned</span> vbl = t-&gt;vfp + t-&gt;vsync + t-&gt;vbp + <span>2</span> * t-&gt;vborder; <span><span>//</span> 70</span>
<span>unsigned</span> htotal = t-&gt;hact + hbl; <span><span>//</span> 3680</span>
<span>double</span> vtotal = vact + vbl; <span><span>//</span> 1510</span>

<span>if</span> (t-&gt;even_vtotal) <span><span>//</span> false</span>
  vtotal = vact + t-&gt;vfp + t-&gt;vsync + t-&gt;vbp;
<span>else</span> <span>if</span> (t-&gt;interlaced) <span><span>//</span> false</span>
  vtotal = vact + t-&gt;vfp + t-&gt;vsync + t-&gt;vbp + <span>0.5</span>;</pre></div>
<p dir="auto">Ok, trying our math again:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{799750 \cdot 1000}{3680 \cdot 1510} = 143.922\dots
$$</math-renderer></p>
<p dir="auto"><strong>There we go!</strong> Finally, we understand roughly how that refresh rate that Linux Mint was erroneously using was being derived. Now let's see what we can do to fix things.</p>
<h2 dir="auto">Working backwards</h2>
<p dir="auto">At this point, we're trying to work backwards from the end result we want to figure out what bytes need to be updated in the EDID. First, we want that fraction to evaluate out to 60 instead, and we don't want to muck with either of the resolution values. That basically means we should change the <code>pixclk_khz</code> value:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{x \cdot 1000}{3680 \cdot 1510} = 60
$$</math-renderer></p>
<p dir="auto">This is really easy math, but I already have the whole thing plugged into WolframAlpha so it's easy to just change that to an <code>x</code> and set the whole thing equal to 60:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
x = 333408
$$</math-renderer></p>
<p dir="auto">Computers are great, aren't they?</p>
<p dir="auto">Ok, so we need <code>t-&gt;pixclk_khz</code> to be <code>333408</code>. Where does that get set?</p>
<div dir="auto"><pre><span><span>//</span> edid_state::parse_displayid_type_1_7_timing</span>
<span><span>//</span> type7 = false</span>
t.pixclk_khz = (type7 ? <span>1</span> : <span>10</span>) * (<span>1</span> + (x[<span>0</span>] + (x[<span>1</span>] &lt;&lt; <span>8</span>) + (x[<span>2</span>] &lt;&lt; <span>16</span>))); </pre></div>
<p dir="auto">It took me a bit longer than I'd like to admit to realize that this is really just 10 times 1 plus the first 3 bytes of the DisplayID section payload in little endian (look, I don't work with bit manipulation that often). Those bytes in decimal are <code>102</code>, <code>56</code>, and <code>1</code> so let's double check our understanding</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
10 \cdot (1 + 102 + 56\cdot 2^8 + 1 \cdot 2^{16}) = 799750
$$</math-renderer></p>
<p dir="auto">Great, that was the value for <code>pixclk_khz</code> that our debugger showed us earlier. So if we want that to instead be <code>333408</code>, what are the values we need for those three bytes?</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
10 \cdot (1 + x_0 + x_1 \cdot 2^8 + x_2 \cdot 2^{16}) = 333408
$$</math-renderer></p>
<p dir="auto">Rearranging a bit:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
x_0 + x_1 \cdot 2^8 + x_2 \cdot 2^{16} = 33339.8
$$</math-renderer></p>
<p dir="auto">It looks like we're gonna have to add a bit of imprecision since we're working with whole numbers here. I'm not entirely sure how this is typically handled, but lets just round up to <code>33340</code> and hope that nothing blows up.</p>
<p dir="auto">There's lots of ways to solve this, but at the core it's a base change to base 8. I like to do this with division and remainders. To get the most significant digit <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$x_2$</math-renderer> we divide our number by the value of that place and look at the whole number.</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{33340}{2^{16}} = 0.5
$$</math-renderer></p>
<p dir="auto">It turns out <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$33340 &amp;lt; 2^{16}$</math-renderer> so <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$x_2$</math-renderer> is 0. Since it's zero, we use the same number with the next place value:</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
\frac{33340}{2^{8}} = 130.234375
$$</math-renderer></p>
<p dir="auto">So we know that <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$x_1$</math-renderer> needs to be 130. We can look at the remainder there by multiplying just the decimals by the place value: <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$0.234375 * 2^8 = 60$</math-renderer> and we've already made it to our 1s place so <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$x_0$</math-renderer> just needs to be 60.</p>
<p dir="auto"><math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="88b5f6df76e42a40ece9439c58c5af65">$$
60 + 130 \cdot 2^8 + 0 \cdot 2^{16} = 33340
$$</math-renderer></p>
<h2 dir="auto">Making the change</h2>
<p dir="auto">We've got our new 3 bytes, so let's use a hex editor to fix the file. <code>60 130 0</code> is <code>0x3c 0x82 0x00</code> in hex, so we go in and update the first line of our 3rd EDID block:</p>
<div dir="auto"><pre><span><span>-</span>70 12 79 00 00 03 01 14 **66 38 01** 86 6f 0d ef 00</span>
<span><span>+</span>70 12 79 00 00 03 01 14 **3c 82 00** 86 6f 0d ef 00</span></pre></div>
<p dir="auto">Let's save and run <code>edid-decode</code> on the new file. Hopefully it agrees with the change that we made.</p>
<pre><code>Block 2, DisplayID Extension Block:
  Version: 1.2
  Extension Count: 0
  Display Product Type: Extension Section
  Video Timing Modes Type 1 - Detailed Timings Data Block:
    DTD:  3440x1440   60.000 Hz  64:27   90.601 kHz 333.410 MHz (aspect 64:27, no 3D stereo, preferred)
               Hfront   48 Hsync  32 Hback 160 Hpol P
               Vfront    3 Vsync  10 Vback  57 Vpol N
  Checksum: 0x0b (should be 0xec)
Checksum: 0x90 (should be 0x71)
</code></pre>
<p dir="auto">I do not have the language to explain how excited I was when I saw that <code>60.000 Hz</code> in the output! You will have to trust me when I tell you I shouted in excitement!</p>
<p dir="auto">One slight issue which probably isn't a big deal, but I'm going to fix it anyways: those checksums are off since we modified the file. I think fixing this with math wouldn't be that difficult (based on the spec, the checksums should be chosen such that the sum of all the bytes in the section modulo 256 is 0), but <code>edid-decode</code> was so kind as to just tell us what they should be, so we can just go and update those lines in the file directly:</p>
<div dir="auto"><pre><span><span>-</span>00 00 00 00 00 00 00 00 00 00 00 00 00 00 0B 90</span>
<span><span>+</span>00 00 00 00 00 00 00 00 00 00 00 00 00 00 EC 71</span></pre></div>
<p dir="auto">Just a quick check to make sure:</p>
<pre><code>Block 2, DisplayID Extension Block:
  Version: 1.2
  Extension Count: 0
  Display Product Type: Extension Section
  Video Timing Modes Type 1 - Detailed Timings Data Block:
    DTD:  3440x1440   60.000 Hz  64:27   90.601 kHz 333.410 MHz (aspect 64:27, no 3D stereo, preferred)
               Hfront   48 Hsync  32 Hback 160 Hpol P
               Vfront    3 Vsync  10 Vback  57 Vpol N
  Checksum: 0xec
Checksum: 0x71 (should be 0x90)
</code></pre>
<p dir="auto">Doh! The checksum for Block 2 is part of the bytes considered for the overall checksum, so by fixing it we've caused issues for the final checksum. No matter, we can fix that real quick:</p>
<div dir="auto"><pre><span><span>-</span>00 00 00 00 00 00 00 00 00 00 00 00 00 00 EC 71</span>
<span><span>+</span>00 00 00 00 00 00 00 00 00 00 00 00 00 00 EC 90</span></pre></div>
<p dir="auto">Astute readers might notice that the final checksum changed <strong>back</strong> to 90 just now. This is an interesting artifact of the way that the checksum for EDID works, where all the bytes of a section must be congruent to 0 mod 256. When we fixed the checksum for Block 2, we made the sum of that whole section (including our only changes) cancel out to 0, just like it must have done before for the checksums to be valid.</p>
<h2 dir="auto">The moment of truth</h2>
<p dir="auto">At this point, we've got a valid updated EDID which seems to pass the parse test for <code>edid-decode</code>. The real question is if this EDID would work for the monitor or not. Fortunately, it seems that there's a way to force the Linux kernel to ignore the EDID that the monitor self reports and override it with a custom firmware (very convenient for this very situation).</p>
<p dir="auto">Very briefly, the steps seem to be:</p>
<ol dir="auto">
<li>Copy the new firmware to <code>/usr/lib/firmware</code> (call it <code>desktop_edid.bin</code> for compatibility with the scripts below)</li>
<li>Create this file: <code>/etc/initramfs-tools/hooks/desktop_edid.sh</code></li>
</ol>
<div dir="auto"><pre><span><span>#!</span>/bin/sh</span>
<span><span>#</span> Copy local EDID monitor description data</span>

PREREQ=<span><span>"</span><span>"</span></span>
<span>prereqs</span>()
{
    <span>echo</span> <span><span>"</span><span>$PREREQ</span><span>"</span></span>
}

<span>case</span> <span>$1</span> <span>in</span>
prereqs)
    prereqs
    <span>exit</span> 0
    ;;
<span>esac</span>

<span>.</span> /usr/share/initramfs-tools/hook-functions

EDID_DATA=<span><span>"</span>/usr/lib/firmware/desktop_edid.bin<span>"</span></span>

<span>if</span> [ <span>!</span> <span>-f</span> <span><span>"</span><span>$EDID_DATA</span><span>"</span></span> ]<span>;</span> <span>then</span>
    <span>exit</span> 0
<span>fi</span>

add_firmware <span><span>"</span><span><span>$(</span>basename <span>$EDID_DATA</span><span>)</span></span><span>"</span></span>

<span>exit</span> 0</pre></div>
<ol start="3" dir="auto">
<li>Make it executable: <code>sudo chmod +x /etc/initramfs-tools/hooks/desktop_edid.sh</code></li>
<li>Rebuild your initramfs: <code>sudo update-initramfs -u -v</code></li>
</ol>
<p dir="auto">If all goes well, you should see a log line about copying firmware <code>desktop_edid.bin</code>. This means that when you next reboot, you can force the kernel to use that firmware.</p>
<p dir="auto">Let's do a quick test first before making any permanent changes:</p>
<ol dir="auto">
<li>Reboot</li>
<li>When you get to grub, use <code>e</code> to edit the boot configuration for the current boot</li>
<li>Find the linux line which should have something like <code>quiet splash</code> at the end of it. Update that line to instead end in <code>quiet splash drm.edid_firmware=desktop_edid.bin</code></li>
<li>Press F10 and boot!</li>
</ol>
<p dir="auto">After all this I saw a sight I've never seen before. The loading spinner for Linux Mint was showing up on my new monitor. A few seconds later, I was greeted with an even more impressive sight: the login screen popped open on the new monitor. To be honest, I wasn't sure that I believed it would work from the start, but I'm amazed that it actually worked in the end.</p>
<h2 dir="auto">Follow ups:</h2>
<ul dir="auto">
<li>One issue is that now my second monitor isn't displaying anything. When I go into Display Settings, it seems like my computer thinks that <strong>both</strong> monitors are sending this EDID so it's put the second monitor into the wrong mode. There is a way to override the firmware for only one connector according to the Arch wiki: <code>drm.edid_firmware=VGA-1:edid/your_edid.bin</code> so I'm going to need to explore that a bit.</li>
<li>Once I get it working, I'll update the grub files for real and make it permanent.</li>
<li>One last thing I might consider doing at some point would be to try to overwrite the EDID on the monitor itself. I'd probably want to confirm that my changes are 100% safe and correct before doing this, but that would mean that my system doesn't need any special configuration for this monitor. Any Linux laptop that I plug it into or any port that I plug it into should just work. I've found docs on this online, but it's pretty risky because if I write a bad EDID I can brick the monitor, and if I do the process wrong it sounds like I could actually brick any of the components in my computer... not good!</li>
</ul>
<h2 dir="auto">Used resources</h2>
<ul dir="auto">
<li>EDID and DisplayID formats / specs:
<ul dir="auto">
<li><a href="https://glenwing.github.io/docs/VESA-EEDID-A2.pdf" rel="nofollow">https://glenwing.github.io/docs/VESA-EEDID-A2.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/DisplayID" rel="nofollow">https://en.wikipedia.org/wiki/DisplayID</a></li>
</ul>
</li>
<li>Arch linux wiki:
<ul dir="auto">
<li><a href="https://wiki.archlinux.org/title/kernel_mode_setting" rel="nofollow">https://wiki.archlinux.org/title/kernel_mode_setting</a></li>
</ul>
</li>
<li>gdb usage:
<ul dir="auto">
<li><a href="https://stackoverflow.com/questions/6121094/how-do-i-run-a-program-with-commandline-arguments-using-gdb-within-a-bash-script" rel="nofollow">https://stackoverflow.com/questions/6121094/how-do-i-run-a-program-with-commandline-arguments-using-gdb-within-a-bash-script</a></li>
<li><a href="https://unix.stackexchange.com/questions/297982/how-to-step-into-step-over-and-step-out-with-gdb" rel="nofollow">https://unix.stackexchange.com/questions/297982/how-to-step-into-step-over-and-step-out-with-gdb</a></li>
</ul>
</li>
<li>new hex editor (that lets you take notes on the file, although I didn't use this as much as I planned)
<ul dir="auto">
<li><a href="https://hackaday.com/2023/01/10/imhex-an-open-hex-editor-for-the-modern-hacker/" rel="nofollow">https://hackaday.com/2023/01/10/imhex-an-open-hex-editor-for-the-modern-hacker/</a></li>
</ul>
</li>
<li>Testing the new EDID in my kernel
<ul dir="auto">
<li><a href="https://sleeplessbeastie.eu/2022/02/02/how-to-incorporate-edid-into-initrd-image/" rel="nofollow">https://sleeplessbeastie.eu/2022/02/02/how-to-incorporate-edid-into-initrd-image/</a></li>
</ul>
</li>
</ul>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The global trading system is starting to rearrange itself (109 pts)]]></title>
            <link>https://www.noahpinion.blog/p/decoupling-isnt-phoney</link>
            <guid>37323066</guid>
            <pubDate>Wed, 30 Aug 2023 14:53:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noahpinion.blog/p/decoupling-isnt-phoney">https://www.noahpinion.blog/p/decoupling-isnt-phoney</a>, See on <a href="https://news.ycombinator.com/item?id=37323066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>If there’s one broad consensus in American policymaking circles today, it’s that global supply chains need to become less dependent on China. Whether you’re concerned about Chinese military capabilities, or import competition, or the risk of cutoffs of critical imports in an emergency, it seems like a bad idea to have every physical good on the planet made in China. So from Trump’s tariffs to Biden’s “friend-shoring” initiatives, U.S. leaders have begun pushing companies to either make things locally or import them from countries other than China. </p><p><span>What’s astonishing is that this already seems to be happening, much faster than one might have naively predicted. China’s share of U.S. imports, which already took a hit from Trump’s tariffs before rebounding somewhat in the pandemic, is </span><a href="https://www.wsj.com/articles/how-u-s-and-china-are-breaking-up-in-charts-282bd878" rel="">now falling relentlessly</a><span>:</span></p><p>For advocates of friend-shoring, de-risking, decoupling, etc., this looks like a startlingly sudden victory. </p><p><span>But a few people are claiming that this change is a statistical illusion, and that U.S. dependence on Chinese imports hasn’t fallen. In a recent pair of articles, The Economist declares that “</span><a href="https://www.economist.com/leaders/2023/08/10/joe-bidens-china-strategy-is-not-working" rel="">Joe Biden’s China strategy is not working</a><span>” and offers to tell us “</span><a href="https://www.economist.com/finance-and-economics/2023/08/08/how-america-is-failing-to-break-up-with-china" rel="">How America is failing to break up with China</a><span>”. In the first and shorter of these pieces, they write:</span></p><blockquote><p>The consequences of [the Biden administration’s] new thinking [on trade] are now becoming clear. Unfortunately, it is bringing neither resilience nor security. Supply chains have become more tangled and opaque as they have adapted to the new rules. And, if you look closely, it becomes clear that America’s reliance on Chinese critical inputs remains. More worrying, the policy has had the perverse effect of pushing America’s allies closer to China.</p></blockquote><p>The Economist’s basic argument is that because many of the goods exported to the U.S. from countries like Vietnam contain a lot of parts and components made in China, “decoupling is phoney”. They write:</p><blockquote><p>America may be redirecting its demand from China to other countries. But production in those places now relies more on Chinese inputs than ever. As South-East Asia’s exports to America have risen, for instance, its imports of intermediate inputs from China have exploded. China’s exports of car parts to Mexico, another country that has benefited from American de-risking, have doubled over the past five years…Supply chains have become more complex, and trade has become more expensive. But China’s dominance is undiminished.</p></blockquote><p><span>Make no mistake: The general issue The Economist is talking about here is very real. The way we normally count imports and exports only takes into account where the final assembly was done; it doesn’t count the value of the components that the assembler has to import in order to create the final good. So when you see a label on your phone that says “Made in Vietnam”, the screen, the processor, the memory, the camera, and other components may not have been made in Vietnam. The true amount of economic value that each country exports to each other country is the amount it contributes to the final product, which is called “value added”. </span><a href="https://www.stlouisfed.org/on-the-economy/2019/april/value-added-trade-measure" rel="">Here’s a good explainer</a><span> from the St. Louis Fed, which includes a helpful explanatory diagram:</span></p><p><span>In fact, over much of the past 20 years, China’s trade surplus with the U.S. was </span><em>much smaller</em><span> in value-added terms than in gross terms. Because China used to import most of its high-value components from Japan, Korea, Taiwan, etc., the actual amount it exported to the U.S. was much less than the headline amount. In recent years this gap has </span><a href="https://www.stlouisfed.org/on-the-economy/2019/april/value-added-trade-measure" rel="">shrunk to a modest size</a><span>, due to China’s onshoring of component manufacturing. </span></p><p>So anyway, when The Economist says that decoupling is “phoney” and “China’s dominance [of supply chains] is undiminished”, it means that although China is exporting fewer finished goods to the U.S., it’s exporting the same amount of value added. </p><p><span>Startlingly, however, The Economist provides </span><em>no actual data to support this claim</em><span>. It does provide a few scattered data points, which I will quote here:</span></p><blockquote><p>The latest official data, published in 2018, concerning exports by the Association of South-East Asian Nations (ASEAN), a regional club, show that 7% by value were actually attributable to some form of production in China. Fresher data suggest that China has only grown in importance since then. The country has increased its share of exports to [ASEAN]…In the first six months of the year Chinese sales of [electronics] in Indonesia, Malaysia, Thailand, the Philippines and Vietnam rose to $49bn, up by 80% compared with five years ago…</p><p>In the past year Chinese companies exported $300m a month in parts to Mexico, more than twice the amount they managed five years ago….In 2018 China provided just 3% of automotive parts brought into the Czech Republic, Hungary, Poland, Slovakia, Slovenia and Romania…China now provides 10% of all car parts imported into central and eastern Europe, more than any other country outside the EU.</p></blockquote><p>This is incredibly thin and scattershot evidence on which to base a claim that U.S. dependence on Chinese production remains undiminished. </p><p>Yes, there are cases of Southeast Asian nations slapping their labels on Chinese goods and then sending them to America to allow Chinese manufacturers to get around tariffs. But look at the graph of U.S. import shares above, and you’ll see that if 7% of imports from Asia ex-China are due to re-exporting, that would only shave 1.7 percentage points off of Asia ex-China’s share — only a fraction of the 4 percentage point increase in U.S. imports from that region since 2019. </p><p>The Economist claims, of course, that that 7% number has since risen. But the evidence it provides — an increase in Southeast Asian imports of Chinese electronics — says nothing about how many of those new imports are components for goods bound for America. The Economist simply assumes, without evidence, that that’s what they are. </p><p><span>Similarly, The Economist notes a recent increase in China’s auto parts exports to Mexico, which they say is “more than half” of a total of $300 million a month (or $3.6 billion a year). So let’s say that China has increased its car parts exports to Mexico by $2 billion over the last five years. Well, Mexico’s car exports to the U.S. have </span><a href="https://tradingeconomics.com/mexico/exports/united-states/vehicles-not-railway-tramway" rel="">increased by around $13 billion</a><span> since 2017, which is a heck of a lot more than $2 billion. Mexico’s auto exports to the U.S. are around $100 billion a year, utterly dwarfing the amount of car parts it imports from China. Once again, The Economist’s numbers just don’t add up. </span></p><p><span>Finally, The Economist notes a rise in East Europe’s auto parts import share from China, from 3% to 10%. But those East European-made cars </span><em>aren’t even destined for sale in the United States</em><span>! Poland, for example, sells </span><a href="https://oec.world/en/profile/country/pol" rel="">approximately 0%</a><span> of its cars to the U.S. Same for </span><a href="https://oec.world/en/profile/country/cze" rel="">Czechia</a><span>. Hungary sells </span><a href="https://oec.world/en/profile/country/hun" rel="">about 3%</a><span> of its car exports to the U.S., which is minuscule. And so on. </span></p><p><span>So as best I can tell, The Economist seems to be picking random data points about countries importing more stuff from China, and just assuming, without evidence, or</span><em> in direct contradiction</em><span> of the evidence, that A) these imports are components that will be used to make stuff that countries then sell to the U.S., and that B) this quantitatively cancels out the shift in U.S. imports away from China. </span></p><p><span>The Economist does present one other piece of circumstantial evidence — </span><a href="https://www.imf.org/-/media/Files/News/Seminars/2023/fragmentation-conference/session-5-paper-2-reconfiguration-of-global-value-chains.ashx" rel="">a working paper</a><span> by Caroline Freund of the IMF that finds that countries that have been increasing their exports to the U.S. tend to be countries that trade more with China. But this is a very far cry from showing that China’s value-added exports to the U.S. have remained constant. </span></p><p><span>Thus The Economist made a bold claim that it was unable to back up with hard data. But </span><em>that doesn’t mean the claim is false</em><span>. As they say, absence of evidence is not evidence of absence. It still </span><em>might be true</em><span> that China’s value-added share of U.S. imports has remained constant! Until we get the value-added trade data from the OECD — which comes out with about a 5 or 6 year lag — we won’t really know for sure.</span></p><p><span>But there are reasons to think that at least part of the drop in U.S. dependence on Chinese imports is real — at least, the recent drop in the last couple of years. First, Chinese exports </span><em>overall </em><span>are </span><a href="https://www.bloomberg.com/news/articles/2023-08-08/china-s-trade-plunges-more-than-forecast-in-blow-to-recovery?sref=R8NfLgwS" rel="">down substantially from 2021</a><span>:</span></p><p><span>Of course that’s not due to any sort of re-shoring policy, but some of it may be due to companies deciding to disinvest in China due to the risk of a war over Taiwan. FDI into China has probably been </span><a href="https://rhg.com/research/cutting-through-the-fog/" rel="">falling outright for a few years now</a><span>; overall foreign investment in the country </span><a href="https://www.forbes.com/sites/brandonkochkodin/2023/02/13/add-this-to-the-turmoil-around-china-foreign-investment-has-gone-negative-for-the-first-time-since-2016/?sh=3009206b3216" rel="">is now negative</a><span> as investors race for the exits. If multinational companies no longer see China as a safe place to make things for sale in the U.S. market, that’s a definite tailwind for decoupling.</span></p><p><span>To the extent that falling investment in China is due to corporate de-risking, it’s probably eventually going to spread all the way up the supply chain. If you’re a U.S. company that moves production from China to Vietnam because of the risk of war, and your Vietnamese factory is importing most of its parts from China, </span><em>you’ll know it</em><span>. And you’ll know that you’re still exposed to China risk, and you’ll look around for alternative places for your Vietnamese factory to get its parts and components. That might take time — years even, given China’s dominance of manufacturing. But as long as the risk of conflict stays high for years, companies will keep looking for ways to build entire supply chains that never run through China. And they will succeed. </span></p><p><span>Second, </span><a href="https://www.kansascityfed.org/documents/9747/JH_Paper_Alfaro.pdf" rel="">Alfaro and Chor (2023)</a><span> find some evidence in favor of </span><em>reshoring</em><span> of production from China — i.e., moving production to the U.S. instead of to Mexico or Vietnam. By definition, reshoring </span><em>can’t </em><span>involve the sort of “phoney decoupling” that The Economist talks about. They also find that Vietnam’s exports to the U.S. have become more capital-intensive “upstream” goods — not the kind of shift you’d see if Vietnam were merely importing Chinese upstream components and then slapping together. </span></p><p>Now, in fairness, Alfaro and Chor do find one big trend that they believe constitutes evidence of continued U.S. dependence on China: Chinese FDI into countries that export a lot to the U.S. (The Economist also mentions this briefly). The idea here is that if the companies that own the Vietnamese and Mexican factories that make stuff for the U.S. are Chinese companies, the U.S. is still dependent on China in some way. </p><p><span>Well, that’s one way of looking at it. But if the factory is in Vietnam, that means that even if the owner is Chinese, it’s Vietnamese workers who are getting paid, and it’s Vietnamese workers who are </span><em>learning</em><span> from working at the factory. Eventually, some will break off and form their own competing, domestically-owned factories. </span></p><p><span>In fact, this is </span><em>exactly what happened with China</em><span>. For decades, multinational companies used China as a cheap production platform, until Chinese entrepreneurs learned their tricks and learned how to level the playing field and move up the value chain. Remember that as recently as 2013, China itself was still largely stuck at the bottom of the “smile curve”, doing low-value assembly work using imported foreign components.</span></p><p>Chinese people learned how to produce high-value components themselves. If you think Vietnamese and Mexican people won’t eventually learn to do the same, I think you should probably have a little more faith in the universality of humanity’s ability to learn how to make stuff.</p><p><em>This will take time.</em><span> Decoupling is not an instantaneous process. China spent two decades becoming the workshop of the world; the rest of the developing world isn’t going to mirror that accomplishment in one or two years. </span></p><p><span>But there are reasons to think the process is just getting started. Although the Economist articles present decoupling as being driven by Biden’s policies, the truth is that the U.S. government has barely begun to push. Export controls are narrowly focused on the chip industry, and the Inflation Reduction Act isn’t really about decoupling at all. Other than that, all Biden has done so far is a set of </span><a href="https://www.wsj.com/articles/u-s-to-ban-some-investments-in-china-71f519d6" rel="">investment restrictions</a><span> that was very weak and narrowly targeted at a couple of high-tech industries, some </span><a href="https://www.energy.gov/articles/biden-harris-administration-announces-30-million-build-domestic-supply-chain-critical" rel="">minuscule investments</a><span> in onshoring of critical minerals, and some </span><a href="https://www.piie.com/publications/working-papers/how-united-states-solved-south-koreas-problems-electric-vehicle" rel="">minor deals with allies</a><span> to share the benefits of green energy tax credits. He kept Trump’s old tariffs on China in place, but didn’t strengthen them. </span></p><p><span>That means that so far, most of the decoupling that’s being done is being driven by the prudent decisions of private companies and investors — </span><em>natural market forces</em><span>, rather than the visible hand of government policy. There’s no reason to expect those forces to reverse themselves — even setting aside the threat of war, every nation has seen some manufacturing activity leave for cheaper countries once costs rise. And meanwhile, the engine of U.S. government policy — and European government policy, etc. — is just revving up. Once policymakers figure out which policies actually speed decoupling without injuring the economy, you can expect them to double down on those.</span></p><p>This is only the very beginning of the decoupling story.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.noahpinion.blog/p/decoupling-isnt-phoney?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.noahpinion.blog/p/decoupling-isnt-phoney?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to get transactions between almost any data stores (125 pts)]]></title>
            <link>https://petereliaskraft.net/blog/epoxy</link>
            <guid>37322460</guid>
            <pubDate>Wed, 30 Aug 2023 14:13:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://petereliaskraft.net/blog/epoxy">https://petereliaskraft.net/blog/epoxy</a>, See on <a href="https://news.ycombinator.com/item?id=37322460">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                            <p>
                            These days, it's pretty common for a single application to use multiple data stores.
                            Sometimes, it's because each data store is specialized for a particular task.
                            For example, an e-commerce site might use Postgres for robust customer data management, Elasticsearch for fast product search, and S3 for cheap image storage.
                            Other times, it's because an app was built by multiple teams over time, so it ends up with one service storing customer data in Postgres and another storing customer data in MongoDB.
                            </p>

                            <p>
                            Building robust apps with multiple data stores is tricky because it's hard to coordinate operations across those stores.
                            Most databases have a notion of a <a href="https://en.wikipedia.org/wiki/Database_transaction">transaction</a>, a group of related operations which execute as a single <a href="https://en.wikipedia.org/wiki/ACID">atomic, consistent, isolated, and durable</a> unit of work.
                            The canonical example of a transaction is transferring money between bank accounts: to transfer $100 from me to you, the bank runs a transaction which withdraws $100 from my account and deposits $100 in your account.  
                            By running both operations in one transaction, we guarantee that either both go through (and the money is transferred) or neither do (and nothing happens), avoiding failure states where $100 disappears from my account but doesn't appear in yours.
                            However, transactions only work within a single database, so if our our system spans multiple data stores, performing operations reliably is much harder.
                            The traditional solution is to use <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit</a> through a protocol like <a href="https://en.wikipedia.org/wiki/X/Open_XA">X/Open XA</a>. 
                            However, while XA is supported by most big relational databases like Postgres and MySQL, it's not supported by popular newer data stores like MongoDB, Cassandra, or Elasticsearch, even though those stores are increasingly embracing transactions.
                            This means that if you want transactions across multiple data stores, you probably have to do all the hard work of synchronization, concurrency control, and failure management yourself.
                            </p>

                            <p>
                            In this blog post, I want to tell you about a new protocol named Epoxy (<a href="http://petereliaskraft.net/res/p2732-kraft.pdf">paper</a>), developed as part of my PhD work at Stanford in the <a href="https://dbos-project.github.io/">DBOS</a> project, which should make this problem easier by providing ACID transactions across heterogeneous data stores.
                            The basic idea behind Epoxy is that we can use a single transactional "primary database", like Postgres, to coordinate transactions among itself and multiple potentially non-transactional "secondary stores", like MongoDB or Elasticsearch.
                            Here's the high-level architecture:
                            </p>

                            <p>
                                <img src="https://petereliaskraft.net/images/EpoxyBlogPost/epoxy-architecture.jpg" alt="">
                            </p>

                            <p>
                            Epoxy works by adapting <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">multi-version concurrency control (MVCC)</a> to a cross-data store setting.
                            We start an Epoxy transaction by initiating a transaction on the primary database.
                            We then ask the primary database for a <i>snapshot</i>, a list of all past primary database transactions whose effects are visible to our new transaction.
                            Typically, that's all transactions that committed before our new transaction began.
                            Here's what a snapshot might look like:
                            </p>

                            <p>
                                <img src="https://petereliaskraft.net/images/EpoxyBlogPost/epoxy-snapshot.jpg" alt="">
                            </p>

                            <p>
                            Here, the blue transactions form the snapshot of our new transaction T11, while the pink transactions are invisible to it because they weren't complete when T11 started.
                            </p>

                            <p>
                            After beginning a transaction, Epoxy interposes on all its reads and writes to secondary stores to enforce <a href="https://jepsen.io/consistency/models/snapshot-isolation">snapshot isolation</a>.
                            This provides the abstraction that the transaction operates on a fixed view of data containing only the changes committed by transactions in its snapshot on any data store. 
                            Whenever a record is updated on any secondary store, Epoxy doesn't change it in place, but instead creates a new version of it.
                            It tags that version with two pieces of metadata: the ID of the transaction that created the version (beginTxn) and the ID of the transaction that superseded it with a new version (endTxn; this is initialized to infinity, then set when the version is superseded).
                            Epoxy stores this metadata inside the record itself, for example as an additional field in a MongoDB or Elasticsearch document.
                            Then, whenever a read occurs, Epoxy interposes on it, filtering its data source to only include records whose beginTxn field is in the transaction snapshot and whose endTxn field is not.
                            In other words, each transaction can only see the one unique version of a record that was created by a transaction in its snapshot, but not superseded by any transaction in its snapshot.
                            To make that more clear, let's extend our example from earlier and imagine that T11 is a transaction between Postgres and MongoDB:
                            </p>

                            <p>
                                <img src="https://petereliaskraft.net/images/EpoxyBlogPost/epoxy-example.jpg" alt="">
                            </p>

                            <p>
                            Here, the blue record versions are visible because they were created but not superseded by transactions in T11's snapshot, while the pink versions are not visible because they were either created by a transaction not in T11's snapshot (like T9) or superseded by a transaction in T11's snapshot (like T8).
                            Thus, Epoxy guarantees that if the effects of a transaction are visible to us in Postgres, they're also visible in MongoDB, and vice versa.
                            </p>

                            <p>
                            Epoxy also guarantees that transactions are atomic and durable: they either commit on all stores or abort and are rolled back on all stores.
                            After a transaction is done with all operations on all stores, Epoxy validates it, verifying that none of its operations conflict with concurrent committed transactions on any data store (this is a form of <a href="https://en.wikipedia.org/wiki/Optimistic_concurrency_control">optimistic concurrency control</a>).
                            It then asks all secondary stores to persist the transaction's changes to durable storage.
                            If these steps succeed, Epoxy commits the transaction by committing on the primary database.
                            This causes the transaction to appear in future snapshots, atomically making it visible to all future transactions on all data stores.
                            If anything goes wrong, Epoxy aborts, rolling back the transaction on the primary database and undoing its changes on all secondary stores.
                            Even if that process takes a long time, that's fine—none of its changes are visible to any other transactions because, unless committed, they aren't in anyone's snapshots.
                            </p>

                            <p>
                            Of course, like any new protocol, Epoxy has limitations.
                            First, it makes assumptions about secondary stores, specifically that they provide a way to tag records with metadata and efficiently filter records based on that metadata.
                            Luckily, these assumptions are met by most popular data stores; for example, in MongoDB, you can store all Epoxy metadata in additional document fields and create an index on the field to quickly filter it.
                            Second, Epoxy's interposition comes with some overhead.
                            We analyzed it in <a href="http://petereliaskraft.net/res/p2732-kraft.pdf">our paper</a>—on simulated microservices, Epoxy adds overhead of &lt;10% on read-mostly workloads and ~70% on write-heavy workloads compared to a coordination-free baseline.
                            We found this is similar to the overhead added by XA, even though Epoxy supports more data stores and provides stronger guarantees.
                            Third, and most importantly, Epoxy must be the exclusive mode of accessing a secondary store table because it physically modifies records on secondary stores by adding versioning metadata to them.
                            If one application accessing a secondary store table uses Epoxy, all other applications using that store must use Epoxy for operations on that table.
                            This might make Epoxy tricky to adopt for a database used by multiple services, and it's something that can hopefully be improved in future research.
                            </p>

                            <p>
                            If you're interested in learning more about Epoxy, please check out <a href="http://petereliaskraft.net/res/p2732-kraft.pdf">our paper</a> (appearing at <a href="https://vldb.org/2023/">VLDB 2023</a>).
                            The paper contains a formal description of the protocol, proofs of correctness, and detailed experiments.
                            There's also source code for our research prototype available <a href="https://github.com/DBOS-project/apiary/blob/main/Epoxy.md">on GitHub</a>, which implements Epoxy for one primary database (Postgres) and four secondary stores (MongoDB, Elasticsearch, MySQL, and Google Cloud Storage).  Let us know what you think!
                            </p>

                            <p>
                                <img src="https://petereliaskraft.net/images/EpoxyBlogPost/epoxy-logo.jpg" alt="">
                            </p>                            



                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding Llama 2 and the New Code Llama LLMs (154 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models</link>
            <guid>37321032</guid>
            <pubDate>Wed, 30 Aug 2023 12:32:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models">https://magazine.sebastianraschka.com/p/ahead-of-ai-11-new-foundation-models</a>, See on <a href="https://news.ycombinator.com/item?id=37321032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>In this edition of the newsletter, we direct our attention to one of the most prominent highlights of the summer: the release of the Llama 2 base and chat models, as well as CodeLlama, the latest highlights in the open-source AI large language model (LLM) landscape.</p><p>Additionally, we delve into the leaked GPT-4 model details, discussing an analysis of its performance over time and covering emerging alternatives to the prevalent transformer-based LLMs.</p><p>OpenAI announced its new finetuning API this week, designed to train the GPT-3.5-turbo on custom data sets. This new offering might further fuel the ongoing conversations around closed, proprietary AI systems and the open-source AI models that can be deployed on-premises.</p><p>The field of AI continues to shift and evolve, with significant contributions from the open-source community, including in the development of finetuning technologies such as Llama-Adapters, LoRA, QLoRA, and beyond. I am particularly excited to see what innovations will come from the NeurIPS LLM Efficiency Challenge.</p><p><em>Ahead of AI</em><span> is a reader-supported, ad-free magazine featuring occasional bonus posts for supporters. Your support through a paid subscription would be immensely appreciated.</span></p><p>Those who are interested in bonus content can find more information about Llama 2 in the following article:</p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-missing-bits-llama-2-weights" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9552fd6-161f-47f1-a2ea-e9f196f4e14b_1600x857.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9552fd6-161f-47f1-a2ea-e9f196f4e14b_1600x857.png" sizes="100vw" alt="The Missing Bits: Llama 2 Weights Have Changed" width="140" height="140"></picture></div></a></div><p>Let's dive right in and start with (what I consider) the biggest LLM-related release this summer, Llama 2. Meta's Llama 2 is the official successor to the popular LLaMA model. Both the original LLaMA model and Llama 2 releases were accompanied by very detailed research articles, which I highly appreciate:</p><ul><li><p><em><a href="https://arxiv.org/abs/2302.13971" rel="">LLaMA: Open and Efficient Foundation Language Models</a></em><span>, Touvron et al., Feb 2023</span></p></li><li><p><em><a href="https://arxiv.org/abs/2307.09288" rel="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></em><span>, Touvron et al., Jul 2023</span></p></li></ul><p>Like LLaMA, Llama 2 is a model suite consisting of multiple pretrained LLMs of different sizes. So, what's new and what's interesting about Llama 2? Let's discuss this one by one.</p><p><span>First things first. LLaMA stands for </span><strong>L</strong><span>arge </span><strong>La</strong><span>nguage </span><strong>M</strong><span>odel Meta </span><strong>A</strong><span>I. The capitalization makes it a bit tedious to type, but the good news is that the official spelling of its successor is just "Llama 2". So, if you stumble upon LLaMA in this article, I am referring to LLaMA v1, and Llama 2 refers to the new model, which we are focusing on here.</span></p><p>Before we discuss individual details further below, here's a brief summary of what's new:</p><ul><li><p>Llama 2 was trained on 40% more data than the original LLaMA model.</p></li><li><p>Also, Llama 2 now supports 2x larger inputs.</p></li><li><p>The larger 34B and 70B models have a small architectural change using multi-query attention, which we will discuss later.</p></li><li><p>Another big change is that the Llama 2 modeling suite now includes finetuned models (via supervised finetuning and reinforcement learning with human feedback); more details later.</p></li><li><p>The Llama 2 license now permits commercial use.</p></li></ul><blockquote></blockquote><p><span>LLaMA (7B, 13B, 33B, and 65B) and Llama 2 (7B, 13B, 34B*, and 70B) come in various sizes, offering different modeling performance and computational requirement trade-offs. However, while LLaMA only offered pretrained base models, Llama 2 offers both pretrained </span><em><strong>base</strong></em><span>&nbsp;models and finetuned </span><em><strong>chat</strong></em><span>&nbsp;models. Here, </span><em>base</em><span>&nbsp;model means that it's only been trained on the next-word pretraining task. The finetuned </span><em>chat</em><span>&nbsp;models have been finetuned on instruction datasets using supervised learning and reinforcement learning with human feedback (RLHF), but more later.</span></p><p>* (Interestingly, Meta only releases the 7B, 13B, and 70B models, not the 34B model.)</p><p><span>Most readers may want to know the license and usage restrictions before deciding whether to invest more time into Llama 2 and read the further details below. While LLaMA was a big hit and a celebrated effort in the research community, it was also mildly criticized for its restrictive license. The LLaMA inference code was open source, whereas the model weights, granted on a case-by-case basis to academic researchers, were not released as open source. In short, the LLaMA license allowed research but not commercial use, whereas the </span><a href="https://github.com/facebookresearch/llama/blob/main/LICENSE" rel="">Llama 2 license</a><span>&nbsp;now also permits use in commercial applications (see the </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="">Request to Access page</a><span>&nbsp;for details). However, while the Llama 2 license is permissive enough for most use cases, it is not an open-source license; as discussed in the article </span><a href="https://www.google.com/url?q=https://blog.opensource.org/metas-llama-2-license-is-not-open-source/&amp;sa=D&amp;source=editors&amp;ust=1692833158282053&amp;usg=AOvVaw12tE3bEYV5jVbDiv6dE-jg" rel="">Meta's LLaMa 2 license is not Open Source</a><span>. Personally, I find the Llama 2 usage terms more than reasonable, even though using the term "open source" may be confusing and potentially misleading here. (Nonetheless, for simplicity, I will group Llama 2 among the open-source models for the remainder of the article.)</span></p><p>Now that we have addressed the license and usage question in the previous paragraph, let's get to the most interesting part and address the elephant in the room: "How good is Llama 2"? To address this question fairly, we have to distinguish between Llama 2 base models and finetuned Llama 2 chat models.</p><p>Let's talk about the base models first, which are decoder-style LLMs that have been only trained via the conventional next-word prediction pretraining task.</p><p>For more information about decoder-style vs encoder-style LLMs, see my article below:</p><p>In short, Llama 2 base models compare extremely favorably to other open-source models as shown in the annotated figure below.</p><p>However, Llama 2 base models perform worse than popular closed-source models, but this is expected since the closed-source models listed in the table below are 1) finetuned and 2) much larger (GPT-3.5 has 175B parameters versus 70B parameters in Llama 2). We will see a fairer comparison to the finetuned Llama 2 models later.</p><p>The 77-page Llama 2 report is surprisingly detailed, which I really appreciate. Interestingly, though, in contrast to the first LLaMA model, the report doesn't offer any insights into the training data besides the following short description:</p><p><em>&gt; more robust data cleaning, updated our data mixes, trained on 40% more total tokens</em></p><p>We can speculate that this is either due to 1) keeping a competitive advantage over other open source models or 2) avoiding copyright related lawsuits.</p><p><span>The following Business Insider article argues that it might be due to the latter: </span><em><a href="https://www.businessinsider.in/tech/news/llama-copyright-drama-meta-stops-disclosing-what-data-it-uses-to-train-the-companys-giant-ai-models/articleshow/101887460.cms" rel="">Llama copyright drama: Meta stops disclosing what data it uses to train the company's giant AI models</a><span>.</span></em></p><p>Among the highlights of the Llama 2 model suite are the chat models. These chat models are based on the Llama 2 base models and have undergone additional instruction finetuning similar to InstructGPT and ChatGPT.</p><p>The Llama 2-chat models compare very favorably against other popular chat models, as shown in the annotated figure below.</p><p>The finetuning was done in multiple stages: supervised-instruction finetuning as in Alpaca (the first LLaMA model finetuned by researchers at Stanford, and later many other open source LLMs) followed by reinforcement learning with human feedback (RLHF) similar to ChatGPT, as summarized in the annotated figure below.</p><p><span>The section on supervised tuning placed a focus on utilizing a smaller subset of high-quality data, comprising only thousands of samples from their pool of millions. This approach was aligned with the "less is more" philosophy as discussed in the </span><a href="https://www.google.com/url?q=https://arxiv.org/abs/2305.11206&amp;sa=D&amp;source=editors&amp;ust=1692833158286565&amp;usg=AOvVaw2cZIpO1zw7yT7xRnrVyB4N" rel="">LIMA paper</a><span>, a subject I previously covered in a previous issue of this newsletter:</span></p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e54422-a516-49b2-aa28-5f9ee1980541_680x488.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff4e54422-a516-49b2-aa28-5f9ee1980541_680x488.png" sizes="100vw" alt="Ahead of AI #9: LLM Tuning &amp; Dataset Perspectives" width="140" height="140"></picture></div></a></div><p>Following the initial supervised finetuning, the models were further refined using RLHF. While an in-depth discussion of RLHF is beyond the scope of this already extensive article, further information can be found in a previous issue of this newsletter as well:</p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-6-train-differently" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e8e33cc-364e-43f3-8302-d35c984a2b97_1960x1195.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e8e33cc-364e-43f3-8302-d35c984a2b97_1960x1195.jpeg" sizes="100vw" alt="Ahead of AI #6: TrAIn Differently" width="140" height="140"></picture></div></a></div><p><span>It is worth mentioning that InstructGPT and ChatGPT leveraged proximal policy optimization (PPO) for RLHF training. In contrast, the Llama 2 authors conducted experiments with two different methods, </span><a href="https://arxiv.org/abs/2302.01318" rel="">rejection sampling</a><span>&nbsp;and RLHF with PPO.</span></p><p>The authors also nicely illustrated the evolution of the Llama 2 70B Chat models, tracing their journey from the initial supervised finetuning (SFT-v1) to the final RLHF finetuning stage with PPO (RLHF-v5). The chart reflects consistent improvements in both the harmlessness and helpfulness axes, as shown in the annotated plots below.</p><p><span>In the future, it will be interesting how </span><a href="https://arxiv.org/abs/2305.18290" rel="">Direct Policy Optimization (DPO)</a><span>, a reinforcement learning-free alternative I previously covered in </span><a href="https://magazine.sebastianraschka.com/p/ai-research-highlights-in-3-sentences-2a1" rel="">AI Research Highlights In 3 Sentences Or Less (May-June 2023)</a><span>, compares to these RLHF approaches.</span></p><p>The Llama 2 paper is very detailed, and covering all its aspects in this newsletter issue would be impossible. However, below, I wanted to highlight a few more tidbits that I found interesting.</p><p><span>For instance, 34B and 70B parameter Llama models use </span><a href="https://arxiv.org/abs/2305.13245" rel="">grouped-query attention (GQA)</a><span>. GQA can be regarded as a more generalized form of multi-query attention, a concept previously employed in models such as Falcon.</span></p><p>The likely motivation behind this is to reduce computational requirements with minimal impacts on the modeling performance.</p><p>While the Llama 2 base and chat models perform well, there's still room for improvement through finetuning. Think of Llama 2 as a revised foundational model that can be further fine-tuned for more specialized tasks.</p><p>To illustrate the potential benefit of finetuning, let's take the example of the BoolQ benchmark. Surprisingly, Llama 2 doesn't outperform DeBERTa-1.5B, an encoder-only model, on this particular task. It's a reminder that even strong models like Llama 2 can be outperformed by smaller finetuned models, especially when targeting specific tasks.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png" width="1456" height="626" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:626,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe44fb450-26ed-45fc-8202-32f75f6e517c_1999x860.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>For fairness, the DeBERTa-1.5B model (initially proposed at ICLR 2021) was finetuned on the training data portion of BoolQ, whereas Llama 2 was used via few-shot prompting.</p><p>While this experiment has not been done yet, a finetuned Llama 2 model could perform even better on this dataset -- or at least better than the Llama 2 base model.</p><p><span>(And as shown via </span><a href="https://arxiv.org/abs/2111.09543" rel="">DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</a><span>&nbsp;at ICLR 2023, it's also possible to even use a much smaller 300M parameter model for good performance on this task.)</span></p><p>As we've seen above, Llama 2 is an extremely capable model. However, how do we use Llama 2 in a custom project?</p><p><span>Meta maintains </span><a href="https://github.com/facebookresearch/llama" rel="">a GitHub repository</a><span>&nbsp;with the inference code to use the Llama 2 weights.</span></p><p>In addition, Llama is now also supported in Lit-GPT, an open-source repository that supports a large variety of LLMs for pretraining and finetuning. Among others, I recently helped implement QLoRA support in Lit-GPT and included a few Llama 2 performance benchmarks below.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png" width="1456" height="803" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:803,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83472bb7-1ff9-4aa7-be4a-612f6de45121_1999x1103.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><a href="https://github.com/Lightning-AI/lit-gpt" rel="">Lit-GPT</a><span>&nbsp;Llama 2 benchmark with QLora.</span></figcaption></figure></div><p><a href="https://github.com/karpathy/llama2.c" rel="">Andrej Karpathy also released </a><span>a GitHub repository for Llama 2 inference (not pretraining or finetuning) in just plain C code, which can be interesting for tinkers who want to just run but not finetune Llama 2.</span></p><p><span>Llama 2 is awesome. However, coding tasks were not its strong suit. For instance, the HumanEval benchmark is a coding-related benchmark from the paper </span><em><a href="https://arxiv.org/abs/2107.03374" rel="">Evaluating Large Language Models Trained on Code</a></em><span>.</span></p><p><span>Two days ago, </span><a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/" rel="">Meta released a new suite of code models</a><span> (7B, 13B, and 34B), under the same license as Llama 2, that have been specifically trained on code. These models come in three flavors: a general code model (Code Llama), an instruction-following model (Code Llama-instruct), and a version specialized to Python code (Code Llama-Python). Note that all Code Llama models were initialized with Llama 2 weights before they were further trained on code.</span></p><p>The 34B Code Llama model is about twice as good as the original 70B Llama 2 model, closing the gap to (the much larger) GPT-4.</p><p><strong>Why No 70B Code Llama Model?</strong></p><p>Interestingly, Code Llama is only available as 7B, 13B, and 34B parameter versions; unlike Llama 2, there is no 70B version. While the paper doesn’t provide any explanation, I suspect this could be due to two reasons:</p><ol><li><p><span>The Code Llama models were trained on 500B additional code tokens, starting with Llama 2 weights, whereas Llama 2 models were trained on 2T tokens. Since the Code Llama model was trained on 4x fewer domain-specific tokens, maybe a CodeLlama 70B version did not perform well enough due to&nbsp;</span><a href="https://arxiv.org/abs/2001.08361" rel="">LLM scaling laws</a><span>—there was not enough training data.</span></p></li><li><p>Code Llama models support context sizes of 100k, which is very useful when working with code. Llama 2, in contrast, only supports up to 4k tokens as input. These 70B models may be computationally infeasible (or reasonable hardware clusters) if they were to support 100k token inputs.</p></li></ol><p><span>Switching gears from open-source to closed-source models, there have also been a few interesting news on the GPT-4 front last month. For instance, the GPT-4 mode</span><a href="https://www.reddit.com/r/LocalLLaMA/comments/14wbmio/gpt4_details_leaked/" rel="">&nbsp;details have now apparently (finally) been leaked</a><span>:</span></p><p><em>&gt; &nbsp;GPT-4 is a language model with approximately 1.8 trillion parameters across 120 layers, 10x larger than GPT-3. It uses a Mixture of Experts (MoE) model with 16 experts, each having about 111 billion parameters. Utilizing MoE allows for more efficient use of resources during inference, needing only about 280 billion parameters and 560 TFLOPs, compared to the 1.8 trillion parameters and 3,700 TFLOPs required for a purely dense model.</em></p><p><em>&gt; The model is trained on approximately 13 trillion tokens from various sources, including internet data, books, and research papers. To reduce training costs, OpenAI employs tensor and pipeline parallelism, and a large batch size of 60 million. The estimated training cost for GPT-4 is around $63 million.</em></p><p><span>Source: </span><a href="https://www.reddit.com/r/LocalLLaMA/comments/14wbmio/gpt4_details_leaked/" rel="">https://www.reddit.com/r/LocalLLaMA/comments/14wbmio/gpt4_details_leaked/</a></p><p>In particular, the fact that GPT-4 is apparently using a Mixture of Experts (MoE) approach is very interesting and important to highlight. MoE means that it is to improve the performance of a system by combining the predictions or decisions of several specialized sub-models or "experts."</p><p>So, suppose we want to improve models like Llama 2 further and outperform GPT-4-like offerings (by a large margin). In that case, we may not only have to scale the model to a similar size (in this case, 25x larger to be comparable) but also consider using MoE approaches.</p><p><span>We often see claims that OpenAI degrades the ChatGPT performance over time to save computation time and cost. In the recent </span><em><a href="https://arxiv.org/abs/2307.09009" rel="">How is ChatGPT's behavior changing over time?</a></em><span>&nbsp;paper, researchers make an interesting observation that GPT-4's modeling performance appears to be indeed getting worse over time. Is it due to distillation methods to save costs or guard rails to prevent certain types of misuse? (By the way, it is interesting to see research efforts being diverted into studying these changes that could likely already be answered by a researcher and engineers who worked on these models.)</span></p><p><span>As a follow-up, I highly recommend reading the Substack article</span><em><span>&nbsp;</span><a href="https://www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time" rel="">Is GPT-4 getting worse over time?</a></em><span>&nbsp;by </span></p><p><span> and </span></p><p><span>who highlight important shortcomings of the research article above. For instance,</span></p><p><em>&gt; What seems to have changed is that the March version of GPT-4 almost always guesses that the number is prime, and the June version almost always guesses that it is composite. The authors interpret this as a massive performance drop — since they only test primes. For GPT-3.5, this behavior is reversed.</em></p><p>So, the claim that GPT-4 is getting worse over time may not be necessarily true. There are ongoing changes with regard to its behavior, though.</p><p>In practice, one of the biggest advantages of using an API is that we don't have to worry about hosting and serving the model itself. However, this is also the biggest shortcoming if we build services on top of it. Sure, performance improvements are usually appreciated. However, intransparent changes to the model behavior can also cause all your previously working queries to not work reliably the next morning. The more you build on top of a closed API, the more annoying this problem can become.</p><p><span>Last Wednesday, </span><a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates" rel="">OpenAI announced its new finetuning service for ChatGPT</a><span>. This is super interesting for those who want to finetune LLMs on custom data.</span></p><p>It will be interesting to see how it fares against open-source solutions from which the most recent cutting-edge finetuning techniques emerged (Llama-Adapters, LoRA, QLoRA, and more).</p><p>Also, the recent open-source Llama 2 model, which we discussed above, compares very favorably to ChatGPT / GPT-3.5, as shown below.</p><p>Based on the OpenAI documentation and marketing materials, it doesn't seem like the service will be restricted to just changing the style or tone of the LLM's responses. Instead, it appears that OpenAI is offering a fully-fledged instruction tuning service.</p><p>However, this also means it will likely still be impossible to fully adapt the models to new specialized target domains like medical, financial, or legal contexts. That's because new knowledge is usually ingested via pretraining, not finetuning; this is also true for open-source models.</p><p>While OpenAI does not specify which finetuning method they will use, opting for low-rank adaptation (LoRA) as a finetuning technique would make sense. I believe this would be the easiest and most cost-effective because they won't have to store a new 175B parameter LLM for each customer. With LoRA, you can use the same base LLM but only need to store the LoRA weights.</p><p>One of the open research challenges is how to extend LLMs to larger input contexts. Recent approaches include</p><p><span>1) the </span><a href="https://arxiv.org/abs/2304.11062#:~:text=This%20technical%20report%20presents%20the,models%20in%20natural%20language%20processing." rel="">RMT paper on scaling transformers to 1 million tokens</a><span>;</span></p><p><span>2) the</span><a href="https://arxiv.org/abs/2306.15794" rel="">&nbsp;convolutional Hyena LLM for 1 million tokens</a><span>;</span></p><p><span>3) </span><a href="https://arxiv.org/abs/2307.02486" rel="">LongNet: scaling transformers to 1 billion tokens</a><span>.</span></p><p>While there are several use cases for such long LLMs, for example, asking questions about particular long document inputs, the elephant in the room is: How well do LLMs use these longer contexts?</p><p><span>New research (</span><em><a href="https://arxiv.org/abs//2307.03172" rel="">Lost in the Middle: How Language Models Use Long Contexts</a></em><span>) shows that LLMs are good at retrieving information at the beginning of documents. They do less well in terms of retrieving information if it's contained in the middle of a document.</span></p><p>The analysis in this paper is focused on ChatGPT (as it's shown in the figure) and Claude. Of course, it would be interesting to include other models (Hyena, LongNet etc.) in the future.</p><p>Nonetheless, this is quite interesting!</p><p><span>1) I would expect that the opposite is true for, e.g., RNN-based LLMs like </span><a href="https://www.google.com/url?q=https://arxiv.org/abs/2305.13048&amp;sa=D&amp;source=editors&amp;ust=1692833158299736&amp;usg=AOvVaw2h6bCGXGmMWf0X9Af5mXGh" rel="">RWKV</a><span>&nbsp;(since it's processing information sequentially, it might rather forget early information)</span></p><p>2) To my knowledge, there is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it's then how LLMs parameterize the attention weights during training.</p><p><strong>More Transformer Alternatives</strong></p><p><span>I mentioned several alternatives to transformer-based LLMs in the paragraph above, such as the recurrent </span><em><a href="https://arxiv.org/abs/2305.13048" rel="">RWKV</a></em><span>&nbsp;LLM and the convolutional </span><em><a href="https://arxiv.org/abs/2306.15794" rel="">Hyena LLM</a></em><span>.</span></p><p><span>The newest transformer alternative is Retentive Network (also called RentionNet), which was proposed in a paper last month with a bold headline: </span><em><a href="https://arxiv.org/abs/2307.08621" rel="">Retentive Network: A Successor to Transformer for Large Language Models</a><span>.</span></em></p><p>This paper proposes another alternative to large language transformers, one that has linear instead of quadratic complexity concerning the input sequence lengths. RetentionNet can be trained in a parallel mode and switched to a recurrent model to extend context lengths without increasing memory cost to achieve good inference performance. The largest model was 6.7B parameters, and it will be interesting to see future studies comparing RetNet to Llama-2 70B and others.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png" width="670" height="444.5192307692308" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:966,&quot;width&quot;:1456,&quot;resizeWidth&quot;:670,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739e2c39-00d9-49f8-a92e-58deccae5259_1926x1278.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Will it be a widely used alternative to transformer-based LLMs? After the initial hype, it doesn't seem like it's being adopted in any other projects at the moment, but time will tell.</p><p>The AI community loves coining new terms!</p><p>For quite a while, I found myself resistant to using the term "foundation model." I was convinced that there was no need to introduce a new phrase to describe pretrained large language models (LLMs) or pretrained vision transformers (ViTs).</p><p><span>Initially, many people saw the paper that coined the term "foundation model" appeared to provide academics as a means to garner citations. (This sentiment was evidenced by the </span><a href="https://arxiv.org/abs/2108.07258" rel="">113-author Stanford article that coined this term</a><span>, collecting 1372 citations as of today.) However, despite early criticisms, the term has started to gain more widespread acceptance in recent times. In keeping with this trend, I've also begun incorporating "foundation models" into my vocabulary.</span></p><p><span>Last month, authors from the </span><em><a href="https://www.governance.ai/post/frontier-ai-regulation" rel="">Centre for the Governance of AI</a></em><span>&nbsp;(with co-authors from Google DeepMind, OpenAI, and Microsoft): wrote a new paper, </span><a href="https://arxiv.org/abs/2307.03718" rel="">Frontier AI Regulation: Managing Emerging Risks to Public Safety</a><span>&nbsp;to coin a new term </span><em>“frontier AI models” </em><span>on top of foundation models:</span></p><p><span>&gt; </span><em>We define “frontier AI models” as highly capable foundation models that could exhibit dangerous capabilities.</em></p><p>We live in interesting times.</p><p><span>Despite or because of all the recent LLM progress, there's been an ongoing GPU shortage, especially when it comes to NVIDIA's H100 chips. As VentureBeat recently reported, </span><a href="https://venturebeat.com/ai/nvidia-gpu-shortage-is-top-gossip-of-silicon-valley/" rel="">Nvidia GPU shortage is ‘top gossip’ of Silicon Valley</a><span>.</span></p><p><span>While everyone is fighting over H100, NVIDIA also announced its next generation of GPUs, </span><a href="https://www.cnbc.com/2023/08/08/nvidia-reveals-new-ai-chip-says-cost-of-running-large-language-models-will-drop-significantly-.html" rel="">the GH200</a><span>, which is going to offer 141 GB of RAM (as opposed to 80 GB in the H100) -- maybe another way of solving the LLM long-context problem with brute (hardware) force.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png" width="334" height="334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1304,&quot;width&quot;:1304,&quot;resizeWidth&quot;:334,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1b12450-3221-4c39-a7e2-47e6ab0c1433_1304x1304.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Machine learning humor</figcaption></figure></div><p>What is the current status on LLMs and generative AI regarding copyright laws? 8 months into 2023, it seems that this question still doesn't have (m)any definitive answers. Below, I polled a few articles based on the best of my knowledge. However if you have any additional insights or resources, I'd love to hear! Note that I am not a lawyer and cannot comment on any of the legal aspects. Also, since I am not a lawyer, the following information should not be taken as legal advice.</p><p><a href="https://en.wikipedia.org/wiki/Wikipedia:Large_language_models_and_copyright#:~:text=3%20Notes-,Does%20LLM%20output%20inherently%20violate%20copyright%20law%3F,works%20created%20by%20non%2Dhumans." rel="">According to Wikipedia</a><span>, "The copyright status of LLMs trained on copyrighted material is not yet fully understood."</span></p><p><span>In this context, a recent article and study that appeared in the Atlantic, </span><a href="https://www.theatlantic.com/technology/archive/2023/08/books3-ai-meta-llama-pirated-books/675063/" rel="">Revealed: The Authors Whose Pirated Books Are Powering Generative AI</a><span>&nbsp;is worth highlighting.</span></p><p><span>This article addresses the recent legal action taken by various authors against Meta, accusing the company of utilizing their copyrighted works in the training of </span><a href="https://arxiv.org/abs/2302.13971" rel="">LLaMA v1</a><span>. The reason for this lawsuit was the training data, which included Books3 section of </span><a href="https://arxiv.org/abs/2101.00027" rel="">The Pile</a><span>, encompassing no less than 170,000 books. Notably, the same dataset was used for EleutherAI's </span><a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/" rel="">GPT-J</a><span>&nbsp;(a model similar to GPT-3) and </span><a href="https://arxiv.org/abs/2303.17564" rel="">BloombergGPT</a><span>, Bloomberg's language model for finance, which was detailed in the third issue of Ahead of AI.</span></p><p><span>Moreover, </span><a href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl" rel="">NPR recently reported</a><span>&nbsp;that the </span><em>New York Times</em><span>&nbsp;is preparing a lawsuit against OpenAI since it's concerned that ChatGPT generates text that responds to inquiries, drawing from the initial reporting and writing produced by the paper's team.</span></p><p>Laws are laws, and developers should work with content creators to avoid violating copyright laws. In addition, it would be helpful if there were updated legal guidelines that clearly address the use of LLMs. For drafting policies or amending laws, it could be helpful to consider "fair use" contexts. In other words, it may also be helpful to distinguish between LLMs developed for commercial purposes (for example, ChatGPT) and LLMs that are solely developed for research purposes whose licenses forbid commercial applications (for example, Meta's LLaMA).</p><p><span>What is "fair use"? </span><a href="https://en.wikipedia.org/wiki/Fair_use" rel="">According to Wikipedia</a><span>, in the United States, the principle of "fair use" within copyright law may allow the utilization of copyrighted materials without authorization, provided that the use involves a substantial transformation of the work and does not endanger the copyright owner's interests. However, whether this concept of fair use extends to the training of machine learning models is still unresolved.</span></p><p>How do other countries handle the use and training of AI concerning copyrighted materials?</p><p><a href="https://petapixel.com/2023/06/05/japan-declares-ai-training-data-fair-game-and-will-not-enforce-copyright/" rel="">A few months ago, PetaPixel and other news websites reported</a><span>&nbsp;that Japan declared that it would permit generative AI training on any kind of data and that it will not enforce copyrights when it comes to training generative AI models.</span></p><p><span>The AI Act in the EU requires companies to publicly disclose any copyrighted work used in training, as reported by </span><a href="https://www.lexology.com/library/detail.aspx" rel="">Lexology</a><span>. However, this article is not very clear on what happens if copyrights are violated.</span></p><p>Overall, it seems that a lot of rules are still being drafted and amended. I am hoping that the rules, whatever they are, remain clear so that AI researchers and practitioners can adjust and act accordingly.</p><p><span>I am excited to be giving the LLM-focused keynote talk at </span><strong><a href="https://www.southerndatascience.com/" rel="">SDSC in Atlanta</a></strong><span>&nbsp;on Tuesday, September 5th. If you are attending as well, I'd be more than happy to meet and chat!</span></p><p><span>If you are looking for an introduction to machine learning and deep learning in Python, I'll also be at </span><strong><a href="https://www.google.com/url?q=https://posit.co/conference/&amp;sa=D&amp;source=editors&amp;ust=1692833158307384&amp;usg=AOvVaw0gMi7ke3OTff3uhUk5ebk0" rel="">posit::conf(2023)</a></strong><span>, giving an 8-hour hands-on tutorial in Chicago on September 18th.</span></p><p><span>The </span><a href="https://llm-efficiency-challenge.github.io/" rel="">NeurIPS 2023 LLM Efficiency Challenge</a><span>&nbsp;is a competition focused on training 1 LLM for 24 hours on 1 GPU – the team with the best LLM gets to present their results at NeurIPS 2023.</span></p><p><span>This challenge is a very exciting opportunity to develop and try new methods to train LLMs more efficiently. If you are interested in participating (the submission deadline is October 15th), check out the official competition website. I also wrote a </span><a href="https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/neurips_challenge_quickstart.md" rel="">short starter guide</a><span>&nbsp;that might be helpful!</span></p><p><em>Ahead of AI</em><span> is a reader-supported, ad-free magazine. Your support through a paid subscription would be immensely appreciated.</span></p><p>Those who are interested in bonus content can find more information about Llama 2 in the following article:</p><div data-component-name="DigestPostEmbed"><a href="https://magazine.sebastianraschka.com/p/the-missing-bits-llama-2-weights" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9552fd6-161f-47f1-a2ea-e9f196f4e14b_1600x857.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9552fd6-161f-47f1-a2ea-e9f196f4e14b_1600x857.png" sizes="100vw" alt="The Missing Bits: Llama 2 Weights Have Changed" width="140" height="140"></picture></div></a></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A DIY ‘bionic pancreas’ is changing diabetes care (377 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02648-9</link>
            <guid>37321028</guid>
            <pubDate>Wed, 30 Aug 2023 12:32:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02648-9">https://www.nature.com/articles/d41586-023-02648-9</a>, See on <a href="https://news.ycombinator.com/item?id=37321028">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Ten years ago, a tech-savvy group of people with type 1 <a href="https://www.nature.com/immersive/d42859-021-00002-5/index.html" data-track="click" data-label="https://www.nature.com/immersive/d42859-021-00002-5/index.html" data-track-category="body text link">diabetes</a> (T1D) decided to pursue a DIY approach to their own treatment. They knew that a fairly straightforward piece of software could make their lives much easier, but no companies were developing it quickly enough.</p><p>What this software promised was freedom from having to constantly measure and control their blood-glucose levels. In people without T1D, when glucose levels rise, cells in the pancreas release insulin, a hormone that helps tissues to absorb that glucose. In T1D, these cells are killed by the immune system, leaving people with the condition to manage their blood sugar by taking insulin.</p><p>“It is almost inhumane,” says Shane O’Donnell, a medical sociologist at University College Dublin, who, like everyone quoted in this article, lives with T1D. “You’re constantly having to think about diabetes in order to survive.”</p><p>Members of the nascent DIY community were using the most sophisticated technology available: insulin pumps and wearable devices called constant glucose monitors. But they still had to read the monitor’s data, forecast their diet and exercise and then calculate the appropriate insulin dose.</p><p>What they wanted was automation — an algorithm that would analyse glucose data and program the pump itself. Coalescing around this aim in 2013, the community debuted a hashtag: #WeAreNotWaiting.</p><p>Then, in February 2015, group member Dana Lewis shared the code for an algorithm that she and two collaborators had developed and tested.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-03055-2" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25943882.jpg"><p>‘Bionic pancreas’ tames diabetes blood-sugar spikes</p></a>
 </article><p>“We didn’t set out do to anything big,” says Lewis, now an independent researcher in Seattle, Washington. But soon, people who had downloaded and used the algorithm shared their personal experiences and gave feedback. When users suggested tweaks and potential improvements, others tried them and reported back.</p><p>Katarina Braune, an endocrinologist at Charité – Berlin University Medicine, estimates that around 30,000 people now use open-source technology for automated insulin delivery (AID). Some use Lewis and colleagues’ original OpenAPS system, which requires a minicomputer to control it, whereas others use either AndroidAPS (which evolved from Lewis’ system) or Loop, which are smartphone applications.</p><p>The movement has continued to mature. After years of relying on self-reported data, in the past year, two randomized controlled trials<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup><sup>,</sup><sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> have shown the <a href="https://www.nature.com/articles/d41586-022-03055-2" data-track="click" data-label="https://www.nature.com/articles/d41586-022-03055-2" data-track-category="body text link">safety and effectiveness of open-source systems</a>. And this January, the US Food and Drug Administration (FDA) granted regulatory clearance to an <a href="https://www.tidepool.org/blog/tidepool-loop-has-received-fda-clearance" data-track="click" data-label="https://www.tidepool.org/blog/tidepool-loop-has-received-fda-clearance" data-track-category="body text link">AID system based on an open-source algorithm</a> for the first time.</p><p>Today, however, the technology landscape for T1D is much more crowded. The first commercial AID system was launched in 2017 and, currently, five companies sell such systems, with more than 750,000 users.</p><p>Is this the beginning of the end for the open-source movement in diabetes care? Some diabetologists think so. But many advocates reject that idea, saying that the community is still pushing the technology in new directions that promise more personalization and automation than commercial versions currently provide.</p><h2><b>Gaining traction</b></h2><p>Sufyan Hussain, an endocrinologist at King’s College London, says that he was initially sceptical about the DIY AID community. But when he started to engage with it in around 2016, he was “shocked at how well engineered the solutions were in terms of the safety and understanding”.</p><p>In 2022, Hussain co-authored an international consensus statement<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> — signed by more than 40 medical and legal experts and backed by 9 diabetes charities — calling for health-care professionals to support those wanting to use open-source AID.</p><p>Results from randomized trials have further elevated the status of DIY technology. A study published this year<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> found that an open-source and a commercial AID system both controlled glucose levels similarly well. And a September 2022 study<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> demonstrated the efficacy of an algorithm that runs an artificial pancreas system on Android smartphones. By recruiting participants who were new to the technology, the study addressed long-standing criticisms that advocates had previously cherry-picked data from highly motivated, tech-literate members.</p><p>Despite their tens of thousands of users, many of whom have chosen not to use available commercial systems, DIY devices for T1D remain relatively niche. O’Donnell says that a welcoming and supportive community guides people with limited technology skills through setting up systems. But most people with T1D — and most doctors — haven’t encountered these systems, says Aaron Kowalski, president and chief executive of JDRF, a non-profit research organization in New York City that focuses on T1D.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25957846.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25957846.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Close-up of a DIY rig on a wooden table with a one cent coin for scale" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25957846.jpg">
  <figcaption>
   <p><span>An example of an early OpenAPS set-up from 2016. The minicomputer running the algorithm (left) connects to the constant glucose monitor (pink), a battery pack, and an insulin pump (right).</span><span>Credit: Jack Darrell</span></p>
  </figcaption>
 </picture>
</figure><p>The FDA’s approval of the open-source system Tidepool Loop could change things, says Kowalski. The algorithm underlying it was created in 2016 by people living with T1D, and was initially rolled out through online forums, before a version of it was taken through FDA clearance by Tidepool, a non-profit organization in Palo Alto, California.</p><p>Tidepool’s goal in getting Loop through regulation is “to make it more accessible to a wider audience”, says spokesperson Saira Khan-Gallo. “Downloading code and building an app on your phone is not for everyone,” she says. But “the algorithm, the novel features and technology should be available to anyone who is interested”.</p><p>Tidepool and others hoping to roll out open-source algorithms face a big challenge: their products do not stand alone. The algorithms require compatibility with continuous glucose monitors and pumps — made by other companies — thereby demanding a cooperative relationship between manufacturers. Tidepool is yet to announce which device company it will collaborate with to launch Loop.</p><p>Interoperability between different products and algorithms could shake up a marketplace in which individual manufacturers have usually developed proprietary, exclusive software. This could have ramifications beyond T1D treatment, affecting any computerized medical hardware that software developers might attempt to improve.</p><p>Kowalski points to examples in other industries, such as aerospace, in which companies use engines made by others. “They’re plugging [in] different components from different manufacturers to get the best performance,” he says. “People with diabetes should have the opportunity to use the best tools that work the best for them.”</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-02322-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25943886.jpg"><p>As COVID-19 cases rose, so did diabetes — no one knows why</p></a>
 </article><p>Several device manufacturers, however, told <i>Nature</i> that they are wary of relying on third parties to provide crucial pieces of AID systems. For instance, Tandem, a pump manufacturer in San Diego, California, says that if an algorithm runs on a smartphone app and not on the pump itself, phone damage or connectivity problems could be a risk to therapy. And Medtronic, a medical technology company in Watford, UK, has decided to prioritize designing their own complete system rather than interoperable components.</p><p>Nonetheless, Tidepool’s regulatory approval will hopefully ease the way for future standalone algorithms seeking clearance, says Khan-Gallo. She hopes that having more options will incentivize companies to make their devices cross-compatible.</p><p>And where algorithms have led, open-source hardware might follow, says O’Donnell. A team at the University of Otago in New Zealand has run a successful early-stage clinical trial of an open-source insulin pump<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>. The goal is to provide free-of-charge design plans to qualified manufacturers to build pumps for a fraction of the cost of current commercial ones.</p><h2><b>Algorithms ablaze </b></h2><p>When asked whether the open-source community of users still has a part to play, Kowalski says: “The DIY community is always going to be there. I think they’re the ultimate testing ground for what people want and need.”</p><p>Hussain agrees. Participants’ lived experiences of T1D constantly generate ideas for novel features, he says. But more importantly, the network of online forums and highly motivated members has created a rapid and powerful way to test algorithm functions.</p><article data-label="Related">
  <a href="https://www.nature.com/immersive/d42859-021-00002-5/index.html" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02648-9/d41586-023-02648-9_25943900.jpg"><p>Milestones in diabetes</p></a>
 </article><p>Problematic updates are quickly weeded out, and beneficial innovations self-propagate and achieve widespread use in just months. Currently, Hussain says, “The commercial systems don’t have advanced features that the open-source systems allow.”</p><p>Some of these features mean that certain open-source systems come close to regulating blood sugar completely autonomously. Although commercial systems are advancing, none is as near to solving this important problem; they currently use hybrid algorithms that manage insulin dosing most of the time, but they require a lot of manual input. For instance, users must program in meals to ensure that the devices deliver large corrective insulin doses.</p><p>By contrast, many open-source systems do not require meal announcements and come close to being a fully closed loop<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>.</p><p>Several advances in technology made this possible, says Lewis. The apps can analyse long-term changes in glucose control and insulin sensitivity resulting from, say, hormonal changes or illness. They do this by retrospectively comparing glucose levels with insulin doses over days or weeks, to adjust upcoming dosing according to changes in insulin sensitivity. They can also deal with perturbations to blood glucose “without knowing whether it was a meal, adrenaline, stress, excitement — whatever, it doesn’t matter”, says Lewis.</p><p>Whether open-source systems are better than commercial ones “is a big ‘it depends’ question”, says Rayhan Lal, an endocrinologist at Stanford University in California, and Tidepool’s chief medical adviser, who says that he has started more than 3,000 people on open-source AID systems, and uses one himself.</p><p>What matters, Lal says, is that individuals can find what works best for them, in terms of the control they gain or the effort they want to make. Some people might want to tailor their devices; others might prefer the simplicity of a package from a commercial manufacturer.</p><p>The DIY community and industry are not in opposition, says Lewis. She is delighted that a safety feature that she wrote and freely shared was incorporated into a commercial device. The open-source community will stay relevant, she says, as long as it offers users choice. “The vision of where I’d love to get to — whether it’s commercial or DIY — is really, really about the person with diabetes and our safety and our quality of life.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is using malware-like pop-ups in Win11 to get people to ditch Chrome (400 pts)]]></title>
            <link>https://www.theverge.com/2023/8/30/23851902/microsoft-bing-popups-windows-11-malware</link>
            <guid>37321002</guid>
            <pubDate>Wed, 30 Aug 2023 12:29:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/8/30/23851902/microsoft-bing-popups-windows-11-malware">https://www.theverge.com/2023/8/30/23851902/microsoft-bing-popups-windows-11-malware</a>, See on <a href="https://news.ycombinator.com/item?id=37321002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I thought I had malware on my main Windows 11 machine this weekend. There I was minding my own business in Chrome before tabbing back to a game and <em>wham</em> a pop-up appeared asking me to switch my default search engine to Microsoft Bing in Chrome. Stunningly, Microsoft now thinks it’s ok to shove a pop-up in my face above my apps and games just because I dare to use Chrome instead of Microsoft Edge.</p><p>This isn’t a normal notification, either. It didn’t appear in the notification center in Windows 11, nor is it connected to the part of Windows 11 that suggests new features to you. It’s quite literally a rogue executable file that has somehow appeared in c:\windows\temp\mubstemp and is digitally signed by Microsoft.</p><p>“We are aware of these reports and have paused this notification while we investigate and take appropriate action to address this unintended behavior,” says Caitlin Roulston, director of communications, in a statement to <em>The Verge</em>.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="The latest Bing popup appeared above the taskbar on Windows 11." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/376x212/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/384x216/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/415x233/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/480x270/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/540x304/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/640x360/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/750x422/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/828x466/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1080x608/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1200x675/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1440x810/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1920x1080/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2048x1152/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2400x1350/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2400x1350/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883698/bingpopup0.jpeg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>The latest Bing popup appeared above the taskbar on Windows 11.</em></figcaption> <p><cite>Screenshot by Tom Warren / The Verge</cite></p></div></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="This annoying popup even appeared above a game." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/376x212/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/384x216/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/415x233/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/480x270/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/540x304/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/640x360/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/750x422/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/828x466/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1080x608/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1200x675/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1440x810/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/1920x1080/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2048x1152/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2400x1350/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2560x1440/2400x1350/filters:focal(1280x720:1281x721):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24883699/bingpopup1.jpeg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>This annoying popup even appeared above a game.</em></figcaption> <p><cite>Screenshot by Tom Warren / The Verge</cite></p></div></div><p>I have no idea why Microsoft thinks it’s ok to fire off these pop-ups to Windows 11 users in the first place. I wasn’t alone in thinking it was malware, with posts dating back three months showing <a href="https://www.reddit.com/r/computerviruses/comments/149x25h/bgaupsell_what_is_this_bing_popup/">Reddit users trying to figure out</a> why they were seeing the pop-up.</p><p>I’m sure Microsoft is legally covered by the myriad of license agreements that nobody reads, but in reality I never knowingly consented to&nbsp;Microsoft&nbsp;abusing its ability to analyze my PC usage to show me a&nbsp;Bing&nbsp;pop-up just because I use Chrome with Google search.</p><p>This isn’t Microsoft’s first rodeo, either. I’m growing increasingly frustrated by the company’s methods of getting people to switch from Google and Chrome to Bing and Edge. Microsoft has been using a <a href="https://www.theverge.com/2021/12/2/22813733/microsoft-windows-edge-download-chrome-prompts">variety of prompts for years now</a>, with pop-ups appearing inside Chrome, on the Windows taskbar, and elsewhere. Microsoft has even <a href="https://www.theverge.com/21310611/microsoft-edge-browser-forced-update-chromium-editorial">forced people into Edge after a Windows Update</a>, and regularly presents a full-screen message to switch to Bing and Edge after updates.</p><p>Microsoft also started <a href="https://www.theverge.com/2023/6/6/23736289/microsoft-bing-chrome-search-fake-ai-chatbot">taking over Chrome searches in Bing</a> recently to deliver a canned response that looks like it’s generated from Microsoft’s GPT-4-powered chatbot. The fake AI interaction produced a full Bing page to entirely take over the search result for Chrome and convince Windows users to stick with Edge and Bing.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Microsoft even took over Chrome search results in Bing recently." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/376x204/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/384x209/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/415x226/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/480x261/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/540x294/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/640x348/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/750x408/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/828x450/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/1080x587/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/1200x653/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/1440x783/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/1920x1044/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/2048x1114/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/2400x1305/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1920x1044/2400x1305/filters:focal(960x522:961x523):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24707688/chrome___Search___Personal___Microsoft__Edge_6_6_2023_3_52_36_PM.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Microsoft even took over Chrome search results in Bing recently.</em></figcaption> <p><cite>Screenshot by Sean Hollister / The Verge</cite></p></div></div><p>You’ve probably never seen this latest pop-up or even some of the ones in the past, and that’s because Microsoft only experiments with a small number of Windows users before there’s an outcry and the company pivots to try and find another way to nag Windows users. Microsoft even <a href="https://www.theverge.com/2020/2/11/21133372/microsoft-search-chrome-extension-bing-default-search-plans-canceled">had to backtrack on plans</a> to force the Chrome default search to Bing for businesses installing its Office apps.</p><p>You might argue that this is Microsoft’s operating system, or that when using Microsoft’s browser and search engine it’s well within its rights to try and sway people away from Chrome. After all, Google runs similar notifications on its webpages to get people to use Chrome or it’s <a href="https://www.theverge.com/2019/11/23/20946114/google-youtube-premium-subscription-ads-pop-ups-spam-rant">annoying YouTube premium spam</a>. But Microsoft’s behaviors here are totally beyond a simple webpage prompt. I shouldn’t have to be dismissing pop-ups that appear on top of my apps and games, or ones that magically appear after I update my copy of Windows.</p><p>Windows isn’t freeware, it requires a license that almost every consumer ultimately pays for. That could be in the form of the price of a laptop that has a Windows OEM license baked in, or a product key if you built your own PC. Microsoft should respect the fact that people already pay for Windows and don’t want ads shoved down their throats. Windows is an important productivity tool for many people, and shouldn’t be treated like a cheap streaming box loaded with ads.</p><p>I truly hope Microsoft changes its ways here, but this has been going on in several different forms for years now so I’m just counting the days until the next annoying pop-up appears.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hundreds of thousands trafficked to work as online scammers in SE Asia (332 pts)]]></title>
            <link>https://www.ohchr.org/en/press-releases/2023/08/hundreds-thousands-trafficked-work-online-scammers-se-asia-says-un-report</link>
            <guid>37320975</guid>
            <pubDate>Wed, 30 Aug 2023 12:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ohchr.org/en/press-releases/2023/08/hundreds-thousands-trafficked-work-online-scammers-se-asia-says-un-report">https://www.ohchr.org/en/press-releases/2023/08/hundreds-thousands-trafficked-work-online-scammers-se-asia-says-un-report</a>, See on <a href="https://news.ycombinator.com/item?id=37320975">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>GENEVA (29 August 2023)</strong> – Hundreds of thousands of people are being forcibly engaged by organised criminal gangs into online criminality in Southeast Asia - from romance-investment scams and crypto fraud to illegal gambling - a <a href="https://bangkok.ohchr.org/wp-content/uploads/2023/08/ONLINE-SCAM-OPERATIONS-2582023.pdf">report</a> issued today by the UN Human Rights Office shows.</p>

<p>Victims face a range of serious violations and abuses, including threats to their safety and security; and many have been subjected to torture and cruel, inhuman and degrading treatment or punishment, arbitrary detention, sexual violence, forced labour, and other human rights abuses, the report says.</p>

<p>“People who are coerced into working in these scamming operations endure inhumane treatment while being forced to carry out crimes. They are victims. They are not criminals,” said UN High Commissioner for Human Rights Volker Türk.</p>

<p>“In continuing to call for justice for those who have been defrauded through online criminality, we must not forget that this complex phenomenon has two sets of victims.”</p>

<p>The enormity of online scam trafficking in Southeast Asia is difficult to estimate, the reports says, because of the clandestine nature and gaps in the official response. Credible sources indicate that at least 120,000 people across Myanmar may be held in situations where they are forced to carry out online scams, with estimates in Cambodia similarly at around 100,000. Other States in the region, including Lao PDR, the Philippines and Thailand, have also been identified as main countries of destination or transit where at least tens of thousands of people have been involved.</p>

<p>The scam centres generate revenue amounting to billions of US dollars each year.</p>

<p>The COVID-19 pandemic and associated response measures had a drastic impact on illicit activities across the region. Public health measures closed casinos in many countries and in response, casino operators moved operations to less regulated spaces including conflict-affected border areas and Special Economic Zones, as well as to the increasingly lucrative online space, the report says.</p>

<p>Faced with new operational realities, criminal actors increasingly targeted migrants in vulnerable situations – who were stranded in these countries and out of work due to border and business closures – for recruitment into criminal operations, under the pretence of offering them real jobs. As COVID-related shutdowns saw millions of people restricted to their homes, spending more time online, there were more ready targets for online fraud schemes and more people susceptible to fraudulent recruitment.</p>

<p>Most people trafficked into the online scam operations are men, although women and adolescents are also among the victims, the report says. Most are not citizens of the countries in which the trafficking occurs. Many of the victims are well-educated, sometimes coming from professional jobs or with graduate or even post-graduate degrees, computer-literate and multi-lingual. Victims come from across the ASEAN region (from Indonesia, Lao PDR, Malaysia, Myanmar, Philippines, Singapore, Thailand and Vietnam), as well as mainland China, Hong Kong and Taiwan, South Asia, and even further afield from Africa and Latin America.</p>

<p>While some countries in Southeast Asia have put in place legal and policy frameworks relevant to counter trafficking, in some cases they fall short of international standards. In many cases their implementation has failed to respond adequately to the context and sophistication of these online scams, the report says.</p>

<p>Victims of trafficking and other human rights abuse are erroneously identified as criminals or as immigration offenders and, rather than being protected and given access to the rehabilitation and remedy they need, they are subjected to criminal prosecution or immigration penalties, it says.</p>

<p>“All affected States need to summon the political will to strengthen human rights and improve governance and the rule of law, including through serious and sustained efforts to tackle corruption. This must be as much a part of the response to these scams as a robust criminal justice response,” said Türk.</p>

<p>“Only such a holistic approach can break the cycle of impunity and ensure protection and justice for the people who have been so horrifically abused.”</p>

<p>To read the report, click <a href="https://bangkok.ohchr.org/wp-content/uploads/2023/08/ONLINE-SCAM-OPERATIONS-2582023.pdf">here</a></p>

<p>ENDS</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fake town built exclusively for filming TV and movies (117 pts)]]></title>
            <link>https://petapixel.com/2023/08/23/this-giant-fake-town-was-built-exclusively-for-filming-tv-and-movies/</link>
            <guid>37320897</guid>
            <pubDate>Wed, 30 Aug 2023 12:18:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://petapixel.com/2023/08/23/this-giant-fake-town-was-built-exclusively-for-filming-tv-and-movies/">https://petapixel.com/2023/08/23/this-giant-fake-town-was-built-exclusively-for-filming-tv-and-movies/</a>, See on <a href="https://news.ycombinator.com/item?id=37320897">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article data-post-id="699321">
<header>


</header>

<div>
<figure id="attachment_699358" aria-describedby="caption-attachment-699358"><img data-perfmatters-preload="" decoding="async" fetchpriority="high" src="https://petapixel.com/assets/uploads/2023/08/Fake-Town-800x420.jpg" alt="Fake TV town" width="800" height="420" srcset="https://petapixel.com/assets/uploads/2023/08/Fake-Town-800x420.jpg 800w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-320x168.jpg 320w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-1536x806.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-150x79.jpg 150w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-300x157.jpg 300w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-400x209.jpg 400w, https://petapixel.com/assets/uploads/2023/08/Fake-Town-550x288.jpg 550w, https://petapixel.com/assets/uploads/2023/08/Fake-Town.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"><figcaption id="caption-attachment-699358">The fake town in Canada.</figcaption></figure> <p>There is a fake town in Canada built exclusively to accommodate film crews producing television series, movies, and commercials. </p>  <p>Cinematographer Matt Aitia recently gave a tour on his TikTok page of the 23-acre fake town which is the biggest backlot in Canada. A backlot is an outdoor film studio and even small details like the bricks and ATMS are fake. </p> <blockquote cite="https://www.tiktok.com/@mattaitia/video/7260716446521527557" data-video-id="7260716446521527557"> <section> <a target="_blank" title="@mattaitia" href="https://www.tiktok.com/@mattaitia?refer=embed" rel="noopener follow external" data-wpel-link="external">@mattaitia</a> Backlots are crazy <a title="filmtok" target="_blank" href="https://www.tiktok.com/tag/filmtok?refer=embed" rel="noopener follow external" data-wpel-link="external">#filmtok</a> <a title="filmmaking" target="_blank" href="https://www.tiktok.com/tag/filmmaking?refer=embed" rel="noopener follow external" data-wpel-link="external">#filmmaking</a> <a title="behindthescenes" target="_blank" href="https://www.tiktok.com/tag/behindthescenes?refer=embed" rel="noopener follow external" data-wpel-link="external">#behindthescenes</a> <a target="_blank" title="♬ Expensive Shit - Fela Kuti" href="https://www.tiktok.com/music/Expensive-Shit-5000000000683654359?refer=embed" rel="noopener follow external" data-wpel-link="external">♬ Expensive Shit – Fela Kuti</a> </section> </blockquote>  <p><img decoding="async" src="https://petapixel.com/assets/uploads/2023/08/Close-Up_11-800x600.jpg" alt="Fake TV town" width="800" height="600" srcset="https://petapixel.com/assets/uploads/2023/08/Close-Up_11-800x600.jpg 800w, https://petapixel.com/assets/uploads/2023/08/Close-Up_11-320x240.jpg 320w, https://petapixel.com/assets/uploads/2023/08/Close-Up_11-1536x1152.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/Close-Up_11.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"></p> <p>The studio is home to five interior sets as well as boasting a fake gas station, a fake bar and grill, a fake dry cleaners, a fake variety store, and many other fake establishments. However, there is a real diner and barbershop according to Aitia. </p>
<p>The backlot is located in Pickering, Canada, and TV fans might recognize it from Amazon’s <em>Reacher</em>. In a <a href="https://www.tiktok.com/@mattaitia/video/7260962606981123333?embed_source=121355059%2C121351166%2C121331973%2C120811592%2C120810756%3Bnull%3Bembed_masking&amp;refer=embed&amp;referer_url=www.dailymail.co.uk%2Ffemail%2Farticle-12381843%2FFake-town-Canada-gas-station-restaurant-filming-movies.html&amp;referer_video_id=7260716446521527557" data-wpel-link="external" target="_blank" rel="follow external noopener">further video</a>, Aitia explains why a company would go through all that trouble of making such an elaborate set. </p> <p>“Filming on a backlot is usually cheaper than real locations for a few reasons. It can be super costly to shut down a whole street for a film shoot backlots eliminated the need for permits, scheduling, and coordinating with property owners,” Aitia explains. </p> <p>“They also save time as there’s less travel time between locations allowing for more shooting hours each day. They’re also designed for versatility enabling filmmakers to shoot at various locations with logistical and transportational headaches.” </p> <p><img decoding="async" src="https://petapixel.com/assets/uploads/2023/08/4_Backlot-Drone-View-800x600.jpg" alt="Fake TV town" width="800" height="600" srcset="https://petapixel.com/assets/uploads/2023/08/4_Backlot-Drone-View-800x600.jpg 800w, https://petapixel.com/assets/uploads/2023/08/4_Backlot-Drone-View-320x240.jpg 320w, https://petapixel.com/assets/uploads/2023/08/4_Backlot-Drone-View-1536x1152.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/4_Backlot-Drone-View.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"></p> <p><img decoding="async" loading="lazy" src="https://petapixel.com/assets/uploads/2023/08/Close-Up_15-800x600.jpg" alt="Fake TV town" width="800" height="600" srcset="https://petapixel.com/assets/uploads/2023/08/Close-Up_15-800x600.jpg 800w, https://petapixel.com/assets/uploads/2023/08/Close-Up_15-320x240.jpg 320w, https://petapixel.com/assets/uploads/2023/08/Close-Up_15-1536x1152.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/Close-Up_15.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"></p> <p><img decoding="async" loading="lazy" src="https://petapixel.com/assets/uploads/2023/08/Close-Up_1-800x600.jpg" alt="Fake TV town" width="800" height="600" srcset="https://petapixel.com/assets/uploads/2023/08/Close-Up_1-800x600.jpg 800w, https://petapixel.com/assets/uploads/2023/08/Close-Up_1-320x240.jpg 320w, https://petapixel.com/assets/uploads/2023/08/Close-Up_1-1536x1152.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/Close-Up_1.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"></p> <p>The backlot is owned and operated by <a href="http://wfwstudios.com/backlot/" data-wpel-link="external" target="_blank" rel="follow external noopener">William F. White International</a> which has a number of studios across Canada. The huge set is northeast of Toronto and the company describes the town as “easily expandable and can be converted to suit productions of all types, from rustic western settings to bustling New York City streets.” </p>
<p>Viewers of Aitia’s TikTok video compared it to the <em>Truman Show</em>, starring Jim Carey. “Oh so that’s why most towns in movies and TV shows look extraordinarily clean,” writes one person. </p> <p><img decoding="async" loading="lazy" src="https://petapixel.com/assets/uploads/2023/08/Close-Up_7-800x600.jpg" alt="Fake TV town" width="800" height="600" srcset="https://petapixel.com/assets/uploads/2023/08/Close-Up_7-800x600.jpg 800w, https://petapixel.com/assets/uploads/2023/08/Close-Up_7-320x240.jpg 320w, https://petapixel.com/assets/uploads/2023/08/Close-Up_7-1536x1152.jpg 1536w, https://petapixel.com/assets/uploads/2023/08/Close-Up_7.jpg 1600w" sizes="(max-width: 800px) 100vw, 800px"></p> <h2>Could it be Used for Still Photography?</h2> <p>The answer to that question is probably not. But some photographers such as <a href="https://petapixel.com/2016/05/18/interview-gregory-crewdson/" data-wpel-link="internal">Gregory Crewdson</a> who create movie-like sets for atmospheric and unsettling photos might be interested in a space such as Pickering. Crewdson shoots most of his photos near his home in rural Massachusetts. </p> <hr> <p><em><strong>Image credits:</strong> Photos couretsy of WFW Studios.</em></p>  </div>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows 11 will respect default browser for system apps in the European Union (376 pts)]]></title>
            <link>https://blogs.windows.com/windows-insider/2023/08/25/announcing-windows-11-insider-preview-build-23531-dev-channel/</link>
            <guid>37320786</guid>
            <pubDate>Wed, 30 Aug 2023 12:07:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.windows.com/windows-insider/2023/08/25/announcing-windows-11-insider-preview-build-23531-dev-channel/">https://blogs.windows.com/windows-insider/2023/08/25/announcing-windows-11-insider-preview-build-23531-dev-channel/</a>, See on <a href="https://news.ycombinator.com/item?id=37320786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-js="panel-article-header">
					<p><span>
						<span>
							Written By
						</span>
						
					</span>
					<span>
						<span>
							published
						</span>
						<span>
							August 25, 2023
						</span>
					</span></p>
			</div><div data-index="0" data-js="panel" data-type="wysiwyg-with-aside" data-modular-content="" data-modular-content-collection="">
<p>Hello Windows Insiders, today we are releasing <strong>Windows 11 Insider Preview Build 23531 </strong>to the Dev Channel.</p>
<h2><strong>Changes and Improvements</strong></h2>
<h3><strong>[General]</strong></h3>
<ul>
<li><strong>[REMINDER] Insider Preview Build Expiration:</strong> The expiration date for Insider Preview builds flighted to the Dev Channel has been updated to 9/15/2024 starting with Build 23526. Please make sure you are updated to the latest build in the Dev Channel.</li>
<li>In the European Economic Area (EEA), Windows system components use the default browser to open links.</li>
</ul>
<h3><strong>[Search on the Taskbar]</strong></h3>
<ul>
<li>We’re re-introducing the search experience for invoking the search flyout when you hover over the search box gleam. This behavior can be adjusted by right-clicking on the taskbar, choosing “Taskbar settings” and adjusting your preferred search box experience.</li>
</ul>
<h2><strong>Fixes</strong></h2>
<h3><strong>[General]</strong></h3>
<ul>
<li>Fixed an issue which was causing explorer.exe to not work in safe mode.</li>
</ul>
<h3><strong>[File Explorer]</strong></h3>
<ul>
<li>Fixed an issue which could cause explorer.exe to crash when closing File Explorer.</li>
<li>Fixed an issue which was causing File Explorer to crash when navigating to Gallery if you had a large number of images.</li>
<li>Fixed an issue where “Automatically type into the Search Box” wasn’t working when File Explorer was open to Home.</li>
<li>Fixed an issue where if you tried to copy and paste, a file out of a compressed folder and into a OneDrive folder might fail with an error code.</li>
</ul>
<h3><strong>[HDR Backgrounds]</strong></h3>
<ul>
<li>Fixed an issue which could cause HDR wallpapers to appear washed out when HDR was enabled.</li>
</ul>
<h3><strong>[Task Manager]</strong></h3>
<ul>
<li>Fixed an issue where it wasn’t possible to move the Task Manager window with touch or pen.</li>
<li>Fixed an issue where Task Manager was crashing when using the reset to default button in Settings.</li>
<li>Fixed an issue which was causing sporadic crashes when using Task Manager, including when ending tasks.</li>
<li>Fixed an issue where if you did a search and then cleared it, the view would still be filtered even though there was no search anymore.</li>
<li>Fixed an issue where the search icon would become overlapped with the Task Manager text in the title bar.</li>
<li>Did some work to help improve the performance when switching between different pages in Task Manager.</li>
<li>Updated the summary view for the Performance page (that you see when double clicking the graphs in the navigation pane) to make the window smaller.</li>
</ul>
<p><em>NOTE: Some fixes noted here in Insider Preview builds from the Dev Channel may make their way into the servicing updates for the released version of Windows 11.</em></p>
<h2><strong>Known issues</strong></h2>
<h3><strong>[Start menu]</strong></h3>
<ul>
<li>Some apps under All apps on the Start menu, such as PWA apps installed via Microsoft Edge, may incorrectly be labeled as a system component.</li>
</ul>
<h3><strong>[Search on the Taskbar]</strong></h3>
<ul>
<li>Sometimes the tooltip when mousing over the search box does not match the current search highlight.</li>
</ul>
<h3><strong>[Input]</strong></h3>
<ul>
<li><strong>[ADDED 8/28]</strong> <a href="https://www.unicode.org/review/pri435/pri435-background-emoji-candidates.pdf">Unicode Emoji 15 </a>support which began rolling out with <a href="https://blogs.windows.com/windows-insider/2023/06/07/announcing-windows-11-insider-preview-build-23475/">Build 23475</a> and the updated color font format with&nbsp;<a href="https://learn.microsoft.com/typography/opentype/spec/colr#colr-version-1-rendering-algorithm">COLRv1</a> support that began rolling out with Build 23506 no longer appear after updating to <a href="https://blogs.windows.com/windows-insider/2023/08/25/announcing-windows-11-insider-preview-build-23531-dev-channel/">Build 23531</a> due to a bug. This issue will be fixed in a future flight soon.</li>
</ul>
<h3><strong>[Windows Copilot]</strong></h3>
<ul>
<li>You can use Alt + Tab to switch out of Windows Copilot, but not back into it. Windows + C will move focus back to Windows Copilot</li>
<li>When first launching or after refreshing Copilot while using voice access you’ll need to use “Show grid” commands to click in the “Ask me anything” box for the first time.</li>
</ul>
<h2><strong>For developers</strong></h2>
<p>You can download the latest Windows Insider SDK at&nbsp;<a href="https://aka.ms/windowsinsidersdk">aka.ms/windowsinsidersdk</a>.</p>
<p>SDK NuGet packages are now also flighting at <a href="https://www.nuget.org/profiles/WindowsSDK">NuGet Gallery | WindowsSDK</a> which include:</p>
<ul>
<li><a href="https://www.nuget.org/packages/Microsoft.Windows.SDK.NET.Ref/">.NET TFM packages</a> for use in .NET apps as described at <a href="https://aka.ms/windowsinsidersdk">aka.ms/windowsinsidersdk</a></li>
<li><a href="https://www.nuget.org/packages/Microsoft.Windows.SDK.CPP/">C++ packages</a> for Win32 headers and libs per architecture</li>
<li><a href="https://www.nuget.org/packages/Microsoft.Windows.SDK.BuildTools/">BuildTools package</a> when you just need tools like MakeAppx.exe, MakePri.exe, and SignTool.exe</li>
</ul>
<p>These NuGet packages provide more granular access to the SDK and better integration in CI/CD pipelines.</p>
<p><strong>SDK flights are now published for both the Canary and Dev Channels, so be sure to choose the right version for your Insider Channel.</strong></p>
<p>Remember to use&nbsp;<a href="https://learn.microsoft.com/windows/uwp/debug-test-perf/version-adaptive-apps">adaptive code&nbsp;</a>when targeting new APIs to make sure your app runs on all customer machines, particularly when building against the Dev Channel SDK.&nbsp;<a href="https://learn.microsoft.com/uwp/api/windows.foundation.metadata.apiinformation?view=winrt-22621">Feature detection</a>&nbsp;is recommended over OS version checks, as OS version checks are unreliable and will not work as expected in all cases.</p>
<h2><strong>About the Dev Channel</strong></h2>
<p><strong>REMINDER: </strong><a href="https://blogs.windows.com/windows-insider/2023/03/06/whats-coming-for-the-windows-insider-program-in-2023/"><strong>The Dev Channel has been rebooted</strong></a><strong>. Windows Insiders who were in the Dev Channel on 25000 series builds have been moved to the new Canary Channel. Going forward, the Dev Channel will receive 23000 series builds. Insiders who were moved to the Canary Channel and want to move back to the Dev Channel can </strong><a href="https://aka.ms/HowtoCleanInstall"><strong>follow these instructions</strong></a><strong> to do a clean installation of Windows 11 and then re-join the Dev Channel to receive 23000 series builds. </strong></p>
<p>The Dev Channel receives builds that represent long lead work from our engineers&nbsp;<strong>with features and experiences that may never get released</strong>&nbsp;as we try out different concepts and get feedback. It is important to remember that the builds we release to the Dev Channel should not be seen as matched to any specific release of Windows and the features included may change over time, be removed, or replaced in Insider builds or may never be released beyond Windows Insiders to general customers. For more information,&nbsp;<a href="https://blogs.windows.com/windows-insider/2022/02/03/whats-coming-for-the-windows-insider-program-in-2022/">please read this blog post</a>&nbsp;about how we plan to use the Dev Channel to incubate new ideas, work on long lead items, and control the states of individual features.</p>
<p>In some cases, features and experiences may go out to the Canary Channel first before going out to the Dev Channel however the Dev Channel will provide better platform stability. As we get closer to shipping, some features and experiences will also make their way to the Beta Channel when they are ready.</p>
<p><strong>The desktop watermark you see at the lower right corner of your desktop is normal for these pre-release builds.</strong></p>
<h2><strong>Important Insider Links</strong></h2>
<ul>
<li>You can <a href="https://aka.ms/wip-docs"><strong>check out our Windows Insider Program documentation here</strong></a>.</li>
<li>Check out <a href="https://aka.ms/flighthub"><strong>Flight Hub</strong></a> for a complete look at what build is in which Insider channel.</li>
</ul>
<p>Thanks,<br>
Amanda &amp; Brandon</p>

</div></div>]]></description>
        </item>
    </channel>
</rss>