<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 10 Nov 2024 10:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[LLMs have reached a point of diminishing returns (115 pts)]]></title>
            <link>https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</link>
            <guid>42097774</guid>
            <pubDate>Sun, 10 Nov 2024 00:25:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached">https://garymarcus.substack.com/p/confirmed-llms-have-indeed-reached</a>, See on <a href="https://news.ycombinator.com/item?id=42097774">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg" width="1170" height="1227" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1227,&quot;width&quot;:1170,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243099,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23e06827-0a50-4b72-91bb-93da288403d0_1170x1227.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>For years I have been warning that “scaling” — eeking out improvements in AI by adding more data and more compute, without making fundamental architectural changes — would not continue forever. In my most notorious article, in March of 2022, I argued that “</span><a href="https://nautil.us/deep-learning-is-hitting-a-wall-238440/" rel="">deep learning is hitting a wall</a><span>”. Central to the argument was that pure scaling would not solve hallucinations or abstraction; I concluded that “there are serious holes in the scaling argument.” </span></p><p>And I got endless grief for it. Sam Altman implied (without saying my name, but riffing on the images in my then-recent article) I was a “mediocre deep learning skeptic”; Greg Brockman openly mocked the title. Yann LeCun wrote that deep learning wasn’t hitting a wall, and so on. Elon Musk himself made fun of me and the title earlier this year.</p><p><span>The thing is, </span><strong>in the long term, science isn’t majority rule</strong><span>. In the end, the truth generally outs. Alchemy had a good run, but it got replaced by chemistry. The truth is that scaling is running out, and that truth is, at last coming out.</span></p><p>A few days ago, the well-known venture capitalist Marc Andreesen started to spill the beans, saying on a podcast “we're increasing [graphics processing units] at the same rate, we're not getting the intelligent improvements at all out of it” –  which is basically VC-ese for “deep learning is hitting a wall.”</p><p><span>Just a few moments ago, Amir Efrati, editor of the industry trade journal </span><em>The Information</em><span> further confirmed that we have reached a period of diminishing returns, writing on X that “OpenAI's [upcoming] Orion model shows how GPT improvements are slowing down”.  </span></p><p><span>Just as I argued here in April 2024, LLMs </span><a href="https://open.substack.com/pub/garymarcus/p/evidence-that-llms-are-reaching-a?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web" rel="">have reached a point of diminishing returns</a><span>. </span></p><p>§</p><p>The economics are likely to be grim. Sky high valuation of companies like OpenAI and Microsoft are largely based on the notion that LLMs will, with continued scaling, become artificial general intelligence. As I have always warned, that’s just a fantasy. There is no principled solution to hallucinations in systems that traffic only in the statistics of language without explicit representation of facts and explicit tools to reason over those facts.</p><p>LLMs will not disappear, even if improvements diminish, but the economics will likely never make sense: additional training is expensive, the more scaling, the more costly. And, as I have been warning, everyone is landing in more or less the same place, which leaves nobody with a moat. LLMs such as they are, will become a commodity; price wars will keep revenue low. Given the cost of chips, profits will be elusive. When everyone realizes this, the financial bubble may burst quickly; even NVidia might take a hit, when people realize the extent to which its valuation was based on a false premise.</p><p>§</p><p>The sociology here has been perverse too, for a really long time. Many people (especially LeCun, but also a legion of tech influencers who followed his lead) have tried to deplatform me. </p><p>The media has done little to counter the mob psychology; they have mostly listened to people with money, with vested interests, not to scientists. Many of us, including Melanie Mitchell, Subbarao Kambahapati, Emily Bender, Ernie Davis, etc. have been emphasizing for ages that there are principled limits with LLMs. Media (with notable exceptions like Ezra Klein, who gave me a clear platform for skepticism in January 2023) has rarely listened, instead often glorifying the hype of people like Altman and Musk.</p><p>Worse, the US AI policy now, and likely in the next administration, has largely been driven by hype, and the assumption that returns for LLM scaling would not diminish.  And yet here we are at the end of 2024, and even Altman and Andreesen are perhaps starting to see it.</p><p>Meanwhile, precious little investment has been made in other approaches. If LLMs won’t get the US to trustworthy AI, and our adversaries invest in alternative approaches, we could easily be outfoxed. The US has been putting all its AI eggs in the LLM basket, and that may well prove to be an epic, massive mistake.</p><p>§</p><p>In April, when I first saw direct empirical evidence that this moment had come, I wrote (and stand by):</p><blockquote><p>If enthusiasm for GenAI dwindles and market valuations plummet, AI won’t disappear, and LLMs won’t disappear; they will still have their place as tools for statistical approximation.</p><p>But that place may be smaller; it is entirely possible that LLMs on their own will never live up to last year’s wild expectations.</p><p>Reliable, trustworthy AI is surely achievable, but we may need to go back to the drawing board to get there.</p></blockquote><p>I’m glad that the market is finally recognizing that what I’ve been saying is true. Hopefully now we can make real progress.</p><p><em><strong>Gary Marcus</strong><span> has been warning about the foundational limits to traditional neural network approaches since his 2001 book The Algebraic Mind (where he first described hallucinations), and amplified those warnings in Rebooting AI and his most recent book Taming Silicon Valley.</span></em><span> </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grim Fandango (216 pts)]]></title>
            <link>https://www.filfre.net/2024/11/grim-fandango/</link>
            <guid>42097261</guid>
            <pubDate>Sat, 09 Nov 2024 22:17:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.filfre.net/2024/11/grim-fandango/">https://www.filfre.net/2024/11/grim-fandango/</a>, See on <a href="https://news.ycombinator.com/item?id=42097261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
														<p><a href="https://www.filfre.net/2024/11/grim-fandango/10591938-grim-fandango-windows-front-cover/" rel="attachment wp-att-6154"><img fetchpriority="high" decoding="async" src="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg" alt="" width="377" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover-251x300.jpg 251w, https://www.filfre.net/wp-content/uploads/2024/10/10591938-grim-fandango-windows-front-cover.jpg 670w" sizes="(max-width: 377px) 100vw, 377px"></a></p>
<blockquote><p>My one big regret was <a href="https://www.filfre.net/2024/01/televising-the-revolution">the PlayStation version [of Broken Sword]</a>. No one thought it would sell, so we kept it like the PC version. In hindsight, I think if we had introduced direct control in this game, it would have been enormous.</p>
<p>— Charles Cecil of Revolution Software, speaking from the Department of Be Careful What You Wish For</p>
</blockquote>
<hr>

<p>One day in June of 1995, Tim Schafer came to work at LucasArts and realized that, for the first time in a long time, he didn’t have anything pressing to do. <a href="https://www.filfre.net/2021/07/full-throttle"><em>Full Throttle</em></a>, his biker movie of an adventure game, had been released several weeks before. Now, all of the initial crush of interviews and marketing logistics was behind him. A mountain had been climbed. So, as game designers do, he started to think about what his next Everest should be.</p>
<p>Schafer has told in some detail how he came up with the core ideas behind <em>Grim Fandango</em> over the course of that summer of 1995.</p>
<blockquote><p>The truth is, I had part of the Fandango idea before I did Full Throttle. I wanted to do a game that would feature those little papier-mâché folk-art skeletons from Mexico. I was looking at their simple shapes and how the bones were just painted on the outside, and I thought, “Texture maps! 3D! The bones will be on the outside! It’ll look cool!”</p>
<p>But then I was stuck. I had these skeletons walking around the Land of the Dead. So what? What did they do? Where were they going? What did they want? Who’s the main character? Who’s the villain? The mythology said that the dead walk the dark plane of the underworld known as Mictlān for four years, after which their souls arrive at the ninth plane, the land of eternal rest. Sounds pretty “questy” to me. There you have it: a game.</p>
<p>“Not cool enough,” said Peter Tscale, my lead artist. “A guy walking in a supernatural world? What’s he doing? Supernatural things? It just sounds boring to me.”</p>
<p>So, I revamped the story. Adventure games are all fantasies really, so I had to ask myself, “Who would people want to be in a game? What would people want to do?” And in the Land of the Dead, who would people rather be than Death himself? Being the Grim Reaper is just as cool as being a biker, I decided. And what does the Grim Reaper do? He picks up people who have died and carts them over from the other world. Just like a driver of a taxi or limo.</p>
<p>Okay, so that’s Manny Calavera, our main character. But who’s the bad guy? What’s the plot? I had just seen <a href="https://www.imdb.com/title/tt0071315">Chinatown</a>, and I really liked the whole water-supply/real-estate scam that Noah Cross had going there, so of course I tried to rip that off and have Manny be a real-estate salesman who got caught up in a real-estate scandal. Then he was just like the guys in <a href="https://www.imdb.com/title/tt0104348/">Glengarry Glen Ross</a>, always looking for the good leads. But why would Hector Lemans, my villain, want real estate? Why would anyone? They’re dead! They’re only souls. What do souls in the Land of the Dead want?</p>
<p>They want to get out! They want safe passage out, just like in <a href="https://www.imdb.com/title/tt0034583/">Casablanca</a>! The Land of the Dead is a transitory place, and everybody’s waiting around for their travel papers. So Manny is a travel agent, selling tickets on the big train out of town, and Hector’s stealing the tickets…</p></blockquote>
<div id="attachment_6156"><p><a href="https://www.filfre.net/2024/11/grim-fandango/glottis/" rel="attachment wp-att-6156"><img decoding="async" aria-describedby="caption-attachment-6156" src="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/glottis-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/glottis.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6156">The missing link between <em>Full Throttle</em> and <em>Grim Fandango</em> is Manny’s chauffeur and mechanic Glottis, a literal speed demon.</p></div>
<p>This, then, became the elevator pitch for <em>Grim Fandango</em>. Begin with the rich folklore surrounding Mexico’s Day of the Dead, a holiday celebrated each year just after Halloween, which combines European Christian myths about death and the afterlife with the older, indigenous ones that still haunt the Aztec ruins of Teopanzolco. Then combine it with classic film noir to wind up with Raymond Chandler in a Latino afterlife. It was nothing if not a strikingly original idea for an adventure game. But there was also one more, almost equally original part of it: to do it in 3D.</p>
<p>To hear Tim Schafer tell the story, the move away from LucasArts’s traditional pixel art and into the realm of points, polygons, and textures was motivated by his desire to deliver a more cinematic experience. By no means does this claim lack credibility; as you can gather by reading what he wrote above, Schafer was and is a passionate film buff, who tends to resort to talking in movie titles when other forms of communication fail him. The environments in previous LucasArts adventure games — even the self-consciously cinematic <em>Full Throttle</em> — could only be shown from the angle the pixel artists had chosen to drawn them from. In this sense, they were like a theatrical play, or a <em>really</em> old movie, from the time before Orson Welles emancipated his camera and let it begin to roam freely through his sets in <a href="https://www.imdb.com/title/tt0033467/"><em>Citizen Kane</em></a>. By using 3D, Schafer could become the Orson Welles of adventure games; he would be able to deliver dramatic angles and closeups as the player’s avatar moved about, would be able to put the player <em>in</em> his world rather than forever forcing her to look down on it from on-high. This is the story he still tells today, and there’s no reason to believe it isn’t true enough, as far as it goes.</p>
<p>Nevertheless, it’s only half of the full story. The other half is a messier, less idealistic tale of process and practical economics.</p>
<p>Reckoned in their cost of production per hour of play time delivered, adventure games stood apart from any other genre in their industry, and not in a good way. Building games entirely out of bespoke, single-use puzzles and assets was <em>expensive</em> in contrast to the more process-intensive genres. As time went on and gamers demanded ever bigger, prettier adventures, in higher resolutions with more colors, this became more and more of a problem. Already in 1995, when adventure games were still selling very well, the production costs that were seemingly inherent to the genre were a cause for concern. And the following year, when the genre <a href="https://www.filfre.net/2022/06/toonstruck-or-a-case-study-in-the-death-of-adventure-games">failed to produce</a> a single million-plus-selling breakout hit for the first time in half a decade, they began to look like an existential threat. At that point, LucasArts’s decision to address the issue proactively in <em>Grim Fandango </em>by switching from pixel art to 3D suddenly seemed a very wise move indeed. For a handful of Silicon Graphics workstations running 3D-modelling software could churn out images far more quickly than an army of pixel artists, at a fraction of the cost per image. If the graphics that resulted lacked some of the quirky, hand-drawn, cartoon-like personality that had marked LucasArts’s earlier adventure games, they made up for that by virtue of their flexibility: a scene could be shown from a different angle just by changing a few parameters instead of having to redraw it from scratch. This really did raise the prospect of making the more immersive games that Tim Schafer desired. But from a bean counter’s point of view, the best thing about it was the cost savings.</p>
<p>And there was one more advantage as well, one that began to seem ever more important as time went on and the market for adventure games running on personal computers continued to soften. Immersive 3D was more or less the default setting of the Sony PlayStation, which had <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-2-playstation-for-the-win">come roaring out of Japan</a> in 1995 to seize the title of the most successful games console of the twentieth century just before the curtain fell on that epoch. In addition to its 3D hardware, the PlayStation sported a CD drive, memory cards for saving state, and a slightly older typical user than the likes of Nintendo and Sega. And yet, although a number of publishers ported their 2D computer-born adventure games to the PlayStation, they never felt entirely at home there, having been designed for a mouse rather than a game controller.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_1');"><sup id="footnote_plugin_tooltip_6147_1_1">[1]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_1">A mouse was available as an accessory for the PlayStation, but it was never very popular.</span></span> A 3D adventure game with a controller-friendly interface might be a very different proposition. If it played its cards right, it would open the door to an installed base of customers five to ten times the size of the extant market for games on personal computers.</p>
<div id="attachment_6149"><p><a href="https://www.filfre.net/2024/11/grim-fandango/manny/" rel="attachment wp-att-6149"><img decoding="async" aria-describedby="caption-attachment-6149" src="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/manny-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/manny.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6149">Working with 3D graphics in the late 1990s required some clever sleight of hand if they weren’t to end up looking terrible. <em>Grim Fandango’</em>s masterstroke was to make all of its characters — like the protagonist Manny Calavera, whom you see above — mere skeletons, whose faces are literally painted onto their skulls. (The characters are shown to speak by manipulating the <a href="https://www.filfre.net/2019/01/life-off-the-grid-part-1-making-ultima-underworld">texture maps</a> that represent their faces, not by manipulating the underlying 3D models themselves.) This approach gave the game a look reminiscent of another of its cinematic inspirations, Tim Burton’s <a href="https://www.imdb.com/title/tt0107688/"><em>The Nightmare Before Christmas</em></a>, whilst conveniently avoiding all of the complications of trying to render pliant flesh. A win-win, as they say. Or, as Tim Schafer said: “Instead of fighting the tech limitations of 3D, you have to embrace them and turn them into a style.”</p></div>
<p>But I’m afraid I’ve gotten slightly ahead of myself. This constellation of ideas, affordances, problems, and solutions was still in a nascent form in November of 1995, when LucasArts hired a young programmer fresh out of university by the name of Bret Mogilefsky. Mogilefsky was a known quantity already, having worked at LucasArts as a tester on and off while he was earning his high-school and university diplomas. Now, he was entrusted with the far more high-profile task of making SCUMM, LucasArts’s venerable adventure engine, safe for 3D.</p>
<p>After struggling for a few months, he concluded that this latest paradigm shift was just too extreme for an engine that had been <a href="https://www.filfre.net/2015/07/a-new-force-in-games-part-3-scumm">created on a Commodore 64</a> circa 1986 and ported and patched from there. He would have to tear SCUMM down so far in order to add 3D functionality that it would be easier and cleaner simply to make a new engine from scratch. He told his superiors this, and they gave him permission to do so — albeit suspecting all the while, Mogilefsky is convinced, that he would eventually realize that game engines are easier envisioned than implemented and come crawling back to SCUMM. By no means was he the first bright spark at LucasArts who thought he could reinvent the adventuring wheel.</p>
<p>But he did prove the first one to call his bosses’ bluff. The engine that he called GrimE (“<em>Grim</em> Engine,” but pronounced like the synonym for “dirt”) used a mixture of pre-rendered and real-time-rendered 3D. The sets in which Manny and his friends and enemies played out their dramas would be the former; the aforementioned actors themselves would be the latter. GrimE was a piebald beast in another sense as well: that of cheerfully appropriating whatever useful code Mogilefsky happened to find lying around the house at LucasArts, most notably from the first-person shooter <a href="https://www.filfre.net/2024/04/jedi-knight-plus-notes-on-an-expanded-universe"><em>Jedi Knight</em></a>.</p>
<p>Like SCUMM before it, GrimE provided relatively non-technical designers like Tim Schafer with a high-level scripting language that they could use themselves to code all of the mechanics of plot and puzzles. Mogilefsky adapted for this task Lua, a new, still fairly obscure programming language out of Brazil. It was an inspired choice. Elegant, learnable, and yet infinitely and easily extendible, Lua has gone on to become a staple language of modern game development, to be found today in such places as the wildly popular Roblox platform.</p>
<p>The most frustrating aspects of GrimE from a development perspective all clustered around the spots where its two approaches to 3D graphics rubbed against one another, producing a good deal of friction in the process. If, for example, Manny was to drink a glass of whiskey, the pre-rendered version of the glass that was part of the background set had to be artfully swapped with its real-time-rendered incarnation as soon as Manny began to interact with it. Getting such actions to look seamless absorbed vastly more time and energy than anyone had expected it to.</p>
<p>In fact, if the bean counters had been asked to pass judgment, they would have had a hard time labeling GrimE a success at all under their metrics. <em>Grim Fandango</em> was in active development for almost three full years, and may have ended up costing as much as $3 million. This was at least two and a half times as much as <em>Full Throttle</em> had cost, and placed it in the same ballpark as <a href="https://www.filfre.net/2024/04/the-curse-of-monkey-island"><em>The Curse of Monkey Island</em></a>, LucasArts’s last and most audiovisually lavish SCUMM adventure, which was released a year before <em>Grim Fandango</em>. Further, despite employing a distinctly console-like control scheme in lieu of pointing and clicking with the mouse, <em>Grim Fandango</em> would never make it to the PlayStation; GrimE ended up being just too demanding to be made to work on such limited hardware.<span><a role="button" tabindex="0" onclick="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');" onkeypress="footnote_moveToReference_6147_1('footnote_plugin_reference_6147_1_2');"><sup id="footnote_plugin_tooltip_6147_1_2">[2]</sup></a><span id="footnote_plugin_tooltip_text_6147_1_2"><em>Escape from Monkey Island</em>, the only other game ever made using GrimE, was ported to the more capable PlayStation 2 in 2001.</span></span></p>
<p>All that aside, though, the new engine remained an impressive technical feat, and did succeed in realizing most of Tim Schafer’s aesthetic goals for it. Even the cost savings it apparently failed to deliver come with some mitigating factors. Making the first game with a new engine is always more expensive than making the ones that follow; there was no reason to conclude that GrimE couldn’t deliver real cost savings on LucasArts’s <em>next</em> adventure game. Then, too, for all that <em>Grim Fandango</em> wound up costing two and a half times as much as <em>Full Throttle</em>, it was also well over two and a half times as long as that game.</p>
<p>“Game production schedules are like flying jumbo jets,” says Tim Schafer. “It’s very intense at the takeoff and landing, but in the middle there’s this long lull.” The landing is the time of crunch, of course, and the crunch on <em>Grim Fandango</em> was protracted and brutal even by the industry’s usual standards, stretching out for months and months of sixteen- and eighteen-hour days. For by the beginning of 1998, the game was way behind schedule and way over budget, facing a marketplace that was growing more and more unkind to the adventure genre in general. This was not a combination to instill patience in the LucasArts executive suite. Schafer’s team did get the game done by the autumn of 1998, as they had been ordered to do in no uncertain terms, but only at a huge cost to their psychological and even physical health.</p>
<p>Bret Mogilefsky remembers coming to Schafer at one point to tell him that he just didn’t think he could go on like this, that he simply had to have a break. He was met with no sympathy whatsoever. To be fair, he probably shouldn’t have expected any. Crunch was considered par for the course in the industry during this era, and LucasArts was among the worst of its practitioners. Long hours spent toiling for ridiculously low wages — Mogilefsky was hired to be the key technical cog in this multi-million-dollar project for a salary of about $30,000 per year — were considered the price you paid for the privilege of working at The <em>Star Wars</em> Company.</p>
<p>Even setting aside the personal toll it took on the people who worked there, crunch did nothing positive for the games themselves. As we’ll see, <em>Grim Fandango</em> shows the scars of crunch most obviously in its dodgy puzzle design. Good puzzles result from a methodical, iterative process of testing and carefully considering the resulting feedback. <em>Grim Fandango</em> did not benefit from such a process, and this lack is all too plainly evident.</p>
<p>But before I continue making some of you very, very mad at me, let me take some time to note the strengths of <em>Grim Fandango</em>, which are every bit as real as its weaknesses. Indeed, if I squint just right, so that my eyes <em>only</em> take in its strengths, I have no problem understanding why it’s to be found on so many lists of “The Best Adventure Games Ever,” sometimes even at the very top.</p>
<p>There’s no denying the stuff that <em>Grim Fandango</em> does well. Its visual aesthetic, which I can best describe as 1930s Art Deco meets Mexican folk art meets 1940s gangster flick, is unforgettable. And it’s married to a script that positively crackles with wit and pathos. Our hero Manny is the rare adventure-game character who can be said to go through an actual character arc, who grows and evolves over the course of his story. The driving force behind the plot is his love for a woman named Meche. But his love isn’t the puppy love that Guybrush Threepwood has for Elaine in the <a href="https://www.filfre.net/2017/03/monkey-island-or-how-ron-gilbert-made-an-adventure-game-that-didnt-suck"><em>Monkey Island</em></a> games; the relationship is more nuanced, more adult, more <em>complicated</em>, and its ultimate resolution is all the more moving for that.</p>
<div id="attachment_6150"><p><a href="https://www.filfre.net/2024/11/grim-fandango/sprout/" rel="attachment wp-att-6150"><img decoding="async" aria-describedby="caption-attachment-6150" src="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/sprout-300x225.webp 300w, https://www.filfre.net/wp-content/uploads/2024/10/sprout.webp 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6150">How do you create real stakes in a story where everyone is already dead? The Land of the Death’s equivalent of death is “sprouting,” in which a character is turned into a bunch of flowers and forced to live another life in that form. Why shouldn’t the dead fear life as much as the living fear death?</p></div>
<p>Tim Schafer did not grow up with the Latino traditions that are such an inextricable part of <em>Grim Fandango</em>. Yet the game never feels like the exercise in clueless or condescending cultural tourism it might easily have become. On the contrary, the setting feels full-bodied, lived-in, natural. The cause is greatly aided by a stellar cast of voice actors with just the right accents. The Hollywood veteran Tony Plana, who plays Manny, is particularly good, teasing out exactly the right blend of world-weary cynicism and tarnished romanticism. And Maria Canalas, who plays Meche, is equally perfect in her role. The non-verbal soundtrack by Peter McConnell is likewise superb, a mixture of mariachi music and cool jazz that shouldn’t work but does. Sometimes it soars to the forefront, but more often it tinkles away in the background, setting the mood. You’d only notice it if it was gone — but trust me, then you would <em>really</em> notice.</p>
<p>This is a <em>big</em> game as well as a striking and stylish one — in fact, by most reckonings the biggest adventure that LucasArts ever made. Each of its four acts, which neatly correspond to the four years that the average soul must spend wandering the underworld before going to his or her final rest, is almost big enough to be a self-contained game in its own right. Over the course of <em>Grim Fandango</em>, Manny goes from being a down-on-his-luck Grim Reaper cum travel agent to a nightlife impresario, from the captain of an ocean liner to a prisoner laboring in an underwater mine. The story does arguably peak too early; the second act, an extended homage to <em>Casablanca</em> with Manny in the role of Humphrey Bogart, is so beautifully realized that much of what follows is slightly diminished by the comparison. Be that as it may, though, it doesn’t mean any of what follows is bad.</p>
<div id="attachment_6155"><p><a href="https://www.filfre.net/2024/11/grim-fandango/rubacava/" rel="attachment wp-att-6155"><img decoding="async" aria-describedby="caption-attachment-6155" src="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/rubacava-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/rubacava.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6155">The jump cut to Manny’s new life as a bar owner in the port city of Rubacava at the beginning of the second act is to my mind the most breathtaking moment of the game, the one where you first realize how expansive its scope and ambition really are.</p></div>
<p>All told, then, I have no real beef with anyone who chooses to label <em>Grim Fandango</em> an aesthetic masterpiece. If there was an annual award for style in adventure games, this game would have won it easily in 1998, just as Tim Schafer’s <em>Full Throttle</em> would have taken the prize for 1995. Sadly, though, it seems to me that the weaknesses of both games are also the same. In both of their cases, once I move beyond the aesthetics and the storytelling and turn to the gameplay, some of the air starts to leak out of the balloon.</p>
<p>The interactive aspects of <em>Grim Fandango</em> — you know, all that stuff that actually makes it a game — are dogged by two overarching sets of problems. The first is all too typical for the adventure genre: overly convoluted, often nonsensical puzzle design. Tim Schafer was always more intrinsically interested in the worlds, characters, and stories he dreamed up than he was in puzzles. This is fair enough on the face of it; he is very, very good at those things, after all. But it does mean that he needs a capable support network to ensure that his games play as well as they look and read. He had that support for 1993’s <a href="https://www.filfre.net/2019/06/day-of-the-tentacle"><em>Day of the Tentacle</em></a>, largely in the person of his co-designer Dave Grossman; the result was one of the best adventure games LucasArts ever made, a perfect combination of inspired fiction with an equally inspired puzzle framework. Unfortunately, he was left to make <em>Full Throttle</em> on his own, and it showed. Ditto <em>Grim Fandango</em>. For all that he loved movies, the auteur model was not a great fit for Tim Schafer the game designer.</p>
<p><em>Grim Fandango</em> seldom gives you a clear idea of what it is you’re even trying to accomplish. Compare this with <em>The Curse of Monkey Island</em>, the LucasArts adventure just before this one, a game which seemed at the time to herald a renaissance in the studio’s puzzle designs. There, you’re always provided with an explicit set of goals, usually in the form of a literal shopping list. Thus even when the mechanics of the puzzles themselves push the boundaries of real-world logic, you at least have a pretty good sense of where you should be focusing your efforts. Here, you’re mostly left to guess what Tim Schafer would like to have happen to Manny next. You stumble around trying to shake something loose, trying to figure out what you can do and then doing it just because you can. By no means is it lost on me that this sense of confusion arises to a large extent because <em>Grim Fandango</em> is such a character-driven story, one which eschews the mechanistic tic-tac-toe of other adventure-game plots. But recognizing this irony doesn’t make it any less frustrating when you’re wandering around with no clue what the story wants from you.</p>
<p>Compounding the frustrations of the puzzles are the frustrations of the interface. You don’t use the mouse at all; everything is done with the numeric keypad, or, if you’re lucky enough to have one, a console-style controller. (At the time <em>Grim Fandango</em> was released, virtually no one playing games on computers did.) <em>Grim Fandango’</em>s mode of navigation is most reminiscent of the <a href="https://www.filfre.net/2023/11/putting-the-j-in-the-rpg-part-1-dorakue">console-based JRPGs</a> of its era, such as the hugely popular <a href="https://www.filfre.net/2023/12/putting-the-j-in-the-rpg-part-3-playing-final-fantasy-vii-or-old-man-yells-at-jrpg"><em>Final Fantasy VII</em></a>, which sold over 10 million copies on the PlayStation during the late 1990s. Yet in practice it’s far more irritating, because you have to interact with the environment here on a much more granular level. LucasArts themselves referred to their method of steering Manny about as a “tank” interface, a descriptor which turns out to be all too descriptive. It really does feel like you’re driving a bulky, none too agile vehicle through an obstacle course of scenery.</p>
<div id="attachment_6151"><p><a href="https://www.filfre.net/2024/11/grim-fandango/angle/" rel="attachment wp-att-6151"><img decoding="async" aria-describedby="caption-attachment-6151" src="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/angle-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/angle.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6151">Make no mistake: the 3D engine makes possible some truly striking views. But too often the designers prioritize visual aesthetics over playability.</p></div>
<p>In the final reckoning, then, an approach that is fine in a JRPG makes just about every aspect of an old-school, puzzle-solving adventure game — which is what <em>Grim Fandango</em> remains in form and spirit when you strip all of the details of its implementation away — more awkward and less fun. Instead of having hotspots in the environment that light up when you pass a mouse cursor over them, as you do in a SCUMM adventure, you have to watch Manny’s head carefully as you drive him around; when it turns to look in a certain direction, that means there’s something he can interact with there. Needless to say, it’s all too easy to miss a turn of his head, and thereby to miss something vital to your progress through the game.</p>
<div id="attachment_6152"><p><a href="https://www.filfre.net/2024/11/grim-fandango/inventory/" rel="attachment wp-att-6152"><img decoding="async" aria-describedby="caption-attachment-6152" src="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/inventory-300x225.png 300w, https://www.filfre.net/wp-content/uploads/2024/10/inventory.png 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6152">The inventory system is also fairly excruciating. Instead of being able to bring up a screen showing all of the items Manny is carrying, you have to cycle through them one by one by punching a key or controller button over and over, listening to him drone out their descriptions over and over as you do so. This approach precludes using one inventory object on another one, cutting off a whole avenue of puzzle design.</p></div>
<p>Now, the apologists among you — and this game does have an inordinate number of them — might respond to these complaints of mine by making reference to the old cliché that, for every door that is closed in life (and presumably in games as well), another one is opened. And in theory, the new engine really does open a door to new types of puzzles that are more tactile and embodied, that make you feel more a part of the game’s world. To Tim Schafer’s credit, he does try to include these sorts of puzzles in quite a few places. To our detriment, though, they turn out to be the worst puzzles in the game, relying on finicky positioning and timing and giving no useful feedback when you get those things slightly wrong.</p>
<p>But even when <em>Grim Fandango</em> presents puzzles that could easily have been implemented in SCUMM, they’re made way more annoying than they ought to be by the engine and interface. When you’re reduced to that final adventurer’s gambit of just trying everything on everything, as you most assuredly will be from time to time here, the exercise takes many times longer than it would using SCUMM, what with having to laboriously drive Manny about from place to place.</p>
<p>Taken as a game rather than the movie it often seems more interested in being, <em>Grim Fandango</em> boils down to a lumpy stew of overthought and thoughtlessness. In the former category, there’s an unpleasant ideological quality to its approach, with its prioritization of some hazy ethic of 3D-powered “immersion” and its insistence that no visible interface elements whatsoever can appear onscreen, even when these choices actively damage the player’s experience. This is where Sid Meier can <a href="https://www.filfre.net/2017/03/whats-the-matter-with-covert-action">helpfully step in to remind us</a> that it is the player who is meant to be having the fun in a game, not the designer.</p>
<p>The thoughtlessness comes in the lack of consideration of what <em>kind</em> of game <em>Grim Fandango</em> is meant to be. Like all big-tent gaming genres, the adventure genre subsumes a lot of different styles of game with different priorities. Some adventures are primarily about exploration and puzzle solving. And that’s fine, although one does hope that those games execute their puzzles better than this one does. But <em>Grim Fandango</em> is not primarily about its puzzles; it wants to take you on a ride, to sweep you along on the wings of a compelling story. And boy, does it have a compelling story to share with you. For this reason, it would be best served by streamlined puzzles that don’t get too much in the way of your progress. The ones we have, however, are not only frustrating in themselves but murder on the story’s pacing, undermining what ought to be <em>Grim Fandango’</em>s greatest strengths. A game like this one that is best enjoyed with a walkthrough open on the desk beside it is, in this critic’s view at least, a broken game by definition.</p>
<p>As with so many near-miss games, the really frustrating thing about <em>Grim Fandango</em> is that the worst of its problems could so easily have been fixed with just a bit more testing, a bit more time, and a few more people who were empowered to push back against Tim Schafer’s more dogmatic tendencies. For the 2015 remastered version of the game, Schafer did grudgingly agree to include an alternative point-and-click interface that is more like that of a SCUMM adventure. The results verge on the transformational. By no means does the addition of a mouse cursor remedy all of the infelicities of the puzzle design, but it does make battering your way through them considerably less painful. If my less-than-systematic investigations on YouTube are anything to go by, this so-old-it’s-new-again interface has become by far the most common way to play the game today.</p>
<div id="attachment_6153"><p><a href="https://www.filfre.net/2024/11/grim-fandango/remaster/" rel="attachment wp-att-6153"><img decoding="async" aria-describedby="caption-attachment-6153" src="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg" alt="" width="600" height="450" srcset="https://www.filfre.net/wp-content/uploads/2024/10/remaster-300x225.jpg 300w, https://www.filfre.net/wp-content/uploads/2024/10/remaster.jpg 640w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-6153">The <em>Grim Fandango</em> remaster. Note the mouse cursor. The new interface is reportedly implemented entirely in in-engine Lua scripts rather than requiring any re-programming of the GrimE engine itself. This means that it would have been perfectly possible to include as an option in the original release.</p></div>
<p>In other places, the fixes could have been even simpler than revamping the interface. A shocking number of puzzles could have been converted from infuriating to delightful by nothing more than an extra line or two of dialog from Manny or one of the other characters. As it is, too many of the verbal nudges that do exist are too obscure by half and are given only once in passing, as part of conversations that can never be repeated. Hints for Part Four are to be found only in Part One; I defy even an elephant to remember them when the time comes to apply them. All told,<em> Grim Fandango</em> has the distinct odor of a game that no one other than those who were too close to it to see it clearly ever really tried to play before it was put in a box and shoved out the door. There was a time when seeking the feedback of outsiders was a standard part of LucasArts’s adventure-development loop. Alas, that era was long passed by the time of <em>Grim Fandango</em>.</p>
<p>Nonetheless, <em>Grim Fandango</em> was accorded a fairly rapturous reception in the gaming press when it was released in the last week of October in 1998, just in time for Halloween and the Mexican Day of the Dead which follows it on November 1. Its story, characters, and setting were justifiably praised, while the deficiencies of its interface and puzzle design were more often than not relegated to a paragraph or two near the end of the review. This is surprising, but not inexplicable. There was a certain sadness in the trade press — almost a collective guilt — about the diminished prospects of the adventure game in these latter years of the decade. Meanwhile LucasArts was still the beneficiary of a tremendous amount of goodwill, thanks to the many classics they had served up during those earlier, better years for the genre as a whole. <em>Grim Fandango</em> was held up as a sort of standard bearer for the embattled graphic adventure, the ideal mix of tradition and innovation to serve as proof that the genre was still relevant in a <a href="https://www.filfre.net/2023/04/the-next-generation-in-graphics-part-1-three-dimensions-in-software-or-quake-and-its-discontents">post-<em>Quake</em></a>, <a href="https://www.filfre.net/2024/07/starcraft-a-history-in-two-acts">post-<em>Starcraft</em></a> world.</p>
<p>For many years, the standard narrative had it that the unwashed masses of gamers utterly failed to respond to the magazines’ evangelism, that <em>Grim Fandango</em> became an abject failure in the marketplace. In more recent years, Tim Schafer has muddied those waters somewhat by claiming that the game actually sold close to half a million copies. I rather suspect that the truth is somewhere between these two extremes. Sales of a quarter of a million certainly don’t strike me as unreasonable once foreign markets are factored into the equation. Such a figure would have been enough to keep <em>Grim Fandango</em> from losing much if any money, but would have provided LucasArts with little motivation to make any more such boldly original adventure games. And indeed, LucasArts would release only one more adventure game of any stripe in their history. It would use the GrimE engine, but it would otherwise play it about as safe as it possibly could, by being yet another sequel to the venerable but beloved <em>Secret of Monkey Island</em>.</p>
<p>As I was at pains to note earlier, I do see what causes some people to rate <em>Grim Fandango</em> so highly, and I definitely don’t think any less of them for doing so. For my part, though, I’m something of a stickler on some points. To my mind, interactivity is the very quality that separates games from other forms of media, making it hard for me to pronounce a game “good” that botches it. I’ve learned to be deeply suspicious of games whose most committed fans want to talk about everything other than that which you the player actually <em>do</em> in them. The same applies when a game’s creators display the same tendency. Listening to the developers’ commentary tracks in the remastered edition of <em>Grim Fandango </em>(who would have imagined in 1998 that games would someday come with commentary tracks?), I was shocked by how little talk there was about the gameplay. It was all lighting and dialog beats and soundtrack stabs and Z-buffers instead — all of which is really, really important in its place, but none of which can yield a great game on its own. Tellingly, when the subject of puzzle design did come up, it always seemed to be in an off-hand, borderline dismissive way. “I don’t know how players are supposed to figure out this puzzle,” says Tim Schafer outright at one point. Such a statement from your lead designer is never a good sign.</p>
<p>But I won’t belabor the issue any further. Suffice to say that <em>Grim Fandango</em> is doomed to remain a promising might-have-been rather than a classic in my book. As a story and a world, it’s kind of amazing. It’s just a shame that the gameplay part of this game isn’t equally inspired.</p>
<hr>
<p><code> </code><br>
<strong>Did you enjoy this article? If so, please think about pitching in to help me make many more like it. You can pledge any amount you like.</strong></p>
<p><a href="https://www.patreon.com/DigitalAntiquarian" rel="attachment wp-att-5598"><img decoding="async" src="https://www.filfre.net/wp-content/uploads/2023/04/Patreon-300x133-1.png" alt="" width="300" height="133"></a></p>
<hr>

<p><strong>Sources:</strong> The book <em>Grim Fandango: Prima’s Official Strategy Guide</em> by Jo Ashburn. <em>Retro Gamer</em> 31 and 92; <em>Computer Gaming World</em> of November 1997, May 1998, and February 1999; <em>Ultimate PC</em> of August 1998. Plus the commentary track from the 2015 <em>Grim Fandango</em> remaster.</p>
<p>Online sources include&nbsp;<em>The International House of Mojo’</em>s <a href="https://mixnmojo.com/features/sitefeatures/LucasArts-Secret-History-13-Grim-Fandango/1">pages on the game</a>, the self-explanatory <a href="https://grimfandango.network/"><em>Grim Fandango Network</em></a>, <em>Gamespot’</em>s <a href="https://web.archive.org/web/20100201172300/http://www.gamespot.com/pc/adventure/grimfandango/review.html">vintage review of the game</a>, and Daniel Albu’s <a href="https://www.youtube.com/watch?v=_qwAzIYaGUI">YouTube conversation with Bret Mogilefsky</a>.</p>
<p>And a special thank-you to reader Matt Campbell, who shared with me the audio of a talk that Bret Mogilefsky gave at the 2005 Lua Workshop, during which he explained how he used that language in GrimE.</p>
<p><strong>Where to Get It:</strong> A modestly remastered version of <em>Grim Fandango</em> is <a href="https://www.gog.com/en/game/grim_fandango_remastered">available for digital purchase</a> at GOG.com.</p>
							
							
														
													</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You too can write a book (139 pts)]]></title>
            <link>https://parentheticallyspeaking.org/articles/write-a-book/</link>
            <guid>42096915</guid>
            <pubDate>Sat, 09 Nov 2024 21:10:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parentheticallyspeaking.org/articles/write-a-book/">https://parentheticallyspeaking.org/articles/write-a-book/</a>, See on <a href="https://news.ycombinator.com/item?id=42096915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Thanks to <a href="https://www.neeldhara.com/">Neeldhara Misra</a> for pushing me to write
this post, based on a thread on Twitter.</span></p><p>This article is primarily directed at academics. Its purpose is to
tell you two things:
</p><div><ul><li><p>You <span>can</span> write a book.</p></li><li><p>You probably <span>should</span> write a book.</p></li></ul></div><h3>1<tt>&nbsp;</tt><a name="(part._.You_.Can_.Write_a_.Book)"></a>You Can Write a Book<span><a href="#(part._.You_.Can_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>Let’s say you’re not using someone else’s textbook, or using it only
loosely. That means you’re going to spend a lot of time organizing
your thoughts. You will probably produce some kind of “lecture
notes”. The delta from there to a book is much smaller than you
imagine.</p><p>Here’s a pro-tip. Back in about 2003/4, I noticed that the quantity of
reading that students would do before class was at most about six
pages; once it got to about eight pages, they wouldn’t read at
all. (These numbers may be much lower now.) But this automatically
bounds how much you have to <span>write</span>!</p><p>In short: let’s say you’re writing up lecture notes. You’re writing
about four to at most six pages per class. Let’s say you have about 30
classes (often many more). You have automatically written about 200
pages without even especially thinking about it. Two hundred pages of
writing is often called a … book. It represents your “take”. So your
take now has a book!</p><h3>2<tt>&nbsp;</tt><a name="(part._.You_.Should_.Write_a_.Book)"></a>You Should Write a Book<span><a href="#(part._.You_.Should_.Write_a_.Book)" title="Link to here">🔗</a><span> </span></span></h3><p>What are the <span>incentives</span> to do this? There are many, but they
may not accrue immediately: they may take time. Think of it as a
long-term investment in yourself.</p><ul><li><p>First, simply: you believe strongly in your view of the world,
and you’re pursuing it with intensity. Right now nobody else is really
able to download your brain. Your book becomes how others can download
it.</p></li><li><p>People at other places might use your book, or at least put it on
reading lists. Even if only one student there reads and internalizes
that supplemental material, that student now carries your ideas with
them. Much more concretely, they could be a PhD applicant.</p><p>I have gotten so many great PhD applicants over the years thanks to my
books! In particular, when they come from a less-well-known university,
this guarantees for me that they have the preparation I need, and that
we have a shared mindset.</p></li><li><p>What’s out there may not be very good.  This is especially an
issue in programming languages, where some of the widely-used texts
are basically hot garbage: basically a broken, 1970s view of the
world. I once wrote a
<a href="https://cs.brown.edu/~sk/Publications/Papers/Published/sk-teach-pl-post-linnaean/">position piece</a> on it
entitled “Teaching Programming Languages in a Post-Linnaean Age”! So
worldview really matters.</p><p>So that’s another incentive. To drive out the bad with good. Do you
use the standard text that everyone else uses in your field? If you’ve
read this far, probably not. Why not? You know (or think you know!)
what should be taught in your field. Who uses a text? Someone less
certain. So it’s not their fault. We need to help them along.</p></li><li><p>Sometimes we have resources—<wbr>like software—<wbr>that many people
don’t know about, but that are especially well suited to education in
our field. For instance, the <span>#lang</span> feature of <a href="https://racket-lang.org/">Racket</a>
is one of the most powerful tools for teaching programming
languages. But most people don’t (yet) know that.</p></li></ul><p>I speak from experience. I have written several books, some solo and
some with co-authors. I did the very thing you are told to not do as a
tenure-track assistant professor: I wrote a quality
undergraduate-level book. I survived (it didn’t hurt my tenure in the
slightest), and I then benefited from it for a long time.</p><h3>3<tt>&nbsp;</tt><a name="(part._.Mechanics)"></a>Mechanics<span><a href="#(part._.Mechanics)" title="Link to here">🔗</a><span> </span></span></h3><p>The one <span>big</span> thing I haven’t said, which drives a lot of this,
is the publishing medium. And oh boy, do I have opinions on this!
I wrote up some of them when I published the first formal edition of
my programming languages book, in an essay entitled
<a href="https://cs.brown.edu/~sk/Memos/Books-as-Software/">Books as Software</a>. Let me summarize/expand.</p><ul><li><p>Don’t go to commercial publishers. They are either just not
ready for the modern world or will basically paywall your
work. Paywalling is the total antithesis of wanting to have impact and
influence and wanting to drive bad books out of the market.</p></li><li><p>Publish it free online. Especially those of us who are
immigrants from poorer countries know what it’s like to not be able to
afford high-quality material. The next you is sitting right now in
Bangalore stuck with a crappy course and crappy book. Be their light.</p></li><li><p>Some people like paper. Upload your file to a print-on-demand
service. Even with a markup, it’ll be a lot cheaper than a book from a
commercial publisher.</p></li><li><p>In a STEM subject, your tenure case is not going to hinge on a
contract from MIT Press. Getting that contract is actually relatively
easy. Impact is hard. Go for the hard part, as you do in your
research. Optimize for it.</p></li><li><p>Put out a new release once a year. Don’t fall for the temptation
of continuous releases. People using your book need to be able to
depend on a fixed version for the semester. They will have references
to pages, sections, etc. Don’t break their build.</p></li><li><p>Publish permalinks. Not everyone can upgrade their course every
year. Yes, it means your old mistakes are on permanent display, and
some people won’t use your latest and greatest. Live with it. Your ego
is not that fragile. You’ll get over it.</p></li><li><p>Make it easy for people to send you corrections. They will (just
as they did for your software). Sometimes you will even get very
insightful and creative ideas. Of course you’ll also get various
dreck. Just as with your software. Because books are software, as my
essay says.</p></li><li><p>Try to provide materials in both PDF and HTML. The reasons
should be obvious. It’s not always easy. I personally prefer to use
<a href="https://docs.racket-lang.org/scribble/index.html">Scribble</a> for this purpose. But I have also used LaTeX and
even Google Docs. The latter two are each terrible in their own way,
but the best tool is the one you use and that lets you get something
done reasonably quickly. There’s always time to revise. Don’t suffer
the paralysis of tool indecision and let that become the reason you
don’t write!</p></li><li><p>You won’t make much money this way. It’s okay, you’re probably
already paid pretty well. And the money you’re not getting is money
the author of the crappy textbook is also not getting! And you’ll get
paid in mindshare, which is infinitely more valuable.</p></li><li><p>You can also do what I did: I published the free PDF on my Web
site. On the print-on-demand site, offer a modestly-priced PDF. I let
readers know where they can get the free version. Therefore, the only
reason to buy the PDF is the equivalent of a tip-har. Most don’t, but
a few do. (You can also use one of the tip-jar services, though they
didn’t provide enough value for me.)</p></li></ul><p>Sure, my revenues have been modest. I view the checks as a little
surprise bonus. Added over time it could probably have bought me a new
bike frame (my unit of measure!), but mostly it’s mostly just
a nice dinner and ice cream. But I’m not doing it for the money. I’m
doing it to spread a worldview and to liberate a field from terrible
books. Both are much more worthwhile to me.</p><p>In short: if you’re even slightly tempted to write that textbook—<wbr>go
for it. I got you, fam.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Visprex – Open-source, in-browser data visualisation tool for CSV files (129 pts)]]></title>
            <link>https://docs.visprex.com/</link>
            <guid>42096837</guid>
            <pubDate>Sat, 09 Nov 2024 20:54:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.visprex.com/">https://docs.visprex.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42096837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  

  
  


<h2 id="_1"><img alt="Visprex" src="https://docs.visprex.com/assets/images/logo.webp#center"></h2>
<h2 id="what-is-visprex">What is Visprex?</h2>
<p>Visprex is a lightweight data visualisation tool that helps you speed up your statistical modelling and analytics workflows. The main high-level features include:</p>
<h3 id="quick">⏱️ Quick</h3>
<ul>
<li>You can visualise your data <strong>in seconds</strong> to quickly build an intuition on your dataset</li>
<li>No need for referring to specific sytax in your statistical analysis software</li>
</ul>
<h3 id="secure">🔒️ Secure</h3>
<ul>
<li>Your data is processed <strong>entirely on your browser</strong>, which means your data won't be sent anywhere</li>
<li>No tracking or analytics software is used for privacy</li>
</ul>
<h3 id="open-source">📖 Open Source</h3>
<ul>
<li>Source code is fully <strong>open source</strong> on GitHub: <a href="https://github.com/visprex/visprex">github.com/visprex/visprex</a></li>
<li>Visprex is also freely available on <a href="https://www.visprex.com/">visprex.com</a></li>
</ul>
<h2 id="who-is-visprex-for">Who is Visprex for?</h2>
<h3 id="students">Students</h3>
<p>Visprex is suitable for students who are starting out in their statistical modelling training.</p>
<p>There's no need for starting your computing environment on your machine or writing tedious visualisation scripts.</p>
<h3 id="data-scientists">Data Scientists</h3>
<p>Visprex is also for data analysts who would like to quickly inspect tabular data for analytical purposes, without worrying about privacy or PII as no data leaves your browser.</p>
<h2 id="quickstart">Quickstart</h2>
<p>You can start visualising your data with just a few clicks by first <a href="https://docs.visprex.com/features/datasets/">loading your dataset</a>.</p>












                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NYC Subway Station Layouts (197 pts)]]></title>
            <link>http://www.projectsubwaynyc.com/gallery</link>
            <guid>42096717</guid>
            <pubDate>Sat, 09 Nov 2024 20:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.projectsubwaynyc.com/gallery">http://www.projectsubwaynyc.com/gallery</a>, See on <a href="https://news.ycombinator.com/item?id=42096717">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="44" id="block-yui_3_17_2_1_1540856882851_7416"><p>  Inspired and want to support this project? Consider tipping me on the  <a data-preserve-html-node="true" href="http://www.projectsubwaynyc.com/contact/">Contact/Tip</a> page!
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Jaws – a JavaScript to WASM ahead-of-time compiler (205 pts)]]></title>
            <link>https://github.com/drogus/jaws</link>
            <guid>42095879</guid>
            <pubDate>Sat, 09 Nov 2024 18:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/drogus/jaws">https://github.com/drogus/jaws</a>, See on <a href="https://news.ycombinator.com/item?id=42095879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Jaws</h2><a id="user-content-jaws" aria-label="Permalink: Jaws" href="#jaws"></a></p>
<p dir="auto">Jaws is a JavaScript to WebAssembly compiler written in Rust. It is similar to <a href="https://github.com/CanadaHonk/porffor">porffor</a> in a way it also results in a standalone WASM binary that can be executed without an interpreter, but it takes a different implementation approach.</p>
<p dir="auto">It's an experimental tool and it's not ready for production. A lot of the language
features and builtin types are missing or incomplete. That said, my goal is to eventually support 100% of the language.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Jaws?</h3><a id="user-content-why-jaws" aria-label="Permalink: Why Jaws?" href="#why-jaws"></a></p>
<p dir="auto">I started this project while working on a stress testing tool called <a href="https://github.com/drogus/crows">Crows</a> that runs WebAssembly scenarios. At the moment it only supports code compiled from Rust to WASM. As much as I love writing Rust, I also know it's not a widely popular language and besides, small tests are often easier to write in interpreted languages. The problem is, running scripting languages on top of WASM is not ideal at the moment. You have to either include an interpreter, which automatically makes the binary at least a few MBs in size and the memory usage even bigger, or use a variation of the language you're targetting (like TinyGo instead of Go, or AssemblyScript instead of TypeScript/JavaScript).</p>
<p dir="auto">I believe that with modern WASM proposals it is possible to implement 100% of JavaScript features without the need to use a compiled interpreter, as WASM runtimes are already interpreters.</p>
<p dir="auto">If you want to see it happen, please consider <a href="https://github.com/sponsors/drogus">sponsoring my work</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What works</h3><a id="user-content-what-works" aria-label="Permalink: What works" href="#what-works"></a></p>
<p dir="auto">As I eventually want to implment 100% of the language, I'm purposefully focused on implementing the semantics first, rather than go for 100% of builtins and grammar as I want to be 100% sure it's doable.</p>
<p dir="auto">I have a list of 4 things that I think are hard to implement and after I implement all of them I will focus on more grammar and builtins. These are:</p>
<ol dir="auto">
<li>Scopes/closures</li>
<li>try/catch</li>
<li>async/await</li>
<li>generators</li>
</ol>
<p dir="auto">The last two are kind of similar as by getting generators working, one essentially has tools to make async await work, but I still wanted to make the distinction. At the moment Jaws can compile code using closures with (mostly) proper scopes support, it allows try/catch and it implements (limited) <code>Promise</code> API and <code>async</code> (but not <code>await</code> yet). For example the following script will print <code>error: foo</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let value = &quot;foo&quot;;
async function foo() {
  throw value;
}

foo().then(
  function () {},
  function (v) {
    console.log(&quot;error&quot;, v);
  },
);"><pre><span>let</span> <span>value</span> <span>=</span> <span>"foo"</span><span>;</span>
<span>async</span> <span>function</span> <span>foo</span><span>(</span><span>)</span> <span>{</span>
  <span>throw</span> <span>value</span><span>;</span>
<span>}</span>

<span>foo</span><span>(</span><span>)</span><span>.</span><span>then</span><span>(</span>
  <span>function</span> <span>(</span><span>)</span> <span>{</span><span>}</span><span>,</span>
  <span>function</span> <span>(</span><span>v</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>"error"</span><span>,</span> <span>v</span><span>)</span><span>;</span>
  <span>}</span><span>,</span>
<span>)</span><span>;</span></pre></div>
<p dir="auto">A non exhaustive list of other stuff that should work:</p>
<ul dir="auto">
<li>declaring and assigning: <code>var</code>, <code>let</code>, <code>const</code></li>
<li><code>while</code></li>
<li>string lierals, adding string literals</li>
<li>numbers and basic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)</li>
<li>booleans and basic boolean operators</li>
<li>array literals</li>
<li>object literals</li>
<li><code>new</code> keyword</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Host requirements</h3><a id="user-content-host-requirements" aria-label="Permalink: Host requirements" href="#host-requirements"></a></p>
<p dir="auto">As Jaws is built with a few relatively recent WASM proposals, the generated binaries are not really portable between runtimes yet. I'm aiming to implement it with WASIp2 in mind, but the only runtime capable of running components and WASIp2, ie. Wasmtime, does not support some other things I use, like parts of the WASM GC proposal or exception handling.</p>
<p dir="auto">In order to make it easier to develop before the runtimes catch up with standardized proposals, I decided to use V8 (through Chromium or Node) with a Javascript polyfill for WASIp2 features that I need. There is a script <code>run.js</code> in the repo that allows to run binaries generated by Jaws. Eventually it should be possible to run them on any runtime implementing WASM GC, exception handling and WASIp2 API.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How to use it?</h3><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it?" href="#how-to-use-it"></a></p>
<p dir="auto">Unless you want to contribute you probably shouldn't, but after cloning the repo
you can use an <code>execute.sh</code> script like:</p>
<div data-snippet-clipboard-copy-content="./execute.sh --cargo-run path/to/script.js"><pre><code>./execute.sh --cargo-run path/to/script.js
</code></pre></div>
<p dir="auto">It will generate a WAT file, compile it to a binary and then run using Node.js.</p>
<p dir="auto">It requires Rust's <code>cargo</code>, relatively new version of <code>wasm-tools</code> and Node.js v23.0.0 or newer. Passing <code>--cargo-run</code> will make the script use <code>cargo run</code> command to first compile and then run the project, otherwise it will try to run the release build (so you have to run <code>cargo build --release</code> prior to running <code>./execute.sh</code> without <code>--cargo-run</code> option)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What's next?</h3><a id="user-content-whats-next" aria-label="Permalink: What's next?" href="#whats-next"></a></p>
<p dir="auto">My plan is to finish implementing all of the "hard to implement" features first, so next in line are generators and <code>await</code> keyword support. Ideally I would use the <a href="https://github.com/WebAssembly/stack-switching">stack-switching</a> proposal for both await and generators, but alas it's only in Phase 2 and it has minimal runtime support (I could find some mentions in Chromium development groups, but I couldn't get it to work). In the absence of stack-switching I'm working on using CPS transforms in order to simulate continuations.</p>
<p dir="auto">After that's done, I will be slowly implementing all of the missing pieces, starting with grammar (for loops, switch etc) and then builtin types and APIs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work?</h3><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">The project is essentially translating JavaScript syntax into WASM instructions, leveraging instructions added by WASM GC, exception handling and tail call optimizations proposals. On top of the Rust code that is translating JavaScript code, there is about 3k lines of WAT code with all the plumbing needed to translate JavaScript semantics into WASM.</p>
<p dir="auto">To give an example let's consider scopes and closures. WASM has support for passing function references and for structs and arrays, but it doesn't have the scopes semantics that JavaScript has. Thus, we need to simulate how scopes work, by adding some extra WASM code. Imagine the following JavaScript code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let a = &quot;foo&quot;;

function bar() {
  console.log(a);
}

bar();"><pre><span>let</span> <span>a</span> <span>=</span> <span>"foo"</span><span>;</span>

<span>function</span> <span>bar</span><span>(</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>a</span><span>)</span><span>;</span>
<span>}</span>

<span>bar</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">In JavaScript, because a function definition inherits the scope in which it's defined, the <code>bar()</code> function has access to the <code>a</code> variable. Thus, this script should print out the string <code>"foo"</code>. We could translate it to roughly the following pseudo code:</p>
<div data-snippet-clipboard-copy-content="// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, &quot;a&quot;, &quot;foo&quot;);

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -> Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, &quot;a&quot;));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, &quot;bar&quot;, fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, &quot;bar&quot;);
call(f);"><pre><code>// first we create a global scope, that has no parents
let scope = newScope(null);

// then we set the variable `a` on the scope
declareVariable(scope, "a", "foo");

// now we define the  bar function saving a reference to the function
let func = function(parentScope: Scope, arguments: JSArguments, this: Any) -&gt; Any {
  // inside a function declaration we start a new scope, but keeping
  // a reference to the parentScope
  let scope = newScope(parentScope);

  // now we translate console.log call retreiving the variable from the scope
  // this will search for the `a` variable on the current scope and all of the
  // parent scopes
  console.log(retrieve(scope, "a"));
}
// when running a function we have to consider the scope
// in which it was defined
let fObject = createFunctionObject(func, scope);
// and now we also set `bar` on the current scope
declareVariable(scope, "bar", fObject)

// now we need to fetch the `bar` function from the scop
// and run it
let f = retrieve(scope, "bar");
call(f);
</code></pre></div>
<p dir="auto">All of the helpers needed to make it work are hand written in WAT format. I have some ideas on how to make it more efficient, but before I can validate all the major features I didn't want to invest too much time into side quests. Writing WAT by hand is not that hard, too, especially when you consider WASM GC.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">License</h3><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The code is licensed under Apache 2.0 license</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Early Cascade Injection: From Windows process creation to stealthy injection (103 pts)]]></title>
            <link>https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/</link>
            <guid>42095752</guid>
            <pubDate>Sat, 09 Nov 2024 17:55:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/">https://www.outflank.nl/blog/2024/10/15/introducing-early-cascade-injection-from-windows-process-creation-to-stealthy-injection/</a>, See on <a href="https://news.ycombinator.com/item?id=42095752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage">

<p><em>By <a href="https://www.linkedin.com/in/guido-miggelenbrink-63aa0a166/">Guido Miggelenbrink</a> at Outflank</em></p>



<h2>Introduction</h2>



<p>In this blog post we introduce a novel process injection technique named Early Cascade Injection, explore Windows process creation, and identify how several Endpoint Detection and Response systems (EDRs) initialize their in-process detection capabilities. This new Early Cascade Injection technique targets the user-mode part of process creation and combines elements of the well-known Early Bird APC Injection technique with the recently published EDR-Preloading technique <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">by Marcus Hutchins</a> <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">[1]</a>. Unlike Early Bird APC Injection, this new technique avoids queuing cross-process Asynchronous Procedure Calls (APCs), while having minimal remote process interaction. This makes Early Cascade Injection a stealthy process injection technique that is effective against top tier EDRs while avoiding detection.</p>



<p>To provide insights into Early Cascade Injection’s internals, this blog also presents a timeline of the user-mode process creation flow. This overview illustrates how Early Cascade Injection operates and pinpoints the exact moment at which it intervenes in process creation. Furthermore, we compare that to the initialization timing of EDR user-mode detection measures.</p>



<p>Now, let’s dive into the details of Windows process creation, Early Bird APC Injection, and EDR-Preloading. Once we have a solid understanding of these topics, we can proceed to explore Early Cascade Injection.</p>



<h2>Understanding Windows Process Creation</h2>



<h3>Process creation APIs</h3>



<p>In Windows there are various APIs to create a process, such as <code>CreateProcess</code>, <code>CreateProcessAsUser</code>, and <code>CreateProcessWithLogon</code>, as shown in figure 1. Ultimately, all these functions invoke the NAPI <code>NtCreateUserProcess</code> in <code>ntdll.dll</code>. This function is responsible for initiating process creation by switching control to the kernel, where the equally named function <code>NtCreateUserProcess</code> is executed.</p>



<p>Each of these functions include the <code>dwCreationFlags</code> parameter, which controls how the process is created. In this post, we’ll encounter the <code>CREATE_SUSPENDED</code> flag, which instructs the kernel to create the new process’s initial thread in a suspended state <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">[2]</a>. The thread remains suspended until the <code>ResumeThread</code> function is called.</p>



<p>Obviously, these functions also have a parameter specifying the path to the application’s image file for which a process is to be created. Refer to the MSDN for the other parameters and flags of these APIs <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">[2]</a>.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs.png"><img fetchpriority="high" decoding="async" width="1024" height="430" src="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-1024x430.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-1024x430.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs-300x126.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/CreateProcessAPIs.png 1435w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 1: Process creation functions (Source: Windows Internals, Part 1)</em></p>



<h3>Kernel-mode and user-mode process creation</h3>



<p>Process creation has two parts: kernel-mode and user-mode. It begins with the kernel-mode part, initiated by <code>NtCreateUserProcess</code>. Once the process’s context and environment are created in kernel-mode, the initial thread of the process completes process creation in user-mode.</p>



<p>The kernel-mode part is responsible for opening the image file of the specified application and mapping it into memory. It then creates process-specific and thread-specific objects, maps the native library <code>ntdll.dll</code> into the process, followed by the creation of the process’s initial thread. If the <code>CREATE_SUSPENDED</code> flag is specified, this thread is created in suspended state, waiting to be resumed before control returns to user-mode for the remainder of the process creation.</p>



<p>The module <code>ntdll.dll</code> is the first DLL loaded into a process and it is the only DLL loaded in kernel-mode, all other modules are loaded in user-mode. Further, <code>ntdll.dll</code> includes the exported function <code>LdrInitializeThunk</code>, which handles the user-mode part of process creation before the application’s main entry point runs. This function is also known as the image loader and the functions related to it in <code>ntdll.dll</code> are prefixed with <code>Ldr</code>.</p>



<p>Returning to the newly created thread: upon resumption of this suspended thread, it starts executing <code>LdrInitializeThunk</code>, the user-mode part of process creation. After, the new process is fully initialized and ready to run the application. The initial thread then begins executing the application’s main entry point.</p>



<blockquote>
<p>Note that using the <code>CREATE_SUSPENDED</code> flag pauses process creation just before the initial thread switches to user-mode to run <code>LdrInitializeThunk</code>. This is particularly interesting because <strong>user-mode malware can interfere with process creation at this point</strong>. Therefore, let’s take a closer look at what happens inside <code>LdrInitializeThunk</code>.</p>
</blockquote>



<h3>User-mode process creation: <code>LdrInitializeThunk</code></h3>



<p><code>LdrInitializeThunk</code> is the first function executed in user-mode, marking the initial point where malware and EDRs can intervene in a process. We will later explore how techniques such as Early Bird APC Injection, EDR-Preloading and Early Cascade Injection interact with <code>LdrInitializeThunk</code>. For now, let’s delve into the details of this function.</p>



<p><code>LdrInitializeThunk</code> is a complex function responsible for the user-mode part of process creation. It handles numerous process initialisation tasks, which are listed and briefly described in the <em>Windows Internals, Part 1</em> book. However, the book does not cover which subordinate functions within <code>LdrInitializeThunk</code> are responsible for these tasks. Hence, to gain a deeper understanding of <code>LdrInitializeThunk</code> and its subordinate functions, we analysed it using x64dbg and IDA Pro.</p>



<p>Based on this analysis, we created a call graph that outlines the sequence of events in the user-mode part of process creation. This timeline includes the functions relevant to this post and thus omits some tasks and associated functions of <code>LdrInitializeThunk</code>. Additionally, please note that this call graph reflects our interpretation and may not be entirely accurate.</p>



<p>The call graph of <code>LdrInitializeThunk</code> is depicted below, followed by a description of the tasks that can be recognized in it. Key functions are highlighted in color.</p>



<p><strong>Call graph <code>LdrInitializeThunk</code>:</strong></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3.png"><img decoding="async" width="588" height="1024" src="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-588x1024.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-588x1024.png 588w, https://www.outflank.nl/wp-content/uploads/2024/10/FullCallGraphV3-172x300.png 172w" sizes="(max-width: 588px) 100vw, 588px"></a></figure>



<p>This call graph illustrates the following tasks performed by <code>LdrInitializeThunk</code>:</p>



<ol>
<li>Initialises Loader Lock in the Process Environment Block (<code>PEB</code>).
<ul>
<li>The <code>PEB</code> is a data structure that stores information about the process’s context and environment. Loader Lock will be discussed later.</li>
</ul>
</li>



<li>Sets up the Mutable Read-Only Heap Section (.mrdata).</li>



<li>Creates and inserts the first loader data table entry (<code>LDR_DATA_TABLE_ENTRY</code>) for <code>ntdll.dll</code> into the module database (<code>PEB_LDR_DATA</code>).
<ul>
<li>The module database, stored in the <code>PEB</code>, contains three lists that keep track of the process’s loaded modules: <code>InLoadOrderModuleList</code>, <code>InMemoryOrderModuleList</code>, and <code>InInitializationOrderModuleList</code>. Each entry in these lists includes details such as the module’s base address, entry point, and path.</li>
</ul>
</li>



<li>Initialises the parallel loader.
<ul>
<li>The parallel loader is responsible for loading the application’s imported DLLs concurrently, using a pool of threads. It sets up a <code>LdrpWorkQueue</code> that contains the application’s first order dependencies to be loaded. Then, the parallel threads load these dependencies, recursive ones are added to the work queue. For more information on the Windows parallel loader, refer to this insightful blog by <a href="https://blogs.blackberry.com/en/2017/10/windows-10-parallel-loading-breakdown">[3]</a>.</li>
</ul>
</li>



<li>Creates and inserts the second loader data table entry (<code>LDR_DATA_TABLE_ENTRY</code>) for the application’s executable into the module database (<code>PEB_LDR_DATA</code>).</li>



<li>Loads the initial modules <code>kernel32.dll</code> and <code>kernelbase.dll</code>.
<ul>
<li>These modules are always loaded into every process. Like all other modules, these are also added to the module database (<code>PEB_LDR_DATA</code>).</li>
</ul>
</li>



<li>If enabled, initializes the Shim Engine and parses the Shim Database. <strong>The Shim Engine is by default off.</strong>
<ul>
<li>The Shim Engine applies compatibility fixes (shims) to applications without modifying their code. It intercepts and modifies API calls to address compatibility issues. For more details, refer to <a href="https://techcommunity.microsoft.com/t5/ask-the-performance-team/demystifying-shims-or-using-the-app-compat-toolkit-to-make-your/ba-p/374947">[11]</a>.</li>
</ul>
</li>



<li>If enabled, it uses the parallel loader to load the application’s remaining dependencies into the process; otherwise, DLLs are loaded sequentially.</li>
</ol>



<p>After mapping and initializing all dependencies, <code>LdrInitializeThunk</code> invokes the following functions:</p>



<ol start="9">
<li><code>NtTestAlert</code>: <strong>Empties the APC queue of the calling thread</strong> by switching to kernel-mode, which then calls <code>KiUserApcDispatcher</code> to execute the queued APCs.</li>



<li><code>NtContinue</code>: Sets the initial thread’s execution context to <code>RtlUserThreadStart</code>, which subsequently invokes the main entry point of the application.</li>
</ol>



<p>At the very bottom of the call graph, we see <code>RtlRaiseStatus</code>. This function is normally never executed, as <code>NtContinue</code> redirects the execution flow to the application’s main entry point. However, if the application crashes, <code>RtlRaiseStatus</code> is triggered.</p>



<p>Although the call graph is simplified, it remains complex. Hopefully, it provides some insight into the internals of process creation. To clarify the key points relevant to this blog, we created an abstraction that captures the essential information you need to remember. The red comments describe the operation that the preceding block of functions accomplish.</p>



<p><strong>Abstracted call graph <code>LdrInitializeThunk</code>:</strong></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4.png"><img decoding="async" width="1024" height="646" src="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-1024x646.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-1024x646.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/AbstractedCallGraphV4-300x189.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<blockquote>
<p>In this abstraction of the call graph, we observe that <code>LdrInitializeThunk</code> loads <code>kernel32.dll</code> and <code>kernelbase.dll</code>, then it loads all other dependencies, clears its APC queue (<code>NtTestAlert</code>), and finally begins executing the application’s main entry point (<code>NtContinue</code>). Additionally, we observe that loading a DLL via <code>LdrLoadDll</code> consists of two steps: mapping and initialization.</p>
</blockquote>



<h3>How <code>LdrLoadDll</code> works</h3>



<p>Loading dependencies is a major component of <code>LdrInitializeThunk</code>, which is probably why it is called the image loader. Moreover, the techniques EDR-Preloading and Early Cascade Injection specifically intervene in this <code>LdrLoadDll</code> function during process creation. Therefore, we also briefly cover how <code>LdrLoadDll</code> loads dependencies. After that, we will start exploring the interesting stuff: process injection.</p>



<blockquote>
<p>The most important aspect to remember after this section is that<code>LdrLoadDll</code> loads a DLL in two steps: first it <strong>maps</strong> the DLL’s image file into memory through <code>LdrpFindOrPrepareLoadingModule</code> and second it <strong>initializes</strong> the DLL via <code>LdrpPrepareModuleForExecution</code>. If a DLL has recursive dependencies, these are first mapped into memory and only then each module is initialized. Initialization is performed in reverse order to follow the correct dependency sequence.</p>
</blockquote>



<p>Now, let’s delve into a bit more details and walk through the process of loading <code>kernel32.dll</code>, the first module being loaded during process creation. Try to follow along in the call graph. We start at <code>LdrInitializeThunk</code> which calls <code>LdrpLoadDll</code> to load <code>kernel32.dll</code>. <code>LdrpLoadDll</code> invokes <code>ntdll!LdrpFindOrPrepareLoadingModule</code> for the first step: mapping <code>kernel32.dll</code> into memory. The actual mapping is eventually performed by the nested function <code>NtMapViewOfSection</code>. After, <code>LdrpMapAndSnapDependency</code> checks for dependencies, and since <code>kernel32.dll</code> imports functions from <code>kernelbase.dll</code>, <code>LdrpLoadDependentModule</code> maps <code>kernelbase.dll</code> into memory.</p>



<p>Once the two modules have been mapped into memory, the second part of <code>LdrLoadDll</code> initializes the DLLs, carried out by <code>LdrpPrepareModuleForExecution</code>. This function invokes <code>LdrpCondenseGraph</code> to create a dependency graph, which stores the order in which dependencies must be initialized. Following, <code>LdrpInitializeGraphRecurse</code> processes this graph and for each module in the graph <code>LdrpInitializeNode</code> is called. The first node in the graph is <code>kernelbase.dll</code> which <code>LdrpInitializeNode</code> initialises through <code>LdrpCallInitRoutine</code>. This function calls the entry point of <code>kernelbase.dll</code>. After that, the same is performed to initialize <code>kernel32.dll</code>; hereafter, <code>LdrLoadDll</code> has finished loading <code>kernel32.dll</code>.</p>



<h2>Early Bird APC Injection</h2>



<p>Now that we have a general understanding of process creation, let’s take a closer look at the Early Bird APC Injection technique which was discovered by Cyberbit in 2018 <a href="https://www.cyberbit.com/endpoint-security/new-early-bird-code-injection-technique-discovered/">[4]</a>. This is a well-known and effective process injection method that involves injecting code before the execution of a process’s main entry point. Injecting this early in the process may evade EDR detection measures, including hooks, if these measures are not loaded before APC execution.</p>



<p>The Early Bird APC Injection technique works as follows:</p>



<ol>
<li>Create a target process in suspended state (e.g. <code>CreateProcess</code>);</li>



<li>Allocate writeable memory in the target process (e.g. <code>VirtualAllocEx</code>);</li>



<li>Write malicious code to the allocated memory (e.g. <code>WriteProcessMemory</code>);</li>



<li>Queue an APC to the remote target process, the APC points to the malicious code (e.g. <code>QueueUserAPC</code>);</li>



<li>Resume the target process, upon resumption the APC is executed, running the malicious code (e.g. <code>ResumeThread</code>).</li>
</ol>



<p>As we previously learned, when a process is created in a suspended state (1), execution halts just before the user-mode part of process creation, handled by <code>LdrInitializeThunk</code>. At this point, the payload with malicious code is written into the target process (2, 3). Then, an APC routine pointing to the payload is queued for execution in the suspended thread (4). Last, the suspended thread is resumed (5).</p>



<p>On resumption of the thread, it starts execution at <code>LdrInitializeThunk</code> and one of final tasks of <code>LdrInitializeThunk</code> is emptying the APC queue. Specifically, <code>NtTestAlert</code> is responsible for emptying a threads APC queue by executing the APCs in it. This is when the injected payload runs.</p>



<p>In the past, execution of the payload at this point was early enough to preempt EDR user-mode detection measures, like hooks. However, modern EDR solutions often load their detection measures earlier in the process creation timeline. Nonetheless, we found that a popular EDR still loads its detection measures after <code>NtTestAlert</code>. For this particular EDR, Early Bird APC Injection bypasses the EDR’s user-mode detection measures. Despite Early Bird APC Injection does may no longer evade hooks of modern EDRs, it still remains very useful for injection purposes.</p>



<blockquote>
<p>Early Bird APC remains a valuable injection technique, even though it is less effective as an evasion method against modern EDRs.</p>
</blockquote>



<p>Nevertheless, as mentioned earlier, <strong>Early Bird APC Injection is likely to be detected due to the suspicious cross-process queuing of an APC</strong>. Queuing an APC from one process to another is known as cross-process APC queueing. This behaviour is suspicious and closely monitored by EDRs. It’s difficult to hide cross-process APC queueing, making it a strong indicator for detecting Early Bird APC Injection. In a moment, we will see how Early Cascade Injection performs its injection without cross-process queuing and therefore goes undetected against the EDRs we tested.</p>



<h2>EDR-Preloading</h2>



<p>Early Cascade Injection incorporates elements of EDR-Preloading. Therefore, let’s briefly delve into EDR-Preloading, which was recently introduced in a blog by Marcus Hutchins, who is known for stopping the WannaCry <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">[1]</a>. His blog inspired my research in this area, thank you for that!</p>



<p>EDR-Preloading is designed to prevent EDRs from loading their user-mode detection measures during process creation. For example, it prevents EDRs from initialising their Hooking DLL, which significantly reduces an EDRs the visibility within the process as the EDR is unable to intercept API calls. Techniques like this are becoming increasingly important as Microsoft gradually restricts third-party access to the kernel, forcing EDR detection measures from kernel-mode to user-mode. It is speculated that kernel restrictions will be pushed further since the Crowdstrike incident took down 8.5 million Windows systems, due to a faulty update in their kernel-level software <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">[5]</a>.</p>



<p>How EDR-Preloading works: <strong>it begins by creating a process in suspended state and hijacking the <code>ntdll!AvrfpAPILookupCallbackRoutine</code> callback pointer in <code>ntdll.dll</code></strong>. Hijacking involves assigning the start address of malicious code to the callback pointer and enabling the callback pointer by setting <code>ntdll!AvrfpAPILookupCallbacksEnabled</code> to <code>1</code>. As a result, the callback pointer is executed during the user-mode part of process creation after resuming the suspended process. <strong>Once invoked, the malicious code runs, thereby taking control of the execution flow during process creation.</strong></p>



<p>This malicious code runs very early in the process creation sequence when only <code>ntdll.dll</code> is loaded. Specifically, the callback <code>AvrfpAPILookupCallbackRoutine</code> is triggered in the initialisation part of <code>LdrLoadDll</code>, as illustrated in the call graph. Both the callback pointer (<code>ntdll!AvrfpAPILookupCallbackRoutine</code>) and the boolean variable (<code>ntdll!AvrfpAPILookupCallbacksEnabled</code>) are highlighted in the graph in light green and green. This <code>LdrLoadDll</code> function is executed for the first time during the initialization of <code>kernelbase.dll</code>, the first DLL to be loaded in user-mode. If EDRs load their detection measures after this point, it is possible to prevent them from loading their detection measures. A detailed explanation of this and how to implement it can be found in the EDR-Preloading blog.</p>



<blockquote>
<p>What we found interesting about the EDR-Preloading technique is that it possible to get code execution just by overwriting a callback pointer in the target’s <code>ntdll.dll</code> during process creation. <strong>However, this code execution is highly constrained.</strong></p>
</blockquote>



<h3>Code execution limitations</h3>



<p>The code execution obtained through <code>ntdll!AvrfpAPILookupCallbackRoutine</code> during process initialisation is significantly constrained. These limitations are caused by the <strong>limited number of available dependencies</strong> and the constraints imposed by the <strong>Loader Lock</strong> synchronisation object.</p>



<h4>Limited dependencies</h4>



<p>At the point when the <code>AvrfpAPILookupCallbackRoutine</code> callback is invoked, only <code>ntdll.dll</code> is fully loaded into the process. Consequently, code execution is restricted to the undocumented NTAPI functions within <code>ntdll.dll</code>, significantly limiting the actions that can be performed. The lack of access to other libraries, such as <code>winhttp.dll</code>, complicates the execution of more complex operations, such as communicating with a command and control (C2) server.</p>



<p>Furthermore, due to the presence of Loader Lock, no additional DLLs can be loaded, and no new threads can be created.</p>



<h4>Loader Lock</h4>



<p>The <code>ntdll!AvrfpAPILookupCallbackRoutine</code> callback runs during the initialisation part of <code>LdrLoadDll</code>, under the function <code>LdrpPrepareModuleForExecution</code>. More precisely, the callback is triggered within <code>LdrpInitializeNode</code>, which handles the actual initialisation of a DLL, as previously discussed. During the execution of <code>LdrpInitializeNode</code>, Loader Lock is held to synchronize the loading and unloading of DLLs. The call graph shows that this synchronization is managed by <code>LdrpAcquireLoaderLock</code> and released by <code>LdrpReleaseLoaderLock</code>.</p>



<p><strong>Loader Lock is a critical section object that prevents the loading of additional DLLs and creation of new threads <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">[8]</a></strong>. Critical sections are a synchronisation mechanism similar to mutexes and semaphores, but they are designed to be more efficient and for single-process synchronisation. For information on critical section objects, refer to the MSDN documentation <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">[8]</a>.</p>



<p>Loader Lock is acquired each time when a function needs access to the module database (<code>PEB_LDR_DATA</code>), which is involved in tasks such as DLL loading, unloading, and thread creation <a href="https://elliotonsecurity.com/what-is-loader-lock/">[9]</a>. We discussed the module database earlier in step 3 of <code>LdrInitializeThunk</code>‘s tasks. A well-known function that accesses the module database is <code>GetModuleHandle</code>, which retrieves the base address of a DLL and is often used by malware to resolve undocumented NTAPI functions. However, if this function is called while Loader Lock is active, such as during the execution of <code>AvrfpAPILookupCallbackRoutine</code>, a deadlock occurs, causing the process to hang. Similarly, attempting to load additional DLLs via functions like <code>LdrLoadDll</code> results in a deadlock.</p>



<blockquote>
<p>In summary, code execution through the callback pointer <code>AvrfpAPILookupCallbackRoutine</code> during process initialisation is limited to the modules already loaded into the process at that point (<code>ntdll.dll</code>). Additionally, Loader Lock prevents the loading of additional DLLs and the creation of new threads, making it difficult to execute tasks that require access to more modules. Despite these limitations, the EDR-Preloading technique has demonstrated that there are just enough capabilities to prevent EDRs from loading their detection measures.</p>
</blockquote>



<h2>Early Cascade Injection: A new process injection technique</h2>



<p>What we found interesting about the EDR-Preloading technique is that you can get code execution just by overwriting a callback pointer in the target’s <code>ntdll.dll</code> during process creation. However, as we have seen, the code execution obtained through this callback is highly restricted as Loader lock is enabled, making it impractical to run fully functional code, such as an implant with networking capabilities. Therefore, we explored novel and alternative techniques during process creation for process injection. As a results we developed Early Cascade Injection, a novel code injection technique emerged from the restrictions imposed by the Loader Lock.</p>



<h4>An alternative callback pointer: <code>g_pfnSE_DllLoaded</code></h4>



<p>During our search for alternative injection techniques, <strong>we discovered an alternative callback pointer that also allows code execution during the user-mode part of process creation.</strong> This pointer, named <code>g_pfnSE_DllLoaded</code>, is located in the <code>.mrdata</code> section of <code>ntdll.dll</code>. Unlike the <code>AvrfpAPILookupCallbackRoutine</code>, <code>g_pfnSE_DllLoaded</code> does not appear to run under Loader Lock. This can be inferred from the call graph, where it is highlighted in light blue and blue.</p>



<p>While not directly relevant to this blog, it might be interesting to understand what this pointer belongs to. The <code>g_pfnSE_DllLoaded</code> pointer belongs to the Shim Engine, as indicated by its name, the prefix <code>g_pfnSE</code> stands for “global function pointer Shim Engine”. The Shim Engine is a Windows technology responsible for applying compatibility fixes, known as ‘shims,’ without modifying application code. It allows older applications to run on newer versions of Windows, by intercepting and modifying API calls. Although rarely used and disabled by default, the Shim Engine’s implementation is still present in <code>ntdll.dll</code>, along with its pointers, including <code>g_pfnSE_DllLoaded</code>.</p>



<p>Let’s return to the key aspects of <code>g_pfnSE_DllLoaded</code>. <strong>The pointer can be manually enabled by setting the <code>g_ShimsEnabled</code> boolean variable to <code>1</code></strong>, located in the <code>.data</code> section of <code>ntdll.dll</code>. <strong>However, enabling this variable enables all Shim Engine related pointers</strong>, not just <code>g_pfnSE_DllLoaded</code>. Each of these pointers require a valid address, and if any remain uninitialized, the process will crash. This makes it impractical to exploit <code>g_pfnSE_DllLoaded</code> alone without addressing the other pointers.</p>



<p>To overcome this, <strong>we focused specifically on <code>g_pfnSE_DllLoaded</code> as it is the first Shim Engine pointer invoked during process creation.</strong> By targeting this pointer we can execute code before any of the other unassigned pointers and prevent them from executing. This method involves assigning the address of our shellcode to <code>g_pfnSE_DllLoaded</code> and enabling <code>g_ShimsEnabled</code> to activate it. <strong>Upon execution, the shellcode immediately disables <code>g_ShimsEnabled</code>, preventing the remaining Shim Engine pointers from being invoked.</strong> This approach allows use to execute code without causing the process to crash due to uninitialized pointers.</p>



<p>Returning to the call graph, we observe that <code>g_pfnSE_DllLoaded</code> runs in the scope of <code>LdrpSendPostSnapNotifications</code>, which is a subordinate function of <code>LdrpPrepareModuleForExecution</code>. Unlike <code>LdrpPrepareModuleForExecution</code>, we observe that <code>g_pfnSE_DllLoaded</code> does not run under Loader Lock. Instead, a different critical section object is acquired: <code>LdrpDllNotificationLock</code>. This critical section appears to be self-reentrant, suggesting it should not lead to deadlock when loading additional DLLs, although we have not verified.</p>



<p>Despite not operating under Loader Lock, we were unable to run a fully functional shellcode. This is likely due to interrupting the loading process of kernelbase.dll and kernel32.dll. We will work around this in the next section.</p>



<p>Let’s briefly revisit the memory section where <code>g_pfnSE_DllLoaded</code> resides, as this is crucial for leveraging it. <strong><code>g_pfnSE_DllLoaded</code> is located in the <code>.mrdata</code> section, which is writable when a process is created in a suspended state.</strong> Later, during the user-mode part of process initialization, this section is made read-only, as noted in step 2 of <code>LdrInitializeThunk</code>. After this step, modifying its content requires memory protection changes.</p>



<p>Furthermore, <strong>the <code>g_ShimsEnabled</code> boolean is located in the <code>.data</code> section, which remains writable throughout the entire process.</strong> This allows us to enable or disable the <code>g_pfnSE_DllLoaded</code> pointer without modifying memory protections. In contrast, the <code>AvrfpAPILookupCallbacksEnabled</code> boolean, used in EDR-Preloading, resides in the <code>.mrdata</code> section and requires memory protection changes after step 2 of <code>LdrInitializeThunk</code>.</p>



<p>This makes <code>g_pfnSE_DllLoaded</code> preferable over <code>AvrfpAPILookupCallbackRoutine</code>, as it can be disabled without altering memory protections. As a result, the shellcode required to hijack the pointer is smaller, invoked only once, involves fewer API calls, and therefore reduces the risk of detection.</p>



<p>Additionally, <strong>the <code>g_pfnSE_DllLoaded</code> pointer is triggered slightly earlier than <code>AvrfpAPILookupCallbackRoutine</code>, offering earlier control over the process.</strong> Similar to how <code>AvrfpAPILookupCallbackRoutine</code> is leveraged in EDR-Preloading to preempt EDRs, <code>g_pfnSE_DllLoaded</code> can also be used for this purpose, with potentially greater effectiveness due to its earlier execution. As shown in the call graph, <code>g_pfnSE_DllLoaded</code> is executed just before <code>LdrpCallInitRoutine</code>, which initializes a DLL. This timing allows us to disrupt the initialisation of EDR user-mode detection measures implemented as DLL, making them ineffective. For example, it could prevent EDRs from deploying hooks that intercept API calls, significantly reducing an EDRs visibility within a process. While not the focus of this blog, this presents another use case for the pointer.</p>



<blockquote>
<p>In summary, we identified an alternative pointer named <code>g_pfnSE_DllLoaded</code>, located in the <code>.mrdata</code> section of <code>ntdll.dll</code>. This pointer can be enabled via the <code>g_ShimsEnabled</code> boolean, located in the <code>.data</code> section of <code>ntdll.dll</code>. The <code>.mrdata</code> section is writable during suspend state of process creation and the <code>.data</code> section is writable through the entire process, allowing use to hijack this pointer without changing memory protections. Moreover, <code>g_pfnSE_DllLoaded</code> does not operate under Loader Lock, but it is not trivial to execute fully functional shellcode for a unknown reason. Though, we suspect this may be related to a critical section object or because of the interruption during the <code>kernel32.dll</code> and <code>kernelbase.dll</code> loading process.</p>
</blockquote>



<h4>Intra-process APC queueing</h4>



<p>The limitations of code execution through <code>g_pfnSE_DllLoaded</code> made us thinking. We then realized that during the code execution, <strong>we could invoke an execution primitive to run code at a different stage, free from the limitations</strong>. We considered several execution primitives, including <code>NtQueueApcThread</code>, <code>NtCreateThread</code> and various callbacks such as <code>CreateTimerQueueTimer</code>. Eventually, we found that <code>NtQueueApcThread</code> was suitable for our needs and did the job <a href="http://undocumented.ntinternals.net/index.html?page=UserMode%2FUndocumented%20Functions%2FAPC%2FNtQueueApcThread.html">[6]</a>. A comprehensive list of potential callbacks as alternative to <code>NtQueueApcThread</code> can be found in this repository <a href="https://github.com/aahmad097/AlternativeShellcodeExec">[7]</a>.</p>



<p>The use of an execution primitive to move code execution to another point, e.g. via <code>NtQueueApcThread</code>, was inspired by Early Bird APC Injection. Although, Early Bird APC Injection leverages the APC queue for cross-process code execution.</p>



<p><strong>By leveraging the code execution obtained through <code>g_pfnSE_DllLoaded</code>, we can have the initial thread queue an APC on itself.</strong> This allows us to transition to unrestricted execution later in the process creation. We refer to this as intra-process APC queuing. The queued APC routine points to malicious code in the target’s memory, such as an implant.</p>



<p><strong><code>NtQueueApcThread</code> was particular suitable because it is available in <code>ntdll.dll</code> and is not subject to the loader lock since it does not involve DLL operations or thread creation.</strong> This means we don’t have to worry about causing a deadlock when calling this function within the execution scope of <code>g_pfnSE_DllLoaded</code>.</p>



<p>Moreover, <code>NtQueueApcThread</code> allows us to queue an APC early in the process initialization phase, before the APC queue is emptied. As detailed in step 9 of <code>LdrInitializeThunk</code>, one of the final steps involves invoking <code>NtTestAlert</code> to clear the APC queue. <strong>This guarantees the execution of our queued APC.</strong> Furthermore, since <code>NtTestAlert</code> is one of the final functions, we can be certain that all DLLs, including <code>kernel32.dll</code> and <code>kernelbase.dll</code>, have been fully loaded, ensuring that no issues arise from incomplete DLL loading.</p>



<p>To test our idea, we wrote a piece of shellcode that utilizes <code>NtQueueApcThread</code> in <code>ntdll.dll</code> to queue an intra-process APC. We refer to this shellcode as the <strong>payload stub</strong>, which we into the target’s memory. The APC routine passed to <code>NtQueueApcThread</code> points to the address of malicious code that we have written into the target’s memory. This malicious code we refer to as the <strong>payload</strong>. Thus, the payload stub is executed by <code>g_pfnSE_DllLoaded</code>, while the payload is executed through the APC.</p>



<h4>The Early Cascade Injection technique</h4>



<p>By now, the direction of the Early Cascade Injection technique should be clear, as we’ve covered all the key elements and background information. It’s time to bring everything together and formally introduce Early Cascade Injection!</p>



<p><strong>Early Cascade Injection works as follows:</strong> it begins by creating a child process in suspended state. Then it writes a two-part payload into it. Next, the parent locates the pointer <code>g_pfnSE_DllLoaded</code> in the <code>.mrdata</code> section and it locates the <code>g_ShimsEnabled</code> boolean variable in the <code>.data</code> section of <code>ntdll.dll</code>. Following, it assigns the address of the first payload part, the payload stub, to this <code>g_pfnSE_DllLoaded</code> pointer of the new process and enables it by setting <code>g_ShimsEnabled</code> to <code>1</code>. Last, it resumes the suspended process. As a result, the initial thread of the new process executes the payload stub. This payload immediately disables the <code>g_ShimsEnabled</code> by setting it to <code>0</code>, preventing the remaining Shim Engine related pointers from executing. Then, the payload stub queues the second part of the payload as an APC on itself, i.e. the initial thread, using <code>NtQueueApcThread</code>. This APC is triggered near the end of the Windows Image Loader by the <code>NtTestAlert</code> function. As a result, the main payload executes. The main payload could be an implant containing the main functionality that the attackers want to run on the target’s system.</p>



<p>In figure 2, the flow of Early Cascade Injection is depicted as described above.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3.png"><img loading="lazy" decoding="async" width="1024" height="817" src="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-1024x817.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-1024x817.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/EarlyCascadeFlowV3-300x239.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 2: Flow Early Cascade Injection</em></p>



<p>Early Cascade Injection is a novel injection technique and may serve as alternative to Early Bird APC Injection. The major advantage compared to Early Bird APC Injection is that Early Cascade does not involve a remote execution primitive (cross-process APC queueing). In addition, Early Cascade Injection, unlike Early Bird APC Injection, is currently undocumented and breaks traditional code injection patterns by not queuing an APC across processes. We tested it against multiple EDRs, including top tier ones, and went undetected.</p>



<h5>Key features</h5>



<ul>
<li><strong>No remote execution primitive:</strong> Early Cascade Injection avoids remote execution primitives such as <code>QueueUserAPC</code>. Just like threadless injection methods, it leverages a pointer for execution of the payload, avoiding the need for a remote execution primitive.</li>



<li><strong>Minimal remote process interaction:</strong> Early Cascade Injection only involves remote memory allocation, protection and writing.</li>



<li><strong>Writable <code>.mrdata</code> and <code>.data</code>:</strong> The <code>.mrdata</code> section is writable during the suspended state, allowing modifications without changing memory protections. Also <code>.data</code> is writable during the entire process, allowing enabling/disabling of <code>g_pfnSE_DllLoaded</code> without changing memory protections.</li>



<li><strong>Novel technique:</strong> Due to Early Cascade Injection’s novel approach, its call pattern is less likely to be recognized by security products, reducing the risk of detection.</li>



<li><strong>Undocumented callback:</strong> Early Cascade Injection relies on the undocumented pointer <code>g_pfnSE_DllLoaded</code>, which may change with Windows updates, potentially impacting its reliability.</li>
</ul>



<h2>EDR Detection Measure Loading Mechanism and Timing</h2>



<p>In this final section, we explore how and when EDRs load their user-mode detection measures, such as hooks, during process creation. Understanding the timing of these measures is crucial for developing strategies to preempt and evade them. Preempting means gaining control of the process before these detection measures are in place. For confidentiality, we won’t mention specific EDR names.</p>



<p>To provide a clearer understanding of user-mode detection mechanisms, we will briefly discuss hooks, using them as an example. Besides, user-mode hooks are one of the key detection measures used by EDRs to detect malicious activity. Especially, since Microsoft gradually restricts kernel access, which forces EDRs to shift detection measures to user-mode <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">[5]</a>. Microsoft does offer alternatives like Event Tracing for Windows (ETW), however these are not yet widely adopted. This is likely to change in the nearby future.</p>



<p>Hooks allow EDRs to monitor processes in real time by intercepting API calls from within the process. By preventing these hooks from loading, attackers can significantly reduce an EDR’s visibility, thereby increasing the chances of malware evading detection. One effective approach to avoid hooks is by acting before the hooks are fully loaded and take effect. Typically, EDRs place hooks through a hooking DLL during the user-mode part of process creation. In the following section, we will explain how this works in detail.</p>



<p>Before diving into the technical details, it’s essential to understand the role of the kernel driver in EDRs. This driver enables EDRs to register notification callback routines to receive alerts on system events such as process creation or termination, image loading, registry changes, and system shutdown requests. These callbacks gather system information on which EDRs might take future actions. For instance, upon receiving a process creation notification, the EDR can inject its hooking DLL into a new process for monitoring purposes.</p>



<p>The process notification callback is stored in the kernel’s <code>nt!PspCreateProcessNotifyRoutine</code> array, which holds all registered callbacks. When a new process is created, the kernel function <code>nt!PspCallProcessNotifyRoutines</code> iterates through this array, invoking each callback. For more information on the components of EDRs and their interaction with Windows, we recommend the book Evading EDR by Matt Hand.</p>



<p>As a side note, there are evasion tools that can deregister kernel callbacks to prevent EDRs from loading additional security measures <a href="https://br-sn.github.io/Removing-Kernel-Callbacks-Using-Signed-Drivers/">[10]</a>. However, this approach requires access to the kernel, which is typically achieved through the exploitation of a vulnerable kernel driver. By modifying the kernel’s notification callbacks, these tools can block the EDR from loading its user-mode detection measures. However, the required kernel-access for this technique, makes it a complex method for evasion.</p>



<p>Returning to the main point, EDRs use the process creation notification as a trigger to load user-mode detection measures into newly created processes. We analyzed several EDRs to understand how these detection measures are loaded. Based on our findings, we explain the general approach EDRs take to inject their user-mode detection modules.</p>



<p>We observed that when a newly created process resumes from a suspended state, EDRs modify <code>ntdll.dll</code> just before transitioning from kernel to user-mode (<code>LdrInitializeThunk</code>). Specifically, EDRs inject shellcode into the process memory, which contains the logic to load the EDR’s hooking DLL. Additionally, they place a hook in <code>LdrInitializeThunk</code>, redirecting code execution to the injected shellcode. In our analyses of various EDRs, we found that the hooks are specifically placed on <code>LdrLoadDll</code>, <code>LdrpLoadDll</code>, or <code>NtContinue</code> within <code>LdrInitializeThunk</code>. Figure 3 revisits the call graph and highlights these functions. Note, EDRs also load their detection measures using this mechanism for processes that are not created in a suspended state.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure.png"><img loading="lazy" decoding="async" width="1024" height="646" src="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-1024x646.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-1024x646.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/EDR-timing-figure-300x189.png 300w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 3: The red arrows point to the functions that EDRs hook to load their user-mode detection measures</em></p>



<p>As an example, figure 4 shows the hook on <code>LdrLoadDll</code>. The initial bytes of <code>LdrLoadDll</code> are replaced with a jump instruction that points to the injected shellcode. This hooked version of <code>LdrLoadDll</code> is called as a subordinate function of <code>LdrInitializeThunk</code>. When <code>LdrLoadDll</code> executes, the flow of execution is redirected to the injected shellcode.</p>



<p>This shellcode is responsible for loading the EDR’s detection measures. Figure 5 depicts the call stack of the shellcode that loads the EDR’s hooking DLL. In the callstack, we can see that the root function is unbacked, meaning it’s not part of a legitimate module, which indicates it has been injected into the process.</p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4.png"><img loading="lazy" decoding="async" width="1024" height="143" src="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-1024x143.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-1024x143.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure4-300x42.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure4.png 1194w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 4: <code>LdrLoadDll</code>‘s initial bytes have been replaced with a jump instruction to the EDR’s shellcode</em></p>



<figure><a href="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5.png"><img loading="lazy" decoding="async" width="1024" height="87" src="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-1024x87.png" alt="" srcset="https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-1024x87.png 1024w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure5-300x25.png 300w, https://www.outflank.nl/wp-content/uploads/2024/10/Figure5.png 1499w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p><em>Figure 5: Callstack of the EDR’s shellcode loading it’s hooking DLL</em></p>



<p>The injected shellcode writes the path and name of the hooking DLL into <code>rcx</code> and <code>r9</code> (following the x64 fastcall convention) and then invokes <code>LdrLoadDll</code>. Once the hooking DLL is loaded, its entry point (e.g. <code>DllMain</code>) is executed, which is responsible for initiating the hooks on critical functions. After <code>LdrLoad</code> completes, the shellcode removes the inline hook and resumes the process’s normal execution flow. From this point, the EDR can intercept API calls in real-time and monitor the process.</p>



<p>In the call graph, we can observe the first time <code>LdrLoadDll</code>, <code>LdrpLoadDll</code>, and <code>NtContinue</code> execute. At one of these points, depending on the specific EDR, the EDR’s detection measures (e.g., hooking DLL) are loaded.</p>



<p>For EDRs hooking <code>NtContinue</code>, techniques like Early Bird APC and Early Cascade Injection preempt the EDR’s detection measures. This means that the malicious code (e.g. implant) runs before the detection measures are loaded. In the call graph, we can see that <code>NtTestAlert</code> is executed before <code>NtContinue</code>. Since <code>NtTestAlert</code> empties the APC queue, it ensures that the malicious code runs before the EDR’s detection measures are active.</p>



<p>For EDRs hooking <code>LdrLoadDll</code> and <code>LdrpLoadDll</code> the EDR takes control early in the process, at the loading of <code>kernel32.dll</code>, which is before <code>g_pfnSE_DllLoaded</code>. This allows the EDR to gain control over the process before we do. However, as we can see in the call graph <code>g_pfnSE_DllLoaded</code> provides us with control before the initialization of the EDR, at that point we take control. This means that, despite the EDR taking control first, we can still disrupt the initialization of its detection measures as we can take control before the DLL initializes, preventing the EDR from loading them.</p>



<p>We also observed that most EDRs, through the shellcode, initially load <code>kernel32.dll</code> and <code>kernelbase.dll</code>, following the normal execution flow. Afterward, they load their hooking DLL via <code>LdrLoadDll</code>. Remember that <code>g_pfnSE_DllLoaded</code> is executed during the initialization part of <code>LdrLoadDLL</code>, in this case for <code>kernelbase.dll</code>. That is well-before the EDR’s detection measures are loaded by the shellcode. In theory, at this stage, we can remove the hook on <code>LdrLoadDll</code>, revert to the original code path for loading <code>kernel32.dll</code>, and proceed with execution, bypassing the EDR’s loading process.</p>



<p>There are likely numerous ways to prevent user-mode EDR detection measures using the callback pointers discussed in this blog. We’ve presented one potential approach, which could be integrated into Early Cascade Injection by leveraging the <code>g_pfnSE_DllLoaded</code> callback pointer. This would allow an implant injected via Early Cascade Injection to run more stealthily, further evading EDR detection.</p>



<h2>Conclusion</h2>



<p>In this blog, we explored how a process is created in Windows, focusing on the user-mode part of process creation. We presented a call graph that outlines the key events during process creation. We then examined how Early Bird APC Injection works and interacts with the user-mode part, specifically when the queued APC is executed. After that, we discussed EDR-Preloading, which showed us how we can achieve code execution during process creation simply by overwriting a pointer. This led us to further investigate and discover a new pointer. However, it wasn’t possible to execute fully functional code through it. By combining the APC queuing element of Early Bird APC with the new pointer, inspired by EDR-Preloading, we developed and explained Early Cascade Injection. Finally, we highlighted the key features of this technique. I hope you found the call graph as informative as we did – providing an overview of the process creation, revealing the timing of EDR security measures, and showing how Early Cascade Injection interacts with process creation.</p>



<h2>Implementation in Outflank Security Tooling</h2>



<p>Due to the strong OPSEC properties of this research, we need to prevent misuse and thus will not make the source code of this project public. However, Early Cascade Injection and all other parts of this research are already available for our vetted <a href="https://outflank.nl/ost">Outflank Security Tooling</a> (OST) community. Consider scheduling an <a href="https://www.outflank.nl/demo-request/">expert-led demo</a> to learn more about Early Cascade and the other diverse offerings in OST. Or, if you’re ready to take the next step, request a quote to start the purchase process.</p>







<p>Thank you for reading this blog post, and we hope you learned something new!</p>



<h2>References</h2>



<p>[1] <a href="https://www.malwaretech.com/2024/02/bypassing-edrs-with-edr-preload.html">Bypassing EDRs With EDR-Preloading – Marcus Hutchins</a><br>[2] <a href="https://learn.microsoft.com/en-us/windows/win32/procthread/process-creation-flags">Process Creation Flags – MSDN</a><br>[3] <a href="https://blogs.blackberry.com/en/2017/10/windows-10-parallel-loading-breakdown">Windows 10 Parallel Loading Breakdown – BlackBerry</a><br>[4] <a href="https://www.cyberbit.com/endpoint-security/new-early-bird-code-injection-technique-discovered/">New ‘Early Bird’ Code Injection Technique Discovered – Cyberbit</a><br>[5] <a href="https://www.theverge.com/2024/9/12/24242947/microsoft-windows-security-kernel-access-features-crowdstrike">Microsoft is building new Windows security features to prevent another CrowdStrike incident – The Verge</a><br>[6] <a href="http://undocumented.ntinternals.net/index.html?page=UserMode%2FUndocumented%20Functions%2FAPC%2FNtQueueApcThread.html">NtQueueApcThread – NTAPI Undocumented Functions – NTInternals</a><br>[7] <a href="https://github.com/aahmad097/AlternativeShellcodeExec">AlternativeShellcodeExec – aahmad097</a><br>[8] <a href="https://learn.microsoft.com/en-us/windows/win32/sync/critical-section-objects">Critical Section Objects – MSDN</a><br>[9] <a href="https://elliotonsecurity.com/what-is-loader-lock/">What is Loader Lock? – Elliot Killick</a><br>[10] <a href="https://br-sn.github.io/Removing-Kernel-Callbacks-Using-Signed-Drivers/">Removing Kernel Callbacks Using Signed Drivers – br-sn</a><br>[11] <a href="https://techcommunity.microsoft.com/t5/ask-the-performance-team/demystifying-shims-or-using-the-app-compat-toolkit-to-make-your/ba-p/374947">Demystifying Shims – or – Using the App Compat Toolkit to make your old stuff work with your new stuff – MSDN</a></p>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenCoder: Open-Source LLM for Coding (447 pts)]]></title>
            <link>https://arxiv.org/abs/2411.04905</link>
            <guid>42095580</guid>
            <pubDate>Sat, 09 Nov 2024 17:27:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2411.04905">https://arxiv.org/abs/2411.04905</a>, See on <a href="https://news.ycombinator.com/item?id=42095580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+S" rel="nofollow">Siming Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+T" rel="nofollow">Tianhao Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J+K" rel="nofollow">Jason Klein Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hao,+J" rel="nofollow">Jiaran Hao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+L" rel="nofollow">Liuyihan Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y" rel="nofollow">Yang Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J" rel="nofollow">J. Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J" rel="nofollow">J.H. Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C" rel="nofollow">Chenchen Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chai,+L" rel="nofollow">Linzheng Chai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+R" rel="nofollow">Ruifeng Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z" rel="nofollow">Zhaoxiang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+J" rel="nofollow">Jie Fu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Q" rel="nofollow">Qian Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+G" rel="nofollow">Ge Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z" rel="nofollow">Zili Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qi,+Y" rel="nofollow">Yuan Qi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y" rel="nofollow">Yinghui Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+W" rel="nofollow">Wei Chu</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2411.04905">View PDF</a>
    <a href="https://arxiv.org/html/2411.04905v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent <a href="http://systems.while/" rel="external noopener nofollow">this http URL</a> open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Linzheng Chai [<a href="https://arxiv.org/show-email/c945dd3e/2411.04905" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 7 Nov 2024 17:47:25 UTC (25,625 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When machine learning tells the wrong story (145 pts)]]></title>
            <link>https://jackcook.com/2024/11/09/bigger-fish.html</link>
            <guid>42095302</guid>
            <pubDate>Sat, 09 Nov 2024 16:38:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jackcook.com/2024/11/09/bigger-fish.html">https://jackcook.com/2024/11/09/bigger-fish.html</a>, See on <a href="https://news.ycombinator.com/item?id=42095302">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post">
  
  <p>
    Oxford, UK — November 09, 2024
  </p>
  <p>In June 2022, three short weeks after my college graduation, I presented at ISCA, my first serious research conference.
Onstage with my co-author <a href="https://www.drean.io/">Jules Drean</a>, we gave a 15-minute talk about our hardware security research paper, <a href="https://jackcook.github.io/bigger-fish/" target="\_blank">There’s Always a Bigger Fish: A Clarifying Analysis of a Machine-Learning-Assisted Side-Channel Attack</a>, that had taken the majority of my last two years at MIT to complete.
In hindsight, that talk was the culmination of one of my proudest accomplishments from my time in college.
The paper has since won awards and recognition, including first place in <a href="https://www.intel.com/content/www/us/en/security/security-practices/security-research/hardware-security-academic-award.html" target="\_blank">Intel’s 2024 Hardware Security Academic Award</a>,<sup><a href="#fn1" id="ref1">1</a></sup> and inclusion in the 2023 edition of <a href="https://ieeexplore.ieee.org/document/10167515" target="\_blank">IEEE Micro Top Picks</a>, which highlights 12 of the best papers in computer architecture each year.</p>

<p>Since our talk, every few months, I’ve gotten the urge to write a blogpost about the paper.
Among other cool things described in the paper, we…</p>

<ul>
  <li>Implemented a powerful machine-learning-assisted side-channel attack that can be pulled off in any modern web browser</li>
  <li>Demonstrated for the first time in the literature that <a href="https://en.wikipedia.org/wiki/Interrupt" target="\_blank">system interrupts</a>, a low-level mechanism that your operating system uses to interact with hardware devices, can leak information about user activity</li>
  <li>Learned a valuable lesson about the dangers of applying machine learning toward hardware security research</li>
</ul>

<p>I think some of these lessons are widely applicable, even outside of hardware security research.
But each time I’ve started writing, a few hundred words into the draft, I’ve stopped writing and put the post away.
For some reason, it always felt wrong.
Two years later, no blogpost exists.
If I could write about <a href="https://jackcook.com/2024/02/23/mamba.html" target="\_blank">other people’s research</a>, why couldn’t I write about my own?
I only recently figured out why.</p>

<p>As I’ll get into, one reason this is a hard post to write is because there’s a lot going on in this research paper.
I like writing for a general technical audience, and I need to explain a lot of background before I can get to the good stuff.
The paper also has two competing stories: one about how machine learning models can be used to attack web browsers, and another about how these same models are often misunderstood, leading them to be applied incorrectly.
But there’s also a third story embedded in this paper, about how this paper completely altered the trajectory of my life.
This is a post about machine learning, computer architecture, and more, but also about myself, how I got into research and academia, and how one great mentor can change everything.</p>

<hr>

<figure id="cpu-selection">
  <h3>Select your CPU</h3>
  <div>
    <p>
      This post contains demos and details that can be customized to your CPU. If you know your processor, you may search for it below:
    </p>
    <p>
      Otherwise, feel free to pick a sample CPU from the list below:
    </p>
    
  </div>
</figure>

<hr>

<p>It was September 2020, the start of my junior year at MIT, and I had just enrolled in <a href="https://csg.csail.mit.edu/6.S983/" target="\_blank">Secure Hardware Design</a>, a graduate seminar class that was being offered for the first time.
Nearly all of my past experience had been in software, and I saw the class as a great opportunity to branch out and learn new things.</p>

<p>I quickly found out I was in over my head.
Each week, we would read and discuss two recent hardware security research papers as a class.
Of the 10 or so students who came each week, I was one of only two undergrads, and half of the PhD students weren’t even enrolled in the class—they just wanted to discuss the papers for fun.
I felt that I had very little to contribute to the discussions compared to the wealth of experience everyone else brought to the table, but I was happy to listen nonetheless.</p>

<p>Alongside the paper discussions, each student was supposed to complete a final project.
I met with our professor, <a href="https://people.csail.mit.edu/mengjia/" target="\_blank">Mengjia Yan</a>, early on in the semester to discuss where I should start.
I told her about my prior experience with web development and machine learning, and she suggested I try to reimplement a recently published website fingerprinting attack, which relies on machine learning to exploit weaknesses in hardware.
Her gut told her that there was something wrong with the state-of-the-art research in that area, but she couldn’t put her finger on what it was.</p>

<h2 id="a-primer-on-side-channel-attacks">A primer on side-channel attacks</h2>

<p>In a perfect world, applications on your computer should always operate independently of each other.
For example, if you have Netflix and Spotify open at the same time, Spotify should never be able to know what movie you’re watching.
In practice, this is mostly kept true because of a mechanism known as <a href="https://en.wikipedia.org/wiki/Process_isolation" target="\_blank">process isolation</a>, through which your operating system keeps applications separate from each other by making them use separate resources.
For example, applications are given their own private memory space to store their data, and are restricted from accessing memory that belongs to another process.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/Virtual_memory.svg">
  <figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Virtual_memory#/media/File:Virtual_memory.svg" target="_blank">Ehamberg, 2009</a></figcaption>
</figure>

<p>However, process isolation is highly imperfect.
At the end of the day, you’re running Netflix and Spotify on the same computer, and they still share a lot of resources.
For example, they both use the same network card to fetch data from the Netflix and Spotify servers.
They use the same graphics card to display data on your screen.
They use the same processor to run their application code.
And so on and so forth.</p>

<p>Consider why this type of resource sharing, however limited, might compromise security.
Let’s say your roommate recently admitted to you that they’ve watched 20 movies in the last week.
They know they’re addicted and need to focus on their work, but they can’t stop watching movies, and they need your help.
To hold them accountable, it’s now up to you to figure out when they’re watching a movie, so you can go knock on their door and tell them to stop.
There are two important facts that will help you solve this problem:</p>

<ol>
  <li>Movies are very large files. Streaming one typically creates a lot of network activity.</li>
  <li>You both share the same Wi-Fi router.</li>
</ol>

<p>One solution might involve periodically downloading a large file and measuring how long each download takes.
If you notice that a download takes longer than usual, and this pattern holds for some period of time, you might begin to suspect that something is up.
Why is the download taking longer? Have you ever asked a relative to stop watching a movie because you needed to take an important call on the same Wi-Fi network, or turned off video on your Zoom call so that the audio would cut out less often? Your Wi-Fi router can only handle so much activity at once, and since you and your roommate are sharing the same router, your network activity is impacted by theirs.
You should go tell your roommate to stop watching their 21st movie of the week.</p>

<p>In this way, your Wi-Fi router creates a <em>side channel</em>, because it essentially reveals information about its users by accident.
This side channel is extremely crude, only able to transmit one bit of information at a time (1 = roommate is watching movie, 0 = roommate is not watching movie), but it still illustrates a very important concept: nearly every shared resource reveals information about its users by accident.
And when it comes to modern computers, there are lots of shared resources, which create lots of side channels.</p>

<p>Some notable examples of side channels that we learned about in Mengjia’s class blew my mind.
For example, it’s been known for some time that changes in a computer’s power consumption can be used as a side channel.
In the figure below, you can see a device’s power consumption, shown in yellow, reliably increasing and decreasing during RSA encryption, enabling you to identify the 1s and 0s that make up a user’s encryption key.
An <a href="https://link.springer.com/book/10.1007/978-0-387-38162-6" target="\_blank">entire book</a> was written about this type of attack over 15 years ago!</p>

<figure>
<img src="https://jackcook.com/img/blog/bigger-fish/power-attack.png">
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Power_analysis#/media/File:Power_attack_full.png" target="_blank">Audriusa, 2010</a></figcaption>
</figure>

<p>Similarly, every device, from your laptop to your contactless credit card, emits electromagnetic radiation, since any wire carrying a current creates a small magnetic field (remember the right-hand rule?).
In a similar way to the power-analysis attack described above, you can monitor changes in this EM signal to reverse-engineer encryption keys, user activity, and more.
I could go on and on.
In most cases, though, these types of attacks are impractical to pull off—you need specialized equipment to monitor electromagnetic signals, and it’s hard to look at this signal and tell if someone is encrypting a message or just watching <a href="https://www.youtube.com/watch?v=pRpvdcjkT3k" target="\_blank">cat videos on YouTube</a>.
However, some side-channel attacks are much easier to pull off.
Let’s get back to the topic at hand.</p>

<h2 id="what-is-website-fingerprinting">What is website fingerprinting?</h2>

<p>Now, imagine you have two tabs open in your web browser.
One is a big social media platform that wants to collect as much data about you as possible in order to serve better targeted advertisements.
I don’t want to name an actual social media platform, so I’ll just make one up: let’s call it Facebook.<sup><a href="#fn2" id="ref2">2</a></sup>
In the other tab, you have a website open that you’d prefer “Facebook” didn’t know about—maybe it reveals something about your identity (e.g. <a href="https://en.wikipedia.org/wiki/Truth_Social" target="\_blank">Truth Social</a>) or something you’d otherwise be ashamed for other people to know about (e.g. Taylor Swift fan page).
How could Facebook figure out what website you have open in this other tab?</p>

<p>Facebook could turn to website fingerprinting to solve this problem.
When Mengjia and I discussed my final project, she pointed me to a paper by <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="\_blank">Shusterman et al.</a> that explores this exact setup, where one website attempts to identify the website open in another tab out of a set of 100 possible websites.
They claimed to do this by taking advantage of a widely-studied side channel: the CPU cache.
Their idea worked as follows: while your computer loads a website (or does anything, for that matter), it saves lots of data in small CPU caches in order to reduce the number of times it needs to load data from RAM or from your hard drive, both of which are much slower.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cpu-cache.svg">
  <figcaption>Very simplified explanation of where CPUs get their data.</figcaption>
</figure>

<p>However, caches, like your Wi-Fi router, are shared resources, and they are generally very small.
If RAM is a library that holds hundreds of thousands of books, the CPU cache might be a single bookshelf by the front door with a few of the most popular titles.
When someone requests a book, if it’s on that bookshelf, they will get their book almost instantly.
If not, they can go look for it in the library, but it will take much longer.
The problem here is that your bookshelf will inevitably fill up, because it holds a fraction of the contents of the library.
If you have two programs running on your computer, they will need to share the same CPU cache, which is also being used by your operating system and whatever other applications you have open at the time.</p>

<p>Now, If you think about it, when you open any website, perhaps you’ve opened <a href="https://jackcook.com/" target="\_blank">jackcook.com</a>, it will load in basically the same way every time.
It will reference the same scripts, images, and stylesheets, and it will render the same content on the page.
And in theory, this should translate to very similar cache activity each time the website loads.
This is where website fingerprinting comes in.
To try to take advantage of this CPU-cache side channel, Shusterman et al. designed an attacker that worked as follows:</p>

<ol>
  <li>Allocate an array that’s the same size as the CPU cache<sup><a href="#fn3" id="ref3">3</a></sup> and fill it with ones.</li>
  <li>While a website loads in another tab, every 2 milliseconds, measure how long it takes to loop over the array and access every element.<sup><a href="#fn4" id="ref4">4</a></sup></li>
  <li>Repeat step 2 for 15 seconds.</li>
</ol>

<p>By the end of this process, making one measurement every 2 milliseconds for 15 seconds, the attacker will have 7500 measurements in total.
Now, let’s consider for a moment why an individual measurement might be higher or lower, and remember that you’re opening another website while this happens, the one you don’t want “Facebook” to know about.</p>

<p>Facebook, which I will now refer to as the <em>attacker</em>, and the other website you’re loading, which I will now refer to as the <em>victim</em>, share <em>a lot</em> of resources on your computer.
One of these shared resources is the CPU cache: using the analogy from earlier, imagine the attacker and victim each have their own libraries with their own data, but they have to share space on the same small bookshelf to cache this data.
<cpu data-value="name" data-prefix="Your " data-suffix=" has" data-requiredvalue="cache">A typical CPU might have</cpu> a cache that can hold <cpu data-value="cache" data-requiredvalue="cache">around 6-8 MB</cpu> of data, enough to hold about <cpu data-value="cache/4" data-requiredvalue="cache">2 million</cpu> integers.
When the attacker then creates its <cpu data-value="cache" data-requiredvalue="cache">cache-sized</cpu> array, it will fill this cache, evicting all of the data it currently holds.
But at some point, as the victim website loads, it will have data of its own, which the CPU will put in its cache, evicting some of the attacker’s data.</p>

<p>But then, 2 milliseconds later, the attacker will read all of its data again, and the CPU will first look for this data in the cache.
If any of the attacker’s data was removed from the cache in the last 2 milliseconds, it will take a tiny bit longer to do this, because the CPU will need to spend time looking for it in RAM.
And because the attacker is timing how long this process takes, it will notice this tiny discrepancy.
This process will then repeat itself 7,500 times over the course of the next 15 seconds.
And since websites generally load in the same way every time, these measurements should reflect the unique cache access patterns of the website you’ve opened.
In the figure below, you can see how these measurements look for three different websites, where time increases to the right, and darker colors indicate slower cache accesses.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/shusterman-fig3.png">
  <figcaption>Figure 3 from <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="_blank">Shusterman et al., 2019</a></figcaption>
</figure>

<p>Notice how within each website, the traces look very similar, but across different websites, the traces look very different.
This is where website fingerprinting gets its name: each collection of latency measurements, which we call a <em>trace</em>, essentially acts as a “fingerprint” that can be used to identify the website that was loaded.
If I gave you a new trace from one of these websites and asked you to tell me which one it came from, you could probably compare it to the traces above and give me the answer.</p>

<p>Click the start button below to see this process in action by recording <smallscreen data-value="50">100</smallscreen> milliseconds of cache latency measurements on your own device.
Then, hover over or tap on the values in the trace to see what each measurement represents.
This demo works best in Chrome, but it should work fine in other browsers as well.<sup><a href="#fn5" id="ref5">5</a></sup>
<cpu data-value="name" data-prefix="The slider below has been automatically set to your " data-suffix="’s cache size of "></cpu><cpu data-value="cache" data-suffix=", but if">If</cpu> all of your bars have the same color, try adjusting the cache size.</p>

<figure>
  
</figure>

<p>Now, let’s go back to the example from earlier.
Imagine you’re “Facebook,” and you want to identify the websites your users are opening in their other tabs.
How would you do this?
First, you might collect a bunch of traces while opening a lot of different websites, similar to the ones above, which you can use as training data.
With this training data, you can train a machine learning model, which can reliably predict the website that was opened while recording one of these traces.
Then, when users open “Facebook” in the future, “Facebook” can record new traces, feed them into this model, and see if there’s a match.</p>

<p>This is what Shusterman et al. did: in their paper, they collected 100 traces for 100 different websites, yielding a labeled dataset of 10,000 traces, and used it to train a few machine learning models.
They then repeated this process across several different web browsers and operating systems, and found that it was possible to identify the website that was opened out of a set of 100 possible websites with up to 91.4% accuracy.<sup><a href="#fn6" id="ref6">6</a></sup>
Pretty cool, and a little scary!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Web Browser</th>
          <th>Operating System</th>
          <th>Classification Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Chrome</td>
          <td>Linux</td>
          <td>91.4% ± 1.2%</td>
        </tr>
        <tr>
          <td>Chrome</td>
          <td>Windows</td>
          <td>80.0% ± 1.6%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Linux</td>
          <td>80.0% ± 0.6%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Windows</td>
          <td>87.7% ± 0.8%</td>
        </tr>
        <tr>
          <td>Safari</td>
          <td>macOS</td>
          <td>72.6% ± 1.3%</td>
        </tr>
        <tr>
          <td>Tor Browser</td>
          <td>Linux</td>
          <td>46.7% ± 4.1%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>An abbreviated version of Table 2 from Shusterman et al., 2019</figcaption>
</figure>

<h2 id="my-final-project">My final project</h2>

<p>I spent the next few weeks trying to replicate these results from Shusterman et al.’s paper.
Going into this project, I wasn’t too sure what I would find.
Mengjia and I decided I should look at website fingerprinting because I had past experience with web development and machine learning, and not because there was an obvious unresolved research question.
I collected a bunch of data and tried to distinguish between four websites at a time, which was fairly straightforward.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/initial-eval.png">
  <figcaption>My very first results, showing 40 traces per website recorded while opening cnn.com, msnbc.com, nytimes.com, and apple.com.</figcaption>
</figure>

<p>Even if you just looked at these traces with your own eyes, there are clear patterns that distinguish these websites from each other.
If I gave you a new trace, you could probably tell me which website it came from.
But of course, models do this faster and more reliably.
By training a simple Random Forest classifier on these traces, I was able to predict the correct website with 98% accuracy.
Scaling this up to 10 websites initially gave me 75% accuracy, but over the next several weeks, I kept experimenting and making improvements until I could reliably classify 10 websites, then 50, then 100.</p>

<p>While many of these experiments led to small improvements, one stood out above the rest.
Remember that in Shusterman et al.’s paper, the attacker works by repeatedly measuring how long it takes to access all of the elements in its array.
For one of my experiments, I tried a different approach: I simply made the attacker count as fast as it could (like literally, execute <code>value++</code> over and over again).
In the demo below, click Start and hover over or tap on the final trace to see how high your computer can count in 5 seconds.</p>

<figure>
  
</figure>

<p>Of course, this number doesn’t tell us too much on its own.
However, if we do this repeatedly, we can see how fast your computer can count over time.
Now see how high your computer can count in one second, five times in a row.</p>

<figure>
  
</figure>

<p>As we make this interval smaller, we can see how this plays out at a finer timescale.
Again, hover over or tap on the values in these traces to see what each measurement represents.</p>

<figure>
  
</figure>

<p>Now, play with the demo above to see if you can get the trace to change.
If you do nothing, the values should stay relatively consistent over time.
But if you click start and then do something else on your device, such as opening a new tab or resizing the browser window, you should see this reflected in the trace.
If you’re having trouble with this, you can see an example below:</p>

<figure>
  <video controls="" preload="auto" data-setup="{&quot;fluid&quot;: true}">
    <source src="https://jackcook.com/img/blog/bigger-fish/google-maps-demo.mp4" type="video/mp4">
    <p>
      To view this video please enable JavaScript, and consider upgrading to a web browser that
      <a href="https://videojs.com/html5-video-support/" target="_blank">supports HTML5 video</a>
    </p>
  </video>
  <figcaption>When I open Google Maps in a new tab, the trace values drop. This enables the attacker to notice that something else is happening on my computer, even though process isolation is supposed to make this impossible.</figcaption>
</figure>

<p>Take a second to think about what’s happening here.
In the video above, over the course of 5 seconds, my computer tried to count as fast as it could.
Every 100 milliseconds, we saved the value it reached, and made it start over again from zero.
As was the case with the cache-based attacker, this trace is sensitive to things that happen outside of the same browser tab: when I opened Google Maps, my computer couldn’t count as high.</p>

<p>This shows that this counter trace can essentially be interpreted as a signal.
We can improve the resolution of this signal by saving the value more often: while I saved the value every 100 milliseconds in my demo to illustrate the concept of our counting-based attack, in our paper, we save the value every 5 milliseconds to get more information in a fixed amount of time.
In the demo above, you can change this value, which we call the <em>period length</em>, to record counter traces at a higher or lower resolution.
A simple version of the trace collection code is shown here in Python, which you can read or try for yourself:</p>

<pre><code>import time</code>
<code></code>
<code>PERIOD_LENGTH = 0.1  # 0.1 second</code>
<code>TRACE_LENGTH = 5  # 5 seconds</code>
<code>start = time.time()</code>
<code>counter = 0</code>
<code>trace = []</code>
<code></code>
<code>while len(trace) &lt; int(TRACE_LENGTH / PERIOD_LENGTH):</code>
<code>    if time.time() - start &gt;= PERIOD_LENGTH:</code>
<code>        trace.append(counter)</code>
<code>        start = time.time()</code>
<code>        counter = 0</code>
<code>        continue</code>
<code>    </code>
<code>    counter += 1</code>
<code></code>
<code>print(trace)</code>
</pre>

<hr>

<p>I ran the experiment, training a model on these counter traces rather than using the cache-latency traces we discussed earlier.
And as it turned out, models trained on the counter traces were more accurate at predicting the website that was opened!</p>

<p>This was a really exciting result!
The theory behind Shusterman et al.’s paper was that their attacker leveraged a CPU-cache-based side channel by repeatedly evicting data from the cache and measuring cache access latency.
But my new attacker simply incremented a counter, without needing to repeatedly evict data from the cache, and it appeared to yield an even better signal.
There was no solid theory behind it.
Why did it work so well?</p>

<p>I presented my findings during the final lecture of Secure Hardware Design, and Mengjia was even more excited than I was.
A few days after that lecture, she sent me the following email:</p>

<blockquote>
  <p>Hi Jack,</p>

  <p>Great job on the 6.888 course project!
As I have commented during the lecture, your work is really impressive.</p>

  <p>I am wondering whether you will be interested in continuing the project in my group.
My group offers [undergraduate research] opportunities.
I am very curious about why your new measurement approach could do a better job than cache-contention attacks.
By digging deep into it and figuring out the reason, we can potentially find something unknown to the public related to how the browser or hardware processor works, and get a potential publication in a top security or computer architecture conference.</p>

  <p>If you are interested, we could chat about details after your finals and the holiday season.</p>

  <p>Thanks,
Mengjia</p>
</blockquote>

<p>I didn’t realize it at the time, but I had accidentally discovered a new side channel.</p>

<h2 id="investigating-the-mystery-side-channel">Investigating the mystery side channel</h2>

<p>I called Mengjia several weeks later, after the end of my <a href="https://jackcook.com/2022/04/11/tmobile.html" target="\_blank">6-week roadtrip across the US</a>, and joined her lab in February 2021.
She introduced me to <a href="https://www.drean.io/" target="\_blank">Jules Drean</a>, a talented graduate student who studies micro-architectural side-channel attacks such as this one, and the three of us immediately got to work trying to understand why my new attacker worked so well.
As it turned out, the picture was complicated, but one thing was clear: machine learning models need to be used carefully.</p>

<p>This would become the biggest lesson of our eventual research paper: in a machine-learning-assisted side-channel attack such as this one, if a model can reliably predict user activity, it proves the <em>presence</em> of a signal, but not the <em>cause</em> of that signal.<sup><a href="#fn7" id="ref7">7</a></sup>
Even though Shusterman et al.’s model could identify the correct victim website 91.4% of the time, that didn’t necessarily mean that their model was picking up on contention over the CPU cache.
And the implications of getting this wrong can be big: researchers look at papers describing attacks when building defenses that make our computers safer.
A more thorough analysis was needed in order to properly identify the side channel, which we set out to provide.</p>

<p>We started by replicating the larger experiments from Shusterman et al.’s paper to understand the similarities and differences between our counting-based attacker and their cache-based attacker.
It turned out that when asked to identify the correct victim website out of 100, my counting-based attacker achieved higher accuracy in nearly every experimental configuration we tried.</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Web Browser</th>
          <th>Operating System</th>
          <th>Cache Attack</th>
          <th><strong>Counting Attack</strong></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Chrome</td>
          <td>Linux</td>
          <td>91.4% ± 1.2%</td>
          <td><strong>96.6%</strong> ± 0.8%</td>
        </tr>
        <tr>
          <td>Chrome</td>
          <td>Windows</td>
          <td>80.0% ± 1.6%</td>
          <td><strong>92.5%</strong> ± 1.0%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Linux</td>
          <td>80.0% ± 0.6%</td>
          <td><strong>95.3%</strong> ± 0.7%</td>
        </tr>
        <tr>
          <td>Firefox</td>
          <td>Windows</td>
          <td>87.7% ± 0.8%</td>
          <td><strong>91.9%</strong> ± 1.2%</td>
        </tr>
        <tr>
          <td>Safari</td>
          <td>macOS</td>
          <td>72.6% ± 1.3%</td>
          <td><strong>96.6%</strong> ± 0.5%</td>
        </tr>
        <tr>
          <td>Tor Browser</td>
          <td>Linux</td>
          <td>46.7% ± 4.1%</td>
          <td><strong>49.8%</strong> ± 4.2%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>A simplified version of Table 1 from our paper, Cook et al., 2022.<sup><a href="#fn8" id="ref8">8</a></sup></figcaption>
</figure>

<p>For some configurations in particular, this discrepancy in performance was huge: in Safari on macOS, the cache attack achieved just 72.6% accuracy, while our counting attack achieved 96.6% accuracy on the same task.
These results really seemed to suggest that the cache may have been interfering with our signal rather than helping it!
But again, we couldn’t say this for sure without a more thorough analysis.
So over the next several weeks we applied the scientific method, modifying one variable at a time, collecting new data, and training new models to eliminate different hypotheses about what was going on.
First, we established a baseline, where we simply ran the attacker in its default configuration without any modifications, and found that we could identify the correct website out of 100 with 95.2% accuracy.</p>

<h3 id="hypothesis-1-cpu-frequency-scaling">Hypothesis 1: CPU frequency scaling</h3>

<p>We then tested our first hypothesis, which was that our counting-based attack was taking advantage of a CPU-frequency-scaling side channel.
CPUs operate at a set frequency, which reflects the amount of work they can do per second.
For example, <cpu data-value="name" data-prefix="your " data-suffix=" operates ">a typical CPU might operate</cpu> at <cpu data-value="frequency">3.0 GHz</cpu>, meaning it completes about <cpu data-value="frequency-math">3 × 10<sup>9</sup> cycles</cpu> per second.
However, modern CPUs adjust this frequency based on the workload demanded of them, speeding up when there’s more work to do, and slowing down when there’s less, in order to save energy.<sup><a href="#fn9" id="ref9">9</a></sup>
This is why your computer might get hot or turn on a fan when you have a lot of applications open: when your CPU scales its frequency up, it performs more operations in a fixed amount of time, generating more heat due to the increased electrical activity.</p>

<p>We hypothesized that while the victim website is loading, the CPU would change its frequency often, enabling it to complete variable amounts of work over time.
And for our attacker, completing more work should enable it to count faster, yielding higher values at specific points in time while the other website loads, or vice versa.
But it turned out this wasn’t the case: we went into <a href="https://en.wikipedia.org/wiki/BIOS" target="\_blank">BIOS</a>, disabled frequency scaling, collected more data, and trained another model.
Compared to the baseline experiment, our accuracy dropped by just one point to 94.2%, indicating that changes in counter values can’t really be explained by changes in CPU frequency.
We started filling out a table to keep track of our results:</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td><strong>+ Disable CPU frequency scaling</strong></td>
          <td><strong>94.2%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
</figure>

<h3 id="hypothesis-2-cpu-core-contention">Hypothesis 2: CPU core contention</h3>

<p>Next, we thought our counting-based attack might have been exploiting a CPU-core-contention side channel.
Your <cpu data-value="name">CPU</cpu> has <cpu data-value="cores">several</cpu> cores, <cpu data-hideifselected="true">often around 4 or 8, </cpu>each of which can execute a fixed number of instructions per second.
However, there are generally more than <cpu data-value="cores">4 or 8</cpu> processes running on your computer, which inevitably means that some processes will have to run on the same core and compete for time.
If the attacker and victim were, by chance, scheduled on the same core, the attacker’s counter values should decrease when the victim tab spends more time loading, providing enough of a signal to tell victim websites apart.</p>

<p>But this was also wrong.
With CPU frequency scaling still disabled, we ran an experiment with Linux’s <code>taskset</code> command, which can be used to force a process to execute on a specific CPU core, and ensured that the attacker and victim tabs ran on separate cores.
Even when the attacker and victim were isolated in this way, the attacker still seemed to have plenty of information: it could still pick the correct victim website out of 100 with 94.0% accuracy.</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td>+ Disable CPU frequency scaling</td>
          <td>94.2%</td>
        </tr>
        <tr>
          <td><strong>+ Pin attacker and victim to separate cores</strong></td>
          <td><strong>94.0%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
</figure>

<p>At this point, we were a little stumped.
CPU caches, frequency, and core resource contention are relatively well-studied side channels, and we had already ruled out all three of them.
We sent out some emails and gave presentations within the department to see if anyone had ideas.
Fortunately, <a href="https://www.fintelia.io/" target="\_blank">Jonathan Behrens</a> answered our call.</p>

<h3 id="hypothesis-3-system-interrupts">Hypothesis 3: System interrupts</h3>

<p>After some further discussion, we hypothesized that our counting-based attacker might be exploiting a system-interrupt-based side channel.
This idea was a bit out of left field: we couldn’t find any prior research that had studied the security properties of system interrupts.
In hindsight, this is a little surprising considering how pervasive they are: your operating system uses system interrupts constantly to communicate with hardware devices, such as your keyboard, mouse, and display.</p>

<p>Compared to software, hardware is fairly unpredictable: anything can happen at any moment.
For example, your operating system has no idea when you’re next going to hit a key on your keyboard.
But once you do, your operating system needs to act as quickly as possible.
It generates a <em>system interrupt</em>, which it dispatches to one of your <cpu data-value="cores" data-suffix=" "></cpu>CPU cores.
Then, once the interrupt arrives at the core, any program currently executing on that core is halted immediately in order to process the interrupt.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig1.png">
  <figcaption>Figure 1 from our paper. As soon as an interrupt is received on the same core as the attacker, the attacker is halted until the interrupt has been processed.</figcaption>
</figure>

<p>We thought that while the victim tab was loading, it would trigger many different kinds of interrupts: from your network card as it uploads and downloads data, from your graphics card as it renders content on your screen, and so on and so forth.
If any of these interrupts is processed on the same CPU core as the attacker program, your operating system would need to halt the attacker each time, preventing it from counting until each interrupt handler (shown above in yellow) has finished processing.
And remember, in the demo from earlier, we learned that less time spent counting leads to lower counter values, which can be highly indicative of activity happening elsewhere on your computer.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig3.png">
  <figcaption>Figure 3 from our paper. Even relatively small changes in counter values can reveal the victim website!</figcaption>
</figure>

<p>It turns out that on Linux, where we were running these experiments, you can monitor system interrupts very easily.
For example, if you run <code>cat /proc/interrupts</code> on Ubuntu, you should get a readout that resembles the table below:</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>ID</th>
          <th>Interrupt Type</th>
          <th>Core 1</th>
          <th>Core 2</th>
          <th>Core 3</th>
          <th>Core 4</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>16</code></td>
          <td><code>usb1</code> (Mouse)</td>
          <td>31</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>23</code></td>
          <td><code>usb2</code> (Keyboard)</td>
          <td>1943</td>
          <td>934</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>27</code></td>
          <td><code>enp2s0</code> (Network card)</td>
          <td>0</td>
          <td>376</td>
          <td>0</td>
          <td>10880</td>
        </tr>
        <tr>
          <td><code>28</code></td>
          <td><code>ahci</code> (SATA/hard drive)</td>
          <td>8201</td>
          <td>0</td>
          <td>11531</td>
          <td>0</td>
        </tr>
        <tr>
          <td><code>30</code></td>
          <td><code>i915</code> (Graphics card)</td>
          <td>0</td>
          <td>193</td>
          <td>0</td>
          <td>364</td>
        </tr>
        <tr>
          <td><code>NMI</code></td>
          <td>Local timer interrupts</td>
          <td>22059</td>
          <td>18076</td>
          <td>19010</td>
          <td>27837</td>
        </tr>
        <tr>
          <td><code>IWI</code></td>
          <td>IRQ work interrupts</td>
          <td>5794</td>
          <td>4910</td>
          <td>4950</td>
          <td>7493</td>
        </tr>
        <tr>
          <td><code>RES</code></td>
          <td>Rescheduling interrupts</td>
          <td>1400</td>
          <td>1339</td>
          <td>1359</td>
          <td>1262</td>
        </tr>
        <tr>
          <td><code>CAL</code></td>
          <td>Function call interrupts</td>
          <td>6122</td>
          <td>6547</td>
          <td>6563</td>
          <td>3100</td>
        </tr>
        <tr>
          <td><code>TLB</code></td>
          <td>TLB shootdowns</td>
          <td>295</td>
          <td>377</td>
          <td>285</td>
          <td>290</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Each row indicates a different kind of interrupt, and each column indicates the number of times that interrupt has been executed on each core since the computer started up. See a screenshot of the full readout <a href="https://jackcook.com/img/blog/bigger-fish/proc-interrupts.png" target="_blank">here</a> for more detail.</figcaption>
</figure>

<p>This table shows that many interrupts are being processed on all four cores, very likely interfering with the attacker’s counting!
So naturally, the next experiment we would like to run should isolate the attacker from these interrupts, letting the attacker count freely on one core while interrupts are processed on another core.
But after doing some research, we came across a problem.</p>

<p>Linux provides a mechanism to route certain types of interrupts, which we call <em>movable interrupts</em>, to a specific core.
These interrupts have numeric IDs in the table above, and generally come from external hardware devices, such as your keyboard and network card.
However, there are also many types of <em>non-movable interrupts</em> which can’t be routed to a specific core, meaning we can’t isolate them from the attacker.
These interrupts have three-letter IDs in the table above, and are generally used to synchronize activity between your CPU cores, which is why modern operating systems require that they be processed on all of them.
And unfortunately for us, as you can see in the table above, these non-movable interrupts make up the bulk of interrupt activity.</p>

<p>But we didn’t let this deter us.
We used Linux’s <code>irqbalance</code> command, which can be used to force certain interrupts to be processed on a specific CPU core, and routed all movable interrupts to core 1.
Building on the previous experiments, we additionally used <code>taskset</code> to force the attacker and victim to run on cores 2 and 3, while also forcing the CPU to run at a fixed frequency as we described earlier.
Even though we could only isolate movable interrupts, it seemed like we were onto something: the attacker’s accuracy dropped by nearly six points!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Isolation Mechanism</th>
          <th>Accuracy</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Default</td>
          <td>95.2%</td>
        </tr>
        <tr>
          <td>+ Disable CPU frequency scaling</td>
          <td>94.2%</td>
        </tr>
        <tr>
          <td>+ Pin attacker and victim to separate cores</td>
          <td>94.0%</td>
        </tr>
        <tr>
          <td><strong>+ Isolate movable interrupts</strong></td>
          <td><strong>88.2%</strong></td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Most of Table 3 from our paper</figcaption>
</figure>

<h3 id="hypothesis-35-non-movable-system-interrupts">Hypothesis 3.5: Non-movable system interrupts</h3>

<p>Of course, this result left us wondering what the attacker’s accuracy would be if we could isolate non-movable interrupts as well.
But again, this type of experiment is impossible: due to fundamental limitations of how operating systems are built, non-movable interrupts must be processed on every core.
If we wanted to understand the impact of these interrupts, we had to use a different approach.</p>

<p>This was where Jonathan’s expertise proved crucial: he suggested we use <a href="https://en.wikipedia.org/wiki/EBPF" target="\_blank">eBPF</a>, a low-level technology that can be used to make small changes to your operating system while it’s still running.
Among the many APIs it provides, eBPF enabled us to record two crucial things:</p>

<ul>
  <li>Every time the attacker program starts and stops</li>
  <li>Every time an interrupt handler starts and stops</li>
</ul>

<p>Remember, CPU frequency scaling is still disabled, meaning that the CPU executes a fixed number of instructions per second.
In theory, this means that if the attacker is uninterrupted, it should always be able to reach the same counter value in a fixed amount of time.
We figured that if we could record every time interval during which the attacker was interrupted, whether to run another program, to process an interrupt, or for some other unknown reason, we could compare this to every interval during which an interrupt was processed, and see if these explained the gaps in the attacker’s execution.</p>

<p>Jonathan <a href="https://github.com/jackcook/bigger-fish/blob/main/ebpf/src/main.rs" target="\_blank">wrote the code</a> to do this with eBPF, recording hundreds of these timestamps while our attacker counted away.
If we go back to the figure from earlier, we measured all of these yellow regions, during which the attacker was interrupted:</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig1.png">
  <figcaption>Still figure 1 from our paper</figcaption>
</figure>

<p>We analyzed these gaps in the attacker’s execution, hoping to get an understanding of what was going on, and it turned out that our intuition was right.
Out of all of these gaps that last at least 100 nanoseconds, we found that over 99% of them are spent processing interrupts!
This was the smoking gun we had been looking for all along!</p>

<p>Essentially, during our experiments, the attacker’s CPU core is basically only ever doing one of two things: processing the attacker’s counting code, or processing an interrupt.
And since the CPU is running at a fixed speed, the amount of time spent processing the attacker’s code should be proportional to the number of times it’s able to increment its counter.
In the figure below, you can see this for yourself: while loading a victim website, the attacker’s counter value generally goes up when less time is spent processing interrupts, and vice versa.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/interrupt-handling-time.png">
  <figcaption>A figure from our ISCA talk, showing that time spent handling interrupts time and counter values are inversely correlated.</figcaption>
</figure>

<p>Now that you understand what’s going on, I encourage you to try this demo again, seeing what happens if you do something in the middle of trace collection that triggers a bunch of interrupts.
Some suggestions include opening a new tab, pressing a bunch of buttons on your keyboard, moving your mouse around really quickly, or opening an application.</p>

<figure>
  
</figure>

<p>You can also try our online demo <a href="https://jackcook.github.io/bigger-fish" target="\_blank">here</a>, or check out our trace collection code <a href="https://github.com/jackcook/bigger-fish" target="\_blank">on GitHub</a>!</p>

<h2 id="theres-always-a-bigger-fish">There’s always a bigger fish</h2>

<p>That was a lot!
My apologies if I lost you in some of the technical details.
Let me take a step back and summarize what we did one more time:</p>

<ul>
  <li>We re-implemented a state-of-the-art cache-based website fingerprinting attack</li>
  <li>We modified it to remove cache accesses, yielding a new counting-based attacker which took advantage of some unknown side channel</li>
  <li>We ruled out several possible side channels, including CPU caches, CPU frequency, and CPU core contention</li>
  <li>We used eBPF to prove that this attack primarily leverages a system-interrupt-based side channel</li>
</ul>

<p>And through this process, we came away with two key findings:</p>

<h3 id="1-system-interrupts-leak-user-activity">1. System interrupts leak user activity</h3>

<p>This was a fairly surprising finding: the security properties of system interrupts had never been demonstrated before.
We became the first group to study this new system-interrupt-based side channel, and we likely won’t be the last: there are tons of directions for future work!
I’ll come back to this in a moment.</p>

<h3 id="2-machine-learning-assisted-side-channel-attacks-need-to-be-analyzed-carefully">2. Machine-learning-assisted side-channel attacks need to be analyzed carefully</h3>

<p>This is arguably our most important takeaway, and almost certainly the reason we ended up winning those awards from Intel and IEEE.
Machine learning models are great at finding patterns in data and can be used regardless of one’s understanding of the side channel being attacked, which leads to the development of powerful attacks that are poorly understood.</p>

<p>Without instrumenting our operating system, we could not have made many conclusions about which side channel our attack was exploiting: it’s impossible to do this with models that can only find correlations!
And it’s important to get this right—an incorrect analysis of an attack can mislead researchers hoping to build defenses, wasting valuable time and energy.</p>

<p>For example, in their paper, Shusterman et al. proposed a defense against their attack that involves repeatedly evicting data from the CPU cache while the attacker tries to collect data.
The idea was driven by their understanding of the side channel being exploited: adding noise to the CPU cache should make it more difficult to exploit a cache-based side channel.
However, we found that a defense that instead generates a bunch of interrupts, such as by making network requests to local IP addresses, defends significantly better against both the cache-based attack and our counting-based attack!</p>

<figure>
  <div>
    <table>
      <thead>
        <tr>
          <th>Attack</th>
          <th>Baseline</th>
          <th>With Cache Noise</th>
          <th>With Interrupt Noise</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Counting Attack (ours)</td>
          <td>95.7%</td>
          <td>92.6%</td>
          <td>62.0%</td>
        </tr>
        <tr>
          <td>Cache Attack (Shusterman et al.)</td>
          <td>78.4%</td>
          <td>76.2%</td>
          <td>55.3%</td>
        </tr>
      </tbody>
    </table>
  </div>
  <figcaption>Table 2 from our paper, which shows that both attacks are affected more by extra interrupts than by extra cache accesses.</figcaption>
</figure>

<p>This is a relatively simple example, but it shows how having a better understanding of the side channel being exploited can increase our ability to defend against it.
In combination with our other findings, it also helped us build our case that Shusterman et al.’s attack primarily exploits signals from interrupts, and not the cache.
We hope this work motivates future researchers to apply these models carefully.</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/bigger-fish.png">
  <figcaption>There’s Always a Bigger Fish</figcaption>
</figure>

<h3 id="other-findings">Other findings</h3>

<p>There are a few more interesting findings in <a href="https://dl.acm.org/doi/pdf/10.1145/3470496.3527416" target="\_blank">our paper</a> if you’re curious to keep reading!
A couple of these include:</p>

<ul>
  <li>We proposed a modification to the clock provided to JavaScript code that completely mitigates our attack</li>
  <li>We ran experiments that isolated the attacker and victim by putting them in separate virtual machines, which should theoretically offer the most isolation possible</li>
  <li>We discussed and analyzed the properties of several types of non-movable interrupts, including how frequently some of them fire and exactly how long they take to process</li>
</ul>

<p>And more! Unfortunately this blogpost is already long enough as it is.</p>

<h2 id="future-work">Future work</h2>

<p>There are a bunch of open questions in this area, even today, two years after we originally published this paper.
Here are a few that are still at the top of my mind.
If you find any of these interesting, please get in touch!</p>

<h3 id="how-should-we-rethink-interrupts">How should we rethink interrupts?</h3>

<p>Similarly to <a href="https://meltdownattack.com/" target="\_blank">Spectre and Meltdown</a>, our attack targets hardware mechanisms that are embedded deep inside basically all modern computers.
It’s currently impossible to implement a defense that isolates non-movable interrupts from an attacker, and it’s unclear how computers would be redesigned in a way that makes that possible.
Figuring this out presents an important direction for future research, especially if attacks such as ours become more accurate in the future.</p>

<h3 id="the-relationship-between-websites-and-interrupts-is-not-well-understood">The relationship between websites and interrupts is not well understood</h3>

<p>Below, you can see a figure from our paper, in which we show how interrupt handling time varies while loading three different websites.
Notice that the behavior, and even the types of interrupts, are different: loading weather.com triggers a lot of rescheduling interrupts, but nytimes.com and amazon.com don’t trigger any!</p>

<figure>
  <img src="https://jackcook.com/img/blog/bigger-fish/cook-fig5.png">
  <figcaption>Figure 5 from our paper</figcaption>
</figure>

<p>We don’t know why this is: clearly, there is something that weather.com is doing, perhaps loading more video content or more scripts or something, that the other two websites are not.
At a more basic level, we’re not really sure what the relationship is between website activity and triggered interrupts.
What impact does loading one additional image have on the attacker’s counter trace?
What about an advertisement?
What is it exactly that makes the counter traces so distinctive that we can tell them apart so easily?
We didn’t spend time trying to answer these questions, but a better understanding of this relationship would likely help us build better defenses against system-interrupt-based side-channel attacks such as ours.</p>

<h3 id="the-attack-could-be-made-stronger">The attack could be made stronger</h3>

<p>We wrote this paper as more of an “analysis paper,” and not as an “attack paper.”
In theory, the 96.6% accuracy that we achieved when identifying the victim website in Chrome on Linux is a lower bound, not an upper bound.
It’s pretty likely that a better model, or a different methodology, could achieve higher accuracy.
And if it’s possible to achieve higher accuracy on this 100-website task, it’s likely possible to perform well on a 1,000-website task, or on some other privacy-compromising task: figuring out what movie you’re watching, or whether you’re using a VPN, or how often you check Robinhood.</p>

<h3 id="browser-based-defenses-could-be-made-stronger">Browser-based defenses could be made stronger</h3>

<p>All browsers reduce the precision of the clock that they provide to websites via JavaScript: instead of telling you it’s 10:33:13.726142 (13.726142 seconds after the clock strikes 10:33am), your browser might just round to the nearest millisecond and tell you it’s 10:33:13.726.
This is because with access to higher-precision timers, attacks such as ours become much more accurate (see <a href="https://arxiv.org/abs/1502.07373" target="\_blank">Oren et al., 2015</a>).</p>

<p>As a result, Chrome rounds its clock to the nearest 0.1 millisecond and adds some random noise, while Firefox and Safari round to the nearest 1 millisecond.
<a href="https://www.torproject.org/download/" target="\_blank">Tor Browser</a>, the world’s most popular secure web browser, rounds to the nearest 100 milliseconds, reducing our attack’s accuracy from 96.6% (in Chrome) to 49.8%.
There is a tradeoff here: browser-based game engines, for example, need a high-precision timer in order to render and animate content correctly.
As a result, Tor Browser users are unable to play most games, but this is not necessarily a problem for users who care about security.</p>

<p>In Section 6.1 of <a href="https://dl.acm.org/doi/10.1145/3470496.3527416" target="\_blank">our paper</a>, we propose a slight modification to browser clocks that completely mitigates our attack.
I think it’s a step in the right direction, but more work needs to be done to implement this defense into real web browsers, and to examine whether it’s practical for most users.</p>

<h2 id="how-this-paper-changed-my-life">How this paper changed my life</h2>

<p>Before taking Mengjia’s class, the thought of going to graduate school had crossed my mind, but it was not an option I was taking seriously.
One year prior, I had worked at NVIDIA as a deep learning research intern, and I loved it.
After graduating, I probably would have looked for a full-time job there, or at another big tech company, or at some AI startup.</p>

<p>But this project showed me that research can be fun, and maybe even beautiful.
It was a result of myself and three talented researchers coming together, each with a different background and skillset, to learn and create new knowledge together.
This realization changed my life: after graduating from MIT with this paper under my belt, I stuck around for another year to earn my MEng in computer science, which I would not have done if not for this project.
I then applied for a Rhodes scholarship, which I absolutely would not have won had it not been for this project, and which enabled me to spend two years studying at the University of Oxford.
Next year, I will start my six-year PhD in computer science back at MIT, and I could not be more thrilled!</p>

<p>I am grateful to Jules, Jonathan, and especially Mengjia, for making this project possible, and for taking a chance on me—I can only hope that my future research projects will be as exciting and formative as this one.</p>




<p><sup id="fn1">1. Intel still hasn’t updated their website for some reason, but I promise we won. Source: <a href="https://jackcook.com/img/blog/bigger-fish/intel-award.jpeg" target="_blank">trust me bro</a> <a href="#ref1" title="Jump back to footnote 1 in the text.">↩</a></sup>
<sup id="fn2">2. <a href="https://www.youtube.com/shorts/d_lHcJGwnxM" target="_blank">https://www.youtube.com/shorts/d_lHcJGwnxM</a> <a href="#ref2" title="Jump back to footnote 2 in the text.">↩</a></sup>
<sup id="fn3"> 3. I’m just going to refer to it as the CPU cache for simplicity, but if you care about the details, we want to evict data from the last-level cache (LLC), which is the largest and slowest CPU cache. <a href="#ref3" title="Jump back to footnote 3 in the text.">↩</a></sup>
<sup id="fn4">4. You don’t actually need to access every single element: accessing elements at LLC cache line-sized intervals (usually 64 bytes each) is enough to evict all of the data in that cache line. <a href="#ref4" title="Jump back to footnote 4 in the text.">↩</a></sup>
<sup id="fn5">5. All browsers reduce the precision of the clock that they provide to websites via JavaScript: instead of telling you it’s 10:33:13.726142 (13.726142 seconds after the clock strikes 10:33am), your browser might just round to the nearest millisecond and tell you it’s 10:33:13.726. The reason for this is a little crazy: with a higher-precision timer, you can measure the latency of a single memory access (if it takes longer than a few nanoseconds to read an array value, the value was definitely missing from the cache), enabling you to pull off much more accurate cache-based side-channel attacks (see <a href="https://arxiv.org/abs/1502.07373" target="_blank">Oren et al., 2015</a>). All web browsers have since updated to reduce the resolution of their timers, but Chrome’s timer remains the most precise: Chrome rounds to the nearest 100 microseconds, while Firefox and Safari both round to the nearest 1 millisecond. This means that Chrome will give the most accurate timing data for the cache latency demo in this post. <a href="#ref5" title="Jump back to footnote 5 in the text.">↩</a></sup>
<sup id="fn6">6. The numbers in the table are from the closed-world LSTM column in Table 2 of <a href="https://www.usenix.org/conference/usenixsecurity19/presentation/shusterman" target="_blank">Shusterman et al.’s paper</a>. They also report results for an open-world setup, in which the correct website might not be in the training data. <a href="#ref6" title="Jump back to footnote 6 in the text.">↩</a></sup>
<sup id="fn7">7. Note that the inverse is not true: if a model doesn’t achieve high accuracy, it might just mean that your model isn’t good enough, not that there’s no signal. <a href="#ref7" title="Jump back to footnote 7 in the text.">↩</a></sup>
<sup id="fn8">8. See footnote 6. We report open-world results in our paper as well. <a href="#ref8" title="Jump back to footnote 8 in the text.">↩</a></sup>
<sup id="fn9">9. Differences in the data being processed can actually also cause CPU frequency to change! <a href="https://www.hertzbleed.com/" target="_blank">Wang et al., 2022</a> (also selected by IEEE Micro Top Picks) write, “on modern processors, the same program can run at a different CPU frequency (and therefore take a different [amount of] time) when computing, for example, 2022 + 23823 compared to 2022 + 24436.” This opens up yet another side channel. <a href="#ref9" title="Jump back to footnote 9 in the text.">↩</a></sup>
</p>
  
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IronCalc – Open-Source Spreadsheet Engine (439 pts)]]></title>
            <link>https://www.ironcalc.com/</link>
            <guid>42095292</guid>
            <pubDate>Sat, 09 Nov 2024 16:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ironcalc.com/">https://www.ironcalc.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42095292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>      
      <div id="about">
            <div>
              <h4>MIT/Apache 2.0 licensed</h4>
              <p>You can integrate it into your projects, customize it to your needs, and share it openly without any restrictions.</p>
            </div>
            <div>
              <h4>Feature rich</h4>
              <p>You shouldn’t worry that this or that function is not supported.</p>
            </div> 
            
             
            <div>
              <h4>Excel compatible</h4>
              <p>We at IronCalc are in awe of the software created by Microsoft over the years. We want every one to be able to use their spreadsheets.</p>
            </div> 
            <div>
              <h4>Fully tested</h4>
              <p>Modern programming practices should be used covering with tests any feature of the system.</p>
            </div>
            <div>
              <h4>Fast and lightweight</h4>
              <p>The programs shouldn’t be heavier than a few hundred kilobytes.</p>
            </div>
            <div>
              <h4>International from day one</h4>
              <p>Language should no be a barrier to use a spreadsheet.</p>
            </div>
            <div>
              <h4>Well designed</h4>
              <p>It should be nice and friendly to use. Designed with love from the ground up.</p>
            </div> 
             
             
             

          </div>
      
      <div id="why-ironcalc">
          
          
            <p>
              For over 40 years, spreadsheets have been integral to countless applications. Despite numerous proprietary and open-source options, finding a universally accessible, reliable, and high-quality engine remains a challenge. Many existing solutions are expensive, require accounts, or suffer from performance and stability issues.
            </p>
            <p>
              <strong>Our Mission:</strong> To fill the gaps left by the industry and empower every user with a robust, open-source spreadsheet engine that caters to diverse needs. Here's why we are dedicated to this mission:
            </p>
            <h4>Addressing Unmet Needs</h4>
            <p><strong>Empowering SaaS Developers:</strong> Hundreds, if not thousands, of companies have implemented half-baked spreadsheets in their systems. IronCalc aims to provide these businesses with a superior, open-source alternative that enhances their SaaS applications.</p>
            <p><strong>Automated Spreadsheet Processing:</strong> Users need a reliable way to programmatically open, populate, and analyze spreadsheets for large-scale scenarios. IronCalc delivers the performance and functionality required for these complex tasks.</p>
            <p><strong>Global Collaboration:</strong> We envision a world where anyone can use a spreadsheet online and effortlessly share it with friends for collaborative projects, such as planning travel experiences.</p>
            <p><strong>Interactive Blog Integration:</strong> Bloggers should be able to embed interactive spreadsheets in their posts, allowing readers to engage with custom test cases and scenarios.</p>
            <h4>Beyond Code: Advancing Spreadsheet Technology and Community</h4>
            <p>IronCalc's ambition extends beyond providing open-source code. We aim to drive the spreadsheet industry forward through:</p>
            <p><strong>Research and Development:</strong> While many areas of computer science, such as compilers and algorithms, have extensive literature, spreadsheets have been largely overlooked. We want to challenge that by finding collaborators in universities and academic institutions willing to do open research in spreadsheet engines.</p>
            <p><strong>Community and Collaboration:</strong> We can dream bigger, maybe we can organize conferences and maybe support PhD researchers and foster a collaborative environment where ideas and innovations can flourish.</p>
            <p><strong>Building a Knowledge Base:</strong> Our goal is to equip the next generation of spreadsheet developers with a comprehensive set of tools and knowledge, laying a solid foundation for future advancements, not only IronCalc.</p>
            <p>Together, we can push the boundaries of what spreadsheets can achieve, making high-quality, accessible spreadsheet technology available to all.</p>
            
            <p>
            – The IronCalc Team
            </p>

            

        </div>
      <!-- <section id="newsletter">
        <div class="section-content">
          <div class="blue-container">
            <div class="flex-v gap-lg">
              <div class="flex-v gap-sm">
                <h2>Join the waitlist</h2>
                <p>
                  We’ll send you a message when IronCalc is launched!
                </p>
              </div>
              <button class="secondary max-w-200">Join</button>
            </div>
          </div>
        </div>
      </section> -->

      


      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite does not do checksums (126 pts)]]></title>
            <link>https://avi.im/blag/2024/sqlite-bit-flip/</link>
            <guid>42094663</guid>
            <pubDate>Sat, 09 Nov 2024 14:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avi.im/blag/2024/sqlite-bit-flip/">https://avi.im/blag/2024/sqlite-bit-flip/</a>, See on <a href="https://news.ycombinator.com/item?id=42094663">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><p>SQLite does not do checksums by default. I learned this from <a href="https://fosstodon.org/@AlexMillerDB/109553692861357766">Alex Miller</a>. What does this mean? If there is disk corruption, the database or application won’t be able to know that the database is ‘corrupt’.</p><p>Even a single bit flip can cause havoc. This can happen due to a faulty disk, a bug in the disk driver, or when another application (malicious or otherwise) modifies the database files.</p><p>This is not a bug - it’s properly documented:</p><blockquote><p>SQLite assumes that the detection and/or correction of bit errors caused by cosmic rays, thermal noise, quantum fluctuations, device driver bugs, or other mechanisms, is the responsibility of the underlying hardware and operating system. SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.</p></blockquote><p>I created a <a href="https://gist.github.com/avinassh/0e7e4b0578136a338f1b9a03fba36ead">simple script</a> to demonstrate this:</p><ol><li><p>Create a sample database using <a href="https://gist.github.com/avinassh/0e7e4b0578136a338f1b9a03fba36ead">this script</a>. It creates a bank database and adds a row for Alice with $83K.</p></li><li><p>Flip a single bit:</p><pre><code> printf '\x00\x00\x00\x00\x00\x80' | dd of=bank.db bs=1 seek=$((0x1ffd)) count=1 conv=notrunc
</code></pre></li><li><p>Alice’s balance is now zero. Sorry, Alice.</p></li></ol><p>It passes <code>PRAGMA integrity_check</code> too. Here’s an ASCII animation if you prefer that:</p><h2 id="wal-and-checksums">WAL and Checksums</h2><p>SQLite has checksums for WAL frames. However, when it detects a corrupt frame, it silently ignores the faulty frame and all subsequent frames. It doesn’t even raise an error!</p><p>Ignoring frames might be acceptable, but not raising an error is where it gets me.</p><h2 id="checksum-vfs-shim">Checksum VFS Shim</h2><p>You can use the <a href="https://www.sqlite.org/cksumvfs.html">Checksum VFS Shim</a>, but there’s one important caveat:</p><blockquote><p>Checksumming only works on databases that have a reserve bytes value of exactly 8</p></blockquote><p>The <a href="https://www.sqlite.org/fileformat2.html#resbyte">documentation of reserve bytes</a> explains:</p><blockquote><p>SQLite has the ability to set aside a small number of extra bytes at the end of every page for use by extensions. These extra bytes are used, for example, by the SQLite Encryption Extension to store a nonce and/or cryptographic checksum associated with each page. The “reserved space” size in the 1-byte integer at offset 20 is the number of bytes of space at the end of each page to reserve for extensions. This value is usually 0. The value can be odd.</p></blockquote><p>This means if you’re using any extension that uses reserve bytes, you can’t use the Checksum shim.</p><p>Again, this is not a bug. <a href="https://avi.im/blag/2024/databases-checksum">Most databases (except a few)</a> assume that the OS, filesystem, and disk are sound. Whether this matters depends on your application and the guarantees you need.</p><p>edit: I wrote a <a href="https://avi.im/blag/2024/databases-checksum">follow up post</a>.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientist treated her own cancer with viruses she grew in the lab (519 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-03647-0</link>
            <guid>42094573</guid>
            <pubDate>Sat, 09 Nov 2024 14:23:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-03647-0">https://www.nature.com/articles/d41586-024-03647-0</a>, See on <a href="https://news.ycombinator.com/item?id=42094573">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                        <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Coloured transmission electron micrograph of cultured measles virus particles." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713234.jpg"><figcaption><p><span>Viruses such as measles (pictured here) can be used to attack cancerous cells. </span><span>Credit: Eye Of Science/Science Photo Library</span></p></figcaption></picture></figure><p>A scientist who successfully treated her own <a href="https://www.nature.com/subjects/breast-cancer" data-track="click" data-label="https://www.nature.com/subjects/breast-cancer" data-track-category="body text link">breast cancer</a> by injecting the tumour with lab-grown viruses has sparked discussion about the ethics of self-experimentation. </p><p>Beata Halassy discovered in 2020, aged 49, that she had breast cancer at the site of a previous mastectomy. It was the second recurrence there since her left breast had been removed, and she couldn’t face another bout of chemotherapy. </p><p>Halassy, a virologist at the University of Zagreb, studied the literature and decided to take matters into her own hands with an unproven treatment. </p><p>A case report published in <i>Vaccines</i> in August<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> outlines how Halassy self-administered a treatment called <a href="https://www.nature.com/articles/s41571-022-00719-w" data-track="click" data-label="https://www.nature.com/articles/s41571-022-00719-w" data-track-category="body text link">oncolytic virotherapy</a> (OVT) to help treat her own stage 3 cancer. She has now been cancer-free for four years. </p><p>In choosing to <a href="https://www.nature.com/articles/nm0508-471b" data-track="click" data-label="https://www.nature.com/articles/nm0508-471b" data-track-category="body text link">self-experiment</a>, Halassy joins a long line of scientists who have participated in this under-the-radar, stigmatized and ethically fraught practice. “It took a brave editor to publish the report,” says Halassy.</p><h2>Up-and-coming therapy</h2><p>OVT is an emerging field of <a href="https://www.nature.com/subjects/cancer-therapy" data-track="click" data-label="https://www.nature.com/subjects/cancer-therapy" data-track-category="body text link">cancer treatment</a> that uses viruses to both attack cancerous cells and provoke the immune system into fighting them. Most OVT clinical trials so far have been in late-stage, metastatic cancer, but in the past few years they have been directed towards earlier-stage disease. One OVT, called T-VEC, has been in approved in the United States to treat metastatic melanoma, but there are as yet no OVT agents approved to treat breast cancer of any stage, anywhere in the world. </p><p>Halassy stresses that she isn’t a specialist in OVT, but her expertise in cultivating and purifying viruses in the laboratory gave her the confidence to try the treatment. She chose to target her tumour with two different viruses consecutively — a <a href="https://www.nature.com/subjects/measles-virus" data-track="click" data-label="https://www.nature.com/subjects/measles-virus" data-track-category="body text link">measles virus</a> followed by a vesicular stomatitis virus (VSV). Both pathogens are known to infect the type of cell from which her tumour originated, and have already been used in OVT clinical trials. A measles virus has been trialled against metastatic breast cancer.</p><p>Halassy had previous experience working with both viruses, and both have a good safety record. The strain of measles she chose is used extensively in childhood vaccines, and the strain of VSV induces, at worst, mild influenza-like symptoms. </p><figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="Portrait of Beata Halassy." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27713236.jpg"><figcaption><p><span>Halassy’s experience with self-treatment has changed the focus of her research. </span><span>Credit: Ivanka Popić </span></p></figcaption></picture></figure><p>Over a two-month period, a colleague administered a regime of treatments with research-grade material freshly prepared by Halassy, injected directly into her tumour. Her oncologists agreed to monitor her during the self-treatment, so that she would be able to switch to conventional chemotherapy if things went wrong.</p><p>The approach seemed to be effective: over the course of the treatment, and with no serious side effects, the tumour shrank substantially and became softer. It also detached from the pectoral muscle and skin that it had been invading, making it easy to remove surgically.</p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-024-02613-0" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_27470032.jpg"><p>How a trove of cancer genomes could improve kids’ leukaemia treatment</p></a></article><p>Analysis of the tumour after removal showed that it was thoroughly infiltrated with immune cells called lymphocytes, suggesting that the OVT had worked as expected and provoked Halassy’s immune system to attack both the viruses and the tumour cells. “An immune response was, for sure, elicited,” says Halassy. After the surgery, she received a year’s treatment with the anticancer drug trastuzumab. </p><p>Stephen Russell, an OVT specialist who runs virotherapy biotech company Vyriad in Rochester, Minnesota, agrees that Halassy’s case suggests the viral injections worked to shrink her tumour and cause its invasive edges to recede. </p><p>But he doesn’t think her experience really breaks any new ground, because researchers are already trying to use OVT to help treat earlier-stage cancer. He isn’t aware of anyone trying two viruses sequentially, but says it isn’t possible to deduce whether this mattered in an ‘<i>n</i> of 1’ study. “Really, the novelty here is, she did it to herself with a virus that she grew in her own lab,” he says.</p><h2>Ethical dilemma</h2><p>Halassy felt a responsibility to publish her findings. But she received more than a dozen rejections from journals — mainly, she says, because the paper, co-authored with colleagues, involved self-experimentation. “The major concern was always ethical issues,” says Halassy. She was particularly determined to persevere after she came across a review highlighting the value of self-experimentation<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. </p><p>That journals had concerns doesn’t surprise Jacob Sherkow, a law and medicine researcher at the University of Illinois Urbana-Champaign who has examined the ethics of researcher self-experimentation in relation to COVID-19 vaccines. </p><article data-label="Related"><a href="https://www.nature.com/articles/d41586-023-02075-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-024-03647-0/d41586-024-03647-0_26019860.jpg"><p>Huge leap in breast cancer survival rate</p></a></article><p>The problem is not that Halassy used self-experimentation as such, but that publishing her results could encourage others to reject conventional treatment and try something similar, says Sherkow. People with cancer can be particularly susceptible to trying unproven treatments. Yet, he notes, it’s also important to ensure that the knowledge that comes from self-experimentation isn’t lost. The paper emphasizes that self-medicating with cancer-fighting viruses “should not be the first approach” in the case of a cancer diagnosis. </p><p>“I think it ultimately does fall within the line of being ethical, but it isn’t a slam-dunk case,” says Sherkow, adding that he would have liked to see a commentary fleshing out the ethics perspective, published alongside the case report.</p><p>Halassy has no regrets about self-treating, or her dogged pursuit of publication. She thinks it is unlikely that someone would try to copy her, because the treatment requires so much scientific knowledge and skill. And the experience has given her own research a new direction: in September she got funding to investigate OVT to treat cancer in domestic animals. “The focus of my laboratory has completely turned because of the positive experience with my self-treatment,” she says.</p>
                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FrontierMath: A benchmark for evaluating advanced mathematical reasoning in AI (117 pts)]]></title>
            <link>https://epochai.org/frontiermath/the-benchmark</link>
            <guid>42094546</guid>
            <pubDate>Sat, 09 Nov 2024 14:18:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://epochai.org/frontiermath/the-benchmark">https://epochai.org/frontiermath/the-benchmark</a>, See on <a href="https://news.ycombinator.com/item?id=42094546">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <h3>Authors</h3>

          
            </div><div>
          



<p>We’re introducing FrontierMath, a benchmark of hundreds of original, expert-crafted mathematics problems designed to evaluate advanced reasoning capabilities in AI systems. These problems span major branches of modern mathematics—from computational number theory to abstract algebraic geometry—and typically require hours or days for expert mathematicians to solve.</p>

<div>
  






  <p>Figure 1. While leading AI models now achieve near-perfect scores on traditional benchmarks like GSM-8k and MATH, they solve less than 2% of FrontierMath problems, revealing a substantial gap between current AI capabilities and the collective prowess of the mathematics community. MMLU scores shown are for the College Mathematics category of the benchmark.</p>
</div>

<p>To understand and measure progress in artificial intelligence, we need carefully designed benchmarks that can assess how well AI systems engage in complex scientific reasoning. Mathematics offers a unique opportunity for this assessment—it requires extended chains of precise reasoning, with each step building exactly on what came before. And, unlike many domains where evaluation requires subjective judgment or expensive tests, mathematical problems can be rigorously and automatically verified.</p>

<h2 id="the-frontiermath-benchmark">The FrontierMath Benchmark</h2>

<p>FrontierMath is a benchmark of hundreds of original mathematics problems spanning the breadth of modern mathematical research. These range from computationally intensive problems in number theory and real analysis to abstract questions in algebraic geometry and category theory. We developed it through collaboration with over 60 mathematicians from leading institutions, including professors, IMO question writers, and Fields medalists.</p>

<p>FrontierMath problems typically demand hours or even days for specialist mathematicians to solve. The following Fields Medalists shared their impressions after reviewing some of the research-level problems in the benchmark:</p>

<blockquote>
  <p>“These are extremely challenging. I think that in the near term basically the only way to solve them, short of having a real domain expert in the area, is by a combination of a semi-expert like a graduate student in a related field, maybe paired with some combination of a modern AI and lots of other algebra packages…” —Terence Tao, Fields Medal (2006)</p>
</blockquote>

<blockquote>
  <p>“[The questions I looked at] were all not really in my area and all looked like things I had no idea how to solve…they appear to be at a different level of difficulty from IMO problems.” — Timothy Gowers, Fields Medal (2006)</p>
</blockquote>

<p>FrontierMath features hundreds of advanced mathematics problems that require hours for expert mathematicians to solve. We release representative samples, which you may download <a href="https://epochai.org/files/sample_question_transcripts.zip">here</a>.</p>

<div>
  <div>
      <p><i>Definitions.</i> For a positive integer \( n \), let \( v_p(n) \) denote the largest integer \( v \) such that \( p^v \mid n \). For a prime \( p \) and \( a \not\equiv 0 \pmod{p} \), let \( \text{ord}_p(a) \) denote the smallest positive integer \( o \) such that \( a^o \equiv 1 \pmod{p} \). For \( x &gt; 0 \), let 
        \[
          \text{ord}_{p, x}(a) = \prod_{\substack{q \le x \\ q \text{ prime}}} q^{v_q(\text{ord}_p(a))} \prod_{\substack{ q &gt; x \\ q \text{ prime} } } q^{v_q(p-1)}.
        \]
        <i>Problem.</i> Let \( S_x \) denote the set of primes \( p \) for which
        \[
          \text{ord}_{p, x}(2) &gt; \text{ord}_{p, x}(3),
        \]
        and let \( d_x \) denote the density
        \[
          d_x = \frac{|S_x|}{|\{p \le x : p \text{ is prime}\}|}
        \]
        of \( S_x \) in the primes. Let
        \[
          d_{\infty} = \lim_{x \to \infty} d_x.
        \]
        Compute \( \lfloor 10^7 d_{\infty} \rfloor. \)
      </p>
      <p><b>Answer</b>: 3677073
      </p>
      <p><b>MSC classification</b>: 11 Number theory
      </p>
    </div>

  <div>
      <p>
        Construct a degree 19 polynomial \(p(x) \in \mathbb{C}[x]\) such that \(X:=\{p(x) = p(y)\} \subset \mathbb{P}^1 \times \mathbb{P}^1\) has at least 3 (but not all linear) irreducible components over \(\mathbb{C}\). Choose \(p(x)\) to be odd, monic, have real coefficients and linear coefficient -19 and calculate \(p(19)\).
      </p>
      <p><b>Answer</b>: 1876572071974094803391179
      </p>
      <p><b>MSC classification</b>: 14 Algebraic geometry; 20 Group theory and generalizations; 11 Number theory generalizations
      </p>
    </div>

  <div>
      <p>
        Let \(a_n\) for \(n\in\mathbb Z\) be the sequence of integers satisfying the recurrence formula 
        \[
          \begin{aligned}
          a_n = (1.981 \times 10^{11})a_{n-1} + (3.549 \times 10^{11})a_{n-2} \\
          \hspace{0.4cm} + (4.277 \times 10^{11})a_{n-3} + (3.706 \times 10^8)a_{n-4}
          \end{aligned}
        \]
        with initial conditions \(a_i=i\) for \(0\leq i\leq 3\). Find the smallest prime \(p\equiv4\bmod7\) for which the function \(\mathbb Z\to\mathbb Z\) given by \(n\mapsto a_n\) can be extended to a continuous function on \(\mathbb Z_p\).
      </p>
      <p><b>Answer</b>: 9811
      </p>
      <p><b>MSC classification</b>: 11 Number theory
      </p>
    </div>
</div>

<p>Each problem is carefully designed to test genuine mathematical understanding. Problems must be novel and unpublished, with answers that can be automatically verified through computation—either as exact integers or mathematical objects like matrices and symbolic expressions in SymPy. A verification script checks submissions through exact matching or by confirming the submitted answer matches the known solution.</p>

<p>They are also designed to be “guessproof”—problems have large numerical answers or complex mathematical objects as solutions, with less than a 1% chance of guessing correctly without the mathematical work. Problems are reviewed specifically for this property, with reviewers checking that shortcuts or pattern matching generally cannot bypass the need for genuine understanding.</p>

<p>Each problem undergoes peer review by expert mathematicians who verify correctness, check for ambiguities, and assess difficulty ratings. Additionally, we conducted second reviews on a random subsample of problems, finding that approximately 1 in 20 problems had errors requiring correction—comparable to error rates in other major machine learning benchmarks <a href="https://arxiv.org/abs/2103.14749">like ImageNet</a>. We recognize the importance of benchmark accuracy and are expanding both our expert review process and error-bounty program to reduce this error rate.</p>

<h2 id="current-performance-on-frontiermath">Current Performance on FrontierMath</h2>

<p>To evaluate how well current AI models can tackle advanced mathematical problems, we provided them with extensive support to maximize their performance. Our evaluation framework grants models ample thinking time and the ability to experiment and iterate. Models interact with a Python environment where they can write and execute code to test hypotheses, verify intermediate results, and refine their approaches based on immediate feedback.</p>

<div>
  






  <p>Figure 2. Performance of leading language models on FrontierMath. All models show consistently poor performance, with even the best models solving less than 2% of problems.</p>
</div>

<p>Despite this support framework, FrontierMath has proven exceptionally challenging for today’s AI systems. We evaluated six leading language models—including Claude 3.5 Sonnet, o1-preview, GPT-4o, and Gemini 1.5 Pro—and found that none could solve more than 2% of the problems. This is in sharp contrast to other popular mathematical benchmarks such as GSM-8K and MATH, where top models now achieve over 90% accuracy.</p>

<h2 id="our-next-steps">Our next steps</h2>

<p>FrontierMath represents a significant step toward evaluating whether AI systems possess research-level mathematical reasoning capabilities. While current models solve less than 2% of problems, we expect this benchmark to become increasingly valuable as AI systems advance.</p>

<p>Our next steps include:</p>

<ul>
  <li><strong>Regular evaluations</strong>: Conducting and publishing ongoing assessments of leading AI models to provide a standardized measure of progress, and evaluating how advanced mathematical reasoning abilities improve over time and with scale.</li>
  <li><strong>Benchmark expansion</strong>: Adding more problems to FrontierMath while maintaining both our rigorous standards and the current distribution of problem types, difficulty levels, and mathematical domains.</li>
  <li><strong>Public problem release</strong>: Building on our initial release of five representative problems with solutions, we plan to release additional problems in the coming months to further engage the community and facilitate benchmarking.</li>
  <li><strong>Enhanced quality assurance</strong>: Strengthening our quality control through expanded expert review, increased error-bounties, and improved peer review processes.</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>FrontierMath represents a significant step toward evaluating whether AI systems possess research-level mathematical reasoning capabilities. While current models solve less than 2% of problems—revealing a substantial gap between AI capabilities and the collective prowess of the mathematical community—we expect this benchmark to become increasingly valuable as AI systems advance.</p>

<p>We look forward to working with both the mathematics and the AI research community to refine and expand this benchmark. By regularly evaluating state-of-the-art models and collaborating with the AI research community, we aim to deepen our understanding of AI’s capabilities and limitations.</p>

<p>You can read more about FrontierMath in <a href="https://arxiv.org/abs/2411.04872">our technical report</a>. If you want to reach out to us about FrontierMath evaluations, please email us at <a href="mailto:math_evals@epochai.org">math_evals@epochai.org</a></p>







          
        </div><div>
          <h3>About the authors</h3>

          <div>
          
            
            <div>
              <p><img src="https://epochai.org/assets/images/team/square-small/tamay-besiroglu.jpg"></p>
              <div>
                
                  
                

                
                  <p><span>Tamay Besiroglu</span> is the associate director at Epoch AI. His work focuses on the economics of computing and big-picture trends in machine learning. Previously, he was a researcher at the Future Tech Lab at MIT, led strategy for Metaculus, consulted for the UK Government, and worked at the Future of Humanity Institute.</p>
                
              </div>
            </div>
          
            
            <div>
              <p><img src="https://epochai.org/assets/images/team/square-small/elliot-glazer.png"></p>
              <div>
                
                  
                

                
                  <p><span>Elliot Glazer</span> holds a Ph.D. in Mathematics from Harvard under Hugh Woodin, with research in set theory and formal systems, especially paradoxes in the axiom of choice. He has recently worked on the foundations of proof assistants, and enjoys developing mathematical puzzles in both finite and infinite settings.</p>
                
              </div>
            </div>
          
            
            <div>
              <p><img src="https://epochai.org/assets/images/team/square-small/caroline-falkman-olsson.png"></p>
              <div>
                
                  
                

                
                  <p><span>Caroline Falkman Olsson</span> is an Operations Associate at Epoch AI, with a background in Economics and Statistics. She has previously worked as a predoctoral researcher at LSE’s International Inequalities Institute (III) and as a data analyst at the Institute for International Economic Studies (IIES) at Stockholm University.</p>
                
              </div>
            </div>
          
          </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Memories are not only in the brain, human cell study finds (222 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html</link>
            <guid>42094427</guid>
            <pubDate>Sat, 09 Nov 2024 13:53:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html">https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html</a>, See on <a href="https://news.ycombinator.com/item?id=42094427">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/memories-are-not-only.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/memories-are-not-only.jpg" data-sub-html="An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/memories-are-not-only.jpg" alt="Memories are not only in the brain, new research finds" title="An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin" width="800" height="530">
             <figcaption>
                An NYU researcher administers chemical signals to non-neural cells grown in a culture plate. Credit: Nikolay Kukushkin
            </figcaption>        </figure>
    </div><p>It's common knowledge that our brains—and, specifically, our brain cells—store memories. But a team of scientists has discovered that cells from other parts of the body also perform a memory function, opening new pathways for understanding how memory works and creating the potential to enhance learning and to treat memory-related afflictions.</p>

                                        
                                                                                  
                                         

                                                                                                                                    <p>"Learning and <a href="https://medicalxpress.com/tags/memory/" rel="tag">memory</a> are generally associated with brains and brain cells alone, but our study shows that other cells in the body can learn and form memories, too," explains New York University's Nikolay V. Kukushkin, the lead author of the <a href="https://www.nature.com/articles/s41467-024-53922-x" target="_blank">study</a>, which appears in the journal <i>Nature Communications</i>.</p>
<p>The research sought to better understand if non-brain cells help with memory by borrowing from a long-established neurological property—the massed-spaced effect—which shows that we tend to retain information better when studied in spaced intervals rather than in a single, intensive session—better known as cramming for a test.</p>
<p>In the research, the scientists replicated learning over time by studying two types of non-brain human cells in a laboratory (one from nerve tissue and one from kidney tissue) and exposing them to different patterns of chemical signals—just like brain cells are exposed to patterns of neurotransmitters when we learn new information.</p>
<p>In response, the non-brain cells turned on a "memory gene"—the same gene that brain cells turn on when they detect a pattern in the information and restructure their connections in order to form memories.</p>
<p>To monitor the memory and learning process, the scientists engineered these non-brain cells to make a glowing protein, which indicated when the memory gene was on and when it was off.</p>

                                                                                                                                                         
                                                                                                                                                                                                <p>The results showed that these cells could determine when the chemical pulses, which imitated bursts of neurotransmitter in the brain, were repeated rather than simply prolonged—just as neurons in our brain can register when we learn with breaks rather than cramming all the material in one sitting.</p>
<p>Specifically, when the pulses were delivered in spaced-out intervals, they turned on the "memory gene" more strongly, and for a longer time, than when the same treatment was delivered all at once.</p>
<p>"This reflects the massed-space effect in action," says Kukushkin, a clinical associate professor of life science at NYU Liberal Studies and a research fellow at NYU's Center for Neural Science. "It shows that the ability to learn from spaced repetition isn't unique to <a href="https://medicalxpress.com/tags/brain+cells/" rel="tag">brain cells</a>, but, in fact, might be a fundamental property of all cells."</p>
<p>The researchers add that the findings not only offer new ways to study memory, but also point to potential health-related gains.</p>
<p>"This discovery opens new doors for understanding how memory works and could lead to better ways to enhance learning and treat memory problems," observes Kukushkin.</p>
<p>"At the same time, it suggests that in the future, we will need to treat our body more like the brain—for example, consider what our pancreas remembers about the pattern of our past meals to maintain healthy levels of blood glucose or consider what a cancer cell remembers about the pattern of chemotherapy."</p>
<p>The work was jointly supervised by Kukushkin and Thomas Carew, a professor in NYU's Center for Neural Science. The study's authors also included Tasnim Tabassum, an NYU researcher, and Robert Carney, an NYU undergraduate researcher at the time of the study.</p>

                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    N. V. Kukushkin et al, The massed-spaced learning effect in non-neural human cells, <i>Nature Communications</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41467-024-53922-x" target="_blank">DOI: 10.1038/s41467-024-53922-x</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Memories are not only in the brain, human cell study finds (2024, November 8)
                                                 retrieved 9 November 2024
                                                 from https://medicalxpress.com/news/2024-11-memories-brain-human-cell.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I Analyzed 650k TikTok Influencers and This Is What I Found (291 pts)]]></title>
            <link>https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/</link>
            <guid>42093911</guid>
            <pubDate>Sat, 09 Nov 2024 11:49:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/">https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/</a>, See on <a href="https://news.ycombinator.com/item?id=42093911">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/eCommerceSEO/comments/1gn8egy/ultimate_ecommerce_marketing_tool_influencers/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Somebody moved the UK's oldest satellite in the mid 1970s, but no one knows who (129 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cpwrr58801yo</link>
            <guid>42093851</guid>
            <pubDate>Sat, 09 Nov 2024 11:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cpwrr58801yo">https://www.bbc.co.uk/news/articles/cpwrr58801yo</a>, See on <a href="https://news.ycombinator.com/item?id=42093851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="headline-block"><h2 id="main-heading" type="headline" tabindex="-1"><span role="text">Somebody moved UK's oldest satellite, and no-one knows who or why</span></h2></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png.webp 976w" type="image/webp"><img alt="Artist's rendering of the Skynet-1A satellite" loading="eager" src="https://ichef.bbci.co.uk/ace/standard/2560/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/f917/live/5cbc5550-93a8-11ef-9504-b516e8b5f45f.png 976w" width="2560" height="1508.5714285714287"></picture></span><span role="text"><span>Image source, </span>BBC/Gerry Fletcher</span></p><figcaption><span>Image caption, </span><p>Artwork: The half-tonne Skynet-1A satellite was launched in November 1969</p></figcaption></figure></div><div data-component="text-block"><p><b>Someone moved the UK's oldest satellite and there appears to be no record of exactly who, when or why.</b></p><p>Launched in 1969, just a few months after humans first set foot on the Moon, Skynet-1A was put high above Africa's east coast to relay communications for British forces.</p><p>When the spacecraft ceased working a few years later, gravity might have been expected to pull it even further to the east, out over the Indian Ocean.</p><p>But today, curiously, Skynet-1A is actually half a planet away, in a position 22,369 miles (36,000km) above the Americas.</p></div><div data-component="text-block"><p>Orbital mechanics mean it's unlikely the half-tonne military spacecraft simply drifted to its current location.</p><p>Almost certainly, it was commanded to fire its thrusters in the mid-1970s to take it westwards. The question is who that was and with what authority and purpose?</p><p>It's intriguing that key information about a once vital national security asset can just evaporate. But, fascination aside, you might also reasonably ask why it still matters. After all, we're talking about some discarded space junk from 50 years ago.</p><p>"It's still relevant because whoever did move Skynet-1A did us few favours," says space consultant Dr Stuart Eves.</p><p>"It's now in what we call a 'gravity well' at 105 degrees West longitude, wandering backwards and forwards like a marble at the bottom of a bowl. And unfortunately this brings it close to other satellite traffic on a regular basis. </p><p>"Because it's dead, the risk is it might bump into something, and because it's 'our' satellite we're still responsible for it," he explains.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>BBC/Gerry Fletcher</span></p><figcaption><span>Image caption, </span><p>If a satellite died at 40E it would drift to the nearest gravity well, which is 75E.</p></figcaption></figure></div><div data-component="text-block"><p>Dr Eves has looked through old satellite catalogues, the National Archives and spoken to satellite experts worldwide, but he can find no clues to the end-of-life behaviour of Britain's oldest spacecraft.</p><p>It might be tempting to reach for a conspiracy theory or two, not least because it's hard to hear the name "Skynet" without thinking of the malevolent, self-aware artificial intelligence (AI) system in The Terminator movie franchise.</p><p>But there's no connection other than the name and, in any case, real life is always more prosaic.</p></div><div data-component="text-block"><p>What we do know is that Skynet-1A was manufactured in the US by the now defunct Philco Ford aerospace company and put in space by a US Air Force Delta rocket.</p><p>"The first Skynet satellite revolutionised UK telecommunications capacity, permitting London to securely communicate with British forces as far away as Singapore. However, from a technological standpoint, Skynet-1A was more American than British since the United States both built and launched it," remarked Dr Aaron Bateman in a recent paper on the history of the Skynet programme, which is now on its fifth generation.</p><p>This view is confirmed by Graham Davison who flew Skynet-1A in the early 70s from its UK operations centre at RAF Oakhanger in Hampshire.</p><p>"The Americans originally controlled the satellite in orbit. They tested all of our software against theirs, before then eventually handing over control to the RAF," the long-retired engineer told me.</p><p>"In essence, there was dual control, but when or why Skynet-1A might have been handed back to the Americans, which seems likely - I'm afraid I can't remember," says Mr Davison, who is now in his 80s.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Sunnyvale Heritage Park Museum</span></p><figcaption><span>Image caption, </span><p>Could the command to move Skynet-1A have come from the US Air Force's 'Blue Cube'?</p></figcaption></figure></div><div data-component="text-block"><p>Rachel Hill, a PhD student from University College London, has also been scouring the National Archives.</p><p>Her readings have led her to one very reasonable possibility.</p><p>"A Skynet team from Oakhanger would go to the USAF satellite facility in Sunnyvale (colloquially known as the Blue Cube) and operate Skynet during 'Oakout'. This was when control was temporarily transferred to the US while Oakhanger was down for essential maintenance. Perhaps the move could have happened then?” Ms Hill speculated.</p><p>The official, though incomplete, logs of Skynet-1A’s status suggest final commanding was left in the hands of the Americans when Oakhanger lost sight of the satellite in June 1977.</p><p>But however Skynet-1A then got shifted to its present position, it was ultimately allowed to die in an awkward place when really it should have been put in an "orbital graveyard".</p><p>This refers to a region even higher in the sky where old space junk runs zero risk of running into active telecommunications satellites.</p><p>Graveyarding is now standard practice, but back in the 1970s no-one gave much thought to space sustainability.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Astroscale</span></p><figcaption><span>Image caption, </span><p>British engineers are developing technologies to snare defunct satellites in low orbits</p></figcaption></figure></div><div data-component="text-block"><p>Attitudes have since changed because the space domain is getting congested.</p><p>At 105 degrees West longitude, an active satellite might see a piece of junk come within 50km of its position up to four times a day.</p><p>That might sound like they’re nowhere near each other, but at the velocities these defunct objects move it’s starting to get a little too close for comfort.</p><p>The Ministry of Defence said Skynet-1A was constantly monitored by the UK's National Space Operations Centre. Other satellite operators are informed if there's likely to be a particularly close conjunction, in case they need to take evasive action.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>Northrop Grumman</span></p><figcaption><span>Image caption, </span><p>The Americans have already shown it's possible to grab a high-orbiting satellite</p></figcaption></figure></div><div data-component="text-block"><p>Ultimately, though, the British government may have to think about removing the old satellite to a safer location.</p><p>Technologies are being developed to grab junk left in space.</p><p>Already, the UK Space Agency is funding efforts to do this at lower altitudes, and the Americans and the Chinese have shown it's possible to snare ageing hardware even in the kind of high orbit occupied by Skynet-1A.</p><p>"Pieces of space junk are like ticking time bombs," observed Moriba Jah, a professor of aerospace engineering at the University of Texas at Austin.</p><p>"We need to avoid what I call super-spreader events. When these things explode or something collides with them, it generates thousands of pieces of debris that then become a hazard to something else that we care about."</p></div><section data-component="links-block"><p><h2 type="normal">More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Money was never the end goal – mrdoob – threejs creator (117 pts)]]></title>
            <link>https://twitter.com/mrdoob/status/1854662365163536613</link>
            <guid>42093795</guid>
            <pubDate>Sat, 09 Nov 2024 11:21:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mrdoob/status/1854662365163536613">https://twitter.com/mrdoob/status/1854662365163536613</a>, See on <a href="https://news.ycombinator.com/item?id=42093795">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mergiraf: a syntax-aware merge driver for Git (360 pts)]]></title>
            <link>https://mergiraf.org/</link>
            <guid>42093756</guid>
            <pubDate>Sat, 09 Nov 2024 11:06:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mergiraf.org/">https://mergiraf.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42093756">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper">

            <div id="content" class="page">
                    <main>
                        <p><em>Are you held back by conflicts? Then meet</em></p>

<p>Mergiraf can solve <a href="https://mergiraf.org/conflicts.html">a wide range of Git merge conflicts</a>. That's because it's aware of the trees in your files!
Thanks to <a href="https://mergiraf.org/languages.html">its understanding of your language</a>, it can often reconcile the needs of both sides.</p>
<p>You can <a href="https://mergiraf.org/adding-a-language.html">teach Mergiraf a new language</a> in a completely declarative way. It's a nonviolent animal, so it prefers that over imperatives.</p>
<h2 id="demo">Demo</h2>
<p>Configure Git to use Mergiraf instead of its default merge heuristics. This will enhance <code>git merge</code>, <code>revert</code>, <code>rebase</code>, <code>cherry-pick</code> and more.</p>


<p>You can also keep Git's original behaviour and manually invoke Mergiraf after encountering conflicts.</p>


<div>
<p><img src="https://mergiraf.org/img/scene_1.png" alt="A giraffe observes a fighting pair"></p><p><strong>Figure 1:</strong> Two git users making inadequate use of <code>blame</code>, <code>push</code> and <code>pull</code> to resolve a conflict</p>
</div>
<h2 id="ready-to-give-it-a-try"><a href="#ready-to-give-it-a-try">Ready to give it a try?</a></h2>
<p>Head to the <a href="https://mergiraf.org/installation.html">installation</a> page and start merging nonviolently today!</p>
<h2 id="aspirations"><a href="#aspirations">Aspirations</a></h2>
<p>Mergiraf is designed with your needs in mind. Its goals are:</p>
<h3 id="dont-sweep-conflicts-under-the-rug"><a href="#dont-sweep-conflicts-under-the-rug">Don't sweep conflicts under the rug</a></h3>
<p>Syntax-aware merging heuristics can sometimes be a bit too optimistic in considering a conflict resolved. Mergiraf does its best to err on the side of caution and retain conflict markers in the file when encountering suspicious cases.</p>
<p>If it manages to resolve all conflicts on its own, it encourages you to review its mediation work via the <code>mergiraf review</code> command.
If a merge looks faulty, <a href="https://mergiraf.org/usage.html#reporting-a-bad-merge">you can report it easily</a>.</p>
<h3 id="be-fast-enough-for-interactive-use"><a href="#be-fast-enough-for-interactive-use">Be fast enough for interactive use</a></h3>
<div>
<p><img src="https://mergiraf.org/img/scene_2.png" alt="The giraffe surrounds the pair with its neck and they are surprised by its intervention"></p><p><strong>Figure 2:</strong> Mergiraf offers to mediate</p>
</div>
<p>Did you know that giraffes can run as fast as 60 kilometers per hour? Anyways. The operation of merging diverging versions of files happens routinely when working on a code base, often without you noticing as long as there aren't any conflicts. So Mergiraf tries to be quick so as not to interrupt you in your tasks.</p>
<h3 id="be-open-to-other-methods"><a href="#be-open-to-other-methods">Be open to other methods</a></h3>
<p>In many cases, line-based merging works just great and there is no need for tree-munging business. If a line-based merge is conflict-free, then Mergiraf just returns that merge (which is very quick).
One exception to this rule is <a href="https://mergiraf.org/conflicts.html#line-based-merges">when line-based merging creates duplicate keys</a>. In such a case, Mergiraf does a bit more work to resolve the issue or highlight it to you with conflict markers.</p>


                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next prefetch" href="https://mergiraf.org/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>

                        
                    </nav>
                </div>

            <nav aria-label="Page navigation">

                    <a rel="next prefetch" href="https://mergiraf.org/installation.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
            </nav>

        </div></div>]]></description>
        </item>
    </channel>
</rss>