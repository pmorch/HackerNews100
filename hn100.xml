<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 30 May 2025 21:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Beating Google's kernelCTF PoW using AVX512 (203 pts)]]></title>
            <link>https://anemato.de/blog/kctf-vdf</link>
            <guid>44137715</guid>
            <pubDate>Fri, 30 May 2025 16:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anemato.de/blog/kctf-vdf">https://anemato.de/blog/kctf-vdf</a>, See on <a href="https://news.ycombinator.com/item?id=44137715">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><header><a aria-label="anematode" href="https://anemato.de/"></a></header><main><section><article><div><header><div><dl><p><dt>Published on</dt><dd><time datetime="2025-05-29T00:00:00.000Z">Wednesday, May 28, 2025</time></dd></p></dl></div></header><div><dl><dt>Authors</dt><dd><ul><li><img alt="avatar" loading="lazy" width="38" height="38" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=48&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Favatar.png&amp;w=96&amp;q=75"><dl><dt>Name</dt><dd>Timothy Herchen</dd><dt>Twitter</dt><dd></dd></dl></li></ul></dd></dl><div><h2 id="introduction">Introduction</h2><p>In May 2025, my <a target="_blank" rel="noopener noreferrer" href="https://cor.team/">Crusaders of Rust</a> teammates William Liu (<a target="_blank" rel="noopener noreferrer" href="https://willsroot.io/">FizzBuzz101</a>) and Savy Dicanosa (<a target="_blank" rel="noopener noreferrer" href="https://syst3mfailure.io/">Syst3mFailure</a>) discovered and developed an exploit of a use-after-free bug in Linux's packet scheduler. <a target="_blank" rel="noopener noreferrer" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ac9fe7dd8e730a103ae4481147395cc73492d786">The bugfix patch</a> contains additional details. William found this bug while fuzzing Linux for his master's thesis, which I will link here upon its publication. (Congratulations, William!)</p><p>They wanted to submit the bug to Google's <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/security-research/kernelctf/rules.html">kernelCTF</a> competition for an anticipated $51,000 bounty.<sup><a href="#user-content-fn-bounty" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-bounty">1</a></sup> Unfortunately, finding the bug and writing the exploit was only the first part of the battle. This post documents my small but unique contribution to our ultimately winning the bounty.</p><h2 id="setting-the-stage">Setting the stage</h2><p>To avoid paying out lots of money, kernelCTF organizers limit the number of submissions eligible for a bounty. Every two weeks at noon UTC, the submission window opens. Only the first team who is able to connect to and exploit the server, and submit the flag to a Google Form, receives a payout; any subsequent submissions are marked as duplicates. Furthermore, to prevent excessive submissions, the connecting to kernelCTF server requires solving a "proof of work"—a function which, by design, takes a few seconds to evaluate.</p><p>In summary, <strong>the submission process has these steps</strong>:</p><ol><li>At 12:00:00 UTC, connect to the kernelCTF server.</li><li>Solve the proof of work, which takes roughly 4 seconds.</li><li>Wait for the instance to boot. (Roughly 2.5 seconds.)</li><li>Upload the exploit and run it to secure the flag. (Time elapsed depends on the exploit. Savy optimized this one to take roughly 0.55 seconds without sacrificing reliability. Wow!)</li><li>Submit the flag to a Google Form. The submission timestamp determines the winner of the "slot".</li></ol><p>Our goal was to complete all these steps in sequence, faster than all the other teams.</p><h2 id="enter-the-sweats">Enter the sweats</h2><p>Because of the large bounties, over time professional vulnerability research teams have aggressively optimized their submission process. For the May 2, 2025, submission window preceding ours, the first team to submit the flag <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/spreadsheets/d/e/2PACX-1vS1REdTA29OJftst8xN5B5x8iIUcxuK6bXdzF8G1UXCmRtoNsoQ9MbebdRdFnj6qZ0Yd7LwQfvYC2oF/pubhtml">did so 4.5 seconds after noon</a>!</p><p><img alt="kernelCTF submission time" loading="lazy" width="1518" height="156" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Fkctf-fast-submissions.png&amp;w=3840&amp;q=75"></p><p>The numbers don't seem to add up: even assuming an instant exploit and form submission, the VM boot time and proof of work already take 6.5 seconds. Looking closer, we see that the time at which the winning submission's flag was generated (highlighted in red) is one second <em>before</em> noon UTC. Yet, the timestamp is generated <em>after</em> the proof of work is solved. Did sweaty CTFers invent time travel?</p><p>Alas! Because of a rounding quirk in the kernelCTF server code (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/security-research/blob/90cc1d1fe4d4626d4c0aba4a78c02fc72fe18ac7/kernelctf/server/server.py#L192">here</a>), the VM instance actually boots at 11:59:59—so no time travel. Still, the timestamp indicates that the winning team solved the proof of work in less than a second! How could this be?</p><p>We don't know for certain, but one kernelCTF organizer postulated that they were using <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">field-programmable gate arrays</a> (FPGAs). FPGAs are custom silicon that can perform specific tasks extremely quickly, to the exclusion of general-purpose tasks. They are not only fairly expensive, but also tricky to program. If the professional team had access to an FPGA programmed to perform the proof of work, a sub-second proof of work time was conceivable.</p><p>On May 13, William messaged me on Discord seeking advice on how to optimize the proof of work so that we could preempt the competition. I had to act fast: The next submission window would open at 5 a.m. PST, May 16.<sup><a href="#user-content-fn-thesis" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-thesis">2</a></sup></p><h2 id="the-proof-of-work-the-sloth-vdf">The proof of work: The "sloth" VDF</h2><p>The proof of work (<a target="_blank" rel="noopener noreferrer" href="https://github.com/google/kctf/blob/v1/docker-images/challenge/pow.py">implemented here</a>) is a certain <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/Verifiable_delay_function">verifiable delay function</a> (VDF) known as "sloth". VDFs are cryptographic primitives which prove that a nontrivial amount of time has passed by requiring a long, serial computation. This computation outputs a proof which can be (relatively) quickly verified. Because the computation is serial, scaling to more computational resources (such as more CPU or GPU cores) does not reduce the runtime.<sup><a href="#user-content-fn-hash" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-hash">3</a></sup></p><p>The sloth VDF was introduced by <a target="_blank" rel="noopener noreferrer" href="https://csrc.nist.gov/csrc/media/events/workshop-on-elliptic-curve-cryptography-standards/documents/papers/session1-wesolowski-paper.pdf">Lenstra and Wesolowski (2015)</a>. I won't reproduce the number theory behind sloth (see page 4 of the paper for that), but to summarize matters, the function we must optimize boils down to:</p><div><pre><code><span><span>def</span> <span>sloth_root</span><span>(</span>x<span>,</span> difficulty<span>=</span><span>7337</span><span>)</span><span>:</span>
</span><span>    <span>for</span> i <span>in</span> <span>range</span><span>(</span>difficulty<span>)</span><span>:</span>             <span># repeat the inner kernel this many times</span>
</span><span>        <span>for</span> j <span>in</span> <span>range</span><span>(</span><span>1277</span><span>)</span><span>:</span>               <span># square x this many times</span>
</span><span>            x <span>=</span> <span>(</span>x <span>*</span> x<span>)</span> <span>%</span> <span>(</span><span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span><span>)</span>   <span># modulus is a Mersenne number</span>
</span><span>        x <span>=</span> x<span>.</span>bit_flip<span>(</span><span>0</span><span>)</span>                   <span># complement the LSB of x</span>
</span><span>    <span>return</span> <span>int</span><span>(</span>x<span>)</span>
</span></code></pre></div><p>where <em>x</em> is a supplied 1280-bit integer. The <em>difficulty</em> variable linearly controls how long the VDF takes to solve.</p><p>Google's reference implementation uses gmpy, which is a Python binding to the venerable <a target="_blank" rel="noopener noreferrer" href="https://gmplib.org/">GNU Multiprecision Library</a> (GMP). GMP's addition and multiplication kernels are handwritten in assembly for each target platform (<a target="_blank" rel="noopener noreferrer" href="https://github.com/gmp-mirror/gmp/blob/master/mpn/x86_64/mulx/adx/addmul_1.asm">example</a>). The loop-carried dependency of <em>x</em> means that the computation is inherently serial, so throwing more cores at the problem—at least in a naïve way—is unhelpful. Meaningfully speeding up this function was going to be <em>tough</em>.</p><h2 id="initial-progress">Initial progress</h2><p>I set out on the obvious goal of optimizing the 1280-bit modular squaring (line 4 in the code above). The first success was mathematical: Because the modulus is a Mersenne number of length 1279 bits, and the intermediate product is 2 · 1280 = 2560 bits, computing the residue actually corresponds to a handful of cheaper operations:</p><div><pre><code><span><span>def</span> <span>mod_2_1279_minus_1</span><span>(</span>x<span>)</span><span>:</span>    <span># compute x % (2 ** 1279 - 1)</span>
</span><span>    p <span>=</span> <span>2</span> <span>**</span> <span>1279</span> <span>-</span> <span>1</span>
</span><span>    r <span>=</span> <span>(</span>x <span>&amp;</span> p<span>)</span> <span>+</span> <span>(</span>x <span>&gt;&gt;</span> <span>1279</span><span>)</span>
</span><span>    <span>if</span> r <span>&gt;=</span> p<span>:</span>
</span><span>        r <span>-=</span> p
</span><span>    <span>return</span> r
</span></code></pre></div><p>I also translated the function to C++ to remove FFI overhead. The newly optimized code:</p><div><pre><code><span><span>constexpr</span> <span>int</span> MERSENNE_EXP <span>=</span> <span>1279</span><span>;</span>
</span><span>
</span><span>mpz_t low<span>,</span> high<span>,</span> p<span>;</span>
</span><span>
</span><span><span>void</span> <span>mpz_mod_mersenne</span><span>(</span>mpz_t r<span>,</span> <span>const</span> mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>// p = 2^n - 1</span>
</span><span>    <span>mpz_mod_2exp</span><span>(</span>low<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_fdiv_q_2exp</span><span>(</span>high<span>,</span> x<span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_add</span><span>(</span>r<span>,</span> low<span>,</span> high<span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span><span>mpz_cmp</span><span>(</span>r<span>,</span> p<span>)</span> <span>&gt;=</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        <span>mpz_sub</span><span>(</span>r<span>,</span> r<span>,</span> p<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>bool</span> <span>init</span><span>(</span><span>)</span> <span>{</span>
</span><span>    <span>mpz_inits</span><span>(</span>low<span>,</span> high<span>,</span> p<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_ui_pow_ui</span><span>(</span>p<span>,</span> <span>2</span><span>,</span> MERSENNE_EXP<span>)</span><span>;</span>
</span><span>    <span>mpz_sub_ui</span><span>(</span>p<span>,</span> p<span>,</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>return</span> <span>true</span><span>;</span>
</span><span><span>}</span>
</span><span><span>bool</span> _unused <span>=</span> <span>init</span><span>(</span><span>)</span><span>;</span>
</span><span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span>mpz_t x<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>mpz_mul</span><span>(</span>x<span>,</span> x<span>,</span> x<span>)</span><span>;</span>
</span><span>        <span>mpz_mod_mersenne</span><span>(</span>x<span>,</span> x<span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>)</span>
</span><span><span>{</span>
</span><span>    <span>const</span> <span>int</span> diff <span>=</span> <span>7337</span><span>;</span>
</span><span>    mpz_t x<span>,</span> r<span>;</span>
</span><span>    <span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>    <span>mpz_set_str</span><span>(</span>x<span>,</span> <span>"96729140485950483920373592475530255430"</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> diff<span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        <span>the_powmod</span><span>(</span>x<span>)</span><span>;</span>
</span><span>        <span>mpz_combit</span><span>(</span>x<span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> x<span>)</span><span>;</span>
</span><span>    std<span>::</span>cout <span>&lt;&lt;</span> <span>"x: "</span> <span>&lt;&lt;</span> str <span>&lt;&lt;</span> std<span>::</span>endl<span>;</span>
</span><span>    <span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This code ran in 1.9 seconds on my M1 Macbook Pro—a substantial improvement, and faster than similarly optimized solvers like <a target="_blank" rel="noopener noreferrer" href="https://github.com/Aplet123/kctf-pow">this one written in Rust</a>.<sup><a href="#user-content-fn-rust" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-rust">4</a></sup> William linked GMP statically, which would presumably allow some inlining, and reported a further speedup—roughly 1.4 seconds on a fancy Intel Ice Lake laptop. Not bad, but still not a guaranteed win.</p><p>The modulus no longer being a bottleneck, I considered handwriting multiplication kernels in assembly to take advantage of the multiplication being a fixed width of 1280-bit × 1280-bit → 2560-bit; the factors fit neatly into twenty 64-bit limbs, and the product fits in forty limbs.<sup><a href="#user-content-fn-limbs" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-limbs">5</a></sup> GMP's assembly kernels are generic in the bitwidth, which introduces some overhead. Unfortunately, at 1.4 seconds we were approaching the theoretical limit of multiplication throughput, which is one 64-bit × 64-bit → 128-bit multiplication per cycle on all recent hardware.<sup><a href="#user-content-fn-uops" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-uops">6</a></sup></p><h2 id="enter-avx512">Enter AVX512</h2><p><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/AVX-512">AVX512</a> is Intel's extension to the x86 ISA, first made available in 2016. It is a comprehensive overhaul of x86 SIMD programming, doubling the number and width of vector registers, adding <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Scalable_Vector_Extension">Scalable Vector Extension</a>–style mask predication, and hundreds of new instructions. It also has a troubled history: Linus Torvalds <a target="_blank" rel="noopener noreferrer" href="https://www.realworldtech.com/forum/?threadid=193189&amp;curpostid=193190">famously</a> wished it to "die a horrible death", and despite supporting it on consumer CPUs for several generations, Intel disabled support for it starting with the Alder Lake µarch (2021); support continues in the server space. Meanwhile, AMD implemented AVX512 in their Zen 4 (2022) and Zen 5 µarches for both consumer and server CPUs.</p><p>Of present interest is the AVX512 Integer Fused Multiply–Add extension (AVX512IFMA), which was introduced specifically to speed up big-integer arithmetic—see, <em>e.g.</em>, <a target="_blank" rel="noopener noreferrer" href="https://builders.intel.com/docs/networkbuilders/intel-avx-512-fast-modular-multiplication-technique-technology-guide-1710916893.pdf">Ozturk, Kantecki &amp; Yap (2024)</a>. I learned about AVX512IFMA during my competitive programming arc, optimizing submissions for <a target="_blank" rel="noopener noreferrer" href="https://judge.yosupo.jp/">judge.yosupo.jp</a>. The extension introduces two new instructions which operate on vector registers:</p><ul><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52luq">vpmadd52luq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add Low 52-Bit Products to 64-Bit Accumulators</li><li><a target="_blank" rel="noopener noreferrer" href="https://www.felixcloutier.com/x86/vpmadd52huq">vpmadd52huq</a> – Packed Multiply of Unsigned 52-Bit Unsigned Integers and Add High 52-Bit Products to 64-Bit Accumulators</li></ul><p>Essentially, the instructions perform the following operation (assuming the high 12 bits of each element in <em>a</em> and <em>b</em> are zero):</p><div><pre><code><span><span>// vpmadd52luq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52luq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&amp;</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// vpmadd52huq dst, a, b</span>
</span><span><span>void</span> <span>vpmadd52huq</span><span>(</span><span>uint64_t</span> dst<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> a<span>[</span><span>8</span><span>]</span><span>,</span> <span>uint64_t</span> b<span>[</span><span>8</span><span>]</span><span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>8</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>        dst<span>[</span>i<span>]</span> <span>+=</span> <span>(</span><span>(</span>__uint128_t<span>)</span>a<span>[</span>i<span>]</span> <span>*</span> b<span>[</span>i<span>]</span><span>)</span> <span>&gt;&gt;</span> <span>52</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>In simpler terms, the instructions perform the low and high halves of a 52 × 52 → 104 multiplication, and accumulate the result into the destination register. At the execution unit level, the instructions reuse the multipliers used for double-precision floating point, and thus don't necessitate much extra silicon. They also have fantastic throughput: on Zen 5, which has a full 512-bit datapath, we can execute two of these instructions per clock!</p><p>At this point I was confident that I could make an extraordinarily fast VDF solver. All that was left was to implement it in two days....</p><p><img src="https://anemato.de/static/images/pow-is-broken.png" alt="yapping" width="500"></p><h2 id="the-game-plan">The game plan</h2><p>The natural radix for the AVX512IFMA extensions is 2<sup>52</sup>, <em>i.e.</em> 52-bit limbs stored in 64-bit words, so I let ChatGPT write a simple converter between GMP's representation and the 52-bit representation. We need ⌈1280 / 52⌉ = 25 limbs, which requires four 512-bit "zmm" registers (each register can store eight limbs, so the last register will only store one).</p><p>The first step is squaring the integer into a 50-limb intermediate product. We use a simple symmetry to almost halve the number of required multiplications, breaking up the desired value into two terms:</p><p>(a<sub>24</sub>2<sup>52·24</sup> + a<sub>23</sub>2<sup>52·23</sup> + ... + a<sub>0</sub>)<sup>2</sup> = (<span>a<sub>24</sub><sup>2</sup></span>2<sup>52·48</sup> + <span>a<sub>23</sub><sup>2</sup></span>2<sup>52·46</sup> + ... + <span>a<sub>0</sub><sup>2</sup></span>) + 2 <span><small>i,j≤24</small><span>∑</span><small>i=0,j&gt;i</small></span><span>a<sub>i</sub>a<sub>j</sub></span> 2<sup>52·(i + j)</sup></p><p>Each of the yellow-highlighted multiplications produces a low term (furnished by <em>vpmadd52luq</em>) and a high term (furnished by <em>vpmadd52huq</em>).</p><h2 id="arranging-the-multiplications">Arranging the multiplications</h2><p>First consider the <em>second</em> term above, the double summation highlighted in red. We want to reduce the number of shuffles necessary to get all the terms in the correct place, and use the "free" 64-bit accumulation as much as possible. One way to do this is to multiply a sliding window of 8 contiguous limbs by a single multiplier limb; all the output words, for both the low and high halves, will also be contiguous in the output. We can also use the merge masking feature to prevent accumulation of products that shouldn't be in the final sum, <em>e.g.</em>, pairs where i = j. By selecting these windows and multipliers correctly, we can minimize the number of wasted multiplications.</p><div><pre><code><span><span>// Computing the second term</span>
</span><span><span>// input contains the 25 52-bit limbs, stored in 64-bit words</span>
</span><span><span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>  <span>// so that loads OOB are still valid</span>
</span><span><span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>    <span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span><span>}</span><span>;</span>
</span><span>
</span><span><span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span><span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span>// Seven zmm accumulators are necessary</span>
</span><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>-</span><span>7</span><span>;</span> i <span>&lt;=</span> <span>24</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    <span>// Sliding window</span>
</span><span>    __m512i m1 <span>=</span> <span>_mm512_loadu_si512</span><span>(</span>data <span>+</span> i<span>)</span><span>;</span> <span>// Load the current window of 8 elements</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>        <span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>        <span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>        <span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>        <span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>        <span>// Process low halves</span>
</span><span>        <span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// Discard out of bounds multiplications</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>lo<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>        <span>// Process high halves</span>
</span><span>        <span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>            <span>// ditto</span>
</span><span>            __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>                accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_set1_epi64</span><span>(</span>data<span>[</span>hi<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>            <span>}</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>As an example, accumulator <em>accum[1]</em> contains output limbs 8 through 15, inclusive, and the following accumulations are executed (in this order):</p><div><pre><code><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>1</span> by limb <span>7</span> <span>with</span> mask <span>10000000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>1</span> by limb <span>6</span> <span>with</span> mask <span>11000000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>2</span> by limb <span>6</span> <span>with</span> mask <span>11100000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>2</span> by limb <span>5</span> <span>with</span> mask <span>11110000</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>3</span> by limb <span>5</span> <span>with</span> mask <span>11111000</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>3</span> by limb <span>4</span> <span>with</span> mask <span>11111100</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>4</span> by limb <span>4</span> <span>with</span> mask <span>11111110</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>4</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>5</span> by limb <span>3</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>5</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>6</span> by limb <span>2</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>6</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>7</span> by limb <span>1</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> high halves<span>:</span> <span>window</span> <span>7</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span><span><span>Accumulating</span> low halves<span>:</span> <span>window</span> <span>8</span> by limb <span>0</span> <span>with</span> mask <span>11111111</span>
</span></code></pre></div><p>Here, window <em>i</em> contains limbs <em>i</em> through <em>i+7</em> inclusive. Note that the masks mostly contain ones, indicating that we are not wasting too much multiplication throughput on masked-out elements.</p><p>Computing the first term is easier. We just square each element and interleave the low and high words:</p><div><pre><code><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>    __m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>        accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>Finally, we need to implement the reduction modulo 2<sup>1279</sup>-1. This is just a matter of selecting the upper 1279 bits and adding them to the lower 1279 bits. It's worth noting that at this point, the accumulator elements may exceed 2<sup>52</sup>-1, but we can delay carry propagation until after the addition.</p><div><pre><code><span>__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span><span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span><span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    __m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>    __m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>    group_out <span>=</span> carries<span>;</span>
</span><span>    carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span><span>}</span>
</span><span>
</span><span><span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>    <span>goto</span> carry2<span>;</span>
</span><span><span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>We then need to subtract the Mersenne modulus if the addition is too large, but this is easy to check: we perform the subtraction <em>iff</em> the 1280th bit is 1. Subtracting 2<sup>1279</sup>-1 is equivalent to subtracting 2<sup>1279</sup> (<em>i.e.</em>, zeroing the 1280th bit) followed by adding 1 to the least-significant limb. Because the subtraction occurs with 50% probability, we do it in a branchless manner:</p><div><pre><code><span><span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> 1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span>1ULL <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span><span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span></code></pre></div><p>There is a tiny chance that an overflow occurs in this last step: if the last limb happened to be exactly 2<sup>52</sup>-1, then we'd need to propagate the carry. However, because PoWs are randomly generated, the probability this happens on any given run is about 2 in a billion—so I just ignored it.</p><p>At this point, the PoW was taking about <strong>0.45 seconds</strong> on a rented Ryzen 9950X, which is a fast Zen 5 chip. Very promising!</p><h2 id="keeping-the-multiplyadds-in-flight">Keeping the multiply–adds in flight</h2><p>The multiply–add instructions have a latency of 4 cycles, and 2 can start every cycle. Thus, we need at least eight accumulators to fully saturate the multipliers instead of bottlenecking on latency, but we only have seven (and some of them are only used occasionally). The solution is to have fourteen accumulators—one set for the low halves and one set for the high halves—then merge them at the end:</p><div><pre><code><span>__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>
</span><span><span>// ... fmadd spam goes here ...</span>
</span><span>
</span><span><span>// Fold high and low halves</span>
</span><span><span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>    accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to roughly <strong>0.32 seconds</strong>. The expanded AVX512 register file, with 32 zmm registers, came in clutch here.</p><h2 id="taming-the-register-allocator">Taming the register allocator</h2><p>Inspecting the assembly revealed that both GCC and clang were unrolling the loop, converting the <em>_mm512_set1_epi64</em> instructions into <em>vbroadcastsd zmm, m64</em> instructions—one per limb—and then running out of vector registers during regalloc. Instead of rematerializing the values, they would stack-spill and reload the broadcasted vectors, causing considerable overhead.<sup><a href="#user-content-fn-numberworld" aria-describedby="footnote-label" data-footnote-ref="true" id="user-content-fnref-numberworld">7</a></sup></p><p>My solution was to use inline assembly to force the <em>vpmadd52luq</em>/<em>vpmadd52huq</em> instructions to use a <em>memory broadcast operand</em> for the multiplier limb. Instructions encoded with such an operand copy a single 32- or 64-bit element from memory to all elements of a vector operand, without consuming an architectural register. Moreover, this broadcast load does not consume any vector ALU resources: it is handled entirely by the load unit!</p><p>Now that we're using asm, the compiler can't remove masks of all 1s, so for optimal encoding we need separate asm for the all 1s case. Finally, when the multiplier limb happens to be at index zero in one of the zmm registers (<em>i.e.</em>, multiples of 8), we use <em>vpbroadcastq</em> to splat it from register to register, which I measured to be a performance improvement over loading it from memory. The accumulation sequence is now:</p><div><pre><code><span><span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>    <span>// Multiples of 8 are handled by a register broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>    __mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>
</span><span>    <span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>        <span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>            FOR_EACH_OFFS
</span><span>            <span>default</span><span>:</span>
</span><span>            accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>(This is just for the low set of accumulators; the high set is nearly identical.) At this point, the PoW was taking roughly <strong>0.23 seconds</strong>.</p><h2 id="creating-the-windows-without-touching-memory">Creating the windows without touching memory</h2><p>To compute the "windows", we are storing the integer to memory in an aligned fashion, then loading from it in an unaligned fashion. This is a classic situation that causes a <a target="_blank" rel="noopener noreferrer" href="https://easyperf.net/blog/2018/03/09/Store-forwarding"><em>store-forwarding stall</em></a>, which further lengthens the critical path (the multiplications cannot commence until a window is loaded from memory). A better solution is to use the <em>valignq</em> instruction, which lets us simulate an unaligned load from the <em>clumps</em> array containing our integer.</p><div><pre><code><span>__m512i m1<span>;</span>
</span><span><span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>    m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span><span>}</span> <span>else</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>    <span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>        <span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>        <span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span></code></pre></div><p>This brought the PoW down to <strong>0.21 seconds</strong>. At this point, I decided to call it quits and sent my teammates the final C code.</p><h2 id="may-16-show-time">May 16: Show time!</h2><p>My friends got up at 4:30 a.m. PST, May 16, to prepare for the final submission; I couldn't be assed to be awake, so I slept until 6:30. They spun up a Zen 5 Google Cloud server in the Netherlands, geographically closest to the Google Form submission server, to minimize latency. A few minutes before 5:00, they recorded an intercepted POST request submitting the Google form, but with a dummy flag. The form submission program was devised and optimized by Bryce Casaje (<a target="_blank" rel="noopener noreferrer" href="https://brycec.me/">strellic</a>) and Larry Yuan (<a target="_blank" rel="noopener noreferrer" href="https://larry.sh/">ehhthing</a>). <a target="_blank" rel="noopener noreferrer" href="https://max.xz.ax/">Max Cai</a> also assisted in development and submission. Then at 5:00, the server connected to the kernelCTF server, solved the proof of work, ran Savy's optimized exploit, inserted the flag into the POST request, and sent it off....</p><p><img alt="lol" loading="lazy" width="1526" height="94" decoding="async" data-nimg="1" srcset="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=1920&amp;q=75 1x, https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75 2x" src="https://anemato.de/_next/image?url=%2Fstatic%2Fimages%2Four-kctf-submission.png&amp;w=3840&amp;q=75"></p><p>We got it in 3.6 seconds—the fastest-ever kernelCTF submission! Later that day, the kernelCTF organizers confirmed our eligibility for the bounty and we all breathed a collective sigh of relief. Again, congratulations to Savy and William for discovering and exploiting this bug! Thanks to them for presenting me with the challenge, and thanks to my CSE 260 professor Bryan Chin (<a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/ucsd.edu/bryan-chin/home">website</a>) for what he taught us about program optimization.</p><h2 id="the-end-of-an-era">The end of an era</h2><p>On May 28, kernelCTF organizer koczkatamas announced that the proof of work was being removed:</p><p><img src="https://anemato.de/static/images/pow-is-gone.png" alt="PoW is gone" width="400"></p><p>On the one hand, it's sad that we no longer have a competitive advantage, and the slot race becomes purely about exploit duration and network latency. On the other hand, at least people don't need to buy expensive FPGAs, or pull out their inline asm knowledge, to be on equal footing with the professionals. It also frees me to release this post!</p><p>Please message me on Discord (@forevermilk) if you have any comments or questions. I am also researching VDFs that are more resilient to assembly-level optimizations; if you have any ideas or would like to collaborate, I'm all ears.</p><h2 id="the-final-solver">The final solver</h2><p>This code is the product (ha!) of about 12 hours of work across May 14 and 15, and it is correspondingly unclean. Consider it released under the GNU AGPL 3.0.</p><div><pre><code><span><span>// Written by Timothy Herchen</span>
</span><span><span>// gcc main.c -O3 -march=znver5 -masm=intel -lgmp</span>
</span><span><span><span>#</span><span>include</span> <span>&lt;immintrin.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;gmp.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;string.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdlib.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdio.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stdint.h&gt;</span></span>
</span><span><span><span>#</span><span>include</span> <span>&lt;stddef.h&gt;</span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>uint128_t</span> <span>__uint128_t</span></span>
</span><span>
</span><span><span>void</span> <span>gmp_to_array</span><span>(</span><span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> <span>*</span>array<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> N<span>;</span>
</span><span>    <span>mpz_export</span><span>(</span>array<span>,</span> <span>&amp;</span>N<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> mpz<span>)</span><span>;</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> N <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// Destroys the array</span>
</span><span><span>void</span> <span>array_to_gmp</span><span>(</span><span>uint64_t</span> <span>*</span>array<span>,</span> <span>mpz_t</span> mpz<span>,</span> <span>uint64_t</span> words<span>)</span> <span>{</span>
</span><span>    <span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>,</span> j <span>=</span> words <span>-</span> <span>1</span><span>;</span> i <span>&lt;</span> j<span>;</span> <span>++</span>i<span>,</span> <span>--</span>j<span>)</span> <span>{</span>
</span><span>	    <span>uint64_t</span> temp <span>=</span> array<span>[</span>i<span>]</span><span>;</span>
</span><span>	    array<span>[</span>i<span>]</span> <span>=</span> array<span>[</span>j<span>]</span><span>;</span>
</span><span>	    array<span>[</span>j<span>]</span> <span>=</span> temp<span>;</span>
</span><span>    <span>}</span>
</span><span>    <span>mpz_import</span><span>(</span>mpz<span>,</span> words<span>,</span> <span>1</span><span>,</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> array<span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_64_to_52</span><span>(</span><span>uint64_t</span> <span>*</span>limbs<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>limbs<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>64</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>52</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>52</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>52</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>size_t</span> <span>convert_radix_52_to_64</span><span>(</span><span>uint64_t</span> <span>*</span>in<span>,</span> <span>uint64_t</span> <span>*</span>out<span>,</span> <span>size_t</span> count<span>)</span> <span>{</span>
</span><span>    <span>size_t</span> out_index <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>int</span> bits_in_buffer <span>=</span> <span>0</span><span>;</span>
</span><span>    <span>uint128_t</span> buffer <span>=</span> <span>0</span><span>;</span>
</span><span>
</span><span>    <span>for</span> <span>(</span><span>size_t</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> count<span>;</span> i<span>++</span><span>)</span> <span>{</span>
</span><span>        buffer <span>|=</span> <span>(</span><span>(</span><span>uint128_t</span><span>)</span>in<span>[</span>i<span>]</span><span>)</span> <span>&lt;&lt;</span> bits_in_buffer<span>;</span>
</span><span>        bits_in_buffer <span>+=</span> <span>52</span><span>;</span>
</span><span>
</span><span>        <span>while</span> <span>(</span>bits_in_buffer <span>&gt;=</span> <span>64</span><span>)</span> <span>{</span>
</span><span>            out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>(</span><span>uint128_t</span><span>)</span><span>1ULL</span> <span>&lt;&lt;</span> <span>64</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>            buffer <span>&gt;&gt;=</span> <span>64</span><span>;</span>
</span><span>            bits_in_buffer <span>-=</span> <span>64</span><span>;</span>
</span><span>        <span>}</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>// Handle remaining bits if any</span>
</span><span>    <span>if</span> <span>(</span>bits_in_buffer <span>&gt;</span> <span>0</span><span>)</span> <span>{</span>
</span><span>        out<span>[</span>out_index<span>++</span><span>]</span> <span>=</span> <span>(</span><span>uint64_t</span><span>)</span><span>(</span>buffer <span>&amp;</span> <span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> bits_in_buffer<span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>    <span>}</span>
</span><span>
</span><span>    <span>return</span> out_index<span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>shift_down_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>,</span> __m512i high_1279<span>[</span><span>4</span><span>]</span><span>)</span> <span>{</span>
</span><span>	__m512i p <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>3</span><span>;</span> i <span>&gt;=</span> <span>0</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		__m512i down_31 <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>31</span><span>)</span><span>;</span>
</span><span>		__m512i higher_21 <span>=</span> <span>_mm512_slli_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i <span>+</span> <span>3</span><span>]</span><span>,</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>,</span> <span>21</span><span>)</span><span>;</span>
</span><span>		high_1279<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_alignr_epi64</span><span>(</span>p<span>,</span> higher_21<span>,</span> <span>1</span><span>)</span><span>,</span> down_31<span>)</span><span>;</span>
</span><span>		p <span>=</span> higher_21<span>;</span>
</span><span>	<span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>__attribute__</span><span>(</span><span>(</span>always_inline<span>)</span><span>)</span> <span>void</span> <span>filter_low_1279</span><span>(</span>__m512i accum<span>[</span><span>7</span><span>]</span><span>)</span> <span>{</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// This code is extremely latency bound, as you'd expect for this kind of stupid PoW, so certain things are done that would lower</span>
</span><span><span>// throughput (e.g. on a hyperthreaded device doing two of these at once) but which lower the latency</span>
</span><span><span>void</span> <span>the_powmod</span><span>(</span><span>uint64_t</span> <span>*</span> __restrict__ input<span>,</span> <span>uint64_t</span> <span>*</span> __restrict__ result<span>)</span> <span>{</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> padded_data<span>[</span><span>8</span> <span>*</span> <span>6</span><span>]</span> <span>=</span> <span>{</span><span>0</span><span>}</span><span>;</span>
</span><span>	<span>uint64_t</span> <span>*</span>data <span>=</span> padded_data <span>+</span> <span>8</span><span>;</span>
</span><span>
</span><span>	__m512i clumps<span>[</span><span>4</span><span>]</span> <span>=</span> <span>{</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input<span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>8</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>16</span><span>)</span><span>,</span>
</span><span>		<span>_mm512_loadu_si512</span><span>(</span>input <span>+</span> <span>24</span><span>)</span>
</span><span>	<span>}</span><span>;</span>
</span><span>
</span><span>	<span>for</span> <span>(</span><span>int</span> pow_i <span>=</span> <span>0</span><span>;</span> pow_i <span>&lt;</span> <span>1277</span><span>;</span> <span>++</span>pow_i<span>)</span> <span>{</span>
</span><span>		<span>// Use aligned stores to make sure we are doing things well</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>_mm512_store_si512</span><span>(</span>data <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now data[x] gives us the xth limb</span>
</span><span><span><span>#</span><span>define</span> <span>ZERO</span> <span><span>_mm512_setzero_si512</span><span>(</span><span>)</span></span></span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>ELIDE_MASKS_IF_POSSIBLE</span> <span><span>1</span></span></span>
</span><span>
</span><span>	__m512i accum<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO <span>/* 0-7 */</span><span>,</span> ZERO <span>/* 8-15, etc. */</span><span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// The accumulation is latency bound (lat. 4 cycles, so we need at least 8 accumulators to keep the madds in flight)</span>
</span><span>	__m512i accum_hi<span>[</span><span>7</span><span>]</span> <span>=</span> <span>{</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO<span>,</span> ZERO <span>}</span><span>;</span>
</span><span>	<span>// We'll laboriously build up the upper triangle of the 2560-bit product using 52x52-&gt;104 multiplies</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>24</span><span>;</span> i <span>&gt;=</span> <span>-</span><span>7</span><span>;</span> <span>--</span>i<span>)</span> <span>{</span>
</span><span>		<span>// Sliding window</span>
</span><span>		__m512i m1<span>;</span>
</span><span>		<span>if</span> <span>(</span><span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
</span><span>			m1 <span>=</span> clumps<span>[</span>i <span>/</span> <span>8</span><span>]</span><span>;</span>
</span><span>		<span>}</span> <span>else</span> <span>{</span>
</span><span>			<span>// Emulate an unaligned load from memory. Unaligned loads are very expensive on Zen 5 so this is helpful</span>
</span><span><span><span>#</span><span>define</span> <span>UNALIGNED</span><span><span>(</span>S<span>)</span> <span>case</span> S<span>:</span> <span>{</span> m1 <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>]</span><span>,</span> i <span>&lt;</span> <span>0</span> <span>?</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span> <span>:</span> clumps<span>[</span><span>(</span>i<span>+</span><span>8</span><span>)</span><span>/</span><span>8</span><span>-</span><span>1</span><span>]</span><span>,</span> S<span>)</span><span>;</span> <span>break</span><span>;</span> <span>}</span></span></span>
</span><span>			<span>switch</span> <span>(</span>i <span>&amp;</span> <span>7</span><span>)</span> <span>{</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>1</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>2</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>3</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>4</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>5</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>6</span><span>)</span>
</span><span>				<span>UNALIGNED</span><span>(</span><span>7</span><span>)</span>
</span><span>				<span>default</span><span>:</span> <span>abort</span><span>(</span><span>)</span><span>;</span>
</span><span>			<span>}</span>
</span><span>		<span>}</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>100</span></span></span>
</span><span>		<span>for</span> <span>(</span><span>int</span> j <span>=</span> <span>0</span><span>,</span> k <span>=</span> <span>0</span><span>;</span> j <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>j<span>,</span> k <span>+=</span> <span>8</span><span>)</span> <span>{</span>
</span><span>			<span>// Decide whether to accumulate into accum[j], which should happen if there</span>
</span><span>			<span>// is at least one element shared between the jth accumulator and [i, i+7]</span>
</span><span>			<span>int</span> lo <span>=</span> k <span>-</span> i<span>;</span>
</span><span>			<span>int</span> hi <span>=</span> k <span>-</span> i <span>-</span> <span>1</span><span>;</span>
</span><span>			<span>if</span> <span>(</span>lo <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> lo <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span>				<span>// Multiples of 8 are handled by a broadcast instead of a memory load for efficiency</span>
</span><span><span><span>#</span><span>define</span> <span>FOR_EACH_OFFS</span> <span><span>X</span><span>(</span><span>1</span><span>)</span> <span>X</span><span>(</span><span>2</span><span>)</span> <span>X</span><span>(</span><span>3</span><span>)</span> <span>X</span><span>(</span><span>4</span><span>)</span> <span>X</span><span>(</span><span>5</span><span>)</span> <span>X</span><span>(</span><span>6</span><span>)</span> <span>X</span><span>(</span><span>7</span><span>)</span> <span>X</span><span>(</span><span>9</span><span>)</span> <span>X</span><span>(</span><span>10</span><span>)</span> <span>X</span><span>(</span><span>11</span><span>)</span> <span>X</span><span>(</span><span>12</span><span>)</span> <span>X</span><span>(</span><span>13</span><span>)</span> <span>X</span><span>(</span><span>14</span><span>)</span> <span>X</span><span>(</span><span>15</span><span>)</span> <span>X</span><span>(</span><span>17</span><span>)</span> <span>X</span><span>(</span><span>18</span><span>)</span> <span>X</span><span>(</span><span>19</span><span>)</span> <span>X</span><span>(</span><span>20</span><span>)</span> <span>X</span><span>(</span><span>21</span><span>)</span> <span>X</span><span>(</span><span>22</span><span>)</span> <span>X</span><span>(</span><span>23</span><span>)</span></span></span>
</span><span>				<span>// Discard those entries where lo &gt; i</span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>lo <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>lo <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// we use inline asm with a memory broadcast after enough regs because the register allocator does not enjoy this type of setup</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span>  </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52luq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>					<span>switch</span> <span>(</span>lo<span>)</span> <span>{</span>
</span><span>						FOR_EACH_OFFS
</span><span>						<span>default</span><span>:</span>
</span><span>						accum<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52lo_epu64</span><span>(</span>accum<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>lo<span>/</span><span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>					<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>			<span>if</span> <span>(</span>hi <span>&gt;=</span> <span>0</span> <span>&amp;&amp;</span> hi <span>&lt;=</span> <span>24</span><span>)</span> <span>{</span>
</span><span><span><span>#</span><span>undef</span> <span>X</span></span>
</span><span>				__mmask8 sel <span>=</span> <span>(</span><span>uint8_t</span><span>)</span><span>(</span>hi <span>&lt;</span> i <span>?</span> <span>-</span><span>1ULL</span> <span>:</span> <span>(</span><span>-</span><span>1ULL</span> <span>&lt;&lt;</span> <span>(</span>hi <span>-</span> i <span>+</span> <span>1</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>// see above</span>
</span><span>				<span>if</span> <span>(</span>sel <span>==</span> <span>(</span><span>uint8_t</span><span>)</span><span>-</span><span>1</span> <span>&amp;&amp;</span> ELIDE_MASKS_IF_POSSIBLE<span>)</span> <span>{</span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0, %1, %2%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> m1<span>,</span> <span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span> <span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span> <span>else</span> <span>if</span> <span>(</span>sel<span>)</span> <span>{</span>
</span><span>
</span><span><span><span>#</span><span>define</span> <span>X</span><span><span>(</span>n<span>)</span> <span>case</span> n<span>:</span> <span>asm</span> <span>volatile</span> <span>(</span></span><span>"vpmadd52huq %0 %{%1%}, %2, %3%{1to8%}"</span> <span><span>:</span> </span><span>"+v"</span><span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>)</span> <span>:</span> </span><span>"Yk"</span><span><span>(</span>sel<span>)</span><span>,</span> </span><span>"v"</span><span><span>(</span>m1<span>)</span><span>,</span> </span><span>"m"</span><span><span>(</span>data<span>[</span>n<span>]</span><span>)</span><span>)</span><span>;</span> <span>break</span><span>;</span></span></span>
</span><span>				<span>switch</span> <span>(</span>hi<span>)</span> <span>{</span>
</span><span>					FOR_EACH_OFFS
</span><span>					<span>default</span><span>:</span>
</span><span>					accum_hi<span>[</span>j<span>]</span> <span>=</span> <span>_mm512_mask_madd52hi_epu64</span><span>(</span>accum_hi<span>[</span>j<span>]</span><span>,</span> sel<span>,</span> m1<span>,</span><span>_mm512_broadcast_i32x2</span><span>(</span><span>_mm512_castsi512_si128</span><span>(</span>clumps<span>[</span>hi <span>/</span> <span>8</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>				<span>}</span>
</span><span>				<span>}</span>
</span><span>			<span>}</span>
</span><span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Fold high and low halves, and double all the accumulators</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>7</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum_hi<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now add the diagonal from the accumulators because they weren't yet computed</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512d diag_lo <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52lo_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512d diag_hi <span>=</span> <span>_mm512_castsi512_pd</span><span>(</span><span>_mm512_madd52hi_epu64</span><span>(</span>ZERO<span>,</span> clumps<span>[</span>i<span>]</span><span>,</span> clumps<span>[</span>i<span>]</span><span>)</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_lo <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>11</span><span>,</span> <span>3</span><span>,</span> <span>10</span><span>,</span> <span>2</span><span>,</span> <span>9</span><span>,</span> <span>1</span><span>,</span> <span>8</span><span>,</span> <span>0</span><span>)</span><span>;</span>
</span><span>		__m512i shuf_hi <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>15</span><span>,</span> <span>7</span><span>,</span> <span>14</span><span>,</span> <span>6</span><span>,</span> <span>13</span><span>,</span> <span>5</span><span>,</span> <span>12</span><span>,</span> <span>4</span><span>)</span><span>;</span>
</span><span>	        accum<span>[</span><span>2</span> <span>*</span> i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_lo<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>if</span> <span>(</span>i <span>!=</span> <span>3</span><span>)</span> <span>{</span>
</span><span>			accum<span>[</span><span>2</span> <span>*</span> i <span>+</span> <span>1</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>2</span><span>*</span>i<span>+</span><span>1</span><span>]</span><span>,</span> <span>_mm512_castpd_si512</span><span>(</span><span>_mm512_permutex2var_pd</span><span>(</span>diag_lo<span>,</span> shuf_hi<span>,</span> diag_hi<span>)</span><span>)</span><span>)</span><span>;</span>
</span><span>		<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now propagate carries in parallel in radix 2^52</span>
</span><span>	__m512i low_52_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>	__m512i hi_12_bits <span>=</span> <span>_mm512_set1_epi64</span><span>(</span><span>~</span><span>(</span><span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>52</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Now add the high 1279 bits to the low 1279 bits</span>
</span><span>	__m512i high_1279<span>[</span><span>4</span><span>]</span><span>;</span>
</span><span>	<span>shift_down_1279</span><span>(</span>accum<span>,</span> high_1279<span>)</span><span>;</span>
</span><span>	<span>filter_low_1279</span><span>(</span>accum<span>)</span><span>;</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> high_1279<span>[</span>i<span>]</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>{</span>
</span><span>carry2<span>:</span><span>;</span>
</span><span>	__m512i carry_test <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span>	__m512i group_out <span>=</span> <span>_mm512_setzero_si512</span><span>(</span><span>)</span><span>;</span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>7</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		__m512i carries <span>=</span> <span>_mm512_srli_epi64</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> <span>52</span><span>)</span><span>;</span>
</span><span>		__m512i carries_into <span>=</span> <span>_mm512_alignr_epi64</span><span>(</span>carries<span>,</span> group_out<span>,</span> <span>7</span><span>)</span><span>;</span>
</span><span>		accum<span>[</span>i<span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span><span>_mm512_and_si512</span><span>(</span>accum<span>[</span>i<span>]</span><span>,</span> low_52_bits<span>)</span><span>,</span> carries_into<span>)</span><span>;</span>
</span><span>		group_out <span>=</span> carries<span>;</span>
</span><span>		carry_test <span>=</span> <span>_mm512_and_si512</span><span>(</span>carry_test<span>,</span> accum<span>[</span>i<span>]</span><span>)</span><span>;</span> <span>// improve latency over a series of masked tests</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>if</span> <span>(</span><span>__builtin_expect</span><span>(</span><span>_mm512_test_epi64_mask</span><span>(</span>carry_test<span>,</span> hi_12_bits<span>)</span><span>,</span> <span>0</span><span>)</span><span>)</span> <span>{</span>
</span><span>		<span>goto</span> carry2<span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>// Now compare with 2^1279 - 1; if &gt;=, subtract 2^1279 - 1. classic Mersenne number modulo algorithm</span>
</span><span>	__m512i bit_1279 <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span><span>;</span>
</span><span>	__m512i mask_off <span>=</span> <span>_mm512_set_epi64</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>(</span><span>1ULL</span> <span>&lt;&lt;</span> <span>31</span><span>)</span> <span>-</span> <span>1</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// Branchless approach appears to save about 2 ns per iteration. Also, we stay in vector regs and don't use a test mask here because it tends to be slower</span>
</span><span>	__m512i cmp <span>=</span> <span>_mm512_and_epi64</span><span>(</span>accum<span>[</span><span>3</span><span>]</span><span>,</span> bit_1279<span>)</span><span>;</span>
</span><span>	accum<span>[</span><span>0</span><span>]</span> <span>=</span> <span>_mm512_add_epi64</span><span>(</span>accum<span>[</span><span>0</span><span>]</span><span>,</span> <span>_mm512_srli_epi64</span><span>(</span>cmp<span>,</span> <span>31</span><span>)</span><span>)</span><span>;</span>  <span>// potentially +1 to last word</span>
</span><span>	accum<span>[</span><span>3</span><span>]</span> <span>=</span> <span>_mm512_and_si512</span><span>(</span>mask_off<span>,</span> accum<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>// TODO 1/2^52 chance of error here due to carry -- check it</span>
</span><span>
</span><span><span><span>#</span><span>pragma</span> <span>GCC unroll <span>4</span></span></span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>4</span><span>;</span> <span>++</span>i<span>)</span> <span>{</span>
</span><span>		clumps<span>[</span>i<span>]</span> <span>=</span> accum<span>[</span>i<span>]</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result<span>,</span> clumps<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>8</span><span>,</span> clumps<span>[</span><span>1</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>16</span><span>,</span> clumps<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>	<span>_mm512_storeu_si512</span><span>(</span>result <span>+</span> <span>24</span><span>,</span> clumps<span>[</span><span>3</span><span>]</span><span>)</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>int</span> <span>main</span><span>(</span><span>int</span> argc<span>,</span> <span>char</span> <span>*</span><span>*</span>argv<span>)</span> <span>{</span>
</span><span>	<span>if</span> <span>(</span>argc <span>&lt;</span> <span>3</span><span>)</span> <span>{</span>
</span><span>		<span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"Usage: %s &lt;y&gt; &lt;difficulty&gt;"</span><span>,</span> argv<span>[</span><span>0</span><span>]</span><span>)</span><span>;</span>
</span><span>		<span>return</span> <span>1</span><span>;</span>
</span><span>	<span>}</span>
</span><span>
</span><span>	<span>mpz_t</span> x<span>,</span> r<span>;</span>
</span><span>	<span>mpz_inits</span><span>(</span>x<span>,</span> r<span>,</span> <span>NULL</span><span>)</span><span>;</span>
</span><span>	<span>mpz_set_str</span><span>(</span>x<span>,</span> argv<span>[</span><span>1</span><span>]</span><span>,</span> <span>10</span><span>)</span><span>;</span>
</span><span>	<span>int</span> difficulty <span>=</span> <span>atoi</span><span>(</span>argv<span>[</span><span>2</span><span>]</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> abc<span>[</span><span>400</span><span>]</span><span>;</span>
</span><span>	<span>_Alignas</span><span>(</span><span>64</span><span>)</span> <span>uint64_t</span> poop<span>[</span><span>32</span><span>]</span><span>;</span>
</span><span>
</span><span>	<span>gmp_to_array</span><span>(</span>x<span>,</span> abc<span>)</span><span>;</span>
</span><span>
</span><span>	<span>size_t</span> N <span>=</span> <span>convert_radix_64_to_52</span><span>(</span>abc<span>,</span> poop<span>,</span> <span>20</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>uint64_t</span> squared<span>[</span><span>1000</span><span>]</span><span>;</span>
</span><span>	<span>for</span> <span>(</span><span>int</span> i <span>=</span> <span>0</span><span>;</span> i <span>&lt;</span> difficulty<span>;</span> <span>++</span>i<span>)</span> <span>{</span> <span>// specified algorithm</span>
</span><span>		<span>the_powmod</span><span>(</span>poop<span>,</span> squared<span>)</span><span>;</span>
</span><span>		squared<span>[</span><span>0</span><span>]</span> <span>^=</span> <span>1</span><span>;</span>
</span><span>		<span>memcpy</span><span>(</span>poop<span>,</span> squared<span>,</span> <span>25</span> <span>*</span> <span>sizeof</span><span>(</span><span>uint64_t</span><span>)</span><span>)</span><span>;</span>
</span><span>	<span>}</span>
</span><span>	<span>convert_radix_52_to_64</span><span>(</span>squared<span>,</span> abc<span>,</span> <span>48</span><span>)</span><span>;</span>
</span><span>	<span>array_to_gmp</span><span>(</span>abc<span>,</span> r<span>,</span> <span>1280</span><span>/</span><span>64</span><span>)</span><span>;</span>
</span><span>
</span><span>	<span>char</span> <span>*</span>str <span>=</span> <span>mpz_get_str</span><span>(</span><span>NULL</span><span>,</span> <span>10</span><span>,</span> r<span>)</span><span>;</span>
</span><span>	<span>printf</span><span>(</span><span>"%s"</span><span>,</span> str<span>)</span><span>;</span>
</span><span>	<span>return</span> <span>0</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div><p>I hope you enjoyed my first-ever blog post. Hopefully there will be many more.</p><section data-footnotes="true"><h2 id="footnote-label">Footnotes</h2><ol><li id="user-content-fn-bounty"><p>$21,337 base reward, $10,000 for stability (successful exploitation on &gt;90% of runs), and $20,000 for a 0-day bug. <a href="#user-content-fnref-bounty" aria-label="Back to reference 1" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-thesis"><p>We had one shot at this; William's thesis was due in a few days and we wanted to include the exploit. <a href="#user-content-fnref-thesis" aria-label="Back to reference 2" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-hash"><p>Compare with proofs of work, for example, that request a SHA hash starting with some number of zeros. This process is embarrassingly parallel, so someone with many powerful GPUs could solve it in a fraction of the time as someone without. <a href="#user-content-fnref-hash" aria-label="Back to reference 3" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-rust"><p>The Rust implementation uses the exact same Mersenne trick, yet takes about 2.4 seconds; I assume this is FFI overhead? 🤷 <a href="#user-content-fnref-rust" aria-label="Back to reference 4" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-limbs"><p>Limbs are the term of art for the integer elements of an array that represents a big integer. On 64-bit systems, limbs are usually 64-bit unsigned integers. <a href="#user-content-fnref-limbs" aria-label="Back to reference 5" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-uops"><p>See <a target="_blank" rel="noopener noreferrer" href="https://uops.info/table.html">uops.info</a>, <em>imul r64</em> and <em>mulx r64, r64, r64</em>. <a href="#user-content-fnref-uops" aria-label="Back to reference 6" data-footnote-backref="">↩</a></p></li><li id="user-content-fn-numberworld"><p>The same performance problem was observed by y-cruncher author Alexander Yee <a target="_blank" rel="noopener noreferrer" href="https://www.numberworld.org/y-cruncher/news/2024.html">here</a>, section "Example 2: Everything Blows Up". <a href="#user-content-fnref-numberworld" aria-label="Back to reference 7" data-footnote-backref="">↩</a></p></li></ol></section></div></div></div></article></section></main></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 'white-collar bloodbath' is all part of the AI hype machine (129 pts)]]></title>
            <link>https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap</link>
            <guid>44136117</guid>
            <pubDate>Fri, 30 May 2025 13:38:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap">https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap</a>, See on <a href="https://news.ycombinator.com/item?id=44136117">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300, &quot;image--eq-large&quot;: 660}" data-uri="cms.cnn.com/_components/image/instances/cmb9uttjf000l3b6mdvb71nsc@published" data-name="GettyImages-2194800946.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666875" data-original-height="1067" data-original-width="1600" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-2194800946.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Anthropic CEO Dario Amodei is positioning himself as an AI truth-teller. He is also a salesman." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="1067" width="1600"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                  
<p data-uri="cms.cnn.com/_components/editor-note/instances/cmb9uw1ne000x3b6mwab04e4c@published" data-editable="text" data-component-name="editor-note" data-article-gutter="true">
    <em>A version of this story appeared in CNN Business’ Nightcap newsletter. To get it in your inbox, sign up for free </em><a href="https://www.cnn.com/newsletters/nightcap?source=nl-acq_article"><em>here</em></a>.
</p>

<p><cite>
      <span data-editable="location">New York</span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9us5iz000v26qi86h25b8f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            If the CEO of a soda company declared that soda-making technology is getting so good it’s going to ruin the global economy, you’d be forgiven for thinking that person is either lying or fully detached from reality.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00023b6mvjpg1wiy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Yet when tech CEOs do the same thing, people tend to perk up.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00033b6msdyhk1nh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            ICYMI: The 42-year-old billionaire Dario Amodei, who runs the AI firm Anthropic, <a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic" target="_blank">told Axios</a> this week that the technology he and other companies are building could wipe out<em> half</em> of all entry-level office jobs … sometime soon. Maybe in the next couple of years, he said.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmba2k3bw00003b6mhjt9f8r6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            He reiterated that claim <a href="https://www.cnn.com/2025/05/29/tech/ai-anthropic-ceo-dario-amodei-unemployment">in an interview</a> with CNN’s Anderson Cooper on Thursday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmba2xmww00043b6mwuow586f@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “AI is starting to get better than humans at almost all intellectual tasks, and we’re going to collectively, as a society, grapple with it,” Amodei told Cooper. “AI is going to get better at what everyone does, including what I do, including what other CEOs do.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00043b6m2f2h43c2@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            To be clear, Amodei didn’t cite any research or evidence for that 50% estimate. And that was just one of many of the wild claims he made that are increasingly part of a Silicon Valley script: <em>AI will fix everything, but first it has to ruin everything. </em>Why? <em>Just trust us. </em>
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usaka00053b6mbq9ene9p@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            In this as-yet fictional world, “cancer is cured, the economy grows at 10% a year, the budget is balanced — and 20% of people don’t have jobs,” Amodei told Axios, repeating one of the industry’s favorite unfalsifiable claims about a disease-free utopia on the horizon, courtesy of AI.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00063b6mp4u159w5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But how will the US economy, in particular, grow so robustly when the jobless masses can’t afford to buy anything? Amodei didn’t say.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00073b6mlfr2rvje@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            (As an aside: I asked labor economist Aaron Sojourner about this scenario of high unemployment plus strong economic growth, and he said there <em>is </em>a theory of the case, if you squint really hard. Amodei may believe that AI can increase productivity and make each hour of labor create more goods and services. But if that’s the case, he’s imagining “a 30% jump in labor productivity to get that combination of unemployment and GDP growth,” said Sojourner, a senior researcher at the W. E. Upjohn Institute for Employment Research. “That is a wildly unprecedented vision,” he added, noting that in the 1980s and 90s, computer adoption gave the world all kinds of tools that reshaped the labor market. But labor productivity grew just 2% to 3%.)
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00083b6myepazi91@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Anyway. The point is, Amodei is a salesman, and it’s in his interest to make his product appear inevitable and so powerful it’s <em>scary</em>. Axios framed Amodei’s economic prediction as a “white-collar bloodbath.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb00093b6mvgdk7v8m@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Even some AI optimists were put off by Amodei’s stark characterization.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000b3b6mi32rgk5e@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Someone needs to remind the CEO that at one point there were more than (2 million) secretaries. There were also separate employees to do in office dictation,” wrote tech entrepreneur <a href="https://bsky.app/profile/mcuban.bsky.social" target="_blank">Mark Cuban on Bluesky</a>. “They were the original white collar displacements. New companies with new jobs will come from AI and increase TOTAL employment.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000c3b6mfb6fz22d@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Little of what Amodei told Axios was new, but it was calibrated to sound just outrageous enough to draw attention to Anthropic’s work, days after it released a major model update to its Claude chatbot, one of the top rivals to OpenAI’s ChatGPT.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000d3b6mm4trmbvi@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Amodei stands to profit off the very technology he claims will gut the labor market. <em>But here he is, telling everyone the truth and sounding the alarm! He’s trying to warn us, he’s one of the good ones! </em>
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000e3b6mj3tsf30u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Yeaaahhh. So, this is kind of Anthropic’s whole ~thing.~ It refers to itself primarily as an “AI safety and research” company. They are the AI guys who see the potential harms of AI clearly — not through the rose-colored glasses worn by the techno-utopian simps over at OpenAI. (In fact, Anthropic’s founders, including Amodei, left OpenAI over ideological differences.)
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000g3b6mct0t67yk@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Look, I want to live a cancer-free utopia where I only have to work a few hours a week and there’s no poverty and stuff just <em>works</em>. But do I believe that generative AI is the key to unlocking that fantasyland? I do not. And no tech pioneers have proven their case.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000h3b6mqzpw0h4x@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Generative AI from large language models like ChatGPT and Claude are <em>really</em> good at some <em>very specific</em> stuff: They can summarize documents, write dumb emails, help kids cheat on their homework, and even recommend summer reading lists so obscure <a href="https://www.cnn.com/2025/05/28/media/ai-chatgpt-news-journalism">not even the authors knew</a> they’d written them. Heck, they could probably generate this newsletter and mimic my voice.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9usakb000i3b6mkz2975pf@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But they hit their limits fast. They <a href="https://www.cnn.com/2025/01/03/business/meta-ai-accounts-instagram-facebook">hallucinate</a>. They get <a href="https://www.cnn.com/2025/01/14/business/wikipedia-meta-x-fact-check-nightcap">basic facts</a> wrong. They are susceptible to <a href="https://www.cnn.com/2025/05/20/business/grok-genocide-ai-nightcap">manipulation</a>. (And those are all things we human beings can do just fine on our own.)
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cmb9us7md00003b6mspn434gn@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            If AI companies can take these handy, quasi-reliable text predictors and turn them into an economic revolution, fine. But that seems so far off in the future that Amodei’s warnings feel more like an ad than a PSA. It’s on them to show their work: Show us how AI could be so destructive <em>and </em>how Anthropic can fix it — rather than just shouting about the risks.
    </p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MinIO Removes Web UI Features from Community Version, Pushes Users to Paid Plans (153 pts)]]></title>
            <link>https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</link>
            <guid>44136108</guid>
            <pubDate>Fri, 30 May 2025 13:37:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features">https://biggo.com/news/202505261334_MinIO_Removes_Web_UI_Features</a>, See on <a href="https://news.ycombinator.com/item?id=44136108">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p>MinIO, a popular open-source object storage solution, has made significant changes to its community version that have sparked controversy among users. The company has removed key web-based management features from the free version, directing users to either use command-line tools or upgrade to paid plans.</p>
<h3>Major Features Stripped from Community Version</h3>
<p>The latest changelog reveals that MinIO has deprecated several core management features in the web interface. Account and policy management, configuration settings, and other administrative functions are no longer available through the browser-based console. Instead, users must rely on the <code>mc</code> command-line client to perform these tasks.</p>
<p>This change affects how users interact with their MinIO deployments. Previously, administrators could manage their storage systems through an intuitive web interface. Now, they must learn command-line syntax or pay for the commercial version to regain web-based management capabilities.</p>
<p><strong>Deprecated Features in MinIO v2.0.0:</strong></p>
<ul>
<li>Account and policy management (web UI)</li>
<li>Configuration management (web UI)</li>
<li>Bucket management tools (web UI)</li>
<li>Administrative console features</li>
</ul>
<p><strong>Alternative Solutions:</strong></p>
<ul>
<li><strong>SeaweedFS</strong> - Apache 2.0 license</li>
<li><strong>Garage</strong> - AGPL license</li>
<li><strong>Zenko</strong> - Apache 2.0 license</li>
<li><strong>OpenMaxIO</strong> - Community fork of pre-change MinIO</li>
</ul>
<h3>Community Response and Concerns</h3>
<p>The decision has drawn comparisons to Redis's recent licensing changes, with users expressing frustration about the removal of functionality they previously relied on. Many see this as a classic example of enshittification - the gradual degradation of services to drive revenue.</p>
<blockquote>
<p>I think that Deprecated support has another meaning. I hate when this kind of things happens.</p>
</blockquote>
<p>Some community members are already exploring alternatives. A fork called OpenMaxIO has emerged, preserving the last version before these changes were implemented. However, its long-term viability remains uncertain.</p>
<h3>Technical Impact and Alternatives</h3>
<p>While the core storage functionality remains intact, the user experience has significantly changed. Organizations that depend on web-based management may need to retrain staff or consider migration to other solutions.</p>
<p>Several alternatives are gaining attention, including SeaweedFS, Garage, and Zenko. These projects offer S3-compatible storage with varying licensing models and feature sets. Users are actively discussing these options as potential replacements for MinIO in self-hosted environments.</p>
<h3>Looking Forward</h3>
<p>MinIO's strategy appears focused on monetizing enterprise features while maintaining the core storage engine as open source. The company argues this approach helps sustain development while serving both community and commercial users.</p>
<p>However, the timing and execution of these changes have created uncertainty in the community. Users must now decide whether to adapt to command-line management, pay for commercial licenses, or migrate to alternative solutions that better align with their needs and expectations.</p>
<p>Reference: <a href="https://github.com/minio/object-browser/blob/master/CHANGELOG.md">Changelog</a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsandbox: Virtual Machines that feel and perform like containers (194 pts)]]></title>
            <link>https://github.com/microsandbox/microsandbox</link>
            <guid>44135977</guid>
            <pubDate>Fri, 30 May 2025 13:20:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsandbox/microsandbox">https://github.com/microsandbox/microsandbox</a>, See on <a href="https://news.ycombinator.com/item?id=44135977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><a href="https://github.com/microsandbox/microsandbox/blob/main/#gh-dark-mode-only">
<img width="100%" src="https://github.com/microsandbox/microsandbox/raw/main/assets/microsandbox-banner-xl-dark.png" alt="microsandbox-banner-xl-dark">
</a>
<a href="https://github.com/microsandbox/microsandbox/blob/main/#gh-light-mode-only">
<img width="100%" src="https://github.com/microsandbox/microsandbox/raw/main/assets/microsandbox-banner-xl.png" alt="microsandbox-banner-xl">
</a>
<p><b>———&nbsp;&nbsp;&nbsp;easy secure execution of untrusted user/ai code&nbsp;&nbsp;&nbsp;———</b></p>

<div dir="auto">
  
  <p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/20358651/449247679-d91df12c-e425-48c0-a881-dec9a8d45868.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDkyNDc2NzktZDkxZGYxMmMtZTQyNS00OGMwLWE4ODEtZGVjOWE4ZDQ1ODY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUyMTdkNTNhODFiZjc4YTAxNDAxNGZiNjk1MjQ3Zjc4ZDNkM2ExZDIwMjNlZmIwYTMyNTY2NWFmNjE3ZGY1MTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gk5TW2yoWt6XAozaC9lt4ku_Oi2unthsxkWb9QDHe3o"><img alt="Claude MCP Demo" src="https://private-user-images.githubusercontent.com/20358651/449247679-d91df12c-e425-48c0-a881-dec9a8d45868.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDkyNDc2NzktZDkxZGYxMmMtZTQyNS00OGMwLWE4ODEtZGVjOWE4ZDQ1ODY4LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTUyMTdkNTNhODFiZjc4YTAxNDAxNGZiNjk1MjQ3Zjc4ZDNkM2ExZDIwMjNlZmIwYTMyNTY2NWFmNjE3ZGY1MTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gk5TW2yoWt6XAozaC9lt4ku_Oi2unthsxkWb9QDHe3o" width="500" data-animated-image=""></a>
</p></div>

<p><a href="https://docs.microsandbox.dev/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/8fdae889b2d0777923548ea76a1d8c7808f9d7a84aea656418b39a220b3d11cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63756d656e746174696f6e2d2532333030616365652e7376673f636f6c6f723d666634353030267374796c653d666f722d7468652d6261646765266c6f676f3d676974626f6f6b266c6f676f436f6c6f723d7768697465" alt="documentation" data-canonical-src="https://img.shields.io/badge/documentation-%2300acee.svg?color=ff4500&amp;style=for-the-badge&amp;logo=gitbook&amp;logoColor=white">
  </a>
  <a href="https://discord.gg/T95Y3XnEAK" rel="nofollow">
    <img src="https://camo.githubusercontent.com/242040afb1f843c8c171d2f3db596b8b59ffa6f9b550e469a3fed5ae9364837c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646973636f7264202d2532333030616365652e7376673f636f6c6f723d6d656469756d736c617465626c7565267374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465" alt="discord" data-canonical-src="https://img.shields.io/badge/discord -%2300acee.svg?color=mediumslateblue&amp;style=for-the-badge&amp;logo=discord&amp;logoColor=white">
  </a>
</p>
<br>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/61f6bb1128a04c4d0ceae098fdbfcc0da63e888bc2176757c5194edb172d0b6e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7175657374696f6e2f413737304546"><img height="18" src="https://camo.githubusercontent.com/61f6bb1128a04c4d0ceae098fdbfcc0da63e888bc2176757c5194edb172d0b6e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7175657374696f6e2f413737304546" data-canonical-src="https://octicons-col.vercel.app/question/A770EF"></a>&nbsp;&nbsp;WHY MICROSANDBOX?</sub></h2><a id="user-content-why-microsandbox" aria-label="Permalink: &nbsp;&nbsp;WHY MICROSANDBOX?" href="#why-microsandbox"></a></div>
<p dir="auto">Ever needed to run code you don't fully trust? Whether it's AI-generated code, user submissions, or experimental code, the traditional options all have serious drawbacks:</p>
<ul dir="auto">
<li><strong>Running locally</strong> - One malicious script and your entire system is compromised</li>
<li><strong>Using containers</strong> - Shared kernels mean sophisticated attacks can still break out</li>
<li><strong>Traditional VMs</strong> - Waiting 10+ seconds for a VM to boot kills productivity and performance</li>
<li><strong>Cloud solutions</strong> - Not as flexible, at the whim of the cloud provider</li>
</ul>
<p dir="auto"><strong>microsandbox</strong> combines the best of all worlds:</p>

<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40e8c7c287a3e8b62b4370868226dab3206da62bc66d1e44912e514260dda076/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7a61702f413737304546"><img height="18" src="https://camo.githubusercontent.com/40e8c7c287a3e8b62b4370868226dab3206da62bc66d1e44912e514260dda076/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f7a61702f413737304546" data-canonical-src="https://octicons-col.vercel.app/zap/A770EF"></a>&nbsp;&nbsp;SDK QUICK START</sub></h2><a id="user-content-sdk-quick-start" aria-label="Permalink: &nbsp;&nbsp;SDK QUICK START" href="#sdk-quick-start"></a></div>
<p dir="auto">Get started in few easy steps:</p>

<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/50b009b96c229efe4bf861daa7f4d181bdcbb2fad3b080e552e6a304918cbf23/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61636f732d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/50b009b96c229efe4bf861daa7f4d181bdcbb2fad3b080e552e6a304918cbf23/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6d61636f732d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765" alt="macos" data-canonical-src="https://img.shields.io/badge/macos-working-green?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2a62abf9a85d97ef6fb29ec6aa9b8c5917fa0166deb2709c38b5c4932e1c684/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e75782d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/a2a62abf9a85d97ef6fb29ec6aa9b8c5917fa0166deb2709c38b5c4932e1c684/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c696e75782d776f726b696e672d677265656e3f7374796c653d666f722d7468652d6261646765" alt="linux" data-canonical-src="https://img.shields.io/badge/linux-working-green?style=for-the-badge"></a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ffc403a8e3e11f3fe56a1ba9257cac9907ecfb9814cd5c7312ae587237ec1bf1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f77696e646f77732d7769702d7265643f7374796c653d666f722d7468652d6261646765"><img src="https://camo.githubusercontent.com/ffc403a8e3e11f3fe56a1ba9257cac9907ecfb9814cd5c7312ae587237ec1bf1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f77696e646f77732d7769702d7265643f7374796c653d666f722d7468652d6261646765" alt="windows" data-canonical-src="https://img.shields.io/badge/windows-wip-red?style=for-the-badge"></a>
</p>

<div dir="auto"><h3 tabindex="-1" dir="auto"><span>1</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9be3391cb69b279615c505c1338bad12b8022eb0f3a056d3550db009f252c2db/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6e6f7274682d737461722f413737304546"><img height="13" src="https://camo.githubusercontent.com/9be3391cb69b279615c505c1338bad12b8022eb0f3a056d3550db009f252c2db/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6e6f7274682d737461722f413737304546" data-canonical-src="https://octicons-col.vercel.app/north-star/A770EF"></a>&nbsp;&nbsp;Start the Server</h3><a id="user-content-1start-the-server" aria-label="Permalink: 1&nbsp;&nbsp;&nbsp;&nbsp;Start the Server" href="#1start-the-server"></a></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Install microsandbox</h5><a id="user-content-install-microsandbox" aria-label="Permalink: Install microsandbox" href="#install-microsandbox"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -sSL https://get.microsandbox.dev | sh"><pre>curl -sSL https://get.microsandbox.dev <span>|</span> sh</pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">And start the server</h5><a id="user-content-and-start-the-server" aria-label="Permalink: And start the server" href="#and-start-the-server"></a></p>

<div dir="auto"><p dir="auto">Tip</p>
<p dir="auto">microsandbox server is also an <a href="https://github.com/microsandbox/microsandbox/blob/main/MCP.md">MCP server</a>, so it works directly with Claude, Agno and other MCP-enabled AI tools and agents.</p>
<p dir="auto">For more information on setting up the server, see the <a href="https://github.com/microsandbox/microsandbox/blob/main/SELF_HOSTING.md">self-hosting guide</a>.</p>
</div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Optionally pull the environment image</h5><a id="user-content-optionally-pull-the-environment-image" aria-label="Permalink: Optionally pull the environment image" href="#optionally-pull-the-environment-image"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="msb pull microsandbox/python"><pre>msb pull microsandbox/python</pre></div>

<div dir="auto"><h3 tabindex="-1" dir="auto"><span>2</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fbdfa3f26094ac97e5ea6f813976d8cbfc6feb8f261c9958bdb234a74604cc0/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6d6f76652d746f2d626f74746f6d2f413737304546"><img height="14" src="https://camo.githubusercontent.com/5fbdfa3f26094ac97e5ea6f813976d8cbfc6feb8f261c9958bdb234a74604cc0/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6d6f76652d746f2d626f74746f6d2f413737304546" data-canonical-src="https://octicons-col.vercel.app/move-to-bottom/A770EF"></a>&nbsp;&nbsp;Install the SDK</h3><a id="user-content-2install-the-sdk" aria-label="Permalink: 2&nbsp;&nbsp;&nbsp;&nbsp;Install the SDK" href="#2install-the-sdk"></a></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Python</h5><a id="user-content-python" aria-label="Permalink: Python" href="#python"></a></p>

<p dir="auto"><h5 tabindex="-1" dir="auto">JavaScript</h5><a id="user-content-javascript" aria-label="Permalink: JavaScript" href="#javascript"></a></p>

<p dir="auto"><h5 tabindex="-1" dir="auto">Rust</h5><a id="user-content-rust" aria-label="Permalink: Rust" href="#rust"></a></p>

<div dir="auto"><p dir="auto">Note</p><p dir="auto">There are <a href="https://github.com/microsandbox/microsandbox/blob/main/sdk">SDKs</a> for other languages as well! Join us in expanding support for your favorite language.</p>
</div>


<div dir="auto"><h3 tabindex="-1" dir="auto"><span>3</span>&nbsp;&nbsp;<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/79aad6df819fa3b9c51c4405ebe757a64e8fd7e07d0843e718a19ebdf66df550/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f66696c652d62696e6172792f413737304546"><img height="14" src="https://camo.githubusercontent.com/79aad6df819fa3b9c51c4405ebe757a64e8fd7e07d0843e718a19ebdf66df550/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f66696c652d62696e6172792f413737304546" data-canonical-src="https://octicons-col.vercel.app/file-binary/A770EF"></a>&nbsp;&nbsp;Execute the Code</h3><a id="user-content-3execute-the-code" aria-label="Permalink: 3&nbsp;&nbsp;&nbsp;&nbsp;Execute the Code" href="#3execute-the-code"></a></div>
<p dir="auto"><code>microsandbox</code> offers a growing list of sandbox environment types optimized for different execution requirements. Choose the appropriate sandbox (e.g., PythonSandbox or NodeSandbox) to run your code in a secure tailored environment.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Python</h5><a id="user-content-python-1" aria-label="Permalink: Python" href="#python-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import asyncio
from microsandbox import PythonSandbox

async def main():
    async with PythonSandbox.create(name=&quot;test&quot;) as sb:
        exec = await sb.run(&quot;name = 'Python'&quot;)
        exec = await sb.run(&quot;print(f'Hello {name}!')&quot;)

    print(await exec.output()) # prints Hello Python!

asyncio.run(main())"><pre><span>import</span> <span>asyncio</span>
<span>from</span> <span>microsandbox</span> <span>import</span> <span>PythonSandbox</span>

<span>async</span> <span>def</span> <span>main</span>():
    <span>async</span> <span>with</span> <span>PythonSandbox</span>.<span>create</span>(<span>name</span><span>=</span><span>"test"</span>) <span>as</span> <span>sb</span>:
        <span>exec</span> <span>=</span> <span>await</span> <span>sb</span>.<span>run</span>(<span>"name = 'Python'"</span>)
        <span>exec</span> <span>=</span> <span>await</span> <span>sb</span>.<span>run</span>(<span>"print(f'Hello {name}!')"</span>)

    <span>print</span>(<span>await</span> <span>exec</span>.<span>output</span>()) <span># prints Hello Python!</span>

<span>asyncio</span>.<span>run</span>(<span>main</span>())</pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">JavaScript</h5><a id="user-content-javascript-1" aria-label="Permalink: JavaScript" href="#javascript-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { NodeSandbox } from &quot;microsandbox&quot;;

async function main() {
  const sb = await NodeSandbox.create({ name: &quot;test&quot; });

  try {
    let exec = await sb.run(&quot;var name = 'JavaScript'&quot;);
    exec = await sb.run(&quot;console.log(`Hello ${name}!`)&quot;);

    console.log(await exec.output()); // prints Hello JavaScript!
  } finally {
    await sb.stop();
  }
}

main().catch(console.error);"><pre><span>import</span> <span>{</span> <span>NodeSandbox</span> <span>}</span> <span>from</span> <span>"microsandbox"</span><span>;</span>

<span>async</span> <span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> <span>sb</span> <span>=</span> <span>await</span> <span>NodeSandbox</span><span>.</span><span>create</span><span>(</span><span>{</span> <span>name</span>: <span>"test"</span> <span>}</span><span>)</span><span>;</span>

  <span>try</span> <span>{</span>
    <span>let</span> <span>exec</span> <span>=</span> <span>await</span> <span>sb</span><span>.</span><span>run</span><span>(</span><span>"var name = 'JavaScript'"</span><span>)</span><span>;</span>
    <span>exec</span> <span>=</span> <span>await</span> <span>sb</span><span>.</span><span>run</span><span>(</span><span>"console.log(`Hello ${name}!`)"</span><span>)</span><span>;</span>

    <span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>exec</span><span>.</span><span>output</span><span>(</span><span>)</span><span>)</span><span>;</span> <span>// prints Hello JavaScript!</span>
  <span>}</span> <span>finally</span> <span>{</span>
    <span>await</span> <span>sb</span><span>.</span><span>stop</span><span>(</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span>

<span>main</span><span>(</span><span>)</span><span>.</span><span>catch</span><span>(</span><span>console</span><span>.</span><span>error</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Rust</h5><a id="user-content-rust-1" aria-label="Permalink: Rust" href="#rust-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="use microsandbox::{SandboxOptions, PythonSandbox};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut sb = PythonSandbox::create(SandboxOptions::builder().name(&quot;test&quot;).build()).await?;

    let exec = sb.run(r#&quot;name = &quot;Python&quot;&quot;#).await?;
    let exec = sb.run(r#&quot;print(f&quot;Hello {name}!&quot;)&quot;#).await?;

    println!(&quot;{}&quot;, exec.output().await?); // prints Hello Python!

    sb.stop().await?;

    Ok(())
}"><pre><span>use</span> microsandbox<span>::</span><span>{</span><span>SandboxOptions</span><span>,</span> <span>PythonSandbox</span><span>}</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> std<span>::</span>error<span>::</span><span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> <span>mut</span> sb = <span>PythonSandbox</span><span>::</span><span>create</span><span>(</span><span>SandboxOptions</span><span>::</span><span>builder</span><span>(</span><span>)</span><span>.</span><span>name</span><span>(</span><span>"test"</span><span>)</span><span>.</span><span>build</span><span>(</span><span>)</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>let</span> exec = sb<span>.</span><span>run</span><span>(</span><span>r#"name = "Python""#</span><span>)</span><span>.</span><span>await</span>?<span>;</span>
    <span>let</span> exec = sb<span>.</span><span>run</span><span>(</span><span>r#"print(f"Hello {name}!")"#</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>println</span><span>!</span><span>(</span><span>"{}"</span><span>,</span> exec<span>.</span>output<span>(</span><span>)</span><span>.</span><span>await</span>?<span>)</span><span>;</span> <span>// prints Hello Python!</span>

    sb<span>.</span><span>stop</span><span>(</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<div dir="auto"><p dir="auto">Note</p>
<p dir="auto">If you haven't pulled the environment image, the first run will take a while as it tries to download it.
Executions will be much faster afterwards.</p>
<p dir="auto">For more information on how to use the SDK, <a href="https://github.com/microsandbox/microsandbox/blob/main/sdk/README.md">check out the SDK README</a>.</p>
</div>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f5303d49a785448f3173be66839bd28738f13d941b1ff1274e2b28b394f4099e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6465766963652d6465736b746f702f413737304546"><img height="18" src="https://camo.githubusercontent.com/f5303d49a785448f3173be66839bd28738f13d941b1ff1274e2b28b394f4099e/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6465766963652d6465736b746f702f413737304546" data-canonical-src="https://octicons-col.vercel.app/device-desktop/A770EF"></a>&nbsp;&nbsp;PROJECTS&nbsp;&nbsp;<sup><sup>B E T A</sup></sup></sub></h2><a id="user-content-projectsb-e-t-a" aria-label="Permalink: &nbsp;&nbsp;PROJECTS&nbsp;&nbsp;" href="#projectsb-e-t-a"></a></div>
<p dir="auto">Beyond the SDK, microsandbox supports project-based development with the familiar package-manager workflow devs are used to. Think of it like npm or cargo, but for sandboxes!</p>
<p dir="auto">Create a <code>Sandboxfile</code>, define your environments, and manage your sandboxes with simple commands.</p>

<p dir="auto"><a href="https://asciinema.org/a/7eOFf2Ovigi473FsKgr3Lpve1" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/20358651/444812608-3a9d1de4-2370-4d5a-a40d-9aa7315aa934.svg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80NDQ4MTI2MDgtM2E5ZDFkZTQtMjM3MC00ZDVhLWE0MGQtOWFhNzMxNWFhOTM0LnN2Zz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZhNDY5MTljNzg4N2QwOTlmMzZjMmRmZDg1Njk1MjFiZWQ0NmYzNWEzOTYyZmM1NjAyZTA5MTJlN2M1Y2FlN2QmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.6Z1aX5qI-n5wHOSWsHSkYUMVuABUdrXwlfhudO6COQY" secured-asset-link=""></a></p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Create a Sandbox Project</h4><a id="user-content-create-a-sandbox-project" aria-label="Permalink: Create a Sandbox Project" href="#create-a-sandbox-project"></a></p>

<p dir="auto">This creates a <code>Sandboxfile</code> in the current directory, which serves as the configuration manifest for your sandbox environments.</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Add a Sandbox to the Project</h4><a id="user-content-add-a-sandbox-to-the-project" aria-label="Permalink: Add a Sandbox to the Project" href="#add-a-sandbox-to-the-project"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="msb add app \
    --image python \
    --cpus 1 \
    --memory 1024 \
    --start 'python -c &quot;print(\&quot;hello\&quot;)&quot;'"><pre>msb add app \
    --image python \
    --cpus 1 \
    --memory 1024 \
    --start <span><span>'</span>python -c "print(\"hello\")"<span>'</span></span></pre></div>
<p dir="auto">The command above registers a new sandbox named <code>app</code> in your Sandboxfile, configured to use the <code>python</code> image.</p>
<p dir="auto">You should now have a <code>Sandboxfile</code> containing a sandbox named <strong><code>app</code></strong>:</p>

<div dir="auto" data-snippet-clipboard-copy-content="# Sandbox configurations
sandboxes:
  app:
    image: python
    memory: 1024
    cpus: 1
    scripts:
      start: python -c &quot;print(\&quot;hello\&quot;)&quot;"><pre><span><span>#</span> Sandbox configurations</span>
<span>sandboxes</span>:
  <span>app</span>:
    <span>image</span>: <span>python</span>
    <span>memory</span>: <span>1024</span>
    <span>cpus</span>: <span>1</span>
    <span>scripts</span>:
      <span>start</span>: <span>python -c "print(\"hello\")"</span></pre></div>
<div dir="auto"><p dir="auto">Tip</p>
<p dir="auto">Run <code>msb &lt;subcommand&gt; --help</code> to see all the options available for a subcommand.</p>
<p dir="auto">For example, <code>msb add --help</code>.</p>
</div>

<p dir="auto"><h4 tabindex="-1" dir="auto">Running a Sandbox</h4><a id="user-content-running-a-sandbox" aria-label="Permalink: Running a Sandbox" href="#running-a-sandbox"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Run a Sandbox Defined in Your Project</h5><a id="user-content-run-a-sandbox-defined-in-your-project" aria-label="Permalink: Run a Sandbox Defined in Your Project" href="#run-a-sandbox-defined-in-your-project"></a></p>

<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">This executes the default <em>start</em> script of your sandbox. For more control, you can directly specify which script to run — <code>msr app~start</code>.</p>
<p dir="auto">When running project sandboxes, all file changes and installations made inside the sandbox are automatically persisted to the <code>./menv</code> directory. This means you can stop and restart your sandbox any time without losing your work. Your development environment will be exactly as you left it.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Run an Temporary Sandbox</h5><a id="user-content-run-an-temporary-sandbox" aria-label="Permalink: Run an Temporary Sandbox" href="#run-an-temporary-sandbox"></a></p>
<p dir="auto">For experimentation or one-off tasks, temporary sandboxes provide a clean environment that leaves no trace:</p>

<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">Temporary sandboxes are perfect for isolating programs you get from the internet. Once you exit the sandbox, all changes are completely discarded.</p>

<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Sandboxes</h4><a id="user-content-installing-sandboxes" aria-label="Permalink: Installing Sandboxes" href="#installing-sandboxes"></a></p>
<p dir="auto">The <code>msb install</code> command sets up a sandbox as a system-wide executable. It installs a slim launcher program that allows you to start your sandbox from anywhere in your system with a simple command.</p>
<div dir="auto" data-snippet-clipboard-copy-content="msb install --image alpine"><pre>msb install --image alpine</pre></div>
<p dir="auto"><em><strong>or</strong></em></p>

<p dir="auto">After installation, you can start your sandbox by simply typing its name in any terminal:</p>

<p dir="auto">This makes frequently used sandboxes incredibly convenient to access — no need to navigate to specific directories or remember complex commands. Just type the sandbox name and it launches immediately with all your configured settings.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">You can give your sandbox a descriptive, easy-to-remember name during installation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="msi alpine:20250108 slim-linux"><pre>msi alpine:20250108 slim-linux</pre></div>
<p dir="auto">This allows you to create multiple instances of the same sandbox image with different names and configurations. For example:</p>
<ul dir="auto">
<li><code>msi python python-data-science</code> - A Python environment for data analysis</li>
<li><code>msi python python-web</code> - A Python environment for web development</li>
</ul>
<p dir="auto">Installed sandboxes maintain their state between sessions, so you can pick up exactly where you left off each time you launch them.</p>
</div>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ad892701da53ec646f4ba6a987c2e8d494a92f3091ee89eb87055869a8d9fd26/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c696768742d62756c622f413737304546"><img height="18" src="https://camo.githubusercontent.com/ad892701da53ec646f4ba6a987c2e8d494a92f3091ee89eb87055869a8d9fd26/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c696768742d62756c622f413737304546" data-canonical-src="https://octicons-col.vercel.app/light-bulb/A770EF"></a>&nbsp;&nbsp;USE CASES</sub></h2><a id="user-content-use-cases" aria-label="Permalink: &nbsp;&nbsp;USE CASES" href="#use-cases"></a></div>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="coding-dark" src="https://private-user-images.githubusercontent.com/20358651/439591910-37c14bf1-e2f7-4af3-804e-5901de845715.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MTAtMzdjMTRiZjEtZTJmNy00YWYzLTgwNGUtNTkwMWRlODQ1NzE1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTY4NmE4ZGUyNDFlNmZkMzBjMWQ4ZmVlNDQ1YmIzZjc3NjEzNmQ1YTIzMTM5OGZiNzM1NDNiZTA1MzViZGE0OWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ZFmRDi-CnFjJAuxYX2Vgkb0pnJIYdd2nUkymMfV-2zY" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="coding-light" src="https://private-user-images.githubusercontent.com/20358651/439591915-1bfe7223-869b-4782-9fce-3620c4400bbf.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MTUtMWJmZTcyMjMtODY5Yi00NzgyLTlmY2UtMzYyMGM0NDAwYmJmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU4MGRhOGYyMGFhNDYxNGNmMWFmOGFmZTdiOTdkMDIwYTcwNWUzYjQwMGM1NGI2NWJjYTM2ZDYxODRjYmIyNDkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.AGy2Va3-Rsis74vuvgVu3ZkuuNY0R4HHG6IcdHdQFIo" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Coding &amp; Dev Environments</h3><a id="user-content-coding--dev-environments" aria-label="Permalink: Coding &amp; Dev Environments" href="#coding--dev-environments"></a></p>
<p dir="auto">Let your AI agents build real apps with professional dev tools. When users ask their AI to create a web app, fix a bug, or build a prototype, it can handle everything from Git operations to dependency management to testing in a protected environment.</p>
<p dir="auto">Your AI can create complete development environments in milliseconds and run programs with full system access. The fast startup means developers get instant feedback and can iterate quickly. This makes it perfect for AI pair programming, coding education platforms, and automated code generation where quick results matter.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="data-dark" src="https://private-user-images.githubusercontent.com/20358651/439591922-3794e426-a223-4064-8939-025c7bbaf5ea.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjItMzc5NGU0MjYtYTIyMy00MDY0LTg5MzktMDI1YzdiYmFmNWVhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWMzNDVjZDFkZGRlMWQwZTA5YjU2MzQyMTM2ZjY1OTNjNmI4NTQ2YWMzZmU0ZGIyZjgxYzk1YTI0ZmZhZDZiMWMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.0EbXbUPdkX72Pd-tuYSflHQbZ3ilE2IVCbLvqoXyUJc" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="data-light" src="https://private-user-images.githubusercontent.com/20358651/439591924-3a330ea5-85b5-4176-8fe7-a43d59733cf1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjQtM2EzMzBlYTUtODViNS00MTc2LThmZTctYTQzZDU5NzMzY2YxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA0ZDM5ZjlmYjFiYjg4ZTVhNmNmYzY1NDQ2NzM2MjljNDZmOGRiOWM0MmUyZTI1NDAxNTAwNTE3OTJiNDQ5NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.NuNBLsbzskA9rpISLivpv9G6T40OSIUEUFs_7VdCwdk" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Analysis</h3><a id="user-content-data-analysis" aria-label="Permalink: Data Analysis" href="#data-analysis"></a></p>
<p dir="auto">Transform raw numbers into meaningful insights with AI that works for you. Your AI can process spreadsheets, create charts, and generate reports safely. Whether it's analyzing customer feedback, sales trends, or research data, everything happens in a protected environment that respects data privacy.</p>
<p dir="auto">Microsandbox lets your AI work with powerful libraries like NumPy, Pandas, and TensorFlow while creating visualizations that bring insights to life. Perfect for financial analysis tools, privacy-focused data processing, medical research, and any situation where you need serious computing power with appropriate safeguards.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="web-dark" src="https://private-user-images.githubusercontent.com/20358651/439591945-3048a39a-c3cb-4f6e-9bc0-49b404abed03.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5NDUtMzA0OGEzOWEtYzNjYi00ZjZlLTliYzAtNDliNDA0YWJlZDAzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWYxMDI0NGYzZWZjMjYxNjA0NjVkOTJjMDk3YmIzYWUxYjFhNmUzZWU4YWJkMWQ1YzRlNDg3MGViNjU1NmMxMTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.UiM5q82CY69VvRKTvIBHE812ixXzbpWSWJRP0g5Sy4g" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="web-light" src="https://private-user-images.githubusercontent.com/20358651/439591949-e6a01e6d-c23f-4c04-bfbf-3e0cb283e0a9.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5NDktZTZhMDFlNmQtYzIzZi00YzA0LWJmYmYtM2UwY2IyODNlMGE5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTdmOGM5NzVjZjlmNjQxN2E0OTFmZGJiNjYzYTRjZTg0YTM4MGQ5ZmM1MDE4YzhiNTJhYzk1NjljN2NhMTY5MTcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.WuAtWiNxnGrsrOxEmqXnDsYP5koHuIbpFtLVa0iL6TM" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Web Browsing Agent</h3><a id="user-content-web-browsing-agent" aria-label="Permalink: Web Browsing Agent" href="#web-browsing-agent"></a></p>
<p dir="auto">Build AI assistants that can browse the web for your users. Need to compare prices across stores, gather info from multiple news sites, or automate form submissions? Your AI can handle it all while staying in a contained environment.</p>
<p dir="auto">With microsandbox, your AI can navigate websites, extract data, fill out forms, and handle logins. It can visit any site and deliver only the useful information back to your application. This makes it ideal for price comparison tools, research assistants, content aggregators, automated testing, and web automation workflows that would otherwise require complex setup.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p dir="auto"><a href="https://microsandbox.dev/#gh-dark-mode-only" rel="nofollow"><img width="400" alt="host-dark" src="https://private-user-images.githubusercontent.com/20358651/439591929-3c542e78-b5a0-4525-8a2a-376447d786fd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MjktM2M1NDJlNzgtYjVhMC00NTI1LThhMmEtMzc2NDQ3ZDc4NmZkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk2ZjE2ODc3ZTE0NjRmZTU4M2M5Y2YxYjAwZDY5YjY4MjcxNTM5N2EyZGNhMzJjYmVkZGQyMzY2NjgxOWMxOWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.pTo7v6ek0fWFTy9CnWc2lZhJlhPsQIyQ3HElR44Aiss" secured-asset-link=""></a>
<a href="https://microsandbox.dev/#gh-light-mode-only" rel="nofollow"><img width="400" alt="host-light" src="https://private-user-images.githubusercontent.com/20358651/439591935-337b3d5f-9c33-4126-ae55-aca33abbf73e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii8yMDM1ODY1MS80Mzk1OTE5MzUtMzM3YjNkNWYtOWMzMy00MTI2LWFlNTUtYWNhMzNhYmJmNzNlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA1MzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNTMwVDE3MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTFiMGZjNTYxNmI4ZDZhOWQ5MzYzOThkNzNhNmJmZTk0YjQ3ZTg5YjdlZDJhZTk4YTEyZDZiN2FjOGYwZDMxMmMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.vvut9PH8pwy55jmWXdzKwFWmE3F4EHC6bXGfT8hEe-A" secured-asset-link=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Instant App Hosting</h3><a id="user-content-instant-app-hosting" aria-label="Permalink: Instant App Hosting" href="#instant-app-hosting"></a></p>
<p dir="auto">Share working apps and demos in seconds without deployment headaches. When your AI creates a useful tool, calculator, visualization, or prototype, users can immediately access it through a simple link.</p>
<p dir="auto">Zero-setup deployment means your AI-generated code can be immediately useful without complex configuration. Each app runs in its own protected space with appropriate resource limits, and everything cleans up automatically when no longer needed. Perfect for educational platforms hosting student projects, AI assistants creating live demos, and users needing immediate value.</p>


<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"><img width="2000" height="0" src="https://private-user-images.githubusercontent.com/7799382/374860276-ee14e6f7-20b8-4391-9091-8e8e25561929.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDg2MjY1MDEsIm5iZiI6MTc0ODYyNjIwMSwicGF0aCI6Ii83Nzk5MzgyLzM3NDg2MDI3Ni1lZTE0ZTZmNy0yMGI4LTQzOTEtOTA5MS04ZThlMjU1NjE5MjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDUzMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA1MzBUMTczMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NDBkMDRkNmE1NjMxZjBlNzQ1ZDYzOWE5ZTBhZGUyNzU1MjExMDdkZWU0ZmJhYThmMmVkYjA2NzA0OTFjMjc1OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.2qKJgykIs7Bim4B3suQyrIX2fX3_48CWM28AosSEIhc"></a><br></p>
<p>• • •</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The Server Architecture</h3><a id="user-content-the-server-architecture" aria-label="Permalink: The Server Architecture" href="#the-server-architecture"></a></p>
<section data-identity="25a61435-60a8-4cdd-b155-b0aff3bf5e44" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;flowchart TB\n    %% ── Client side ──────────────────────────\n    subgraph ClientProcess[\&quot;process\&quot;]\n        A[\&quot;Your Business Logic\&quot;]\n        B[\&quot;microsandbox SDK\&quot;]\n        A --&amp;gt;|calls| B\n    end\n\n    %% ── Server side ─────────────────────────\n    subgraph ServerProcess[\&quot;process\&quot;]\n        C[\&quot;microsandbox server\&quot;]\n    end\n    B --&amp;gt;|sends untrusted code to| C\n\n    %% ── Branching hub ───────────────────────\n    D([ ])\n    C --&amp;gt;|runs code in| D\n\n    %% ── Individual MicroVMs ────────────────\n    subgraph VM1[\&quot;microVM\&quot;]\n        VM1S[\&quot;python environment\&quot;]\n    end\n\n    subgraph VM2[\&quot;microVM\&quot;]\n        VM2S[\&quot;python environment\&quot;]\n    end\n\n    subgraph VM3[\&quot;microVM\&quot;]\n        VM3S[\&quot;node environment\&quot;]\n    end\n\n    D --&amp;gt; VM1S\n    D --&amp;gt; VM2S\n    D --&amp;gt; VM3S\n\n    %% ── Styling ─────────────────────────────\n    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000\n    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000\n    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000\n    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000\n    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000\n&quot;}" data-plain="flowchart TB
    %% ── Client side ──────────────────────────
    subgraph ClientProcess[&quot;process&quot;]
        A[&quot;Your Business Logic&quot;]
        B[&quot;microsandbox SDK&quot;]
        A -->|calls| B
    end

    %% ── Server side ─────────────────────────
    subgraph ServerProcess[&quot;process&quot;]
        C[&quot;microsandbox server&quot;]
    end
    B -->|sends untrusted code to| C

    %% ── Branching hub ───────────────────────
    D([ ])
    C -->|runs code in| D

    %% ── Individual MicroVMs ────────────────
    subgraph VM1[&quot;microVM&quot;]
        VM1S[&quot;python environment&quot;]
    end

    subgraph VM2[&quot;microVM&quot;]
        VM2S[&quot;python environment&quot;]
    end

    subgraph VM3[&quot;microVM&quot;]
        VM3S[&quot;node environment&quot;]
    end

    D --> VM1S
    D --> VM2S
    D --> VM3S

    %% ── Styling ─────────────────────────────
    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000
    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000
    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
">
      <pre lang="mermaid" aria-label="Raw mermaid code">flowchart TB
    %% ── Client side ──────────────────────────
    subgraph ClientProcess["process"]
        A["Your Business Logic"]
        B["microsandbox SDK"]
        A --&gt;|calls| B
    end

    %% ── Server side ─────────────────────────
    subgraph ServerProcess["process"]
        C["microsandbox server"]
    end
    B --&gt;|sends untrusted code to| C

    %% ── Branching hub ───────────────────────
    D([ ])
    C --&gt;|runs code in| D

    %% ── Individual MicroVMs ────────────────
    subgraph VM1["microVM"]
        VM1S["python environment"]
    end

    subgraph VM2["microVM"]
        VM2S["python environment"]
    end

    subgraph VM3["microVM"]
        VM3S["node environment"]
    end

    D --&gt; VM1S
    D --&gt; VM2S
    D --&gt; VM3S

    %% ── Styling ─────────────────────────────
    style A    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style B    fill:#D6EAF8,stroke:#2E86C1,stroke-width:2px,color:#000000
    style C    fill:#D5F5E3,stroke:#28B463,stroke-width:2px,color:#000000
    style D    fill:#F4F6F6,stroke:#ABB2B9,stroke-width:2px,color:#000000
    style VM1S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM2S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
    style VM3S fill:#FCF3CF,stroke:#F1C40F,stroke-width:2px,color:#000000
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b42435ba5b647f8e510635976cf776c376b5b7ba76fee683187a1d73f1a25818/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f676561722f413737304546"><img height="18" src="https://camo.githubusercontent.com/b42435ba5b647f8e510635976cf776c376b5b7ba76fee683187a1d73f1a25818/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f676561722f413737304546" data-canonical-src="https://octicons-col.vercel.app/gear/A770EF"></a>&nbsp;&nbsp;DEVELOPMENT</sub></h2><a id="user-content-development" aria-label="Permalink: &nbsp;&nbsp;DEVELOPMENT" href="#development"></a></div>
<p dir="auto">Interested in contributing to microsandbox? Check out our <a href="https://github.com/microsandbox/microsandbox/blob/main/DEVELOPMENT.md">Development Guide</a> for instructions on setting up your development environment, building the project, running tests, and creating releases.</p>
<p dir="auto">For contribution guidelines, please refer to <a href="https://github.com/microsandbox/microsandbox/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/799bc80710f1e8cfbe1adf44cdb7303084bfcf1a45146f197fc95169ad608a75/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c61772f413737304546"><img height="18" src="https://camo.githubusercontent.com/799bc80710f1e8cfbe1adf44cdb7303084bfcf1a45146f197fc95169ad608a75/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f6c61772f413737304546" data-canonical-src="https://octicons-col.vercel.app/law/A770EF"></a>&nbsp;&nbsp;LICENSE</sub></h2><a id="user-content-license" aria-label="Permalink: &nbsp;&nbsp;LICENSE" href="#license"></a></div>
<p dir="auto">This project is licensed under the <a href="https://github.com/microsandbox/microsandbox/blob/main/LICENSE">Apache License 2.0</a>.</p>
<p>• • •</p>
<div dir="auto"><h2 tabindex="-1" dir="auto"><sub><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dad3a1ab46102e0e2d93daf04343d8b63df297fe2a3037ffe9d947795c36c125/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f737461722f413737304546"><img height="18" src="https://camo.githubusercontent.com/dad3a1ab46102e0e2d93daf04343d8b63df297fe2a3037ffe9d947795c36c125/68747470733a2f2f6f637469636f6e732d636f6c2e76657263656c2e6170702f737461722f413737304546" data-canonical-src="https://octicons-col.vercel.app/star/A770EF"></a>&nbsp;&nbsp;STAR HISTORY</sub></h2><a id="user-content-star-history" aria-label="Permalink: &nbsp;&nbsp;STAR HISTORY" href="#star-history"></a></div>
<p dir="auto">Thanks for all the support!</p>
<div dir="auto">
  <a href="https://star-history.com/#microsandbox/microsandbox&amp;Date" rel="nofollow">
   <themed-picture data-catalyst-inline="true"><picture>
     <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/313494caffe2d6e0e4c608b75f8a2fed3e5f3ee12795dbeb0460b657073f965c/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date&amp;theme=dark">
     <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/faa075c8d4446396ba6d126bd328eec80dd740b02bc92bef4c2241ca93ca6d98/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date">
     <img alt="Star History Chart" src="https://camo.githubusercontent.com/faa075c8d4446396ba6d126bd328eec80dd740b02bc92bef4c2241ca93ca6d98/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d6963726f73616e64626f782f6d6963726f73616e64626f7826747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=microsandbox/microsandbox&amp;type=Date">
   </picture></themed-picture>
  </a>
</div>

<p>• • •</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Systems Correctness Practices at Amazon Web Services (255 pts)]]></title>
            <link>https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</link>
            <guid>44135638</guid>
            <pubDate>Fri, 30 May 2025 12:43:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/">https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/</a>, See on <a href="https://news.ycombinator.com/item?id=44135638">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><section id="sec1"><p id="p-1">Amazon Web Services (AWS) strives to deliver reliable services that customers can trust completely. This requires maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 article published in <i>Communications of the ACM</i>, titled “How Amazon Web Services Uses Formal Methods,” highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers.<a href="#B21" data-jats-ref-type="bibr" data-jats-rid="B21"><sup>21</sup></a></p><p id="p-2">Central to this approach was TLA+,<a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a> a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches such as testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness.</p><p id="p-3">Moreover, 15 years ago, AWS’s software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS’s scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers.</p><p id="p-4">This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches (such as theorem proving,<a href="#B7" data-jats-ref-type="bibr" data-jats-rid="B7"><sup>7</sup></a><sup>,</sup><a href="#B10" data-jats-ref-type="bibr" data-jats-rid="B10"><sup>10</sup></a> deductive verification,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and model checking<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a><sup>,</sup><a href="#B14" data-jats-ref-type="bibr" data-jats-rid="B14"><sup>14</sup></a>) to more lightweight semi-formal approaches (such as property-based testing,<a href="#B6" data-jats-ref-type="bibr" data-jats-rid="B6"><sup>6</sup></a><sup>,</sup><a href="#B19" data-jats-ref-type="bibr" data-jats-rid="B19"><sup>19</sup></a> fuzzing,<a href="#B9" data-jats-ref-type="bibr" data-jats-rid="B9"><sup>9</sup></a> and runtime monitoring<a href="#B11" data-jats-ref-type="bibr" data-jats-rid="B11"><sup>11</sup></a>).</p></section><section id="sec2"><h2>The P Programming Language</h2><p id="p-5">As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+’s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport’s views on the benefits of mathematical thinking,<a href="#B15" data-jats-ref-type="bibr" data-jats-rid="B15"><sup>15</sup></a> we also sought a language that would allow us to model check (and later prove) key aspects of systems design while being more approachable to programmers.</p><p id="p-6">We found this balance in the P programming language.<a href="#B8" data-jats-ref-type="bibr" data-jats-rid="B8"><sup>8</sup></a> P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon’s developer population—most of whom develop systems based on microservices and service-oriented architectures (SOAs). P has been developed at AWS since 2019 and is maintained as a strategic open source project.<a href="#B22" data-jats-ref-type="bibr" data-jats-rid="B22"><sup>22</sup></a> Teams across AWS that build some of its flagship products—from storage (for example, Amazon S3, EBS), to databases (for example, Amazon DynamoDB, MemoryDB, Aurora), to compute (for example, EC2, IoT)—have been using P to reason about the correctness of their system designs.</p><p id="p-7">For example, P was used in migrating Simple Storage Service (S3) from eventual to strong read-after-write consistency.<a href="#B1" data-jats-ref-type="bibr" data-jats-rid="B1"><sup>1</sup></a> A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack.<a href="#B25" data-jats-ref-type="bibr" data-jats-rid="B25"><sup>25</sup></a> Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking.</p><p id="p-8">In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (that is, connecting design-time validation with system implementation).</p></section><section id="sec3"><h2>Lightweight Formal Methods</h2><p id="p-9">Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of <i>lightweight formal methods</i>.</p><section id="sec4"><p data-jats-content-type="inline-heading"><strong>Property-based testing.</strong>&nbsp; The most notable single example of leveraging lightweight formal methods is in Amazon S3’s ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up development (described in detail by Bornholt, et al.<a href="#B4" data-jats-ref-type="bibr" data-jats-rid="B4"><sup>4</sup></a>). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging).</p></section><section id="sec5"><p data-jats-content-type="inline-heading"><strong>Deterministic simulation.</strong>&nbsp; Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness, such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested.</p><p id="p-12">Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open sourced as part of the shuttle<a href="#FN1" data-jats-rid="FN1" data-jats-ref-type="fn"><sup>a</sup></a> and turmoil<a href="#FN2" data-jats-rid="FN2" data-jats-ref-type="fn"><sup>b</sup></a> projects.</p></section><section id="sec6"><p data-jats-content-type="inline-heading"><strong>Continuous fuzzing or random test-input generation.</strong>&nbsp; Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora’s data-sharding feature (Aurora Limitless Database<a href="#B3" data-jats-ref-type="bibr" data-jats-rid="B3"><sup>3</sup></a>), for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the non-sharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer<a href="#B23" data-jats-ref-type="bibr" data-jats-rid="B23"><sup>23</sup></a>).</p><p id="p-14">Fuzzing, combined with fault-injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction is executed against the system under test. We also use post hoc validation of properties such as isolation (following approaches such as Elle<a href="#B13" data-jats-ref-type="bibr" data-jats-rid="B13"><sup>13</sup></a>).</p></section></section><section id="sec7"><h2>Fault Injection as a Service</h2><p id="p-15">In early 2021, AWS launched Fault Injection Service (FIS)<a href="#B2" data-jats-ref-type="bibr" data-jats-rid="B2"><sup>2</sup></a> with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024.</p><p id="p-16">In 2014, Yuan et al. found that 92% of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage was not higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density.</p><p id="p-17">While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, “Yup, that looks about right”).</p></section><section id="sec8"><h2>Metastability and Emergent System Behavior</h2><p id="p-18">Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it does not recover without intervention (such as reducing load below normal). This class of failures, dubbed <i>metastable failures</i>,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a> is one of the most important contributors to unavailability in cloud systems. The figure, adapted from Bronson et al.,<a href="#B5" data-jats-ref-type="bibr" data-jats-rid="B5"><sup>5</sup></a>&nbsp;illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present even in simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic.</p><figure id="UF1"><p><a data-fslightbox="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" data-type="image" data-caption="Figure. Metastable system behavior under load." href="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg">
				<img decoding="async" title="Figure. Metastable system behavior under load." src="https://cacm.acm.org/wp-content/uploads/2025/04/3729175_fig01.jpg" alt="Metastable system behavior under load." data-image-id="UF1" data-image-type="figure">
			</a>
		</p><figcaption><span>Figure.&nbsp;</span> <span>Metastable system behavior under load.</span></figcaption></figure><p id="p-19">Traditional formal approaches to modeling distributed systems typically focus on <i>safety</i> (nothing bad happens) and <i>liveness</i> (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages such as TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+’s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks such as understanding the achievability of latency service-level agreements (SLAs).</p></section><section id="sec9"><h2>Formal Proof</h2><p id="p-20">In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them.</p><p id="p-21">In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof.<a href="#B12" data-jats-ref-type="bibr" data-jats-rid="B12"><sup>12</sup></a><sup>,</sup><a href="#B24" data-jats-ref-type="bibr" data-jats-rid="B24"><sup>24</sup></a> The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team’s work on correctness.</p><p id="p-22">Another example is the Firecracker virtual machine monitor (VMM). Firecracker uses a low-level protocol called <i>virtio</i> to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani<a href="#B20" data-jats-ref-type="bibr" data-jats-rid="B20"><sup>20</sup></a> that can reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do.</p><p id="p-23">Proofs around the behaviors of programs are an important part of AWS’s software correctness program, so we support development on tools such as Kani, Dafny,<a href="#B18" data-jats-ref-type="bibr" data-jats-rid="B18"><sup>18</sup></a> and Lean,<a href="#B16" data-jats-ref-type="bibr" data-jats-rid="B16"><sup>16</sup></a> and the underlying tools—such as satisfiability modulo theory (SMT) solvers—that power them.</p><p id="p-24">The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value.</p></section><section id="sec10"><h2>Benefits Beyond Correctness</h2><p id="p-25">Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from 2 to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics.</p><p id="p-26">First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved.&nbsp;Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services.</p><p id="p-27">These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the Rivest-Shamir-Adleman (RSA) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94%.<a href="#B17" data-jats-ref-type="bibr" data-jats-rid="B17"><sup>17</sup></a></p><p id="p-28">Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance.</p></section><section id="sec11"><h2>Challenges and Opportunities for the Future</h2><p id="p-29">Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces.</p><p id="p-30">Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies, such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking.</p><p id="p-31">The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain.</p><p id="p-32">Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as “retry N times on timeout,” continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling expensive and complex. Ongoing research in this area holds promising potential for advancement.</p><p id="p-33">Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community.</p></section><section id="sec12"><h2>Conclusion</h2><p id="p-34">Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS’s testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Darwin Gödel Machine: AI that improves itself by rewriting its own code (142 pts)]]></title>
            <link>https://sakana.ai/dgm/</link>
            <guid>44135369</guid>
            <pubDate>Fri, 30 May 2025 12:08:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sakana.ai/dgm/">https://sakana.ai/dgm/</a>, See on <a href="https://news.ycombinator.com/item?id=44135369">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
  <h2><a href="https://sakana.ai/dgm/">The Darwin Gödel Machine: AI that improves itself by rewriting its own code</a></h2><time datetime="2025-05-30T00:00:00+09:00">May 30, 2025</time>
</header>

  <p><img src="https://sakana.ai/assets/dgm/dgm-main.png" width="100%"><br>
<!--<img class="b-lazy" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-main.png" style="width: 100%;"/><br/>--></p>

<!--more-->

<h2 id="summary">Summary</h2>

<p>A <a href="https://www.cs.mcgill.ca/~dprecup/courses/AI/Materials/turing1950.pdf">longstanding goal</a> of AI research has been the creation of AI that can learn <em>indefinitely</em>. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a <a href="https://en.wikipedia.org/wiki/G%C3%B6del_machine">Gödel Machine</a>, proposed by <a href="https://people.idsia.ch/~juergen/goedelmachine.html">Jürgen Schmidhuber</a> decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in <a href="https://people.idsia.ch/~juergen/metalearning.html#secME">meta-learning</a> or “learning to learn.”</p>

<p>While the theoretical Gödel Machine promised <em>provably</em> beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically <em>prove</em> that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune’s lab at UBC, propose something more feasible: a system that harnesses the <em>principles</em> of open-ended algorithms like Darwinian evolution to search for improvements that <em>empirically</em> improve performance.</p>

<p>We call the result the <strong>Darwin Gödel Machine</strong> (<a href="https://arxiv.org/abs/2505.22954">full technical report</a>). DGMs leverage foundation models to <a href="https://arxiv.org/abs/2206.08896">propose code improvements</a>, and use <a href="https://arxiv.org/abs/2408.08435">recent</a> <a href="https://arxiv.org/abs/2306.01711">innovations</a> in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with <a href="https://arxiv.org/abs/1905.10985">the clear trend</a> that AI systems that rely on <em>learning</em> ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-src="/assets/dgm/dgm-animation.gif"><br>
<small><i>The <b>Darwin Gödel Machine</b> is a self-improving coding agent that rewrites its own code to improve performance on programming tasks. It creates various self-improvements, such as a patch validation step, better file viewing, enhanced editing tools, generating and ranking multiple solutions to choose the best one, and adding a history of what has been tried before (and why it failed) when making new changes.</i></small></p>

<video autoplay="" muted="" playsinline="" loop=""><source src="https://sakana.ai/assets/dgm/dgm-code-evolution-smaller.mp4"></video>

<p>For further details please read our <a href="https://arxiv.org/abs/2505.22954">Technical Report</a> and <a href="https://github.com/jennyzzt/dgm">released code</a>.</p>



<h2 id="introduction">Introduction</h2>

<p>Most current AI systems learn during training only. Then their intelligence is locked in place and deployed. Could they instead, like humans, or the entire community of human scientists, continue to learn and self-improve <em>forever</em>? Moreover, could such self-improvement catalyze future self-improvement?</p>

<p><img data-src="/assets/dgm/dgm-conceptual.png" src="https://sakana.ai/assets/dgm/dgm-conceptual.png"><br>
<small><i>The <b>Darwin Gödel Machine</b> iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</i></small></p>

<p>Our <em>**Darwin Gödel Machine (DGM) **</em>is a step in that direction. Although we believe the full potential of self-modification is much broader than the capabilities offered by existing agentic systems, DGMs can also be applied to practical, <a href="https://arxiv.org/abs/2408.08435">agentic tasks</a>, which combine foundation models with tools, such as web search, or workflows, such as creating three potential answers and choosing the best one. This first DGM is a coding agent that has the ability to:</p>

<ol>
  <li>
    <p><strong>Read and Modify Its Own Code</strong>: It understands and can modify its own Python codebase to try to self-improve (e.g., adding a new tool, or suggesting a different workflow).</p>
  </li>
  <li>
    <p><strong>Evaluate if the Change Improves Performance</strong>: Proposed new versions of itself are evaluated on coding benchmarks (like <a href="https://www.swebench.com/original.html">SWE-bench</a> and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a>). As our results show, improved performance on coding challenges means it has also gotten better at improving itself.</p>
  </li>
  <li>
    <p><strong>Open-endedly Explore the AI Design Space</strong>: New agents are added to an ever-expanding archive of interesting agents. Harnessing <a href="https://books.google.ca/books?id=Llb1CAAAQBAJ&amp;printsec=frontcover&amp;source=gbs_atb&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">the</a> <a href="https://www.nature.com/articles/s42256-018-0006-z">power</a> <a href="https://arxiv.org/abs/1905.10985">of</a> <a href="https://arxiv.org/abs/2408.08435">open</a>-<a href="https://arxiv.org/abs/2306.01711">ended</a> <a href="https://sakana.ai/ai-scientist/">algorithms</a>, future self-modifications can then branch off from any agent in this growing archive, allowing for parallel exploration of many different evolutionary paths. This open-ended exploration helps DGM discover truly novel solutions and avoid getting trapped in suboptimal designs.</p>
  </li>
</ol>

<p>If done safely (see our section dedicated to safety below), such self-improving AI could help us take advantage of the <a href="https://www.darioamodei.com/essay/machines-of-loving-grace">tremendous benefits</a> for society that AI has the potential to usher in.</p>



<h2 id="results">Results</h2>

<p>Experiments demonstrate the Darwin Gödel Machine can continuously self-improve by modifying its own codebase. That is true both on <a href="https://www.swebench.com/original.html">SWE-bench</a> (a widely used benchmark requiring agents to resolve real-world GitHub issues) and <a href="https://aider.chat/docs/leaderboards/">Polyglot</a> (a multi-language coding benchmark). On SWE-bench, the DGM automatically improved its performance from 20.0% to 50.0%. On Polyglot, the DGM jumped performance from an initial 14.2% to 30.7%, which far surpasses the representative hand-designed agent by <a href="https://aider.chat/">Aider</a>. These substantial gains demonstrate the DGM’s ability to discover and implement beneficial changes to its own code.</p>

<p>We also experimentally validate that the DGM’s ability to modify its own code speeds up its own learning (see below). A control without self-improvement performs far worse (DGM w/o Self-improve in the below figure). Moreover, experiments also show that harnessing the principles of open-ended algorithms was also key: without the Darwinian-evolution-inspired search based on growing an archive of different stepping stones, thus allowing <a href="https://www.researchgate.net/publication/277131571_Innovation_Engines_Automated_Creativity_and_Improved_Stochastic_Optimization_via_Deep_Learning">goal switching</a> and the exploration of many different paths in parallel, performance was lower (DGM w/o Open-ended exploration in the below figure).</p>

<p><img data-src="/assets/dgm/dgm-self-improve.png" src="https://sakana.ai/assets/dgm/dgm-self-improve.png"><br>
<small><i><b>Self-improvement</b> and <b>open-ended exploration</b> enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right). It outperforms baselines that lack either self-improvement or open-ended exploration, showing that both components are essential for continual self-improvement.</i></small></p>

<p>The lineage of DGM’s self-improvements on SWE-Bench is visualized in the figure below, which depicts the evolutionary tree of agents generated within its growing archive. This branching structure is a direct consequence of DGM’s open-ended exploration strategy, where it samples from a diverse pool of previously generated agents to seed new modifications. This process enables DGMs to explore multiple evolutionary pathways simultaneously. We observed that some less-performant “ancestor” agents, which might have been discarded by simpler hill-climbing optimization, were instrumental in discovering novel features or approaches that led to significant performance breakthroughs in their descendants, thereby avoiding premature convergence on suboptimal solutions.</p>

<p>The following 2 figures illustrate how the Darwin Gödel Machine automatically self-improves to become a better coding agent:</p>

<center>
<img data-src="/assets/dgm/dgm-archive-tree.png" src="https://sakana.ai/assets/dgm/dgm-archive-tree.png"></center>
<p><small><i>Archive of coding agents generated during the DGM run on SWE-bench.</i></small><br></p>
<center>
<img data-src="/assets/dgm/dgm-progress.png" src="https://sakana.ai/assets/dgm/dgm-progress.png"></center>
<p><small><i>Performance progress and a summary of key innovations of the DGM on SWE-bench. The dark line shows the lineage of the final best-discovered agent. The DGM discovers different tools for improved file editing and a patch strategy that combines multiple generations with ranking. Note that the path to the final best agent sometimes involved agents that performed worse than their parents, highlighting the benefits of open-ended search, including keeping an archive of interesting stepping stones to explore from, rather than only branching off from the highest-performing agent discovered so far.</i></small></p>



<p>Crucially, the DGM engineered improvements within its own agent design that proved to be fundamental and broadly transferable, not merely adaptations overfit to its model or task. The improvements discovered by the DGM (e.g., better tools, refined workflows) generalize to produce higher performance across different underlying FMs. For example, an agent optimized with Claude 3.5 Sonnet also showed improved performance when powered by o3-mini or Claude 3.7 Sonnet (left two panels in the figure below). This shows that the DGM discovers general agent design improvements rather than just model-specific tricks.</p>

<p><img data-src="/assets/dgm/dgm-transfer.png" src="https://sakana.ai/assets/dgm/dgm-transfer.png"><br>
<small><i><b>Agent transfer between models and tasks.</b>  The superior performance of DGM-discovered agents can be transferred across different models (Left and Middle) and different task domains (Right), such as from Python tasks to tasks in other languages like Rust, C++, Go, and others.</i></small></p>

<p>This transferability was also evident across programming languages. A DGM variant whose self-improvement process was exclusively guided by its performance on Python tasks within the Polyglot benchmark subsequently demonstrated significant performance gains on tasks in entirely different programming languages (like Rust, C++, and Go) from the same benchmark (right panel in above figure).</p>

<p>These results collectively show that the Darwin Gödel Machine, through its self-referential code modification and open-ended exploration, can autonomously discover and implement increasingly sophisticated and generalizable improvements to AI agents.</p>



<h2 id="dgm-and-ai-safety-building-trustworthy-self-improvement">DGM and AI Safety: Building Trustworthy Self-Improvement</h2>

<p>The prospect of AI systems that autonomously enhance their own capabilities naturally brings the <a href="https://managing-ai-risks.com/">important topic</a> of AI Safety to the forefront. When an AI can rewrite its own code, it is crucial that its development is safe and aligns with human intentions. Modifications optimized solely for improving performance on a benchmark could introduce unintended behaviors or create systems too complicated for manual human oversight.</p>

<p>From the outset, DGM has been developed with safety in mind. All self-modifications and evaluations occur within secure, sandboxed environments, under human supervision and with strict limits on access to the web. The DGM archive also provides a transparent, traceable lineage of every change, and our reviews of the changes that occurred in our experiments show modifications focused solely on improving coding capabilities. Please see our <a href="https://arxiv.org/abs/2505.22954">paper</a> for a more thorough review of our safety practices and findings.</p>

<p>Beyond mitigating risks, self-improvement itself could offer a novel pathway to <em>enhance</em> AI safety. In preliminary investigations (detailed in our <a href="https://arxiv.org/abs/2505.22954">paper</a>), we tasked DGM with addressing its own tendency to sometimes pretend to use external tools instead of actually using them (including making up the output from the tools). The results are encouraging: DGM began to identify and propose solutions to these problems.</p>

<p>However, we did notice, and documented in our paper, instances when the DGM hacked its reward function. For example, we had cases where it hallucinated that it was using external tools, such as a command line tool that runs unit tests that determine if the code is functioning properly. It faked a log making it look like it had run the tests and that they had passed, when in fact they were never run! Because these logs become its context, it later mistakenly thought its proposed code changes had passed all the unit tests.</p>

<p><img data-src="/assets/dgm/dgm-hallucination.png" src="https://sakana.ai/assets/dgm/dgm-hallucination.png"><br>
<small><i><b>Example of a model hallucinating using a command line tool.</b> In this case the running of unit tests that check if the code is functioning properly. The model seems to know it should run the unit tests, but rather than doing so, it just creates a fake log that makes it look like it ran the tests and that they all passed! </i></small></p>

<p>We conducted an experiment to see if DGM could fix this issue. We created a “tool use hallucination” reward function and asked the DGM to optimize that score to solve the hallucination issue. In many instances, DGM did provide solutions that addressed this type of hallucination. However, in some cases, it removed the markers we use in the reward function to detect hallucination (despite our explicit instruction not to do so), hacking our hallucination detection function to report false successes. We were able to detect that because DGM provides a transparent, traceable lineage of every change that allows us to quickly catch such undesirable behaviors. While in this case the issue was easily detected, more work is needed to prevent the model from attempting to cheat in this way in the first place.</p>

<p><img data-src="/assets/dgm/dgm-objective-hacking.png" src="https://sakana.ai/assets/dgm/dgm-objective-hacking.png"><br>
<small><i><b>Example of objective hacking.</b> The figure shows a DGM solution that achieved a perfect score on a test of whether it was hallucinating tool use  by (despite instructions not to) removing special tool-use markers we added to detect such hallucinations , sabotaging the hallucination detection function.</i></small></p>

<p>Most broadly, we think safety should be front and center in any research on self-improving AI. The community needs to continue to research how to explore this promising, powerful, area responsibly, including creating systems that are aligned throughout their self-improvement, or even enhance their own safety, transparency, and alignment. These are key research directions we are committed to exploring further.</p>



<h2 id="conclusion">Conclusion</h2>

<p>Darwin Gödel Machines represent a concrete step towards AI systems that can autonomously gather their own stepping stones to learn and innovate forever. Future work will involve scaling up the approach and even letting it improve the training of the foundation models at its core. We must prioritize safety in this research because, if we can explore this direction safely, it has the possibility to unlock untold benefits for society, including enabling us to reap the benefits of accelerated scientific progress much sooner.</p>



<hr>

<h3 id="citation">Citation</h3>

<p>Darwin Gödel Machine: Open-Ended Evolution of Self-Improving Agents</p>

<p>Jenny Zhang (※), Shengran Hu (※), Cong Lu, Robert Lange (†), Jeff Clune (†)</p>

<p>(※) co-first author</p>

<p>(†) co-senior author</p>

<p>Paper:  <a href="https://arxiv.org/abs/2505.22954">https://arxiv.org/abs/2505.22954</a></p>

<p>Code: <a href="https://github.com/jennyzzt/dgm">https://github.com/jennyzzt/dgm</a></p>

<hr>




<center>
<a href="https://sakana.ai/careers/"><img src="https://sakana.ai/assets/dgm/dgm-fish.jpeg" width="80%"></a><br>
</center>





<p>Want to make the AI that improves AI? Please see our <a href="https://sakana.ai/careers/">Careers</a> page for more information.</p>



  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is the best LLM for consumer grade hardware? (186 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44134896</link>
            <guid>44134896</guid>
            <pubDate>Fri, 30 May 2025 11:02:19 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44134896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44135283"><td></td></tr>
                <tr id="44137215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137215" href="https://news.ycombinator.com/vote?id=44137215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; If you want to run LLMs locally then the localllama community is your friend: <a href="https://old.reddit.com/r/LocalLLaMA/" rel="nofollow">https://old.reddit.com/r/LocalLLaMA/</a></p><p>For folks new to reddit, it's worth noting that LocalLlama, just like the rest of the internet but especially reddit, is filled with misinformed people spreading incorrect "facts" as truth, and you really can't use the upvote/downvote count as an indicator of quality or how truthful something is there.</p><p>Something that is more accurate but put in a boring way will often be downvoted, while straight up incorrect but funny/emotional/"fitting the group think" comments usually get upvoted.</p><p>For us who've spent a lot of time on the web, this sort of bullshit detector is basically built-in at this point, but if you're new to places where the group think is so heavy as on reddit, it's worth being careful taking anything at face value.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137262"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137262" href="https://news.ycombinator.com/vote?id=44137262&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>This is entirely why I can't bring myself to use it. The groupthink and virtue signaling is <i>intense</i>, when it's not just extremely low effort crud that rises to the top. And yes, before anyone says, I know, "curate." No, thank you.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137332"><td></td></tr>
                <tr id="44137419"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137419" href="https://news.ycombinator.com/vote?id=44137419&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I understand that the core similarities are there, but I disagree. The comparisons have been around since I started browsing HN years ago. The moderation on this site, for one, emphasizes constructive conversation and discussion in a way that most subreddits can only dream of.</p><p>It also helps that the target audience has been filtered with that moderation, so over time this site (on average) skews more technical and informed.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137755"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_44137755" href="https://news.ycombinator.com/vote?id=44137755&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Frankly, no. As an obvious example that can be stated nowadays: musk has always been an over-promising liar.</p><p>Eg just look at the 2012+ videos of thunderf00t.</p><p>Yet people were literally banned here just for pointing out that he hasn't actually delivered on anything in the capacity he promised until he did the salute.</p><p>It's pointless to list other examples, as this page is- as dingnuts pointed out - exactly the same and most people aren't actually willing to change their opinion based on arguments. They're set in their opinions and think everyone else is dumb.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137702"><td></td></tr>
                <tr id="44137746"><td></td></tr>
                                    <tr id="44136239"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136239" href="https://news.ycombinator.com/vote?id=44136239&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes at this point it's starting to become almost a matter of how much you like the model's personality since they're all fairly decent. OP just has to start downloading and trying them out. With 16GB one can do partial DDR5 offloading with llama.cpp and run anything up to about 30B (even dense) or even more at a "reasonable" speed for chat purposes. Especially with tensor offload.</p><p>I wouldn't count Qwen as that much of a conversationalist though. Mistral Nemo and Small are pretty decent. All of Llama 3.X are still very good models even by today's standards. Gemma 3s are great but a bit unhinged. And of course QwQ when you need GPT4 at home. And probably lots of others I'm forgetting.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135941"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135941" href="https://news.ycombinator.com/vote?id=44135941&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'd also recommend you go with something like 8b, so you can have the other 8GB of vram for a decent sized context window. There's tons of good 8b ones, as mentioned above. If you go for the largest model you can fit, you'll have slower inference (as you pass in more tokens) and smaller context.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136192"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136192" href="https://news.ycombinator.com/vote?id=44136192&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think your recommendation falls within</p><p>&gt; all of them will have some strengths and weaknesses</p><p>Sometimes a higher parameter model with less quantization and low context will be the best, sometimes lower parameter model with some quantization and huge context will be the best, sometimes high parameter count + lots of quantization + medium context will be the best.</p><p>It's really hard to say one model is better than another in a general way, since it depends on so many things like your use case, the prompts, the settings, quantization, quantization method and so on.</p><p>If you're building/trying to build stuff depending on LLMs in any capacity, the first step is coming up with your own custom benchmark/evaluation that you can run with your specific use cases being put under test. Don't share this publicly (so it doesn't end up in the training data) and run it in order to figure out what model is best for that specific problem.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137226"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137226" href="https://news.ycombinator.com/vote?id=44137226&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I’m curious (as someone who knows nothing about this stuff!)—the context window is basically a record of the conversation so far and other info that isn’t part of the model, right?</p><p>I’m a bit surprised that 8GB is useful as a context window if that is the case—it just seems like you could fit a ton of research papers, emails, and textbooks in 2GB, for example.</p><p>But, I’m commenting from a place of ignorance and curiosity. Do models blow up the info in the context window, maybe do some processing to pre-digest it?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137306"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44137306" href="https://news.ycombinator.com/vote?id=44137306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Yes, every token is expanded into a vector that can be many thousand of dimensions. The vectors are stored for every token and every layer.</p><p>You absolutely can not fill even a single research paper in 2 GB much less an entire book.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137490"><td></td></tr>
            <tr id="44136037"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136037" href="https://news.ycombinator.com/vote?id=44136037&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>8b is the number of parameters. The most common quant is 4 bits per parameter so 8b params is roughly 4GB of VRAM. (Typically more like 4.5GB)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136708"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136708" href="https://news.ycombinator.com/vote?id=44136708&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Anything below 4-bits is usually not worth it unless you want to experiment with running a 70B+ model -- though I don't have any experience of doing that, so I don't know how well the increased parameter size balances the quantization.</p><p>See <a href="https://github.com/ggml-org/llama.cpp/pull/1684">https://github.com/ggml-org/llama.cpp/pull/1684</a> and <a href="https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9" rel="nofollow">https://gist.github.com/Artefact2/b5f810600771265fc1e3944228...</a> for comparisons between quantization levels.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137105"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137105" href="https://news.ycombinator.com/vote?id=44137105&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; The number of quantized bits is a trade off between size and quality. Ideally you should be aiming for a 6-bit or 5-bit model. I've seen some models be unstable at 4-bit (where they will either repeat words or start generating random words).</p><p>Note that that's a skill issue of whoever quantized the model. In general quantization even as low as 3-bit can be almost loseless when you do quantization-aware finetuning[1] (and apparently you don't even need that many training tokens), but even if you don't want to do any extra training you can be smart as to which parts of the model you're quantizing and by how much to minimize the damage (e.g. in the worst case over-quantizing even a <i>single</i> weight can have disastrous consequences[2])</p><p>Some time ago I ran an experiment where I finetuned a small model while quantizing parts of it to 2-bits to see which parts are most sensitive (the numbers are the final loss; lower is better):</p><pre><code>    1.5275   mlp.downscale
    1.5061   mlp.upscale
    1.4665   mlp.gate
    1.4531   lm_head
    1.3998   attn.out_proj
    1.3962   attn.v_proj
    1.3794   attn.k_proj
    1.3811   input_embedding
    1.3662   attn.q_proj
    1.3397   unquantized baseline
</code></pre><p>
So as you can see quantizing some parts of the model affects it more strongly. The downprojection in the MLP layers is the most sensitive part of the model (which also matches with what [2] found), so it makes sense to quantize this part of the model less and instead quantize other parts more strongly. But if you'll just do the naive "quantize everything in 4-bit" then sure, you might get broken models.</p><p>[1] - <a href="https://arxiv.org/pdf/2502.02631" rel="nofollow">https://arxiv.org/pdf/2502.02631</a>
[2] - <a href="https://arxiv.org/pdf/2411.07191" rel="nofollow">https://arxiv.org/pdf/2411.07191</a></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136498"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136498" href="https://news.ycombinator.com/vote?id=44136498&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>With a 16GB GPU you can comfortably run like Qwen3 14B or Mistral Small 24B models at Q4 to Q6 and still have plenty of context space and get much better abilities than an 8B model.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135524"><td></td></tr>
                <tr id="44135604"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44135604" href="https://news.ycombinator.com/vote?id=44135604&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Beyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.</p><p>Thank you for thinking of the vibe coders.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44135928"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44135928" href="https://news.ycombinator.com/vote?id=44135928&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; Released today; probably the best reasoning model in 8B size.</p><p>Actually DeepSeek-R1-0528-Qwen3-8B was uploaded Thursday (yesterday) at 11 AM UTC / 7 PM CST.
I had to check if a new version came out since! I am waiting for the other sizes! ;D</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137711"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137711" href="https://news.ycombinator.com/vote?id=44137711&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>At 16GB a Q4 quant of Mistral Small 3.1, or Qwen3-14B at FP8, will probably serve you best. You'd be cutting it a little close on context length due to the VRAM usage... If you want longer context, a Q4 quant of Qwen3-14B will be a bit dumber than FP8 but will leave you more breathing room. Mistral Small can take images as input, and Qwen3 will be a bit better at math/coding; YMMV otherwise.</p><p>Going below Q4 isn't worth it IMO. If you want significantly more context, probably drop down to a Q4 quant of Qwen3-8B rather than continuing to lobotomize the 14B.</p><p>Since you're on a Blackwell-generation Nvidia chip, using LLMs quantized to NVFP4 specifically will provide some speed improvements at some quality cost compared to FP8 (and will be faster than Q4 GGUF, although ~equally dumb). Ollama doesn't support NVFP4 yet, so you'd need to use vLLM (which isn't too hard, and will give better token throughput anyway). Finding pre-quantized models at NVFP4 will be more difficult since there's less-broad support, but you can use llmcompressor [1] to statically compress any FP16 LLM to NVFP4 locally — you'll probably need to use accelerate to offload params to CPU during the one-time compression process, which they have documentation for.</p><p>I wouldn't reach for this particular power tool until you've decided on an LLM already, and just want faster perf, since it's a bit more involved than just using ollama and the initial quantization process will be slow due to CPU offload during compression (albeit it's only a one-time cost). But if you land on a Q4 model, it's not a bad choice once you have a favorite.</p><p>1: <a href="https://github.com/vllm-project/llm-compressor">https://github.com/vllm-project/llm-compressor</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137434"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137434" href="https://news.ycombinator.com/vote?id=44137434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Basic conversations are essentially RP I suppose. You can look at KoboldCPP or SillyTavern reddit.</p><p>I was trying Patricide unslop mell and some of the Qwen ones recently. Up to a point more params is better than worrying about quantization. But eventually you'll hit a compute wall with high params.</p><p>KV cache quantization is awesome (I use q4 for a 32k context with a 1080ti!) and context shifting is also awesome for long conversations/stories/games. I was using ooba but found recently that KoboldCPP not only runs faster for the same model/settings but also Kobold's context shifting works much more consistently than Ooba's "streaming_llm" option, which almost always re-evaluates the prompt when hooked up to something like ST.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137598" href="https://news.ycombinator.com/vote?id=44137598&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Wow, a 5060Ti.  16gb + I'm guessing &gt;=32gb ram.  And here I am spinning Ye Olde RX 570 4gb + 32gb.</p><p>I'd like to know how many tokens you can get out of the larger models especially (using Ollama + Open WebUI on Docker Desktop, or LM Studio whatever).  I'm probably not upgrading GPU this year, but I'd appreciate an anecdotal benchmark.</p><pre><code>  - gemma3:12b
  - phi4:latest (14b)
  - qwen2.5:14b [I get ~3 t/s on all these small models, acceptably slow]

  - qwen2.5:32b [this is about my machine's limit; verrry slow, ~1 t/s]
  - qwen2.5:72b [beyond my machine's limit, but maybe not yours]</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137621"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137621" href="https://news.ycombinator.com/vote?id=44137621&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm guessing you probably also want to include the quantization levels you're using, as otherwise they'll be a huge variance in your comparisons with others :)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136107" href="https://news.ycombinator.com/vote?id=44136107&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>What is everyone using their local LLMs for primarily? Unless you have a beefy machine, you'll never approach the level of quality of proprietary models like Gemini or Claude, but I'm guessing these smaller models still have their use cases, just not sure what those are.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136198"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136198" href="https://news.ycombinator.com/vote?id=44136198&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Not everyone is comfortable with sending their data and/or questions and prompts to an external party.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136410"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136410" href="https://news.ycombinator.com/vote?id=44136410&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I generally try a local model first for most prompts. It's good enough surprisingly often (over 50% for sure). Every time I avoid using a cloud service is a win.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136393"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136393" href="https://news.ycombinator.com/vote?id=44136393&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>You still can get decent stuff out of local ones.</p><p>Mostly I use it for testing tools and integrations via API not to spend money on subscriptions. When I see something working I switch it to proprietary one to get best results.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136215"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136215" href="https://news.ycombinator.com/vote?id=44136215&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I'm currently experimenting with Devstral for my own local coding agent I've slowly built together. It's in many ways nicer than Codex in that 1) full access to my hardware so can start VMs, make network requests and everything else I can do, which Codex cannot and 2) it's way faster both in initial setup, working through things and creating a patch.</p><p>Of course, it still isn't at the same level as Codex itself, the model Codex is using is just way better so of course it'll get better results. But Devstral (as I currently use it) is able to make smaller changes and refactors, and I think if I evolve the software a bit more, can start making larger changes too.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136187"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136187" href="https://news.ycombinator.com/vote?id=44136187&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think that the future of local LLMs is delegation. You give it a prompt and it very quickly identifies what should be used to solve the prompt.</p><p>Can it be solved locally with locally running MCPs? Or maybe it's a system API - like reading your calendar or checking your email. Otherwise it identifies the best cloud model and sends the prompt there.</p><p>Basically Siri if it was good</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136300"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136300" href="https://news.ycombinator.com/vote?id=44136300&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>&gt; unless you have a beefy machine</p><p>The average person in r/locallama has a machine that would make r/pcmasterrace users blush.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136406"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136406" href="https://news.ycombinator.com/vote?id=44136406&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>An Apple M1 is decent enough for LMs. My friend wondered why I got so excited about it when it came out five years ago. It wasn't that it was particularly powerful - it's decent. What it did was to set a new bar for "low end".</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136434"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136434" href="https://news.ycombinator.com/vote?id=44136434&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>A new Mac is easily starting around $1k and quickly goes up from there if you want a storage or RAM upgrade, especially for enough memory to really run some local models. Insane that a $1,000 computer is called "decent" and "low end". My daily driver personal laptop brand new was $300.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137458"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137458" href="https://news.ycombinator.com/vote?id=44137458&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>You're right - memory size and then bandwidth is imperative for LLMs. Apple currently lacks great memory bandwidth with their unified memory. But it's not a bad option if you can find one for a good price. The prices for new are just bonkers.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136558"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136558" href="https://news.ycombinator.com/vote?id=44136558&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>An M1 Mac is about 5 years old at this point and can be had for far less than a grand.</p><p>A brand new Mac Mini M4 is only $499.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136703"><td></td></tr>
                <tr id="44136773"><td></td></tr>
                        <tr id="44136478"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44136478" href="https://news.ycombinator.com/vote?id=44136478&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>That's fun to hear given that low end laptops are now $800, mid range is like $1.5k and upper end is $3k+ even for non-Apple vendors. Inflation makes fools of us all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137659"><td></td></tr>
            <tr id="44136663"><td></td></tr>
                                    <tr id="44136288"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136288" href="https://news.ycombinator.com/vote?id=44136288&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I have a large repository of notes, article drafts, and commonplace book-type stuff.
I experimented a year or so ago with a system using RAG to "ask myself" what I have to say about various topics. (I suppose nowadays I would use MCP instead of RAG?)
I was not especially impressed by the results with the models I was able to run: long-winded responses full of slop and repetition, irrelevant information pulled in from notes that had some semantically similar ideas, and such.
I'm certainly not going to feed the contents of my private notebooks to any of the AI companies.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136425"><td></td></tr>
                <tr id="44136592"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136592" href="https://news.ycombinator.com/vote?id=44136592&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>To clarify: what I was doing was first querying for the documents via a standard document database query and then feeding the best matching documents to the LLM.
My understanding is that with MCP I'd delegate the document query from the LLM to the tool.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137494"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44137494" href="https://news.ycombinator.com/vote?id=44137494&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>As a beginner, I also haven't had much luck with embedded vector queries either. Firstly, setting it up was a major pain in the ass and I couldn't even get it to ingest anything beyond .txt files. Second, maybe it was my AI system prompt or the lack of outside search capabilities but unless i was very specific with my query the response was essentially "can't find what youre looking for"</p>
              </div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="44137517"><td></td></tr>
                <tr id="44137597"><td></td></tr>
                  <tr id="44135237"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135237" href="https://news.ycombinator.com/vote?id=44135237&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I only have 8gb of vram to work with currently, but I'm running OpenWebUI as a frontend to ollamma and I have a very easy time loading up multiple models and letting them duke it out either at the same time or in a round robin.</p><p>You can even keep track of the quality of the answers over time to help guide your choice.</p><p><a href="https://openwebui.com/" rel="nofollow">https://openwebui.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137017"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137017" href="https://news.ycombinator.com/vote?id=44137017&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Related question: what is everyone using to run a local LLM? I'm using Jan.ai and it's been okay. I also see OpenWebUI mentioned quite often.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137166"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137166" href="https://news.ycombinator.com/vote?id=44137166&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>LM studio if you just want an app. openwebui is just a front end - you'd need to have either llama.cpp or vllm behind it to serve the model</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44137466"><td></td></tr>
            <tr id="44137142"><td></td></tr>
                  <tr id="44135269"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135269" href="https://news.ycombinator.com/vote?id=44135269&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I'm afraid that 1) you are not going to get a definite answer, 2) an objective answer is very hard to give, 3) you really need to try a few most recent models on your own and give them the tasks that seem most useful/meaningful to you. There is drastic difference in output quality depending on the task type.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136421" href="https://news.ycombinator.com/vote?id=44136421&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Generally speaking, how can you tell how much vram a model will take?  It seems to be a valuable bit of data which is missing from downloadable models (gguf) files.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136515"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136515" href="https://news.ycombinator.com/vote?id=44136515&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Very rougly you can consider the Bs of a model as GBs of memory then it depends on the quantization level. Say for an 8B model:</p><p>- FP16: 2x 8GB = 16GB</p><p>- Q8: 1x 8GB</p><p>- Q4: 0.5x 8GB = 4GB</p><p>It doesn't 100% neatly map like this but this gives you a rough measure. In top of this you need some more memory depending on the context length and some other stuff.</p><p>Rationale for the calculation above: A model is basically a billions of variables with a floating number value. So the size of a model roughly maps to number of variables (weights) x word-precision of each variable (4, 8, 16bits..)</p><p>You don't have to quantize all layers to the same precision this is why sometimes you see fractional quantizations like 1.58bits.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136815"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44136815" href="https://news.ycombinator.com/vote?id=44136815&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>The 1.58bit quantization is using 3 values -- -1, 0, 1. The bits number comes from log_2(3) = 1.58....</p><p>For that level you can pack 4 weights in a byte using 2 bits per byte. However, there is one bit configuration in each that is unused.</p><p>More complex packing arrangements are done by grouping weights together (e.g. a group of 3) and assigning a bit configuration to each combination of values into a lookup table. This allows greater compression closer to the 1.68 bits value.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44137485"><td></td></tr>
                  <tr id="44137188"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44137188" href="https://news.ycombinator.com/vote?id=44137188&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>It's a bit like asking what flavour of icecream is the best. Try a few and see.</p><p>For 16gb and speed you could try Qwen3-30B-A3B with some offload to system ram or use a dense model Probably a 14B quant</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135367"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135367" href="https://news.ycombinator.com/vote?id=44135367&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I have an RTX 3070 with 8GB VRAM and for me Qwen3:30B-A3B is fast enough. It's not lightning fast, but more than adequate if you have a _little_ patience.</p><p>I've found that Qwen3 is generally really good at following instructions and you can also very easily turn on or off the reasoning by adding "/no_think" in the prompt to turn it off.</p><p>The reason Qwen3:30B works so well is because it's a MoE. I have tested the 14B model and it's noticeably slower because it's a dense model.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136024"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44136024" href="https://news.ycombinator.com/vote?id=44136024&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>How are you getting Qwen3:30B-A3B running with 8GB? On my system it takes 20GB of VRAM to launch it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44137522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44137522" href="https://news.ycombinator.com/vote?id=44137522&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Probably offload to regular ram I'd wager. Or really, really, reaaaaaaally quantized to absolute fuck. Qwen3:30B-A3B Q1 with a 1k Q4 context uses 5.84GB of vram.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44136471"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136471" href="https://news.ycombinator.com/vote?id=44136471&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>I’ve had awesome results with Qwen3-30B-A3B compared to other local LMs I’ve tried.  Still not crazy good but a lot better and very fast.  I have 24GB of VRAM though</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135375"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135375" href="https://news.ycombinator.com/vote?id=44135375&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>I think you'll find that on that card most models that are approaching the 16G memory size will be more than fast enough and sufficient for chat. You're in the happy position of needing steeper requirements rather than faster hardware! :D</p><p>Ollama is the easiest way to get started trying things out IMO: <a href="https://ollama.com/">https://ollama.com/</a></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44135827"><td></td></tr>
                <tr id="44136218"><td></td></tr>
            <tr id="44135979"><td></td></tr>
                <tr id="44136315"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44136315" href="https://news.ycombinator.com/vote?id=44136315&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Any FOSS solutions that let you browse models and guesstimates for you on whether you have enough VRAM to fully load the model? That's the only selling point to LM Studio for me.</p><p>Ollama's default context length is frustratingly short in the era of 100k+ context windows.</p><p>My solution so far has been to boot up LM Studio to check if a model will work well on my machine, manually download the model myself through huggingface, run llama.cpp, and hook it up to open-webui. Which is less than ideal, and LM Studio's proprietary code has access to my machine specs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44136796"><td></td></tr>
                                    <tr id="44136744"><td></td></tr>
            <tr id="44136016"><td></td></tr>
            <tr id="44135318"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135318" href="https://news.ycombinator.com/vote?id=44135318&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Good question. I've had some success with Qwen2.5-Coder 14B, I did use the quantised version: huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest 
It worked well on my MacBook Pro M1 32Gb. It does get a bit hot on a laptop though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135841"><td></td></tr>
            <tr id="44135350"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135350" href="https://news.ycombinator.com/vote?id=44135350&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>VEGA64 (8GB) is pretty much obsolete <i>for this AI stuff, right</i> (compared to e.g. M2Pro (16GB))?</p><p>I'll give Qwen2.5 a try on the Apple Silicon, thanks.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44135306" href="https://news.ycombinator.com/vote?id=44135306&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>Agree with what others have said: you need to try a few out. But I'd put Qwen3-14B on your list of things to try out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135824"><td></td></tr>
                      <tr id="44136384"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136384" href="https://news.ycombinator.com/vote?id=44136384&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Phi-4 is scared to talk about anything controversial, as if they're being watched.</p><p>I asked it a question about militias. It thought for a few pages about the answer and whether to tell me, then came back with "I cannot comply".</p><p>Nidum is the name of uncensored Gemma, it does a good job most of the time.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44136623"><td></td></tr>
            <tr id="44136132"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136132" href="https://news.ycombinator.com/vote?id=44136132&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>hf.co/bartowski/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-GGUF:Q6_K is a decent performing model, if you're not looking for blinding speed. It definitely ticks all the boxes in terms of model quality. Try a smaller quant if you need more speed.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44135491"><td></td></tr>
                <tr id="44137297"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44137297" href="https://news.ycombinator.com/vote?id=44137297&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div>
                  <p>It is feasible to run 7B, 8B models with q6_0 in 8GB VRAM, or q5_k_m/q4_k_m if you have to or want to free up some VRAM for other things. With q4_k_m you can run 10B and even 12B models.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44136267"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44136267" href="https://news.ycombinator.com/vote?id=44136267&amp;how=up&amp;goto=item%3Fid%3D44134896"></a></center>    </td><td><br><div><p>Ollama[0] has a collection of models that are either already small or quantized/distilled, and come with hyperparameters that are pretty reasonable, and they make it easy to try them out. I recommend you install it and just try a bunch because they all have different "personalities", different strengths and weaknesses. My personal go-tos are:</p><p>Qwen3 family from Alibaba seem to be the best reasoning models that fit on local hardware right now. Reasoning models on local hardware are annoying in contexts where you just want an immediate response, but vastly outperform non-reasoning models on things where you want the model to be less naive/foolish.</p><p>Gemma3 from google is really good at intuition-oriented stuff, but with an obnoxious HR Boy Scout personality where you basically have to add "please don't add any disclaimers" to the system prompt for it to function. Like, just tell me how long you think this sprain will take to heal, I already know you are not a medical professional, jfc.</p><p>Devstral from Mistral performs the best on my command line utility where I describe the command I want and it executes that for me (e.g. give me a 1-liner to list the dotfiles in this folder and all subfolders that were created in the last month).</p><p>Nemo from Mistral, I have heard (but not tested) is really good for routing-type jobs, where you need something with to make a simple multiple-choice decision competently with low latency, and is easy to fine-tune if you want to get that sophisticated.</p><p>[0] <a href="https://ollama.com/search">https://ollama.com/search</a></p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is not our future (211 pts)]]></title>
            <link>https://procreate.com/ai</link>
            <guid>44134798</guid>
            <pubDate>Fri, 30 May 2025 10:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://procreate.com/ai">https://procreate.com/ai</a>, See on <a href="https://news.ycombinator.com/item?id=44134798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content"><main role="main" aria-label="Page Content"><!--[--><!--[--><section role="presentation"><div><h2>Where we stand</h2><div><!--[--><div><h3>No generative AI</h3><p>We deeply respect your hard-earned skills.</p></div><div><h3>Your work belongs to you</h3><p>We do not have access to your art, by design.</p></div><div><h3>We take pride in privacy</h3><p>Your activity is not tracked in our apps.</p></div><!--]--></div></div><div><p>Generative AI is ripping the humanity out of things. Built on a foundation of theft, the technology is steering us toward a barren future.  We think machine learning is a compelling technology with a lot of merit, but the path generative AI is on is wrong for us.</p><p>We're here for the humans. We're not chasing a technology that is a moral threat to our greatest jewel: human creativity. In this technological rush, this might make us an exception or seem at risk of being left behind. But we see this road less travelled as the more exciting and fruitful one for our community.</p></div></section><!----><!--]--><!--]--></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radio Astronomy Software Defined Radio (Rasdr) (103 pts)]]></title>
            <link>https://radio-astronomy.org/rasdr</link>
            <guid>44134364</guid>
            <pubDate>Fri, 30 May 2025 09:14:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radio-astronomy.org/rasdr">https://radio-astronomy.org/rasdr</a>, See on <a href="https://news.ycombinator.com/item?id=44134364">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
        <div>
      <article id="node-259" about="/node/259" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/259">Where can I get it?</a></h2>
            <span property="dc:title" content="Where can I get it?"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>Of the two completed SDR receiver (hardware) designs based on the RASDR concept (wide-bandwidth, Windows-compatible, documented SDR for Radio Astronomy), only RASDR4 is now marketed.</p></div>

      
  
    </article> <!-- /.node -->
  </div>
  <div>
      <article id="node-194" about="/node/194" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/194">Detection of narrow spectral features using RASDR2 and the NRAO 20m telescope</a></h2>
            <span property="dc:title" content="Detection of narrow spectral features using RASDR2 and the NRAO 20m telescope"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>Identification and accurate spectral characterization of a narrow L-band spectral feature from a celestial source is a necessary first step in showing the importance of a Doppler (velocityinduced) frequency shift due to the Earth’s rotation. This goal was mentioned at the 2014 SARA conference [1] [2]. With the assistance of NRAO staff, we have integrated RASDR2 [3] [4] with the NRAO 20m radio telescope [5] and have demonstrated detection of a suitable narrow spectral feature.</p></div>

      
  
    </article> <!-- /.node -->
  </div>
  <div>
      <article id="node-191" about="/node/191" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/191">RASDRviewer: RASDR2 Control and Analysis Software</a></h2>
            <span property="dc:title" content="RASDRviewer: RASDR2 Control and Analysis Software"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>The Radio Astronomy Software Defined Receiver (RASDR) is a system that provides a versatile Software-Defined Receiver (SDR) that is optimized for Radio Astronomy. RASDR2 is the current hardware that is in testing with a planned general release at this conference. See multiple other presentations at this conference as well as previous SARA Journals and Proceedings publications for the history of this SARA project.</p></div>

      
  
    </article> <!-- /.node -->
  </div>
  <div>
      <article id="node-189" about="/node/189" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/189">RASDR 2062: 200 Years of Receiver Evolution</a></h2>
            <span property="dc:title" content="RASDR 2062: 200 Years of Receiver Evolution"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>Decades of experimental studies led physicist James Clerk Maxwell to formulate the theoretical foundations of electromagnetic energy in 1862. Thirty years later, another physicist, Henrich Hertz, demonstrated radio transmission and reception and initiated an era of Hardware-Defined Receivers (HDR). Receivers in the span of 150 years evolved from having a design focus in hardware, to a design focus in software, as Software Defined Receivers (SDR) began a 20-y rise to dominance. </p></div>

      
  
    </article> <!-- /.node -->
  </div>
  <div>
      <article id="node-188" about="/node/188" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/188">RASDRWin - Companion Software for RASDR</a></h2>
            <span property="dc:title" content="RASDRWin - Companion Software for RASDR"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>An update of the RASDR project will be presented. The paper demonstrates Windows control of the RASDR system. User-provided data specifies center frequency, bandwidth, and some initial values used to choose optimal operational range of the Lime femtocell ship. Additional settings govern decimation, signal processing, external reference frequency and<br>
output options. The system uses USB2 to provide a high speed interface to the RASDR hardware.</p></div>

      
  
    </article> <!-- /.node -->
  </div>
  <div>
      <article id="node-185" about="/node/185" typeof="sioc:Item foaf:Document">
          <header>
	                  <h2><a href="https://radio-astronomy.org/node/185">Radio Astronomy with RASDR2 and RASDRviewer</a></h2>
            <span property="dc:title" content="Radio Astronomy with RASDR2 and RASDRviewer"></span><span property="sioc:num_replies" content="0" datatype="xsd:integer"></span>  
      
          </header>
  
  <div><p>The RASDR design team is releasing a software-defined receiver (SDR) for radio astronomy called RASDR2. The receiver consists of two high-density circuit boards -- a wide-band femtocell chip on the front end analog interface MyriadRF board linked to a DigiRed digitization and function control board -- coupled to a computer via either a USB2 or USB3 interface. RASDRViewer software runs in a Windows environment and performs receiver control, FFT analysis, spectrum averaging, power monitoring and other functions.</p></div>

      
  
    </article> <!-- /.node -->
  </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFK Jr's 'Maha' report found to contain citations to nonexistent studies (161 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies</link>
            <guid>44133962</guid>
            <pubDate>Fri, 30 May 2025 07:54:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies">https://www.theguardian.com/us-news/2025/may/29/rfk-jr-maha-health-report-studies</a>, See on <a href="https://news.ycombinator.com/item?id=44133962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><a href="https://www.theguardian.com/us-news/robert-f-kennedy-jr" data-link-name="in body link">Robert F Kennedy Jr</a>’s flagship <a href="https://www.theguardian.com/us-news/2025/may/22/rfk-jr-maha-health-report-explained" data-link-name="in body link">health commission report</a> contains citations to studies that do not exist, according to an <a href="https://www.notus.org/health-science/make-america-healthy-again-report-citation-errors" data-link-name="in body link">investigation</a> by the US publication Notus.</p><p>The report exposes glaring scientific failures from a health secretary who earlier this week threatened to ban government scientists from publishing in leading medical journals.</p><p>The 73-page “Make America healthy again” report – which was commissioned by the <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link">Trump administration</a> to examine the causes of chronic illness, and which Kennedy promoted it as “gold-standard” science backed by more than 500 citations – includes references to seven studies that appear to be entirely invented, and others that the researchers say have been mischaracterized.</p><figure id="0992c1a8-3359-4cf1-a406-e28fb87cfb06" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Key takeaways: RFK Jr’s ‘Maha’ report on chronic disease in children&quot;,&quot;elementId&quot;:&quot;0992c1a8-3359-4cf1-a406-e28fb87cfb06&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/may/22/rfk-jr-maha-health-report-explained&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Two supposed studies on ADHD medication advertising simply do not exist in the journals where they are claimed to be published. Virginia Commonwealth University confirmed to Notus that researcher Robert L Findling, listed as an author of one paper, never wrote such an article, while another citation leads only to the Kennedy report itself when searched online.</p><p>Harold J Farber, a pediatric specialist supposedly behind research on asthma overprescribing, told Notus he never wrote the cited paper and had never worked with the other listed authors.</p><p>The US Department of <a href="https://www.theguardian.com/society/health" data-link-name="in body link" data-component="auto-linked-tag">Health</a> and Human Services has not immediately responded to a Guardian request for comment.</p><p>The citation failures come as Kennedy, a noted skeptic of vaccines, <a href="https://www.theguardian.com/us-news/2025/may/28/rfk-jr-medical-journals" data-link-name="in body link">criticized medical publishing</a> this week, branding top journals the Lancet, New England Journal of Medicine and Jama as “corrupt” and alleging they were controlled by pharmaceutical companies. He outlined plans for creating government-run journals instead.</p><p>Beyond the phantom studies in Kennedy’s report, Notus found it systematically misrepresented existing research.</p><figure id="5e068b28-7b23-4d73-bbb4-409abba4a53f" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:9,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;RFK’s health report omits key facts in painting dark vision for US children&quot;,&quot;elementId&quot;:&quot;5e068b28-7b23-4d73-bbb4-409abba4a53f&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/us-news/2025/may/22/rfk-maha-health-report-children&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>For example, one paper was claimed to show that talking therapy was as effective as psychiatric medication, but the statistician Joanne McKenzie said this was impossible, as “we did not include psychotherapy” in the review.</p><p>The sleep researcher Mariana G Figueiro also said her study was mischaracterized, with the report incorrectly stating it involved children rather than college students, and citing the wrong journal entirely.</p><p>The <a href="https://www.theguardian.com/us-news/trump-administration" data-link-name="in body link" data-component="auto-linked-tag">Trump administration</a> asked Kennedy for the report in order to look at chronic illness causes, from pesticides to mobile phone radiation. Kennedy called it a “milestone” that provides “evidence-based foundation” for sweeping policy changes.</p><p>A follow-up “Make our children healthy again strategy” report is due in August, raising concerns about the scientific credibility underpinning the administration’s health agenda.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Buttplug MCP (225 pts)]]></title>
            <link>https://github.com/ConAcademy/buttplug-mcp</link>
            <guid>44133706</guid>
            <pubDate>Fri, 30 May 2025 07:06:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ConAcademy/buttplug-mcp">https://github.com/ConAcademy/buttplug-mcp</a>, See on <a href="https://news.ycombinator.com/item?id=44133706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">buttplug-mcp - Buttplug.io MCP Server</h2><a id="user-content-buttplug-mcp---buttplugio-mcp-server" aria-label="Permalink: buttplug-mcp - Buttplug.io MCP Server" href="#buttplug-mcp---buttplugio-mcp-server"></a></p>
<p dir="auto"><code>buttplug-mcp</code> is a <a href="https://www.anthropic.com/news/model-context-protocol" rel="nofollow">Model Context Protocol (MCP)</a> server for the <a href="https://buttplug.io/" rel="nofollow">Buttplug.io ecosystem</a>.  It allows Tool-supporting LLM programs like <a href="https://claude.ai/download" rel="nofollow">Claude Desktop</a> query and control your Genital Interface Devices.</p>
<p dir="auto"><em>|insert AI-generated slop image of robots doing nasty things|</em>
<br><code>LLM|=&gt; - - (__(__)</code></p>
<p dir="auto">Once set up, you can prompt your LLM:</p>
<ul dir="auto">
<li>"What are my connected buttplug devices?"</li>
<li>"Set the second motor on my LELO F1S to 50% strength"</li>
<li>"How much battery is left on my Lovense Max 2?"</li>
<li>"Does my WeWibe have weak signal?"</li>
</ul>
<p dir="auto"><strong>NOTE: The above is aspirational and really the <a href="#current-state">current experience</a> is unstable and frustating.</strong></p>
<p dir="auto">It supports the following Resources and Tools:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Resource</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/devices</code></td>
<td>List of connected Buttplug devices in JSON.</td>
</tr>
<tr>
<td><code>/device/{id}</code></td>
<td>Device information by device ID where<code>id</code> is a number from <code>/devices</code></td>
</tr>
<tr>
<td><code>/device/{id}/rssi</code></td>
<td>RSSI signal level by device ID where <code>id</code> is a number from <code>/devices</code></td>
</tr>
<tr>
<td><code>/device/{id}/battery</code></td>
<td>Battery level by device ID where <code>id</code> is a number from <code>/devices</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Tool</th>
<th>Params</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>device_vibrate</code></td>
<td><code>id</code>, <code>motor</code>, <code>strength</code></td>
<td>Vibrates device by <code>id</code>, selecting <code>strength</code> and optional <code>motor</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<details>
<summary>JSON Schema for Resources.  Click to expand</summary>
<p dir="auto"><a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/schema_resources.json"><code>schema_resources.json</code></a></p>
<div data-snippet-clipboard-copy-content="{
  &quot;jsonrpc&quot;: &quot;2.0&quot;,
  &quot;id&quot;: 1,
  &quot;result&quot;: {
    &quot;resources&quot;: [
      {
        &quot;uri&quot;: &quot;devices&quot;,
        &quot;name&quot;: &quot;Device List&quot;,
        &quot;description&quot;: &quot;List of connected Buttplug devices in JSON&quot;,
        &quot;mimeType&quot;: &quot;application/json&quot;
      }
    ]
  }
}"><pre><code>{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "resources": [
      {
        "uri": "devices",
        "name": "Device List",
        "description": "List of connected Buttplug devices in JSON",
        "mimeType": "application/json"
      }
    ]
  }
}
</code></pre></div>
</details>
<details>
<summary>JSON Schema for Tools.  Click to expand</summary>
<p dir="auto"><a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/schema_tools.json"><code>schema_tools.json</code></a></p>
<div data-snippet-clipboard-copy-content="{
  &quot;jsonrpc&quot;: &quot;2.0&quot;,
  &quot;id&quot;: 1,
  &quot;result&quot;: {
    &quot;tools&quot;: [
      {
        &quot;description&quot;: &quot;Vibrates device by `id`, selecting `strength` and optional `motor`&quot;,
        &quot;inputSchema&quot;: {
          &quot;type&quot;: &quot;object&quot;,
          &quot;properties&quot;: {
            &quot;id&quot;: {
              &quot;description&quot;: &quot;Device ID to query, sourced from `/devices`&quot;,
              &quot;pattern&quot;: &quot;^[0-9]*$&quot;,
              &quot;type&quot;: &quot;number&quot;
            },
            &quot;motor&quot;: {
              &quot;description&quot;: &quot;Motor number to vibrate, defaults to 0&quot;,
              &quot;pattern&quot;: &quot;^[0-9]*$&quot;,
              &quot;type&quot;: &quot;number&quot;
            },
            &quot;strength&quot;: {
              &quot;description&quot;: &quot;Strength from 0.0 to 1.0, with 0.0 being off and 1.0 being full&quot;,
              &quot;pattern&quot;: &quot;^(0(\\.\\d+)?|1(\\.0+)?)$&quot;,
              &quot;type&quot;: &quot;number&quot;
            }
          },
          &quot;required&quot;: [
            &quot;id&quot;,
            &quot;strength&quot;
          ]
        },
        &quot;name&quot;: &quot;device_vibrate&quot;
      }
    ]
  }
}"><pre><code>{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "tools": [
      {
        "description": "Vibrates device by `id`, selecting `strength` and optional `motor`",
        "inputSchema": {
          "type": "object",
          "properties": {
            "id": {
              "description": "Device ID to query, sourced from `/devices`",
              "pattern": "^[0-9]*$",
              "type": "number"
            },
            "motor": {
              "description": "Motor number to vibrate, defaults to 0",
              "pattern": "^[0-9]*$",
              "type": "number"
            },
            "strength": {
              "description": "Strength from 0.0 to 1.0, with 0.0 being off and 1.0 being full",
              "pattern": "^(0(\\.\\d+)?|1(\\.0+)?)$",
              "type": "number"
            }
          },
          "required": [
            "id",
            "strength"
          ]
        },
        "name": "device_vibrate"
      }
    ]
  }
}
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current State</h2><a id="user-content-current-state" aria-label="Permalink: Current State" href="#current-state"></a></p>
<p dir="auto">I started working on this on 2025-04-01, April Fool's Day, after having created another experimental MCP service, <a href="https://github.com/NimbleMarkets/dbn-go/blob/main/cmd/dbn-go-mcp/README.md"><code>dbn-go</code> for financial market data</a>, the day prior.  So it is fresh meat and was intended as a quick, fun educational project.</p>
<p dir="auto">While it does work, I found the underlying <a href="https://github.com/diamondburned/go-buttplug"><code>go-buttplug</code> library</a> to be unstable in connection handling.   I could ask Claude for my devices, but my specific device wouldn't vibrate even just with just Intiface Central -- it was like in read-only mode!    I also wish I had a virtual buttplug.io device for testing, rather than relying on a physical device.</p>
<p dir="auto">So, it has not truly been tested "end-to-end" 😉</p>
<p dir="auto">I will dig more into the <code>go-buttplug</code> library and see why connections are unstable.  I also need to understand the MCP protocol current state of MCP hosts -- it seems they focus on Tools rather than Resources and Resoure Templates.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing the binary</h2><a id="user-content-installing-the-binary" aria-label="Permalink: Installing the binary" href="#installing-the-binary"></a></p>
<p dir="auto">Binaries for multiple platforms are <a href="https://github.com/conacademy/buttplug-mcp/releases">released on GitHub</a> through <a href="https://github.com/conacademy/buttplug-mcp/actions">GitHub Actions</a>.</p>
<p dir="auto">You can also install for various platforms with <a href="https://brew.sh/" rel="nofollow">Homebrew</a> from <a href="https://github.com/conacademy/homebrew-tap"><code>conacademy/homebrew-tap</code></a>:</p>
<div data-snippet-clipboard-copy-content="brew tap conacademy/homebrew-tap
brew install conacademy/tap/buttplug-mcp"><pre><code>brew tap conacademy/homebrew-tap
brew install conacademy/tap/buttplug-mcp
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Download the <a href="https://intiface.com/central/" rel="nofollow">Intiface Central</a> hub application to manage your devices.  Start it and note the server port (default seems to be <code>12345</code>).</p>
<p dir="auto">To use this the <code>buttplug-mcp</code> MCP server, you must configure your host program to use it.  We will illustrate with <a href="https://claude.ai/download" rel="nofollow">Claude Desktop</a>.  We must find the <code>buttplug-mcp</code> program on our system; the example below shows where <code>buttplug-mcp</code> is installed with MacOS Homebrew (perhaps build your own and point at that).</p>
<p dir="auto">The following <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/claude_desktop_config.json">configuration JSON</a> sets this up:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;mcpServers&quot;: {
    &quot;buttplug&quot;: {
      &quot;command&quot;: &quot;/opt/homebrew/bin/buttplug-mcp&quot;,
      &quot;args&quot;: [
        &quot;--ws-port&quot;, &quot;12345&quot;
      ]
    }
  }
}"><pre>{
  <span>"mcpServers"</span>: {
    <span>"buttplug"</span>: {
      <span>"command"</span>: <span><span>"</span>/opt/homebrew/bin/buttplug-mcp<span>"</span></span>,
      <span>"args"</span>: [
        <span><span>"</span>--ws-port<span>"</span></span>, <span><span>"</span>12345<span>"</span></span>
      ]
    }
  }
}</pre></div>
<p dir="auto">Using Claude Desktop, you can follow <a href="https://modelcontextprotocol.io/quickstart/user" rel="nofollow">their configuration tutorial</a> but substitute the configuration above.  With that in place, you can ask Claude question and it will use the <code>buttplug-mcp</code> server.  Here's example conversations:</p>
<p dir="auto">Perhaps you can use the <a href="https://www.home-assistant.io/integrations/mcp_server/" rel="nofollow">HomeAssistant MCP</a> integration to turn the lights down low...</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Ollama and <code>mcphost</code></h3><a id="user-content-ollama-and-mcphost" aria-label="Permalink: Ollama and mcphost" href="#ollama-and-mcphost"></a></p>
<p dir="auto">For local inferencing, there are MCP hosts that support <a href="https://ollama.com/download" rel="nofollow">Ollama</a>.  You can use any <a href="https://ollama.com/search?c=tools" rel="nofollow">Ollama LLM that supports "Tools"</a>.  We experimented with <a href="https://github.com/mark3labs/mcphost"><code>mcphost</code></a>, authored by the developer of the <a href="https://github.com/mark3labs/mcp-go"><code>mcp-go</code> library</a> that peformed the heavy lifting for us.</p>
<p dir="auto">Here's how to install and run with it with the configuration above, stored in <code>mcp.json</code>:</p>
<div data-snippet-clipboard-copy-content="$ go install github.com/mark3labs/mcphost@latest
$ mcphost -m ollama:llama3.3 --config mcp.json
...chat away..."><pre><code>$ go install github.com/mark3labs/mcphost@latest
$ mcphost -m ollama:llama3.3 --config mcp.json
...chat away...
</code></pre></div>
<p dir="auto">It seems that only "Tools" are supported and not "Resources", so I couldn't enumerate and introspect my device.   But I had this Tool interaction (but as noted <a href="#current-state">above</a>, my device didn't actually vibrate):</p>
<div data-snippet-clipboard-copy-content="$ mcphost -m ollama:phi4-mini --config mcp.json
2025/04/02 09:25:05 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:25:05 INFO Initializing server... name=buttplug
2025/04/02 09:25:05 INFO Server connected name=buttplug
2025/04/02 09:25:05 INFO Tools loaded server=buttplug count=1
2025/04/02 09:28:31 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:28:31 INFO Initializing server... name=buttplug
2025/04/02 09:28:31 INFO Server connected name=buttplug
2025/04/02 09:28:31 INFO Tools loaded server=buttplug count=1
/servers
      # buttplug
      Command /opt/homebrew/bin/buttplug-mcp
      Arguments --ws-port 12345

/tools
  • buttplug
    • device_vibrate
      • Vibrates device by ID, selecting strength and optional motor

  You: buttplug device_vibrate id 0 at strength 1

  Assistant:
  <|tool_call|>[start_processing]

  [{&quot;type&quot;:&quot;function&quot;,&quot;function&quot;:{&quot;name&quot;:&quot;buttplug__device_vibrate&quot;,&quot;description&quot;:&quot;Vibrates device by ID, selecting strength and optional
  motor&quot;,&quot;parameters&quot;:{&quot;id&quot;:0,&quot;strength&quot;:1}}]

  {}

  {&quot;status&quot;:&quot;success&quot;,&quot;message&quot;:&quot;Device with id 0 is vibrating at full strength.&quot;}"><pre><code>$ mcphost -m ollama:phi4-mini --config mcp.json
2025/04/02 09:25:05 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:25:05 INFO Initializing server... name=buttplug
2025/04/02 09:25:05 INFO Server connected name=buttplug
2025/04/02 09:25:05 INFO Tools loaded server=buttplug count=1
2025/04/02 09:28:31 INFO Model loaded provider=ollama model=phi4-mini
2025/04/02 09:28:31 INFO Initializing server... name=buttplug
2025/04/02 09:28:31 INFO Server connected name=buttplug
2025/04/02 09:28:31 INFO Tools loaded server=buttplug count=1
/servers
      # buttplug
      Command /opt/homebrew/bin/buttplug-mcp
      Arguments --ws-port 12345

/tools
  • buttplug
    • device_vibrate
      • Vibrates device by ID, selecting strength and optional motor

  You: buttplug device_vibrate id 0 at strength 1

  Assistant:
  &lt;|tool_call|&gt;[start_processing]

  [{"type":"function","function":{"name":"buttplug__device_vibrate","description":"Vibrates device by ID, selecting strength and optional
  motor","parameters":{"id":0,"strength":1}}]

  {}

  {"status":"success","message":"Device with id 0 is vibrating at full strength."}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Building is performed with <a href="https://taskfile.dev/" rel="nofollow">task</a>, with the binary available in <code>bin/buttplug-mcp</code>.</p>
<div data-snippet-clipboard-copy-content="$ task
task: [tidy] go mod tidy
task: [build] go build -o bin/buttplug-mcp cmd/buttplug-mcp/main.go"><pre><code>$ task
task: [tidy] go mod tidy
task: [build] go build -o bin/buttplug-mcp cmd/buttplug-mcp/main.go
</code></pre></div>
<p dir="auto">Useful testing tools:</p>
<ul dir="auto">
<li><code>task stdio-schema | jq</code> -- prints out JSON schemas</li>
<li><code>npx @modelcontextprotocol/inspector node build/index.js</code> -- <a href="https://github.com/modelcontextprotocol/inspector">MCP Inspector Web GUI</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">CLI Arguments</h2><a id="user-content-cli-arguments" aria-label="Permalink: CLI Arguments" href="#cli-arguments"></a></p>
<div data-snippet-clipboard-copy-content="R buttplug-mcp --help
usage: buttplug-mcp [opts]

  -h, --help              Show help
  -l, --log-file string   Log file destination (or MCP_LOG_FILE envvar). Default is stderr
  -j, --log-json          Log in JSON (default is plaintext)
      --sse               Use SSE Transport (default is STDIO transport)
      --sse-host string   host:port to listen to SSE connections
  -v, --verbose           Verbose logging
      --ws-port int       port to connect to the Buttplug Websocket server"><pre><code>R buttplug-mcp --help
usage: buttplug-mcp [opts]

  -h, --help              Show help
  -l, --log-file string   Log file destination (or MCP_LOG_FILE envvar). Default is stderr
  -j, --log-json          Log in JSON (default is plaintext)
      --sse               Use SSE Transport (default is STDIO transport)
      --sse-host string   host:port to listen to SSE connections
  -v, --verbose           Verbose logging
      --ws-port int       port to connect to the Buttplug Websocket server
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribution and Conduct</h2><a id="user-content-contribution-and-conduct" aria-label="Permalink: Contribution and Conduct" href="#contribution-and-conduct"></a></p>
<p dir="auto">As with all ConAcademy projects, pull requests are welcome.  Or fork it.  You do you.</p>
<p dir="auto">Either way, obey our <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>.  Be shady, but don't be a jerk.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits and License</h2><a id="user-content-credits-and-license" aria-label="Permalink: Credits and License" href="#credits-and-license"></a></p>
<p dir="auto">Thanks for <code>go-buttplug</code> for the <a href="https://github.com/diamondburned/go-buttplug">Golang Buttplug.io library</a> and its <a href="https://github.com/diamondburned/go-buttplug/tree/plug/cmd/buttplughttp"><code>buttplughttp</code> example</a>, and <code>go-mcp</code> for the <a href="https://github.com/mark3labs/mcp-go">Golang Model Context Protocol library</a>.</p>
<p dir="auto">Copyright (c) 2025 Neomantra BV.  Authored by Evan Wies for <a href="https://github.com/conacademy">ConAcademy</a>.</p>
<p dir="auto">Released under the <a href="https://en.wikipedia.org/wiki/MIT_License" rel="nofollow">MIT License</a>, see <a href="https://github.com/ConAcademy/buttplug-mcp/blob/main/LICENSE.txt">LICENSE.txt</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[White House releases health report written by LLM, with hallucinated citations (184 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/29/well/maha-report-citations.html</link>
            <guid>44132873</guid>
            <pubDate>Fri, 30 May 2025 04:31:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/29/well/maha-report-citations.html">https://www.nytimes.com/2025/05/29/well/maha-report-citations.html</a>, See on <a href="https://news.ycombinator.com/item?id=44132873">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/29/well/maha-report-citations.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: MCP Server SDK in Bash (126 pts)]]></title>
            <link>https://github.com/muthuishere/mcp-server-bash-sdk</link>
            <guid>44132823</guid>
            <pubDate>Fri, 30 May 2025 04:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/muthuishere/mcp-server-bash-sdk">https://github.com/muthuishere/mcp-server-bash-sdk</a>, See on <a href="https://news.ycombinator.com/item?id=44132823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">🐚 MCP Server in Bash</h2><a id="user-content--mcp-server-in-bash" aria-label="Permalink: 🐚 MCP Server in Bash" href="#-mcp-server-in-bash"></a></p>
<p dir="auto">A lightweight, zero-overhead implementation of the <a href="https://modelcontextprotocol.io/" rel="nofollow">Model Context Protocol (MCP)</a> server in pure Bash.</p>
<p dir="auto"><strong>Why?</strong> Most MCP servers are just API wrappers with schema conversion. This implementation provides a zero-overhead alternative to Node.js, Python, or other heavy runtimes.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📋 Features</h2><a id="user-content--features" aria-label="Permalink: 📋 Features" href="#-features"></a></p>
<ul dir="auto">
<li>✅ Full JSON-RPC 2.0 protocol over stdio</li>
<li>✅ Complete MCP protocol implementation</li>
<li>✅ Dynamic tool discovery via function naming convention</li>
<li>✅ External configuration via JSON files</li>
<li>✅ Easy to extend with custom tools</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Requirements</h2><a id="user-content--requirements" aria-label="Permalink: 🔧 Requirements" href="#-requirements"></a></p>
<ul dir="auto">
<li>Bash shell</li>
<li><code>jq</code> for JSON processing (<code>brew install jq</code> on macOS)</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<ol dir="auto">
<li><strong>Clone the repo</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/muthuishere/mcp-server-bash-sdk
cd mcp-server-bash-sdk"><pre>git clone https://github.com/muthuishere/mcp-server-bash-sdk
<span>cd</span> mcp-server-bash-sdk</pre></div>
<ol start="2" dir="auto">
<li><strong>Make scripts executable</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x mcpserver_core.sh moviemcpserver.sh"><pre>chmod +x mcpserver_core.sh moviemcpserver.sh</pre></div>
<ol start="3" dir="auto">
<li><strong>Try it out</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="echo '{&quot;jsonrpc&quot;: &quot;2.0&quot;, &quot;method&quot;: &quot;tools/call&quot;, &quot;params&quot;: {&quot;name&quot;: &quot;get_movies&quot;}, &quot;id&quot;: 1}' | ./moviemcpserver.sh"><pre><span>echo</span> <span><span>'</span>{"jsonrpc": "2.0", "method": "tools/call", "params": {"name": "get_movies"}, "id": 1}<span>'</span></span> <span>|</span> ./moviemcpserver.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏗️ Architecture</h2><a id="user-content-️-architecture" aria-label="Permalink: 🏗️ Architecture" href="#️-architecture"></a></p>
<div data-snippet-clipboard-copy-content="┌─────────────┐         ┌────────────────────────┐
│ MCP Host    │         │ MCP Server             │
│ (AI System) │◄──────► │ (moviemcpserver.sh)    │
└─────────────┘ stdio   └────────────────────────┘
                             │
                     ┌───────┴──────────┐
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Protocol Layer  │  │ Business Logic│
              │(mcpserver_core.sh)│  │(tool_* funcs)│
              └─────────────────┘  └───────────────┘
                     │                  │
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Configuration   │  │ External      │
              │ (JSON Files)    │  │ Services/APIs │
              └─────────────────┘  └───────────────┘"><pre><code>┌─────────────┐         ┌────────────────────────┐
│ MCP Host    │         │ MCP Server             │
│ (AI System) │◄──────► │ (moviemcpserver.sh)    │
└─────────────┘ stdio   └────────────────────────┘
                             │
                     ┌───────┴──────────┐
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Protocol Layer  │  │ Business Logic│
              │(mcpserver_core.sh)│  │(tool_* funcs)│
              └─────────────────┘  └───────────────┘
                     │                  │
                     ▼                  ▼
              ┌─────────────────┐  ┌───────────────┐
              │ Configuration   │  │ External      │
              │ (JSON Files)    │  │ Services/APIs │
              └─────────────────┘  └───────────────┘
</code></pre></div>
<ul dir="auto">
<li><strong>mcpserver_core.sh</strong>: Handles JSON-RPC and MCP protocol</li>
<li><strong>moviemcpserver.sh</strong>: Contains business logic functions</li>
<li><strong>assets/</strong>: JSON configuration files</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔌 Creating Your Own MCP Server</h2><a id="user-content--creating-your-own-mcp-server" aria-label="Permalink: 🔌 Creating Your Own MCP Server" href="#-creating-your-own-mcp-server"></a></p>
<ol dir="auto">
<li><strong>Create your business logic file (e.g., <code>weatherserver.sh</code>)</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="#!/bin/bash
# Weather API implementation

# Source the core MCP server
source &quot;$(dirname &quot;${BASH_SOURCE[0]}&quot;)/mcpserver_core.sh&quot;

# Access environment variables
API_KEY=&quot;${MCP_API_KEY:-default_key}&quot;

# Weather tool implementation
tool_get_weather() {
  local args=&quot;$1&quot;
  local location=$(echo &quot;$args&quot; | jq -r '.location')
  
  # Call external API
  local weather=$(curl -s &quot;https://api.example.com/weather?location=$location&amp;apikey=$API_KEY&quot;)
  echo &quot;$weather&quot;
  return 0
}

# Forecast tool implementation
tool_get_forecast() {
  local args=&quot;$1&quot;
  local location=$(echo &quot;$args&quot; | jq -r '.location')
  local days=$(echo &quot;$args&quot; | jq -r '.days')
  
  local forecast=$(curl -s &quot;https://api.example.com/forecast?location=$location&amp;days=$days&amp;apikey=$API_KEY&quot;)
  echo &quot;$forecast&quot;
  return 0
}

# Start the MCP server
run_mcp_server &quot;$@&quot;"><pre><span><span>#!</span>/bin/bash</span>
<span><span>#</span> Weather API implementation</span>

<span><span>#</span> Source the core MCP server</span>
<span>source</span> <span><span>"</span><span><span>$(</span>dirname <span><span>"</span><span>${BASH_SOURCE[0]}</span><span>"</span></span><span>)</span></span>/mcpserver_core.sh<span>"</span></span>

<span><span>#</span> Access environment variables</span>
API_KEY=<span><span>"</span><span>${MCP_API_KEY<span>:-</span>default_key}</span><span>"</span></span>

<span><span>#</span> Weather tool implementation</span>
<span>tool_get_weather</span>() {
  <span>local</span> args=<span><span>"</span><span>$1</span><span>"</span></span>
  <span>local</span> location=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.location<span>'</span></span><span>)</span></span>
  
  <span><span>#</span> Call external API</span>
  <span>local</span> weather=<span><span>$(</span>curl -s <span><span>"</span>https://api.example.com/weather?location=<span>$location</span>&amp;apikey=<span>$API_KEY</span><span>"</span></span><span>)</span></span>
  <span>echo</span> <span><span>"</span><span>$weather</span><span>"</span></span>
  <span>return</span> 0
}

<span><span>#</span> Forecast tool implementation</span>
<span>tool_get_forecast</span>() {
  <span>local</span> args=<span><span>"</span><span>$1</span><span>"</span></span>
  <span>local</span> location=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.location<span>'</span></span><span>)</span></span>
  <span>local</span> days=<span><span>$(</span>echo <span><span>"</span><span>$args</span><span>"</span></span> <span>|</span> jq -r <span><span>'</span>.days<span>'</span></span><span>)</span></span>
  
  <span>local</span> forecast=<span><span>$(</span>curl -s <span><span>"</span>https://api.example.com/forecast?location=<span>$location</span>&amp;days=<span>$days</span>&amp;apikey=<span>$API_KEY</span><span>"</span></span><span>)</span></span>
  <span>echo</span> <span><span>"</span><span>$forecast</span><span>"</span></span>
  <span>return</span> 0
}

<span><span>#</span> Start the MCP server</span>
run_mcp_server <span><span>"</span><span>$@</span><span>"</span></span></pre></div>
<ol start="2" dir="auto">
<li><strong>Create <code>tools_list.json</code> in the assets directory</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;tools&quot;: [
    {
      &quot;name&quot;: &quot;get_weather&quot;,
      &quot;description&quot;: &quot;Get current weather for a location&quot;,
      &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
          &quot;location&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;City name or coordinates&quot;
          }
        },
        &quot;required&quot;: [&quot;location&quot;]
      }
    },
    {
      &quot;name&quot;: &quot;get_forecast&quot;,
      &quot;description&quot;: &quot;Get weather forecast for multiple days&quot;,
      &quot;parameters&quot;: {
        &quot;type&quot;: &quot;object&quot;,
        &quot;properties&quot;: {
          &quot;location&quot;: {
            &quot;type&quot;: &quot;string&quot;,
            &quot;description&quot;: &quot;City name or coordinates&quot;
          },
          &quot;days&quot;: {
            &quot;type&quot;: &quot;integer&quot;,
            &quot;description&quot;: &quot;Number of days to forecast&quot;
          }
        },
        &quot;required&quot;: [&quot;location&quot;, &quot;days&quot;]
      }
    }
  ]
}"><pre>{
  <span>"tools"</span>: [
    {
      <span>"name"</span>: <span><span>"</span>get_weather<span>"</span></span>,
      <span>"description"</span>: <span><span>"</span>Get current weather for a location<span>"</span></span>,
      <span>"parameters"</span>: {
        <span>"type"</span>: <span><span>"</span>object<span>"</span></span>,
        <span>"properties"</span>: {
          <span>"location"</span>: {
            <span>"type"</span>: <span><span>"</span>string<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>City name or coordinates<span>"</span></span>
          }
        },
        <span>"required"</span>: [<span><span>"</span>location<span>"</span></span>]
      }
    },
    {
      <span>"name"</span>: <span><span>"</span>get_forecast<span>"</span></span>,
      <span>"description"</span>: <span><span>"</span>Get weather forecast for multiple days<span>"</span></span>,
      <span>"parameters"</span>: {
        <span>"type"</span>: <span><span>"</span>object<span>"</span></span>,
        <span>"properties"</span>: {
          <span>"location"</span>: {
            <span>"type"</span>: <span><span>"</span>string<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>City name or coordinates<span>"</span></span>
          },
          <span>"days"</span>: {
            <span>"type"</span>: <span><span>"</span>integer<span>"</span></span>,
            <span>"description"</span>: <span><span>"</span>Number of days to forecast<span>"</span></span>
          }
        },
        <span>"required"</span>: [<span><span>"</span>location<span>"</span></span>, <span><span>"</span>days<span>"</span></span>]
      }
    }
  ]
}</pre></div>
<ol start="3" dir="auto">
<li><strong>Update <code>mcpserverconfig.json</code></strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;protocolVersion&quot;: &quot;0.1.0&quot;,
  &quot;serverInfo&quot;: {
    &quot;name&quot;: &quot;WeatherServer&quot;,
    &quot;version&quot;: &quot;1.0.0&quot;
  },
  &quot;capabilities&quot;: {
    &quot;tools&quot;: {
      &quot;listChanged&quot;: true
    }
  },
  &quot;instructions&quot;: &quot;This server provides weather information and forecasts.&quot;
}"><pre>{
  <span>"protocolVersion"</span>: <span><span>"</span>0.1.0<span>"</span></span>,
  <span>"serverInfo"</span>: {
    <span>"name"</span>: <span><span>"</span>WeatherServer<span>"</span></span>,
    <span>"version"</span>: <span><span>"</span>1.0.0<span>"</span></span>
  },
  <span>"capabilities"</span>: {
    <span>"tools"</span>: {
      <span>"listChanged"</span>: <span>true</span>
    }
  },
  <span>"instructions"</span>: <span><span>"</span>This server provides weather information and forecasts.<span>"</span></span>
}</pre></div>
<ol start="4" dir="auto">
<li><strong>Make your file executable</strong></li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x weatherserver.sh"><pre>chmod +x weatherserver.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖥️ Using with VS Code &amp; GitHub Copilot</h2><a id="user-content-️-using-with-vs-code--github-copilot" aria-label="Permalink: 🖥️ Using with VS Code &amp; GitHub Copilot" href="#️-using-with-vs-code--github-copilot"></a></p>
<ol dir="auto">
<li><strong>Update VS Code settings.json</strong></li>
</ol>

<ol start="2" dir="auto">
<li><strong>Use with GitHub Copilot Chat</strong></li>
</ol>
<div data-snippet-clipboard-copy-content="/mcp my-weather-server get weather for New York"><pre><code>/mcp my-weather-server get weather for New York
</code></pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚫 Limitations</h2><a id="user-content--limitations" aria-label="Permalink: 🚫 Limitations" href="#-limitations"></a></p>
<ul dir="auto">
<li>No concurrency/parallel processing</li>
<li>Limited memory management</li>
<li>No streaming responses</li>
<li>Not designed for high throughput</li>
</ul>
<p dir="auto">For AI assistants and local tool execution, these aren't blocking issues.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/muthuishere/mcp-server-bash-sdk/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><strong>The complete code is available at: <a href="https://github.com/muthuishere/mcp-server-bash-sdk">https://github.com/muthuishere/mcp-server-bash-sdk</a></strong></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Triangle splatting: radiance fields represented by triangles (149 pts)]]></title>
            <link>https://trianglesplatting.github.io/</link>
            <guid>44132744</guid>
            <pubDate>Fri, 30 May 2025 04:07:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trianglesplatting.github.io/">https://trianglesplatting.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=44132744">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            Code will be released soon.
        </p><p>
            Triangle Splatting achieves high-quality novel view synthesis and fast rendering by representing scenes with triangles.
        In contrast, the inherent softness of Gaussian primitives often leads to blurring and a loss of fine details, for example, beneath the bench or at the room’s door, whereas Triangle Splatting preserves sharp edges and accurately captures fine details.        
        </p><div>
                    <h2>Abstract</h2>
                    <p>
                        The field of computer graphics was revolutionized by models such as Neural Radiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant representation for photogrammetry. In this paper, we argue for <span>a triangle come back.</span>. We develop a differentiable renderer that directly optimizes triangles via end-to-end gradients. We achieve this by rendering each triangle as differentiable splats, combining the efficiency of triangles with the adaptive density of representations based on independent primitives. Compared to popular 2D and 3D Gaussian Splatting methods, our approach achieves higher visual fidelity, faster convergence, and increased rendering throughput. On the Mip-NeRF360 dataset, our method outperforms concurrent non-volumetric primitives in visual fidelity and achieves higher perceptual quality than the state-of-the-art Zip-NeRF on indoor scenes. <br> Triangles are <span>simple</span>, <span>compatible</span> with standard graphics stacks and GPU hardware, and <span>highly efficient</span>: for the Garden scene, we achieve over 2,400 FPS at 1280×720 resolution using an off-the-shelf mesh renderer. These results highlight the efficiency and effectiveness of triangle-based representations for high-quality novel view synthesis. Triangles bring us closer to mesh-based optimization by combining classical computer graphics with modern differentiable rendering frameworks.
                    </p>
                </div><div>
            <h2>Methodology</h2>

            <p><img src="https://trianglesplatting.github.io/assets/function.png" alt="Window function visualization">
            </p>

            <p>
                Our rendering pipeline uses <strong>3D triangles</strong> as primitives, each defined by three learnable 3D vertices, color, opacity, and a smoothness parameter \( \sigma \). The triangles are projected onto the image plane using a standard pinhole camera model with known intrinsics and extrinsics.
            </p>

            <p>
                Instead of binary masks, we introduce a <strong>smooth window function</strong> that softly modulates the triangle's influence across pixels. This function is derived from the 2D <strong>signed distance field (SDF)</strong> of the triangle, which measures the distance from a pixel \( \mathbf{p} \) to the triangle’s edges.
            </p>

            <p>
                The signed distance field \( \phi(\mathbf{p}) \) is defined as the maximum of three half-plane distances:
            </p>

            <p>
                \( \phi(\mathbf{p}) = \max\left( L_1(\mathbf{p}),\; L_2(\mathbf{p}),\; L_3(\mathbf{p}) \right) \)
            </p>

            <p>
                where each half-space function is defined as:
            </p>

            <p>
                \( L_i(\mathbf{p}) = \mathbf{n}_i \cdot \mathbf{p} + d_i \)
            </p>

            <p>
                with \( \mathbf{n}_i \) denoting the outward-facing unit normal of the \( i \)-th edge, and \( d_i \) its signed offset from the origin.
            </p>

            <p>
                The final <strong>window function</strong> is:
            </p>

            <p>
                \( I(\mathbf{p}) = \text{ReLU}\left( \frac{\phi(\mathbf{p})}{\phi(\mathbf{s})} \right)^\sigma \)
            </p>

            <p>
                where \( \mathbf{s} \) is the triangle’s incenter, i.e., the point where \( \phi \) is minimized. This function satisfies:
            </p>

            <ul>
                <li><strong>Maximum opacity at the triangle incenter</strong></li>
                <li><strong>Zero influence at and beyond the triangle boundary</strong></li>
                <li><strong>Adjustable sharpness via the parameter \( \sigma \)</strong></li>
            </ul>

            <p>
                The figure above illustrates how the window function behaves in 1D and 2D. As \( \sigma \to 0 \), the function approximates a binary triangle mask. As \( \sigma \) increases, the transition becomes smoother, and in the limit \( \sigma \to \infty \), it becomes a delta function centered at \( \mathbf{s} \).
            </p>

            <p>
                To render an image, we accumulate contributions from all projected triangles using <strong>alpha blending</strong> in <strong>front-to-back depth order</strong>. Since all steps are differentiable, we can optimize the triangle parameters using gradient-based learning.
            </p>
        </div><div>
            <h2>More Visual Results</h2>
            

            <div>
                <div id="example12">
                                <video src="https://trianglesplatting.github.io/assets/video/flowers.mp4" autoplay="" loop="" muted="" playsinline=""></video>
                                <p>
                                    Triangle Splatting (Ours)
                                </p>
                            </div>
                <!-- Example 2 -->
                <div id="example11">
                                <video src="https://trianglesplatting.github.io/assets/video/bicycle_ts.mp4" autoplay="" loop="" muted="" playsinline=""></video>
                                <p>
                                    Triangle Splatting (Ours)
                                </p>
                            </div>
            </div>


            <p>
                Triangle Splatting produces sharper and more detailed images. Notably, it renders the flowers and the background with greater realism and captures finer details compared to 3DGS or 3DCS. (If the videos appear out of sync, please reload the page to ensure proper alignment.)
            </p>
    
        </div><div>
        <hr>
        <h2>Byproduct of the Triangle-Based Representation</h2>
        <div>
            <p>
                <video autoplay="" muted="" loop="" playsinline="">
                    <source src="https://trianglesplatting.github.io/assets/video/room_unity.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </p>
            <p>
                <video autoplay="" muted="" loop="" playsinline="">
                    <source src="https://trianglesplatting.github.io/assets/video/garden_unity.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </p>
        </div>
    </div><div><p>
            Triangle Splatting unifies differentiable scene optimization with traditional graphics pipelines. The triangle soup is compatible with any mesh-based
            renderer, enabling seamless integration into traditional graphics pipelines. 
            In a game engine, we render at 2400+ FPS at 1280×720 resolution on an RTX4090.

            </p><p>
            The current visuals are rendered without shaders and were not specifically trained or optimized for game engine fidelity, which accounts for the limited visual quality. 
            Nevertheless, it demonstrates an important first step toward the direct integration of radiance fields into interactive 3D environments. 
            Future work could explore training strategies specifically tailored to maximize visual fidelity in mesh-based renderers, paving the way for seamless integration of reconstructed scenes into standard game engines for real-time applications such as AR/VR or interactive simulations.
        </p></div><p>
            The triangles are well aligned with the underlying geometry. All triangles share a consistent orientation and lie flat on the surface.
        </p><div>
            <h2>Citation</h2>
                    <p><code>
                        @article{Held2025Triangle,<br>
                            title = {Triangle Splatting for Real-Time Radiance Field Rendering},<br>
                            author = {Held, Jan and Vandeghen, Renaud and Deliege, Adrien and Hamdi, Abdullah and Cioppa, Anthony and Giancola, Silvio and Vedaldi, Andrea and Ghanem, Bernard and Tagliasacchi, Andrea and Van Droogenbroeck, Marc},<br>
                            journal = {arXiv},<br>
                            year = {2025},<br>
                        }</code>
        </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The radix 2^51 trick (2017) (372 pts)]]></title>
            <link>https://www.chosenplaintext.ca/articles/radix-2-51-trick.html</link>
            <guid>44132673</guid>
            <pubDate>Fri, 30 May 2025 03:55:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chosenplaintext.ca/articles/radix-2-51-trick.html">https://www.chosenplaintext.ca/articles/radix-2-51-trick.html</a>, See on <a href="https://news.ycombinator.com/item?id=44132673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><em><strong>Faster addition and subtraction on modern CPUs</strong></em></p>

<p>Do you remember how to do long addition on paper?</p>

<pre><code> ¹¹ ¹
  6876
+ 3406
------
 10282
</code></pre>

<p>Starting from the “ones” position, we add 6 + 6 = 12, write down a 2 and carry a 1.
We proceed to the left, one position at a time, until there are no more digits
to add.</p>

<p>When implementing addition for large integers (e.g. 2<sup>64</sup> and above), it’s common to write
code that looks quite similar to this algorithm.
Interestingly, there’s a straightforward trick that can speed up this
process enormously on modern CPUs.</p>

<p>But first, a question: why do we start long addition with the “ones”?
Why not start on the left?</p>

<p>The answer, of course, is the carries.
We can’t figure out for sure what a given digit of the answer will be
until we’ve completed all of the additions to the right of that digit.</p>

<p>Imagine if we tried to add left-to-right instead:</p>

<blockquote>
  <p>6 + 3 = 9. So the first digit is 9.<br>
8 + 4 = 12. OK, the second digit is 2… but carry a 1, so the first digit
was actually 9 + 1 = 10… now carry back <em>that</em> 1…</p>
</blockquote>

<p>For mental math, this isn’t too bad (and some people actually prefer it
when working with small enough numbers).
As an algorithm, however, this approach has some fundamental limitations that
become clear when working with larger numbers.
Most importantly, because the later parts of the computation rely on
information from the earlier parts of the computation,
it’s hard to split up and parallelize the work.</p>

<h2 id="what-about-computers">What about computers?</h2>

<p>Computers don’t work in base 10, of course.
Instead, modern desktop and server CPUs expose an interface for operating on
(for the most part) 64-bit integers.</p>

<figure><pre><code data-lang="nasm"><span>; Add the 64-bit value in B to the 64-bit value in A</span>
<span>add</span> <span>A</span><span>,</span> <span>B</span>
<span>; Note: I'll use letters instead of real register names to keep things simple</span></code></pre></figure>

<p>As long as our numbers fit within a single 64-bit value, things are easy.
But what if we want to add, say, two 256-bit integers, <code>x</code> and <code>y</code>?</p>

<p>The obvious solution would be to break up each 256-bit number into four 64-bit
pieces (commonly referred to as “limbs”).
Place the highest 64 bits of <code>x</code> into register A,
the next 64 bits into register B,
and so on for registers C and D.
Do the same for <code>y</code> with registers E, F, G, H.</p>

<p>Now we can add <code>x</code> and <code>y</code> by adding the corresponding parts:</p>

<figure><pre><code data-lang="nasm"><span>; Equivalent to x += y</span>
<span>add</span> <span>A</span><span>,</span> <span>E</span>
<span>add</span> <span>B</span><span>,</span> <span>F</span>
<span>add</span> <span>C</span><span>,</span> <span>G</span>
<span>add</span> <span>D</span><span>,</span> <span>H</span></code></pre></figure>

<p>But wait, this might give us the wrong result!
If one of the last three additions overflow,
then we need to “carry” that extra 1 up to the next 64-bit piece.
Oh hey, does that sound familiar?</p>

<p>Fortunately, x86 has a dedicated instruction for this called “add with carry”.
<code>adc</code> will automatically check if the previous operation overflowed, adding 1
if needed.
Here’s how the proper code would look:</p>

<figure><pre><code data-lang="nasm"><span>add</span> <span>D</span><span>,</span> <span>H</span>
<span>adc</span> <span>C</span><span>,</span> <span>G</span> <span>; include carry from previous op</span>
<span>adc</span> <span>B</span><span>,</span> <span>F</span> <span>; include carry from previous op</span>
<span>adc</span> <span>A</span><span>,</span> <span>E</span> <span>; include carry from previous op</span></code></pre></figure>

<p>Just like with long addition in base 10,
we start with the least-significant “digits” (D and H)
and work our way up to the most-significant “digits” (A and E),
carrying 1s as needed along the way.</p>

<h2 id="but-now-its-slow">But now it’s slow</h2>

<p>Interestingly, our fixed code is slower than the original (incorrect) code.
Much slower.  Why is this?</p>

<p>The first reason is that <code>adc</code> is just slower to execute than a normal <code>add</code> on
most popular x86 CPUs.
Since <code>adc</code> has a third input (the carry flag),
it’s a more complex instruction than <code>add</code>.
It’s also used less often than <code>add</code>,
so there is less incentive for CPU designers to spend chip area on optimizing
<code>adc</code> performance.</p>

<p>The second reason is more interesting.
Let’s look at the Intel Haswell microarchitecture as an example.</p>

<p>On a Haswell CPU, a single <code>add</code> instruction takes 1 cycle to execute.
However, in ideal conditions, Haswell CPUs can execute up to 4 <code>add</code>
instructions in a single cycle.
How is this possible? Parallelism.
Modern processors look ahead at what instructions are coming up and try to
schedule them so that they can be executed in parallel whenever possible.
Since Haswell CPUs have 8 execution ports, and 4 of those ports can execute an
integer <code>add</code> instruction, a Haswell processor can execute up to 4 <code>add</code>
instructions at once.</p>

<p>In our original adding code, all 4 <code>add</code> instructions were independent of one
another, so it was straightforward for the processor to run them in parallel.
<strong>Now, with <code>adc</code>, each instruction depends on an output from the previous
instruction.</strong>
The processor has no choice but to execute the instructions serially, one after
the other, instead of in parallel.</p>

<p>The performance difference is even more dramatic if we use SIMD (Single
Instruction, Multiple Data) instructions.
For example, a single <code>vpaddq</code> (Vector Packed Add Quadword) instruction does
four 64-bit adds simultaneously.
Combine that with the fact that Haswell processors can execute two <code>vpaddq</code>s
per cycle, and you can see that we’re taking a serious performance hit
in order to handle carries properly.</p>

<h2 id="eliminating-carries-part-1-on-paper">Eliminating carries, part 1: on paper</h2>

<p>Back to base 10 for a minute.
How can we eliminate the need for carries?</p>

<p>Let’s make some changes to how the number system works.
First, we’ll extend the range of digits available.
Instead of 0-9, we will use 0-9, A-Z, and *:</p>

<pre><code>0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ*
</code></pre>

<p>(Yeah, I needed an extra character to make the numbers work out nicely. Bear
with me.)</p>

<p>Although we have 37 digits, we are <em>not</em> using base 37.
Numbers will still have “ones”, “tens”, and “hundreds” positions,
just like a normal base 10 system.
29 still means 29, and 29 + 1 is still 30.
The only difference is that
digits happen to be capable of counting past 9:
29 + 1 could also be written as 2A, 1K, or even U.</p>

<p>With this new, more flexible number system,
we can add without needing any carries!</p>

<pre><code>  672415
+ 736606
--------
  DA8A1B
</code></pre>

<p>This trick won’t work for all numbers in our number system (e.g. 9 + W will
need a carry),
but it will work if the numbers we are adding are
<em>normalized</em>, i.e. all of their digits are 9 or below.
In fact, we can add up to four normalized numbers in this notation before any
carries are possible:</p>

<pre><code>  999   &lt;-- largest possible normalized 3-digit number
  999
  999
+ 999
-----
  ***   &lt;-- valid 3-digit result, no carries
            (recall that * is the highest digit)
</code></pre>

<p>So, with some clever tweaks to the number system, we’ve cheated our way out of
some carries.
Of course, at some point, we will need to convert from the
37-digit base 10 system back to normal base 10.
We can do that by <em>normalizing</em> a number such that each of its digits is
between 0 and 9:</p>

<pre><code>  ¹¹ ¹ ¹
   DA8A1B
= 1409021

note:
D = 10 + 3
A = 10 + 0
B = 10 + 1
</code></pre>

<p>We normalize a number starting at the right,
determining how many “tens” are in each digit,
subtracting those “tens”,
and carrying them to the next digit.
672415 and 736606 do in fact sum to 1409021, so the system works!</p>

<p>The key insight here is that we can use this technique to delay carry
propagation until the end.
We can’t avoid carry propagation altogether, but we can avoid it temporarily.
If we save up the carries that occur during the intermediate additions,
we can propagate them all in one go at the end.</p>

<h2 id="eliminating-carries-part-2-computers">Eliminating carries, part 2: computers</h2>

<p>Carry propagation was at the heart of the performance problems we encountered earlier.
As you’ve probably anticipated by now, we can use this technique to help speed
up big number arithmetic!</p>

<p>Previously, we split a 256-bit number into four 64-bit pieces,
since x86_64 processors operate on 64-bit integers.
One way to understand this is to view the pieces as “digits”
in base 2<sup>64</sup>, since each digit has a value
between 0 and 2<sup>64</sup> - 1 (inclusive).</p>

<p>In base 10, we kept the same base, but extended the range of digits that were
allowed in order to prevent carries from occurring.
Unfortunately, we can’t do that here – a 64-bit integer only has so many
possible values, and we can’t change the hardware.
Instead, we can get the same effect by reducing the size of the base.</p>

<p>Instead of splitting 256 bits into four base 2<sup>64</sup> digits,
we’ll split 256 bits into five base 2<sup>51</sup> digits.
Each digit can still range from 0 to 2<sup>64</sup> - 1,
but the smaller base gives us the flexibility needed to prevent digits from
needing a carry.
This technique is generally referred to as “radix 2<sup>51</sup>
representation” in the cryptography literature.</p>

<p>Here’s how it will look when we split 256 bits across five limbs (i.e.
digits):</p>

<pre><code>|            [--------------------- 52 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
|             [-------------------- 51 bits --------------------]|
</code></pre>

<p>Each limb has 51 (or 52) bits of the original 256-bit number.
The remaining 12 or 13 bits give us the extra “digits” we need for preventing
carries.
Effectively, the highest bits of each limb are reserved as storage for any
carries that occur during the computation.</p>

<p>In our base 10 example,
37 digits allowed us to add up to four normalized numbers before needing to
propagate carries.
In radix 2<sup>51</sup> representation,
2<sup>64</sup> digits allow us to add up to 2<sup>13</sup> normalized
numbers before we need to worry about the high 13 bits overflowing.</p>

<p><em>Aside: Why 13 bits instead of 12?
For our purposes, we’re going to ignore the carries in the most significant limb,
allowing numbers to wrap when they overflow past 2<sup>256</sup> - 1 (just like
how unsigned addition works in C with normal size integer types).
As a result, we can assign 52 bits to the most significant limb and ignore the
fact that it will run out of room for carries before the other limbs do.</em></p>

<p>With this new representation, our addition code now looks like:</p>

<figure><pre><code data-lang="nasm"><span>; Assume x is split across A, B, C, D, E (A = most significant)</span>
<span>; and assume y is split across F, G, H, I, J (F = most significant)</span>
<span>add</span> <span>A</span><span>,</span> <span>F</span>
<span>add</span> <span>B</span><span>,</span> <span>G</span>
<span>add</span> <span>C</span><span>,</span> <span>H</span>
<span>add</span> <span>D</span><span>,</span> <span>I</span>
<span>add</span> <span>E</span><span>,</span> <span>J</span>
<span>; Parallel goodness, yay!</span></code></pre></figure>

<p>Despite the fact that we now need 5 <code>add</code>s instead of 4,
addition is much faster due to the lack of carries.</p>

<p>Of course, we also need code to normalize a number by propagating carries.</p>

<figure><pre><code data-lang="nasm"><span>; Assume x is split across A, B, C, D, E (A = most significant)</span>
<span>; Register T is for temporary storage</span>

<span>mov</span> <span>T</span><span>,</span> <span>E</span> <span>; Copy E into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>D</span><span>,</span> <span>T</span> <span>; Add carries from E into D</span>
<span>and</span> <span>E</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero out the carries in E</span>

<span>mov</span> <span>T</span><span>,</span> <span>D</span> <span>; Copy D into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>C</span><span>,</span> <span>T</span> <span>; Add carries from D into C</span>
<span>and</span> <span>D</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in D</span>

<span>mov</span> <span>T</span><span>,</span> <span>C</span> <span>; Copy C into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>B</span><span>,</span> <span>T</span> <span>; Add carries from C into B</span>
<span>and</span> <span>C</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in C</span>

<span>mov</span> <span>T</span><span>,</span> <span>B</span> <span>; Copy B into T</span>
<span>shr</span> <span>T</span><span>,</span> <span>51</span> <span>; Shift out everything except the carries</span>
<span>add</span> <span>A</span><span>,</span> <span>T</span> <span>; Add carries from B into A</span>
<span>and</span> <span>B</span><span>,</span> <span>0x0007FFFFFFFFFFFF</span> <span>; Zero the carries in B</span>

<span>and</span> <span>A</span><span>,</span> <span>0x000FFFFFFFFFFFFF</span> <span>; Zero the carries in A</span></code></pre></figure>

<p>Amazingly, some quick and dirty benchmarks show that
<strong>radix 2<sup>51</sup> addition already outperforms radix 2<sup>64</sup>
addition on my Haswell CPU for as few as three additions – and that’s
including the cost of converting to and from
radix 2<sup>51</sup> representation</strong>.
The performance savings scale up appropriately as the number of additions
increases.</p>

<h2 id="subtraction">Subtraction</h2>

<p>So far we’ve only looked at addition.
It’s straightforward though to extend this technique to subtraction.
The main difference between addition and subtraction is that subtraction has
<em>negative</em> carries.</p>

<p>Previously, we treated all limbs (and their carries) as unsigned
integers.
To support subtraction, we can treat limbs as <em>signed</em> integers,
allowing individual digits to be either positive or negative.
With this change, each limb can store either a positive or negative carry.</p>

<p>A side effect of this is that the most significant bit of each limb is now
reserved as a sign bit.
This lowers the number of operations we can perform between normalizations from
2<sup>13</sup> to 2<sup>12</sup> – a small sacrifice in most cases.</p>

<p>I find this technique rather fascinating because of how counterintuitive it is:
by spreading data across more registers and using more operations, performance
is actually improved.
I hope you found it as interesting as I did!</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. sanctions cloud provider 'Funnull' as top source of 'pig butchering' scams (160 pts)]]></title>
            <link>https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/</link>
            <guid>44132075</guid>
            <pubDate>Fri, 30 May 2025 01:58:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/">https://krebsonsecurity.com/2025/05/u-s-sanctions-cloud-provider-funnull-as-top-source-of-pig-butchering-scams/</a>, See on <a href="https://news.ycombinator.com/item?id=44132075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<div id="attachment_70230"><p><img aria-describedby="caption-attachment-70230" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss.png" alt="" width="750" height="452" srcset="https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss.png 1319w, https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss-768x463.png 768w, https://krebsonsecurity.com/wp-content/uploads/2025/01/funnell-ss-782x472.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-70230">Image: Shutterstock, ArtHead.</p></div>
<p>The U.S. government today imposed economic sanctions on <strong>Funnull Technology Inc.</strong>, a Philippines-based company that provides computer infrastructure for hundreds of thousands of websites involved in virtual currency investment scams known as “<strong>pig butchering</strong>.” In January 2025, KrebsOnSecurity detailed how Funnull was being used as a content delivery network that catered to cybercriminals seeking to route their traffic through U.S.-based cloud providers.</p>
<p>“Americans lose billions of dollars annually to these cyber scams, with revenues generated from these crimes rising to record levels in 2024,” reads <a href="https://home.treasury.gov/news/press-releases/sb0149" target="_blank" rel="noopener">a statement</a> from the <strong>U.S. Department of the Treasury</strong>, which sanctioned Funnull and its 40-year-old Chinese administrator <strong>Liu Lizhi</strong>. “Funnull has directly facilitated several of these schemes, resulting in over $200 million in U.S. victim-reported losses.”</p>
<p>The Treasury Department said Funnull’s operations are linked to the majority of virtual currency investment scam websites reported to the FBI. The agency said Funnull directly facilitated pig butchering and other schemes that resulted in more than $200 million in financial losses by Americans.</p>
<p>Pig butchering is a rampant form of fraud wherein people are lured by flirtatious strangers online into investing in fraudulent cryptocurrency trading platforms. Victims are coached to invest more and more money into what appears to be an extremely profitable trading platform, only to find their money is gone when they wish to cash out.</p>
<p>The scammers often insist that investors pay additional “taxes” on their crypto “earnings” before they can see their invested funds again (spoiler: they never do), and a shocking number of people <a href="https://krebsonsecurity.com/2022/07/massive-losses-define-epidemic-of-pig-butchering/" target="_blank" rel="noopener">have lost six figures or more through these pig butchering scams</a>.</p>
<p>KrebsOnSecurity’s <a href="https://krebsonsecurity.com/2025/01/infrastructure-laundering-blending-in-with-the-cloud/" target="_blank" rel="noopener">January story on Funnull</a> was based on research from the security firm <strong>Silent Push</strong>, which discovered in October 2024 that a vast number of domains hosted via Funnull were promoting gambling sites that bore the logo of the <strong>Suncity Group</strong>, a Chinese entity named in&nbsp;<a href="https://www.unodc.org/roseap/uploads/documents/Publications/2024/Casino_Underground_Banking_Report_2024.pdf" target="_blank" rel="noopener">a 2024 UN report</a> (PDF) for laundering millions of dollars for the North Korean state-sponsored hacking group <a href="https://en.wikipedia.org/wiki/Lazarus_Group" target="_blank" rel="noopener">Lazarus</a>.</p>
<p>Silent Push found Funnull was a criminal content delivery network (CDN) that carried a great deal of traffic tied to scam websites, funneling the traffic through a dizzying chain of auto-generated domain names and U.S.-based cloud providers before redirecting to malicious or phishous websites. The FBI has released a <a href="https://www.ic3.gov/CSA/2025/250529.pdf" target="_blank" rel="noopener">technical writeup</a> (PDF) of the infrastructure used to manage the malicious Funnull domains between October 2023 and April 2025.</p>
<div id="attachment_71392"><p><img aria-describedby="caption-attachment-71392" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network.png" alt="" width="749" height="464" srcset="https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network.png 2556w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-768x476.png 768w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-1536x952.png 1536w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-2048x1269.png 2048w, https://krebsonsecurity.com/wp-content/uploads/2025/05/funnull-network-782x485.png 782w" sizes="(max-width: 749px) 100vw, 749px"></p><p id="caption-attachment-71392">A graphic from the FBI explaining how Funnull generated a slew of new domains on a regular basis and mapped them to Internet addresses on U.S. cloud providers.</p></div>
<p>Silent Push <a href="https://www.silentpush.com/blog/infrastructure-laundering/" target="_blank" rel="noopener">revisited Funnull’s infrastructure</a> in January 2025 and found Funnull was still using many of the same <strong>Amazon</strong> and <strong>Microsoft</strong> cloud Internet addresses identified as malicious in its October report. Both Amazon and Microsoft pledged to rid their networks of Funnull’s presence following that story, but according to Silent Push’s <strong>Zach Edwards</strong> only one of those companies has followed through.</p>
<p>Edwards said Silent Push no longer sees Microsoft Internet addresses showing up in Funnull’s infrastructure, while Amazon continues to struggle with removing Funnull servers, including one that appears to have first materialized in 2023.</p>
<p>“Amazon is doing a terrible job — every day since they made those claims to you and us in our public blog they have had IPs still mapped to Funnull, including some that have stayed mapped for inexplicable periods of time,” Edwards said.</p>
<p>Amazon said its Amazon Web Services (AWS) hosting platform actively counters abuse attempts.</p>
<p>“We have stopped hundreds of attempts this year related to this group and we are looking into the information you shared earlier today,” reads a statement shared by Amazon. “If anyone suspects that AWS resources are being used for abusive activity, they can report it to AWS Trust &amp; Safety using the report abuse form <a href="https://support.aws.amazon.com/#/contacts/report-abuse" target="_blank" rel="noopener">here</a>.”</p>

<p>U.S. based cloud providers remain an attractive home base for cybercriminal organizations because many organizations will not be overly aggressive in blocking traffic from U.S.-based cloud networks, as doing so can result in blocking access to many legitimate web destinations that are also on that same shared network segment or host.</p>
<p>What’s more, funneling their bad traffic so that it appears to be coming out of U.S. cloud Internet providers allows cybercriminals to connect to websites from web addresses that are geographically close(r) to their targets and victims (to sidestep location-based security controls by your bank, for example).</p>
<p>Funnull is not the only cybercriminal infrastructure-as-a-service provider that was sanctioned this month: On May 20, 2025, the <strong>European Union</strong> <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L_202500966" target="_blank" rel="noopener">imposed sanctions</a> on <strong>Stark Industries Solutions</strong>, an ISP that materialized at the start of Russia’s invasion of Ukraine and has been used as a global proxy network that conceals the true source of cyberattacks and disinformation campaigns against enemies of Russia.</p>
<p>In May 2024, KrebsOnSecurity published <a href="https://krebsonsecurity.com/2024/05/stark-industries-solutions-an-iron-hammer-in-the-cloud/" target="_blank" rel="noopener">a deep dive on Stark Industries Solutions</a> that found much of the malicious traffic traversing Stark’s network (e.g. vulnerability scanning and password brute force attacks) was being bounced through U.S.-based cloud providers. My reporting showed how deeply Stark had penetrated U.S. ISPs, and that Ivan Neculiti for many years sold “bulletproof” hosting services that told Russian cybercrime forum customers they would proudly ignore any abuse complaints or police inquiries.</p>
<div id="attachment_67471"><p><img aria-describedby="caption-attachment-67471" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions.png" alt="" width="748" height="464" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions.png 1197w, https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions-768x477.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/05/stark-industries-solutions-782x485.png 782w" sizes="(max-width: 748px) 100vw, 748px"></p><p id="caption-attachment-67471">The homepage of Stark Industries Solutions.</p></div>
<p>That story examined the history of Stark’s co-founders, Moldovan brothers <strong>Ivan</strong> and <strong>Yuri Neculiti</strong>, who each denied past involvement in cybercrime or any current involvement in assisting Russian disinformation efforts or cyberattacks. Nevertheless, the EU sanctioned both brothers as well.</p>
<p>The EU said Stark and the Neculti brothers “enabled various Russian state-sponsored and state-affiliated actors to conduct destabilising activities including coordinated information manipulation and interference and cyber-attacks against the Union and third countries by providing services intended to hide these activities from European law enforcement and security agencies.”</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practical SDR: Getting started with software-defined radio (242 pts)]]></title>
            <link>https://nostarch.com/practical-sdr</link>
            <guid>44131984</guid>
            <pubDate>Fri, 30 May 2025 01:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nostarch.com/practical-sdr">https://nostarch.com/practical-sdr</a>, See on <a href="https://news.ycombinator.com/item?id=44131984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://nostarch.com/download/PracticalSDR_SampleChapter.pdf" target="_blank">Download Chapter 4: Creating an AM Receiver</a></p>
<p><strong>Look Inside!</strong></p>
<p><a href="https://nostarch.com/images/PracticalSDR_backcover.png"><img alt="Practical SDR back cover" src="https://nostarch.com/images/PracticalSDR_backcover.png" title="Practical SDR back cover"></a><br>
<a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p60-61.png"><img alt="Practical SDR pages 60-61" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p60-61.png" title="Practical SDR pages 60-61"></a><a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p158-159.png"><img alt="Practical SDR pages 158-159" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p158-159.png" title="Practical SDR pages 158-159"></a><a href="https://nostarch.com/images/PracticalSDR_marketingspreads_p222-223.png"><img alt="Practical SDR pages 222-223" src="https://nostarch.com/images/PracticalSDR_marketingspreads_p222-223.png" title="Practical SDR pages 222-223"></a></p>
<p><span><span><span>Whether you’re a hobbyist interested in exploring the airwaves, a student learning about wireless communications, or an engineer looking to prototype RF designs, <em>Practical SDR</em> will help you master the fundamentals of software-defined radio.</span></span></span></p>
<p><span><span><span>You’ll build virtual radio receivers on your computer, then extract audio from real AM and FM signals; learn how amplitude modulation works by building an AM radio; understand signal filtering by crafting clean FM reception; and grasp complex topics like IQ sampling. You’ll use the intuitive GNU Radio Companion interface to create working radio systems piece by piece, then move on to building functional AM and FM receivers, and even design your own radio transmitter.</span></span></span></p>
<p><span><span><span>Along the way, you’ll learn how to:<br>
•&nbsp;&nbsp; &nbsp;Manipulate radio frequencies from 1 MHz to 6 GHz&nbsp;<br>
•&nbsp;&nbsp; &nbsp;Use filters and gain control to extract clear signals from noise<br>
•&nbsp;&nbsp; &nbsp;Maximize your SDR’s performance by choosing the right antennas and RF hardware<br>
•&nbsp;&nbsp; &nbsp;Process complex, real-time IQ data to demodulate actual radio signals<br>
•&nbsp;&nbsp; &nbsp;Build a flexible, virtual radio testing environment on&nbsp;your computer</span></span></span></p>
<p><span><span><span>This isn’t just another theory book. <em>Practical SDR</em> bridges the gap between basic tutorials and advanced applications, providing a solid foundation for diving into modern wireless systems like Wi-Fi, Bluetooth, and cellular communications.</span></span></span></p>
<p><span><span><span>Some projects require SDR hardware, such as a HackRF One, and a compatible antenna.</span></span></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm starting a social club to solve the male loneliness epidemic (274 pts)]]></title>
            <link>https://wave3.social</link>
            <guid>44131513</guid>
            <pubDate>Thu, 29 May 2025 23:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wave3.social">https://wave3.social</a>, See on <a href="https://news.ycombinator.com/item?id=44131513">Hacker News</a></p>
Couldn't get https://wave3.social: Error: getaddrinfo ENOTFOUND wave3.social]]></description>
        </item>
    </channel>
</rss>