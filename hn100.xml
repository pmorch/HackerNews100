<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 25 Apr 2025 13:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Hegseth had an unsecured internet line set up in his office to connect to Signal (106 pts)]]></title>
            <link>https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d</link>
            <guid>43792157</guid>
            <pubDate>Fri, 25 Apr 2025 10:40:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d">https://apnews.com/article/hegseth-signal-chat-dirty-internet-line-6a64707f10ca553eb905e5a70e10bd9d</a>, See on <a href="https://news.ycombinator.com/item?id=43792157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        
<p>WASHINGTON (AP) — Defense Secretary Pete Hegseth had an internet connection that bypassed the Pentagon’s security protocols set up in his office to <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/hegseth-signal-chat-houthis-attack-8dbf9dd6c711796438a5c1c84831c40b">use the Signal messaging app</a></span> on a personal computer, two people familiar with the line told The Associated Press.</p><p>The existence of the unsecured internet connection is the latest revelation about <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/hegseth-leaks-signal-trump-classified-09f58fa650e44f740c9416c3e6997f5b">Hegseth’s use of the unclassified app</a></span> and raises the possibility that sensitive defense information could have been put at risk of potential hacking or surveillance.</p><p>Known as a “dirty” internet line by the IT industry, it connects directly to the public internet where the user’s information and the websites accessed do not have the same security filters or protocols that the Pentagon’s secured connections maintain. </p><p>Other Pentagon offices have used them, particularly if there’s a need to monitor information or websites that would otherwise be blocked.</p>
    
<p>But the biggest advantage of using such a line is that the user would not show up as one of the many IP addresses assigned to the Defense Department — essentially the user is masked, according to a senior U.S. official familiar with military network security. </p>



<p>But it also can expose users to hacking and surveillance. A “dirty” line — just like any public internet connection — also may lack the recordkeeping compliance required by federal law, the official said. </p><p>All three spoke on condition of anonymity to discuss a sensitive matter.</p>
    
<h2>A ‘dirty’ internet line to use Signal</h2><p>The two people familiar with the line said Hegseth had it set up in his office to <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/hegseth-signal-chat-pentagon-trump-30dc4c3d0e75a89f3fd883f38b26afff">use the Signal app</a></span>, which has become a flashpoint following revelations that he posted sensitive details about <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/war-plans-trump-hegseth-atlantic-230718a984911dd8663d59edbcb86f2a">a military airstrike in two chats</a></span> that each had more than a dozen people. One of the chats included his wife and brother, while the other included President Donald Trump’s top national security officials.</p><p>Asked about Hegseth’s use of Signal in his office, which was first reported by The Washington Post, chief Pentagon spokesman Sean Parnell said the defense secretary’s “use of communications systems and channels is classified.”</p><p>“However, we can confirm that the Secretary has never used and does not currently use Signal on his government computer,” Parnell said in a statement. </p><p>It’s the latest revelation to shake the Pentagon. Besides facing questions from both Democrats and Republicans about his handling of sensitive information, Hegseth has <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/ullyot-leaks-pentagon-hegseth-trump-dei-d5306e0441dacae1a0d03c871be265bb">dismissed or transferred multiple close advisers</a></span>, tightly narrowing his inner circle and adding to the turmoil following the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/trump-brown-joint-chiefs-of-staff-firing-fa428cc1508a583b3bf5e7a5a58f6acf">firings of several senior military officers</a></span> in recent months.</p><p>Trump and other administration officials have given Hegseth their full support. They have blamed employees they say were disgruntled for leaking information to journalists, with Trump saying this week: “It’s just fake news. They just bring up stories.”</p><p>“I have 100% confidence in the secretary,” Vice President JD Vance told reporters Wednesday about Hegseth. ”I know the president does and, really, the entire team does.”</p>
    
<h2>Secure ways to communicate at the Pentagon</h2><p>The Pentagon has a variety of secure ways that enable Hegseth and other military leaders to communicate: </p><p>— The Non-classified Internet Protocol Router Network can handle the lowest levels of sensitive information. It allows some access to the internet but is firewalled and has levels of cybersecurity that a “dirty” line does not. It cannot handle information labeled as secret.</p><p>— The Secure Internet Protocol Router Network is used for secret-level classified information. </p><p>— The Joint Worldwide Intelligence Communications System is for top-secret and secret compartmentalized information, which is some of the highest levels of secrecy, also known as TS/SCI. </p><p>Hegseth initially was going to the back area of his office where he could access Wi-Fi to use his devices, one of the people familiar said, and then he requested a line at his desk where he could use his own computer. </p><p>That meant at times there were three computers around his desk — a personal computer; another for classified information; and a third for sensitive defense information, both people said.</p><p>Because electronic devices are vulnerable to spying, no one is supposed to have them inside the defense secretary’s office. Important offices at the Pentagon have a cabinet or drawer where staff or visitors are required to leave devices. </p>
    
<h2>Fallout over Signal</h2><p><span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/signal-app-atlantic-war-plans-32699da142c5209b845e57f690df4925">Signal is a commercially available app</a></span> that is not authorized to be used for sensitive or classified information. It’s encrypted, but can be hacked.</p><p>While Signal offers more protections than standard text messaging, it’s no guarantee of security. Officials also must ensure their hardware and connections are secure, said Theresa Payton, White House chief information officer under President George W. Bush and now CEO of Fortalice Solutions, a cybersecurity firm.</p><p>The communications of senior government officials are of keen interest to adversaries like Russia or China, Payton said.</p>
    
<p>The National Security Agency issued a warning earlier this year about concerns that foreign hackers could try to target government officials using Signal. Google also advised caution about Russia-aligned hackers targeting Signal users. </p><p>Hegseth’s Signal use is <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/hegseth-signal-messaging-app-attack-plans-f8581bcb447b91d2e7f9cb7809ae0f06">under investigation</a></span> by the Defense Department’s acting inspector general at the request of the bipartisan leadership of the Senate Armed Services Committee. </p><p>Hegseth pulled the information about the strike on Yemen’s Houthi militants last month from a secure communications channel used by U.S. Central Command. He has vehemently denied he posted “war plans” or classified information. </p><p>But the information Hegseth did post in chats — exact launch times and bomb drop times — would have been classified and could have put service members at risk, multiple current and former military and defense officials have said. The airstrike information was sent before the pilots had launched or safely returned from their mission.</p><p>___</p><p>AP reporter David Klepper in Washington contributed to this report.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Avoiding Skill Atrophy in the Age of AI (117 pts)]]></title>
            <link>https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age</link>
            <guid>43791474</guid>
            <pubDate>Fri, 25 Apr 2025 08:30:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age">https://addyo.substack.com/p/avoiding-skill-atrophy-in-the-age</a>, See on <a href="https://news.ycombinator.com/item?id=43791474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>The rise of AI assistants in coding has sparked a paradox: we may be increasing productivity, but at risk of losing our edge to skill atrophy if we’re not careful. Skill atrophy refers to the decline or loss of skills over time due to lack of use or practice.</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2882796,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/162086801?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d567f4b-128f-4ad2-bf6e-d0038e374e00_1536x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><strong>Would you be completely stuck if AI wasn’t available?</strong></p><p><span>Every developer knows the appeal of offloading tedious tasks to machines. Why memorize docs or sift through tutorials when AI can serve up answers on demand? This </span><em>cognitive offloading</em><span> - relying on external tools to handle mental tasks - has plenty of precedents. Think of how GPS navigation </span><a href="https://www.cyberdemon.org/2023/03/29/age-of-ai-skill-atrophy.html#:~:text=I%20grew%20up%20in%20Los,a%20road%20navigator%20have%20atrophied" rel="">eroded</a><span> our knack for wayfinding: one engineer admits his road navigation skills “have atrophied” after years of blindly following Google Maps. Similarly, AI-powered autocomplete and code generators can tempt us to </span><strong>“turn off our brain”</strong><span> for routine coding tasks.</span></p><p><span>Offloading rote work isn’t inherently bad. In fact, many of us are experiencing a renaissance that lets us attempt projects we’d likely not tackle otherwise. As veteran developer Simon Willison </span><a href="https://simonwillison.net/2023/Mar/27/ai-enhanced-development/" rel="">quipped</a><span>, </span><em>“the thing I’m most excited about in our weird new AI-enhanced reality is the way it allows me to be more ambitious with my projects”</em><span>. With AI handling boilerplate and rapid prototyping, ideas that once took </span><em>days</em><span> now seem viable in an afternoon. The boost in speed and productivity is real - depending on what you’re trying to build. The danger lies in </span><strong>where to draw the line</strong><span> between healthy automation and harmful </span><em>atrophy</em><span> of core skills. </span></p><p><span>Recent research is sounding the alarm that our critical thinking and problem-solving muscles may be quietly deteriorating. A </span><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf" rel="">2025 study</a><span> by Microsoft and Carnegie Mellon researchers found that the more people leaned on AI tools, </span><strong>the less critical thinking they engaged in</strong><span>, making it harder to summon those skills when needed. </span></p><p><span>Essentially, high confidence in an AI’s abilities led people to take a mental backseat - “letting their hands off the wheel” - especially on easy tasks It’s human nature to relax when a task feels simple, but over time this </span><strong>“long-term reliance” can lead to “diminished independent problem-solving”</strong><span>. The study even noted that workers with AI assistance produced a </span><em>less diverse set of solutions</em><span> for the same problem, since AI tends to deliver homogenized answers based on its training data. In the researchers’ words, this uniformity could be seen as a </span><em>“deterioration of critical thinking”</em><span> itself. </span></p><p><strong>There are a few barriers to critical thinking:</strong></p><ul><li><p>Awareness barriers (over-reliance on AI, especially for routine tasks)</p></li><li><p>Motivation barriers (time pressure, job scope limitations)</p></li><li><p>Ability barriers (difficulty verifying or improving AI responses)</p></li></ul><p><span>What does this look like in day-to-day coding? It starts subtle. One engineer </span><a href="https://nmn.gl/blog/ai-illiterate-programmers?trk=public_post_comment-text#:~:text=I%20stared%20at%20my%20terminal,it%20out%20without%20AI%E2%80%99s%20help" rel="">confessed</a><span> that after 12 years of programming, AI’s instant help made him </span><em>“worse at [his] own craft”</em><span>. He describes a creeping decay: </span><strong>First, he stopped reading documentation</strong><span> – why bother when an LLM can explain it instantly? </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png" width="1024" height="1536" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1536,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3253510,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/162086801?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6f3f77c2-ebc2-42af-bfa9-833e7bbf025d_1024x1536.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Then </span><strong>debugging skills waned</strong><span> – stack traces and error messages felt daunting, so he just copy-pasted them into AI for a fix. “I’ve become a human clipboard” he laments, blindly shuttling errors to the AI and solutions back to code. Each error used to teach him something new; now the </span><em>solution appears magically and he learns nothing</em><span>. The dopamine rush of an instant answer replaced the satisfaction of hard-won understanding.</span></p><p><span>Over time, this cycle deepens. He notes that </span><strong>deep comprehension was the next to go</strong><span> – instead of spending hours truly understanding a problem, he now implements whatever the AI suggests. If it doesn’t work, he tweaks the prompt and asks again, entering a </span><em>“cycle of increasing dependency”</em><span>. Even the emotional circuitry of development changed: what used to be the joy of solving a tough bug is now frustration if the AI doesn’t cough up a solution in 5 minutes. </span></p><p><span>In short, by outsourcing the thinking to an LLM, he was trading away long-term mastery for short-term convenience. </span><em>“We’re not becoming 10× developers with AI – we’re becoming 10× dependent on AI”</em><span> he observes. </span><em>“Every time we let AI solve a problem we could’ve solved ourselves, we’re trading long-term understanding for short-term productivity”</em><span>.</span></p><p>It’s not just hypothetical - there are telltale signs that reliance on AI might be eroding your craftsmanship in software development:</p><ul><li><p><strong>Debugging despair:</strong><span> Are you skipping the debugger and going straight to AI for every exception? If reading a stacktrace or stepping through code feels </span><em>arduous</em><span> now, keep an eye on this skill. In the pre-AI days, wrestling with a bug was a learning crucible; now it’s tempting to offload that effort. One developer admitted he no longer even reads error messages fully - he just sends them to the AI. The result: when the AI isn’t available or stumped, he’s at a loss on how to diagnose issues the old-fashioned way.</span></p></li></ul><ul><li><p><strong>Blind Copy-Paste coding:</strong><span> It’s fine to have AI write boilerplate, but do you understand </span><em>why</em><span> the code it gave you works? If you find yourself pasting in code that you couldn’t implement or explain on your own, be careful. Young devs especially report shipping code faster than ever with AI, yet when asked </span><em>why</em><span> a certain solution is chosen or how it handles edge cases, they draw blanks. The foundational knowledge that comes from struggling through alternatives is just… </span><a href="https://nmn.gl/blog/ai-and-learning#:~:text=Crickets,Blank%20stares" rel="">missing</a><span>.</span></p></li></ul><ul><li><p><strong>Architecture and big-picture thinking:</strong><span> Complex system design can’t be solved by a single prompt. If you’ve grown accustomed to solving bite-sized problems with AI, you might notice a reluctance to tackle higher-level architectural planning without it. The AI can suggest design patterns or schemas, but it won’t grasp the full context of your unique system. Over-reliance might mean you haven’t practiced piecing components together mentally. For instance, you might accept an AI-suggested component without considering how it fits into the broader performance, security, or maintainability picture - something experienced engineers do via hard-earned intuition. If those system-level thinking muscles aren’t flexed, they can weaken.</span></p></li></ul><ul><li><p><strong>Diminished memory &amp; recall:</strong><span> Are basic API calls or language idioms slipping from your memory? It’s normal to forget rarely-used details, but if everyday syntax or concepts now escape you because the AI autocomplete always fills it in, you might be experiencing skill fade. You don’t want to become the equivalent of a calculator-dependent student who’s forgotten how to do arithmetic by hand.</span></p></li></ul><p>It’s worth noting that some skill loss over time is natural and sometimes acceptable. </p><p>We’ve all let go of obsolete skills (when’s the last time you manually managed memory in assembly, or did long division without a calculator?). Some argue that worrying about “skill atrophy” is just resisting progress - after all, we gladly let old-timers’ skills like handwritten letter writing or map-reading fade to make room for new ones. </p><p><span>The key is distinguishing </span><em>which</em><span> skills are safe to offload and </span><em>which are essential to keep sharp</em><span>. Losing the knack for manual memory management is one thing; losing the ability to debug a live system in an emergency because you’ve only ever followed AI’s lead is another.</span></p><blockquote><p><em>Speed vs. Knowledge trade-off: AI offers quick answers (high speed, low learning), whereas older methods (Stack Overflow, documentation) were slower but built deeper understanding</em></p></blockquote><p>In the rush for instant solutions, we risk skimming the surface and missing the context that builds true expertise.</p><p><span>What happens if this trend continues unchecked? For one, you might hit a </span><strong>“critical thinking crisis”</strong><span> in your career. If an AI has been doing your thinking for you, you could find yourself unequipped to handle novel problems or urgent issues when the tool falls short. </span></p><p><span>As one commentator bluntly </span><a href="https://www.inc.com/suzanne-lucas/microsoft-says-ai-kills-critical-thinking-heres-what-that-means-for-you/91148956#:~:text=AI%20is%20really%20good%20at,make%20appointments%20for%20my%20cats" rel="">put</a><span> it: </span><em>“The more you use AI, the less you use your brain… So when you run across a problem AI can’t solve, will you have the skills to do so yourself?”</em><span>. It’s a sobering question. We’ve already seen minor crises: developers panicking during an outage of an AI coding assistant because their workflow ground to a halt.</span></p><p><span>Over-reliance can also become a </span><strong>self-fulfilling prophecy</strong><span>. The Microsoft study authors warned that if you’re worried about AI taking your job and yet you </span><em>“use it uncritically”</em><span> you might effectively deskill yourself into irrelevance. In a team setting, this can have ripple effects. Today’s junior devs who skip the “hard way” may plateau early, lacking the depth to grow into senior engineers tomorrow. </span></p><p><span>If a whole generation of programmers </span><em>“never know the satisfaction of solving problems truly on their own”</em><span> and </span><em>“never experience the deep understanding”</em><span> from wrestling with a bug for hours, we could end up with a workforce of button-pushers who can only function with an AI’s guidance. They’ll be great at asking AI the right questions, but </span><strong>won’t truly grasp the answers</strong><span>. And when the AI is wrong (which it often is in subtle ways), these developers might not catch it – a recipe for bugs and security vulnerabilities slipping into code.</span></p><p><span>There’s also the </span><strong>team dynamic and cultural impact</strong><span> to consider. Mentorship and learning by osmosis might suffer if everyone is heads-down with their AI pair programmer. Senior engineers may find it harder to pass on knowledge if juniors are accustomed to asking AI instead of their colleagues. </span></p><p>And if those juniors haven’t built a strong foundation, seniors will spend more time fixing AI-generated mistakes that a well-trained human would have caught. In the long run, teams could become less than the sum of their parts – a collection of individuals each quietly reliant on their AI crutch, with fewer robust shared practices of critical review. The bus factor (how many people need to get hit by a bus before a project collapses) might effectively include “if the AI service goes down, does our development grind to a halt?”</p><p><span>None of this is to say we should revert to coding by candlelight. Rather, it’s a call to use these powerful tools </span><em>wisely</em><span>, lest we </span><strong>“outsource not just the work itself, but [our] critical engagement with it”</strong><span>). The goal is to reap AI’s benefits </span><em>without</em><span> hollowing out your skill set in the process.</span></p><p><span>How can we enjoy the productivity gains of AI coding assistants and </span><em>still</em><span> keep our minds sharp? The key is mindful engagement. Treat the AI as a collaborator – a junior pair programmer or an always-available rubber duck – rather than an infallible oracle or a dumping ground for problems. Here are some concrete strategies to consider:</span></p><ul><li><p><strong>Practice “AI hygiene” – always verify and understand.</strong><span> Don’t accept AI output as correct just because it looks plausible. Get in the habit of </span><em>red-teaming</em><span> the AI’s suggestions: actively look for errors or edge cases in its code. If it generates a function, test it with tricky inputs. Ask yourself, “why does this solution work? what are its limitations?” Use the AI as a learning tool by asking it to explain the code line-by-line or to offer alternative approaches. By interrogating the AI’s output, you turn a passive answer into an active lesson.</span></p></li></ul><ul><li><p><strong>No AI for fundamentals – sometimes, struggle is good.</strong><span> Deliberately reserve part of your week for “manual mode” coding. One experienced dev instituted </span><strong>“No-AI Days”</strong><span>: one day a week where he writes code from scratch, reads errors fully, and uses actual documentation instead of AI. It was frustrating at first (“I feel slower, dumber” he admitted), but like a difficult workout, it rebuilt his confidence and deepened his understanding. You don’t have to go cold turkey on AI, but regularly coding without it keeps your base skills from entropy. Think of it as cross-training for your coder brain.</span></p></li></ul><ul><li><p><strong>Always attempt a problem yourself before asking the AI.</strong><span> This is classic “open book exam” rules – you’ll learn more by struggling a bit first. Formulate an approach, even if it’s just pseudocode or a guess, </span><em>before</em><span> you have the AI fill in the blanks. If you get stuck on a bug, spend 15-30 minutes investigating on your own (use print debugging, console logs, or just reasoning through the code). This ensures you exercise your problem-solving muscles. After that, there’s no shame in consulting the AI – but now you can compare its answer with your own thinking and truly learn from any differences.</span></p></li></ul><ul><li><p><strong>Use AI to augment, not replace, code review.</strong><span> When you get an AI-generated snippet, review it as if a human colleague wrote it. Better yet, have human code reviews for AI contributions too. This keeps team knowledge in the loop and catches issues that a lone developer might miss when trusting AI. Culturally, encourage an attitude of </span><em>“AI can draft it, but we own it”</em><span> – meaning the team is responsible for understanding and maintaining all code in the repository, no matter who (or what) originally wrote it.</span></p></li></ul><ul><li><p><strong>Engage in active learning: follow up and iterate.</strong><span> If an AI solution works, don’t just move on. Take a moment to solidify that knowledge. For example, if you used AI to implement a complex regex or algorithm, afterwards try to explain it in plain English (to yourself or a teammate). Or ask the AI </span><em>why</em><span> that regex needs those specific tokens. Use the AI conversationally to deepen your understanding, not just to copy-paste answers. One developer described using ChatGPT to generate code </span><em>and then</em><span> peppering it with follow-up questions and “why not this other way?” - akin to having an infinite patience tutor. This turns AI into a mentor rather than a mere code dispenser.</span></p></li></ul><ul><li><p><strong>Keep a learning journal or list of “AI assists.”</strong><span> Track the things you frequently ask AI help for – it could be a sign of a knowledge gap you want to close. If you notice you’ve asked the AI to center a div in CSS or optimize an SQL query multiple times, make a note to truly learn that topic. You can even make flashcards or exercises for yourself based on AI solutions (embracing that </span><em>retrieval practice</em><span> we know is great for retention). The next time you face a similar problem, challenge yourself to solve it without AI and see if you remember how. Use AI as a </span><em>backstop</em><span>, not the first stop, for recurring tasks.</span></p></li></ul><ul><li><p><strong>Pair program </strong><em><strong>with</strong></em><strong> the AI.</strong><span> Instead of treating the AI like an API you feed queries to, try a pair programming mindset. For example, you write a function and let the AI suggest improvements or catch mistakes. Or vice versa: let the AI write a draft and you refine it. Maintain an ongoing dialog: </span><em>“Alright, that function works, but can you help me refactor it for clarity?”</em><span> – this keeps you in the driver’s seat. You’re not just consuming answers; you’re curating and directing the AI’s contributions in real-time. Some developers find that using AI feels like having a junior dev who’s great at grunt work but needs supervision – you </span><em>are</em><span> the senior in the loop, responsible for the final outcome.</span></p></li></ul><p><span>By integrating habits like these, you ensure that </span><strong>using AI remains a net positive</strong><span>: you get the acceleration and convenience without slowly losing your ability to code unaided. In fact, many of these practices can turn AI into a tool for </span><em>sharpening</em><span> your skills. For instance, using AI to explain unfamiliar code can deepen your knowledge, and trying to stump the AI with tricky cases can enhance your testing mindset. The difference is in staying actively involved rather than passively reliant.</span></p><p><span>The software industry is hurtling forward with AI at the helm of code generation, and there’s no putting that genie back in the bottle. Embracing these tools is not only inevitable; it’s often beneficial. But as we integrate AI into our workflow, we each have to </span><em>“walk a fine line”</em><span> on what we’re willing to cede to the machine. </span></p><p>If you love coding, it’s not just about outputting features faster - it’s also about preserving the craft and joy of problem-solving that got you into this field in the first place.</p><p><span>Use AI it to </span><strong>amplify</strong><span> your abilities, not replace them. Let it free you from drudge work so you can focus on creative and complex aspects - but don’t let those foundational skills atrophy from disuse. Stay curious about how and why things work. Keep honing your debugging instincts and system thinking even if an AI gives you a shortcut. In short, make AI </span><strong>your collaborator, not your crutch</strong><span>.</span></p><p><span>The developers who thrive will be those who pair their human intuition and experience with AI’s superpowers – who can navigate a codebase both with and without the autopilot. By consciously practicing and challenging yourself, you ensure that when the fancy tools fall short or when a truly novel problem arises, you’ll still be </span><strong>behind the wheel, sharp and ready to solve</strong><span>. Don’t worry about AI replacing you; worry about </span><em>not</em><span> cultivating the skills that make you irreplaceable. As the saying goes (with a modern twist): </span><em><span>“What the AI gives, the </span><strong>engineer’s mind</strong><span> must still understand.”</span></em><span> Keep that mind engaged, and you’ll ride the AI wave without wiping out.</span></p><p><strong>Bonus:</strong><span> The next time you’re tempted to have AI code an entire feature while you watch, consider this your nudge to roll up your sleeves and write a bit of it yourself. You might be surprised at how much you </span><em>remember</em><span> – and how good it feels to flex those mental muscles again. Don’t let the future of AI-assisted development leave you intellectually idle. Use AI to </span><em>boost</em><span> your productivity, but never cease to actively </span><strong>practice your craft</strong><span>. </span></p><p><strong>The best developers of tomorrow will be those who didn’t let today’s AI make them forget how to </strong><em><strong>think</strong></em><strong>.</strong></p><p><em><span>I’m excited to share I’m writing a new </span><a href="https://www.oreilly.com/library/view/vibe-coding-the/9798341634749/" rel="">AI-assisted engineering book</a><span> with O’Reilly. If you’ve enjoyed my writing here you may be interested in checking it out.</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:158113,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/162086801?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F54666fde-e566-4571-999c-4cf7ffaaf00b_1890x1890.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Some __nonstring__ Turbulence (102 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1018486/1dcd29863655cb25/</link>
            <guid>43790855</guid>
            <pubDate>Fri, 25 Apr 2025 06:46:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1018486/1dcd29863655cb25/">https://lwn.net/SubscriberLink/1018486/1dcd29863655cb25/</a>, See on <a href="https://news.ycombinator.com/item?id=43790855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>
<p>
New compiler releases often bring with them new warnings; those warnings
are usually welcome, since they help developers find problems before they
turn into nasty bugs.  Adapting to new warnings can also create disruption
in the development process, though, especially when an important developer
upgrades to a new compiler at an unfortunate time.  This is just the
scenario that played out with the <a href="https://lwn.net/ml/all/CAHk-=wgjZ4fzDKogXwhPXVMA7OmZf9k0o1oB2FJmv-C1e=typA@mail.gmail.com/">6.15-rc3
kernel release</a> and the implementation of
</p><tt>-Wunterminated-string-initialization</tt><p> in GCC&nbsp;15.
</p><p>
Consider a C declaration like:
</p><pre>    char foo[8] = "bar";
</pre>
<p>
The array will be initialized with the given string, including the normal
trailing NUL byte indicating the end of the string.  Now consider this
variant:
</p><pre>    char foo[8] = "NUL-free";
</pre>
<p>
This is a legal declaration, even though the declared array now lacks the
room for the NUL byte.  That byte will simply be omitted, creating an
unterminated string.  That is often not what the developer who wrote that
code wants, and it can lead to unpleasant bugs that are not discovered
until some later time.  The <tt>-Wunterminated-string-initialization</tt>
option emits a warning for this kind of initialization, with the result
that, hopefully, the problem — if there is a problem — is fixed quickly.
</p><p>
The kernel community has worked to make use of this warning and, hopefully,
eliminate a source of bugs.  There is only one little problem with the new
warning, though: sometimes the no-NUL initialization is exactly what is
wanted and intended.  See, for example, <a href="https://elixir.bootlin.com/linux/v6.14.3/source/fs/cachefiles/key.c#L11">this
declaration</a> from <tt>fs/cachefiles/key.c</tt>:
</p><pre>    static const char cachefiles_charmap[64] =
	"0123456789"			/* 0 - 9 */
	"abcdefghijklmnopqrstuvwxyz"	/* 10 - 35 */
	"ABCDEFGHIJKLMNOPQRSTUVWXYZ"	/* 36 - 61 */
	"_-"				/* 62 - 63 */
	;
</pre>
<p>
This <tt>char</tt> array is used as a lookup table, not as a string, so
there is no need for a trailing NUL byte.  GCC&nbsp;15, being unaware of
that usage, will emit a false-positive warning for this declaration.  There
are many places in the kernel with declarations like this; the ACPI code,
for example, uses a lot of four-byte string arrays to handle the equally
large set of four-letter ACPI acronyms.
</p><p>
Naturally, there is a way to suppress the warning when it does not apply
by adding an attribute to the declaration indicating that the <tt>char</tt>
array is not actually holding a string:
</p><pre>    __attribute__((__nonstring__))
</pre>
<p>
Within the kernel, the macro <tt>__nonstring</tt> is used to shorten that
attribute syntax.  Work has been ongoing, primarily by Kees Cook, to fix
all of the warnings added by GCC&nbsp;15.  Many patches have been
circulated; quite a few of them are in linux-next.  Cook has also been
working with the GCC developers to improve how this annotation works and to
<a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=118095">fix a
problem</a> that the kernel project ran into.  There was some time left
to get this job done, though, since GCC&nbsp;15 has not actually been
released — or so Cook thought.
</p><p>
Fedora 42 <i>has</i> been released, though, and the Fedora developers, for
better or worse, decided to include a pre-release version of GCC&nbsp;15
with it as the default compiler.  The Fedora project, it seems, has decided
to follow <a href="https://lwn.net/2000/1005/dists.php3">a venerable Red Hat tradition</a>
with this release.  Linus Torvalds, for better or worse,
decided to update his development systems to Fedora&nbsp;42 the day before
tagging and releasing 6.15-rc3.  Once he tried building the kernel with the
new compiler, though, things started to go wrong, since the relevant
patches were not yet in his repository.  Torvalds responded with a series
of changes of his own, applied directly to the mainline about two hours
before the release, to fix the problems that he had encountered.  They
included <a href="https://git.kernel.org/linus/4b4bd8c50f48">this patch</a>
fixing warnings in the ACPI subsystem, and <a href="https://git.kernel.org/linus/05e8d261a34e">this one</a> fixing
several others, including the example shown above.  He then tagged and
pushed out 6.15-rc3 with those changes.
</p><p>
Unfortunately, his last-minute changes broke the build on any version of
GCC prior to the GCC&nbsp;15 pre-release — a problem that was likely to
create a certain amount of inconvenience for any developers who were not
running Fedora&nbsp;42.  So, shortly after the 6.15-rc3 release, Torvalds
tacked on <a href="https://git.kernel.org/linus/9d7a0577c9db">one more
patch</a> backing out the breaking change and disabling the new warning
altogether.
</p><p>
This drew <a href="https://lwn.net/ml/all/202504201840.3C1F04B09@keescook">a somewhat
grumpy note</a> from Cook, who said that he had already sent patches fixing
all of the problems, including the build-breaking one that Torvalds ran
into.  He asked Torvalds to revert the changes and use the planned fixes,
adding: "<q>It is, once again, really frustrating when you update to
unreleased compiler versions</q>".  Torvalds <a href="https://lwn.net/ml/all/CAHk-=whryuuKnd_5w6169EjfRr_f+t5BRmKt+qfjALFzfKQNvQ@mail.gmail.com">disagreed</a>,
saying that he needed to make the changes because the kernel failed to
build otherwise.  He also asserted that GCC&nbsp;15 <i>was</i> released by
virtue of its presence in Fedora&nbsp;42.  Cook <a href="https://lwn.net/ml/all/202504210909.D4EAB689@keescook">was unimpressed</a>:
</p><blockquote>
	Yes, I understand that, but you didn't coordinate with anyone. You
	didn't search lore for the warning strings, you didn't even check
	-next where you've now created merge conflicts. You put
	insufficiently tested patches into the tree at the last minute and
	cut an rc release that broke for everyone using GCC &lt;15. You
	mercilessly flame maintainers for much much less.
</blockquote>
<p>
Torvalds <a href="https://lwn.net/ml/all/CAHk-=whjZ-id_1m7cgp4aC+N6yZj3s5Jy=mf2oiEADJ3Tp8sxw@mail.gmail.com">stood
his ground</a>, though, blaming Cook for not having gotten the fixes into
the mainline quickly enough.
</p><p>
That is where the situation stands, as of this writing.  Others will
undoubtedly take the time to fix the problems properly, adding the changes
that were intended all along.  But this course of events has created some
bad feelings all around, feelings that could maybe have been avoided with a
better understanding of just when a future version of GCC is expected to be
able to build the kernel.
</p><p>
As a sort of coda, it is worth saying that Torvalds also has a fundamental
disagreement with how this attribute is implemented.  The
<tt>__nonstring__</tt> attribute applies to variables, not types, so it
must be used in every place where a <tt>char</tt> array is used without
trailing NUL bytes.  He would rather annotate the type, indicating that
every instance of that type holds bytes rather than a character string, and
avoid the need to mark rather larger numbers of variable declarations.  But
that is not how the attribute works, so the kernel will have to
include <tt>__nonstring</tt> markers for every <tt>char</tt> array that is
used in that way.<br clear="all"></p><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Kernel/Index">Kernel</a></td><td><a href="https://lwn.net/Kernel/Index#GCC">GCC</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepMind releases Lyria 2 music generation model (254 pts)]]></title>
            <link>https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/</link>
            <guid>43790093</guid>
            <pubDate>Fri, 25 Apr 2025 04:25:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/</a>, See on <a href="https://news.ycombinator.com/item?id=43790093">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
    
  
  
  
    
      

      
      
        
          
            <div>
              
                
                
                  
                  
<div>
    <div>
      <p>Technologies</p>
      

      
    <dl>
      
        <dt>Published</dt>
        <dd><time datetime="2025-04-24">24 April 2025</time></dd>
      
      
        <dt>Authors</dt>
        
      
    </dl>
  

      
    </div>

    
      
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1056-h594-n-nu-rw 2x">
      <img alt="An image of Music AI Sandbox's timeline with a visible audio waveform. A cursor hovers over the &quot;Create&quot; button." height="603" src="https://lh3.googleusercontent.com/XFT_zIfIE9A9pNk0hylwxSrXa2z3AncSXWjkjl9064wCTmFqGcQ7dz2NMUOgcZWL3myEyX0qjeBdugClyEvfAwPsFj6xONo7saU52TXLSo6xcAPTdsk=w1072-h603-n-nu" width="1072">
    </picture>
    
  
    
  </div>
                
              
                
                
                  
                  <div>
  <h4 data-block-key="lsvmh">Musicians today are drawing inspiration and crafting their sound using a broad ecosystem of tools — from mobile apps to traditional Digital Audio Workstations, specialized plug-ins and hardware. Now, artificial intelligence (AI) is emerging as a powerful new part of this creative toolkit, opening doors to novel workflows and sonic possibilities.</h4><p data-block-key="9nm0a">Google has long collaborated with musicians, producers, and artists in the research and development of music AI tools. Ever since launching the <a href="https://magenta.withgoogle.com/blog/2016/06/01/welcome-to-magenta/" rel="noopener" target="_blank">Magenta</a> project, in 2016, we’ve been exploring how AI can enhance creativity — sparking inspiration, facilitating exploration and enabling new forms of expression, always hand-in-hand with the music community.</p><p data-block-key="f8beg">Our ongoing collaborations led to the creation of <a href="https://blog.youtube/inside-youtube/ai-and-music-experiment/" rel="noopener" target="_blank">Music AI Sandbox</a>, in 2023, which we’ve shared with musicians, producers and songwriters through YouTube’s <a href="https://blog.youtube/inside-youtube/partnering-with-the-music-industry-on-ai/" rel="noopener" target="_blank">Music AI Incubator</a>.</p><p data-block-key="5lk3q">Building upon the work we've done to date, today, we're introducing new features and improvements to Music AI Sandbox, including <a href="https://deepmind.google/technologies/lyria/">Lyria 2</a>, our latest music generation model. We're giving more musicians, producers and songwriters in the U.S. access to experiment with these tools, and are gathering feedback to inform their development.</p><p data-block-key="m6r">We're excited to see what this growing community creates with Music AI Sandbox and encourage interested musicians, songwriters, and producers to sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLSfmU9T4KF-3ks57ACPnXqz4f9CX4guYEJrDhYSft9zAZItn_w/viewform" rel="noopener" target="_blank">here</a>.</p>
</div>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="ak3mo">Music AI Sandbox</h2><p data-block-key="635bc">We created Music AI Sandbox in close collaboration with musicians. Their input guided our development and experiments, resulting in a set of responsibly created tools that are practical, useful and can open doors to new forms of music creation.</p><p data-block-key="3rm1k">The Music AI Sandbox is a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. Artists can generate fresh instrumental ideas, craft vocal arrangements or simply break through a creative block.</p><p data-block-key="5svk0">With these tools, musicians can discover new sounds, experiment with different genres, expand and enhance their musical libraries, or develop entirely new styles. They can also push further into unexplored territories — from unique soundscapes to their next creative breakthrough.</p>
</div>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Create new musical parts</h3><p data-block-key="8r0n0">Quickly try out music ideas by describing what kind of sound you want — the Music AI Sandbox understands genres, moods, vocal styles and instruments. The Create tool helps generate many different music samples to spark the imagination or for use in a track. Artists can also place their own lyrics on a timeline and specify musical characteristics, like tempo and key.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-3670bac7-9da5-4eaf-85fb-52df0ce1ede5">
  

  <figcaption>
      <p data-block-key="4apvk">Animation of Music AI Sandbox’s interface, showing how to use the Create feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Explore new directions with Extend</h3><p data-block-key="al12t">Need inspiration for where to take an existing musical piece? The Extend feature generates musical continuations based on uploaded or generated audio clips. It’s a way to hear potential developments for your ideas, reimagine your own work, or overcome writer's block.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-b1304d21-7c4e-44d7-b641-0b3b575a6ca3">
  

  <figcaption>
      <p data-block-key="ulaih">Animation of Music AI Sandbox’s interface, showing how to use the Extend feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="ak3mo">Reimagine music with Edit</h3><p data-block-key="9ut5j">Reshape music with fine-grained control. The Edit feature makes it possible to transform the mood, genre or style of an entire clip, or make targeted modifications to specific parts. Intuitive controls enable subtle tweaks or dramatic shifts. Now, users can also transform audio using text prompts, experiment with preset transformations to fill gaps or blend clips and build transitions between different musical sections.</p>
</div>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-8f42a435-76f6-4f93-bf3e-377bade49aa6">
  

  <figcaption>
      <p data-block-key="er6tt">Animation of Music AI Sandbox’s interface, showing how to use the Edit feature.</p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="ua69x">What artists are creating with the Music AI Sandbox</h2><p data-block-key="9k4pn">See how musicians are leveraging this tool to fuel their creativity and generate fresh musical concepts.</p>
</div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <p data-block-key="3rq62">Listen to these demo tracks that artists are bringing to life using the Music AI Sandbox:</p>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <div>
  <h2 data-block-key="8g3r1">High-fidelity and real-time music with Lyria</h2><p data-block-key="efiae">Since introducing Lyria, we’ve continued to innovate with input and insights from music industry professionals. Our latest music generation model, <a href="https://deepmind.google/technologies/lyria/">Lyria 2</a>, delivers high-fidelity music and professional-grade audio outputs that capture subtle nuances across a range of genres and intricate compositions.</p><p data-block-key="8uhj0">We’ve also developed <a href="https://deepmind.google/technologies/lyria/realtime/">Lyria RealTime</a>, which allows users to interactively create, perform and control music in real-time, mixing genres, blending styles and shaping audio moment by moment. Lyria RealTime can help users create continuous streams of music, forge sonic connections and quickly explore ideas on the fly.</p><p data-block-key="d656g">Responsibly deploying generative technologies is core to our values, so all music generated by Lyria 2 and Lyria RealTime models is watermarked using our <a href="https://deepmind.google/technologies/synthid/">SynthID</a> technology.</p>
</div>
                
              
                
                
                  
                  <div>
  <h2 data-block-key="hddx5">Building AI for musicians, with musicians</h2><p data-block-key="ctoh8">Through collaborations like Music AI Sandbox, we aim to build trust with musicians, the industry and artists. Their expertise and valuable feedback help us ensure our tools empower creators, enabling them to realize the possibilities of AI in their art and explore new ways to express themselves. We’re excited to see what artists create with our tools and look forward to sharing more later this year.</p>
</div>
                
              
                
                
                  
                  

<section>
  

  <ul>
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://docs.google.com/forms/d/e/1FAIpQLSfmU9T4KF-3ks57ACPnXqz4f9CX4guYEJrDhYSft9zAZItn_w/viewform" rel="noopener" target="_blank">
      <span>Sign up for the Music AI Sandbox waitlist</span>
      
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://deepmind.google/technologies/lyria/">
      <span>Learn more about Lyria 2</span>
      
    </a>
            </gemini-button>
        </li>
        
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://deepmind.google/technologies/lyria/realtime/">
      <span>Learn more about Lyria RealTime</span>
      
    </a>
            </gemini-button>
        </li>
        
    
  </ul>
</section>
                
              
                
                
                  
                  <div>
      <p data-block-key="r4x9w">Music AI Sandbox was developed by Adam Roberts, Amy Stuart, Ari Troper, Beat Gfeller, Chris Deaner, Chris Reardon, Colin McArdell, DY Kim, Ethan Manilow, Felix Riedel, George Brower, Hema Manickavasagam, Jeff Chang, Jesse Engel, Michael Chang, Moon Park, Pawel Wluka, Reed Enger, Ross Cairns, Sage Stevens, Tom Jenkins, Tom Hume and Yotam Mann. Additional contributions provided by Arathi Sethumadhavan, Brian McWilliams, Cătălina Cangea, Doug Fritz, Drew Jaegle, Eleni Shaw, Jessi Liang, Kazuya Kawakami, and Veronika Goldberg.</p><p data-block-key="hp34">Lyria 2 was developed by Asahi Ushio, Beat Gfeller, Brian McWilliams, Kazuya Kawakami, Keyang Xu, Matej Kastelic, Mauro Verzetti, Myriam Hamed Torres, Ondrej Skopek, Pavel Khrushkov, Pen Li, Tobenna Peter Igwe and Zalan Borsos. Additional contributions provided by Adam Roberts, Andrea Agostinelli, Benigno Uria, Carrie Zhang, Chris Deaner, Colin McArdell, Eleni Shaw, Ethan Manilow, Hongliang Fei, Jason Baldridge, Jesse Engel, Li Li, Luyu Wang, Mauricio Zuluaga, Noah Constant, Ruba Haroun, Tayniat Khan, Volodymyr Mnih, Yan Wu and Zoe Ashwood.</p><p data-block-key="5qeas">Special thanks to Aäron van den Oord, Douglas Eck, Eli Collins, Mira Lane, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the development process.</p><p data-block-key="blpin">We also acknowledge the many other individuals who contributed across Google DeepMind and Alphabet, including our colleagues at YouTube (a particular shout out to the YouTube Artist Partnerships team led by Vivien Lewit for their support partnering with the music industry).</p>
    </div>
                
              
                
                
                  
                  



  
    
  

                
              
            </div>
          
        
      

      
    
  
  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Observability 2.0 and the Database for It (103 pts)]]></title>
            <link>https://greptime.com/blogs/2025-04-25-greptimedb-observability2-new-database</link>
            <guid>43789625</guid>
            <pubDate>Fri, 25 Apr 2025 02:39:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://greptime.com/blogs/2025-04-25-greptimedb-observability2-new-database">https://greptime.com/blogs/2025-04-25-greptimedb-observability2-new-database</a>, See on <a href="https://news.ycombinator.com/item?id=43789625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-ad7896e1=""><p>Observability 2.0 is a concept introduced by <a href="https://www.honeycomb.io/blog/time-to-version-observability-signs-point-to-yes" target="_blank" rel="noreferrer">Charity Majors of Honeycomb</a>, though she later <a href="https://charity.wtf/2024/12/20/on-versioning-observabilities-1-0-2-0-3-0-10-0/" target="_blank" rel="noreferrer">expressed reservations about labeling it as such</a>(follow-up).</p><p>Despite its contested naming, Observability 2.0 represents an evolution <strong>from the foundational "three pillars" of observability, metrics, logs, and traces</strong>, which have dominated the field for nearly a decade. Instead, it emphasizes a single source of truth paradigm as a data foundation of observability. This approach prioritizes high-cardinality, wide-event datasets over traditional siloed telemetry, aiming to address modern system complexity more effectively.</p><h2 id="what-is-observability-2-0-and-wide-events" tabindex="-1">What is Observability 2.0 and Wide Events <a href="#what-is-observability-2-0-and-wide-events" aria-label="Permalink to &quot;What is Observability 2.0 and Wide Events&quot;">​</a></h2><p>For years, observability has relied on the three pillars of metrics, logs, and traces. These pillars spawned countless libraries, tools, and standards—including <a href="https://opentelemetry.io/" target="_blank" rel="noreferrer">OpenTelemetry</a>, one of the most successful cloud-native projects, which is built entirely on this paradigm. However, as systems grow in complexity, the limitations of this approach become evident.</p><h3 id="the-downsides-of-traditional-observability" tabindex="-1">The Downsides of Traditional Observability <a href="#the-downsides-of-traditional-observability" aria-label="Permalink to &quot;The Downsides of Traditional Observability&quot;">​</a></h3><ul><li><strong>Data silos</strong>: Metrics, logs, and traces are often stored separately, leading to uncorrelated, or even inconsistent, data without meticulous management.</li><li><strong>Pre-aggregation trade-offs</strong>: Pre-aggregated metrics (counters, summaries, histograms) were originally designed to reduce storage costs and improve performance by sacrificing granularity. However, the rigid structure of time-series data limits the depth of contextual information, forcing teams to generate millions of distinct time-series to capture necessary details. Ironically, this practice now incurs exponentially higher storage and computational costs—directly contradicting the approach’s original purpose.</li><li><strong>Unstructured logs</strong>: While logs inherently contain structured data, extracting meaning requires intensive parsing, indexing, and computational effort.</li><li><strong>Static instrumentation</strong>: Tools rely on predefined queries and thresholds, limiting detection to 'known knowns'. Adapting observability requires code changes, forcing it to align with slow software development cycles.</li><li><strong>Redundant data</strong>: Identical information is duplicated across metrics, logs, and traces, wasting storage and increasing overhead.</li></ul><h3 id="wide-events-the-approach-of-observability-2-0" tabindex="-1">Wide Events: The Approach of Observability 2.0 <a href="#wide-events-the-approach-of-observability-2-0" aria-label="Permalink to &quot;Wide Events: The Approach of Observability 2.0&quot;">​</a></h3><p>Observability 2.0 addresses these issues by <strong>adopting wide events as its foundational data structure</strong>. Instead of precomputing metrics or structuring logs upfront, it preserves raw, high-fidelity event data as the single source of truth. This allows teams to perform exploratory analysis retroactively, deriving metrics, logs, and traces dynamically from the original dataset.</p><p>Boris Tane, in <a href="https://boristane.com/blog/observability-wide-events-101/" target="_blank" rel="noreferrer">his article Observability Wide Event 101</a>, defines a wide event as a context-rich, high-dimensional, and high-cardinality record. For example, a single wide event might include:</p><div><p><span>json</span></p><pre><code><span><span>{</span></span>
<span><span>  "</span><span>method</span><span>"</span><span>:</span><span> "</span><span>POST</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>path</span><span>"</span><span>:</span><span> "</span><span>/articles</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>service</span><span>"</span><span>:</span><span> "</span><span>articles</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>outcome</span><span>"</span><span>:</span><span> "</span><span>ok</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>status_code</span><span>"</span><span>:</span><span> 201</span><span>,</span></span>
<span><span>  "</span><span>duration</span><span>"</span><span>:</span><span> 268</span><span>,</span></span>
<span><span>  "</span><span>requestId</span><span>"</span><span>:</span><span> "</span><span>8bfdf7ecdd485694</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>timestamp</span><span>"</span><span>:</span><span>"</span><span>2024-09-08 06:14:05.680</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>message</span><span>"</span><span>:</span><span> "</span><span>Article created</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>commit_hash</span><span>"</span><span>:</span><span> "</span><span>690de31f245eb4f2160643e0dbb5304179a1cdd3</span><span>"</span><span>,</span></span>
<span><span>  "</span><span>user</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>id</span><span>"</span><span>:</span><span> "</span><span>fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>activated</span><span>"</span><span>:</span><span> true,</span></span>
<span><span>    "</span><span>subscription</span><span>"</span><span>:</span><span> {</span></span>
<span><span>      "</span><span>id</span><span>"</span><span>:</span><span> "</span><span>1aeb233c-1572-4f54-bd10-837c7d34b2d3</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>trial</span><span>"</span><span>:</span><span> true,</span></span>
<span><span>      "</span><span>plan</span><span>"</span><span>:</span><span> "</span><span>free</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>expiration</span><span>"</span><span>:</span><span> "</span><span>2024-09-16 14:16:37.980</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>created</span><span>"</span><span>:</span><span> "</span><span>2024-08-16 14:16:37.980</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>updated</span><span>"</span><span>:</span><span> "</span><span>2024-08-16 14:16:37.980</span><span>"</span></span>
<span><span>    },</span></span>
<span><span>    "</span><span>created</span><span>"</span><span>:</span><span> "</span><span>2024-08-16 14:16:37.980</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>updated</span><span>"</span><span>:</span><span> "</span><span>2024-08-16 14:16:37.980</span><span>"</span></span>
<span><span>  },</span></span>
<span><span>  "</span><span>article</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>id</span><span>"</span><span>:</span><span> "</span><span>f8d4d21c-f1fd-48b9-a4ce-285c263170cc</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>title</span><span>"</span><span>:</span><span> "</span><span>Test Blog Post</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>ownerId</span><span>"</span><span>:</span><span> "</span><span>fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>published</span><span>"</span><span>:</span><span> false,</span></span>
<span><span>    "</span><span>created</span><span>"</span><span>:</span><span> "</span><span>2024-09-08 06:14:05.460</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>updated</span><span>"</span><span>:</span><span> "</span><span>2024-09-08 06:14:05.460</span><span>"</span></span>
<span><span>  },</span></span>
<span><span>  "</span><span>db</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>query</span><span>"</span><span>:</span><span> "</span><span>INSERT INTO articles (id, title, content, owner_id, published, created, updated) VALUES ($1, $2, $3, $4, $5, $6, $7);</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>parameters</span><span>"</span><span>:</span><span> {</span></span>
<span><span>      "</span><span>$1</span><span>"</span><span>:</span><span> "</span><span>f8d4d21c-f1fd-48b9-a4ce-285c263170cc</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>$2</span><span>"</span><span>:</span><span> "</span><span>Test Blog Post</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>$3</span><span>"</span><span>:</span><span> "</span><span>******</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>$4</span><span>"</span><span>:</span><span> "</span><span>fdc4ddd4-8b30-4ee9-83aa-abd2e59e9603</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>$5</span><span>"</span><span>:</span><span> false,</span></span>
<span><span>      "</span><span>$6</span><span>"</span><span>:</span><span> "</span><span>2024-09-08 06:14:05.460</span><span>"</span><span>,</span></span>
<span><span>      "</span><span>$7</span><span>"</span><span>:</span><span> "</span><span>2024-09-08 06:14:05.460</span><span>"</span></span>
<span><span>    }</span></span>
<span><span>  },</span></span>
<span><span>  "</span><span>cache</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>operation</span><span>"</span><span>:</span><span> "</span><span>write</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>key</span><span>"</span><span>:</span><span> "</span><span>f8d4d21c-f1fd-48b9-a4ce-285c263170cc</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>value</span><span>"</span><span>:</span><span> "</span><span>{</span><span>\"</span><span>article</span><span>\"</span><span>:{</span><span>\"</span><span>id</span><span>\"</span><span>:</span><span>\"</span><span>f8d4d21c-f1fd-48b9-a4ce-285c263170cc</span><span>\"</span><span>,</span><span>\"</span><span>title</span><span>\"</span><span>:</span><span>\"</span><span>Test Blog Post</span><span>\"</span><span>...</span><span>"</span></span>
<span><span>  },</span></span>
<span><span>  "</span><span>headers</span><span>"</span><span>:</span><span> {</span></span>
<span><span>    "</span><span>accept-encoding</span><span>"</span><span>:</span><span> "</span><span>gzip, br</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>cf-connecting-ip</span><span>"</span><span>:</span><span> "</span><span>*****</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>connection</span><span>"</span><span>:</span><span> "</span><span>Keep-Alive</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>content-length</span><span>"</span><span>:</span><span> "</span><span>1963</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>content-type</span><span>"</span><span>:</span><span> "</span><span>application/json</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>host</span><span>"</span><span>:</span><span> "</span><span>website.com</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>url</span><span>"</span><span>:</span><span> "</span><span>https://website.com/articles</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>user-agent</span><span>"</span><span>:</span><span> "</span><span>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>Authorization</span><span>"</span><span>:</span><span> "</span><span>********</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>x-forwarded-proto</span><span>"</span><span>:</span><span> "</span><span>https</span><span>"</span><span>,</span></span>
<span><span>    "</span><span>x-real-ip</span><span>"</span><span>:</span><span> "</span><span>******</span><span>"</span></span>
<span><span>  }</span></span>
<span><span>}</span></span></code></pre></div><p>Wide events contain significantly more contextual data than traditional structured logs, capturing comprehensive application state details. When stored in an observability data store, these events serve as a raw dataset from which teams can compute <strong>any conventional metric</strong> post hoc. For instance:</p><ul><li>QPS (queries per second) for a specific API path.</li><li>Response rate distributions by HTTP status code.</li><li>Error rates filtered by user region or device type.</li></ul><p>This process requires no code changes—metric are derived directly from the raw event data through queries, eliminating the need for pre-aggregation or prior instrumentation.</p><p>For practical implementations, see:</p><ul><li><a href="https://isburmistrov.substack.com/p/all-you-need-is-wide-events-not-metrics" target="_blank" rel="noreferrer">All You Need is Wide Events (Not Metrics)</a></li><li><a href="https://jeremymorrell.dev/blog/a-practitioners-guide-to-wide-events/" target="_blank" rel="noreferrer">A Practitioner’s Guide to Wide Events</a></li></ul><h2 id="challenges-of-observability-2-0-adoption" tabindex="-1">Challenges of Observability 2.0 Adoption <a href="#challenges-of-observability-2-0-adoption" aria-label="Permalink to &quot;Challenges of Observability 2.0 Adoption&quot;">​</a></h2><p>Traditional metrics and logs were designed to prioritize resource efficiency: minimizing compute and storage costs. For example, Prometheus employs a semi-key-value data model to optimize time-series storage, akin to the early NoSQL era: just as developers moved relational database workloads to Redis (counters, sorted sets, lists) for speed and simplicity, observability tools adopted pre-aggregated metrics and logs to reduce overhead.</p><p>However, like the shift to Big Data in software engineering, the move from the "three pillars" to wide events reflects a growing need for raw, granular data over precomputed summaries. This transition introduces key challenges:</p><ul><li><strong>Event generation</strong>: Lack of mature frameworks to instrument applications and emit standardized, context-rich wide events.</li><li><strong>Data transport</strong>: Efficiently streaming high-volume event data without bottlenecks or latency.</li><li><strong>Cost-effective storage</strong>: Storing terabytes of raw, high-cardinality data affordably while retaining query performance.</li><li><strong>Query flexibility</strong>: Enabling ad-hoc analysis across arbitrary dimensions (e.g., user attributes, request paths) without predefining schemas.</li><li><strong>Tooling integration</strong>: Leveraging existing tools (e.g., dashboards, alerts) by deriving metrics and logs retroactively from stored events, not at the application layer.</li></ul><h2 id="what-shapes-the-database-for-observability-2-0" tabindex="-1">What shapes the database for observability 2.0 <a href="#what-shapes-the-database-for-observability-2-0" aria-label="Permalink to &quot;What shapes the database for observability 2.0&quot;">​</a></h2><p>As Charity Majors notes in her recent <a href="https://charity.wtf/2025/03/24/another-observability-3-0-appears-on-the-horizon/" target="_blank" rel="noreferrer">blog post</a>, observability is evolving toward a data lake model. While wide events simplify data modeling by acting as a single source of truth, they demand infrastructure designed to address the challenges outlined earlier.</p><p>Adopting Observability 2.0 aims to maximize raw data utility <strong>without prohibitive complexity</strong>. While technically possible to cobble together a solution using:</p><ul><li>OLAP databases for raw event storage.</li><li>Pre-processing pipelines for derived metrics/traces.</li><li>Separate metric stores for dashboards.</li><li>Trace stores for distributed tracing APIs.</li><li>Backup systems for cold data. …this fragmented approach undermines the core promise of Observability 2.0. Instead, the paradigm demands a dedicated database optimized for its unique workload.</li></ul><figure><img src="https://greptime.com/blogs/2025-04-25-greptimedb-observability2-new-database/observability2infuture.png" alt="（Figure 1: The Architecture of Observability 2.0）"><figcaption>（Figure 1: The Architecture of Observability 2.0）</figcaption></figure><p>Let’s walk through key factors that define the database requirements for Observability 2.0:</p><h3 id="making-good-usage-of-commodity-storage-and-data-format" tabindex="-1">Making good usage of commodity storage and data format <a href="#making-good-usage-of-commodity-storage-and-data-format" aria-label="Permalink to &quot;Making good usage of commodity storage and data format&quot;">​</a></h3><ul><li>Disaggregated architecture, cloud object storage</li><li>Columnar data format for compression and performance</li></ul><p>As demonstrated in Boris Tane’s example, a single uncompressed wide event can exceed <strong>2KB</strong> in size. For high-throughput microservice-based applications, this introduces a multiplier effect on storage demands—especially when retaining data long-term for continuous analysis (e.g., training AI models or auditing historical trends).</p><p>The database must leverage <strong>cloud-based object storage</strong> (e.g., AWS S3, Google Cloud Storage) for cost efficiency and scalability. Ideally, it should automate data tiering between local and cloud storage with minimal management overhead—embodying the <strong>disaggregated compute and storage architecture</strong>, where storage scales independently of compute resources.</p><p><strong>Columnar data formats</strong> (e.g., Apache Parquet, Arrow) are critical for reducing storage costs. By storing values from the same field sequentially, they enable custom encoding (e.g., dictionary encoding, run-length encoding) to compress data at rest. Additionally, columnar formats are inherently optimized for analytical queries, as they allow efficient column pruning and vectorized processing.</p><h3 id="realtime-scalable-and-elastic" tabindex="-1">Realtime, scalable and elastic <a href="#realtime-scalable-and-elastic" aria-label="Permalink to &quot;Realtime, scalable and elastic&quot;">​</a></h3><ul><li>Low latency on query as well as data visibility</li><li>The ingestion rate can scale with site traffic</li></ul><p>Due to their pre-aggregated nature, traditional metrics don't scale as your traffic increases unless you spin up numerous new instances. This makes capacity planning for metrics stores more straightforward. However, with wide events serving as a single source of truth, observability data is generated per-request, meaning your observability 2.0 infrastructure must be as <strong>scalable and elastic</strong> as your application. To scale properly in modern cloud environments, databases must be designed carefully, keeping state contained within minimal scope and establishing clear separation of responsibilities for each type of node.</p><p>Your data has to be ingested and query-able in realtime to fulfill the needs for realtime use-cases like dashboards and alerting.</p><h3 id="flexible-query-capabilities" tabindex="-1">Flexible query capabilities <a href="#flexible-query-capabilities" aria-label="Permalink to &quot;Flexible query capabilities&quot;">​</a></h3><ul><li>The database has to handle Observability 1.0 queries, as well as analytical queries.</li><li>Metrics need to be derived from wide events, within the database.</li></ul><p>An Observability 2.0 database must support two types of queries: <strong>routine queries (for dashboards and alerts) and exploratory queries (for ad-hoc analysis)</strong>.</p><p>Removing metrics as first-class citizens does not eliminate the need for pre-aggregation: it simply shifts this responsibility from the application layer to the database. Users still require fast access to known-knowns (e.g., error rates, latency thresholds) via dashboards or alerts, which demands efficient processing of routine queries. However, this becomes challenging with high-dimensional data. To address this, the database must support:</p><ul><li>Flexible indexing strategies</li><li>Pre-processing capabilities</li><li>Incremental computation (e.g., updating aggregates without reprocessing entire datasets)</li></ul><p>Additionally, the database must handle exploratory analytics, which enables uncovering "unknown unknowns" through offline, long-term analysis. These queries are often unpredictable, spanning large datasets and extended time ranges. Ideally, the database should execute them without degrading performance for routine queries or ingestion. While users could offload this workload to a dedicated OLAP database, the added latency and cost of data duplication create friction, undermining Observability 2.0’s goal of unified, real-time insights.</p><h3 id="backward-compatible-to-observability-1-0" tabindex="-1">Backward-compatible to observability 1.0 <a href="#backward-compatible-to-observability-1-0" aria-label="Permalink to &quot;Backward-compatible to observability 1.0&quot;">​</a></h3><ul><li>We still need Grafana dashboards</li></ul><p>We already have robust dashboards, visualizations, and alert rules from Observability 1.0. Transitioning to wide events <strong>does not require abandoning these tools or rebuilding from scratch</strong>.</p><p>A DSL like PromQL remains well-suited for time-series dashboards compared to SQL. The advantage now is that <strong>complex PromQL queries</strong> become unnecessary, thanks to the clear separation between routine queries (optimized for dashboards/alerts) and exploratory analytics. Crucially, high cardinality should no longer be a systemic limitation of the observability database.</p><p>Trace views and log tailing functionality must remain accessible through the new backend. All established best practices from Observability 1.0—such as alert thresholds, dashboard conventions, and trace analysis workflows—<strong>should be preserved and enhanced, not discarded</strong>.</p><h2 id="that-s-how-we-built-greptimedb" tabindex="-1">That's how we built GreptimeDB <a href="#that-s-how-we-built-greptimedb" aria-label="Permalink to &quot;That's how we built GreptimeDB&quot;">​</a></h2><p>GreptimeDB is the open-source analytical observability database built for wide events and o11y 2.0 practice. We designed it to work seamlessly with modern cloud infrastructure, and provide our user efficient, one-stop and handy experience on observability data management.</p><figure><img src="https://greptime.com/blogs/2025-04-25-greptimedb-observability2-new-database/greptimedbinobservability.png" alt="（Figure 2: Observability 2.0-The Greptime Way）"><figcaption>（Figure 2: Observability 2.0-The Greptime Way）</figcaption></figure><p>This logical flow chart describes key features of GreptimeDB that fits observability 2.0 data lifecycle:</p><ul><li>Accept incoming data in OpenTelemetry format</li><li>Built-in transform engine to pre-process data</li><li>High-throughput realtime data ingestion</li><li>Realtime query API</li><li>Materialized view for data derivation</li><li>Read-replicas for isolated analytical queries</li><li>Built-in rule engine and trigger mechanisms for push-based notification</li><li>Object storage for data persistence</li></ul><h2 id="conclusion" tabindex="-1">Conclusion <a href="#conclusion" aria-label="Permalink to &quot;Conclusion&quot;">​</a></h2><p>We believe raw data based approach will transform how we use observability data and extract value from it. GreptimeDB is committed to be the open source infrastructure that helps you to archive it progressively.</p><p>Start your journey with GreptimeDB today and unlock the future of observability, one event at a time.</p><hr><h2 id="about-greptime" tabindex="-1">About Greptime <a href="#about-greptime" aria-label="Permalink to &quot;About Greptime&quot;">​</a></h2><p>GreptimeDB is an open-source, cloud-native database purpose-built for real-time observability. Built in Rust and optimized for cloud-native environments, it provides unified storage and processing for metrics, logs, and traces—delivering sub-second insights from edge to cloud —at any scale.</p><ul><li><p><a href="https://greptime.com/product/db" target="_blank" rel="noreferrer">GreptimeDB OSS</a> – The open-sourced database for small to medium-scale observability and IoT use cases, ideal for personal projects or dev/test environments.</p></li><li><p><a href="https://greptime.com/product/enterprise" target="_blank" rel="noreferrer">GreptimeDB Enterprise</a> – A robust observability database with enhanced security, high availability, and enterprise-grade support.</p></li><li><p><a href="https://greptime.com/product/cloud" target="_blank" rel="noreferrer">GreptimeCloud</a> – A fully managed, serverless DBaaS with elastic scaling and zero operational overhead. Built for teams that need speed, flexibility, and ease of use out of the box.</p></li></ul><p>🚀 We’re open to contributors—get started with issues labeled good first issue and connect with our community.</p><p>⭐ <a href="https://github.com/GreptimeTeam/greptimedb" target="_blank" rel="noreferrer">GitHub</a> | 🌐 <a href="https://greptime.com/" target="_blank" rel="noreferrer">Website</a> | 📚 <a href="https://docs.greptime.com/" target="_blank" rel="noreferrer">Docs</a></p><p>💬 <a href="https://greptime.com/slack" target="_blank" rel="noreferrer">Slack</a> | 🐦 <a href="https://x.com/Greptime" target="_blank" rel="noreferrer">Twitter</a> | 💼 <a href="https://www.linkedin.com/" target="_blank" rel="noreferrer">LinkedIn</a></p></div><div data-v-ad7896e1="" data-v-f7b8a506=""><h2 data-v-f7b8a506="">Join our community</h2><p>Get the latest updates and discuss with other users.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Notation as a Tool of Thought (1979) (200 pts)]]></title>
            <link>https://www.jsoftware.com/papers/tot.htm</link>
            <guid>43789593</guid>
            <pubDate>Fri, 25 Apr 2025 02:30:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jsoftware.com/papers/tot.htm">https://www.jsoftware.com/papers/tot.htm</a>, See on <a href="https://news.ycombinator.com/item?id=43789593">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Street address errors in Google Maps (132 pts)]]></title>
            <link>https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/</link>
            <guid>43788832</guid>
            <pubDate>Fri, 25 Apr 2025 00:01:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/">https://randomascii.wordpress.com/2025/04/24/google-maps-doesnt-know-how-street-addresses-work/</a>, See on <a href="https://news.ycombinator.com/item?id=43788832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						<p>I was driving around Vernon, BC a few weeks ago and I asked Google Maps for directions to 3207 30th Ave. It confidently told me where to go but luckily my passenger noticed that it was actually directing me to 3207 <em>34th</em> Ave, four blocks north. Well that’s odd.</p>
<p>A few days later my cousin asked me (as the <a href="https://randomascii.wordpress.com/2024/10/01/life-death-and-retirement/">ex-Google</a> still-nerd member of the family) if I could help with a Google Maps issue. The problem was that the address 138 W 6th Ave in Vancouver was being mapped at a location 2.4 km (that’s 1.5 miles or 123 furlongs) away from the actual location.</p>

<p>I could visualize the absurdity of where it maps the W 6th Ave address by asking Google Maps for directions between 136 W 6th Ave and 138 W 6th Ave. These addresses are adjacent in real life, but Google Maps gave me this:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image.png"><img width="650" height="375" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb.png?w=650&amp;h=375"></a></p>
<p>That’s a long walk to get to the building next door.</p>
<p>There’s another fun way to visualize this bug. Search for “Clark &amp; Page Casting Studios” in Google Maps. Then copy its address, shown in Google Maps, to the clipboard and ask for directions <em>to</em> Clark &amp; Page Casting Studios <em>from</em> its address. This should be a zero-meter walk, but of course it isn’t. Instead it is, no surprise, a 2.4 km walk from Clark &amp; Page Casting Studios to its address. Fun!</p>
<p>Or this silliness. If you navigate from “138 W 6th Ave Unit 1B” to “138 W 6th Ave #2b” then it is, you guessed it, a 2.4 km walk.</p>
<p>This error was pointed out to me because apparently aspiring actors kept going to the wrong place and being late for their auditions. These mistakes have real-world consequences.</p>
<h2>There are more</h2>
<p>Finding one error is curious, but two suggests a pattern. I started browsing Google Maps looking for addresses that seemed out of place. I quickly found three more.</p>
<p>1951 W 19th Ave in Vancouver is mapped at a 2.1 km walk from where its address should logically be. It should be in the 1900 block of W 19th Ave but is instead placed ten blocks away by Google Maps:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-1.png"><img width="648" height="336" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-1.png?w=648&amp;h=336"></a></p>
<p>1355 W 17th Ave, North Vancouver is a particularly odd case because it is mapped as being in the wrong city (in Vancouver instead of North Vancouver), but on the right street (W 17th Ave) but in the wrong block (the 900 block instead of the 1300 block). As it turns out W 17th Ave doesn’t actually exist in North Vancouver. What is going on?</p>
<h2>Typos? Street View?</h2>
<p>The answer might be typos. 138 W 6th Ave is being mapped at the location where I would expect to find 1038 W 16th Ave located – a pair of single-digit errors. This requires that somebody/something made two errors when entering the address for 1038 W 16th Ave. The problem with this explanation is that 1038 W 16th Ave doesn’t exist – I cycled over there to check and the addresses go straight from 1020 to 1040.</p>
<p>3207 30th Ave in Vernon got a 30 changed to a 34. Maybe that was a typo?</p>
<p>1951 W 19th Ave is mapped where I would expect to find 951 W 19th Ave. This is another single-digit error. This one is less harmful because (again, I cycled over to check) there is no 1951 W 19th Ave, and 1951 and 951 W 19th Ave both map to roughly the same place. If you ask for directions from 951 to 1951 W 19th Ave (which should be ten blocks) you get these 0.0 km directions:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-2.png"><img width="605" height="393" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-2.png?w=605&amp;h=393"></a></p>
<p>1355 W 17th Ave, North Vancouver is harder to explain. It was mapped adjacent to 979 W 17th Ave, Vancouver. This error severely stretches the definition of “typo” since nothing but the street name is correct (Vancouver and North Vancouver are different cities, separated by Vancouver Harbour).</p>
<p>I also noticed an anomaly in 5 Montcalm St, Vancouver. This address is in the 1300 block of Montcalm so the address makes no sense. I visited this location as well and the building address is actually 1131 W 16th Ave (the house is on a corner) and there is a five on one of the doors on the Montcalm side. Further creeping around the house revealed that there are five units inside the house – the five is a unit number, not a street number! Now I started wondering if a person or AI had seen the five on the door on Montcalm St and assumed that it was an address.</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/pxl_20250424_173954222.jpg"><img width="640" height="453" title="PXL_20250424_173954222" alt="PXL_20250424_173954222" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/pxl_20250424_173954222_thumb.jpg?w=640&amp;h=453"></a></p>
<h2>Internals guesswork</h2>
<p>The fact that Google Maps can have these errors – that apparently the mapped location of addresses need have no relationship to the layout of the city’s streets – makes it clear that Google Maps has no concept of how street addresses work. There are many rules for how most addresses work in Vancouver but Google Maps appears to have no knowledge of these rules.</p>
<p>It appears that there is an address database somewhere – created by Google Maps, or the cities in BC, or perhaps from Street View data. Somehow that database seems to allow addresses to be mapped to parcels of land and when the address of a parcel of land is entered (by a human being or an AI bot) the database software happily accepts any address and maps it to the parcel, with no sanity checks to make sure it makes sense. Possibly sanity checks that are needed include:</p>
<ul>
<li>Is the parcel in the geographical bounds of the city name entered?</li>
<li>Is the parcel in the vicinity of the road name entered?</li>
<li>Is the parcel in the correct hundred block for the road name entered?</li>
</ul>
<p>These checks would detect all five of the errors that I found.</p>
<p>The hundred-block check only makes sense in some cities. In others it might be better to just do a comparison with nearby numbers, or perhaps skip that check completely. And there are enough weird addresses in the world that these checks probably just have to be a suggestion rather than a hard blocker.</p>
<p>Since there are apparently a lot of these bad addresses in the wild (my ability to find five errors in two cities this quickly suggests there must be many thousands) it seems that somebody needs to run a batch process over the database to find these errors – me scrolling through the map really doesn’t scale well.</p>
<p>While it seems clear that Google Maps uses an address database to map arbitrary addresses to parcels of land, it is also capable of guessing where an address would be if that address existed. That is, if I ask it to map the non-existent addresses 1953, 1955, 1957, 1959, and 1961 on W 19th Ave it places the address balloon in plausible locations, interpolating between 1947 and 1981 (the surrounding “real” addresses). This suggests that Google Maps has the knowledge and heuristics needed to correctly place 138 W 16th Ave, but this knowledge is then overridden by a database that contains errors. Fun!</p>
<h2>Something new?</h2>
<p>I talked to the business at 138 W 6th Ave and they said that these problems are new – starting around mid March. I don’t remember noticing this type of error before so it does seem like Google Maps might have just ingested a batch of bad data.</p>
<h2>Attempted fixes</h2>
<p>When I encountered the first two errors I confidently said that I’d use the Google Maps feedback tool to get the errors fixed. I’ve had good luck in the past with this. But this time my luck ran out.</p>
<p>I dutifully submitted feedback for “Wrong pin location or address”:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-3.png"><img width="516" height="281" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-3.png?w=516&amp;h=281"></a></p>
<p>And I got an email the next day saying that my edit was accepted:</p>
<p><a href="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image-4.png"><img width="562" height="299" title="image" alt="image" src="https://randomascii.wordpress.com/wp-content/uploads/2025/04/image_thumb-4.png?w=562&amp;h=299"></a></p>
<p>But it’s been 14 days and the address still maps incorrectly.</p>
<p>I had better luck with my edit to 3207 30th Ave that was accepted the same day. That fix actually went live sometime between April 17th and April 23rd. That is still nowhere near the promised 24-hour latency, but at least it showed up eventually. Maybe the 138 W 6th Ave edit will still go live?</p>
<h2>Not all errors are equal</h2>
<p>The first two errors that I found – 3207 30th Ave in Vernon and 138 W 6th Ave in Vancouver – are problematic because those addresses are real and Google Maps plots them incorrectly. This leads to people going to the wrong place.</p>
<p>The other errors are less important because they are non-existent addresses that are plotted in nonsensical places. This is mostly harmless.</p>
<h2>Anybody else seeing this?</h2>
<p>If you have noticed any similar anomalies then please share them in the comments.</p>
<p>If you work on Google Maps please <a href="https://bsky.app/profile/randomascii.bsky.social">reach out to me</a> if you have any information that you can share. I’ve tried reaching out through some ex-coworker friends, but no luck so far.</p>
<h2>Discussion</h2>
<p><a title="https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s" href="https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s">https://bsky.app/profile/randomascii.bsky.social/post/3lnlwmoayks2s</a></p>
<p><a title="https://news.ycombinator.com/item?id=43788832" href="https://news.ycombinator.com/item?id=43788832">https://news.ycombinator.com/item?id=43788832</a></p>
<p><a title="https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/" href="https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/">https://www.reddit.com/r/GoogleMaps/comments/1k77440/google_maps_doesnt_know_how_street_addresses_work/</a></p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft subtracts C/C++ extension from VS Code forks (191 pts)]]></title>
            <link>https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/</link>
            <guid>43788125</guid>
            <pubDate>Thu, 24 Apr 2025 22:18:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/">https://www.theregister.com/2025/04/24/microsoft_vs_code_subtracts_cc_extension/</a>, See on <a href="https://news.ycombinator.com/item?id=43788125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Microsoft's C/C++ extension for Visual Studio Code (VS Code) no longer works with derivative products such as VS Codium and Cursor – and some developers are crying foul.</p>
<p>In early April, programmers using VS Codium, an open-source fork of Microsoft's MIT-licensed <a target="_blank" rel="nofollow" href="https://github.com/microsoft/vscode">VS Code</a>, and Cursor, a commercial AI code assistant built from the VS Code codebase, noticed that the <a target="_blank" rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools">C/C++ extension</a> <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976">stopped</a> <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300">working</a>.</p>
<p>The extension adds C/C++ language support, such as Intellisense code completion and debugging, to VS Code. The removal of these capabilities from competing tools breaks developer workflows, hobbles the editor, and arguably hinders competition.</p>

    

<p>The breaking change appears to have occurred with the release of v1.24.5 on April 3, 2025.</p>

        


        

<p>Following the April update, attempts to install the C/C++ extension outside of VS Code generate this error message: "The C/C++ extension may be used only with Microsoft Visual Studio, Visual Studio for Mac, Visual Studio Code, Azure DevOps, Team Foundation Server, and successor Microsoft products and services to develop and test your applications."</p>
<p>Microsoft has forbidden the use of its extensions outside of its own software products <a target="_blank" rel="nofollow" href="https://github.com/microsoft/vscode-cpptools/commit/1a03dd2a1d37e41359d3f2352bd889e8059237bf">since at least September 2020</a>, when the current licensing terms were published. But it hasn't enforced those terms in its C/C++ extension with <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300#issuecomment-2779861379">an environment check</a> in its binaries until now.</p>

        

<p>(Microsoft's PyLance extension for Python coding <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300#issuecomment-2779864293">is said</a> to have exhibited this behavior for years, preventing its use in VS Code forks.)</p>
<blockquote>

<p>The latest releases of the specific extensions no longer work in Cursor or other non-MSFT editors</p>
</blockquote>
<p>Michael Truell, co-founder and CEO of Anysphere, which makes Cursor, said in the discussion thread two weeks ago that a temporary fix has been rolled out and a more permanent solution is planned.</p>
<p>"MSFT has a handful of extensions which are closed-source," he <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976#issuecomment-2787079188">wrote</a>, pointing to Remote Access, Pylance, C/C++, and C#. "The latest releases of the specific extensions no longer work in Cursor or other non-MSFT editors.</p>
<p>"Moving forward, Cursor is transitioning away from these extensions. We are investing in open-source alternatives which already exist in the community and will bundle these into the next version to enable a seamless transition."</p>
<p>Cursor <a target="_blank" rel="nofollow" href="https://github.com/getcursor/cursor/issues/2976#issuecomment-2782541940">allegedly</a> has been flouting Microsoft terms-of-service rules for some time now by setting up a reverse proxy to mask its network requests to the endpoints used by the Microsoft Visual Studio Marketplace. This allows Cursor users to install VS Code extensions from Microsoft's market. Other VS Code forks tend to point to <a target="_blank" rel="nofollow" href="https://github.com/eclipse/openvsx">Open VSX</a>, an alternative extension marketplace.</p>

        

<p>Truell did not respond to a request for comment.</p>
<ul>

<li><a href="https://www.theregister.com/2025/04/23/whats_worth_teaching_when_ai/">As ChatGPT scores B- in engineering, professors scramble to update courses</a></li>

<li><a href="https://www.theregister.com/2025/04/24/microsoft_mystery_folder_fix/">Microsoft mystery folder fix might need a fix of its own</a></li>

<li><a href="https://www.theregister.com/2025/04/24/ninite_rebuild_windows/">Ninite to win it: How to rebuild Windows without losing your mind</a></li>

<li><a href="https://www.theregister.com/2025/04/23/microsoft_365_copilot_agent_refresh/">Microsoft 365 Copilot gets a new crew, including Researcher and Analyst bots</a></li>
</ul>
<p>Meanwhile, users of VS Codium are <a target="_blank" rel="nofollow" href="https://github.com/VSCodium/vscodium/issues/2300">looking for</a> free (as in freedom) and open source alternatives.</p>
<p>Developers discussing the issue in Cursor's GitHub repo have noted that Microsoft <a target="_blank" rel="nofollow" href="https://x.com/code/status/1908207162322460710">recently rolled out</a> a competing AI software agent capability, <a target="_blank" rel="nofollow" href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">dubbed Agent Mode</a>, within its Copilot software.</p>
<p>One such developer who contacted us anonymously told <em>The Register</em> they sent a letter about the situation to the US Federal Trade Commission, asking them to probe Microsoft for unfair competition - alleging self-preferencing, bundling Copilot without a removal option, and blocking rivals like Cursor to lock users into its AI ecosystem.</p>
<p>Microsoft did not immediately respond to a request for comment. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists Develop Artificial Leaf, Uses Sunlight to Produce Valuable Chemicals (200 pts)]]></title>
            <link>https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/</link>
            <guid>43788053</guid>
            <pubDate>Thu, 24 Apr 2025 22:10:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/">https://newscenter.lbl.gov/2025/04/24/scientists-develop-artificial-leaf-that-uses-sunlight-to-produce-valuable-chemicals/</a>, See on <a href="https://news.ycombinator.com/item?id=43788053">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-56089" aria-label="Scientists Develop Artificial Leaf That Uses Sunlight to Produce Valuable Chemicals">
  <div>

    <div>

      
<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <div>
<h4>Key Takeaways</h4>
<ul>
<li><span>The Liquid Sunlight Alliance is a multi-institutional collaboration working to develop the tools needed to use energy from sunlight to produce liquid fuels.</span></li>
<li><span>Researchers built a perovskite and copper-based device that converts carbon dioxide into C</span><span>2</span><span> products </span><span>–</span><span> precursory chemicals of innumerable products in our everyday lives, from plastic polymers to jet fuel.</span></li>
<li><span>This proof-of-concept research opens new opportunities for energy research.</span></li>
</ul>
</div>
<p><span>Researchers from the Department of Energy’s Lawrence Berkeley National Laboratory (Berkeley Lab) along with international collaborators have brought us one step closer to harnessing the sun’s energy to convert carbon dioxide into liquid fuel and other valuable chemicals. In a recent </span><a href="https://www.nature.com/articles/s41929-025-01292-y" target="_blank" rel="noopener"><span>publication</span></a><span> in </span><i><span>Nature Catalysis</span></i><span>, the researchers debut a self-contained carbon-carbon (C2) producing system that combines the catalytic power of copper with </span><a href="https://www.energy.gov/eere/solar/perovskite-solar-cells#:~:text=Perovskites%20are%20a%20family%20of,as%20fuel%20cells%20and%20catalysts." target="_blank" rel="noopener"><span>perovskite</span></a><span>, a material used in photovoltaic solar panels. This advance builds on over 20 years of research and brings the scientific community one step closer to replicating the productivity of a green leaf in nature.&nbsp;</span></p>
<p><span>This work is part of a larger initiative, the Liquid Sunlight Alliance (</span><a href="https://www.liquidsunlightalliance.org/" target="_blank" rel="noopener"><span>LiSA</span></a><span>), which is a Fuels from Sunlight Energy Innovation Hub funded by the U.S. Department of Energy. Led by Caltech in close partnership with Berkeley Lab, LiSA brings together more than 100 scientists from national lab partners at SLAC and the National Renewable Energy Laboratory, and university partners at UC Irvine, UC San Diego, and the University of Oregon</span><span>. </span><span>Researchers involved in this multi-institutional collaboration have made advances in developing our understanding of and the tools needed to develop liquid fuels generated from sunlight, carbon dioxide, and water. (Learn more about the LiSA collaboration in this </span><a href="https://newscenter.lbl.gov/2024/08/29/five-ways-lisa-is-advancing-solar-fuels/"><span>roundup</span></a><span>, “Five Ways LiSA is Advancing Solar Fuels.”)</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white">
  <lbl-section-header title="">
      </lbl-section-header>
</lbl-container>

<lbl-container theme="white">
  <lbl-grid columns="2" layout="grid-card">

<lbl-grid-card layout="gallery-card" date="" text="" link-url="" link-target="" title="">

            <lbl-image slot="media" img-caption="Closeup of the perovskite and copper-based devices developed by a multi-institutional collaboration working to develop the tools needed to turn sunlight into liquid fuel." img-credit="(Credit: Marilyn Sargent/Berkeley Lab)">
        <img decoding="async" width="890" height="665" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/Gallery1_890x665px_XBD-202503-035-015.jpg" alt="A blue, gloved hands puts a perovskite and copper-based device in line with other copper-based devices." slot="media">      </lbl-image>

    
    
  
  
</lbl-grid-card>


<lbl-grid-card layout="gallery-card" date="" text="" link-url="" link-target="" title="">

            <lbl-image slot="media" img-caption="Artistic depiction of an artificial tree with copper nanoflowers wired to perovskite crystals. " img-credit="(Credit: Virgil Andrei)">
        <img decoding="async" width="500" height="500" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/ezgif.com-optimize.gif" alt="Artistic depiction of an artificial tree with copper nanoflowers wired to perovskite crystals." slot="media">      </lbl-image>

    
    
  
  
</lbl-grid-card>

  </lbl-grid>


</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>“Nature was our inspiration,” said Peidong Yang, a senior faculty scientist in Berkeley Lab’s Materials Sciences Division and UC Berkeley professor of chemistry and materials science and engineering involved in the published work. “We had to work on the individual components first, but when we brought everything together and realized that it was successful, it was a very exciting moment.”&nbsp;</span></p>
<p><span>To build a system that mimics photosynthesis, Yang and his team followed the natural processes that occur in the leaf of a plant. Each individual component of a leaf’s photosynthesizing elements had to be replicated and refined. Tapping into the decades’ worth of research, the scientists used lead halide perovskite photoabsorbers to imitate a leaf’s light-absorbing chlorophyll. And inspired by enzymes that regulate photosynthesis in nature, they designed electrocatalysts made of copper that resemble tiny flowers.</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white" wrapper-size="sm">
  <lbl-audio episode="" date="" title="" schema="{
    " @context":="" "http:="" schema.org",="" "@type":="" "audioobject",="" "description":="" "",="" "duration":="" "uploaddate":="" "name":="" "publisher":="" {="" "organization",="" "berkeley="" lab",="" "logo":="" "imageobject",="" "url":="" "https:="" www.buzzsprout.com="" 2206573="" episodes="" 17016953"="" }="" },="" "thumbnailurl":="" ""="" }"="" header-type="h2">
        <lbl-rich-text slot="rich-text">
          </lbl-rich-text>
          <lbl-button link-target="_blank" link-url="https://www.buzzsprout.com/2206573/episodes/17016953" text="View the transcript" slot="inline-go-btn" type="inline-go">
      </lbl-button>
      </lbl-audio>
</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>Previous experiments have successfully replicated photosynthesis through the use of biological materials, but this work incorporated an inorganic material, copper. While the selectivity of copper is lower than biological alternatives, the inclusion of copper presents a more durable, stable, and longer-lasting option for the artificial leaf system design.</span></p>
<p><span>Work led by researchers in the LiSA project developed the cathode and anode components of the new device. Instruments at Berkeley Lab’s </span><a href="https://foundry.lbl.gov/" target="_blank" rel="noopener"><span>Molecular Foundry</span></a><span> allowed Yang’s team to integrate the device with metal contacts. During the experiments in Yang’s lab, a solar simulator mimicking a consistently bright sun was used to test the selectivity of the new device.&nbsp;</span></p>
<p><span>Prior innovations across research groups enabled an organic oxidation reaction to take place in the photoanode chamber and created C2 products in the photocathode chamber. This breakthrough created a realistic artificial-leaf architecture in a device about the size of a postage stamp </span><span>–</span><span> it converts CO<sub>2</sub> into a C2 molecule using only sunlight.&nbsp;</span></p>
<p><span>The C2 chemicals produced from this device are precursory ingredients for many industries that produce valuable products in our everyday lives </span><span>–</span><span> from plastic polymers to fuel for larger vehicles that can’t yet run off a battery, like an airplane. Building upon this fundamental research milestone, Yang is now aimed to increase the system’s efficiency and expand the size of the artificial leaf to begin increasing the scalability of the solution.&nbsp;</span></p>
  </lbl-rich-text>
</lbl-container>



<lbl-container theme="white">
  <lbl-image img-caption="Lin uses an artificial light to activate the postage stamp-sized device to convert carbon dioxide into a C2, a valuable precursory chemical in everyday products." img-credit="(Credit: Marilyn Sargent/Berkeley Lab)">
    <img loading="lazy" decoding="async" width="1190" height="795" src="https://newscenter.lbl.gov/wp-content/uploads/2025/04/Newscenter_1190px_XBD-202503-035-017.jpg" alt="" slot="media">  </lbl-image>
</lbl-container>



<lbl-container wrapper-size="sm" theme="white">
  <lbl-rich-text>
    <p><span>The </span><a href="https://foundry.lbl.gov/" target="_blank" rel="noopener"><span>Molecular Foundry</span></a><span> is a user facility at Berkeley Lab.&nbsp;</span></p>
<p><span>This work was supported by the </span><a href="https://www.energy.gov/science/office-science" target="_blank" rel="noopener"><span>DOE Office of Science</span></a><span>.</span></p>
<p>###</p>
<p><a href="https://www.lbl.gov/" target="_blank" rel="noopener"><span>Lawrence Berkeley National Laboratory</span></a><span> (Berkeley Lab) is committed to groundbreaking research focused on discovery science and solutions for abundant and reliable energy supplies. The lab’s expertise spans materials, chemistry, physics, biology, earth and environmental science, mathematics, and computing. Researchers from around the world rely on the lab’s world-class scientific facilities for their own pioneering research. Founded in 1931 on the belief that the biggest problems are best addressed by teams, Berkeley Lab and its scientists have been recognized with 16 Nobel Prizes. Berkeley Lab is a multiprogram national laboratory managed by the University of California for the U.S. Department of Energy’s Office of Science.&nbsp;</span></p>
<p><span>DOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit <a href="http://energy.gov/science" target="_blank" rel="noopener">energy.gov/science</a>.</span></p>
  </lbl-rich-text>
</lbl-container>

    </div><!-- .entry-content -->

    

<!-- content-single-bottom -->

      <lbl-container no-container-padding-top="" theme="white">
      <lbl-divider></lbl-divider>
    </lbl-container>
  
  <lbl-container wrapper-size="sm" theme="white">

    <!-- /. post-tags -->

  </lbl-container>


  <lbl-container theme="cloud">
    <lbl-section-header layout="centered" margin-bottom="" text="You might also be interested in:">
    </lbl-section-header>
    <lbl-grid columns="3" layout="grid-card">

      <lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=55099" title="Five Ways LiSA is Advancing Solar Fuels">
      <lbl-image slot="media">
      <img width="890" height="665" src="https://newscenter.lbl.gov/wp-content/uploads/2024/08/Newscenter_ALTLanding_890x665px_XBD-202406-103-032.jpg" alt="Two researchers in lab coats and goggles work with outdoor scientific equipment near a modern building." slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="article" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=article" aria-label="View all articles of type: Article">Article</a></li><li><a href="https://newscenter.lbl.gov/all-news/?topic=152620" aria-label="View all articles of type: Alternative Energy">Alternative Energy</a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=55099" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>
<lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=47329" title="Berkeley Lab Part of Multi-Institutional Team Awarded $60M for Solar Fuels Research">
      <lbl-image slot="media">
      <img width="890" height="623" src="https://newscenter.lbl.gov/wp-content/uploads/2020/07/JCAP_FundingRenewal-v3-1200px.png" alt="LiSA JCAP renewal solar fuels hub" slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=" aria-label="View all articles of type: "></a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=47329" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>
<lbl-grid-card link-target="_self" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=52821" title="How a Record-Breaking Copper Catalyst Converts CO2 Into Liquid Fuels">
      <lbl-image slot="media">
      <img width="890" height="611" src="https://newscenter.lbl.gov/wp-content/uploads/2023/02/copper-nanoparticle-homepage-1720x1180-1.jpg" alt="Artist’s rendering of a copper nanoparticle life cycle during CO2 electrolysis: Copper nanoparticles (left) combine into larger metallic copper “nanograins” (right) within seconds of the electrochemical reaction, reducing CO2 into new multicarbon products." slot="media" decoding="async" loading="lazy">    </lbl-image>
    <lbl-tags icon="article" slot="byline"><ul><li><a href="https://newscenter.lbl.gov/all-news/?type=article" aria-label="View all articles of type: Article">Article</a></li><li><a href="https://newscenter.lbl.gov/all-news/?topic=152612" aria-label="View all articles of type: Carbon Management">Carbon Management</a></li></ul></lbl-tags>  <lbl-button slot="btn" link-url="https://newscenter.lbl.gov/?post_type=post&amp;p=52821" text="Read the article" link-target="_self" type="inline">
  </lbl-button>
</lbl-grid-card>

    </lbl-grid>
  </lbl-container>

  </div><!-- .post-content-->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[National Airspace System Status (173 pts)]]></title>
            <link>https://nasstatus.faa.gov/</link>
            <guid>43787730</guid>
            <pubDate>Thu, 24 Apr 2025 21:31:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nasstatus.faa.gov/">https://nasstatus.faa.gov/</a>, See on <a href="https://news.ycombinator.com/item?id=43787730">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[You Can Be a Great Designer and Be Completely Unknown (200 pts)]]></title>
            <link>https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown</link>
            <guid>43787676</guid>
            <pubDate>Thu, 24 Apr 2025 21:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown">https://www.chrbutler.com/you-can-be-a-great-designer-and-be-completely-unknown</a>, See on <a href="https://news.ycombinator.com/item?id=43787676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

<p>
I often find myself contemplating the greatest creators in history — those rare artists, designers, and thinkers whose work transformed how we see the world. What constellation of circumstances made them who they were? Where did their ideas originate? Who mentored them? Would history remember them had they lived in a different time or place?
</p>
<p>
Leonardo da Vinci stands as perhaps the most singular creative mind in recorded history — the quintessential “Renaissance Man” whose breadth of curiosity and depth of insight seem almost superhuman. Yet examples like Leonardo can create a misleading impression that true greatness emerges only once in a generation or century. Leonardo lived among roughly 10-13 million Italians — was greatness truly as rare as one in ten million? We know several of his contemporaries, but still, the ratio remains vanishingly small. This presents us with two possibilities: either exceptional creative ability is almost impossibly rare, or greatness is more common than we realize and the rarity is recognition.
</p>
<p>
I believe firmly in the latter. Especially today, when we live in an attention economy that equates visibility with value. Social media follower counts, speaking engagements, press mentions, and industry awards have become the measuring sticks of design success. This creates a distorted picture of what greatness in design actually means. The truth is far simpler and more liberating: you can be a great designer and be completely unknown.
</p>
<p>
The most elegant designs often fade into the background, becoming invisible through their perfect functionality. Day to day life is scattered with the artifacts of unrecognized ingenuity — the comfortable grip of a vegetable peeler, the intuitive layout of a highway sign, or the satisfying click of a well-made light switch. These artifacts represent design excellence precisely because they don’t call attention to themselves or their creators. Who is responsible for them? I don’t know. That doesn’t mean they’re not out there.
</p>
<p>
This invisibility extends beyond physical objects. The information architect who structures a medical records system that saves lives through its clarity and efficiency may never receive public recognition. The interaction designer who simplifies a complex government form, making essential services accessible to vulnerable populations, might never be celebrated on design blogs or win prestigious awards.
</p>
<p>
Great design isn’t defined by who knows your name, but by how well your work serves human needs. It’s measured in the problems solved, the frustrations eased, the moments of delight created, and the dignity preserved through thoughtful solutions. These metrics operate independently of fame or recognition.
</p>
<p>
Our obsession with visibility also creates a troubling dynamic: design that prioritizes being noticed over being useful. This leads to visual pollution, cognitive overload, and solutions that serve the designer’s portfolio more than the user’s needs. When recognition becomes the goal, the work itself often suffers. I was among the few who didn’t immediately recoil at the brash aesthetics of the Tesla Cybertruck, but it turns out that no amount of exterior innovation changes the fact that it is just not a good truck.
</p>
<p>
There’s something particularly authentic about unknown masters — those who pursue excellence for its own sake, refining their craft out of personal commitment rather than in pursuit of accolades. They understand that their greatest achievements might never be attributed to them, and they create anyway. Their satisfaction comes from the integrity of the work itself.
</p>
<p>
This isn’t to dismiss the value of recognition when it’s deserved, or to suggest that great designers shouldn’t be celebrated. Rather, it’s a reminder that the correlation between quality and fame is weak at best, and that we should be suspicious of any definition of design excellence that depends on visibility. This is especially so today. The products of digital and interaction design are mayflies; most of what we make is lost to the rapid churn of the industry before it can even be lost to anyone’s memory.
</p>
<p>
The next time you use something that works so well you barely notice it, remember that somewhere, a designer solved a problem so thoroughly that both the problem and its solution became invisible. That designer might not be famous, might not have thousands of followers, might not be invited to speak at conferences — but they’ve achieved something remarkable: greatness through invisibility.
</p>
<p>
Design greatness is not measured by the recognition of authorship, but in the creation of work so essential it becomes as inevitable as gravity, as unremarkable as air, and as vital as both.
</p>
        
        
        <hr>
        <p><span color="grey"><small>Written by Christopher Butler on</small></span></p><p>April 24, 2025</p> &nbsp;
        
        
        <p><span color="grey"><small>Tagged</small></span></p><a href="https://www.chrbutler.com/tagged/essays"><p>Essays</p></a>
        <hr>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[People say they’ll pay more for “made in the USA” so we ran a test (139 pts)]]></title>
            <link>https://afina.com/blogs/news/made-in-usa</link>
            <guid>43787647</guid>
            <pubDate>Thu, 24 Apr 2025 21:21:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://afina.com/blogs/news/made-in-usa">https://afina.com/blogs/news/made-in-usa</a>, See on <a href="https://news.ycombinator.com/item?id=43787647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MainContent" role="main" tabindex="-1">
      <section id="shopify-section-template--16168843804849__main">

<article itemscope="" itemtype="http://schema.org/BlogPosting"><header>
            
          
        </header>



  <div itemprop="articleBody">
    <p><em>When we priced a U.S.-made version of our flagship product 85% higher than our Chinese-made one, 25,650 customers had the chance to vote with their wallets. Here’s what happened.</em><br></p>
<p>As small business owners, we’ve heard it a thousand times:</p>
<p><em>“I’d gladly pay more to support American-made.”</em></p>
<p><strong>We wanted to believe it. So we put it to the test.</strong></p>
<div><meta charset="utf-8"><meta charset="utf-8"><p>We make filtered showerheads. Clean, sleek design. But more importantly, with the best shower filters on the market.&nbsp;</p><p>Our bestselling model—manufactured in Asia (China and Vietnam)—sells for $129. But this year, as tariffs jumped from 25% to 170%, we wondered: Could we reshore manufacturing to the U.S. while maintaining margins to keep our lights on?</p><p><em>An important part to mention is that our most filter materials (KDF-55) is sourced from the US. So technically we partly source from Asia.&nbsp;</em></p></div>
<p>We found a U.S.-based supplier. The new unit cost us nearly 3x more to produce. To maintain our margins, we’d have to sell it for $239.</p>
<p><strong>So we ran an experiment.</strong></p>
<p>We created a secret landing page. The product and design were identical. The only difference? One was labeled “Made in Asia” and priced at $129. The other, “Made in the USA,” at $239.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/2025-04-23_15-28-10.jpg?v=1745440146"></p>
<p>The visitors were given the choice to either buy the Made in USA or the Made in Asia version.&nbsp;</p>
<p><strong>The results were sobering.</strong></p>
<p>Add-to-carts for the U.S. version were only 24! <strong>Conversion? 0.0% (zero).</strong> <br>Not a single customer purchased the Made-in-USA version.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/output_2.png?v=1745517906"></p>
<p>We tested everything: color, copy, layout. We ran it over multiple days and traffic sources. Same outcome every time.</p>
<p>For a moment, we thought we’d made a technical error. We hadn’t.</p>
<p>This wasn’t a failure of marketing—it was a referendum on price.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/2025-04-23_15-33-08.jpg?v=1745440456" width="467" height="461"></p>
<p>We wanted to believe customers would back American labor with their dollars. But when faced with a real decision—not a survey or a comment section—they didn’t.</p>
<p>It’s not their fault. Most people are stretched. They’re feeling inflation everywhere: gas, groceries, mortgages. “Supporting U.S. manufacturing” becomes a luxury most can’t afford—even if they want to.</p>
<p>This isn’t just our problem—it’s the economy’s.</p>
<p>Small brands like ours want to manufacture here. We’re willing to invest. But without serious shifts—in consumer incentives, automation, and trade policy—the math doesn’t work. Not for us. Not for our customers.</p>
<p>We’re still committed to exploring local manufacturing. But for now, it’s not viable.</p>
<p>We’re sharing this because the numbers surprised even us. And we think they’re worth talking about.</p>
<p>If policymakers and pundits want to rebuild American industry, they need to grapple with this truth: idealism doesn’t always survive contact with a price tag.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0628/8172/6641/files/output_1.png?v=1745517834"></p>
<p><strong>Ramon van Meer</strong><br>Founder - Afina<br>ramon@afina.com&nbsp;<br>925-548-7758</p>


    

    
  </div>
</article>






</section><div id="shopify-section-template--16168843804849__c187b5fa-a3c0-429b-981a-ff8885432f90">
      <div>
        
        <p>Meet the Afina Filtered<br>Showerhead</p>
        
        
      </div>
      <div>
        
        <p><img src="https://afina.com/cdn/shop/files/Afina-0066-7.png?v=1701271069">
        </p>
        
      </div>
      <div>
        <h2 data-cascade="">
          <p>Filters out 98% of harmful substances in your water</p>
        </h2>
        <p data-cascade=""><a href="https://afina.com/products/a-01-filtered-shower-head">SHOP NOW<svg width="11" height="18" viewBox="0 0 11 18" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.44251 0.820312L9.33984 8.71764L1.44251 16.615" stroke="black" stroke-width="1.31622"></path></svg>
            </a></p>
      </div>
    </div><div id="shopify-section-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190"><p>
        <h2 id="SectionHeading-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190" data-cascade="">
          Recent Article
        </h2></p><slider-component>
      <ul id="Slider-template--16168843804849__8f50fba5-3dff-4371-b88e-4032bdc7a190" role="list">
          
                    
              
</ul>
      
        
      
    </slider-component></div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI releases image generation in the API (430 pts)]]></title>
            <link>https://openai.com/index/image-generation-api/</link>
            <guid>43786506</guid>
            <pubDate>Thu, 24 Apr 2025 19:27:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/image-generation-api/">https://openai.com/index/image-generation-api/</a>, See on <a href="https://news.ycombinator.com/item?id=43786506">Hacker News</a></p>
Couldn't get https://openai.com/index/image-generation-api/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[NSF director to resign amid grant terminations, job cuts, and controversy (275 pts)]]></title>
            <link>https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy</link>
            <guid>43786304</guid>
            <pubDate>Thu, 24 Apr 2025 19:07:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy">https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy</a>, See on <a href="https://news.ycombinator.com/item?id=43786304">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/nsf-director-resign-amid-grant-terminations-job-cuts-and-controversy: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I reverse engineered top websites to build an animated UI library (149 pts)]]></title>
            <link>https://reverseui.com</link>
            <guid>43785464</guid>
            <pubDate>Thu, 24 Apr 2025 17:47:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reverseui.com">https://reverseui.com</a>, See on <a href="https://news.ycombinator.com/item?id=43785464">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><div><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="M8.00065 14.1667C10.8541 14.1667 13.1673 11.6667 13.1673 8.66667C13.1673 5.222 10.0918 2.78077 8.85307 1.9372C8.56929 1.74395 8.19435 1.82757 7.99678 2.10835L6.38863 4.39386C6.15096 4.73163 5.66501 4.77276 5.37203 4.48167C5.11194 4.22325 4.68797 4.22046 4.45454 4.5032C3.37417 5.81171 2.83398 7.44093 2.83398 8.66667C2.83398 11.6667 5.14718 14.1667 8.00065 14.1667ZM8.00065 14.1667C9.10522 14.1667 10.0007 13.1446 10.0007 11.8838C10.0007 10.495 8.89191 9.48319 8.32548 9.05691C8.13084 8.91044 7.87046 8.91044 7.67583 9.05691C7.1094 9.48319 6.00065 10.495 6.00065 11.8838C6.00065 13.1446 6.89608 14.1667 8.00065 14.1667Z" stroke="#FCFBFA" stroke-linejoin="round" stroke-width="1.5"></path></svg> <!-- --><p>28</p><!-- --><p> components available</p></div><div><h2>Reverse engineered UI library from the best sites on the web</h2><p>Effortlessly integrate trending animated components to your projects, with all the styling and animations handled for you. <br></p><div><p><a href="#components-section">Browse components</a></p><div><a href="https://reverseui.com/pricing">Get lifetime access ($50.00)<svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></a><div><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="m256 101 38.8-62.03C309.9 14.73 336.5 0 365.1 0h2.9c44.2 0 80 35.82 80 80 0 18.01-6 34.6-16 48h32c26.5 0 48 21.5 48 48v64c0 20.9-13.4 38.7-32 45.3V448c0 35.3-28.7 64-64 64H96c-35.35 0-64-28.7-64-64V285.3C13.36 278.7 0 260.9 0 240v-64c0-26.5 21.49-48 48-48h31.99C69.95 114.6 64 98.01 64 80c0-44.18 35.82-80 80-80h2.9c28.6 0 55.2 14.73 70.3 38.97L256 101zm109.1-69c-17.6 0-33.9 9.04-43.2 23.93l-45 72.07H368c26.5 0 48-21.5 48-48 0-26.51-21.5-48-48-48h-2.9zm-130 96-45-72.07A50.886 50.886 0 0 0 146.9 32H144c-26.5 0-48 21.49-48 48 0 26.5 21.5 48 48 48h91.1zM48 160c-8.84 0-16 7.2-16 16v64c0 8.8 7.16 16 16 16h192v-96H48zm224 96h192c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16H272v96zm-32 32H64v160c0 17.7 14.33 32 32 32h144V288zm32 192h144c17.7 0 32-14.3 32-32V288H272v192z"></path></svg><p>$50 off</p> <!-- --><p>for limited time</p></div></div></div></div><div id="components-section"><div><div><a href="https://reverseui.com/craft/logo-dots-shader"><div><div><p>Logo Dots Logo</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/bot-protection"><div><div><p>Bot Protection</p><p>March 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/glowing-orb"><div><div><p>Glowing Orb</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/sms-alert"><div><div><p>SMS Alert</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/interactive-envelope"><div><div><p>Interactive Envelope</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/social-links"><div><div><p>Social Links</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/rainbow-shine-button"><div><div><p>Rainbow Shine Button</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/track-invoices"><div><div><p>Track Invoices</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/orbit-rings"><div><div><p>Orbit Rings</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/dots-shader"><div><div><p>Dots Shader</p><p>May 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div><div><div><a href="https://reverseui.com/craft/role-based-access-control"><div><div><p>Role-Based Access Control</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/multifactor-authentication"><div><div><p>Multifactor Authentication</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/ai-chat"><div><div><p>AI Chat</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/fingerprint-scan"><div><div><p>Fingerprint Scan</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/logs-explorer"><div><div><p>Logs Explorer</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/text-blur-reveal"><div><div><p>Text Blur Reveal</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/hex-outline"><div><div><p>Hex Outline</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/operating-systems"><div><div><p>Operating Systems</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/stock-chart"><div><div><p>Stock Chart</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div><div><div><a href="https://reverseui.com/craft/like-button"><div><div><p>Like Button</p><p>April 2025</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/macbook-keyboard"><div><div><p>Macbook Keyboard</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/retro-terminal"><div><div><p>Retro Terminal</p><p>November 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/data-feeding-in"><div><div><p>Data Feeding In</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/caesar-cipher"><div><div><p>Caesar Cipher</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/signature"><div><div><p>Signature</p><p>October 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/cool-badge"><div><div><p>Cool Badge</p><p>September 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/command-k"><div><div><p>Command K</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div><div><a href="https://reverseui.com/craft/shiny-button"><div><div><p>Shiny Button</p><p>June 2024</p></div><div><p>View Component</p><svg width="16px" height="16px" stroke-width="1.5" viewBox="0 0 24 24" fill="none" color="currentColor"><path d="M6 12h12.5m0 0l-6-6m6 6l-6 6" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></a></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Lemon Slice Live – Have a video call with a transformer model (159 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43785044</link>
            <guid>43785044</guid>
            <pubDate>Thu, 24 Apr 2025 17:10:14 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43785044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN, this is Lina, Andrew, and Sidney from Lemon Slice. We’ve trained a custom diffusion transformer (DiT) model that achieves video streaming at 25fps and wrapped it into a demo that allows anyone to turn a photo into a real-time, talking avatar. Here’s an example conversation from co-founder Andrew: <a href="https://www.youtube.com/watch?v=CeYp5xQMFZY" rel="nofollow">https://www.youtube.com/watch?v=CeYp5xQMFZY</a>. Try it for yourself at: <a href="https://lemonslice.com/live">https://lemonslice.com/live</a>.</p><p>(Btw, we used to be called Infinity AI and did a Show HN under that name last year: <a href="https://news.ycombinator.com/item?id=41467704">https://news.ycombinator.com/item?id=41467704</a>.)</p><p>Unlike existing avatar video chat platforms like HeyGen, Tolan, or Apple Memoji filters, we do not require training custom models, rigging a character ahead of time, or having a human drive the avatar. Our tech allows users to create and immediately video-call a custom character by uploading a single image. The character image can be any style - from photorealistic to cartoons, paintings, and more.</p><p>To achieve this demo, we had to do the following (among other things! but these were the hardest):</p><p>1. Training a fast DiT model. To make our video generation fast, we had to both design a model that made the right trade-offs between speed and quality, and use standard distillation approaches. We first trained a custom video diffusion transformer (DiT) from scratch that achieves excellent lip and facial expression sync to audio. To further optimize the model for speed, we applied teacher-student distillation. The distilled model achieves 25fps video generation at 256-px resolution. Purpose-built transformer ASICs will eventually allow us to stream our video model at 4k resolution.</p><p>2. Solving the infinite video problem. Most video DiT models (Sora, Runway, Kling) generate 5-second chunks. They can iteratively extend it by another 5sec by feeding the end of the 1st chunk into the start of the 2nd in an autoregressive manner. Unfortunately the models experience quality degradation after multiple extensions due to accumulation of generation errors. We developed a temporal consistency preservation technique that maintains visual coherence across long sequences. Our technique significantly reduces artifact accumulation and allows us to generate indefinitely-long videos.</p><p>3. A complex streaming architecture with minimal latency. Enabling an end-to-end avatar zoom call requires several building blocks, including voice transcription, LLM inference, and text-to-speech generation in addition to video generation. We use Deepgram as our AI voice partner. Modal as the end-to-end compute platform. And Daily.co and Pipecat to help build a parallel processing pipeline that orchestrates everything via continuously streaming chunks. Our system achieves end-to-end latency of 3-6 seconds from user input to avatar response. Our target is &lt;2 second latency.</p><p>More technical details here: <a href="https://lemonslice.com/live/technical-report">https://lemonslice.com/live/technical-report</a>.</p><p>Current limitations that we want to solve include: (1) enabling whole-body and background motions (we’re training a next-gen model for this), (2) reducing delays and improving resolution (purpose-built ASICs will help), (3) training a model on dyadic conversations so that avatars learn to listen naturally, and (4) allowing the character to “see you” and respond to what they see to create a more natural and engaging conversation.</p><p>We believe that generative video will usher in a new media type centered around interactivity: TV shows, movies, ads, and online courses will stop and talk to us. Our entertainment will be a mixture of passive and active experiences depending on what we’re in the mood for. Well, prediction is hard, especially about the future, but that’s how we see it anyway!</p><p>We’d love for you to try out the demo and let us know what you think! Post your characters and/or conversation recordings below.</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenVSX, which VSCode forks rely on for extensions, down for 24 hours (210 pts)]]></title>
            <link>https://status.open-vsx.org/</link>
            <guid>43785039</guid>
            <pubDate>Thu, 24 Apr 2025 17:09:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.open-vsx.org/">https://status.open-vsx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=43785039">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>
<span>Updated</span>
<span>Apr 23 at 11:31am EDT</span>
</p>
<p>We are working to resolve an issue with our backend storage that is preventing the service from starting correctly</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manufactured Consensus on X.com (298 pts)]]></title>
            <link>https://rook2root.co/articles/20250424-manufacturing-consensus-on-x</link>
            <guid>43784915</guid>
            <pubDate>Thu, 24 Apr 2025 16:57:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rook2root.co/articles/20250424-manufacturing-consensus-on-x">https://rook2root.co/articles/20250424-manufacturing-consensus-on-x</a>, See on <a href="https://news.ycombinator.com/item?id=43784915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Influential users and recommendation algorithm design quietly shape what people see, what gains attention, and what gets silenced.</p>
<p>When an account with 219 million followers interacts with a smaller one — not by blocking or arguing, but simply by muting — the consequences are immediate. The smaller account’s visibility drops from 150,000 views to 20,000 overnight. No notice. No rule broken. Dimmed into irrelevance.</p>
<p>It’s a form of shadowbanning — not imposed by moderators, but activated by the algorithm in response to a high-weight engagement signal.</p>

<p>On the flip side, the same signal that suppresses can also elevate. When a high-reach account interacts — sometimes with nothing more than a vague comment or a repost — the algorithm reads it as endorsement. Content is boosted, visibility spikes, and narratives take flight.</p>
<p>Even low-effort, repetitive interactions—likes, generic replies—can act like a controlled dose of AstroBoost™ - just enough to simulate momentum and trigger amplification.</p>


<p><a target="_blank" rel="noopener noreferrer" href="https://rook2root.co/library/user-influence-and-retention-engineering/perception-management-and-influence/social-proof-manipulation">Social proof</a> used to reflect crowd wisdom. Now it reflects algorithmic endorsement — triggered not by consensus, but by proximity to influence. A single interaction can distort scale, making selected content appear widely supported.</p>
<p>The result? Artificial popularity. Boosted narratives. Organic ideas buried by engineered reach. The crowd didn’t pick it—the algorithm did, based on who touched it.</p>
<p><em>It’s not fraud. It’s influence infrastructure.</em></p>
<h2 id="perception-cascades">Perception Cascades</h2>
<p>Nothing needs to be removed or blocked. Often, content is simply deprioritized—pushed lower in the feed, placed outside key visibility zones, or displaced by fresher signals. The mechanisms are subtle, the outcomes consistent: lower reach, reduced visibility, diminished presence.</p>
<p>Meanwhile, amplification flows downstream. A single high-weight interaction can trigger a cascade—surfacing aligned content, prompting engagement across similar accounts, and reinforcing the same narrative through repetition.</p>
<p>What people see feels organic. In reality, they’re engaging with what’s already been filtered, ranked, and surfaced.</p>
<h2 id="astroturfing-20">Astroturfing 2.0</h2>
<p>There’s no need to simulate support when the platform itself is the amplifier. Traditional <a target="_blank" rel="noopener noreferrer" href="https://rook2root.co/library/user-influence-and-retention-engineering/perception-management-and-influence/astroturfing">astroturfing</a> relied on fake accounts and bots. Today, manufactured consensus is powered by real users—but selected ones. Elite accounts trigger the machine. Everyone else gets pulled into the ripple.</p>
<p>This isn’t about faking the crowd — it’s about guiding it. Real users, real engagement, selectively amplified to create the illusion of widespread agreement. Consensus is just what survived the feed.</p>
<h2 id="seen-and-unseen">Seen and Unseen</h2>
<p>Perception shaped at scale doesn’t just change what people see—it changes how they vote, what they buy, what they protest, and what they ignore. It doesn’t just distort attention. It steers outcomes.</p>
<p>Truth isn’t what’s real — it’s what’s shown.</p>
<p>What’s not shown might as well not exist.</p>
<p>And if you think this only happens on one social network, you’re already caught in the wrong attention loop.</p>
<h2 id="post-scriptum-the-loud-ones-always-fall-first">Post Scriptum: The Loud Ones Always Fall First</h2>
<p>The most effective influence doesn’t announce itself. It doesn’t censor loudly, or boost aggressively. It shapes perception quietly — one algorithmic nudge at a time.</p>
<p>The ones who try to control everything too openly, too quickly, get <a target="_blank" rel="noopener noreferrer" href="https://digital-strategy.ec.europa.eu/en/news/commission-addresses-additional-investigatory-measures-x-ongoing-proceedings-under-digital-services/">caught</a>.
It’s not the blunt force authoritarians who endure. It’s the subtle ones. The ones who let people believe they chose freely — while feeding them only curated choices.</p>
<hr>
<p><em>At rook2root.co we expose the tactics no one talks about. Not to preach, but to illuminate. <a target="_blank" rel="noopener noreferrer" href="https://rook2root.beehiiv.com/subscribe">subscribe</a> to get new articles delivered by mail.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[One quantum transition makes light at 21 cm (237 pts)]]></title>
            <link>https://bigthink.com/starts-with-a-bang/21cm-magic-length/</link>
            <guid>43784721</guid>
            <pubDate>Thu, 24 Apr 2025 16:38:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bigthink.com/starts-with-a-bang/21cm-magic-length/">https://bigthink.com/starts-with-a-bang/21cm-magic-length/</a>, See on <a href="https://news.ycombinator.com/item?id=43784721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div x-data="prose">

                    
<div>
                            <p>
                    Sign up for the Starts With a Bang newsletter              </p>
                                            <p>
                    Travel the universe with Dr. Ethan Siegel as he answers the biggest questions of all.         </p>
                        </div>
<!--?xml encoding="utf-8" ?--><p>In our Universe, quantum transitions are the governing rule behind every nuclear, atomic, and molecular phenomenon. Unlike the planets in our Solar System, which could stably orbit the Sun at any distance if they possessed the right speed, the protons, neutrons, and electrons that make up all the conventional matter we know of can only bind together in a specific set of configurations. These possibilities, although numerous, are finite in number, as the quantum rules that govern electromagnetism and the nuclear forces restrict how atomic nuclei and the electrons that orbit them can arrange themselves.</p><p>In all the Universe, the most common atom of all is hydrogen, with just one proton and one electron. Wherever new stars form, hydrogen atoms become ionized, becoming neutral again if those free electrons can find their way back to a free proton. Although the electrons will typically cascade down the allowed energy levels into the ground state, that normally produces only a specific set of infrared, visible, and ultraviolet light. But more importantly, a special transition occurs in hydrogen that produces light of about the size of your hand: 21 centimeters (about 8¼”) in wavelength. Even as a physicist, you’d be well justified to call this the “magic length” of our Universe, as it just might someday unlock the darkest secrets hiding out in the deepest cosmic recesses from which starlight will never escape.</p><!--?xml encoding="utf-8" ?--><figure><img fetchpriority="high" decoding="async" width="638" height="479" src="https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg" alt="" sizes="(max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg 638w, https://bigthink.com/wp-content/uploads/2022/12/cosmology-with-the-21cm-line-3-638.jpg?resize=375,282 375w"><div><div><p>Backlit by the cosmic microwave background, a cloud of neutral gas can imprint a signal on that radiation at a specific wavelength and redshift. If we can measure this light with great enough sensitivity, we can actually hope to someday map out the locations and densities of gas clouds in the Universe thanks to the science of 21 cm astronomy. A dip in brightness temperature at redshifts of 15-20, observed in 2018, may be due to exactly the effect of 21-cm emission, although better instrumentation and better observational examples will be required to confirm such a claimed detection.
</p></div><figcaption><a href="https://www.slideshare.net/CosmoAIMS/cosmology-with-the-21cm-line" target="_blank">Credit</a>: Gianni Bernardi, via his AIMS talk
</figcaption></div></figure><p>When it comes to the light in the Universe, wavelength is the one property that you can count on to reveal how that light was created. Even though light comes to us in the form of photons — individual quanta that, collectively, make up the phenomenon we know as light — there are two very different classes of quantum process that create the light that surrounds us: continuous ones and discrete ones.</p><p>A continuous process is something like the light emitted by the photosphere of the Sun. It’s a dark object that’s been heated up to a certain temperature, and it radiates light of all different, continuous wavelengths as dictated by that temperature: what physicists know as blackbody radiation. More accurately, because the different layers of the photosphere are at different temperatures, the solar spectrum acts like a series of blackbodies all summed together: an amalgam of continuous processes.</p><p>A discrete process, however, doesn’t allow for the emission of light of a continuous set of wavelengths, but rather only at extremely specific, or discrete (and quantized), wavelengths. A good example of that is the light absorbed by the neutral atoms present within the extreme outer layers of the Sun. As the blackbody radiation from the lower layers of the photosphere strikes those neutral atoms sitting at the surface, a few of those photons will have just the right wavelengths to be absorbed by the electrons within the neutral atoms they encounter. When we break sunlight up into its individual wavelengths, the various absorption lines present against the backdrop of continuous, blackbody radiation reveal both of these processes to us.</p><!--?xml encoding="utf-8" ?--><figure><img decoding="async" width="8192" height="5464" src="https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?w=8192" alt="" sizes="(max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg 8192w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1536,1025 1536w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=2048,1366 2048w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=20,12 20w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=375,250 375w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=640,427 640w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=768,512 768w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1024,683 1024w, https://bigthink.com/wp-content/uploads/2022/02/solarspectrum.jpg?resize=1280,854 1280w"><div><div><p>The visible light spectrum of the Sun, which helps us understand not only its temperature and ionization, but the abundances of the elements present. The long, thick lines are hydrogen and helium, but every other line is from a heavy element that must have been created in a previous-generation star, rather than the hot Big Bang.
</p></div><figcaption><a href="https://solarsystem.nasa.gov/resources/390/the-solar-spectrum/" target="_blank">Credit</a>: N.A.Sharp, NOAO/NSO/Kitt Peak FTS/AURA/NSF
</figcaption></div></figure><p>Each individual atom has its properties primarily defined by its nucleus, made up of protons (which determine its charge) and neutrons (which, combined with protons, determine its mass). Atoms also have electrons, which orbit the nucleus at a distance determined by their charge-to-mass ratio, and each electron can only occupy a specific set of energy levels. In isolation, each atom will come to exist in the ground state: where the electrons cascade down until they occupy the lowest allowable energy levels, limited only by the quantum rules that determine the various properties that electrons are and aren’t allowed to possess.</p><p>Electrons can occupy the ground state — the 1s orbital — of an atom until it’s full, which can hold two electrons. The next energy level up consists of spherical (the 2s) and perpendicular (the 2p) orbitals, which can hold two and six electrons, respectively, for a total of eight. The third energy level can hold 18 electrons: 3s (with two), 3p (with six), and 3d (with ten), and the pattern continues on upward. In general, the “upward” transitions occur when a photon of a particular wavelength gets absorbed, while the “downward” transitions can occur spontaneously, and result in the emission of photons of the exact same wavelengths as are present within the atom’s absorption spectrum.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="960" height="714" src="https://bigthink.com/wp-content/uploads/2021/10/e.jpg?w=960" alt="atom" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2021/10/e.jpg 960w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=375,279 375w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=640,476 640w, https://bigthink.com/wp-content/uploads/2021/10/e.jpg?resize=768,571 768w"><div><div><p>Electron transitions in the hydrogen atom, along with the wavelengths of the resultant photons, showcase the effect of binding energy and the relationship between the electron and the proton in quantum physics. The Bohr model of the atom provides the coarse (or rough, or gross) structure of these energy levels. Hydrogen’s brightest atomic transition is Lyman-alpha (n=2 to n=1), but its second brightest is visible: Balmer-alpha (n=3 to n=2), which emits visible (red) light at a wavelength of 656 nanometers. The energy lost by an electron cascading down the energy levels gets emitted in the form of photons.
</p></div><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hydrogen_transitions.svg" target="_blank">Credit</a>: OrangeDog and Szdori/Wikimedia Commons
</figcaption></div></figure><p>That’s the basic structure of an atom, sometimes referred to as “coarse structure.” When you transition from the third energy level to the second energy level in a hydrogen atom, for example, you produce a photon that’s red in color, with a wavelength of precisely 656.3 nanometers: right in the visible light range of human eyes.</p><p>But there are very, very slight differences between the exact, precise wavelength of a photon that gets emitted if you transition from:</p><ul>
<li>the third energy level down to either the 2s or the 2p orbital,</li>



<li>an energy level where the spin angular momentum and the orbital angular momentum are aligned versus one where they’re anti-aligned,</li>



<li>or one where the nuclear spin and the electron spin are aligned versus anti-aligned.</li>
</ul><p>There are rules as to what’s allowed versus what’s forbidden in quantum mechanics as well, such as the fact that you can transition an electron from a d-orbital to either an s-orbital or a p-orbital, and from an s-orbital to a p-orbital, but not from an s-orbital to another s-orbital. </p><p>The slight differences in energy that arise between transitions of different types of orbital within the same energy level is known as an atom’s fine-structure, arising from the interaction between the spin of each particle within an atom and the orbital angular momentum of the electrons around the nucleus. It causes a shift in wavelength of less than 0.1%: small compared to the atom’s course structure, but still measurable and significant.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="960" height="375" src="https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?w=960" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg 960w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=375,146 375w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=640,250 640w, https://bigthink.com/wp-content/uploads/2022/04/960x0-2.jpg?resize=768,300 768w"><div><div><p>The atomic transition from the 6S orbital in a cesium-133 atom, Delta_f1, is the transition that defines the meter, second, and the speed of light. Slight changes in the observed frequency of this light will occur based on motion and the properties of spatial curvature between any two locations. Spin-orbit interactions, as well as various quantum rules and the application of an external magnetic field, can cause additional splitting at narrow intervals in these energy levels: examples of fine and hyperfine structure.
</p></div><figcaption><a href="https://www.researchgate.net/profile/Juergen-Czarske/publication/255713467_Optical_multi-point_measurements_of_the_acoustic_particle_velocity_with_frequency_modulated_Doppler_global_velocimetry/links/00b7d52693279317cc000000/Optical-multi-point-measurements-of-the-acoustic-particle-velocity-with-frequency-modulated-Doppler-global-velocimetry.pdf" target="_blank">Credit</a>: A. Fischer et al., Journal of the Acoustical Society of America, 2013
</figcaption></div></figure><p>However, owing to the weird phenomena that occur within quantum mechanics, even “forbidden” transitions can sometimes occur. These transitions occur due to the phenomenon of quantum tunneling, where a quantum state can spontaneously transition to another, lower-energy quantum state. Sure, you might not be able to transition from an s-orbital to another s-orbital directly, but if you can:</p><ul>
<li>transition from an s-orbital to a p-orbital and then back to an s-orbital,</li>



<li>transition from an s-orbital to a d-orbital and then back to an s-orbital,</li>



<li>or, more generally, transition from an s-orbital to any other allowable state and then back to an s-orbital,</li>
</ul><p>then that transition can occur. The only thing weird about quantum tunneling is that you don’t have to have a “real” transition occur to the intermediate state. Real transitions require energy, and even with insufficient energies, the intermediate state can be bypassed under the rules of quantum physics. This occurs when transitions happen virtually (as opposed to real transitions), so that you only see the final state emerge from the initial state: something that would be forbidden without the invocation of quantum tunneling.</p><p>This allows us to go beyond mere “coarse structure” and “fine structure,” allowing us to probe what’s known as hyperfine structure. Hyperfine structure appears where the spin of the atomic nucleus and one of the electrons that orbit it begin in an “aligned” state, where the spins are both in the same direction even though the electron is in the lowest-energy, ground (1s) state, and then transitions to an anti-aligned state, where the spins are reversed.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="962" height="990" src="https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg 962w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=20,20 20w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=40,40 40w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=375,386 375w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=640,659 640w, https://bigthink.com/wp-content/uploads/2022/12/Hydrogen_emission1.jpg?resize=768,790 768w"><div><div><p>Whenever a neutral hydrogen atom forms, the electron within it will spontaneously de-excite until it’s in the lowest (1s) state of the atom. With a 50/50 chance of having those spins of the electron and proton aligned, half of those atoms will be able to quantum tunnel into the anti-aligned state, emitting radiation of 21 centimeters (1420 MHz) in the process. This should allow us to probe clumps of neutral hydrogen even farther back than the existence of the first stars.
</p></div><figcaption><a href="https://www.skatelescope.org/radio-astronomy/" target="_blank">Credit</a>: SKA Organisation
</figcaption></div></figure><p>The most famous of these transitions occurs in the simplest type of atom of all: hydrogen. With just one proton and one electron, every time you form a neutral hydrogen atom and the electron cascades down to the ground (lowest-energy) state, there’s a 50% chance that the spins of the central proton and the electron will be aligned, with a 50% chance that the spins will be anti-aligned.</p><p>If the spins are anti-aligned, that’s truly the lowest-energy state; there’s nowhere to go via any known transition that will result in the emission of energy at all. But if the spins are aligned, it’s a slightly higher energy state than in the anti-aligned case. A hydrogen atom whose electron and proton both spin in the same direction could quite possibly transition, through quantum tunneling, to the anti-aligned state. Even though the direct transition process is forbidden, tunneling allows you to go straight from the starting point to the ending point, emitting a photon in the process.</p><p>This transition, because of its “forbidden” nature, takes an extremely long time to occur: approximately 10 million years for the average atom. However, this long lifetime of the slightly excited, aligned case for a hydrogen atom has an upside to it: the photon that gets emitted, at 21 centimeters in wavelength and with a frequency of 1420 megahertz, is intrinsically, extremely narrow. In fact, it’s the narrowest, most precise transition line known in all of atomic and nuclear physics! </p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="826" height="537" src="https://bigthink.com/wp-content/uploads/2022/12/universe.png" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/universe.png 826w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=20,12 20w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=375,244 375w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=640,416 640w, https://bigthink.com/wp-content/uploads/2022/12/universe.png?resize=768,499 768w"><div><div><p>This map of the Milky Way, in red, maps out the neutral hydrogen in 21 centimeter emissions. This map is not uniform, but rather tracks recent ionization and atom formation, as the half-life of spin-aligned atoms to flip is only around ~10 million years: a long time in the lab, but a short time compared to the ~13+ billion year history of our galaxy.
</p></div><figcaption>(<a href="https://labplot.kde.org/2020/12/28/the-universe-full-of-hydrogen-and-a-new-feature-in-labplot/" target="_blank">Credit</a>: J.Dickey/NASA SkyView)
</figcaption></div></figure><p>If you were to go all the way back to the early stages of the hot Big Bang, before any stars had formed, you’d discover that a whopping 92% of the atoms in the Universe were exactly this species of hydrogen: with one proton and one electron in them. (At the present time, after all the stars that have formed some 13.8 billion years later, that number is down to “only” about 90% of all atoms.) As soon as neutral atoms stably form — just a few hundred thousand years after the Big Bang — these neutral hydrogen atoms form with a 50/50 chance of having aligned versus anti-aligned spins. The ones that form anti-aligned will remain so; the ones that form with their spins aligned will undergo this spin-flip transition, emitting radiation of 21 centimeters in wavelength.</p><p>Although it’s never yet been done, this gives us a tremendously provocative way to measure the early stages of the Universe as never before. If we could find a cloud of hydrogen-rich gas, even one that’s never formed stars, we could look for this spin-flip signal — accounting for the expansion of the Universe and the corresponding redshift of the light — to measure the atoms in the Universe from the earliest times ever seen. The only “broadening” to the line we’d expect to see would come from thermal and kinetic effects: from the non-zero temperature and the gravitationally-induced motion of the atoms that emit those 21 centimeter signals.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="699" height="300" src="https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?w=699" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg 699w, https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?resize=375,161 375w, https://bigthink.com/wp-content/uploads/2022/12/thermalbroadening.jpg?resize=640,275 640w"><div><div><p>If particles that emitted radiation were completely at rest and were at a temperature indistinguishable from absolute zero, the width of any emission lines would be determined solely by the speed of the transition. The 21 cm hydrogen line is incredibly, intrinsically narrow, but the kinetic motion of the material in galaxies, as well as the thermal energy because the gas is at a positive, non-zero temperature, both contribute to the observed width of these lines.
</p></div><figcaption>(<a href="https://astronomy.swin.edu.au/cosmos/t/thermal+doppler+broadening" target="_blank">Credit</a>: Swinburne University of Technology)
</figcaption></div></figure><p>In addition to those primordial signals, 21 centimeter radiation arises as a consequence whenever new stars are produced. Every time that a star-forming event occurs, the more massive newborn stars produce large amounts of ultraviolet radiation: radiation that’s energetic enough to ionize hydrogen atoms. All of a sudden, space that was once filled with neutral hydrogen atoms is now filled with free protons and free electrons.</p><p>But those electrons aren’t going to remain ionized forever; if the interstellar environment they’re located in has enough free atomic nuclei (e.g., protons), they’re going to eventually be captured, once again, by those protons. Once the most massive stars have died away, there’s no longer going to be enough ultraviolet radiation to continue to ionize them over and over again, and then those electrons will once again sink down to the ground state, where they’ll have a 50/50 chance of being aligned or anti-aligned with the spin of the atomic nucleus.</p><p>Again, that same radiation — of 21 centimeters in wavelength — gets produced over timescales of ~10 million years. Every time we measure that 21 centimeter wavelength localized in a specific region of space, even if it gets redshifted by the expansion of the Universe, what we’re seeing is evidence of recent star-formation. Wherever star-formation occurs, hydrogen gets ionized, and whenever those atoms become neutral and de-excite again, this specific-wavelength radiation persists for tens of millions of years.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="1120" height="1024" src="https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?w=1120" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png 1120w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=375,343 375w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=640,585 640w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=768,702 768w, https://bigthink.com/wp-content/uploads/2022/04/1120px-Hydrogen-SpinFlip.svg.png?resize=1024,936 1024w"><div><div><p>When a hydrogen atom forms, it has equal probability of having the electron’s and proton’s spins be aligned and anti-aligned. If they’re anti-aligned, no further transitions will occur, but if they’re aligned, they can quantum tunnel into that lower energy state, emitting a photon of a very specific wavelength (21 cm) on very specific, and rather long, timescales. The precision of this transition has been measured to better than 1-part-in-a-trillion, and has not varied over the many decades it’s been known. It is the first light emitted in the Universe after the formation of neutral atoms: even before the formation of the first stars, but also thereafter: whenever new stars are formed, ultraviolet emission ionizes hydrogen atoms, creating this signature once again when those atoms spontaneously re-form.
</p></div><figcaption><a href="https://commons.wikimedia.org/wiki/File:Hydrogen-SpinFlip.svg" target="_blank">Credit</a>: Tiltec/Wikimedia Commons
</figcaption></div></figure><p>If we had the capability of sensitively mapping this 21 centimeter emission in all directions and at all redshifts (i.e., distances) in space, we could literally uncover the star-formation history of the entire Universe, as well as the de-excitation of the hydrogen atoms first formed in the aftermath of the hot Big Bang. With sensitive enough observations, we could answer questions like:</p><ul>
<li>Are there stars present in dark voids in space below the threshold of what we can observe, waiting to be revealed by their de-exciting hydrogen atoms?</li>



<li>In galaxies where no new star-formation is observed, is star-formation truly over, or are there low-levels of new stars being born, just waiting to be discovered from this telltale signature of hydrogen atoms?</li>



<li>Are there any events that heat up and lead to hydrogen ionization prior to the formation of the first stars, and are there star-formation bursts that exist beyond the capabilities of even our most powerful infrared observatories to observe directly?</li>
</ul><p>By measuring light of precisely the needed wavelength — peaking at precisely 21.106114053 centimeters, plus whatever lengthening effects arise from the cosmic expansion of the Universe — we could reveal the answers to all of these questions and more. In fact, this is one of the main science goals of <a href="https://www.astron.nl/telescopes/lofar/" target="_blank" rel="noreferrer noopener">LOFAR</a>: the low-frequency array, and it presents a strong science case for putting an upscaled version of this array on the radio-shielded far side of the Moon.</p><!--?xml encoding="utf-8" ?--><figure><img loading="lazy" decoding="async" width="2290" height="1582" src="https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?w=2290" alt="" sizes="auto, (max-width: 767px) 96vw, (max-width: 1280px) 60vw, (max-width: 1536px) 46vw, 710px" srcset="https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg 2290w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1536,1061 1536w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=2048,1415 2048w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=375,259 375w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=640,442 640w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=768,531 768w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1024,707 1024w, https://bigthink.com/wp-content/uploads/2022/12/niac2020_bandyopadhyay.jpg?resize=1280,884 1280w"><div><div><p>Constructing either a very large radio dish, perhaps in a lunar crater, or alternatively an array of radio telescopes, on the far side of the Moon, could enable unparalleled radio observations of the Universe, including in the all-important 21 centimeter range, both nearby and across cosmic time. The ability to map out where neutral hydrogen has newly formed within the past ~10-20 million years would advance our understanding of cosmic history like nothing else.
</p></div><figcaption><a href="https://www.nasa.gov/directorates/spacetech/niac/2020_Phase_I_Phase_II/lunar_crater_radio_telescope/" target="_blank">Credit</a>: Saptarshi Bandyopadhyay
</figcaption></div></figure><p>Of course, there’s another possibility that takes us far beyond astronomy when it comes to making use of this important length: creating and measuring enough spin-aligned hydrogen atoms in the lab to detect this spin-flip transition directly, in a controlled fashion. The transition takes about ~10 million years to “flip” on average, which means we’d need around a quadrillion (10<sup>15</sup>) prepared atoms, kept still and cooled to cryogenic temperatures, to measure not only the emission line, but the width of it. If there are phenomena that cause an intrinsic line-broadening, such as <a href="https://www.sciencedirect.com/science/article/pii/S0370269305003412">a primordial gravitational wave signal</a>, such an experiment would, quite remarkably, be able to uncover its existence and magnitude.</p><p>In all the Universe, there are only a few known quantum transitions with the precision inherent to the hyperfine spin-flip transition of hydrogen, which results in the emission of radiation that’s 21 centimeters in wavelength. If we want to identify:</p><ul>
<li>ongoing and recent star-formation across the Universe,</li>



<li>the first atomic signals even before the first stars were formed,</li>



<li>or the relic strength of yet-undetected gravitational waves left over from cosmic inflation,</li>
</ul><p>it becomes clear that the 21 centimeter transition is the most important probe we have in all the cosmos. In many ways, it’s the “magic length” for uncovering some of nature’s greatest secrets, and can take us closer to the Big Bang than observations of any stars or galaxies could ever hope to.</p><p><em>This article was originally published in December of 2022. It was updated in 2025.</em></p>

<div>
                            <p>
                    Sign up for the Starts With a Bang newsletter              </p>
                                            <p>
                    Travel the universe with Dr. Ethan Siegel as he answers the biggest questions of all.         </p>
                        </div>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fundamental flaws of SIMD ISAs (2021) (133 pts)]]></title>
            <link>https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/</link>
            <guid>43783416</guid>
            <pubDate>Thu, 24 Apr 2025 14:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/">https://www.bitsnbites.eu/three-fundamental-flaws-of-simd/</a>, See on <a href="https://news.ycombinator.com/item?id=43783416">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-966">
	
	<!-- .entry-header -->

	<div>
		
<p>According to Flynn’s taxonomy <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/SIMD" target="_blank">SIMD</a> refers to a computer architecture that can process multiple data streams with a single instruction (i.e. “Single Instruction stream, Multiple Data streams”). There are different taxonomies, and within those several different sub-categories and architectures that classify as “SIMD”.</p>



<p>In this post, however, I refer to packed SIMD ISA:s, i.e. the type of SIMD <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Instruction_set_architecture" target="_blank">instruction set architecture</a> that is most common in contemporary consumer grade CPU:s. More specifically, I refer to <strong>non-predicated packed SIMD</strong> ISA:s where the details of packed SIMD processing is exposed to the software environment.</p>



<h2>Packed SIMD</h2>



<p>The common trait of packed SIMD architectures is that several data elements are packed into a single register of a fixed width. Here is an example of possible configurations of a packed 128 bits wide SIMD register:</p>



<div><figure><a href="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png"><img width="512" height="131" src="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png" alt="SIMD register" srcset="https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register.png 512w, https://www.bitsnbites.eu/wp-content/uploads/2021/08/simd-register-300x77.png 300w" sizes="(max-width: 512px) 100vw, 512px"></a></figure></div>



<p>For instance, a 128-bit register can hold sixteen integer bytes or four single precision floating-point values.</p>



<p>This type of SIMD architecture has been wildly popular since the mid 1990s, and some packed SIMD ISA:s are:</p>



<ul><li>x86: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MMX_(instruction_set)" target="_blank">MMX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/3DNow!" target="_blank">3DNow!</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions" target="_blank">SSE</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/SSE2" target="_blank">SSE2</a>, …, and <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions" target="_blank">AVX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AVX2" target="_blank">AVX2</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AVX-512" target="_blank">AVX-512</a><sup>1</sup></li><li>ARM: ARMv6 SIMD, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/NEON_(instruction_set)" target="_blank">NEON</a></li><li>POWER: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/AltiVec" target="_blank">AltiVec</a> (a.k.a. VMX and VelocityEngine)</li><li>MIPS: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MDMX" target="_blank">MDMX</a>, <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/MIPS-3D" target="_blank">MIPS-3D</a>, <a rel="noreferrer noopener" href="https://www.mips.com/products/architectures/ase/simd/" target="_blank">MSA</a>, <a rel="noreferrer noopener" href="https://www.mips.com/products/architectures/ase/dsp/" target="_blank">DSP</a></li><li>SPARC: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Visual_Instruction_Set" target="_blank">VIS</a></li><li>Alpha: <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/DEC_Alpha#Motion_Video_Instructions_(MVI)" target="_blank">MVI</a></li></ul>



<p><sup>1</sup> <em>AVX and later x86 SIMD ISA:s (especially AVX-512) incorporate features from <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Vector_processor" target="_blank">vector processing</a>, making them packed SIMD / vector processing hybrids (thus some of the aspects discussed in this article do not fully apply).</em></p>



<p>The promise of all those ISA:s is increased data processing performance, since each instruction executes several operations in parallel. However, there are problems with this model.</p>



<h2>Flaw 1: Fixed register width</h2>



<p>Since the register size is fixed there is no way to scale the ISA to new levels of hardware parallelism without adding new instructions and registers. Case in point: MMX (64 bits) vs SSE (128 bits) vs AVX (256 bits) vs AVX-512 (512 bits).</p>



<p>Adding new registers and instructions has many implications. For instance, the <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Application_binary_interface" target="_blank">ABI</a> must be updated, and support must be added to operating system kernels, compilers and debuggers.</p>



<p>Another problem is that each new SIMD generation requires new instruction opcodes and encodings. In fixed width instruction sets (e.g. ARM) this may prohibit any new extensions, since there may not be enough opcode slots left for adding the new instructions. In variable width instruction sets (e.g. x86) the effect is typically that instructions get longer and longer (effectively hurting code density). Paradoxically each new SIMD generation essentially renders the previous generations redundant (except for supporting binary backwards compatibility), so a large number of instructions are wasted without adding much value.</p>



<p>Finally, any software that wants to use the new instruction set needs to be rewritten (or at least recompiled). What is worse, software developers often have to target several SIMD generations, and add mechanisms to their programs that dynamically select the optimal code paths depending on which SIMD generation is supported.</p>



<h2>Flaw 2: Pipelining</h2>



<p>The packed SIMD paradigm is that there is a 1:1 mapping between the register width and the execution unit width (this is usually required to achieve reasonable performance for instructions that mix inputs from several lanes). At the same time many SIMD operations are pipelined and require several clock cycles to complete (e.g. floating-point arithmetic and memory load instructions). The side effect of this is that the result of one SIMD instruction is not ready to be used until several instructions later in the instruction stream.</p>



<p>Consequently, loops have to be <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Loop_unrolling" target="_blank">unrolled</a> in order to avoid stalls and keep the pipeline busy. This can be done in advanced (power hungry) hardware implementations with register renaming and speculative out-of-order execution, but for simpler (usually more power efficient) hardware implementations loops have to be unrolled in software. Many software developers and compilers aiming to support both in-order and out-of-order processors simply unroll all SIMD loops in software. </p>



<p>However, loop unrolling hurts code density (i.e. makes the program binary larger), which in turn hurts instruction cache performance (fewer program segments fit in the instruction cache, which reduces the cache hit ratio).</p>



<p>Loop unrolling also increases <a href="https://en.wikipedia.org/wiki/Register_pressure" target="_blank" rel="noreferrer noopener">register pressure</a> (i.e. more registers must be used in order to keep the state of multiple loop iterations in registers), so the architecture must provide enough SIMD registers to avoid <a href="https://en.wikipedia.org/wiki/Register_spilling" target="_blank" rel="noreferrer noopener">register spilling</a>.</p>



<h2>Flaw 3: Tail handling</h2>



<p>When the number of array elements that are to be processed in a loop is not a multiple of the number of elements in the SIMD register, special loop tail handling needs to be implemented in software. For instance if an array contains 99 32-bit elements, and the SIMD architecture is 128 bits wide (i.e. a SIMD register contains four 32-bit elements), 4*24=96 elements can be processed in the main SIMD loop, and 99-96=3 elements need to be processed after the main loop.</p>



<p>This requires extra code after the loop for handling the tail. Some architectures support masked load/store that makes it possible to use SIMD instructions to process the tail, while a more common scenario is that you have to use scalar (non-SIMD) instructions to implement the tail (in the latter case there may be problems if scalar and SIMD instructions have different capabilities and/or semantics, but that is not an issue with packed SIMD per se, just with how some ISA:s are designed).</p>



<p>Usually you also need extra control logic before the loop. For instance if the array length is less than the SIMD register width, the main SIMD loop should be skipped.</p>



<p>The added control logic and tail handling code hurts code density (again reducing the instruction cache efficiency), and adds extra overhead (and is generally awkward to code).</p>



<h2>Alternatives</h2>



<p>One alternative to packed SIMD that addresses all of the flaws mentioned above is a <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Vector_processor" target="_blank">Vector Processor</a>. Perhaps the most notable vector processor is the <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Cray-1" target="_blank">Cray-1</a> (released 1975), and it has served as an inspiration for a new generation of instruction set architectures, including RISC-V <a rel="noreferrer noopener" href="https://github.com/riscv/riscv-v-spec" target="_blank">RVV</a>.</p>



<p>Several other (perhaps less known) projects are pursuing a similar vector model, including Agner Fog’s <a rel="noreferrer noopener" href="https://forwardcom.info/" target="_blank">ForwardCom</a>, Robert Finch’s <a href="https://github.com/robfinch/Thor/blob/main/Thor2021/doc/Thor2021.pdf" target="_blank" rel="noreferrer noopener">Thor2021</a> and my own <a rel="noreferrer noopener" href="https://mrisc32.bitsnbites.eu/" target="_blank">MRISC32</a>. An interesting variant is <a rel="noreferrer noopener" href="https://libre-soc.org/" target="_blank">Libre-SOC</a> (based on OpenPOWER) and its <a rel="noreferrer noopener" href="https://libre-soc.org/openpower/sv/" target="_blank">Simple-V</a> extension that maps vectors onto the scalar register files (which are extended to include some 128 registers each).</p>



<p>ARM <a rel="noreferrer noopener" href="https://community.arm.com/developer/research/b/articles/posts/the-arm-scalable-vector-extension-sve" target="_blank">SVE</a> is a predicate-centric, vector length agnostic ISA that addresses many of the traditional SIMD issues.</p>



<p>A completely different approach is taken by Mitch Alsup’s <a rel="noreferrer noopener" href="https://groups.google.com/g/comp.arch/c/SlbYDIPZjH0/m/CLkxJHs1BgAJ" target="_blank">My 66000</a> and its Virtual Vector Method (VVM), which transforms scalar loops into vectorized loops in hardware with the aid of special loop decoration instructions. That way it does not even have to have a vector register file.</p>



<p>Another interesting architecture is the <a rel="noreferrer noopener" href="https://millcomputing.com/" target="_blank">Mill</a>, which also has <a rel="noreferrer noopener" href="https://millcomputing.com/docs/wide-data/" target="_blank">support for vectors</a> without packed SIMD.</p>



<h2>Examples</h2>



<p><em>Edit: This section was added on 2021-08-19 to provide some code examples that show the difference between packed SIMD and other alternatives, and extended on 2023-05-31 with RISC-V RVV and ARM SVE examples and more comments. </em></p>



<p>A simple routine from <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank">BLAS</a> is saxpy, which computes z = a*x + y, where <em>a</em> is a constant, <em>x</em> and <em>y</em> are arrays, and the “s” in “saxpy” stands for single precision floating-point.</p>



<pre><code>// Example C implementation of saxpy:
void saxpy(size_t n, float a, float* x, float* y, float* z)
{
  for (size_t i = 0; i &lt; n ; i++)
    z[i] = a * x[i] + y[i];
}</code></pre>



<p>Below are assembler code snippets that implement saxpy for different ISA:s.</p>



<h3>Packed SIMD (x86_64 / SSE)</h3>



<pre><code>saxpy:
    test    rdi, rdi
    je      .done
    cmp     rdi, 8
    jae     .at_least_8
    xor     r8d, r8d
    jmp     .tail_2_loop
.at_least_8:
    mov     r8, rdi
    and     r8, -8
    movaps  xmm1, xmm0
    shufps  xmm1, xmm0, 0
    lea     rax, [r8 - 8]
    mov     r9, rax
    shr     r9, 3
    add     r9, 1
    test    rax, rax
    je      .dont_unroll
    mov     r10, r9
    and     r10, -2
    neg     r10
    xor     eax, eax
.main_loop:
    movups  xmm2, xmmword ptr [rsi + 4*rax]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 16]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm4, xmmword ptr [rdx + 4*rax]
    addps   xmm4, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 16]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax], xmm4
    movups  xmmword ptr [rcx + 4*rax + 16], xmm2
    movups  xmm2, xmmword ptr [rsi + 4*rax + 32]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 48]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm4, xmmword ptr [rdx + 4*rax + 32]
    addps   xmm4, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 48]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax + 32], xmm4
    movups  xmmword ptr [rcx + 4*rax + 48], xmm2
    add     rax, 16
    add     r10, 2
    jne     .main_loop
    test    r9b, 1
    je      .tail_2
.tail_1:
    movups  xmm2, xmmword ptr [rsi + 4*rax]
    movups  xmm3, xmmword ptr [rsi + 4*rax + 16]
    mulps   xmm2, xmm1
    mulps   xmm3, xmm1
    movups  xmm1, xmmword ptr [rdx + 4*rax]
    addps   xmm1, xmm2
    movups  xmm2, xmmword ptr [rdx + 4*rax + 16]
    addps   xmm2, xmm3
    movups  xmmword ptr [rcx + 4*rax], xmm1
    movups  xmmword ptr [rcx + 4*rax + 16], xmm2
.tail_2:
    cmp     r8, rdi
    je      .done
.tail_2_loop:
    movss   xmm1, dword ptr [rsi + 4*r8]
    mulss   xmm1, xmm0
    addss   xmm1, dword ptr [rdx + 4*r8]
    movss   dword ptr [rcx + 4*r8], xmm1
    add     r8, 1
    cmp     rdi, r8
    jne     .tail_2_loop
.done:
    ret
.dont_unroll:
    xor     eax, eax
    test    r9b, 1
    jne     .tail_1
    jmp     .tail_2</code></pre>



<p>Notice how the packed SIMD code contains a 4x unrolled version of the main SIMD loop and a scalar tail loop. It also contains a setup phase (the first 20 instructions) that should not have a huge performance impact for long arrays, but for short arrays the setup code adds unnecessary overhead.</p>



<p>Unfortunately, this kind of manual setup + unrolling + tail handling code uses up unnecessarily large chunks of the instruction cache of a CPU core.</p>



<p>This demonstrates flaws 2 &amp; 3 described above. Flaw 1 is actually also present, since you need to have multiple implementations for optimal performance on all CPU:s. E.g. in addition to the SSE implementation above, you would also need AVX2 and AVX-512 implementations, and switch between them at run time depending on CPU capabilities.</p>



<h3>Vector (MRISC32)</h3>



<pre><code>saxpy:
    bz    r1, 2f          ; Nothing to do?
    getsr vl, #0x10       ; Query the maximum vector length
1:
    minu  vl, vl, r1      ; Define the operation vector length
    sub   r1, r1, vl      ; Decrement loop counter
    ldw   v1, [r3, #4]    ; Load x[] (element stride = 4 bytes)
    ldw   v2, [r4, #4]    ; Load y[]
    fmul  v1, v1, r2      ; x[] * a
    fadd  v1, v1, v2      ; + y[]
    stw   v1, [r5, #4]    ; Store z[]
    ldea  r3, [r3, vl*4]  ; Increment x pointer
    ldea  r4, [r4, vl*4]  ; Increment y pointer
    ldea  r5, [r5, vl*4]  ; Increment z pointer
    bnz   r1, 1b
2:
    ret</code></pre>



<p>Unlike the packed SIMD version, the vector version is much more compact since it handles unrolling and the tail in hardware. Also, the setup code is minimal (just 1-2 instructions).</p>



<p>The GETSR instruction is used for querying the implementation defined maximum vector length (i.e. the number of 32-bit elements that a vector register can hold). The VL register defines the vector length (number of elements to process) for the vector operations. During the last iteration, VL may be less than the maximum vector length, which takes care of the tail.</p>



<p>Load and store instructions take a “byte stride” argument that defines the address increment between each vector element, so in this case (stride=4 bytes) we load/store consecutive single-precision floating-point values. The FMUL and FADD instructions operate on each vector element separately (either in parallel or in series, depending on the hardware implementation).</p>



<h3>Vector (RISC-V V extension)</h3>



<p>Code and comments graciously provided by Bruce Hoult:</p>



<pre><code>saxpy:
    vsetvli   a4, a0, e32,m8, ta,ma // Get vector length in items, max n
    vle32.v   v0, (a1)              // Load from x[]
    vle32.v   v8, (a2)              // Load from y[]
    vfmacc.vf v8, fa0, v0           // y[] += a * x[]
    vse32.v   v8, (a3)              // Store to z[]
    sub       a0, a0, a4            // Decrement item count
    sh2add    a1, a4, a1            // Increment x pointer
    sh2add    a2, a4, a2            // Increment y pointer
    sh2add    a3, a4, a3            // Increment z pointer 
    bnez      a0, saxpy
    ret</code></pre>



<p>The vector length agnostic <a rel="noreferrer noopener" href="https://github.com/riscv/riscv-v-spec" target="_blank">RISC-V vector ISA</a> enables more efficient code and a much smaller code footprint than packed SIMD, just like MRISC32. RISC-V also has a fused multiply-add instruction (VFMACC) that further shortens the code (FMA is planned to be added to the MRISC32 ISA in the future).</p>



<p>A few notes about the use of VSETVLI (set vector length) in the example:</p>



<ul><li><strong>e32,m8</strong> means 32 bit data items, use 8 vector registers at a time e.g. v0-v7, v8-v15 effectively hardware unrolling by 8x and processing e.g. 32 items at a time with 128 bit vector registers. The last iteration can process anywhere from 1 to 32 items (or 0 if n is 0).</li><li><strong>ta,ma</strong> means we don’t care how masked-off elements are handled (we aren’t using masking), and don’t care how unused tail elements are handled on the last iteration.</li></ul>



<p>The code actually correctly handles n=0 (empty array), so unless we expect that to be very common it would be silly to handle it specially and slow everything else down by one instruction.</p>



<h3>Predicated SIMD (ARM SVE)</h3>



<pre><code>saxpy:
    mov     x4, xzr                     // Set start index = 0
    dup     z0.s, z0.s[0]               // Convert scalar a to a vector
1:
    whilelo p0.s, x4, x0                // Set predicate [index, n)
    ld1w    z1.s, p0/z, [x1, x4, lsl 2] // Load x[]        (predicated)
    ld1w    z2.s, p0/z, [x2, x4, lsl 2] // Load y[]        (predicated)
    fmla    z2.s, p0/m, z0.s, z1.s      // y[] += a * x[]  (predicated)
    st1w    z2.s, p0,   [x3, x4, lsl 2] // Store z[]       (predicated)
    incw    x4                          // Increment start index
    b.first 1b                          // Loop if first bit of p0 is set
    ret</code></pre>



<p>As can be seen, a SIMD ISA with proper predication/masking support can easily do tail handling in hardware, and thus the code is very similar to that of a vector processor. The key is the use of a predication register (p0), which is initialized with a binary true/false mask using the WHILELO instruction, and later used for all the vector operations to mask out vector elements that should not be part of the current iteration (effectively only happens in the last iteration, which takes care of the tail).</p>



<p>Also note how the code is register width agnostic (WHILELO and INCW handle that for you).</p>



<h3>Virtual Vector Method (My 66000)</h3>



<pre><code>saxpy:
    beq0    r1,1f         ; Nothing to do?
    mov     r8,#0         ; Loop counter = 0
    vec     r9,{}         ; Start vector loop
    lduw    r6,[r3+r8&lt;&lt;2] ; Load x[]
    lduw    r7,[r4+r8&lt;&lt;2] ; Load y[]
    fmacf   r6,r6,r2,r7   ; x[] * a + y[]
    stw     r6,[r5+r8&lt;&lt;2] ; Store z[]
    loop    ne,r8,r1,#1   ; Increment counter and loop
1:
    ret</code></pre>



<p>The Virtual Vector Method (VVM) is a novel technique invented by Mitch Alsup, and it allows vectorization without a vector register file. As you can see in this example only scalar instructions and references to scalar register names (“r<em>*</em>“) are used. The key players here are the VEC and LOOP instructions that turn the scalar loop into a vector loop.</p>



<p>Essentially the VEC instruction marks the top of the vectorized loop (r9 stores the address of the loop start, which is implicitly used by the LOOP instruction later). All instructions between VEC and LOOP are decoded and analyzed once and are then performed at the capabilities of the hardware. In this case most register identifiers (r1, r3, r4, r5, r6, r7, r8) are used as virtual vector registers, whereas r2 is used as a scalar register. The LOOP instruction increments the counter by 1, compares it to r1, and repeats the loop as long as the condition, not equal (“ne”), is met.</p>



<h2>Further reading</h2>



<p>Also see: <a rel="noreferrer noopener" href="https://www.sigarch.org/simd-instructions-considered-harmful/" target="_blank">SIMD considered harmful</a> (D. Patterson, A. Waterman, 2017)</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->
	
<!-- #comments -->

</article></div>]]></description>
        </item>
    </channel>
</rss>