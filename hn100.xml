<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Jul 2024 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Proton launches its own version of Google Docs (190 pts)]]></title>
            <link>https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</link>
            <guid>40864914</guid>
            <pubDate>Wed, 03 Jul 2024 11:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html">https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</a>, See on <a href="https://news.ycombinator.com/item?id=40864914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/protons-windows-and-macos-mail-app-is-out-of-beta-and-available-now-110010822.html" data-ylk="slk:Proton;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">Proton</a> now has its own version of Google Docs in its Drive cloud storage service, and like the company's other products, it comes with end-to-end encryption. The company says its flavor of Docs "offers a unique solution in a market where most popular products neglect privacy" and recommends it for use in the healthcare, media, finance and legal industries. <a data-i13n="elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1" href="https://shopping.yahoo.com/rdlw?siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3Byb3Rvbi5tZS9ibG9nL2RvY3MtcHJvdG9uLWRyaXZlIiwiY29udGVudFV1aWQiOiJjMmYyMGEyYS05ZGEzLTQyZmMtYjhmMC03MGI3ZDRlMGFhNmYifQ&amp;signature=AQAAAWZaQ6nJO7zP4rH3JS2Y44qB7q_HbbOpt8RRVvU6nYbn&amp;gcReferrer=https%3A%2F%2Fproton.me%2Fblog%2Fdocs-proton-drive" rel="nofollow noopener" target="_blank" data-ylk="slk:Proton Docs;elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1;itc:0;sec:content-canvas">Proton Docs</a> has advanced formatting and image embed options like Google Docs has and can create, open and edit documents in multiple formats, including Microsoft .docx.</p><p>It has collaboration tools similar to Google Docs', as well. Users can invite anyone to view and edit their documents, though those without a Proton account will be prompted to create one first. The free tier of Proton Drive includes essential document features so people don't have to pay for the service if they don't want to. Participants will be able to add comments to the document, reply to them and resolve them. And users will see other participants' presence and their cursor placements in real time, so that they know who's working on which part of the document and so that their edits don't clash.</p><p>Proton didn't say whether the launch of Docs means it's going to roll out analogues of Google's other Workspace apps in the future, but the company did <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/proton-encrypted-email-vpn-calendar-rebrand-103024950.html" data-ylk="slk:expand its offerings;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">expand its offerings</a> with several different products over the last few years. In addition to Drive cloud storage — and, of course, its email service — the company has a VPN, an encrypted calendar and even a <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/proton-launches-its-own-password-manager-115039870.html" data-ylk="slk:password manager;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">password manager</a>. Docs will make its way to Proton users over the coming days.</p><p>This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Received an AI Email (623 pts)]]></title>
            <link>https://timharek.no/blog/i-received-an-ai-email</link>
            <guid>40862865</guid>
            <pubDate>Wed, 03 Jul 2024 05:05:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timharek.no/blog/i-received-an-ai-email">https://timharek.no/blog/i-received-an-ai-email</a>, See on <a href="https://news.ycombinator.com/item?id=40862865">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why AI Infrastructure Startups Are Insanely Hard to Build (167 pts)]]></title>
            <link>https://nextword.substack.com/p/why-ai-infrastructure-startups-are</link>
            <guid>40862436</guid>
            <pubDate>Wed, 03 Jul 2024 03:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextword.substack.com/p/why-ai-infrastructure-startups-are">https://nextword.substack.com/p/why-ai-infrastructure-startups-are</a>, See on <a href="https://news.ycombinator.com/item?id=40862436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Recently, </span><a href="http://adept.ai/" rel="">Adept AI</a><span> announced </span><a href="https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAM7kyimVR-Nntc7w3SCp466ss9rI61B5U68ESaJExyUE65kA2h6sdH5pHKpdJ0oi6Y0SvXy7wg2OgsX1JTB-hZw9n0esnLkFCY_6JckUVqoIbgGFEy2gSjzMw4YJBYwbCFKJqPR19xgviwpcnO8cCVPa99I_tMkjrjBWHoQqPtTI" rel="">they are being acquired by Amazon</a><span>, and this solidified a somewhat controversial opinion I’ve held for a while - </span><strong>that AI infra startups are a tarpit idea</strong><span>, </span><strong><span>especially as a “venture-scale” business.</span></strong></p><p><span>The term “tarpit idea” refers to startup ideas that sound reasonable on the surface, but when put to test against reality or rigorous thought, fail to hold up.</span><br></p><div><p><span>I believe most AI infra startups will also fall into this category, where AI infra refers to the “building blocks” companies </span><strong>between the cloud layer and the application layer</strong><span> - RAG services, finetuning infrastructure, text processing services, TTS APIs, vector databases, etc. I won’t name specific names, but just think of any AI infra startup that raised sizable seed rounds off of open source or social media momentum.</span></p></div><p><span>I also believe </span><strong>many founders agree with this viewpoint</strong><span>, which explains the sale of Adept (to Amazon), </span><a href="https://openai.com/index/openai-acquires-rockset/" rel="">Rockset (to OpenAI)</a><span>, InflectionAI (to Microsoft), as well as the soon to be acquisitions of Stability (if it happens), </span><a href="http://character.ai/" rel="">Character</a><span>AI, etc. Every incumbent is looking at M&amp;A to paint an “end-to-end AI platform” story. Only a lucky few will get bought.</span><br></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;June 28 updated AI Infra market map&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="June 28 updated AI Infra market map" title="June 28 updated AI Infra market map" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Source: Bessemer Venture Partners</figcaption></figure></div><div><p><span>So why is selling AI infrastructure </span><em>as a startup</em><span> a tarpit idea? On paper, it’s perfectly reasonable to sell picks and shovels amidst proliferation of AI startups and enterprises building Gen AI features. After all, there’s over 30K “.ai” domains registered every month.</span></p></div><div><p><strong>In a nutshell, the new AI infra startups will struggle to succeed because they lack significant differentiation and capital to crack the enterprise segment.</strong><span> It’s not the startups’ fault, the real problem is competitive dynamics. There’s simply too many entities offering the same table stakes features within 1-3 months apart from each other, which creates a collective tarpit dynamic, where only the incumbents can keep swimming.</span></p></div><p>The argument goes:</p><ul><li><p>For AI infra startups to be “venture scale”, they will eventually need to win over enterprise customers. No question. That requires the startups to have some sustainable edge that separates their products from the incumbents’ (GCP, AWS, as well as the likes of Vercel, Databricks, Datadog, etc).</p></li><li><p>Unfortunately, most cutting edge innovation either comes from the incumbents or the research / OSS community - and incumbents are in a better position to commercialize the innovations because they have more usage data than startups, as well as the relationships.</p></li><li><p>To add salt to the injury, any good ideas that originate from startups get benchmarked and copied quickly. For example, I was quite surprised how quickly Databricks and Datadog caught up to the leading LLMOps products from the startup world (e.g. Arize AI). </p></li><li><p>Furthermore, OSS community can’t help but create OSS versions of other AI infra startups’ products - perhaps a testament to how easy it has become to write software.</p></li><li><p>Thus, startups struggle to maintain a sustainable lead over the incumbents to buy them time to win enterprise contracts.</p></li><li><p>And enterprise customers are incentivized to “hold off” on onboarding new vendors, because vendor products diminish in value so quickly because AI landscape changes every few months.</p></li><li><p>This ultimately lengthens sales cycles, and increases churn, which hurts startups more than the incumbents.</p></li></ul><div><p><span>There are also some other dynamics at play (to be discussed in the next section) - but essentially the AI infrastructure space becomes a grind that favors players with the longest runways.</span></p></div><div><p><span>My intention here is not to doom-post, but to highlight some real challenges, which I’m happy to be wrong on (DM me if you disagree). Also, I will end by offering some advice to AI infra startups.</span></p></div><p><em><span>To clarify, by “AI infra startup”, I’m referring to “venture scale” AI infrastructure startups. I’m sure founders can create essentially system integration agencies targeting SMB or mid market, and call themselves an AI infra company. But that’s a completely different business with a much smaller upside.</span><br></em></p><p>There’s three other major forces that’s worsening the competitive environment:</p><ol><li><p>Builders are now conditioned to “demand” composability, a.k.a making it easy to switch out your product for others’. This is great for application layer companies, but not infrastructure companies. Developers can rip out Langchain with Llamaindex, OpenAI models with Claude 3.5 through AWS Bedrock, etc. Every layer of the LLM training and inference stack has at least 10+ viable solutions, that it becomes difficult to create any type of lock-in.</p></li><li><p>The ongoing plummeting of inference costs also plays a role. The COGS are dropping fast, so AI infra players need to constantly price-match the incumbents who have the biggest economies of scale. Models or code have little perceived differentiation, so the consumption goes to the lowest cost providers (incumbents).</p></li><li><p>Incumbents seem to all have the same business strategy of creating an “end-to-end AI platform”. Databricks is getting into AI model training and business intelligence, competing with AWS Sagemaker and Tableau. Github Workspaces is getting into AI-powered security reviews, etc.</p><ol><li><p>Everyone’s default product strategy is to own all upstream and downstream workloads from their core product, which unintentionally makes startups’ lives more difficult, since it becomes hard to compete with a point solution.</p></li></ol></li></ol><p><br><span>With all these challenges, some AI infra startups have chosen to go vertical or move to the application layer. For example, I have been tracking a “Business Intelligence with Natural Language” startup since late 2022 that has pivoted three times already from:</span></p><ul><li><p>a general purpose “chat with data” platform, to</p></li><li><p>“chat with business intelligence data” platform, to</p></li><li><p>“chat with financial data” platform.</p></li></ul><div><p><span>The AI infra darlings LlamaIndex and Langchain also took this path of focus when it comes to their enterprise-oriented products. LlamaIndex is focusing on managed document parsing / OCR, whereas Langchain is focusing on LLMOps and agent building solutions. My guess is that both are working on narrowing their focus even further, since even selling a managed document parsing service is a huge scope for a seed-stage startup, given that Google and AWS already have existing vertical text extraction services. It’s not easy.</span></p></div><div><p><span>Narrowing the scope and going vertical is a typical response for AI infra startups - but I argue that these pivots rarely work out and cause new set of problems. Most importantly, these vertical pivots underestimate the importance of deep domain expertise once you go vertical, which many AI infra founders lack. Accumulating domain knowledge is time consuming. Also, your product may need to be heavily customized for the unique needs of the vertical, which means lower margins.</span></p></div><p><span>Not to mention, these application layer ecosystems have even worse competition (e.g. VCs’ LegalTech ecosystem maps ran out of space to put new logos long time ago). There’s not just the other AI startup competition, but competition from the legacy software companies. Pivoting to a vertical does not suddenly get rid of your competitors - you will just have new ones in that vertical who have been there before you. For example, legal tech industry has existed for ages, and many Legal AI companies are now competing with </span><strong>the legacy legal tech providers plus system integrators.</strong></p><div><p><span>So what’s the solution for AI infra startups? Should we all hope to be acquihired, or is it possible for startups to also stay independent for longer and find product market fit?</span></p><p><span>Here’s a somewhat anti-climatic answer, but the solution for startups goes back to the fundamentals: </span><strong>think deeply about how to be different from the incumbents.</strong><span> Here are four ways to iterate from here:</span></p></div><ul><li><p><strong>Narrow down the scope even further:</strong><span> focus on a very tiny segment of enterprise customers, as opposed to serving all customers. Don’t build all the integrations. Be a managed RAG service for customers using Salesforce with on-prem VMWare, as opposed to a general purpose RAG service. Startups don’t have the resources to solve for every environment, at least initially.</span></p></li><li><p><strong>Focus on just one workload:</strong><span> startups shouldn’t try to solve for too many workloads. Do one thing really well. Don’t try to be a platform for finetuning any LLM - there’s already too many of those. Instead, try to be the best platform for finetuning Tagalog models. The catch: the TAM might be too small.</span></p></li><li><p><strong>Raise more VC money than you think you need:</strong><span> long runways are non-negotiable. It can take a while for enterprises to be receptive to buying startup AI infra solutions, if ever. Be prepared for the worst case scenario.</span></p></li><li><p><strong>Or, don’t raise any VC money at all:</strong><span> raising VC money kind of forces you to orient business strategy around selling to the enterprise - which might be not something you can or want to do. You want the flexibility to work on more interesting and promising problems when they arise, given there’s constantly new changes in AI landscape.</span></p></li></ul><p><span>Lastly, AI startups should be open to being acquired by a larger player, even if it’s not a prestigious destination like OpenAI or Google. </span><strong>My view is that M&amp;A landscape for AI infrastructure sector will become worse, not better, over time.</strong></p><p><span>The acquisition market will become more “efficient” as the winners/losers emerge, and the workloads and enterprise needs become more clearly defined. Thus, in order to sell your startup at an “attractive” valuation, it needs to be marketed prior to the dust settles when the market is less efficient. Don’t wait for another 18 months to shop your startup, when all AI infra startups start running out of runway at the same time.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Bridges Don't Sink (166 pts)]]></title>
            <link>https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</link>
            <guid>40861520</guid>
            <pubDate>Tue, 02 Jul 2024 23:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink">https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</a>, See on <a href="https://news.ycombinator.com/item?id=40861520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-668414c1a012432cf6a43a4b" data-layout-label="Post Body" data-type="item" data-updated-on="1719932256684">
  <p><em>[Note that this article is a transcript of the video embedded above.]</em></p><p>The essence of a bridge is not just that it goes over something, but that there’s clear space underneath for a river, railway, or road. Maybe this is already obvious to you, but bridges present a unique structural challenge. In a regular road, the forces are transferred directly into the ground. On a bridge, all those forces on the span get concentrated into the piers or abutments on either side. Because of that, bridge substructures are among the strongest engineered systems on the planet. And yet, bridge foundations are built in some of the least ideal places for heavy loading. Rivers and oceans have soft, mucky soils that can’t hold much weight. Plus, obviously, a lot of them are underwater.</p><p>What happens when you overload soil with a weight it can’t handle? In engineering-speak, it’s called a bearing failure, but it’s as simple as stepping in the mud. The foundation just sinks into the ground. But, what if you just keep loading it and causing it to sink deeper and deeper? Congratulations! You just invented one of the most widely used structural members on earth: the humble foundation pile. How do they work, and how can you install them underwater? I’m Grady, and this is Practical Engineering. Today we’re having piles of fun talking about deep foundations.</p><p>I did a video all about the different types of foundations used in engineering, but I didn’t go too deep into piles. A pile is a fairly simple structural member, just a long pole driven or drilled into the ground. But, behind that simplicity is a lot of terrifically complex engineering. Volume 1 of the Federal Highway Administration’s manual on the Design and Construction of Driven Pile Foundations is over 500 pages long. There are 11 pages of symbols, 2 pages of acronyms, and you don’t even get to the introduction until page 46. And just a little further than that, you get some history of driven piles. Namely that the history has been lost to time. Humans have been hammering sticks into the ground since way before we knew how to write about it. And that’s pretty much all a driven pile is.</p><p>The first piles were made from timber, and wood is still used all these years around the world. Timber piles are cheap, resilient to driving forces, and easy to install. But, wood rots, it has an upper limit on length from the size of the tree, and it’s not that strong compared to the alternatives. Concrete piles solve a lot of those problems. They come in a variety of sizes and shapes, and again, are widely used for deep foundations. One disadvantage of concrete piles is that they have to be pretty big to withstand the force required to drive them into ground. Some concrete piles can be upwards of 30 inches or 75 centimeters wide. It is hard to hit something that big hard enough to drive it downward into soil, and a lot of ground has to either get out of the way or compress in place to make room. Steel piles solve that problem since they can be a lot more slender. Pipe piles are just what they sound like, and the other major alternative is an H-pile. Your guess is as good as mine why the same steel shape is an <em>I</em>-beam but an <em>H</em>-pile. But, no matter the material, all driven piles are installed in basically the same way.&nbsp;</p><p>Newton’s third law applies to piles like everything else. To push one deep into the ground creates an equal and opposite reaction. You <em>would </em>need either an enormous weight to take advantage of gravity or some other strong structure attached to the ground to react against and develop the pushing force required to drive it downward. Instead of those two options, we usually just use a hammer. By dropping a comparatively small weight from a height, we convert the potential energy of the weight at that height into kinetic energy. The force required to stop the hammer as it falls gets transferred into the pile. Hopefully this is intuitive. It’s pretty hard to push a nail into wood, but it’s pretty easy to hammer it in... well, it’s <em>a little bit</em> easier to hammer it in. There are quite a few types of pile drivers, but most of them use a large hammer or vibratory head to create the forces required.</p><p>Maybe it goes without saying, but the main goal of a foundation is to not move. When you apply a load, you want it to stay put. Luckily, piles have two ways to do that (at least for vertical loads). The first is end-bearing. The end, or toe, of a pile can be driven down to a layer of strong soil or hard rock, making it able to withstand greater loads. But there’s not always a firm stratum at a reasonable depth below the ground. Quote-unquote “bedrock” is a simple idea, but in practice, geology is more complicated than that. Luckily, piles have a second type of resistance: skin friction, also known as shaft resistance. When you drive a pile, it compacts and densifies the surrounding soil, not only adding strength to the soil itself, but creating friction along the walls of the pile that hold it in place. The deeper you go, the more friction you get. Let me show you what I mean.</p><p>I have my own pipe pile in the backyard that I’ve marked with an arbitrary scale. When I drop the hammer at a prescribed height, the pile is driven a certain distance into the ground. Do this enough times, and eventually, you reach a point where the pile kind of stops moving with each successive hammer blow. In technical terms, the pile has reached refusal. I can graph the blow count required to drive the pile to each depth, and you get a pretty nice curve. It’s easy to see how it got stronger against vertical loads the deeper I drove it in. Toward the end, it barely moved with each hit. This is a really nice aspect of driven piles, you install them in a similar way to how they’ll be loaded by the final design. Of course, bridges and buildings don’t hammer on their foundations, but they do impose vertical loads. The tagline of the Pile Driving Contractors Association is “A Driven Pile is a Tested Pile” because, just by installing them, you’ve verified that they can withstand a certain amount of force. After all, you had to overcome that force to get them in the ground. And if you’re not seeing enough resistance, in most cases, you can just keep driving downward until you do!</p><p>But piles don’t just resist downward forces. Structures experience loads in other directions too. Buildings have horizontal, or lateral, loads from wind. Bridges see lateral loads from flowing water, and even ice or boats contacting the piers. Both can experience uplift forces that counteract gravity from floods due to buoyancy or strong winds. If you’ve ever hammered in a tent stake, you know that piles can withstand loading from all kinds of directions. And then there’s scour. The soil along a bridge might look like this right after the bridge is built, but after a few floods, it can look completely different. Engineers have to try and predict how the soil around a bridge will scour over time, from natural changes in the streambed and those created by the bridge itself. Then they make sure to design foundations that can accommodate those changes and stay strong over the long term. This is why bridge foundations sometimes look kind of funny. Loads transfer from the superstructure down into the piers. The piers sit on a pile cap that transfers and distributes loads into the piles themselves. Those piles can be vertical, but if the engineer is expecting serious lateral loads, some of the piles are often inclined, also called battered piles. Inclined piles take better advantage of the shaft resistance to make the foundation stronger against horizontal loads.</p><p>As important and beneficial as they are, driven piles have some limitations too. For one, they’re noisy and disruptive to install. Just last year, I had two friends on separate trips to Seattle who sent me a video of the exact same pile-driving operation. It’s good to have friends who know how much you like construction. But my point is, this type of construction is pretty much impossible to ignore. In dense urban areas, most people are just not willing to put up with the constant banging. Plus the vibrations from installing them can disrupt surrounding infrastructure. Pile driving is crude; in many cases, the piles aren’t designed to withstand the forces of the structure they’ll support but rather the forces they’ll have to experience during installation which are much higher. They can’t easily go through hard geological layers, cobbles, or boulders; they can wander off path, since you can’t really see where you’re going, and they can cause the ground to heave because you’re not removing any soil while you force them into the subsurface. The second major category of piles solves a lot of these problems.</p><p>And, wouldn’t you know it? There’s an FHWA manual that has all the juicy details - Drilled Shafts: Construction Procedures and Design Methods. This one a whopping 747 pages long. A drilled shaft is also exactly what it sounds like. The basic process is pretty simple. Drill a long hole into the ground. Place reinforcing steel in the hole. Then fill the whole thing with concrete. But, bridge piers are often, as you probably know, installed underwater. Pouring concrete underwater is a little tricky. Imagine trying to pour a smoothie at the bottom of a pool! Let me show you what I mean.</p><p>This is my garage-special bridge foundation simulator. It has transparent soil in the form of superabsorbent polymer beads… and you know we have to add some blue water too. You can probably imagine how easy it might be to drill a hole in this soil. It’s just going to collapse in on itself. We need a way to keep the hole open so the rebar and concrete can be installed. So, drilled shafts installed in soft soils or wet conditions usually rely on a casing to support the walls. Installing a casing usually happens while the hole is drilled, following the auger downward. I tried that myself, but I only have two hands, and it was pretty unwieldy. So, just for the sake of the demo, I’m advancing the casing into the soil ahead of time. Now I can drill out the soil to open the shaft. And now I’m realizing the limitations of my soil simulant. It was still pretty hard to do, even with the casing in place. It took a few tries, but I managed to get most of it out.</p><p>So now I have an open hole, but it’s still full of water. Even if your casing runs above the water surface, and you try to pump it out, you can still have water leaking in from the bottom. In ideal conditions, you can get a nice seal between the bottom of the casing and the soil, but even then, it’s pretty hard to keep water out of the hole, and luckily it doesn’t matter.</p><p>Instead of concrete, I’m using bentonite clay as a substitute. It’s got a similar density, and it’s perfect for this demo because you can push it through a small tube… if you get the proportions right. Ask me how I know. This is me pondering the life decisions that led up to me holding a gigantic syringe full of bentonite slurry in my garage. You can’t just drop this stuff through the water. It mixes and dilutes, just turning into a mess. Same is true for concrete. The ratio of water to cement in a concrete mix is essential to its strength and performance, so you can’t do anything that would add water to the mix. The trick is a little device called a tremie. Even though it has a funny name, it’s nothing more than a pipe that runs to the bottom of the hole. As long as you keep the end of the tremie below the surface of the concrete that you’re pumping in, or concrete simulant in my case, there’s no chance for it to mix with the water and dilute. I’m just pushing the clay into the casing with a big syringe, making sure to keep the end of the tube buried. Because concrete is a lot more dense than water, it just displaces it upward, out of the hole.&nbsp;</p><p>In underwater installations, the casing is often left in place. One advantage is that you can build a floating pile cap. Instead of building a big cofferdam and drying out the work area to construct a big concrete structure, sometimes you can raise the pile cap into or above the water surface, reducing the complexity of its construction. These “high rise” pile caps are used a lot in offshore wind turbines. But, not all casings are permanent.</p><p>In some situations, it’s possible to pull the casing once the hole is full of concrete, saving the sometimes enormous cost of each gigantic steel tube. I tried to show this in my demo. It’s not beautiful, but it did work. Again, the concrete is dense, so the pressure it exerts on the walls of the hole is enough to keep the soil from collapsing. And because drilled shafts can be much larger than driven piles, sometimes you don’t even need a group of them. Lots of structures, including wind turbines, highway signs, and more, are built on mono-pile foundations. Just a single drilled shaft deep in the ground, eliminating the need for a pile cap altogether. Another interesting aspect of drilled shafts is that you can ream out the bottom, creating an enlarged base that increases the surface area at the toe. This helps reduce a pile’s tendency to sink, and it can help with uplift resistance too.</p><p>Driven piles and drilled shafts are far from the only types of deep foundation systems. There are tons of variations on the idea that have been developed over the years to solve specific challenges: Continuous flight auger piles do the drilling and concreting in essentially one step, using a hollow-stem auger to fill the hole as it’s removed. Then reinforcement is lowered into the wet concrete. You can fill a hole with compacted aggregate instead of concrete, called a stone column or tradename Geopier if you’re only worried about compressive loads. Helical or screw piles twist into the ground, instead of being hammered, reducing vibrations and disturbance. Micropiles are like tiny drilled shafts used when there are access restrictions or geologic constraints. And of course, there are sheet piles that aren’t really used for foundations but are driven piles meant to create a wall or barrier. Let me know if I forgot to mention your favorite flavor of pile.</p><p>Even though they’re usually much stronger than shallow foundations, piles can and do fail. We’ve talked about San Francisco’s famous Millennium Tower in a previous video. That’s a skyscraper on a pile foundation that sank into the ground, causing the building to tilt. It seems like they mostly have it fixed now, but it’s still in the news every so often, so only time will tell. In 2004, a bridge pier on the Lee Roy Selmon Expressway in Tampa, Florida sank 11 feet (more than 3 meters) while it was still under construction because of the complicated geology. It cost 90 million dollars to fix and delayed the project’s completion by a year. These case studies highlight the complexity of geotechnical engineering when we ask the ground to hold up heavier and heavier loads. The science and technology that goes into designing deep foundations are enough to spend an entire career studying, but hopefully, this video gives you a little insight into how they work.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated Transformer (2018) (130 pts)]]></title>
            <link>https://jalammar.github.io/illustrated-transformer/</link>
            <guid>40861148</guid>
            <pubDate>Tue, 02 Jul 2024 22:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=40861148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=18351674">Hacker News (65 points, 4 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/">Reddit r/MachineLearning (29 points, 3 comments)</a>
</span>
<br>
<span>Translations: <a href="https://www.mundhor.site/post/post14">Arabic</a>, <a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">Chinese (Simplified) 1</a>, <a href="https://blog.csdn.net/qq_36667170/article/details/124359818">Chinese (Simplified) 2</a>, <a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html">French 1</a>, <a href="https://lbourdois.github.io/blog/nlp/Transformer/">French 2</a>, <a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348">Italian</a>, <a href="https://tips-memo.com/translation-jayalmmar-transformer">Japanese</a>, <a href="https://nlpinkorean.github.io/illustrated-transformer/">Korean</a>, <a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/">Persian</a>, <a href="https://habr.com/ru/post/486358/">Russian</a>, <a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/">Spanish 1</a>, <a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp">Spanish 2</a>, <a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/">Vietnamese</a></span>
<br>
<span>Watch: MIT’s <a href="https://youtu.be/53YvP6gdD7U?t=432">Deep Learning State of the Art</a> lecture referencing this post</span>
<br>
<span>Featured in courses at <a href="https://web.stanford.edu/class/cs224n/">Stanford</a>, <a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers">Harvard</a>, <a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf">MIT</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">Princeton</a>, <a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf">CMU</a> and others</span></p>

<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>

<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>

<p><strong>2020 Update</strong>: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-QH8fRhqFHM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>

<p><img src="https://jalammar.github.io/images/t/the_transformer_3.png">
</p>

<!--more-->

<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png">
</p>

<p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png">
</p>

<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_encoder.png">
</p>

<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>

<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p>

<p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq models</a>).</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_decoder.png">
</p>

<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into The Picture</h2>

<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>

<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>.</p>



<p><img src="https://jalammar.github.io/images/t/embeddings.png">
  <br>
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
</p>

<p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>

<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors.png">
  <br>

</p>

<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>

<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>

<h2 id="now-were-encoding">Now We’re Encoding!</h2>

<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png">
  <br>
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
</p>

<h2 id="self-attention-at-a-high-level">Self-Attention at a High Level</h2>
<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>

<p>Say the following sentence is an input sentence we want to translate:</p>

<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>

<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>

<p>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p>

<p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p>

<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png">
  <br>
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".
</p>

<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>

<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.</p>

<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>

<p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png">
  <br>
  Multiplying <span>x1</span> by the <span>WQ</span> weight matrix produces <span>q1</span>, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
</p>



<div><p>What are the “query”, “key”, and “value” vectors?
</p><p>

They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p></div>

<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p>

<p>The score is calculated by taking the dot product of the <span>query vector</span> with the <span>key vector</span> of the respective word we’re scoring. So if we’re processing the self-attention for the word in position <span>#1</span>, the first score would be the dot product of <span>q1</span> and <span>k1</span>. The second score would be the dot product of <span>q1</span> and <span>k2</span>.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png">
  <br>

</p>



<p>The <strong>third and fourth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>



<p><img src="https://jalammar.github.io/images/t/self-attention_softmax.png">
  <br>

</p>

<p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p>



<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>

<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>



<p><img src="https://jalammar.github.io/images/t/self-attention-output.png">
  <br>
</p>

<p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p>

<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</h2>
<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix <span>X</span>, and multiplying it by the weight matrices we’ve trained (<span>WQ</span>, <span>WK</span>, <span>WV</span>).</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png">
  <br>
  Every row in the <span>X</span> matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)
</p>



<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png">
  <br>
  The self-attention calculation in matrix form
</p>





<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>

<p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p>

<ol>
  <li>
    <p>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.</p>
  </li>
  <li>
    <p>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>
  </li>
</ol>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png">
   <br>
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 </p>

<p><br>
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png">
  <br>

</p>



<p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p>

<p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png">
  <br>

</p>

<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>



<p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png">
  <br>

</p>



<p>Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png">
  <br>
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
</p>



<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png">
  <br>
</p>

<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing The Order of The Sequence Using Positional Encoding</h2>
<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>

<p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png">
  <br>
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.
</p>


<p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png">
  <br>
  A real example of positional encoding with a toy embedding size of 4
</p>



<p>What might this pattern look like?</p>

<p>In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png">
  <br>
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.
</p>

<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>

<p><strong>July 2020 Update:</strong> 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">Here’s the code to generate it</a>:</p>

<p><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png">
  <br>
</p>

<h2 id="the-residuals">The Residuals</h2>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450">layer-normalization</a> step.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png">
  <br>
</p>

<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png">
  <br>
</p>

<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png">
  <br>
</p>

<h2 id="the-decoder-side">The Decoder Side</h2>
<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>

<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif">
  <br>
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).
</p>

<p>The following steps repeat the process until a special <end of="" sentence=""> symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</end></p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif">
  <br>

</p>

<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>

<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>

<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>

<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h2>

<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>

<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>

<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>

<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png">
  <br>
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.
</p>



<h2 id="recap-of-training">Recap Of Training</h2>
<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>

<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>

<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;” (short for ‘end of sentence’)).</p>

<p><img src="https://jalammar.github.io/images/t/vocabulary.png">
   <br>
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 </p>

<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:</p>

<p><img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png">
  <br>
  Example: one-hot encoding of our output vocabulary
</p>

<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>

<h2 id="the-loss-function">The Loss Function</h2>
<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>

<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png">
  <br>
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.
</p>



<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  <a href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback–Leibler divergence</a>.</p>

<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>

<ul>
  <li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
  <li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
  <li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
  <li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>

<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png">
   <br>
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 </p>



<p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png">
    <br>
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.
</p>

<p>Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>

<h2 id="go-forth-and-transform">Go Forth And Transform</h2>

<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>

<ul>
  <li>Read the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Tensor2Tensor announcement</a>.</li>
  <li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
  <li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
  <li>Explore the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor repo</a>.</li>
</ul>

<p>Follow-up works:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03059">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
  <li><a href="https://arxiv.org/abs/1706.05137">One Model To Learn Them All</a></li>
  <li><a href="https://arxiv.org/abs/1801.09797">Discrete Autoencoders for Sequence Models</a></li>
  <li><a href="https://arxiv.org/abs/1801.10198">Generating Wikipedia by Summarizing Long Sequences</a></li>
  <li><a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></li>
  <li><a href="https://arxiv.org/abs/1804.00247">Training Tips for the Transformer Model</a></li>
  <li><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></li>
  <li><a href="https://arxiv.org/abs/1803.03382">Fast Decoding in Sequence Models using Discrete Latent Variables</a></li>
  <li><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/ilblackdragon">Illia Polosukhin</a>, <a href="http://jakob.uszkoreit.net/">Jakob Uszkoreit</a>, <a href="https://www.linkedin.com/in/llion-jones-9ab3064b">Llion Jones </a>, <a href="https://ai.google/research/people/LukaszKaiser">Lukasz Kaiser</a>, <a href="https://twitter.com/nikiparmar09">Niki Parmar</a>, and <a href="https://dblp.org/pers/hd/s/Shazeer:Noam">Noam Shazeer</a> for providing feedback on earlier versions of this post.</p>

<p>Please hit me up on <a href="https://twitter.com/JayAlammar">Twitter</a> for any corrections or feedback.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brazil data regulator bans Meta from mining data to train AI models (136 pts)]]></title>
            <link>https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</link>
            <guid>40861057</guid>
            <pubDate>Tue, 02 Jul 2024 22:29:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1">https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</a>, See on <a href="https://news.ycombinator.com/item?id=40861057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>RIO DE JANEIRO (AP) — Brazil’s national data protection authority determined on Tuesday that Meta, the parent company of Instagram and Facebook, cannot use data originating in the country to train its artificial intelligence. </p><p>Meta’s updated privacy policy enables the company to feed people’s public posts into its AI systems. That practice will not be permitted in Brazil, however.</p><p>The decision stems from “the imminent risk of serious and irreparable or difficult-to-repair damage to the fundamental rights of the affected data subjects,” the agency said in the nation’s official gazette. </p><p>Brazil is one of Meta’s biggest markets. Facebook alone has around 102 million active users in the country, the agency said in a statement. The nation has a population of 203 million, according to the country’s 2022 census.</p><p>A spokesperson for Meta said in a statement the company is “disappointed” and insists its method “complies with privacy laws and regulations in Brazil.”</p>
    

<p>“This is a step backwards for innovation, competition in AI development and further delays bringing the benefits of AI to people in Brazil,” the spokesperson added.</p>



<p>The social media company has also encountered resistance to its privacy policy update in Europe, where it recently put on hold its plans to start feeding people’s public posts into training AI systems — which was supposed to start last week.</p><p>In the U.S., where there’s no national law protecting online privacy, such training is already happening.</p>
    
<p>Meta said on its Brazilian blog in May that it could “use information that people have shared publicly about Meta’s products and services for some of our generative AI features,” which could include “public posts or photos and their captions.”</p><p>Refusing to partake is possible, Meta said in that statement. Despite that option, there are “excessive and unjustified obstacles to accessing the information and exercising” the right to opt out, the agency said in a statement. </p>
    

<p>Meta did not provide sufficient information to allow people to be aware of the possible consequences of using their personal data for the development of generative AI, it added.</p><p>Meta isn’t the only company that has sought to train its AI systems on data from Brazilians.</p><p>Human Rights Watch released a report last month that found that personal photos of identifiable Brazilian children sourced from a large database of online images — pulled from parent blogs, the websites of professional event photographers and video-sharing sites such as YouTube — were being used to create AI image-generator tools without families’ knowledge. In some cases, those tools have been used create AI-generated nude imagery.</p><p>Hye Jung Han, a Brazil-based researcher for the rights group, said in an email Tuesday that the regulator’s action “helps to protect children from worrying that their personal data, shared with friends and family on Meta’s platforms, might be used to inflict harm back on them in ways that are impossible to anticipate or guard against.”</p><p>But the decision regarding Meta will “very likely” encourage other companies to refrain from being transparent in the use of data in the future, said Ronaldo Lemos, of the Institute of Technology and Society of Rio de Janeiro, a think-tank. </p>
    

<p>“Meta was severely punished for being the only one among the Big Tech companies to clearly and in advance notify in its privacy policy that it would use data from its platforms to train artificial intelligence,” he said. </p><p>Compliance must be demonstrated by the company within five working days from the notification of the decision, and the agency established a daily fine of 50,000 reais ($8,820) for failure to do so.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All I want for Christmas is a negative leap second (142 pts)]]></title>
            <link>https://qntm.org/leap</link>
            <guid>40860831</guid>
            <pubDate>Tue, 02 Jul 2024 22:01:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qntm.org/leap">https://qntm.org/leap</a>, See on <a href="https://news.ycombinator.com/item?id=40860831">Hacker News</a></p>
Couldn't get https://qntm.org/leap: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Apple poised to get OpenAI board observer role as part of AI pact (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</link>
            <guid>40860363</guid>
            <pubDate>Tue, 02 Jul 2024 21:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement">https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</a>, See on <a href="https://news.ycombinator.com/item?id=40860363">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a search engine for Hacker News (182 pts)]]></title>
            <link>https://hackernews.demo.vectara.com/</link>
            <guid>40860022</guid>
            <pubDate>Tue, 02 Jul 2024 20:11:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackernews.demo.vectara.com/">https://hackernews.demo.vectara.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40860022">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google's carbon emissions surge nearly 50% due to AI energy demand (146 pts)]]></title>
            <link>https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</link>
            <guid>40859993</guid>
            <pubDate>Tue, 02 Jul 2024 20:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html">https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</a>, See on <a href="https://news.ycombinator.com/item?id=40859993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107402399" data-test="InlineImage"><p>A view of the Google headquarters in Mountain View, California, on April 16, 2024.</p><p>Tayfun Coskun | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>'s emissions surged nearly 50% compared to 2019, the company said Tuesday in its <a href="https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf" target="_blank">2024 environmental report</a>, marking a notable setback in its goal to achieve net-zero emissions by 2030.&nbsp;</p><p>Google's emissions also increased 13% year over year in 2023, per the report.</p><p>The company attributed the emissions spike to an increase in data center energy consumption and supply chain emissions driven by rapid advancements in and demand for <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a>. The report noted that the company's total data center electricity consumption grew 17% in 2023.&nbsp;</p><p>The impact of AI on electricity demand is well documented. Electricity demand is <a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html">forecast to grow</a> as much as 20% by 2030, with AI data centers alone expected to add about 323 terawatt hours of electricity demand in the U.S., CNBC previously reported.</p><p>While renewables will likely play an important role in meeting AI energy demands, analysts say that immediate implementation is challenging. This is due to factors such as the time required to build the power lines that transport resources to the data centers, Wells Fargo analyst Roger Read<a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html"> previously told</a> CNBC.</p><p>Google said in the report that its data centers are 1.8 times as energy efficient as a typical data center. The company added that it remains committed to mitigating the environmental impact of AI through model optimization, efficient infrastructure and emissions reductions.&nbsp;</p><p>Google is not the only major tech company to face increased emissions due to AI demand. <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> reported in May that its <a href="https://docs.google.com/document/u/0/d/1Oreyv8tjRotBp3Gl0yGcuBmJjfqTcRu2kGv3Xxet8IM/edit" target="_blank">total carbon emissions</a> rose nearly 30% since 2020 primarily due to the construction of data centers.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sonar is destroying my job and it's driving me to despair (119 pts)]]></title>
            <link>https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438</link>
            <guid>40859937</guid>
            <pubDate>Tue, 02 Jul 2024 20:00:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438">https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438</a>, See on <a href="https://news.ycombinator.com/item?id=40859937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="Sonar is destroying my job and it's driving me to despair">
      
      <meta itemprop="datePublished" content="2023-06-13T12:17:38Z">
        <meta itemprop="articleSection" content="SonarQube">
      <meta itemprop="keywords" content="kotlin, sonarqube">
      


          <div itemprop="text" id="post_1">
              <p>First of all, I understand that Sonar is a well intentioned product.</p>
<p>It’s right <em>most</em> of the time.  Unfortunately coding is, of course, complex and it’s challenging for a product like Sonar to keep pace with the fast changing syntax of newer languages.</p>
<p>I’m an experienced Developer and Team Lead, currently coding in Kotlin with Coroutines. I’ve also worked with Java, C++, C, Objective-C and Swift.  We can all learn more, but I’ve been around the block and know a few things about what code quality looks like and how to make pragmatic decisions around it.</p>
<p>Our ‘default’ Sonar setup is currently making me ‘butcher’ my Kotlin code to comply with it’s rules and, as you may tell, it’s really upsetting.</p>
<p>You may say <em>“The rules can be customised”</em>, or <em>“Allow an exception”</em>.  You need to understand <strong>that is not a simple option for many of your users</strong>.  In my case, I have a superior who administers Sonar and is, let’s say, completely committed to it.  For any ‘exception granted’ we would have to book time with them days in advance then white-board the reason why Sonar is wrong, or produce a sample program - who has got time for that with tight deadlines?</p>
<p>Please rethink your UX with the realisation that there are probably many professional and experienced software engineers out there who - far from appreciating your product, actually feel genuinely oppressed by it: like victims of a cold unfeeling system with no ‘right to reply’.  Unable to merge deadline code until every point is addressed, for better or worse.  It’s a terrible helpless feeling.</p>
<p>A couple of Kotlin examples:</p>
<ul>
<li>
<p>Inability to define a single suspending function interface. Sonar tells me to make it a fun interface, then tells me fun interfaces can’t have suspending functions.  I should make this a functional type instead?  Completely <em>inconsistent</em> with the rest of the code and removes the opportunity to provide information though labelling the function.</p>
</li>
<li>
<p>Sonar doesn’t respect import aliasing.  Let’s say I have Domain and Service models for some entities.  I have ‘Person’ defined in domain and service packages. In one of those packages, I want to write a mapper extension.  I use import aliasing to disambiguate each ‘Person’ as DomainPerson and ServicePerson.  This is an informative convention, but Sonar doesn’t allow it: <code>import service.Person as ServicePerson</code> is considered redundant.  I could then use a private typealias ServicePerson = Person` but this isn’t the same.</p>
</li>
</ul>
<p>These are just two examples, I could find more.</p>
<p><strong>Can you do something with your product</strong> to give users more of a ‘right to reply’ and re-empower them?<br>
There will be creative ways to achieve this without undermining Sonar’s purpose.  Just some ideas:</p>
<ul>
<li>Allow a user role where rules can be overridden but the admin gets informed and can remove the override at a later time.</li>
<li>A mode where an override can be granted by 3-4 other users all agreeing, who aren’t admins.</li>
<li>Button to fast-access (lazily create) a community thread regarding a certain rule, to report a problem with it, so you know your difficulty is being heard at least by Sonar, if not within your organisation.</li>
</ul>
<p>Thank you for listening to my part rant, part suggestions.  This is sincere.</p>
            </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Colin"><span itemprop="name">Colin</span></a>
                (Colin)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-14T08:51:41Z">
                    June 14, 2023,  8:51am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-14T09:29:39Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hey there.</p>
<p>Thanks for the feedback. It’s essential in our quest to ensure developers not only find value in our products but also enjoy and feel empowered by the experience.</p>
<p>If you disagree with a rule implementation, I encourage you to post the details in the category titled <a href="https://community.sonarsource.com/c/clean-code/fp/7">Report a False-Positive/False-Negative</a>, for which we <a href="https://community.sonarsource.com/t/how-to-report-a-false-positive-false-negative/37022">have a post detailing what is required to report</a> (code sample, product versions, rule IDs…). Our teams are very reactive and enjoy engaging in these discussions.</p>

<p>There is an <a href="https://docs.sonarqube.org/latest/instance-administration/security/#:~:text=permission%C2%A0below.-,Project%20permissions,-Project%20permissions%20are"><strong>Administer Issue</strong></a> permission that in most organizations would be granted to Team leaders or experienced developers (like yourself) to be able to mark issues as False-Positive/Won’t Fix.</p>
<p>Somebody else (like your superior) has at least two options (maybe more) to discover these exceptions:</p>
<ul>
<li>Using the <strong>Issues</strong> tab of a project (or the SonarQube instance overall) to filter for issues that are marked as False-Positive / Won’t Fix if they want to do some kind of global review.</li>
<li>Configure a project-level <a href="https://docs.sonarqube.org/latest/instance-administration/notifications/">e-mail notification</a> for <strong>Issues resolved as false positive or won’t fix</strong></li>
</ul>
<p>And, to be honest, Chris, if you have a superior that trusts in Sonar 100% and trusts you and your fellow developers very little (unwilling to delegate permissions, decision making)… I think that’s the root of your problem and not one that can be fixed with product changes. We create a new user role? Enable some kind of consensus-driven issue status? Your superior could just decide not to grant it.</p>
<p>Happy to continue discussing this.</p>
            </div>

            

            

          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/john.clifton"><span itemprop="name">john.clifton</span></a>
                (John Clifton)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-14T09:51:59Z">
                    June 14, 2023,  9:51am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-14T09:51:59Z">
              <span itemprop="position">4</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Hi <a href="https://community.sonarsource.com/u/chris_hatton">@Chris_Hatton</a>, we are looking into something similar to this at the moment. If you would be willing, I’d love to have a short call to talk to you about your suggestion.</p>
            </div>

            

            

          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Jeremy_Cox"><span itemprop="name">Jeremy_Cox</span></a>
                (Jeremy Cox)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-28T13:24:02Z">
                    June 28, 2023,  1:24pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-28T13:30:14Z">
              <span itemprop="position">7</span>
              </span>
            </p>
            <div itemprop="text">
              <blockquote>
<p>if you have a superior that trusts in Sonar 100% and trusts you and your fellow developers very little (unwilling to delegate permissions, decision making)… I think that’s the root of your problem</p>
</blockquote>
<p>You make a fair point that anyone who uses a tool and does not understand is part of the problem.  However, if the tool could be easier to understand, that could go a long way to addressing that issue.</p>
<p>Respectfully, the root of the problem is that SonarQube is not clear to everyone who uses it.  Specifically, it offers little towards gauging what is important and and what is not.  Programmers have been engaged with a tug of war with management since the dawn of the digital era.  Managers, who don’t understand coding well enough to police it, are now offered Sonarqube as a policing tool.  They are given a bat to hit programmers over the head.  This has an easy metric for determining perfections – 0 issues found.  It’s a managers dream – a computer that sums up everything you need to know in one number.  If you give someone a bat, they’re going to use it to hit things.  It’s pretty simple.</p>
<p>The number of issues found in your project is similar to likes on social media – it’s a badge of honor to show off to all your manager friends. I have a coding library that’s tens of thousands of lines.  Military Standard data types are defined by two numbers, so my (Java) class names end with something like “_1234_567”  Like a good programmer, my fields and getters follow the same naming pattern.<br>
This violates S101, S116, S100, S101 for a total of 4.7k hits. This has caused everyone to freak out.</p>
<p>The OP point is well taken, that if I had an opportunity to add some structured comments to that report before a manager read it, the sky would not be falling.  Now I am having trouble explaining in meeting after meeting, “we violated 4 <em>MINOR</em> rules on purpose.  So if we ignore those rules like we’re supposed to, the 4,700 hits go away.”  To which they respond, “You want us to cover up 4,700 hits?”</p>
<p>Clearly, it’s a major problem that they don’t understand a minor sonarqube finding “<a href="https://docs.sonarqube.org/latest/user-guide/issues/#:~:text=MINOR%3A%20A%20quality%20flaw%20that,quality%20flaw%2C%20just%20a%20finding." rel="noopener nofollow ugc">may slightly impact developer productivity</a>” and it <strong>may also not</strong> affect quality.</p>
<p>So thanks to Sonarqube, I am going to have to make my code unreadable to a human in order to please management, which means INTENTIONALLY creating a mountain of technical debt.  But that’s okay, because Sonarqube doesn’t count it.</p>
<p>Sonarqube exacerbates the problem of communication with management; it does not help it. The problem is how the information is presented.  As OP pointed out, Programmers don’t have an opportunity to respond proactively to the issues.  It’s fair to blame managers, it also fair to blame the bad information they are given to make decisions. For example, your issue count could be broken down into “must address” and “not urgent” or the like.  It could further have something like, “4,712 issues have open discussions, here is a link.”  The idea that a single number could sum up technical debt is simply dangerous.  It encourages people to NOT investigate and learn more.  It encourages people to not listen.</p>
<p>“Dude, look, it’s really simple, your project has 4,700 sonarqube findings, everyone else has less than 100.  This is a job performance problem.”  – Overheard in a meeting</p>
<p>Finally, I have a question.  Are there articles posted by Sonarsource telling managers how to interpret sonarqube results properly?  I’ve been scouring the web for “how to handle minor sonarqube issues best practice” and found nothing useful to my case.  I’d welcome knowledge if anyone has any.</p>
            </div>

            

            

          </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Colin"><span itemprop="name">Colin</span></a>
                (Colin)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-07-07T11:10:25Z">
                    July 7, 2023, 11:10am
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T11:10:25Z">
              <span itemprop="position">11</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hey there <a href="https://community.sonarsource.com/u/jeremy_cox">@Jeremy_Cox</a>.</p>
<p>Thanks for the super valuable feedback (that we’ve been discussing a lot internally). I want to provide a few preliminary notes.</p>
<ul>
<li>
<p>We’re conscious of the fact that Sonar can be misused, and we’ve made some efforts to minimize how easy that is. We once had a feature (meant to “gamify” fixing issues and not introducing new ones) which… you guessed it, resulted in some managers using it to measure individual performance. Some organizations loved this feature! We removed it because it absolutely didn’t align with our values.</p>
</li>
<li>
<p>A big shift is going to come to our products soon (this year) that will shift focus on raising <em>findings</em> about <em>facts</em> about the code (<em>this is not a coding practice that aligns with our organization’s practices</em>), rather than <em>issues</em> with a hypothesized impact (“this may slightly impact developer productivity”). This repositioning might also make it easier to explain why you want to ignore a rule. T</p>
</li>
<li>
<p>As part of this shift we are also looking at how to make sure Clean Code (and Clean as You Code) is easy to understand by governance stakeholders and de-emphasize issue count. Yes, there will be product changes that have to happen as well as education/positioning that has to change (no silver bullets, only lots of reuglar ones)</p>
</li>
</ul>

<p>I don’t think we have anything regarding this. It’s a valid point I’ll pass on.</p>
<p>Please don’t hesitate to keep giving us honest feedback as things progress, or if you’ve thought of any other points you want to share. Feedback is a gift. <img src="https://emoji.discourse-cdn.com/twitter/gift.png?v=12" title=":gift:" alt=":gift:" loading="lazy" width="20" height="20"></p>
            </div>

            

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An epigenetic editor to silence genes (110 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/science.adq3334</link>
            <guid>40859876</guid>
            <pubDate>Tue, 02 Jul 2024 19:50:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/science.adq3334">https://www.science.org/doi/10.1126/science.adq3334</a>, See on <a href="https://news.ycombinator.com/item?id=40859876">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/science.adq3334: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Aboriginal ritual passed down over 12,000 years, cave find shows (178 pts)]]></title>
            <link>https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html</link>
            <guid>40859393</guid>
            <pubDate>Tue, 02 Jul 2024 18:48:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html">https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html</a>, See on <a href="https://news.ycombinator.com/item?id=40859393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/aboriginal-ritual-pass.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/aboriginal-ritual-pass.jpg" data-sub-html="The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: <i>Nature Human Behaviour</i> (2024). DOI: 10.1038/s41562-024-01912-w">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/aboriginal-ritual-pass.jpg" alt="Aboriginal ritual passed down over 12,000 years, cave find shows" title="The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: Nature Human Behaviour (2024). DOI: 10.1038/s41562-024-01912-w" width="800" height="530">
             <figcaption>
                The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: <i>Nature Human Behaviour</i> (2024). DOI: 10.1038/s41562-024-01912-w
            </figcaption>        </figure>
    </div><p>Two slightly burnt, fat-covered sticks discovered inside an Australian cave are evidence of a healing ritual that was passed down unchanged by more than 500 generations of Indigenous people over the last 12,000 years, according to new research.</p>


										      
																																	<p>The wooden sticks, found poking out of tiny fireplaces, showed that the <a href="https://phys.org/tags/ritual/" rel="tag">ritual</a> documented in the 1880s had been shared via oral traditions since the end of the last ice age, a study in the journal <i>Nature Human Behaviour</i> said on Monday.</p>
<p>The discovery was made inside Cloggs Cave in the foothills of the Victorian Alps in Australia's southeast, in a region long inhabited by the Gunaikurnai people.</p>
<p>When the cave was first excavated in the 1970s, archaeologists discovered the remains of a long extinct giant kangaroo that had previously lived there.</p>
<p>But the Gunaikurnai people were not involved in those digs, "nor were they asked for permission to do research there", lead study author Bruno David of Monash University told AFP.</p>
<p>Further excavations starting from 2020 included members of the local Gunaikurnai Land and Waters Aboriginal Corporation (GLaWAC).</p>
<p>Carefully digging through the soil, the team found a small stick poking out—then they found another one. Both well-preserved sticks were made from the wood of casuarina trees.</p>
<p>Each one was found in a separate fireplace around the size of the palm of a hand—far too small to have been used for heat or cooking meat.</p>
<p>The slightly charred ends of the sticks had been cut specially to stick into the fire, and both were coated in human or animal fat.</p>
<p>One stick was 11,000 years old and the other 12,000 years old, radiocarbon dating found.</p>

																																						
																																			<h2>'Memoirs of our ancestors'</h2>
<p>"They've been waiting here all this time for us to learn from them," said Gunaikurnai elder Russell Mullett, a co-author of the study and head of GLaWAC.</p>
<p>Mullett spent years trying to find out what they could have been used for, before discovering the accounts of Alfred Howitt, a 19th-century Australian anthropologist who studied Aboriginal culture.</p>
<p>Some of Howitt's notes had never been published, and Mullett said he spent a long time convincing a local museum to share them.</p>
<p>In the notes, Howitt describes in the late 1880s the rituals of Gunaikurnai medicine men and women called "mulla-mullung".</p>
<p>One ritual involved tying something that belonged to a sick person to the end of a throwing stick smeared in human or kangaroo fat. The stick was thrust into the ground before a small fire was lit underneath.</p>
<p>"The mulla-mullung would then chant the name of the sick person, and once the stick fell, the charm was complete," a Monash University statement said.</p>
<p>The sticks used in the ritual were made of casuarina wood, Howitt noted.</p>
<p>Jean-Jacques Delannoy, a French geomorphologist and study co-author, told AFP that "there is no other known gesture whose symbolism has been preserved for such a long time".</p>
<p>"Australia kept the memory of its first peoples alive thanks to a powerful oral tradition that enabled it to be passed on," Delannoy said.</p>
<p>"However in our societies, memory has changed since we switched to the written word, and we have lost this sense."</p>
<p>He lamented that the ancient animal paintings found in French caves would probably "never reveal their meaning" in a similar way.</p>
<p>Indigenous Australians are one of the oldest continuous living cultures, and Mullett said the discovery was a "unique opportunity to be able to read the memoirs of our ancestors".</p>
<p>It was "a reminder that we are a living culture still connected to our ancient past," he added.</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Bruno David et al, Archaeological evidence of an ethnographically documented Australian Aboriginal ritual dated to the last ice age, <i>Nature Human Behaviour</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41562-024-01912-w" target="_blank">DOI: 10.1038/s41562-024-01912-w</a>
																						
																						</p>
																					</div>
                               											
																															 <p>
												  © 2024 AFP
											 </p>
										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Aboriginal ritual passed down over 12,000 years, cave find shows (2024, July 2)
												retrieved 3 July 2024
												from https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring biphasic programming: a new approach in language design (110 pts)]]></title>
            <link>https://rybicki.io/blog/2024/06/30/biphasic-programming.html</link>
            <guid>40858670</guid>
            <pubDate>Tue, 02 Jul 2024 17:20:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rybicki.io/blog/2024/06/30/biphasic-programming.html">https://rybicki.io/blog/2024/06/30/biphasic-programming.html</a>, See on <a href="https://news.ycombinator.com/item?id=40858670">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I’ve noticed a small but interesting trend in the programming languages space. I’m not sure how novel it is, but this pattern, which I’ll refer to as “biphasic programming,” is characterized by languages and frameworks that enable identical syntax to express computations executed in two distinct phases or environments while maintaining consistent behavior (i.e., semantics) across phases. These phases typically differ temporally (when they run), spatially (where they run), or both.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>“Biphasic programming” is a term I’ve coined, but I feel like it helps capture the essence of several languages. What’s interesting to me is how it can be applied to different types of problems. To illustrate the concept, I’ll go through a few examples.</p>

<h2 id="zig">Zig</h2>

<p>The first example is Zig. <a href="https://ziglang.org/">Zig</a> is a systems programming language that lets you write highly performant code with relatively easy incremental adoption into C/C++ codebases. One of its main innovations is a fresh approach to metaprogramming called “comptime” which allows you to run ordinary functions at compile time.</p>

<p>What makes comptime unique compared to preprocessing systems and macro systems like those in C, C++, and Rust is that it gives you the same <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup> expressivity of the base language through the “comptime” keyword, instead of introducing an entirely separate domain-specific language that only advanced users might want to learn. Here’s a (silly) example from their docs:</p>

<div><pre><code><span>const</span> <span>expect</span> <span>=</span> <span>@import</span><span>(</span><span>"std"</span><span>).</span><span>testing</span><span>.</span><span>expect</span><span>;</span>

<span>fn</span> <span>fibonacci</span><span>(</span><span>index</span><span>:</span> <span>u32</span><span>)</span> <span>u32</span> <span>{</span>
    <span>if</span> <span>(</span><span>index</span> <span>&lt;</span> <span>2</span><span>)</span> <span>return</span> <span>index</span><span>;</span>
    <span>return</span> <span>fibonacci</span><span>(</span><span>index</span> <span>-</span> <span>1</span><span>)</span> <span>+</span> <span>fibonacci</span><span>(</span><span>index</span> <span>-</span> <span>2</span><span>);</span>
<span>}</span>

<span>test</span> <span>"fibonacci"</span> <span>{</span>
    <span>// test fibonacci at run-time</span>
    <span>try</span> <span>expect</span><span>(</span><span>fibonacci</span><span>(</span><span>7</span><span>)</span> <span>==</span> <span>13</span><span>);</span>

    <span>// test fibonacci at compile-time</span>
    <span>try</span> <span>comptime</span> <span>expect</span><span>(</span><span>fibonacci</span><span>(</span><span>7</span><span>)</span> <span>==</span> <span>13</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>As a case of biphasic programming, comptime lets Zig users seamlessly switch between running code during build time versus during runtime within their source code in a way that doesn’t introduce a steep learning curve. It shifts the developer’s mental model from thinking of metaprogramming as advanced wizardry to being more of an optimization tool that can also be leveraged to implement generics and other code generation uses. I haven’t had a chance to write many Zig programs yet but the comptime system seems like a clever approach to reduce both compiler complexity and soften the language’s learning curve.</p>

<p>For what it’s worth, compile-time code execution isn’t a brand-new idea. However, Zig’s approach does seem to avoid several drawbacks. For example, unlike Rust and its <a href="https://doc.rust-lang.org/reference/const_eval.html">const functions</a>, Zig doesn’t impose <a href="https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/">function coloring</a> for comptime functions. Likewise, unlike C++’s <a href="https://en.cppreference.com/w/cpp/language/template_parameters">templating system</a>, Zig doesn’t introduce any new syntax for representing generics. And compared to Lisps like Scheme and Racket which support hygenic macros, well, Zig doesn’t require everything to be a list.</p>

<p><strong>TL;DR:</strong> Zig supports a form of biphasic programming where the same functions can run in either of two distinct phases, which differ temporally (build time vs runtime) and spatially (on the build system vs on the machine running the binary).</p>

<h2 id="react-server-components">React Server Components</h2>

<p>The second example of biphasic programming I’ve noticed is <a href="https://react.dev/reference/rsc/server-components">React Server Components</a> (RSC). React isn’t a language of its own, but as a JavaScript web framework, it has quite a sizeable mindshare as a foundational system for writing and composing UI components and their associated UI logic for large websites. Lately, the front-end JavaScript ecosystem has been doing a lot of exploration to figure out how to most efficiently render UI components on either the server or client to improve page performance. Many solutions have been proposed, and one of the most ambitious is RSC.</p>

<p>The idea behind RSC is to allow a React component to specify whether it should be rendered on the server side or the client side and to allow such components to be composed together freely. For example, a <code>Feed</code> component might be rendered on the server (as it needs to fetch the list of feed items from the database), while each child <code>FeedItem</code> can be rendered on the client (as they’re pure functions of the item state), while a <code>FeedItemPreview</code> may be rendered on the server (since it needs to fetch the item’s content from the database). The developer can choose which components should be calculated where, and the underlying engine (usually a JavaScript bundler that produces both server-side code and client-side code) optimizes everything so that components are rendered on the server or client when needed, minimizing the amount of dynamic HTML and component information shipped back and forth.</p>

<p><img src="https://www.plasmic.app/blog/static/images/react-server-components.png" alt="Whiteboard diagram of React components"></p>

<p>This is just my rough understanding of RSC. From what I’ve heard, getting this all working and stabilized is still a massive work in progress. But I think the paradigm is a curious example of biphasic programming. There are many ways one could go about reducing the amount of code that needs to be shipped and executed on a client browser and offloading more work onto the server, but most existing solutions today require developers to treat React components as a pure client-side abstraction, or as a pure server-side abstraction. For example, either an entire page is rendered on the server, or an entire page is rendered on the client, and vice versa. Taking the React component model and letting the developer switch where a component should be rendered feels like it could be a powerful abstraction if the engine can be optimized enough and if the generated code can be made sufficiently debuggable.</p>

<p><strong>TL;DR:</strong> React Server Components promises a form of biphasic programming where the same JavaScript + JSX syntax can be used to represent components that are rendered on the server or client and can be flexibly composed. Server-side and client-side rendering operate at the same time, but they differ spatially (on the server vs on your browser).</p>

<blockquote>
  <p>I also want to give an honorable mention to <a href="https://github.com/hyperfiddle/electric">Electric Clojure</a>, a project I discovered at a lightning talk its creator gave at <a href="https://systemsdistributed.com/">Systems Distributed</a> which applies a similar idea to offer strong composition over the frontend/backend boundary, but using the Clojure language. I’m not familiar enough with it to cover it in detail, but I’ve included a screenshot from their repo that hopefully suggests how it parallels React Server Components.</p>

  <p><img src="https://github.com/hyperfiddle/electric/raw/master/docs/electric-explainer-5.png" alt="Screenshot of Clojure code"></p>
</blockquote>

<h2 id="winglang">Winglang</h2>

<p>A large part of the reason I’ve been so curious about this “biphasic programming” idea is that for the past two years, I’ve been working on <a href="https://www.winglang.io/">Winglang</a>, a new programming language for writing cloud applications, which embraces this concept pretty heavily in its design.
This project is the most nascent of the three examples I’m covering (it’s only been in development for two years), but for this post I’m going to try and keep the introduction as short possible to give just enough context for its biphasic type system.</p>

<p>The gist behind Winglang is that thanks to the availability of vast amounts of compute, major cloud providers like AWS, Azure, and GCP have been able to provide developers with a variety of scalable, high-level services like queues, pub-sub topics, workflows, streams, storage buckets, etc. Colloquially, these are often called resources. Infrastructure-as-code tools like <a href="https://www.terraform.io/">Terraform</a> and <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-overview.html">CloudFormation</a> make it possible to manage these resources with JSON or YAML.</p>

<p>In principle, it shouldn’t be hard build complex applications with these resources. But if your application large enough and has many resources, it can become error-prone explicitly wiring up every single serverless function or container service with the permissions and configuration of its required resources. It’s also difficult to design custom interfaces around these resources.</p>

<p>Winglang aims to let you write libraries and applications that compose both infrastructure resources and application logic together, through what the language calls preflight and inflight code. Here’s an example program to demonstrate:<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup></p>

<div><pre><code><span>// Import some libraries.</span>
<span>bring</span> <span>s3</span><span>;</span>
<span>bring</span> <span>lambda</span><span>;</span>
<span>bring</span> <span>redis</span><span>;</span>
<span>bring</span> <span>triggers</span><span>;</span>

<span>// Define our abstraction.</span>
<span>class</span> <span>Cache</span> <span>{</span>
    <span>_redis</span><span>:</span> <span>redis</span><span>.</span><span>Redis</span><span>;</span>
    <span>_bucket</span><span>:</span> <span>s3</span><span>.</span><span>Bucket</span><span>;</span>
    <span>new</span><span>()</span> <span>{</span>
        <span>this</span><span>.</span><span>_redis</span> <span>=</span> <span>new</span> <span>redis</span><span>.</span><span>Redis</span><span>();</span>
        <span>this</span><span>.</span><span>_bucket</span> <span>=</span> <span>new</span> <span>s3</span><span>.</span><span>Bucket</span><span>();</span>
    <span>}</span>

    <span>pub</span> <span>inflight</span> <span>get</span><span>(</span><span>key</span><span>:</span> <span>str</span><span>):</span> <span>str</span> <span>{</span>
        <span>// Check Redis first, otherwise fall back to S3</span>
        <span>let</span> <span>var</span> <span>value</span> <span>=</span> <span>this</span><span>.</span><span>_redis</span><span>.</span><span>get</span><span>(</span><span>key</span><span>);</span>
        <span>if</span> <span>value</span> <span>==</span> <span>nil</span> <span>{</span>
            <span>value</span> <span>=</span> <span>this</span><span>.</span><span>_bucket</span><span>.</span><span>getObject</span><span>(</span><span>key</span><span>);</span>
            <span>this</span><span>.</span><span>_redis</span><span>.</span><span>set</span><span>(</span><span>key</span><span>,</span> <span>value</span><span>!</span><span>);</span>
        <span>}</span>
        <span>return</span> <span>value</span><span>!</span><span>;</span>
    <span>}</span>

    <span>pub</span> <span>inflight</span> <span>set</span><span>(</span><span>key</span><span>:</span> <span>str</span><span>,</span> <span>value</span><span>:</span> <span>str</span><span>)</span> <span>{</span>
        <span>// Update S3 and redis with the new entry</span>
        <span>this</span><span>.</span><span>_bucket</span><span>.</span><span>putObject</span><span>(</span><span>key</span><span>,</span> <span>value</span><span>);</span>
        <span>this</span><span>.</span><span>_redis</span><span>.</span><span>set</span><span>(</span><span>key</span><span>,</span> <span>value</span><span>);</span>
    <span>}</span>

    <span>pub</span> <span>inflight</span> <span>reset</span><span>()</span> <span>{</span>
        <span>this</span><span>.</span><span>_redis</span><span>.</span><span>flush</span><span>();</span>
        <span>this</span><span>.</span><span>_bucket</span><span>.</span><span>empty</span><span>();</span>
    <span>}</span>
<span>}</span>

<span>let</span> <span>cache</span> <span>=</span> <span>new</span> <span>Cache</span><span>();</span>

<span>// Empty the cache once an hour.</span>
<span>let</span> <span>schedule</span> <span>=</span> <span>new</span> <span>triggers</span><span>.</span><span>Schedule</span><span>(</span><span>rate</span><span>:</span> <span>1</span><span>h</span><span>);</span>
<span>schedule</span><span>.</span><span>onTick</span><span>(</span><span>inflight</span> <span>()</span> <span>=&gt;</span> <span>{</span>
    <span>cache</span><span>.</span><span>reset</span><span>();</span>
<span>});</span>

<span>// Create an AWS Lambda function to do some fake business logic.</span>
<span>let</span> <span>fn</span> <span>=</span> <span>new</span> <span>lambda</span><span>.</span><span>Function</span><span>(</span><span>inflight</span> <span>(</span><span>key</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>let</span> <span>value</span> <span>=</span> <span>cache</span><span>.</span><span>get</span><span>(</span><span>key</span><span>!</span><span>);</span>
    <span>return</span> <span>"</span><span>Found value: </span><span>"</span> <span>+</span> <span>value</span><span>;</span>
<span>});</span>

<span>// Publish the function to a public URL.</span>
<span>fn</span><span>.</span><span>expose</span><span>();</span>
</code></pre></div>

<p>At the top-level scope of the program, all code is preflight. Among other things, we can define classes, instantiate resources, and call preflight functions (like <code>onTick()</code> and <code>expose()</code>) to augment and create infrastructure. These statements are executed at compile time. But wherever the <code>inflight</code> keyword is used, we’re introducing a scope for code that can only run once the application is deployed to the cloud. <code>get()</code>, <code>set()</code>, and <code>reset()</code> are all inflight functions.</p>

<p>The Winglang compiler enforces several phase-related invariants. For example, inflight functions can reference data from preflight, but they can’t call preflight functions, since doing so could modify your graph of resources. Likewise, preflight functions can’t run inflight functions, but they can convert inflight functions into bundled JavaScript. (Yes, Winglang relies on JavaScript as its underlying runtime).<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup> But despite these rules, preflight code and inflight code are otherwise grounded in the same syntax. Both provide access to the same language facilities like variables, for loops, structs, strings, arrays, classes, and so on.</p>

<p>It’s possible to draw parallels between Winglang’s preflight/inflight distinction and Zig’s comptime/runtime distinction. But it’s probably no surprise that since the languages have been built around different use cases, they’ve ended up with pretty different designs. For example, Zig’s comptime aims to avoid all potential side effects, while Winglang’s preflight encourages side effects so you can mutate your infrastructure graph.</p>

<p><strong>TL;DR:</strong> Wing offers a form of biphasic programming where code can be executed for defining cloud infrastructure, or for interacting interacting with cloud infrastructure. These two phases, called preflight and inflight, differ temporally (compile time vs runtime) and spatially (preflight runs on the build system while inflight code may be executed on any compute system that supports a JavaScript runtime).</p>

<h2 id="so-what">So what?</h2>

<p>One takeaway is that this biphasic programming thing can be used to solve a lot of different problems. In Zig, it makes it easy for people to do compile-time metaprogramming. In React, it makes it possible to write more specialized and optimized frontend apps. In Wing, it lets you model the infrastructure and application concerns of a distributed program. That’s pretty cool!</p>

<p>But there’s likely more to explore here - like how the rules of these biphasic solutions overlap or differ. In Zig, every function that you can run at comptime is also safe to run at runtime - so we can say there’s a subset relationship between what functions can be run at comptime and which can be run at runtime. The same applies to React Server Components - any component that you can render on the client can also be rendered on the server. But in Wing, the two phases of preflight and inflight are strictly separate, so to represent code that can run in either phase, you’d need a separate label for these functions (like “unphased functions”).</p>

<p>Another open question is understanding to what degree biphasic programming represents capabilities that can’t be expressed in normal languages. Zig needed a new keyword for this comptime thing - but are there other existing languages that let you do this, perhaps in userland? Does providing it as a dedicated language feature provide any improved safety or error handling?</p>


  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bruce Bastian, WordPerfect co-creator, has died (261 pts)]]></title>
            <link>https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/</link>
            <guid>40858583</guid>
            <pubDate>Tue, 02 Jul 2024 17:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/">https://www.heraldextra.com/news/local/2024/jun/17/bruce-bastian-byu-alum-turned-tech-pioneer-and-equality-advocate-dies-at-76/</a>, See on <a href="https://news.ycombinator.com/item?id=40858583">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article_content">
															
								    <p><img width="350" height="500" src="https://ogden_images.s3.amazonaws.com/www.heraldextra.com/images/2024/06/17151419/50372586-8111-47C2-A83E-7E1AC692B4E5-350x500.jpeg" alt="">								    </p>
																<div id="caption"><p><span>Courtesy Human Rights Campaign</span></p><p>Bruce Bastian, a former tech entrepreneur, philanthropist and advocate for LGBTQ+ communities, died Sunday, June 16, 2024, at age 76.</p></div>														<!-- <input type=radio id="show" name="group"><label for="show">SHOW ARTICLE</label> 
							<div class="hide_article"> -->
								<p>Bruce Bastian, an alumnus of Brigham Young University who pioneered a successful word-processing application and later would become an advocate for the LGBTQ+ community, died Sunday.</p>
<p>Bastian, 76, passed away surrounded by his four sons, husband Clint Ford, friends and other family members, the <a href="https://www.hrc.org/press-releases/human-rights-campaign-mourns-the-loss-of-bruce-bastian-champion-for-lgbtq-equality-hrc-board-member-for-22-years" target="_blank" rel="noopener">Human Rights Campaign</a> reported.</p>
<p>His death was marked family and members of the LGBTQ+ community he championed, as well as other organizations he was involved with.</p>
<p>“I think people will remember him for a number of reasons, from his work with technology, his philanthropy, and within the LGBTQ community. As for others, we’ll remember him as a loving husband and father,” Bastian’s oldest son, Rick, told the Daily Herald.</p>
<p>Bastian was a member of the Human Rights Campaign, Encircle, Equality Utah and the Utah Pride Center. He traveled to the nation’s capital and fought for equal marriage rights while advocating for inclusion of people with differing sexual orientations.</p>
<p>“Bruce was in this fight, working at every level of politics and advocacy, for over three decades,” Kelley Robinson, president of the Human Rights Campaign, said in a press release. “He traveled all across this country on HRC’s behalf and worked tirelessly to help build an inclusive organization where more people could be a part of this work.”</p>
<p>Bastian also was a major supporter of organizations like Equality Utah, Utah Pride Center, and Encircle.</p>
<p><a href="https://encircletogether.org/" target="_blank" rel="noopener">Encircle</a> has a series of homes throughout Utah that provide mental health services, resources and tools for LGBTQ+ youth and families. Encircle opened its first location seven years ago <a href="https://encircletogether.org/visit-a-home" target="_blank" rel="noopener">in Provo</a>, and it was named after Bastian and Ford.</p>
<p>“Bruce was an invaluable member of our community and worked tirelessly to make our country a safer place for LGBTQ+ individuals,” Encircle’s CEO, Jordan Sgro, said in an emailed statement sent to the Daily Herald. “He was instrumental in building Encircle and we would not be where we are today without support from Bruce and his husband, Clint. Most importantly, Bruce was a friend and an incredible mentor, and served for years on our Board of Directors. He will be greatly missed.”</p>
<p>Prior to his social impact and philanthropic work, or even entrepreneurship, Bastian moved to Utah from southern Idaho to attend Brigham Young University. During the mid 1970s, he served as director of the <a href="https://byucougars.com/the-power-of-the-wasatch" target="_blank" rel="noopener">Cougar Marching Band</a>.</p>
<p>In 1979, while still attending the university as a graduate student in computer science, he co-founded what would eventually become WordPerfect Corp. along with faculty member Alan Ashton. It initially was developed as a word-processing software for a minicomputer owned by the City of Orem. Bastian and Ashton were able to maintain ownership of the software.</p>
<p>The company served as a dominant force in the technology space throughout the 1980s and 1990s. At one point, Bastian was worth $840 million, the <a href="https://www.deseret.com/2003/6/22/19730449/bastian-s-profile-low-151-in-utah-at-least/" target="_blank" rel="noopener">Deseret News</a> reported in 2003.</p>
<p>Bastian stepped down from his role as chairman of WordPerfect in 1994 and the company was sold to Novell a short time later.</p>
<p>Bastian would go on to focus his time on charitable causes and philanthropy. In 1997, he started the <a href="https://bastianfoundation.org/about_us" target="_blank" rel="noopener">B.W. Bastian Foundation</a>, whose commitment is to only support organizations that fully embrace equality.</p>
<p>“The impact he had on so many lives was immeasurable,” Michael Marriott, the foundation’s executive director, said in a press release. “His spirit and memory will live on through Clint, his husband of six years, through Bruce’s four sons and their families, and through the many lives he touched through his generosity, time, energy and commitment to making the world a better place. And Bruce’s legacy will continue in the work of the B.W. Bastian Foundation and its mission.”</p>
<p>Bastian also maintained his love for music and the arts. In 2010, then-President Barack Obama <a href="https://obamawhitehouse.archives.gov/realitycheck/the-press-office/president-obama-announces-more-key-administration-posts-22610" target="_blank" rel="noopener">appointed him to the Presidential Advisory Committee on the Arts</a>.</p>
<p>Bastian continued to use his resources and fortune to support organizations providing services to Utah’s LGBTQ+ community and other pro-equality causes, including the Utah Democratic Party.</p>
<p>“Bruce Bastian was a light to the people of our state,” Utah Democratic Party Chair Diane Lewis said in a statement. “His example calls on us to do more, especially when it comes to supporting our LGBTQ+ community.”</p>
<p>Bastian was born March 23, 1948, in Twin Falls, Idaho. He grew up on his family’s farm before moving south to Provo to attend BYU, where he earned a bachelor’s in music education and his master’s in computer science.</p>
<p>His adult life was spent in Orem and Palm Springs, California, where he lived with Ford.</p>
<p>In addition to his partner and four sons, Bastian also leaves behind 14 grandchildren, two sisters and a brother.</p>
<p>Rick Bastian says he wants his father to be remembered as being courageous, someone who stood up for social justice and advocated for others. “We’ll miss him dearly,” he said.</p>

														                        


<h3>Newsletter</h3>
<section id="newsletter">
    <p>Join thousands already receiving our daily newsletter.</p>
    
</section>



					</section></div>]]></description>
        </item>
    </channel>
</rss>