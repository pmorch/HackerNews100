<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 07 Sep 2024 19:30:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[WebP: The WebPage Compression Format (104 pts)]]></title>
            <link>https://purplesyringa.moe/blog/webp-the-webpage-compression-format/</link>
            <guid>41475124</guid>
            <pubDate>Sat, 07 Sep 2024 17:32:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://purplesyringa.moe/blog/webp-the-webpage-compression-format/">https://purplesyringa.moe/blog/webp-the-webpage-compression-format/</a>, See on <a href="https://news.ycombinator.com/item?id=41475124">Hacker News</a></p>
Couldn't get https://purplesyringa.moe/blog/webp-the-webpage-compression-format/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA['Right to Repair for Your Body': The Rise of DIY, Pirated Medicine (128 pts)]]></title>
            <link>https://fourthievesvinegar.org/</link>
            <guid>41474080</guid>
            <pubDate>Sat, 07 Sep 2024 14:19:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fourthievesvinegar.org/">https://fourthievesvinegar.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41474080">Hacker News</a></p>
Couldn't get https://fourthievesvinegar.org/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Conservative GC can be faster than precise GC (105 pts)]]></title>
            <link>https://wingolog.org/archives/2024/09/07/conservative-gc-can-be-faster-than-precise-gc</link>
            <guid>41473061</guid>
            <pubDate>Sat, 07 Sep 2024 10:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wingolog.org/archives/2024/09/07/conservative-gc-can-be-faster-than-precise-gc">https://wingolog.org/archives/2024/09/07/conservative-gc-can-be-faster-than-precise-gc</a>, See on <a href="https://news.ycombinator.com/item?id=41473061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Should your garbage collector be precise or conservative?  The
prevailing wisdom is that precise is always better.  Conservative GC can
retain more objects than strictly necessary, making GC slow: GC has to
more frequently, and it has to trace a larger heap on each collection.
However the calculus is not as straightforward as most people think, and
indeed there are some reasons to expect that conservative root-finding
can result in faster systems.</p><p>(I have made / relayed some of these arguments before but I feel like a
dedicated article can make a contribution here.)</p><h3>problem precision</h3><p>Let us assume that by <i>conservative GC</i> we mean conservative
root-finding, in which the collector assumes that any integer on the
stack that happens to be a heap address indicates a reference on the
object containing that address.  The address doesn’t have to be at the
start of the object.  Assume that objects on the heap are traced
precisely; contrast to BDW-GC which generally traces both the stack and
the heap conservatively.  Assume a collector that will pin referents of
conservative roots, but in which objects not referred to by a
conservative root can be moved, as in <a href="https://dl.acm.org/doi/10.1145/2660193.2660198">Conservative
Immix</a> or Whippet’s
<a href="https://github.com/wingo/whippet/blob/main/doc/collector-mmc.md#conservative-stack-scanning"><tt>stack-conservative-mmc</tt>
collector</a>.</p><p>With that out of the way, let’s look at some reasons why conservative GC
might be faster than precise GC.</p><h3>smaller lifetimes</h3><p>A compiler that does precise root-finding will typically output a
side-table indicating which slots in a stack frame hold references to
heap objects.  These lifetimes aren’t always precise, in the sense that
although they precisely enumerate heap references, those heap references
might actually not be used in the continuation of the stack frame.  When
GC occurs, it might mark more objects as live than are actually live,
which is the imputed disadvantage of conservative collectors.</p><p>This is most obviously the case when you need to explicitly register
roots with some kind of handle API: the handle will typically be kept
live until the scope ends, but that might be an overapproximation of
lifetime.  A compiler that can assume conservative stack scanning may
well exhibit more precision than it would if it needed to emit stack
maps.</p><h3>no run-time overhead</h3><p>For generated code, stack maps are great.  But if a compiler needs to
call out to C++ or something, it needs to precisely track roots in a
<a href="https://github.com/v8/v8/blob/main/src/handles/handles.h">run-time data
structure</a>.
This is overhead, and conservative collectors avoid it.</p><h3>smaller stack frames</h3><p>A compiler may partition spill space on a stack into a part that
contains pointers to the heap and a part containing numbers or other
unboxed data.  This may lead to larger stack sizes than if you could
just re-use a slot for two purposes, if the lifetimes don’t overlap.  A
similar concern applies for compilers that partition registers.</p><h3>no stack maps</h3><p>The need to emit stack maps is annoying for a compiler and makes
binaries bigger.  Of course it’s necessary for precise roots.  But then
there is additional overhead when tracing the stack: for each frame on
the stack, you need to look up the stack map for the return
continuation, which takes time.  It may be faster to just test if words
on the stack might be pointers to the heap.</p><h3>unconstrained compiler</h3><p>Having to make stack maps is a constraint imposed on the compiler.
Maybe if you don’t need them, the compiler could do a better job, or you
could use a different compiler entirely.  A conservative compiler can sometimes have better codegen, for example by the use of interior pointers.</p><h3>anecdotal evidence</h3><p>The <a href="https://dl.acm.org/doi/10.1145/2660193.2660198">Conservative Immix</a>
paper shows that conservative stack scanning can beat precise scanning
in some cases.  I have reproduced these results with
<a href="https://github.com/wingo/whippet/blob/main/doc/collector-mmc.md#conservative-stack-scanning"><tt>parallel-stack-conservative-mmc</tt> compared to
<tt>parallel-mmc</tt></a>.
It’s small—maybe a percent—but it was a surprising result to me and I
thought others might want to know.</p><p>Also, Apple’s JavaScriptCore uses conservative stack scanning, and <a href="https://wingolog.org/archives/2023/12/07/the-last-5-years-of-v8s-garbage-collector">V8 is looking at switching to it</a>.  Funny, right?</p><h3>conclusion</h3><p>When it comes to designing a system with GC, don’t count out
conservative stack scanning; the tradeoffs don’t obviously go one way or the other, and conservative scanning might be the right engineering choice for your system.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The PERQ Computer (124 pts)]]></title>
            <link>https://graydon2.dreamwidth.org/313862.html</link>
            <guid>41472855</guid>
            <pubDate>Sat, 07 Sep 2024 09:58:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graydon2.dreamwidth.org/313862.html">https://graydon2.dreamwidth.org/313862.html</a>, See on <a href="https://news.ycombinator.com/item?id=41472855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A note on the PERQ computer.</p><p>Through a sequence of random events (seeing a note about an Alto emulator, listening to a truly atrocious podcast-retelling of the NeXT computer company) I found myself reading about the <a href="https://bitsavers.org/pdf/perq/PERQ_Brochure.pdf">PERQ</a> computer this evening.</p><p>Reader: the modern <a href="https://en.wikipedia.org/wiki/Mac_(computer)">Mac</a> is not "a copy of an <a href="https://en.wikipedia.org/wiki/Xerox_Alto">Alto</a>". I mean it kinda is. But more recently than that, it's really a copy of a <a href="https://en.wikipedia.org/wiki/PERQ">PERQ</a>.</p><p><img src="https://p.dreamwidth.org/20661e79c4f3/684470-313862/venge.net/graydon/perq.png" width="400"><img src="https://p.dreamwidth.org/f695b1792867/684470-313862/venge.net/graydon/perq-guy.png" width="400"></p><p>The PERQ is an early, commercial, and technical-user-focused version of an Alto. Except not quite. It had the same fast CPU, large local memory and bitmapped display with fast microcoded rasterops (a so-called "<a href="https://en.wikipedia.org/wiki/3M_computer">3M machine</a>" -- 1 MIPS CPU, 1 megabyte RAM, 1 megapixel display, "and 1 megapenny of price", about $10,000). It had the same GUI with overlapping windows. It also had Pascal, Fortran, C and Lisp. It also demoed and started taking orders in 1979 and shipped in 1980, before the competition, including before Xerox. The Xerox <a href="https://en.wikipedia.org/wiki/Xerox_Star">Star</a> (Xerox did finally commercialize the Alto) and <a href="https://en.wikipedia.org/wiki/Apollo/Domain">Apollo/Domain</a> each shipped a year later, in 1981.</p><p>The <a href="https://en.wikipedia.org/wiki/Sun-1">Sun-1</a>? Another year out, 1982. The Apple <a href="https://en.wikipedia.org/wiki/Apple_Lisa">Lisa</a>? A third the screen real estate and another year out, 1983. Mac? <a href="https://en.wikipedia.org/wiki/1984_(advertisement)">1984</a> of course. After the Mac (which was not especially successful), Steve Jobs actually wound up in Apple's SuperMicro division trying to make a 3M machine for real, to crack that market -- a market mostly consisting of PERQs, Suns and Apollos at the time. Apple's foray was going to be the <a href="https://en.wikipedia.org/wiki/Big_Mac_(computer)">Big Mac</a>. It never shipped. When he left Apple, that team went with him to <a href="https://en.wikipedia.org/wiki/NeXT">NeXT</a>, where .. they tried again to build a 3M machine. And they did! Just extremely late, in 1989. And still $10k, despite almost a decade of brutal price competition on the low end.</p><p>The PERQ was <em>literally</em> built to be a commercial Alto, a version-you-could-buy. But it was also not from Palo Alto, or Mountain View, or Cupertino, or anywhere in California. It was from the much less flashy but extremely important computer town of <a href="https://en.wikipedia.org/wiki/Pittsburgh">Pittsburgh, PA</a>. It was built by a <a href="https://en.wikipedia.org/wiki/Carnegie_Mellon_University">CMU</a> spinoff: the <a href="https://en.wikipedia.org/wiki/Three_Rivers_Computer_Corporation">Three Rivers Computer Company</a> (Pittsburgh is at the confluence of 3 rivers). One of the company's founders -- Brian Rosen -- actually went to work on the Star at Xerox PARC for two years, from 76 to 78, and then came back to Three Rivers to pitch everything-he-learned as the basis for a new machine, which became the PERQ. (It was briefly even called the "Pascalto", because like all machines in this genre it supported user-written microcode and custom instruction sets, and the PERQ ran Pascal P-code. Through a microcode emulator. Things were wild.)</p><p>Ok so maybe the PERQ is just Alto in commercial clothing? No there's more! The PERQ didn't run <a href="https://en.wikipedia.org/wiki/Pilot_(operating_system)">Xerox Pilot</a> or whatever, it ran either PNX (a straight Unix port done by <a href="https://en.wikipedia.org/wiki/International_Computers_Limited">ICL</a> running on yet another microcoded VM, C-code) or this other operating system called <a href="https://en.wikipedia.org/wiki/Accent_kernel">Accent</a>. What's that? Why, it's the predecessor of <a href="https://en.wikipedia.org/wiki/Mach_(kernel)">Mach</a>! Written by CMU Professor <a href="https://en.wikipedia.org/wiki/Richard_Rashid">Rick Rashid</a> and his grad student <a href="https://en.wikipedia.org/wiki/Avie_Tevanian">Avie Tevanian</a>. Does that name sound familiar? Avie and Mach are what NeXT bet their farm on in 1988. Mach is what Apple bet their farm on when they bought NeXT, and is what all of today's Apple stuff from watches to phones to laptops runs on. And when Steve pitched Avie to join NeXT in 1986 it was because of the <a href="https://eecs582.github.io/readings/mach-usenix86.pdf">Usenix paper Avie just published</a> about Mach, which ran on VAX, <a href="https://en.wikipedia.org/wiki/IBM_RT_PC">IBM RT/PC</a> (the RS/6000 predecessor) and ... PERQ. Because Avie and Rick, like everyone at CMU, were big PERQ fans, had PERQs all over their department as surrogate Altos. PERQs were the Altos you could buy, that CMU had bought a bunch of, that ran Unix and Mach.</p><p>But wait, why did CMU even want surrogate Altos? How did CMU people have any connection to Altos <em>before</em> PERQs, what motivated that connection, and .. how did Xerox wind up connected here? Aha! Through the even less well-thought-of neighboring city of <a href="https://en.wikipedia.org/wiki/Rochester,_New_York">Rochester, NY</a>! Xerox isn't a west coast company at all. <a href="https://en.wikipedia.org/wiki/Xerox#History">It's from Rochester</a>. Because of even older origin-stories involving the <a href="https://en.wikipedia.org/wiki/Institute_of_Optics">optics</a> business, and Kodak, and (long digression here into east-coast tech history). Anyway it's from Rochester. But their chief scientist <a href="https://en.wikipedia.org/wiki/Jack_Goldman">Jack Goldman</a> was former faculty from CMU which is a short drive from Rochester, and he set up a wild unsupervised west-coast lab in Palo Alto, called Xerox PARC, and when PARC made the Alto, Xerox HQ back in Rochester naturally donated a bunch of them to University of Rochester, and Rick Rashid (then a Rochester PhD) and Avie Tevanian (then a Rochester undergrad) spent their days at Rochester hacking video games on the Alto. And dreaming of someday having their own.</p><p>And then they went down the road to CMU: Rick to a professorship, and Avie to be his student. And CMU was Jack Goldman's alma mater, and so Xerox had also donated a bunch of Altos there. And CMU was enjoying their donated Altos so much they had started up a 3M machine joint hardware-software project: <a href="https://bitsavers.org/pdf/cmu/spice/A_Proposal_For_A_Joint_Effort_In_Personal_Scientific_Computing_Aug1979.pdf">SPICE</a>. Which had ARPA money and was going to involve buying 200 machines from their former colleagues down the street at Three Rivers Computer Company -- 200 PERQs. Which was the first of the 3M machines everyone actually bought and used, in the years between the 3M machine becoming a cool idea and the market imploding right as NeXT tried to enter it.</p><p>So anyway, short story long: the path to modern macOS and iOS machines is less than 100% sunny California people; it involves quite a bit of slushy rust belt grad students (fun fact: mach is named after Pittsburgh winter slush, a mishearing of the word "muck".)</p><p>(There's also much more here involving a <a href="https://www.chilton-computing.org.uk/acd/sus/overview.htm">whole joint development situation with ICL</a>, not just a Unix port, which you definitely should fall down the rabbit hole of -- especially if you're not familiar with ICL itself! -- but I think I've talked enough here already.)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Keyhole – Forge own Windows Store licenses (406 pts)]]></title>
            <link>https://massgrave.dev/blog/keyhole</link>
            <guid>41472643</guid>
            <pubDate>Sat, 07 Sep 2024 09:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://massgrave.dev/blog/keyhole">https://massgrave.dev/blog/keyhole</a>, See on <a href="https://news.ycombinator.com/item?id=41472643">Hacker News</a></p>
Couldn't get https://massgrave.dev/blog/keyhole: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Richard Feynman and the Connection Machine (1989) (109 pts)]]></title>
            <link>https://longnow.org/essays/richard-feynman-and-connection-machine/</link>
            <guid>41472135</guid>
            <pubDate>Sat, 07 Sep 2024 07:20:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://longnow.org/essays/richard-feynman-and-connection-machine/">https://longnow.org/essays/richard-feynman-and-connection-machine/</a>, See on <a href="https://news.ycombinator.com/item?id=41472135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        





<p>
    Published on Sunday, January 15, 01989 &nbsp;•&nbsp; <span>35&nbsp;years, 7&nbsp;months ago</span><br> 
    Written by <span>Danny Hillis</span> for <span>Physics Today</span>
</p>

        <p><img src="https://media.longnow.org/files/2/Feynman.JPG" alt=""></p>
<p>One day when I was having lunch with Richard Feynman, I mentioned to him that        I was planning to start a company to build a parallel computer with a million        processors. His reaction was unequivocal, "That is positively the dopiest idea        I ever heard." For Richard a crazy idea was an opportunity to either prove it        wrong or prove it right. Either way, he was interested. By the end of lunch        he had agreed to spend the summer working at the company.</p>
<p>Richard's interest in computing went back to his days at Los Alamos, where        he supervised the "computers," that is, the people who operated the mechanical        calculators. There he was instrumental in setting up some of the first plug-programmable        tabulating machines for physical simulation. His interest in the field was heightened        in the late 1970's when his son, Carl, began studying computers at MIT.</p>
<p>I got to know Richard through his son. I was a graduate student at the MIT        Artificial Intelligence Lab and Carl was one of the undergraduates helping me        with my thesis project. I was trying to design a computer fast enough to solve        common sense reasoning problems. The machine, as we envisioned it, would contain        a million tiny computers, all connected by a communications network. We called        it a "Connection Machine." Richard, always interested in his son's activities,        followed the project closely. He was skeptical about the idea, but whenever        we met at a conference or I visited CalTech, we would stay up until the early        hours of the morning discussing details of the planned machine. The first time        he ever seemed to believe that we were really going to try to build it was the        lunchtime meeting.</p>
<p>Richard arrived in Boston the day after the company was incorporated. We had        been busy raising the money, finding a place to rent, issuing stock, etc. We        set up in an old mansion just outside of the city, and when Richard showed up        we were still recovering from the shock of having the first few million dollars        in the bank. No one had thought about anything technical for several months.        We were arguing about what the name of the company should be when Richard walked        in, saluted, and said, "Richard Feynman reporting for duty. OK, boss, what's        my assignment?" The assembled group of not-quite-graduated MIT students was        astounded.</p>
<p>After a hurried private discussion ("I don't know, you hired him..."), we        informed Richard that his assignment would be to advise on the application of        parallel processing to scientific problems.</p>
<p>"That sounds like a bunch of baloney," he said. "Give me something real to        do."</p>
<p>So we sent him out to buy some office supplies. While he was gone, we decided        that the part of the machine that we were most worried about was the router        that delivered messages from one processor to another. We were not sure that        our design was going to work. When Richard returned from buying pencils, we        gave him the assignment of analyzing the router.</p>
<h2>The Machine</h2>
<p>The router of the Connection Machine was the part of the hardware that allowed        the processors to communicate. It was a complicated device; by comparison, the        processors themselves were simple. Connecting a separate communication wire        between each pair of processors was impractical since a million processors would        require $10^{12]$ wires. Instead, we planned to connect the processors in a        20-dimensional hypercube so that each processor would only need to talk to 20        others directly. Because many processors had to communicate simultaneously,        many messages would contend for the same wires. The router's job was to find        a free path through this 20-dimensional traffic jam or, if it couldn't, to hold        onto the message in a buffer until a path became free. Our question to Richard        Feynman was whether we had allowed enough buffers for the router to operate        efficiently.</p>
<p>During those first few months, Richard began studying the router circuit diagrams        as if they were objects of nature. He was willing to listen to explanations        of how and why things worked, but fundamentally he preferred to figure out everything        himself by simulating the action of each of the circuits with pencil and paper.</p>
<p>In the meantime, the rest of us, happy to have found something to keep Richard        occupied, went about the business of ordering the furniture and computers, hiring        the first engineers, and arranging for the Defense Advanced Research Projects        Agency (DARPA) to pay for the development of the first prototype. Richard did        a remarkable job of focusing on his "assignment," stopping only occasionally        to help wire the computer room, set up the machine shop, shake hands with the        investors, install the telephones, and cheerfully remind us of how crazy we        all were. When we finally picked the name of the company, Thinking Machines        Corporation, Richard was delighted. "That's good. Now I don't have to explain        to people that I work with a bunch of loonies. I can just tell them the name        of the company."</p>
<p>The technical side of the project was definitely stretching our capacities.        We had decided to simplify things by starting with only 64,000 processors, but        even then the amount of work to do was overwhelming. We had to design our own        silicon integrated circuits, with processors and a router. We also had to invent        packaging and cooling mechanisms, write compilers and assemblers, devise ways        of testing processors simultaneously, and so on. Even simple problems like wiring        the boards together took on a whole new meaning when working with tens of thousands        of processors. In retrospect, if we had had any understanding of how complicated        the project was going to be, we never would have started.</p>
<h2>'Get These Guys Organized'</h2>
<p>I had never managed a large group before and I was clearly in over my head.        Richard volunteered to help out. "We've got to get these guys organized," he        told me. "Let me tell you how we did it at Los Alamos."</p>
<p>Every great man that I have known has had a certain time and place in their        life that they use as a reference point; a time when things worked as they were        supposed to and great things were accomplished. For Richard, that time was at        Los Alamos during the Manhattan Project. Whenever things got "cockeyed," Richard        would look back and try to understand how now was different than then. Using        this approach, Richard decided we should pick an expert in each area of importance        in the machine, such as software or packaging or electronics, to become the        "group leader" in this area, analogous to the group leaders at Los Alamos.</p>
<p>Part Two of Feynman's "Let's Get Organized" campaign was that we should begin        a regular seminar series of invited speakers who might have interesting things        to do with our machine. Richard's idea was that we should concentrate on people        with new applications, because they would be less conservative about what kind        of computer they would use. For our first seminar he invited John Hopfield,        a friend of his from CalTech, to give us a talk on his scheme for building neural        networks. In 1983, studying neural networks was about as fashionable as studying        ESP, so some people considered John Hopfield a little bit crazy. Richard was        certain he would fit right in at Thinking Machines Corporation.</p>
<p>What Hopfield had invented was a way of constructing an [associative memory],        a device for remembering patterns. To use an associative memory, one trains        it on a series of patterns, such as pictures of the letters of the alphabet.        Later, when the memory is shown a new pattern it is able to recall a similar        pattern that it has seen in the past. A new picture of the letter "A" will "remind"        the memory of another "A" that it has seen previously. Hopfield had figured        out how such a memory could be built from devices that were similar to biological        neurons.</p>
<p>Not only did Hopfield's method seem to work, but it seemed to work well on        the Connection Machine. Feynman figured out the details of how to use one processor        to simulate each of Hopfield's neurons, with the strength of the connections        represented as numbers in the processors' memory. Because of the parallel nature        of Hopfield's algorithm, all of the processors could be used concurrently with        100\% efficiency, so the Connection Machine would be hundreds of times faster        than any conventional computer.</p>
<h2>An Algorithm For Logarithms</h2>
<p>Feynman worked out the program for computing Hopfield's network on the Connection        Machine in some detail. The part that he was proudest of was the subroutine        for computing logarithms. I mention it here not only because it is a clever        algorithm, but also because it is a specific contribution Richard made to the        mainstream of computer science. He invented it at Los Alamos.</p>
<p>Consider the problem of finding the logarithm of a fractional number between        1.0 and 2.0 (the algorithm can be generalized without too much difficulty).        Feynman observed that any such number can be uniquely represented as a product        of numbers of the form $1 + 2^{-k]$, where $k$ is an integer. Testing each of        these factors in a binary number representation is simply a matter of a shift        and a subtraction. Once the factors are determined, the logarithm can be computed        by adding together the precomputed logarithms of the factors. The algorithm        fit especially well on the Connection Machine, since the small table of the        logarithms of $1 + 2^{-k]$ could be shared by all the processors. The entire        computation took less time than division.</p>
<p>Concentrating on the algorithm for a basic arithmetic operation was typical        of Richard's approach. He loved the details. In studying the router, he paid        attention to the action of each individual gate and in writing a program he        insisted on understanding the implementation of every instruction. He distrusted        abstractions that could not be directly related to the facts. When several years        later I wrote a general interest article on the Connection Machine for [Scientific        American], he was disappointed that it left out too many details. He asked,        "How is anyone supposed to know that this isn't just a bunch of crap?"</p>
<p>Feynman's insistence on looking at the details helped us discover the potential        of the machine for numerical computing and physical simulation. We had convinced        ourselves at the time that the Connection Machine would not be efficient at        "number-crunching," because the first prototype had no special hardware for        vectors or floating point arithmetic. Both of these were "known" to be requirements        for number-crunching. Feynman decided to test this assumption on a problem that        he was familiar with in detail: quantum chromodynamics.</p>
<p>Quantum chromodynamics is a theory of the internal workings of atomic particles        such as protons. Using this theory it is possible, in principle, to compute        the values of measurable physical quantities, such as a proton's mass. In practice,        such a computation requires so much arithmetic that it could keep the fastest        computers in the world busy for years. One way to do this calculation is to        use a discrete four-dimensional lattice to model a section of space-time. Finding        the solution involves adding up the contributions of all of the possible configurations        of certain matrices on the links of the lattice, or at least some large representative        sample. (This is essentially a Feynman path integral.) The thing that makes        this so difficult is that calculating the contribution of even a single configuration        involves multiplying the matrices around every little loop in the lattice, and        the number of loops grows as the fourth power of the lattice size. Since all        of these multiplications can take place concurrently, there is plenty of opportunity        to keep all 64,000 processors busy.</p>
<p>To find out how well this would work in practice, Feynman had to write a computer        program for QCD. Since the only computer language Richard was really familiar        with was Basic, he made up a parallel version of Basic in which he wrote the        program and then simulated it by hand to estimate how fast it would run on the        Connection Machine.</p>
<p>He was excited by the results. "Hey Danny, you're not going to believe this,        but that machine of yours can actually do something [useful]!" According to        Feynman's calculations, the Connection Machine, even without any special hardware        for floating point arithmetic, would outperform a machine that CalTech was building        for doing QCD calculations. From that point on, Richard pushed us more and more        toward looking at numerical applications of the machine.</p>
<p>By the end of that summer of 1983, Richard had completed his analysis of the        behavior of the router, and much to our surprise and amusement, he presented        his answer in the form of a set of partial differential equations. To a physicist        this may seem natural, but to a computer designer, treating a set of boolean        circuits as a continuous, differentiable system is a bit strange. Feynman's        router equations were in terms of variables representing continuous quantities        such as "the average number of 1 bits in a message address." I was much more        accustomed to seeing analysis in terms of inductive proof and case analysis        than taking the derivative of "the number of 1's" with respect to time. Our        discrete analysis said we needed seven buffers per chip; Feynman's equations        suggested that we only needed five. We decided to play it safe and ignore Feynman.</p>
<p>The decision to ignore Feynman's analysis was made in September, but by next        spring we were up against a wall. The chips that we had designed were slightly        too big to manufacture and the only way to solve the problem was to cut the        number of buffers per chip back to five. Since Feynman's equations claimed we        could do this safely, his unconventional methods of analysis started looking        better and better to us. We decided to go ahead and make the chips with the        smaller number of buffers.</p>
<p>Fortunately, he was right. When we put together the chips the machine worked.        The first program run on the machine in April of 1985 was Conway's game of Life.</p>
<h2>Cellular Automata</h2>
<p>The game of Life is an example of a class of computations that interested        Feynman called [cellular automata]. Like many physicists who had spent their        lives going to successively lower and lower levels of atomic detail, Feynman        often wondered what was at the bottom. One possible answer was a cellular automaton.        The notion is that the "continuum" might, at its lowest levels, be discrete        in both space and time, and that the laws of physics might simply be a macro-consequence        of the average behavior of tiny cells. Each cell could be a simple automaton        that obeys a small set of rules and communicates only with its nearest neighbors,        like the lattice calculation for QCD. If the universe in fact worked this way,        then it presumably would have testable consequences, such as an upper limit        on the density of information per cubic meter of space.</p>
<p>The notion of cellular automata goes back to von Neumann and Ulam, whom Feynman        had known at Los Alamos. Richard's recent interest in the subject was motivated        by his friends Ed Fredkin and Stephen Wolfram, both of whom were fascinated        by cellular automata models of physics. Feynman was always quick to point out        to them that he considered their specific models "kooky," but like the Connection        Machine, he considered the subject sufficiently crazy to put some energy into.</p>
<p>There are many potential problems with cellular automata as a model of physical        space and time; for example, finding a set of rules that obeys special relativity.        One of the simplest problems is just making the physics so that things look        the same in every direction. The most obvious pattern of cellular automata,        such as a fixed three-dimensional grid, have preferred directions along the        axes of the grid. Is it possible to implement even Newtonian physics on a fixed        lattice of automata?</p>
<p>Feynman had a proposed solution to the anisotropy problem which he attempted        (without success) to work out in detail. His notion was that the underlying        automata, rather than being connected in a regular lattice like a grid or a        pattern of hexagons, might be randomly connected. Waves propagating through        this medium would, on the average, propagate at the same rate in every direction.</p>
<p>Cellular automata started getting attention at Thinking Machines when Stephen        Wolfram, who was also spending time at the company, suggested that we should        use such automata not as a model of physics, but as a practical method of simulating        physical systems. Specifically, we could use one processor to simulate each        cell and rules that were chosen to model something useful, like fluid dynamics.        For two-dimensional problems there was a neat solution to the anisotropy problem        since [Frisch, Hasslacher, Pomeau] had shown that a hexagonal lattice with a        simple set of rules produced isotropic behavior at the macro scale. Wolfram        used this method on the Connection Machine to produce a beautiful movie of a        turbulent fluid flow in two dimensions. Watching the movie got all of us, especially        Feynman, excited about physical simulation. We all started planning additions        to the hardware, such as support of floating point arithmetic that would make        it possible for us to perform and display a variety of simulations in real time.</p>
<h2>Feynman the Explainer</h2>
<p>In the meantime, we were having a lot of trouble explaining to people what        we were doing with cellular automata. Eyes tended to glaze over when we started        talking about state transition diagrams and finite state machines. Finally Feynman        told us to explain it like this,</p>
<p>"We have noticed in nature that the behavior of a fluid depends very little        on the nature of the individual particles in that fluid. For example, the flow        of sand is very similar to the flow of water or the flow of a pile of ball bearings.        We have therefore taken advantage of this fact to invent a type of imaginary        particle that is especially simple for us to simulate. This particle is a perfect        ball bearing that can move at a single speed in one of six directions. The flow        of these particles on a large enough scale is very similar to the flow of natural        fluids."</p>
<p>This was a typical Richard Feynman explanation. On the one hand, it infuriated        the experts who had worked on the problem because it neglected to even mention        all of the clever problems that they had solved. On the other hand, it delighted        the listeners since they could walk away from it with a real understanding of        the phenomenon and how it was connected to physical reality.</p>
<p>We tried to take advantage of Richard's talent for clarity by getting him        to critique the technical presentations that we made in our product introductions.        Before the commercial announcement of the Connection Machine CM-1 and all of        our future products, Richard would give a sentence-by-sentence critique of the        planned presentation. "Don't say `reflected acoustic wave.' Say [echo]." Or,        "Forget all that `local minima' stuff. Just say there's a bubble caught in the        crystal and you have to shake it out." Nothing made him angrier than making        something simple sound complicated.</p>
<p>Getting Richard to give advice like that was sometimes tricky. He pretended        not to like working on any problem that was outside his claimed area of expertise.        Often, at Thinking Machines when he was asked for advice he would gruffly refuse        with "That's not my department." I could never figure out just what his department        was, but it did not matter anyway, since he spent most of his time working on        those "not-my-department" problems. Sometimes he really would give up, but more        often than not he would come back a few days after his refusal and remark, "I've        been thinking about what you asked the other day and it seems to me..." This        worked best if you were careful not to expect it.</p>
<p>I do not mean to imply that Richard was hesitant to do the "dirty work." In        fact, he was always volunteering for it. Many a visitor at Thinking Machines        was shocked to see that we had a Nobel Laureate soldering circuit boards or        painting walls. But what Richard hated, or at least pretended to hate, was being        asked to give advice. So why were people always asking him for it? Because even        when Richard didn't understand, he always seemed to understand better than the        rest of us. And whatever he understood, he could make others understand as well.        Richard made people feel like a child does, when a grown-up first treats him        as an adult. He was never afraid of telling the truth, and however foolish your        question was, he never made you feel like a fool.</p>
<p>The charming side of Richard helped people forgive him for his uncharming        characteristics. For example, in many ways Richard was a sexist. Whenever it        came time for his daily bowl of soup he would look around for the nearest "girl"        and ask if she would fetch it to him. It did not matter if she was the cook,        an engineer, or the president of the company. I once asked a female engineer        who had just been a victim of this if it bothered her. "Yes, it really annoys        me," she said. "On the other hand, he is the only one who ever explained quantum        mechanics to me as if I could understand it." That was the essence of Richard's        charm.</p>
<h2>A Kind Of Game</h2>
<p>Richard worked at the company on and off for the next five years. Floating        point hardware was eventually added to the machine, and as the machine and its        successors went into commercial production, they were being used more and more        for the kind of numerical simulation problems that Richard had pioneered with        his QCD program. Richard's interest shifted from the construction of the machine        to its applications. As it turned out, building a big computer is a good excuse        to talk to people who are working on some of the most exciting problems in science.        We started working with physicists, astronomers, geologists, biologists, chemists        --- everyone of them trying to solve some problem that it had never been possible        to solve before. Figuring out how to do these calculations on a parallel machine        requires understanding of the details of the application, which was exactly        the kind of thing that Richard loved to do.</p>
<p>For Richard, figuring out these problems was a kind of a game. He always started        by asking very basic questions like, "What is the simplest example?" or "How        can you tell if the answer is right?" He asked questions until he reduced the        problem to some essential puzzle that he thought he would be able to solve.        Then he would set to work, scribbling on a pad of paper and staring at the results.        While he was in the middle of this kind of puzzle solving he was impossible        to interrupt. "Don't bug me. I'm busy," he would say without even looking up.        Eventually he would either decide the problem was too hard (in which case he        lost interest), or he would find a solution (in which case he spent the next        day or two explaining it to anyone who listened). In this way he worked on problems        in database searches, geophysical modeling, protein folding, analyzing images,        and reading insurance forms.</p>
<p>The last project that I worked on with Richard was in simulated evolution.        I had written a program that simulated the evolution of populations of sexually        reproducing creatures over hundreds of thousands of generations. The results        were surprising in that the fitness of the population made progress in sudden        leaps rather than by the expected steady improvement. The fossil record shows        some evidence that real biological evolution might also exhibit such "punctuated        equilibrium," so Richard and I decided to look more closely at why it happened.        He was feeling ill by that time, so I went out and spent the week with him in        Pasadena, and we worked out a model of evolution of finite populations based        on the Fokker Planck equations. When I got back to Boston I went to the library        and discovered a book by Kimura on the subject, and much to my disappointment,        all of our "discoveries" were covered in the first few pages. When I called        back and told Richard what I had found, he was elated. "Hey, we got it right!"        he said. "Not bad for amateurs."</p>
<p>In retrospect I realize that in almost everything that we worked on together,        we were both amateurs. In digital physics, neural networks, even parallel computing,        we never really knew what we were doing. But the things that we studied were        so new that no one else knew exactly what they were doing either. It was amateurs        who made the progress.</p>
<h2>Telling The Good Stuff You Know</h2>
<p>Actually, I doubt that it was "progress" that most interested Richard. He        was always searching for patterns, for connections, for a new way of looking        at something, but I suspect his motivation was not so much to understand the        world as it was to find new ideas to explain. The act of discovery was not complete        for him until he had taught it to someone else.</p>
<p>I remember a conversation we had a year or so before his death, walking in        the hills above Pasadena. We were exploring an unfamiliar trail and Richard,        recovering from a major operation for the cancer, was walking more slowly than        usual. He was telling a long and funny story about how he had been reading up        on his disease and surprising his doctors by predicting their diagnosis and        his chances of survival. I was hearing for the first time how far his cancer        had progressed, so the jokes did not seem so funny. He must have noticed my        mood, because he suddenly stopped the story and asked, "Hey, what's the matter?"</p>
<p>I hesitated. "I'm sad because you're going to die."</p>
<p>"Yeah," he sighed, "that bugs me sometimes too. But not so much as you think."        And after a few more steps, "When you get as old as I am, you start to realize        that you've told most of the good stuff you know to other people anyway."</p>
<p>We walked along in silence for a few minutes. Then we came to a place where        another trail crossed and Richard stopped to look around at the surroundings.        Suddenly a grin lit up his face. "Hey," he said, all trace of sadness forgotten,        "I bet I can show you a better way home."</p>
<p>And so he did.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asynchronous IO: the next billion-dollar mistake? (144 pts)]]></title>
            <link>https://yorickpeterse.com/articles/asynchronous-io-the-next-billion-dollar-mistake/</link>
            <guid>41471707</guid>
            <pubDate>Sat, 07 Sep 2024 05:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yorickpeterse.com/articles/asynchronous-io-the-next-billion-dollar-mistake/">https://yorickpeterse.com/articles/asynchronous-io-the-next-billion-dollar-mistake/</a>, See on <a href="https://news.ycombinator.com/item?id=41471707">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><header><time datetime="2024-09-06T16:00:00Z">September 6, 2024</time></header><p>
Asynchronous IO (also known as "non-blocking IO") is a technique applications
use to allow performing of many IO operations without blocking the calling OS
thread, and without needing to spawn many threads (i.e. one thread per
operation). In the late 1990s/early 2000s, an increasing amount of people using
the internet meant an increasing amount of traffic web services needed to
handle, better known as the <a href="https://en.wikipedia.org/wiki/C10k_problem">C10K
problem</a>.</p><p>Using asynchronous IO to approach this problem appears compelling: it allows you
to handle many connections at once, without needing to increase the number of OS
threads. This is especially compelling if you consider that support for good
multi-threading was still a hit a miss at the time. For example, Linux didn't
have good support for threads until the 2.6 release in December 2003.</p><p>Since then the use of and support for asynchronous IO has grown. Languages such
as Go and Erlang bake support for asynchronous IO directly into the language,
while others such as Rust rely on third-party libraries such as
<a href="https://tokio.rs/">Tokio</a>.</p><p><a href="https://inko-lang.org/">Inko</a>, a language that I'm working on, also includes
built-in support for asynchronous IO. Similar to Go and Erlang, this is hidden
from the user. For example, when reading from a socket there's no need to
explicitly poll or "await" anything, as the language takes care of it for you:</p><div><pre><code><span>import</span> std.net.ip (IpAddress)
<span>import</span> std.net.socket (TcpClient)

<span>class</span> <span>async</span> Main {
  <span>fn</span> <span>async</span> main {
    <span>let</span> client = TcpClient.new(ip: IpAddress.v4(<span>1</span>, <span>1</span>, <span>1</span>, <span>1</span>), port: <span>80</span>).or_panic(
      <span>'failed to connect'</span>,
    )

    client
      .write_string(<span>'GET / HTTP/1.0\r\nHost: one.one.one.one\r\n\r\n'</span>)
      .or_panic(<span>'failed to write the request'</span>)

    ...
  }
}
</code></pre></div><p>If the write would block, Inko's scheduler sets aside the calling process and
reschedules it when the write can be performed without blocking. Other languages
use a different mechanism, such as callbacks or
<a href="https://en.wikipedia.org/wiki/Async/await">async/await</a>. Each approach comes
with its own set of benefits, drawbacks and challenges.</p><p>Not every IO operation can be performed asynchronously though. File IO is
perhaps the best example of this (at least on Linux). To handle such cases,
languages must provide some sort of alternative strategy such as performing the
work in a dedicated pool of OS threads.</p><div><p>Using <a href="https://en.wikipedia.org/wiki/Io_uring">io_uring</a> is another approach,
but it's a recent addition to Linux, specific <em>to</em> Linux (meaning you need a
fallback for other platforms), and <a href="https://www.phoronix.com/news/Google-Restricting-IO_uring">disabled entirely by
some</a>. Either way,
the point still stands: you end up having to handle sockets and files (and
potentially other types of "files") differently.</p></div><p>For example, Inko handles this by the standard library signalling to the
scheduler it's about to perform a potentially blocking operation. The scheduler
periodically checks threads in a "we might be blocking" state. If the thread is
in such a state for too long, it's flagged as "blocking" and a backup thread is
woken up to take over its work. When the blocked thread finishes its work, it
reschedules the process it was running and becomes a backup thread itself. While
this works, it limits the amount of blocking IO operations you can perform
concurrently to the number of backup threads you have. Automatically adding and
removing threads can improve things, but increases the complexity of the system.</p><p>In 2009, <a href="https://en.wikipedia.org/wiki/Tony_Hoare">Tony Hoare</a> stated that his
invention of NULL pointers was something he considers a "billion-dollar mistake"
due to the problems and headaches it brought with it. The more I work on systems
that use asynchronous IO, the more I wonder: is asynchronous IO the next
billion-dollar mistake?</p><p>More specifically, what if instead of spending 20 years developing various
approaches to dealing with asynchronous IO (e.g. async/await), we had instead
spent that time making OS threads more efficient, such that one wouldn't need
asynchronous IO in the first place?</p><p>To illustrate, consider the Linux kernel today: spawning an OS thread takes
somewhere between 10 and 20 microseconds (<a href="https://github.com/inko-lang/inko/issues/690">based on my own
measurements</a>), while a context
switch takes somewhere in the range of <a href="https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/">1-2
microseconds</a>.
This becomes a problem when you want to spawn many threads such that each
blocking operation is performed on its own thread. Not only do you need many OS
threads, but the time to start them can also vary greatly, and the more OS
threads you have the more context switches occur. The end result is that while
you certainly can spawn many OS threads, performance will begin to deteriorate
as the number of threads increases.</p><p>Now imagine a parallel universe where instead of focusing on making asynchronous
IO work, we focused on improving the performance of OS threads such that one can
easily use hundreds of thousands of OS threads without negatively impacting
performance (= the cost to start threads is lower, context switches are cheaper,
etc). In this universe, asynchronous IO and async/await wouldn't need to exist
(or at least wouldn't be as widely used). You need to handle 100 000 requests
that perform a mixture of IO and CPU bound work? Just use 100 000 threads and
let the OS handle it.</p><p>Not only would this offer an easier mental model for developers, it also leads
to a simpler stack. Libraries such as epoll and kqueue wouldn't need to exist,
as one would just start a new OS thread for their blocking/polling needs.
Need to call a C function that may block the calling thread? Just run it on a
separate thread, instead of having to rely on some sort of mechanism provided by
the IO runtime/language to deal with blocking C function calls.</p><p>Unfortunately, we do not live in such a universe. Instead in our universe the
cost of OS threads is quite high, and inconsistent across platforms. Which
brings me back to Tony Hoare: over the decades, we invested a massive amount of
resources in dealing with asynchronous IO, perhaps billions of dollars worth of
resources. Was that a mistake and should we have instead invested that into
improving the performance of OS threads? I think so, but until an operating
system comes along that dramatically improves the performance of threads ,
becomes as popular as Linux, <em>and</em> is capable of running everything you can run
on Linux or provide better alternatives (such that people will actually want to
switch), we're stuck with asynchronous IO.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malaysia started mandating ISPs to redirect DNS queries to local servers (271 pts)]]></title>
            <link>https://thesun.my/local-news/mcmc-addresses-misinformation-on-dns-redirection-internet-access-restrictions-BN12972452</link>
            <guid>41471510</guid>
            <pubDate>Sat, 07 Sep 2024 04:50:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thesun.my/local-news/mcmc-addresses-misinformation-on-dns-redirection-internet-access-restrictions-BN12972452">https://thesun.my/local-news/mcmc-addresses-misinformation-on-dns-redirection-internet-access-restrictions-BN12972452</a>, See on <a href="https://news.ycombinator.com/item?id=41471510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p><b>KUALA LUMPUR: </b>The Malaysian Communications and Multimedia Commission (MCMC) has instructed Internet Service Providers (ISPs) to redirect Domain Name Service (DNS) traffic that uses third-party DNS servers back to their own DNS servers.</p> <p>MCMC in a statement tonight said this is to ensure that users continue to benefit from the protection provided by the local ISP’s DNS servers and that malicious sites are inaccessible to Malaysians.</p> <p>As a commitment to protecting the safety of Internet users, MCMC has blocked a total of 24,277 websites between between 2018 to Aug 1, classified into various categories, which are online gambling (39 per cent), pornography/obscene content (31 per cent), copyright infringement (14 per cent), other harmful sites (12 per cent), prostitution (two per cent) and unlawful investments/scams (two per cent).</p> <p>“It has been falsely claimed that the measure undertaken by MCMC is a draconian measure. We reiterate that Malaysia’s implementation is for the protection of vulnerable groups from harmful online content.</p> <p>“The DNS system can protect users by blocking access to websites known for distributing malware, phishing, and other malicious activities, as well as filter inappropriate content such as adult material and violent websites,” the statement said.</p> <p>MCMC said some users choose to use third-party DNS servers like Google DNS or Cloudflare, which are said to offer various benefits, such as faster speeds and increased privacy, but they might not have the same level of protection for harmful content particularly in the local context, compared to local ISP’s DNS servers.</p> <p>DNS is a system designed to turn website addresses into numeric IP addresses to locate websites on the Internet, while ISPs typically operate their own DNS servers, which can be configured to block access to certain websites or domains based on their content, a common method used to protect users from harmful content.</p> <p>The statement said an inaccurate claim also suggested that a so-called blanket ban, with some suggesting that legitimate websites have been made inaccessible because of DNS redirection.</p> <p>“Websites are only blocked when they are found to host malicious content, such as copyright infringements, online gambling, or pornography. Legitimate websites remain accessible as usual, and DNS redirection ensures that harmful content is filtered out while safe sites remain reachable without noticeable disruption,” the statement said.</p> <p>Thus, MCMC encouraged users to report any difficulties in accessing legitimate websites directly to their respective ISPs so the issue could be addressed promptly, as they have yet to receive any such complaint.</p> <p>Moreover, any websites that believe they have been unfairly targeted or affected may file an appeal through the established channels, it said.</p> <p>The Appeals Tribunal, established by MCMC and chaired by a High Court judge, operates independently to ensure a fair and impartial review of each case.</p> <p>“ MCMC remains committed to maintaining a safe and secure online environment in Malaysia, balancing the protection of internet users with the need for seamless access to legitimate online content,” it said</p> <p>Previously, it has been announced that MCMC will introduce a new regulatory framework for safe Internet use by children and families on Aug 1, with implementation set for Jan 1, 2025.</p> <p>Under the new framework, social media and Internet messaging services with at least eight million registered users in Malaysia must apply for an Application Service Provider Class Licence under the Communications and Multimedia Act 1998 (Act 588).</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's new in C++26 (part 1) (125 pts)]]></title>
            <link>https://mariusbancila.ro/blog/2024/09/06/whats-new-in-c26-part-1/</link>
            <guid>41471488</guid>
            <pubDate>Sat, 07 Sep 2024 04:44:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mariusbancila.ro/blog/2024/09/06/whats-new-in-c26-part-1/">https://mariusbancila.ro/blog/2024/09/06/whats-new-in-c26-part-1/</a>, See on <a href="https://news.ycombinator.com/item?id=41471488">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-4733">

	

	
	<div>
		
<p>The C++26 version of the C++ standard is a work in progress, but a series of language and library features have been already added. Furthermore, some of them are already supported by Clang and GCC. One of these new changes was discussed in my previous article, <a href="https://mariusbancila.ro/blog/2024/04/21/erroneous-behaviour-has-entered-the-chat/" target="_blank" rel="noopener" title="">Erroneous behaviour has entered the chat</a>. In this post, we will look at several language features added in C++26.</p>



<h2>Specifying a reason for deleting a function</h2>



<p>Since C++11, we can declare a function as deleted, so that the compiler will prevent its use. This can be used to prevent the use of class special member functions, but also to delete any other function. A function can be deleted as follows (example from the proposal paper):</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">class NonCopyable
{
public:
    // ...
    NonCopyable() = default;

    // copy members
    NonCopyable(const NonCopyable&amp;) = delete;
    NonCopyable&amp; operator=(const NonCopyable&amp;) = delete;
    // maybe provide move members instead
};</pre>



<p>In C++26, you can specify a reason why this function is deleted:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">class NonCopyable
{
public:
    // ...
    NonCopyable() = default;

    // copy members
    NonCopyable(const NonCopyable&amp;)
        = delete("Since this class manages unique resources, copy is not supported; use move instead.");
    NonCopyable&amp; operator=(const NonCopyable&amp;)
        = delete("Since this class manages unique resources, copy is not supported; use move instead.");
    // provide move members instead
};</pre>



<p>The reason for having this feature is to help API authors to provide tailored messages for the removal of a function, instead of just relying on the generic compiler error for using a deleted function.</p>



<p>For more info see: <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p2573r2.html" target="_blank" rel="noopener" title="">P2573R2: = delete(“should have a reason”);</a></p>



<h2>Placeholder variables with no name</h2>



<p>There are cases when a variable has to be declared but its name is never used. An example is structure bindings. Another is locks (like <code>lock_guard</code>), that are only used for their side-effects. In the future, another example could be pattern matching (for which several proposals exist).</p>



<p>In C++26, we can use a single underscore (<code>_</code>) to define an unnamed variable.</p>



<p>For instance, in the following example, <code>unused</code> is a variable that is not used:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">[[maybe_unused]] auto [data, unused] = get_data();</pre>



<p>In C++26, the <code>unused</code> variable can be named <code>_</code> (single underscore):</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">auto [data, _] = get_data();</pre>



<p>When the single underscore identifier is used for the declaration of a variable, non-static class member variable, lambda capture, or structure binding, the <code>[[maybe_unused]]</code> attribute is implicitly added, therefore, there is no need to explicitly use it.</p>



<p>A declaration with the name <code>_</code> is said to be <em>name-independent</em> if it declares:</p>



<ul><li>a variable with automatic storage duration</li><li>a structure binding, but not in a namespace scope</li><li>a variable introduced by an init capture</li><li>a non-static data member</li></ul>



<p>The compiler will not emit warnings that a name-independent declaration is used or not. Moreover, multiple name-independent declarations can be used in the same scope (that is not a namespace scope):</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">int main()
{
  int _;
  _ = 0;         // OK

  std::string _; // OK, because _ is a name-independent declaration
  _ = "0";       // Error: ambiguous reference to placeholder '_', which is defined multiple times
}</pre>



<p>On the other hand, the following is not possible:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">int main()
{
  int _;
  _ = 0;                // OK

  static std::string _; // Error: static variables are not name-independent
}</pre>



<p>The following is also not possible, because the declarations are in a namespace scope:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">namespace n
{
  int f() {return 42;}

  auto _ = f(); // OK
  auto _ = f(); // Error: redefinition of _
}</pre>



<p>To learn more about this feature see: <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2169r4.pdf" target="_blank" rel="noopener" title="">P2169: A nice placeholder with no name</a>.</p>



<h2>Structured binding declaration as a&nbsp;condition</h2>



<p>A structure binding defines a set of variables that are bound to sub-objects or elements of their initializer.</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">auto [position, length] = get_next_token(text, offset);</pre>



<p>A structure binding can appear in a for-range declaration, such as in the following example:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">for (auto [position, length] : tokenize(text, offset))
{
  std::println("pos {}, len {}", position, length);
}</pre>



<p>On the other hand, variables can appear in the condition of an <code>if</code>, <code>while</code>, or <code>for</code> statement:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">if (auto it = std::find_if(begin(arr), end(arr), is_even); it != std::end(arr))
{
  std::println("{} is the 1st even number", *it);
}</pre>



<p>However, structure bindings cannot be declared in the condition of an <code>if</code>, <code>while</code>, or <code>for</code> statement. That changes in C++26, which makes it possible:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">if(auto [position, length] = get_next_token(text, offset); position &gt;= 0)
{
  std::println("pos {}, len {}", position, length);
}</pre>



<p>An interesting and very useful case is presented in the proposal paper (P0963). Consider the following C++26 example for using <code>std::to_chars</code>:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">if (auto result = std::to_chars(p, last, 42))
{
​​​​    auto [ptr, _] = result;
​​​​    // okay to proceed
​​​​} 
else 
{
​​​​    auto [ptr, ec] = result;
​​​​    // handle errors
​​​​}</pre>



<p>When the function succeeds, we are only interested in the <code>ptr</code> member of <code>std::to_chars_result</code>, which contains a pointer to the one-past-the-end pointer of the characters written. If the function fails, then we also need to look at the <code>ec</code> member (of the <code>std::errc</code> type) representing an error code.</p>



<p>This code can be simplified with structure bindings, in C++26, as follows:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">​​​​if (auto [ptr, ec] = std::to_chars(p, last, 42))
{
​​​​    // okay to proceed
​​​​} 
else 
{
​​​​    // handle errors
​​​​}</pre>



<p>To learn more about this feature see: <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p0963r3.html" target="_blank" rel="noopener" title="">P0963: Structured binding declaration as a condition</a>.</p>



<h2>user-generated static_assert messages</h2>



<p>The <code>static_assert</code>‘s second parameter, which is a string representing the error message, can now be a compile-time user-generated string-like object. The following example uses a hypothetical constexpr <code>std::format</code>, although this may also later appear in C++26:</p>



<pre data-enlighter-language="cpp" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">static_assert(sizeof(int) == 4, std::format("Expected 4, actual {}", sizeof(int)));</pre>



<p>To learn more about this feature see: <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2741r3.pdf" target="_blank" rel="noopener" title="P2471R3: user-generated static_assert messages">P2471R3: user-generated static_assert messages</a>.</p>





	</div><!-- .entry-content -->

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ford Patents In-Car System That Eavesdrops So It Can Play You Ads (147 pts)]]></title>
            <link>https://www.motortrend.com/news/ford-in-vehicle-advertising-patent/</link>
            <guid>41471417</guid>
            <pubDate>Sat, 07 Sep 2024 04:21:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.motortrend.com/news/ford-in-vehicle-advertising-patent/">https://www.motortrend.com/news/ford-in-vehicle-advertising-patent/</a>, See on <a href="https://news.ycombinator.com/item?id=41471417">Hacker News</a></p>
Couldn't get https://www.motortrend.com/news/ford-in-vehicle-advertising-patent/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Browsing Hacker News in the Terminal (103 pts)]]></title>
            <link>https://hnterm.ggerganov.com/</link>
            <guid>41471157</guid>
            <pubDate>Sat, 07 Sep 2024 02:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hnterm.ggerganov.com/">https://hnterm.ggerganov.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41471157">Hacker News</a></p>
<div id="readability-page-1" class="page">

        

        <p><span>
                |
                Build time: <span>Sun Mar 13 17:35:13 2022</span> |
                Commit hash: <a href="https://github.com/ggerganov/hnterm/commit/563e8787">563e8787</a> |
                Commit subject: <span>emscripten : fix performance issues on some browsers</span> |
            </span>
        </p>
        <p>
            <a href="https://github.com/ggerganov/hnterm"><span>View on GitHub </span>
                
            </a>
        </p>

        
        
    

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[QtCS2024: Compile once, Run everywhere (148 pts)]]></title>
            <link>https://wiki.qt.io/QtCS2024_Compile_once._Run_everywhere</link>
            <guid>41470571</guid>
            <pubDate>Fri, 06 Sep 2024 23:55:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.qt.io/QtCS2024_Compile_once._Run_everywhere">https://wiki.qt.io/QtCS2024_Compile_once._Run_everywhere</a>, See on <a href="https://news.ycombinator.com/item?id=41470571">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><h2><span id="Session_Summary">Session Summary</span></h2>
<p>Compiling and deploying of C++ applications on Windows, Linux, macOS for x86_64 and arm64 can be challenging. By using <i><a rel="nofollow" href="https://justine.lol/cosmopolitan/">Cosmopolitan Libc</a></i> we could have an alternative.
</p><p>Slides at <a rel="nofollow" href="https://git.qt.io/cradam/presentations/-/raw/main/2024.09.06/QtCS2024-CompileOnce-RunEverywhere.pdf?ref_type=heads">2024.09.06/QtCS2024-CompileOnce-RunEverywhere.pdf · GitLab</a>
</p>
<h2><span id="Session_Owners">Session Owners</span></h2>
<p>Cristian Adam
</p>
<h2><span id="Notes_.28AI_transcribed.29"></span><span id="Notes_(AI_transcribed)">Notes (AI transcribed)</span></h2>
<p>Cristian Adam, a member of the Qt Creator team, presented a talk on "Compile Once, Run Everywhere" using Cosmopolitan libc for C++ applications. 
</p><p>Key points include:
</p>
<ul><li>Qt Creator is currently compiled for multiple platforms (X64 and ARM64 for MacOS, separate packages for Linux, Windows ARM64 in progress) using the Qt installer framework.</li>
<li>Cosmopolitan libc is a C runtime that detects the host machine at runtime and provides the right system calls, enabling "compile once, run everywhere" for C++ applications.</li>
<li>Cosmopolitan applications are compiled twice (X64 and ARM64) and packaged as a batch script plus payload, similar to Linux run installers.</li>
<li>Mozilla's llamafile is an example of a Cosmopolitan application that runs locally after downloading and adding execute permissions.</li>
<li>Adam successfully built and ran CMake, Qt Base, and Qt GUI with VNC QPA using Cosmopolitan libc on MacOS and Linux, but encountered issues on Windows due to Cosmopolitan's Libc's POSIX implementation.</li>
<li>Challenges include integrating with native platforms, launching applications, and supporting WebSockets for Qt QPA VNC platform.</li>
<li>Adam demonstrated Qt Creator running in Cosmopolitan, with menus working but window borders missing.</li>
<li>The size of the Cosmopolitan Qt Creator binary is around 230 megabytes, and there were no noteworthy performance differences compared to the native version.</li>
<li>Adam plans to continue working on Cosmopolitan support for Qt Creator and encourages others to contribute and report issues.</li></ul>
<!-- 
NewPP limit report
Cached time: 20240906120349
Cache expiry: 86400
Reduced expiry: false
Complications: []
CPU time usage: 0.002 seconds
Real time usage: 0.003 seconds
Preprocessor visited node count: 8/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/100
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->

<!-- Saved in parser cache with key heroku_app_db:pcache:idhash:10820-0!canonical and timestamp 20240906120352 and revision id 42513.
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hardware Acceleration of LLMs: A comprehensive survey and comparison (243 pts)]]></title>
            <link>https://arxiv.org/abs/2409.03384</link>
            <guid>41470074</guid>
            <pubDate>Fri, 06 Sep 2024 22:09:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2409.03384">https://arxiv.org/abs/2409.03384</a>, See on <a href="https://news.ycombinator.com/item?id=41470074">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2409.03384">View PDF</a>
    <a href="https://arxiv.org/html/2409.03384v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. In this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language Models using hardware accelerators.
<br>The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each framework. The main challenge in comparison is that every proposed scheme is implemented on a different process technology making hard a fair comparison. The main contribution of this paper is that we extrapolate the results of the performance and the energy efficiency on the same technology to make a fair comparison; one theoretical and one more practical. We implement part of the LLMs on several FPGA chips to extrapolate the results to the same process technology and then we make a fair comparison of the performance.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Christoforos Kachris [<a href="https://arxiv.org/show-email/95ee12c7/2409.03384">view email</a>]      <br>    <strong>[v1]</strong>
        Thu, 5 Sep 2024 09:43:25 UTC (1,209 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>