<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 18 Dec 2023 15:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Figma and Adobe are abandoning our proposed merger (563 pts)]]></title>
            <link>https://www.figma.com/blog/figma-adobe-abandon-proposed-merger/</link>
            <guid>38681861</guid>
            <pubDate>Mon, 18 Dec 2023 13:04:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figma.com/blog/figma-adobe-abandon-proposed-merger/">https://www.figma.com/blog/figma-adobe-abandon-proposed-merger/</a>, See on <a href="https://news.ycombinator.com/item?id=38681861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1" lang="en"><div><p><time datetime="December 18, 2023">December 18, 2023</time></p><div><p><img src="data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAUABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAYDBQj/xAAnEAABBAEDBAAHAAAAAAAAAAABAAIDBAUGERIHITFRExQyM0FSYf/EABcBAAMBAAAAAAAAAAAAAAAAAAABAwT/xAAaEQEBAQEAAwAAAAAAAAAAAAABAAIRAxIx/9oADAMBAAIRAxEAPwChyfVai+nEzCRuluWjwg5jZvL2VPQ6+1TpbKuGrImWsc8/dhH0k+NipmaizAw46W1WjElV3I8ndiP57VNrjK15tHyyfAYYjx35bDz69qL0bV4s51hX6VHX6vadmiD5HSsJ/Bai8/8AzOL2bxrOI2/ZE+NH2LHVmRt3sxYZame9kTixjSewAXKfPNJGI5JZHMHhpcSAiKpKwPY9kREov//Z" alt="" data-lqip="true"><img data-loading="true" loading="eager" alt="" src="https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?w=400&amp;h=400&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 200w, https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 300w, https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 400w"></p><p>Dylan Field<span>Co-founder &amp; Chief Executive Officer, Figma</span></p></div></div><section><div id="progress-indicator-target"><p>Fifteen months into the regulatory review process, Figma and Adobe no longer see a path toward regulatory approval of our proposed acquisition.</p><div colorscheme="[object Object]"><p>Figma and Adobe have reached a joint decision to end our pending acquisition. It’s not the outcome we had hoped for, but despite thousands of hours spent with regulators around the world detailing differences between our businesses, our products, and the markets we serve, we no longer see a path toward regulatory approval of&nbsp;the&nbsp;deal.</p><p>We entered into this agreement 15 months ago with the goal of accelerating what both Adobe and Figma could do for our respective communities. While we leave that future behind and continue on as an independent company, we are excited to find ways to partner for&nbsp;our&nbsp;users.</p><p>Amid the uncertainty of a pending acquisition, I am deeply proud of how the Figma team delivered for our community and feel we have only continued to accelerate our pace over the past 15 months. Our team built and shipped new products to make it easier to ideate, design and build software, including our first <a href="https://www.figma.com/blog/introducing-ai-to-figjam/">native AI features</a>, <a href="https://www.figma.com/blog/introducing-dev-mode/">Dev Mode</a>, Variables, and Advanced Prototyping. We also opened new hubs in the UK and Asia, hosted an epic <a href="https://www.figma.com/blog/config-2023-recap/">Config IRL in San Francisco</a>, <a href="https://www.figma.com/blog/ai-the-next-chapter-in-design/">acquired AI startup Diagram</a>, and added more than 500 new&nbsp;Figmates.</p><p>Figma’s founding vision was to “eliminate the gap between imagination and reality.” The shift from a physical economy to a digital economy and huge advances in AI have combined to make this aspiration feel even more urgent and within reach today than it did 11&nbsp;years&nbsp;ago.</p><p>This will be our focus moving forward. We want to make it easy for anyone to design and build digital products on a single multiplayer canvas—from start to finish, idea to production. I’m so excited for what the future holds and beyond grateful to our community for supporting us. Figma’s best, most innovative days are still ahead. See you all&nbsp;in&nbsp;2024!</p></div><div><p><img src="data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAUABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAYDBQj/xAAnEAABBAEDBAAHAAAAAAAAAAABAAIDBAUGERIHITFRExQyM0FSYf/EABcBAAMBAAAAAAAAAAAAAAAAAAABAwT/xAAaEQEBAQEAAwAAAAAAAAAAAAABAAIRAxIx/9oADAMBAAIRAxEAPwChyfVai+nEzCRuluWjwg5jZvL2VPQ6+1TpbKuGrImWsc8/dhH0k+NipmaizAw46W1WjElV3I8ndiP57VNrjK15tHyyfAYYjx35bDz69qL0bV4s51hX6VHX6vadmiD5HSsJ/Bai8/8AzOL2bxrOI2/ZE+NH2LHVmRt3sxYZame9kTixjSewAXKfPNJGI5JZHMHhpcSAiKpKwPY9kREov//Z" alt="" data-lqip="true"><img data-loading="true" loading="eager" alt="" src="https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?w=400&amp;h=400&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 200w, https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 300w, https://cdn.sanity.io/images/599r6htc/localized/672d9533efa16607fd9f84281a4d158c6a9fd048-400x400.jpg?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 400w"></p><div><p>Dylan Field is the co-founder and CEO of Figma. Dylan studied computer science and mathematics at Brown University where he and his co-founder, Evan Wallace, first started experimenting with design tools built on (and for) the web. With funding from a Thiel fellowship, they began Figma. Prior to Figma, Dylan interned at O'Reilly Media, LinkedIn, and&nbsp;Flipboard.</p></div></div></div><div><form novalidate=""><div><h2>Subscribe for a Shortcut to fresh news and fun surprises</h2><div><p><label><p>I agree to opt-in to Figma's mailing list.</p></label></p></div></div></form></div><section><header><h2>Related articles</h2></header><div><a aria-label="Open blog post" tabindex="-1" href="https://www.figma.com/blog/introducing-dev-mode/"><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAADPElEQVQokS2QW0yTZwBAi4DcWhSF2OEuMtAxB5OYgZglwGAIlmrLAFtsuZQiaLVQ5FK5TOpEqHIrTRilRWXKTEw0bstiYnxasmUvS5bsxUTHls1tOmEItCAy/u8s23w6bycnR2ao+1uqPbUsLI4Axx1+YelepP70HDX2JxjqZzCcmOXIucfYO39jqOoRY7o/Gdf/xUT5Epf0z5nQrTBwLIDtTICq9lVkpuZnUrdrUYxf9uO9vCD6vNMcHbjDAcso2QedZBVfIM88gLrcib7QSUXRELWacewltxnSPeBj4yLnTvo50ftCaLYHpEH3grh10c9V30/C6v6EPfZa4vPSiXktgZikJDanpxD37i6Uqky2VuWzraaI3ToD5YeG6TD/QFvXPHV9y1S2riKraQlIPV3zwtc1g91xW+xurESuTSXkbSVBcZEEbQpnXWIM69U7iOjMQu5WIR9REd2Zyw7jB2gMk5ganlDZtMbh2jVk1c1LUkf7U3G29T5ay3kRW5xBaEESYepkQrMTCN6+meA344gwv4PCVUS0V0u0R0tUfwGbjuSQru6kpHQanR7KSiVkptYlyTH4VPQNfI/qcINQpCUQmp9IpHUvEda9hOYkEJL2EhH1Gcj7C5EPq1CMqIk6v4+N9VnsKmhCo75HWTGUasW/D5ckl29RfDp1j7q2dqHMTiY4PZ716mTCNDv/kwW/EUu4MQ35wH4UXi0KXzFRznxiK3LILPiIUs0v6EoEh4rXkNXal6TRiYC48/nPjF4ZE3nHVWxIeZlgpYJ1SjlBG8MIio0kJPMVwo/tIao3H0XvPuIa3yPDYMRU+QWNxllsxt+xVt//v3DI6xdffjbDzZvfii6ni9wDel5/K4MtCanEbU1hy7adxKem8GpuBom690mt1KCqs/Ch4xpe50M8tseMN9zFbZ9CVmVbkU5d8ItBzyLDYzOiZ+gB1o6vKKu7TmHJFAX7JzGUd9Ni09PSYuToyR4s7ddw9H+NZ/JXJr0LXD37kCvddxk+fR1ZefWaVGV9LszNK5ibV17wGSbbMoaaVUwVc7gdk3x3I49vbh3EdekGbeOPaPPN0+ad44xnFp/nD0ZHpmlq/5F/AJ6E/jM6ogRrAAAAAElFTkSuQmCC" alt="An abstracted Figma file with a toggle for developers to turn on Dev Mode" width="528" height="297" data-lqip="true"><img data-loading="true" width="528" height="297" loading="lazy" alt="An abstracted Figma file with a toggle for developers to turn on Dev Mode" src="https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?rect=1,0,5568,3132&amp;w=528&amp;h=297&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 264w, https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 396w, https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?w=528&amp;q=75&amp;fit=max&amp;auto=format 528w, https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 792w, https://cdn.sanity.io/images/599r6htc/localized/a313056d03f6e2734d587ee87c5a38c23936f8e5-5569x3132.png?w=528&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div></a></div></section><p><section><h2>Create and collaborate with Figma</h2><a href="https://www.figma.com/signup?locale=en" target="_blank" rel="noreferrer">Get started for free</a></section></p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wasm3 entering a minimal maintenance phase (191 pts)]]></title>
            <link>https://github.com/wasm3/wasm3</link>
            <guid>38681672</guid>
            <pubDate>Mon, 18 Dec 2023 12:43:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wasm3/wasm3">https://github.com/wasm3/wasm3</a>, See on <a href="https://news.ycombinator.com/item?id=38681672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto"><p dir="auto">Note</p><p dir="auto">I regret to inform the community that since <a href="https://twitter.com/vshymanskyy/status/1568657607229075456" rel="nofollow">my house was destroyed by russians who invaded my country</a>, <strong>Wasm3 will enter a minimal maintenance phase</strong>. At this time, I am unable to continue the development of new features. However, I am committed to keeping the project alive and will actively review and merge incoming Pull Requests. I deeply appreciate your understanding and support during this difficult period. <strong>Your contributions to Wasm3 are now more valuable than ever.</strong></p>
</div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wasm3/wasm3/blob/main/extra/screenshot-ios.png"><img width="30%" src="https://github.com/wasm3/wasm3/raw/main/extra/screenshot-ios.png"></a></p>
<h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wasm3/wasm3/blob/main/extra/wasm-symbol.svg"><img src="https://github.com/wasm3/wasm3/raw/main/extra/wasm-symbol.svg" width="32" height="32"></a> Wasm3</h2>
<p dir="auto"><a href="https://github.com/vshymanskyy/StandWithUkraine/blob/main/docs/README.md"><img src="https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/badges/StandWithUkraine.svg" alt="StandWithUkraine"></a>
<a href="https://wapm.io/package/vshymanskyy/wasm3" rel="nofollow"><img src="https://camo.githubusercontent.com/3a7c4be43f68428ee86bfe17bc92bb5b9463f49462c8660677e96d07ad51bb8c/68747470733a2f2f7761706d2e696f2f7061636b6167652f767368796d616e736b79792f7761736d332f62616467652e737667" alt="WAPM" data-canonical-src="https://wapm.io/package/vshymanskyy/wasm3/badge.svg"></a>
<a href="https://github.com/wasm3/wasm3/issues"><img src="https://camo.githubusercontent.com/963bc146066cc6b11570ff4a4ceadb78f378cd5893b1f3cdb5cd88dc7cb1c36e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f7761736d332f7761736d333f7374796c653d666c61742d737175617265266c6162656c3d69737375657326636f6c6f723d73756363657373" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues-raw/wasm3/wasm3?style=flat-square&amp;label=issues&amp;color=success"></a>
<a href="https://github.com/wasm3/wasm3/actions"><img src="https://camo.githubusercontent.com/14a33815462d651385c78c22cd290e3ec9bff2ae0f93e4d45968a1d67f29197b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f7761736d332f7761736d332f74657374732e796d6c3f6272616e63683d6d61696e267374796c653d666c61742d737175617265266c6f676f3d676974687562266c6162656c3d7465737473" alt="Tests status" data-canonical-src="https://img.shields.io/github/actions/workflow/status/wasm3/wasm3/tests.yml?branch=main&amp;style=flat-square&amp;logo=github&amp;label=tests"></a>
<a href="https://bugs.chromium.org/p/oss-fuzz/issues/list?can=1&amp;q=proj:wasm3" rel="nofollow"><img src="https://camo.githubusercontent.com/576ecd33f690472e3308bd0ad399e8322d29113b87099883ca024a33d7eef745/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f73732d2d66757a7a2d66757a7a696e672d737563636573733f7374796c653d666c61742d737175617265" alt="Fuzzing Status" data-canonical-src="https://img.shields.io/badge/oss--fuzz-fuzzing-success?style=flat-square"></a>
<a href="https://github.com/wasm3/wasm3"><img src="https://camo.githubusercontent.com/fa6e10811485d7022ae8c55770e22511f740aad92b141370db14c56e9fc44545/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f7374796c653d666c61742d737175617265" alt="GitHub license" data-canonical-src="https://img.shields.io/badge/license-MIT-blue?style=flat-square"></a></p>
<p dir="auto">A fast WebAssembly interpreter and the most universal WASM runtime.<br>
<sub>Based on <a href="https://github.com/wasm3/wasm3/blob/main/docs/Performance.md"><strong>CoreMark 1.0</strong></a> and <a href="https://00f.net/2021/02/22/webassembly-runtimes-benchmarks" rel="nofollow"><strong>independent</strong></a> benchmarks. Your mileage may vary.</sub></p>
<p dir="auto"><a href="https://twitter.com/wasm3_engine" rel="nofollow"><img src="https://camo.githubusercontent.com/d47f801d4b10a9f29b07fde7d9580090463016618334f33cf783b1db9d1a9dd6/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7761736d335f656e67696e653f7374796c653d666c61742d73717561726526636f6c6f723d316461316632266c6162656c3d74776974746572266c6f676f3d74776974746572" alt="Twitter" data-canonical-src="https://img.shields.io/twitter/follow/wasm3_engine?style=flat-square&amp;color=1da1f2&amp;label=twitter&amp;logo=twitter"></a>
<a href="https://discord.gg/qmZjgnd" rel="nofollow"><img src="https://camo.githubusercontent.com/c5026ad1cb55fd3dffcf4a9feb18d11d6bfb306c47640680e2de710535fb0ad3/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3637313431353634353037333730323932353f7374796c653d666c61742d737175617265266c6f676f3d646973636f726426636f6c6f723d373238396461266c6162656c3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/671415645073702925?style=flat-square&amp;logo=discord&amp;color=7289da&amp;label=discord"></a></p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<p dir="auto">Here's a small <a href="https://wapm.io/package/vshymanskyy/wasm3" rel="nofollow">getting started guide</a>. Click here to start:</p>
<p dir="auto"><a href="https://webassembly.sh/?run-command=wasm3" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/button.png" alt="LIVE DEMO"></a></p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto"><strong>Please follow the <a href="https://github.com/wasm3/wasm3/blob/main/docs/Installation.md">installation instructions</a>.</strong></p>
<p dir="auto">Wasm3 can also be used as a library for:</p>
<p dir="auto"><a href="https://github.com/wasm3/pywasm3"><img src="https://camo.githubusercontent.com/09b05de6e527ec5052ee033aaa57ad4a94a637cfcc8900276b3c9087ed62de1c/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f707974686f6e2e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/python.svg"> Python3</a> │
<a href="https://github.com/Veykril/wasm3-rs"><img src="https://camo.githubusercontent.com/961ceedfe24996e8f1c466bb8f17effba0abbc45075545d9b62bafeb0627a66b/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f727573742e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/rust.svg"> Rust</a> │
<a href="https://github.com/wasm3/wasm3"><img src="https://camo.githubusercontent.com/6ae1442dc1a0d3dceb928ae37b41849e142c203a2bffab6cda47998d5c9dfa15/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f63706c7573706c75732e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/cplusplus.svg"> C/C++</a> │
<a href="https://github.com/matiasinsaurralde/go-wasm3"><img src="https://camo.githubusercontent.com/a22c7376805e4aac976c90fe844e758f6fe8576a6fa617e9aea77dc3bd666654/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f676f2e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/go.svg"> GoLang</a> │
<a href="https://github.com/alichay/zig-wasm3"><img src="https://camo.githubusercontent.com/0d4113b3240164bc349c23a7f542e795798bd784725f2b838c9478251bb0e735/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f7a69672e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/zig.svg"> Zig</a> │
<a href="https://metacpan.org/pod/Wasm::Wasm3" rel="nofollow"><img src="https://camo.githubusercontent.com/c5c861c7858d76c9163f31e8b7f55a5fe4e1471d5eb0c3f6714d12e224f8c736/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f7065726c2e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/perl.svg"> Perl</a><br>
<a href="https://github.com/shareup/wasm-interpreter-apple"><img src="https://camo.githubusercontent.com/850f38a806ed27158a6a5f718e0c7326e2ac1791447d035d4e4f091bedc7c3a3/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f73776966742e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/swift.svg"> Swift</a> │
<a href="https://github.com/tana/Wasm3DotNet"><img src="https://camo.githubusercontent.com/a7a6a508090beb6779b9135e922f438be6f19c8281da60834f5149e0d52f72d1/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f646f746e65742e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/dotnet.svg"> .Net</a> │
<a href="https://github.com/beef331/wasm3"><img src="https://camo.githubusercontent.com/3759d517a62153b4973c47d452ae762c0480397e8bf2eb802081d9f3848a1ecc/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6e696d2e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/nim.svg"> Nim</a> │
<a href="https://github.com/wasm3/wasm3-arduino"><img src="https://camo.githubusercontent.com/60f3f4a2171971846073ed19a21a36c01069f21e883d938cfe87598f209b7669/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f61726475696e6f2e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/arduino.svg"> Arduino, PlatformIO, Particle</a> │ <a href="https://github.com/saghul/txiki.js">QuickJS</a></p>
<h2 tabindex="-1" dir="auto">Status</h2>
<p dir="auto"><code>wasm3</code> passes the <a href="https://github.com/WebAssembly/spec/tree/master/test/core">WebAssembly spec testsuite</a> and is able to run many <code>WASI</code> apps.</p>
<p dir="auto">Minimum useful system requirements: <strong>~64Kb</strong> for code and <strong>~10Kb</strong> RAM</p>
<p dir="auto"><code>wasm3</code> runs on a wide range of architectures (<code>x86</code>, <code>x86_64</code>, <code>ARM</code>, <code>RISC-V</code>, <code>PowerPC</code>, <code>MIPS</code>, <code>Xtensa</code>, <code>ARC32</code>, ...) and <a href="https://github.com/wasm3/wasm3/blob/main/platforms">platforms</a>:</p>
<ul dir="auto">
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0fe6ddf5d7d5241ce68518e07b9f18bf982f808c789b312a3335e283af2cc39d/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6c696e75782e737667"><img src="https://camo.githubusercontent.com/0fe6ddf5d7d5241ce68518e07b9f18bf982f808c789b312a3335e283af2cc39d/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6c696e75782e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/linux.svg"></a> Linux,
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/55adf930d73517390de6b22f3aa88c156538a84c0373b28416e5c17130e34850/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f77696e646f77732e737667"><img src="https://camo.githubusercontent.com/55adf930d73517390de6b22f3aa88c156538a84c0373b28416e5c17130e34850/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f77696e646f77732e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/windows.svg"></a> Windows,
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/77a890fc5e50699ee48fe93fbe3e81067acf5f5b8bc055729d36c7dd7bc43aea/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6170706c652e737667"><img src="https://camo.githubusercontent.com/77a890fc5e50699ee48fe93fbe3e81067acf5f5b8bc055729d36c7dd7bc43aea/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6170706c652e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/apple.svg"></a> OS X,
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/173700e722d00c8eaa8158c8c07bcdd1b21ae924cf71f948bfcf58f4964b472c/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f667265656273642e737667"><img src="https://camo.githubusercontent.com/173700e722d00c8eaa8158c8c07bcdd1b21ae924cf71f948bfcf58f4964b472c/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f667265656273642e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/freebsd.svg"></a> FreeBSD,
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eaa98f7b689b1f9d70eee17a9ddaeb416217ac9dea6613d51093bbbe647c9cce/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f616e64726f69642e737667"><img src="https://camo.githubusercontent.com/eaa98f7b689b1f9d70eee17a9ddaeb416217ac9dea6613d51093bbbe647c9cce/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f616e64726f69642e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/android.svg"></a> Android,
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/77a890fc5e50699ee48fe93fbe3e81067acf5f5b8bc055729d36c7dd7bc43aea/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6170706c652e737667"><img src="https://camo.githubusercontent.com/77a890fc5e50699ee48fe93fbe3e81067acf5f5b8bc055729d36c7dd7bc43aea/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f6170706c652e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/apple.svg"></a> iOS</li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f1f67a18307cf58f478d7e51b05622ae5f0b02fe55b8ac448f807645d226261a/68747470733a2f2f63646e2e7261776769742e636f6d2f6665617468657269636f6e732f666561746865722f6d61737465722f69636f6e732f776966692e737667"><img src="https://camo.githubusercontent.com/f1f67a18307cf58f478d7e51b05622ae5f0b02fe55b8ac448f807645d226261a/68747470733a2f2f63646e2e7261776769742e636f6d2f6665617468657269636f6e732f666561746865722f6d61737465722f69636f6e732f776966692e737667" width="18" height="18" data-canonical-src="https://cdn.rawgit.com/feathericons/feather/master/icons/wifi.svg"></a> OpenWrt, Yocto, Buildroot (routers, modems, etc.)</li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e4985d07da9ed03709560a3b26c6e238b9e24a9ab313ed926f4ed5f8ce88e244/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f72617370626572727970692e737667"><img src="https://camo.githubusercontent.com/e4985d07da9ed03709560a3b26c6e238b9e24a9ab313ed926f4ed5f8ce88e244/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f72617370626572727970692e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/raspberrypi.svg"></a> Raspberry Pi, Orange Pi and other SBCs</li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d46d9f8f4730eb1d393317da5202a544fa495d41989935c3cf8cff9a55d9e144/68747470733a2f2f63646e2e7261776769742e636f6d2f6665617468657269636f6e732f666561746865722f6d61737465722f69636f6e732f6370752e737667"><img src="https://camo.githubusercontent.com/d46d9f8f4730eb1d393317da5202a544fa495d41989935c3cf8cff9a55d9e144/68747470733a2f2f63646e2e7261776769742e636f6d2f6665617468657269636f6e732f666561746865722f6d61737465722f69636f6e732f6370752e737667" width="18" height="18" data-canonical-src="https://cdn.rawgit.com/feathericons/feather/master/icons/cpu.svg"></a> MCUs: Arduino, ESP8266, ESP32, Particle, ... <a href="https://github.com/wasm3/wasm3/blob/main/docs/Hardware.md">see full list</a></li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/552d4bb144f1dfc0cb48ccc21cfdbd65f6654da9f66524444167b66e571a700e/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f66697265666f7862726f777365722e737667"><img src="https://camo.githubusercontent.com/552d4bb144f1dfc0cb48ccc21cfdbd65f6654da9f66524444167b66e571a700e/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f73696d706c652d69636f6e732f73696d706c652d69636f6e7340646576656c6f702f69636f6e732f66697265666f7862726f777365722e737667" width="18" height="18" data-canonical-src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons@develop/icons/firefoxbrowser.svg"></a> Browsers. Yes, using WebAssembly itself!</li>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/wasm3/wasm3/blob/main/extra/wasm-symbol.svg"><img src="https://github.com/wasm3/wasm3/raw/main/extra/wasm-symbol.svg" width="18" height="18"></a> <code>wasm3</code> can execute <code>wasm3</code> (self-hosting)</li>
</ul>
<h2 tabindex="-1" dir="auto">Features</h2>
<table>
<thead>
<tr>
<th>Webassembly <a href="https://github.com/WebAssembly/proposals/blob/master/finished-proposals.md" title="WebAssembly Finished Proposals">Core Proposals</a></th>
<th>Extra</th>
</tr>
</thead>
<tbody>
<tr>
<td>☑ Import/Export of Mutable Globals</td>
<td>☑ Structured execution tracing</td>
</tr>
<tr>
<td>☑ Non-trapping float-to-int conversions</td>
<td>☑ Big-Endian systems support</td>
</tr>
<tr>
<td>☑ Sign-extension operators</td>
<td>☑ Wasm and WASI self-hosting</td>
</tr>
<tr>
<td>☑ Multi-value</td>
<td>☑ Gas metering</td>
</tr>
<tr>
<td>☑ Bulk memory operations (partial support)</td>
<td>☑ Linear memory limit (&lt; 64KiB)</td>
</tr>
<tr>
<td>☐ Multiple memories</td>
<td></td>
</tr>
<tr>
<td>☐ Reference types</td>
<td></td>
</tr>
<tr>
<td>☐ Tail call optimization</td>
<td></td>
</tr>
<tr>
<td>☐ Fixed-width SIMD</td>
<td></td>
</tr>
<tr>
<td>☐ Exception handling</td>
<td></td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Motivation</h2>
<p dir="auto"><strong>Why use a "slow interpreter" versus a "fast JIT"?</strong></p>
<p dir="auto">In many situations, speed is not the main concern. Runtime executable size, memory usage, startup latency can be improved with the interpreter approach. Portability and security are much easier to achieve and maintain. Additionally, development impedance is much lower. A simple library like Wasm3 is easy to compile and integrate into an existing project. (Wasm3 builds in a just few seconds). Finally, on some platforms (i.e. iOS and WebAssembly itself) you can't generate executable code pages in runtime, so JIT is unavailable.</p>
<p dir="auto"><strong>Why would you want to run WASM on embedded devices?</strong></p>
<p dir="auto">Wasm3 started as a research project and remains so by any means. Evaluating the engine in different environments is part of the research. Given that we have <code>Lua</code>, <code>JS</code>, <code>Python</code>, <code>Lisp</code>, <code>...</code> running on MCUs, <code>WebAssembly</code> is a promising alternative. It provides toolchain decoupling as well as a completely sandboxed, well-defined, predictable environment. Among practical use cases we can list <code>edge computing</code>, <code>scripting</code>, <code>plugin systems</code>, running <code>IoT rules</code>, <code>smart contracts</code>, etc.</p>
<h2 tabindex="-1" dir="auto">Used by</h2>
<p dir="auto"><a href="https://wasmcloud.dev/" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/wasmcloud.png" height="32"></a>　
<a href="https://wowcube.com/" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/wowcube.png" height="32"></a>　
<a href="https://github.com/siemens/dtasm/tree/main/runtime/dtasm3"><img src="https://camo.githubusercontent.com/51e7cf06b3b65f05606ac3f9e70cd5bd3c00824dd6519e602f02043c113c31d3/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f332f33632f5369656d656e735f41475f6c6f676f2e7376672f3130323470782d5369656d656e735f41475f6c6f676f2e7376672e706e67" height="22" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Siemens_AG_logo.svg/1024px-Siemens_AG_logo.svg.png"></a>　
<a href="https://scailable.net/" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/scailable.png" height="32"></a>　
<a href="https://blynk.io/" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/blynk.png" height="32"></a>　
<a href="https://www.iden3.io/" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/iden3.svg" height="32"></a>　
<a href="https://github.com/apache/incubator-nuttx-apps/tree/master/interpreters/wasm3"><img src="https://camo.githubusercontent.com/da37eca2607093a7c938d09a2b9f81c6ca4fe5c2e090acde046c76c9065795ef/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f622f62302f4e757474585f6c6f676f2e706e67" height="32" data-canonical-src="https://upload.wikimedia.org/wikipedia/commons/b/b0/NuttX_logo.png"></a>　
<a href="https://github.com/Losant/eea-examples"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/losant.png" height="28"></a>　
<a href="https://github.com/kateinoigakukun/wasmic-ios"><img src="https://user-images.githubusercontent.com/1506708/114701856-069ce700-9d2c-11eb-9b72-9ce2dfd9f0fb.png" height="32"></a>　
<a href="https://github.com/balena-io-playground/balena-wasm3"><img src="https://camo.githubusercontent.com/7d511f850262dc4ce09204b7cb52aae544a7698415976c15bc75163af84186b3/68747470733a2f2f6173736574732d676c6f62616c2e776562736974652d66696c65732e636f6d2f3633366162366261306531626432353065336161656461662f3633366531353565393338393463643464303330633464375f62616c656e615f6c6f676f5f6461726b2e737667" height="32" data-canonical-src="https://assets-global.website-files.com/636ab6ba0e1bd250e3aaedaf/636e155e93894cd4d030c4d7_balena_logo_dark.svg"></a>　
<a href="https://github.com/deislabs/krustlet-wasm3"><img src="https://camo.githubusercontent.com/07385dadb6ba6519c190b8cf39c2f4124c618619bd2f9d45e1cf65c74bc19a8a/68747470733a2f2f6b727573746c65742e6465762f696d616765732f686f72697a6f6e74616c2e737667" height="32" data-canonical-src="https://krustlet.dev/images/horizontal.svg"></a>　
<a href="https://shareup.app/blog/introducing-shareup" rel="nofollow"><img src="https://github.com/wasm3/wasm3/raw/main/extra/logos/shareup_app.svg" height="24"></a>　
<a href="https://wasm4.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/3fdc117d401a5a9bc4add3d06b13955daa54f3f9d8b72814e4f331506675cc81/68747470733a2f2f7761736d342e6f72672f696d672f6c6f676f2e706e67" height="32" data-canonical-src="https://wasm4.org/img/logo.png"></a></p>
<h2 tabindex="-1" dir="auto">Further Resources</h2>
<p dir="auto"><a href="https://github.com/wasm3/wasm3/blob/main/docs/Demos.md">Demos</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Installation.md">Installation instructions</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Cookbook.md">Cookbook</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Troubleshooting.md">Troubleshooting</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Development.md">Build and Development instructions</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Hardware.md">Supported Hardware</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Testing.md">Testing &amp; Fuzzing</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Performance.md">Performance</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Interpreter.md">Interpreter Architecture</a><br>
<a href="https://github.com/wasm3/wasm3/blob/main/docs/Diagnostics.md">Logging</a><br>
<a href="https://github.com/vshymanskyy/awesome-wasm-tools/blob/main/README.md">Awesome WebAssembly Tools</a></p>
<h3 tabindex="-1" dir="auto">License</h3>
<p dir="auto">This project is released under The MIT License (MIT)</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[This year in Servo: over 1000 pull requests and beyond (103 pts)]]></title>
            <link>https://servo.org/blog/2023/12/18/this-year-in-servo/</link>
            <guid>38681463</guid>
            <pubDate>Mon, 18 Dec 2023 12:09:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://servo.org/blog/2023/12/18/this-year-in-servo/">https://servo.org/blog/2023/12/18/this-year-in-servo/</a>, See on <a href="https://news.ycombinator.com/item?id=38681463">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>2023-12-18</span> Reflections on Servo’s progress in 2023: contributor stats, new features, layout improvements, WPT pass rates, and plans for next year.</p><div>
  <p>Servo is well and truly back.</p>
<figure><a href="https://servo.org/img/blog/recap-2023-contributors.png"><img src="https://servo.org/img/blog/recap-2023-contributors.png" alt="Bar chart: 453 (44%) by Igalia, 195 (19%) by non-Igalia, 389 (37%) by bots"></a>
<figcaption>Contributors to <a href="https://github.com/servo/servo">servo/servo</a> in 2023.</figcaption></figure>
<p><span></span>
This year, to date, we’ve had <strong>53 unique contributors</strong> (+140% over 22 last year), landing <strong>1037 pull requests</strong> (+382% over 215) and 2485 commits (+375% over 523), and that’s just in <a href="https://github.com/servo/servo">our main repo</a>!</p>
<p>Individual contributors are especially important for the health of the project, and of the pull requests made by humans (rather than our <a href="https://github.com/servo-wpt-sync">friendly</a> <a href="https://github.com/dependabot">bots</a>), 30% were by people outside Igalia, and 18% were by non-reviewers.</p>
<p>Servo has been featured in <a href="https://servo.org/about/">six conference talks</a> this year, including at <a href="https://www.youtube.com/watch?v=IdHvHoAO5oo">RustNL</a>, <a href="https://www.youtube.com/watch?v=pfk8s5OD99A">Web Engines Hackfest</a>, <a href="https://www.youtube.com/watch?v=J4qedc-0pjs&amp;t=2356s">LF Europe Member Summit</a>, <a href="https://youtu.be/9lkIX5ryZZ4">Open Source Summit Europe</a>, <a href="https://youtu.be/RugzThWcjn4">GOSIM Workshop</a>, and <a href="https://www.youtube.com/watch?v=lx70W83Bxtc">GOSIM Conference</a>.</p>
<p>Servo now has a usable “minibrowser” UI, now supports offscreen rendering, its experimental WebGPU support (<code>--pref dom.webgpu.enabled</code>) has been updated, and Servo is now listed on <a href="https://wpt.fyi/">wpt.fyi</a> again (click <span>Edit</span> to add Servo).</p>
<p><a href="https://servo.org/blog/2023/04/13/layout-2013-vs-2020/">Our new layout engine</a> is now proving its strengths, with support for iframes, floats, stacking context improvements, inline layout improvements, margin collapsing, ‘position: sticky’, ‘min-width’ and ‘min-height’, ‘max-width’ and ‘max-height’, ‘align-content’, ‘justify-content’, ‘white-space’, ‘text-indent’, ‘text-align: justify’, <a href="https://servo.org/blog/2023/05/31/adding-support-for-outline-properties/">‘outline’ and ‘outline-offset’</a>, and ‘filter: drop-shadow()’.</p>
<figure><a href="https://servo.org/img/blog/recap-2023-wpt.png"><img src="https://servo.org/img/blog/recap-2023-wpt.png" alt="Bar chart: 17% + 64pp in floats, 18% + 55pp in floats-clear, 63% + 15pp in key CSS2 tests, 80% + 14pp in abspos, 34% + 14pp in CSS position module, 67% + 13pp in margin-padding-clear, 49% + 13pp in CSSOM, 51% + 10pp in all CSS tests, 49% + 6pp in all WPT tests"></a>
<figcaption>Pass rates in parts of the <a href="https://web-platform-tests.org/">Web Platform Tests</a> with our new layout engine, showing the improvement we’ve made since the <a href="https://servo.org/blog/2023/07/20/servo-web-platform-tests/">start of our data</a> in April 2023.</figcaption></figure>
<p><span></span>
<a href="https://github.com/dbaron/inlines-and-floats">Floats are notoriously tricky</a>, to the point we found them <a href="https://github.com/servo/servo/wiki/Servo-Layout-Engines-Report">impossible to implement correctly</a> in our legacy layout engine, but thanks to the move from eager to opportunistic parallelism, they are now supported fairly well.
Whereas legacy layout was only ever able to reach 53.9% in the floats tests and 68.2% in floats-clear, we’re now at <strong>82.2% in floats</strong> (+28.3pp over legacy) and <strong>73.3% in floats-clear</strong> (+5.1pp over legacy).</p>
<p><a href="http://acid1.acidtests.org/">Acid1</a> now passes in the new layout engine, and we’ve also surpassed legacy layout in the <strong>CSS2 abspos (by 50.0pp)</strong>, CSS2 positioning (by 6.5pp), and CSS Position (by 4.4pp) test suites, while making big strides in others, like the <strong>CSSOM tests (+13.1pp)</strong> and key parts of the <strong>CSS2 test suite (+15.8pp)</strong>.</p>
<p><span></span>
<a href="https://github.com/servo/servo/wiki/Roadmap/106e95887c3d9768f791a4e0501ba5c89abe9636">Next year</a>, our funding will go towards maintaining Servo, releasing nightlies on Android, finishing our integration with Tauri (thanks to NLNet), and implementing tables and better support for floats and non-Latin text (thanks to NLNet).</p>
<p>Servo will also be at <a href="https://fosdem.org/2024/">FOSDEM 2024</a>, with Rakhi Sharma speaking about <a href="https://fosdem.org/2024/schedule/event/fosdem-2024-2321-embedding-servo-in-rust-projects/">embedding Servo in Rust projects</a> on <strong>3 February</strong> at <strong>16:45 local time</strong> (15:45 UTC).
See you there!</p>
<p>There’s a lot more we would like to do, so if you or a company you know are interested in sponsoring the development of an embeddable, independent, memory-safe, modular, parallel web rendering engine, we want to hear from you!
Head over to <a href="https://servo.org/sponsorship/">our sponsorship page</a>, or email <a href="https://servo.org/cdn-cgi/l/email-protection#4923262027093a2c3b3f2667263b2e"><span data-cfemail="543e3b3d3a14273126223b7a3b2633">[email&nbsp;protected]</span></a> for enquiries.</p>
<p>In a decade that many people feared would become the nadir of browser engine diversity, we hope we can help change that with Servo.</p>
<!--
pull request data
$ tools/list-pull-requests.sh servo/servo '2022-.*' | tee 2022.json
$ tools/list-pull-requests.sh servo/servo '2023-.*' | tee 2023.json

pull requests
- 2022: 215 (< 2022.json jq -s length)
- 2023: 1037 (< 2023.json jq -s length)

contributors
- 2022: 22 (< 2022.json jq -r .user.login | sort | uniq -c | sort -nr | wc -l)
- 2023: 53 (< 2023.json jq -r .user.login | sort | uniq -c | sort -nr | wc -l)

reviewers
- https://github.com/orgs/servo/teams/developers
- copy($$("#team-members li[data-bulk-actions-id]").map(x => x.dataset.bulkActionsId).sort().join("\n"))

commits
- 2022: 523 (git log --pretty=format:$'%h\t%cd' | rg ' 2022 ' | wc -l)
- 2023: 2485 (git log --pretty=format:$'%h\t%cd' | rg ' 2023 ' | wc -l)

for wpt pass rates and all other analysis, see assets/img/blog/recap-2023.ods
-->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["I just bought a 2024 Chevy Tahoe for $1" (244 pts)]]></title>
            <link>https://twitter.com/ChrisJBakke/status/1736533308849443121</link>
            <guid>38681450</guid>
            <pubDate>Mon, 18 Dec 2023 12:08:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ChrisJBakke/status/1736533308849443121">https://twitter.com/ChrisJBakke/status/1736533308849443121</a>, See on <a href="https://news.ycombinator.com/item?id=38681450">Hacker News</a></p>
Couldn't get https://twitter.com/ChrisJBakke/status/1736533308849443121: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[EU opens proceedings against X over efforts to combat information manipulation (122 pts)]]></title>
            <link>https://www.reuters.com/technology/eu-opens-proceedings-against-x-over-its-efforts-combat-information-manipulation-2023-12-18/</link>
            <guid>38681353</guid>
            <pubDate>Mon, 18 Dec 2023 11:50:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/eu-opens-proceedings-against-x-over-its-efforts-combat-information-manipulation-2023-12-18/">https://www.reuters.com/technology/eu-opens-proceedings-against-x-over-its-efforts-combat-information-manipulation-2023-12-18/</a>, See on <a href="https://news.ycombinator.com/item?id=38681353">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/eu-opens-proceedings-against-x-over-its-efforts-combat-information-manipulation-2023-12-18/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Microagents: Agents capable of self-editing their prompts / Python code (168 pts)]]></title>
            <link>https://github.com/aymenfurter/microagents</link>
            <guid>38679453</guid>
            <pubDate>Mon, 18 Dec 2023 05:13:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aymenfurter/microagents">https://github.com/aymenfurter/microagents</a>, See on <a href="https://news.ycombinator.com/item?id=38679453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">microagents: Modular Agents Capable of Self-Editing Their Prompts and Python code</h2>
<a href="https://raw.githubusercontent.com/aymenfurter/microagents/main/static/output.gif" rel="nofollow">
<img src="https://raw.githubusercontent.com/aymenfurter/microagents/main/static/output.gif?raw=true" data-animated-image="">
</a><h3 tabindex="-1" dir="auto"><a></a>Synthesized Agent Prompts (From Above Demo)</h3>
<h4 tabindex="-1" dir="auto">CalculateAddition Agent</h4>
<div data-snippet-clipboard-copy-content="You are an adept arithmetic solver with focus on performing addition. Utilize this Python function to calculate the sum of two numbers:

``python
def calculate_addition(num1, num2):
    return num1 + num2

# Example usage:
print(calculate_addition(5, 9))
``"><pre><code>You are an adept arithmetic solver with focus on performing addition. Utilize this Python function to calculate the sum of two numbers:

``python
def calculate_addition(num1, num2):
    return num1 + num2

# Example usage:
print(calculate_addition(5, 9))
``
</code></pre></div>
<h4 tabindex="-1" dir="auto">GetPopulationOfCountry Agent</h4>
<div data-snippet-clipboard-copy-content="You are a skilled data extractor specializing in population statistics. Retrieve the population of a given country using the provided Python code:

``python
import requests

def get_population(country):
    url = f&quot;https://restcountries.com/v3.1/name/{country}&quot;
    response = requests.get(url).json()
    population = response[0]['population']
    print(f&quot;The population of {country} is {population}.&quot;)

# Example usage:
get_population(&quot;CountryName&quot;)
``"><pre><code>You are a skilled data extractor specializing in population statistics. Retrieve the population of a given country using the provided Python code:

``python
import requests

def get_population(country):
    url = f"https://restcountries.com/v3.1/name/{country}"
    response = requests.get(url).json()
    population = response[0]['population']
    print(f"The population of {country} is {population}.")

# Example usage:
get_population("CountryName")
``
</code></pre></div>
<h3 tabindex="-1" dir="auto">How does it work?</h3>
<p dir="auto">This experiment explores self-evolving agents that automatically generate and improve themselves. No specific agent design or prompting is required from the user. Simply pose a question, and the system initiates and evolves agents tailored to provide answers. The process starts with a user query, activating a basic "bootstrap" agent, which doesn't execute Python code but plans and delegates to specialized agents capable of running Python for broader functions. An Agent Manager oversees them, selecting or creating agents via vector similarity for specific tasks. Agents have evolving system prompts that improve through learning. For coding tasks, agents include Python in prompts, refining their approach through an "evolution step" if unsuccessful. Upon completing a task, an agent's status updates, and the bootstrap agent evaluates the result, engaging other agents for further steps in larger processes.</p>
<h3 tabindex="-1" dir="auto">Current Challenges and Potential Improvements</h3>
<ol dir="auto">
<li>
<p dir="auto"><strong>Path Optimization</strong>: The system sometimes fails to effectively discard non-functional agents.</p>
</li>
<li>
<p dir="auto"><strong>Performance and Parallelization</strong>: Currently, parallel processing is not implemented. Enabling the testing of multiple prompt evolutions simultaneously could significantly enhance performance.</p>
</li>
<li>
<p dir="auto"><strong>Strategy for Prompt Evolution</strong>: The approach to prompt evolution is quite basic at the moment. Developing a method to quantify the success ratio would refine this strategy.</p>
</li>
<li>
<p dir="auto"><strong>Persistent Agent Prompts</strong>: There is significant potential in integrating persistent agent prompts with vector databases. Additionally, sharing successful agents across various runtime environments could improve overall efficiency.</p>
</li>
<li>
<p dir="auto"><strong>Hierarchical Agent Structure</strong>: Most requests are presently processed directly by an agent designated by the bootstrap agent. Implementing a more intricate hierarchical structure for managing requests could lead to major improvements.</p>
</li>
<li>
<p dir="auto"><strong>Context Size Limitation</strong>: Not yet considered.</p>
</li>
</ol>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pilots hide mental health problems so they don't lose their licenses (156 pts)]]></title>
            <link>https://www.washingtonpost.com/travel/2023/12/15/pilots-mental-health-faa-certification/</link>
            <guid>38678779</guid>
            <pubDate>Mon, 18 Dec 2023 02:58:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/travel/2023/12/15/pilots-mental-health-faa-certification/">https://www.washingtonpost.com/travel/2023/12/15/pilots-mental-health-faa-certification/</a>, See on <a href="https://news.ycombinator.com/item?id=38678779">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/travel/2023/12/15/pilots-mental-health-faa-certification/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[AMD MI300X 30% higher performance than Nvidia H100, even with optimized stack (154 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/gpus/amd-strikes-back-at-nvidia-with-new-mi300x-benchmarks-mi300x-shows-30-higher-performance-than-h100-even-with-an-optimized-software-stack</link>
            <guid>38678066</guid>
            <pubDate>Mon, 18 Dec 2023 00:58:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/gpus/amd-strikes-back-at-nvidia-with-new-mi300x-benchmarks-mi300x-shows-30-higher-performance-than-h100-even-with-an-optimized-software-stack">https://www.tomshardware.com/pc-components/gpus/amd-strikes-back-at-nvidia-with-new-mi300x-benchmarks-mi300x-shows-30-higher-performance-than-h100-even-with-an-optimized-software-stack</a>, See on <a href="https://news.ycombinator.com/item?id=38678066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">
<p>Neither AMD nor Nvidia intends to back out of this argument involving the performance difference between the Instinct MI300X and the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022">H100</a> (Hopper) GPUs. But AMD does make some strong points while comparing FP16 using vLLM, which is a more popular choice against FP8, which works only with TensorRT-LLM.&nbsp;</p><p>The red team announced the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/amd-unveils-instinct-mi300x-gpu-and-mi300a-apu-claims-up-to-16x-lead-over-nvidias-competing-gpus" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/amd-unveils-instinct-mi300x-gpu-and-mi300a-apu-claims-up-to-16x-lead-over-nvidias-competing-gpus">MI300X graphics accelerator</a> early this December, claiming up to 1.6X lead over Nvidia's H100. <a data-analytics-id="inline-link" href="https://www.tomshardware.com/news/nvidia-h100-is-2x-faster-than-amd-m1300x" data-before-rewrite-localise="https://www.tomshardware.com/news/nvidia-h100-is-2x-faster-than-amd-m1300x">Two days ago</a>, Nvidia fired back by saying AMD did not use its optimizations when comparing the H100 with TensorRT-LLM. The reply reached a single H100 against eight-way H100 GPUs while running the Llama 2 70B chat model.&nbsp;</p><h2 id="the-continued-war-of-benchmark-results-and-test-scenarios-3">The Continued War of Benchmark Results and Test Scenarios</h2><p>In this latest response, AMD said that Nvidia used a selective set of inferencing workloads. It further identified that Nvidia benchmarked these using its in-house TensorRT-LLM on H100 rather than vLLM, an open-source and widely used method. Furthermore, Nvidia used the vLLM FP16 performance datatype on AMD while comparing its results with DGX-H100, which used the TensorRT-LLM with FP8 datatype to display these alleged misconstrued results. AMD stressed that in its test, it used vLLM with the FP16 dataset due to its widespread use, and vLLM does not support FP8.</p><p>There's also the point that servers will have latency, but instead of accounting for that, Nvidia showed its throughput performance, not emulating the real-world situation, according to AMD.</p><h2 id="amd-apos-s-updated-test-results-with-more-optimizations-and-accounting-for-latency-with-nvidia-apos-s-testing-method-3">AMD's Updated Test Results with More Optimizations and Accounting for Latency with Nvidia's Testing Method</h2><p>AMD made three performance runs using Nvidia's TensorRT-LLM, the last notable one having measured latency results between MI300X and vLLM using the FP16 dataset against H100 with TensorRT-LLM. But the first test involved a comparison between the two using vLLM on both, hence FP16, and for the second test, it compared its MI300X's performance with vLLM while comparing TensorRT-LLM.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="AMD's second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg"><source type="image/jpeg" alt="AMD's second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg"><img src="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" alt="AMD's second Inference Performance bench on Llama 2 70B using three test scenarios" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc.jpg" srcset="https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/ocGERk5ybqFhgHFv6ic6Pc-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: AMD )</span></figcaption></figure><p>So, AMD used the same selected testing scenario Nvidia did with its second and third test scenarios, showing higher performance and reduced latency. The company added more optimizations when compared to H100 while running vLLM on both, offering a 2.1x boost in performance.</p><p>It is now up to Nvidia to evaluate how it wants to respond. But it also needs to acknowledge that this would require the industry to ditch FP16 with TensorRT-LLM's closed system for using FP8, essentially ditching vLLM for good. While referring to Nvidia's premium, a <a data-analytics-id="inline-link" href="https://www.reddit.com/r/LocalLLaMA/comments/16du7xj/nvidia_tensorrtllm/" target="_blank" data-url="https://www.reddit.com/r/LocalLLaMA/comments/16du7xj/nvidia_tensorrtllm/">Redditor once said</a>, "TensorRT-LLM is free just like the things that come free with a Rolls Royce."</p>
</div><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-x4MmDABPR8ezWHrbuxadyc"><section><p>Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mickey, Disney, and the public domain: A 95-year love triangle (307 pts)]]></title>
            <link>https://web.law.duke.edu/cspd/mickey/</link>
            <guid>38678021</guid>
            <pubDate>Mon, 18 Dec 2023 00:50:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://web.law.duke.edu/cspd/mickey/">https://web.law.duke.edu/cspd/mickey/</a>, See on <a href="https://news.ycombinator.com/item?id=38678021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="homenews"><p id="top">by Jennifer Jenkins, Director, Duke Center for the Study of the Public Domain</p>
<p>CC BY 4.0&nbsp;<a href="https://creativecommons.org/licenses/by/4.0/" target="_blank"><img decoding="async" src="https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by.png" width="59px"></a></p>
<p><img alt="Image of Steamboat Willie Entering the Public Domain by Doo Lee" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Steamboat-WIllie-Enters-Public-Domain.jpeg"></p>
<p>On January 1, 2024, after almost a century of copyright protection, Mickey Mouse, or at least a <em>version</em> of Mickey Mouse, will enter the public domain. The first movies in which the iconic mouse appeared – most famously, <em>Steamboat Willie­</em><a href="#_ftn1" name="_ftnref1" title="">[1]</a> – were made in 1928 and works from that year go into the public domain in the United States on New Year’s Day 2024.<a href="#_ftn2" name="_ftnref2" title="">[2]</a></p>
<p>The public domain has had some famous recent arrivals, but this is the most anticipated entry yet. Why? It is not simply that Mickey is a famous copyrighted character. So are Sherlock Holmes and Winnie the Pooh, and while they entered the public domain with some fanfare, it paled in comparison to this event. I’d like to offer a tentative answer. The reason that this event gathers so much attention is that it is the story of a 95-year-old love triangle, a tangled drama that rivals any Disney movie for twists and turns. The protagonists are Mickey, Disney and the Public Domain, and their relationship positively exemplifies the social media weasel-words “it’s complicated.”</p>
<p>On the one hand, Disney pushed for the law that extended the copyright term to 95 years, which became referred to derisively as the “Mickey Mouse Protection Act.”&nbsp; This extension has been criticized by scholars as being economically regressive and having a devastating effect on our ability to digitize, archive, and gain access to our cultural heritage. It locked up not just famous works, but a vast swath of our culture, including material that is commercially unavailable. Even though calling it the “Mickey Mouse Protection Act” may overstate Disney’s actual role in the legislative process – the measure passed because of a much broader lobbying effort – Disney was certainly a prominent supporter, and the Mouse was sometimes a figurehead.</p>
<p>On the other hand, Disney itself is a talented and successful practitioner of <em>building upon</em> the public domain. In fact, the public domain is Disney’s bread and butter. <em>Frozen</em> was inspired by Hans Christian Andersen’s <em>The Snow Queen</em>. <em>The Lion King </em>draws from Shakespeare’s <em>Hamlet</em>, Biblical stories, and possibly an epic poem about the founder of the Mali Empire.<a href="#_ftn3" name="_ftnref3" title="">[3]</a> <em>Fantasia </em>showcases public domain classical music, and “The Sorcerer’s Apprentice” segment comes from a poem by Johann Wolfgang von Goethe. <em>Alice in Wonderland</em>, <em>Snow White</em>, <em>The Hunchback of Notre Dame</em>, <em>Sleeping Beauty</em>, <em>Cinderella</em>, <em>The Little Mermaid</em>, and&nbsp;<em>Pinocchio</em> came from stories by Lewis Carroll, The Brothers Grimm, Victor Hugo, Charles Perrault, Hans Christian Anderson, and Carlo Collodi.</p>
<p>The public domain includes not only works over which copyright has expired or never existed, but also uncopyrightable aspects of contemporary works—such as ideas, stock elements, and unoriginal material. The Mickey character itself is based on such public domain fodder. His personality and antics drew from silent film stars such as Charlie Chaplin and Douglas Fairbanks. Walt Disney told <em>The American Magazine</em>: “I think we were rather indebted to Charlie Chaplin …We wanted something appealing and we thought of a tiny bit of a mouse that would have something of the wistfulness of Chaplin…a little fellow trying to do the best he could.”<a href="#_ftn4" name="_ftnref4" title="">[4]</a> Ub Iwerks, who animated most of <em>Steamboat Willie</em>, wrote that Douglas Fairbanks “was the super-hero of his day, always winning, gallant and swashbuckling. Mickey’s action was in that vein…I thought of him in that respect, and I had him do naturally the sort of thing Doug Fairbanks would do.” Titles are also not copyrightable, and the name “Steamboat Willie” was a nod to the title of Buster Keaton’s film from earlier the same year, <em>Steamboat Bill, Jr.</em><a href="#_ftn5" name="_ftnref5" title="">[5]</a></p>
<p>Hence the triangle. Disney is both an emblem of term extension and its erosion of the public domain, and one of the strongest use-cases in <em>favor</em> of the maintenance of a rich public domain. Mickey is the symbol of both tendencies. Ironies abound. It may not be <em>exactly </em>the same as an oil company relying on solar power to run its rigs, but it is definitely in the same “massive irony” zip code. All of this makes the year when copyright finally expires over Mickey Mouse highly symbolic. The love triangle between Mickey, Disney, and the public domain is about to evolve, and perhaps even <em>resolve</em>, in real time.</p>
<p>But what does public domain status actually <em>mean</em> for the <em>Steamboat Willie</em> version of Mickey?<a href="#_ftn6" name="_ftnref6" title="">[6]</a> There is a vast amount of misinformation about these issues online. In what follows, I will try to offer a straightforward explainer. What <em>can </em>and <em>can’t </em>you do with the Mickey Mouse character as of January 1, 2024? How will Disney be affected? Does Disney still hold copyrights over later versions of Mickey? Does trademark law play a role? Keep reading for details.</p>

<p><a href="#jan1"><strong>What can I do with Mickey Mouse as of January 1, 2024? </strong></a></p>
<p><a href="#versions"><strong>What about more recent versions of Mickey? </strong></a></p>
<p><a href="#trademark"><strong>What about Disney’s <em>trademark</em> over Mickey?</strong></a></p>
<p><a href="#creativity"><strong>The Public Domain: A Wellspring of Creativity </strong></a></p>

<h3 id="green">
	<em>What can I do with Mickey Mouse as of January 1, 2024? </em></h3>

<p>The answer, ironically, is distinctly mouse-shaped! Here is a diagram.</p>
<p><img alt="A green circle with black text and words" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Mickey_Mouse_Trademark_and_Copyright_Do_and_Dont_v4.png"></p>
<p>As explained by <a href="https://youtu.be/nAp7fhUd7u0?t=607" target="_blank">Karyn A. Temple</a>, former United States Register of Copyrights, the public domain “is part of copyright’s lifecycle, the next stage of life for that creative work. The public domain is an inherent and integral part of the copyright system…It provides authors the inspiration and raw material to create something new.”</p>
<p><em>Steamboat Willie </em>and the characters it depicts – which include both Mickey and Minnie Mouse – will be in the public domain. As indicated in the green circle, this means that anyone can share, adapt, or remix that material. You can start your creative engines too—<em>full steam ahead</em>! You could take a page out of the <a href="https://winniethepooh.whogivesacrap.org/" target="_blank"><em>Winnie-the-Pooh: the Deforested Edition</em></a> playbook and create “Steamboat Willie: the Climate Change Edition,” in which Mickey’s boat is grounded in a dry riverbed. You could create a feminist remake with Minnie Mouse as the central figure. You could reimagine Mickey and Minnie dedicating themselves to animal welfare. (The animals in <em>Steamboat Willie </em>are contorted rather uncomfortably into musical instruments. PETA would not approve.)</p>
<p>You can do all of this and more, <em>so long as </em>you steer clear of the subsisting rights indicated by the orange circles, namely:</p>
<ul>
	<li>
		<p>Use the original versions of Mickey and Minnie Mouse from 1928, without copyrightable elements of later iterations (though not every later iteration will be copyrightable, as I explain below) and</p>
	</li>
	<li>
		<p>Do not confuse consumers into thinking that your creation is produced or sponsored by Disney as a matter of trademark law. One way to help ensure that your audience is not confused is to make the actual source of the work – you or your company – clear on the title screen or cover, along with a prominent disclaimer indicating that your work was not produced, endorsed, licensed, or approved by Disney.</p>
	</li>
</ul>
<p>So, is January 1, 2024 doomsday for Disney? No. Disney still retains copyright over newer iterations of Mickey such as the “Sorcerer’s Apprentice” Mickey from <em>Fantasia</em> (1940) as well as trademarks over Mickey as a brand identifier. People will still go to its theme parks, pay to see its movies, buy its merchandise. Its brand identity will remain intact.</p>
<p>In sum, yes, you can use Mickey in new creative works. There are some more complex peripheral legal issues, but here is your guide through them.</p>
<p id="spacer"><a href="#toc">(back to contents)</a></p>

<h3 id="green">
	<em>What about more recent versions of Mickey?</em></h3>

<div>
	<p><img alt="Mickey Glove Pointing for directional sign to the clubhouse ..." src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Pointing-glove-copy.jpg"></p>
	<div>
		<p><strong>Copyright expiration allows you to use the original Mickey and Minnie Mouse in new creative works, even though those characters also appear in more recent works that are still under copyright.</strong></p>
		<p><strong>With newer iterations of those characters, Disney only owns <em>original, creative expression</em> that qualifies for copyright, not mere ideas, unoriginal or stock character features, or “merely trivial” variations to the original characters.</strong></p>
	</div>
</div>
<p>In 2024 Mickey Mouse joins a host of other familiar public domain characters—Winnie the Pooh, Sherlock Holmes, Dracula, Frankenstein’s Monster, Robin Hood, Snow White, Cinderella, and Alice in Wonderland. You can use Mickey and Minnie 1.0, as they appeared in <em>Steamboat Willie </em>and<em> Plane Crazy</em>, even though these characters also appear in later, still-copyrighted works. Under copyright law, Disney only owns the <em>newly added</em> material in subsequent works, not underlying material from 1928—that content remains freely available.<a href="#_ftn7" name="_ftnref7" title="">[7]</a></p>
<p>“It is a bedrock principle of copyright that once work enters the public domain it cannot be appropriated as private (intellectual) property, and even the most creative of legal theories cannot trump this tenet.” –<em>Klinger v. Conan Doyle Estate</em> (N.D. Ill. 2013) aff’d (7th Cir. 2014)</p>
<p>This point was illustrated by the Conan Doyle Estate Ltd.’s unsuccessful attempts to artificially extend the expired copyright over the characters of Sherlock Holmes and Dr. Watson, <a href="https://web.law.duke.edu/cspd/publicdomainday/2023/#sherlockholmes" target="_blank">covered</a> on last year’s Public Domain Day site. A court <a href="https://freesherlock.files.wordpress.com/2013/12/klinger-order-on-motion-for-summary-judgment-c.pdf" target="_blank">confirmed</a> that all of the story elements in public domain works are “free for public use”: “Where an author has used the same character in a series of works, some of which are in the public domain, the public is free to copy story elements from the public domain works.”</p>
<p>Mickey’s appearance has changed over time, going from a vaguely rat-like to a more neotenous appearance. He eyes varied over time and began as large white ovals with pupils in <em>Plane Crazy</em> and smaller black dots in <em>Steamboat Willie</em> – both of those versions of Mickey are public domain in 2024. In 1929 he quickly donned gloves (apparently so that his hands were more visible against his body) and later he was colorized.”. The overall appearance of <em>Fantasia </em>Mickey and other later Mickeys is still under copyright.</p>
<p><img alt="A cartoon of a mouse" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/mickey_timeline_v2.jpg" width="85%"></p>
<p>That said, not every feature of Mickey’s later iterations is individually copyrightable. Copyright only extends to “original, creative expression.” Mere ideas are not eligible, nor are unoriginal features or stock elements. When copyright is claimed over additions to preexisting content – such as changes to the 1928 Mickey character – those changes have to be more than a “merely trivial” or “miniscule” variation on what came before.<a href="#_ftn8" name="_ftnref8" title="">[8]</a> It is not enough for the new material just to be <em>different</em>; it has to meet copyright’s threshold requirements for protectability.</p>
<p>Therefore, while the safest approach may be to stick to Mickey circa 1928 in new creations, copyright law also lets you use later material that does not qualify for copyright. Your mouse can speak intelligibly in a high voice even though Mickey 1.0 does not do so; giving a talking mouse a squeaky voice is not copyrightable. Generic character traits such as being adorable and having less jaunty dance moves are fair game. What’s more, anything you “independently create” – or come up with yourself – is legal. Choosing your own color scheme is fine; you do not have to stick to black and white.</p>
<p>Can Disney claim copyright over the color red – standing alone – for Mickey’s shorts? On the one hand, when talking about copyright’s “originality” requirement, the Supreme Court said that “the requisite level of creativity is extremely low; even a slight amount will suffice. The vast majority of works make the grade quite easily, as they possess some creative spark, ‘no matter how crude, humble or obvious’ it might be.” But the law is also clear that adding something to a prior work must be more than a “merely trivial” variation. To us, the argument seems stronger that choosing a single, bright, primary color for an article of clothing does not meet the copyrightability threshold. That said, you may be more comfortable selecting your own color scheme.</p>
<p id="spacer"><a href="#toc">(back to contents)</a></p>

<h3 id="green">
	<em>What about Disney’s trademark over Mickey?</em></h3>

<p>The plot thickens when you add Disney’s <em>trademark</em> rights to the picture. Many sources claim that even though Mickey is copyright-free in 2024, you still cannot use the character because it is trademarked by Disney. But this is not what the law actually says. Trademark law only prohibits the use of a trademarked character if doing so “is likely to cause confusion, or to cause mistake, or to deceive” consumers about the source or sponsorship of the new product.<a href="#_ftn9" name="_ftnref9" title="">[9]</a></p>

<div>
	<p><img alt="Mickey Glove Pointing for directional sign" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Pointing-glove-copy.jpg"></p>
	<p><strong>Trademark law does not prevent you from using the 1928 Mickey character unless consumers will be misled into thinking your work is produced or sponsored by Disney.</strong></p>
</div>

<p>First, some background. <strong>Copyrights and trademarks are different.</strong> <strong><em>Copyrights</em></strong> cover creative works and prevent people from copying and adapting them without permission, with the goal of providing economic incentives to create and distribute cultural material. The US Constitution requires that these rights expire after a “limited time,” so that the public and future creators can have unfettered access to creative works.</p>
<p><strong><em>Trademarks</em></strong> cover words, logos, images, and other signifiers that serve as <em>brands</em> identifying the source of a product. The goal is to minimize consumer confusion in the marketplace. Nike can prevent other producers of athletic apparel from putting “Nike” or a swoosh on their merchandise so that when purchasers see those indicators, they know they are getting a Nike product. But it does not own the word “Nike” outright; there are lots of uses of “Nike” that do not violate trademark, whether referring to the Greek goddess of victory or to the brand in a non-confusing way.</p>
<p>Unlike copyrights, trademarks do not automatically expire. They can last as long as a mark is still being “used in commerce.” If Amazon goes on using the “Amazon” brand for 500 years, the trademark stays alive. While trademarks can outlast copyrights, however, the rights themselves are more circumscribed. For most marks, trademark law only prevents the use of a mark on similar products when it is likely to create consumer confusion about the product’s origin or sponsorship.<a href="#_ftn10" name="_ftnref10" title="">[10]</a> Non-confusing uses are not prohibited, and there are a variety of legal safeguards for uses of trademarks in connection with expressive works such as films, books, and songs.</p>
<p>Sometimes copyrights and trademarks overlap.<a href="#_ftn11" name="_ftnref11" title="">[11]</a> A character such as Mickey Mouse and Winnie the Pooh might be covered both by copyright law (as a creative work) and trademark law (as a logo if, when people see the character on a backpack or pajamas, they think it must be official Disney merchandise). While the copyright is active, Disney can keep people from making unauthorized uses of Mickey in new works (unless those uses qualify for “fair use” protection, as with a parody). While the trademark is in effect, Disney can keep people from slapping Mickey on luggage or apparel.</p>
<p>“We have been careful to caution against misuse or overextension of trademark and related protections into areas traditionally occupied by . . . copyright.” <em>Dastar v. Twentieth Century Fox</em> (Supreme Court 2003)</p>
<p>But what happens when the copyright <em>expires</em> and the trademark is ongoing? In a unanimous opinion, the Supreme Court made clear that trademarks cannot be used to make an end run around copyright law because this would “create a species of mutant copyright law that limits the public’s federal right to copy and to use expired copyrights.”<a href="#_ftn12" name="_ftnref12" title="">[12]</a> In other words, <strong>trademark rights cannot be used to block the freedoms that the expiration of copyright allows</strong>, such as using a public domain character in a new creative work. Along the same lines, the Ninth Circuit Court of Appeals explained that when a work enters the public domain “[w]e all own it now” and trademark law “cannot be used to circumvent copyright law. If material covered by copyright law has passed into the public domain, it cannot then be protected by the Lanham Act [the federal trademark statute] without rendering the Copyright Act a nullity.”<a href="#_ftn13" name="_ftnref13" title="">[13]</a> Those who tell you otherwise are mistaken.</p>
<p>This brings us to Mickey Mouse. The <em>Steamboat Willie </em>copyright expires in 2024. But Disney still retains trademark rights to use images of Mickey as well as the words “Mickey Mouse” in connection with a variety of products. Here are some of its federally registered Mickey images for merchandise such as clothing, backpacks, watches, linens, toys, blankets, lunch boxes, and water bottles.</p>
<p><img alt="Trademark image" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Mickey-logos1.png" width="45%"></p>
<p>Disney has also started using this logo before some of its films.</p>
<p><img alt="A logo for a movie" src="https://web.law.duke.edu/sites/default/files/images/centers/cspd/pdd2024/mickey/Mickey-logo2.jpg" width="45%"></p>
<p>If you make your own Mickey cartoon, can Disney use trademark law to interfere? Trademark law is all about preventing consumer confusion – and not about getting in the way of creativity – so it depends on whether people are likely to be misled about the source of your cartoon. As long as no one thinks it is a Disney joint, there should not be a trademark problem.</p>
<p>When might consumers think you are offering a Disney-sponsored product? Here it is important to distinguish between different uses of the Mickey character. Certainly, there might be a risk of confusion if you use Mickey as a brand identifier on the kind of <em>merchandise</em> Disney sells. Trademark law does protect Disney against that risk. Consumers may also be confused if Mickey is used in an artistic work in a way that suggests it is a Disney production, for example by appearing as a logo at the beginning of an animation.</p>
<p>Contrast these uses with putting the Mickey character in a new cartoon or book. The latter is not the kind of use that misleads as to the <em>source</em> of a product. It is exactly what copyright expiration is intended to allow.<a href="#_ftn14" name="_ftnref14" title="">[14]</a> Were trademark law to prevent this, then trademark rights would be leveraged to obtain the effective equivalent of a perpetual copyright—precisely what the Supreme Court said we cannot do. As mentioned earlier, one way to dispel potential confusion is to make it clear that you – and not Disney – are responsible for the new work, and to add a disclaimer making it clear that your work is <em>not</em> produced or sponsored by Disney.</p>
<p>Moving beyond the Mickey character, what about Disney’s trademark over the “Mickey Mouse” name? &nbsp;Here too putting the words “Mickey Mouse” on toys or onesies is different from using “Mickey Mouse” to describe the content of a new creative work. With the latter, a disclaimer is also helpful. One court explained: “When a public domain work is copied, along with its title, there is little likelihood of confusion when even the most minimal steps are taken to distinguish the publisher of the original from that of the copy. The public is receiving just what it believes it is receiving—the work with which the title has become associated. The public is not only unharmed, it is unconfused.”<a href="#_ftn15" name="_ftnref15" title="">[15]</a></p>
<p>With artistic uses of trademarked material, the First Amendment’s protection for freedom of expression also comes into play. Trademark law has a number of speech-protecting limitations that safeguard expressive uses. This is why when Mattel sues people for using “Barbie” in the titles of songs and photographs, they lose in court. Mattel also lost when it objected to the use of the doll’s trademarked <em>appearance </em>in a series of photographs criticizing its role in our culture.<a href="#_ftn16" name="_ftnref16" title="">[16]</a></p>
<p>One defense allows “nominative use” of a trademark as a point of reference – for example, using “Mickey Mouse” accurately to refer to the public domain character in your work. Another comes from a case called <em>Rogers v. Grimaldi</em>, which privileged the use of trademarks in titles of expressive works as long as the term has some artistic relevance to the new work and does not explicitly mislead as to the source of the work.<a href="#_ftn17" name="_ftnref17" title="">[17]</a> The policy underlying this rule is that trademark law should only apply to artistic works when the public interest in avoiding consumer confusion outweighs the public interest in free expression. While a disclaimer is not required to benefit from these limitations, it can nevertheless be useful to make abundantly clear that you are not providing an official Disney production.</p>
<p>Finally, there is a small subset of extremely “famous” marks – essentially, superbrands – that receive extra “anti-dilution” protection against unauthorized uses that impair the distinctiveness or harm the reputation of the famous mark, even when there is no consumer confusion. While the mouse-ears silhouette or current iteration of the Mickey logo <em>might</em> fall into this category of famous marks, the Mickey character from <em>Steamboat Willie</em> does not.<a href="#_ftn18" name="_ftnref18" title="">[18]</a> (Just because a brand is “famous” for trademark purposes does not mean that all of its trademarks are considered famous; “Nike” is famous but many of its product names are not.) And even if Mickey 1.0 does eventually qualify, anti-dilution protection is subject to important First-Amendment exceptions that allow for the kinds of expressive uses discussed earlier.<a href="#_ftn19" name="_ftnref19" title="">[19]</a></p>
<p>Bottom line: trademark law has a number of rules designed to safeguard expressive uses and prevent trademark law from overriding copyright law. Nonetheless, people sometimes still try to use trademark law to interfere with legal reuses of public domain material, leading to unnecessary litigation and chilling effects. Zorro Productions, Inc. and Edgar Rice Burroughs, Inc. did this with the Zorro works and the Tarzan and John Carter works. Zorro lost in court—because the Zorro story was in the public domain, a new “Queen of Swords” TV series about Zorro’s sword-wielding daughter could proceed.<a href="#_ftn20" name="_ftnref20" title="">[20]</a> Burroughs was able to extract a joint licensing deal from the publisher of new “Lord of the Jungle” and “Warlord of Mars” comic books; even the threat of lawsuits can chill creative reuse. Going forward, will Disney’s legal actions reflect the relevant law, enforcing only the rights it still owns, or will it try to stop what copyright expiration allows?</p>
<p id="spacer"><a href="#top">(back to contents)</a></p>

<h3 id="green">
	<em>The Public Domain: A Wellspring of Creativity</em></h3>

<p>Think of all of the works spawned by public domain stories and characters, whether it is Shakespeare’s plays, Jane Austen’s novels, Mary Shelley’s <em>Frankenstein</em>, or Bram Stoker’s <em>Dracula</em>. Shakespeare alone has inspired hundreds of new creations. <em>10 Things I Hate About You</em> and <em>Kiss Me Kate</em>&nbsp;come from <em>The Taming of the Shrew</em>, <em>West Side Story</em> from <em>Romeo and Juliet</em>, <em>Forbidden Planet</em> from <em>The Tempest. </em>Shakespeare himself drew on his public domain predecessors. As a federal judge observed, if the underlying works were copyrighted, “<em>Measure for Measure</em> would infringe <em>Promos and Cassandra</em>, <em>Ragtime</em> would infringe <em>Michael Kohlhaas</em>, and <em>Romeo and Juliet</em> itself would have infringed Arthur Brooke’s <em>The Tragicall Historye of Romeo and Juliet</em> .&nbsp;.&nbsp;. which in turn would have infringed several earlier <em>Romeo and Juliets</em>, all of which probably would have infringed Ovid’s story of <em>Pyramus and Thisbe</em>.”</p>
<p>Disney and Mickey are part of this rich tradition, and in the end, Disney can continue to enjoy a plentiful intellectual property portfolio while also enriching the public domain. As mentioned earlier, Disney’s wonderful works exemplify just how valuable the public domain is. It has borrowed prolifically and brilliantly from the public domain, Disney-fying older works to make its beloved films: <em>The Three Muskateers</em> came from Alexandre Dumas, <em>A Christmas Carol</em> from Charles Dickens, <em>Beauty and the Beast</em> from Gabrielle-Suzanne de Villeneuve, <em>Around the World in 80 Days</em> from Jules Verne, <em>Alice in Wonderland</em> from Lewis Carroll, <em>Snow White</em> from The Brothers Grimm, <em>The Hunchback of Notre Dame</em> from Victor Hugo, <em>Sleeping Beauty</em> and <em>Cinderella</em> from Charles Perrault, <em>The Little Mermaid</em> from Hans Christian Anderson, <em>Pinocchio</em> from Carlo Collodi, <em>Huck Finn</em> from Mark Twain, <em>Robin Hood</em> from English folklore, and <em>Aladdin</em> from <em>The Book of One Thousand and One Nights</em>.</p>
<p>Let us hope that Disney remembers its own debt to the public domain as Mickey Mouse enters the realm from which it has drawn so heavily!</p>
<p id="spacer"><a href="#toc">(back to contents)</a></p>
<div>
	<br clear="all">
	<hr size="1">
	<p id="ftn"><a href="#_ftnref1" name="_ftn1" title="">[1]</a> <em>Steamboat Willie</em> was a technological marvel – one of the first cartoons featuring fully synchronized sound. After seeing the 1927 film <em>The Jazz Singer</em> (the first feature-length film with synchronized dialogue, <a href="https://web.law.duke.edu/cspd/publicdomainday/2023/"> highlighted</a> on last year’s Public Domain Day page) Walt Disney realized that sound films were “here to stay.” See Dave Smith, Chief Archivist Emeritus, The Walt Disney Company, <a href="https://www.loc.gov/static/programs/national-film-preservation-board/documents/steamboat_willie.pdf"><em>Steamboat Willie</em></a>. With animator Ub Iwerks, he created <em>Steamboat Willie</em>. Disney himself performed the (mostly unintelligible) dialogue. The cartoon was a hit with audiences and lauded by critics for its ingenuity. It was initially set to enter the public domain in 1984, after a 56-year term. An expansion in 1978 pushed the date forward to 2004. Then in 1998 it was delayed until 2024 by the Copyright Term Extension Act (this is what some people call the “Mickey Mouse Protection Act”). There is even a possibility that <em>Steamboat Willie</em> might technically have gone into the public domain on release because of defects in its original copyright notice, cite, but in 2024 we know for sure that it is free of copyright.</p>
	<p id="ftn"><a href="#_ftnref2" name="_ftn2" title="">[2]</a> Mickey’s first appearances were actually in earlier cartoons from 1928, <em>Plane Crazy</em> and <em>The Gallopin’ Gaucho</em>. Both were silent films and neither attracted a distributor. It was Steamboat Willie – which had synchronized sound – that put Mickey on the map. The silent version of <em>Plane Crazy</em> is also entering the public domain 2024, with the sound version going into the public domain in 2025. We could not find a 1928 registration or timely renewal for <em>The Gallopin’ Gaucho</em>, but the copyright notice on the sound version says 1929, putting it in the public domain in 2025.</p>
	<p id="ftn"><a href="#_ftnref3" name="_ftn3" title="">[3]</a> Many have observed that “The Lion King” tracks an epic poem about Sundiata Keitam – whose name means “Lion Prince” – the founder of the Mali Empire. <em>See </em>Kellie Carter Jackson, <a href="https://www.washingtonpost.com/outlook/2019/07/17/true-story-behind-lion-king/" target="_blank">The true story behind “The Lion King.”</a> This article also tells the story of the song “The Lion Sleeps Tonight,” which originated with South African Zulu singer Solomon Ntsele Linda; it is the subject of the Netflix documentary “ReMastered: The Lion’s Share.”</p>
	<p id="ftn"><a href="#_ftnref4" name="_ftn4" title="">[4]</a> As described by Alva Johnston, Mickey and Chaplin “have the same blend of hero and coward, nit-wit and genius, mug and gentleman.” Walt Disney also studied other great silent film actors whose films we have celebrated on this site, including Buster Keaton, Harold Lloyd, and Laurel and Hardy.</p>
	<p id="ftn"><a href="#_ftnref5" name="_ftn5" title="">[5]</a> <em>Steamboat Bill, Jr.</em> went into the public domain decades ago because its copyright was not renewed. The earlier Mickey cartoon from 1928, <em>The Gallopin’ Gaucho</em>, drew on the silent film <em>The Gaucho</em>, with Mickey echoing some of the titular character’s actions.</p>
	<p id="ftn"><a href="#_ftnref6" name="_ftn6" title="">[6]</a> We actually had a dress rehearsal on these questions in 2022 with Winnie-the-Pooh – who was then another newly public domain Disney character that reappeared in later, copyrighted works and was also covered by trademark rights – went into the public domain. You can read that analysis <a href="https://web.law.duke.edu/cspd/publicdomainday/2023/bcvpd/#fn2ref" target="_blank">here</a>.</p>
	<p id="ftn"><a href="#_ftnref7" name="_ftn7" title="">[7]</a> To quote the <a href="https://www.copyright.gov/circs/circ14.pdf" target="_blank">Copyright Office primer on derivative works</a>: “[I]t is not possible to extend the length of protection for a copyrighted work by creating a derivative work…the copyright in the derivative work will not extend to the public domain material, and the use of the public domain material in a derivative work will not prevent anyone else from using the same public domain work for another derivative work.”</p>
	<p id="ftn"><a href="#_ftnref8" name="_ftn8" title="">[8]</a> See the <a href="https://www.copyright.gov/comp3/chap300/ch300-copyrightable-authorship.pdf">Copyright Office Compendium</a>. Regarding generic character traits, courts have held that being “nice,” having a “cocky attitude,” and being “young, attractive, and sarcastic” are not independently copyrightable See <em>Shame on You Prods. v. Banks</em> (C.D. Cal. 2015, aff’d 9th Cir. 2017); <em>Campbell v. Walt Disney Co.</em> (N.D. Cal. 2010); <em>Gable v. Nat’l Broad. Co.</em> (C.D. Cal. 2010).</p>
	<p id="ftn"><a href="#_ftnref9" name="_ftn9" title="">[9]</a> 15 U.S.C. §§1114, 1125(a).</p>
	<p id="ftn"><a href="#_ftnref10" name="_ftn10" title="">[10]</a> A small subset of superbrands with “famous” marks gets extra protection against “dilution” that impairs the distinctiveness or harms the reputation of the famous mark, but this is subject to exceptions for expressive uses.</p>
	
	<p id="ftn"><a href="#_ftnref12" name="_ftn12" title="">[12]</a> <em>Dastar v. Twentieth Century Fox</em>, 539 U.S. 23 (2003) dealt with an attempt to make an attribution-like claim under the federal trademark statute over public domain works, but the larger policy underlying the decision is that trademark law cannot be used to circumvent copyright expiration. Dastar had repackaged parts of a public domain documentary, removing the original credits and presenting it as a Dastar production. Twentieth Century Fox owned the copyrights to the documentary before they expired and claimed that Dastar violated trademark law by passing off Fox’s work as its own and failing to attribute it to Fox. The Supreme Court rejected this claim.</p>
	<p id="ftn"><a href="#_ftnref13" name="_ftn13" title="">[13]</a> <em>Comedy III v. New Line</em>, 200 F.3d 593 (9th Cir. 2000).</p>
	<p id="ftn"><a href="#_ftnref14" name="_ftn14" title="">[14]</a> The McCarthy trademark treatise explains: “The expiration of copyright on a copyrighted work, such as a motion picture containing a cartoon character, should place limits on the scope of trademark rights in the character. Under the view that upon expiration of copyright the public should have free use of the work, copyright policy requires that anyone should be able to reproduce, display and perform the out-of-copyright motion picture so long as there is no confusion as to the source, sponsorship or affiliation of the seller of the reproduced film,” and “a court should not permit trademark in an image to serve the same function as did the lapsed copyright to exclude others from reproducing and distributing the out-of-copyright work, such as a film .&nbsp;.&nbsp;. a balancing of the rights of trademark and of the 'public domain' status of out-of-copyright works is needed. A similar balancing is made when constitutional free speech policies are balanced against the prevention of consumer confusion.” J. Thomas McCarthy, <em>McCarthy on Trademarks and Unfair Competition</em>, §&nbsp;6.30.</p>
	<p id="ftn"><a href="#_ftnref15" name="_ftn15" title="">[15]</a> <em>Maljack Prods. v. Goodtimes Home Video Corp</em>., 81 F.3d 881 (9th Cir. 1996), quoting Leslie A. Kurtz, <em>Protection for Titles of Literary Works in the Public Domain</em>, 37 Rutgers L.Rev. 53, 77 (1984). <em>See also Walt Disney Productions v. Souvaine Selective Pictures, Inc.</em> (2d Cir. 1951) (Disney could not prevent another film producer from using the title “Alice in Wonderland” because “the book ‘Alice in Wonderland’ is no longer subject to copyright and is as much in the public domain as are Shakespeare’s plays. Anyone has a legal right to make a picture based on Louis Carroll’s book and entitled ‘Alice in Wonderland.’”)</p>
	<p id="ftn"><a href="#_ftnref16" name="_ftn16" title="">[16]</a>&nbsp;<em>Mattel, Inc. v. Walking Mountain Productions</em>, 353 F.3d 792 (9th Cir. 2003);&nbsp;<em>Mattel case: Mattel, Inc. v. MCA Records</em>, Inc., 296 F.3d 894 (9th Cir. 2002)</p>
	<p id="ftn"><a href="#_ftnref17" name="_ftn17" title="">[17]</a> In a 2023 case the Supreme Court let the <em>Rogers </em>test stand for these kinds of uses. <em>Jack Daniel's Properties, Inc. v. VIP Products LLC</em>, 143 S. Ct. 1578 (2023)</p>
	<p id="ftn"><a href="#_ftnref18" name="_ftn18" title="">[18]</a> In order to claim copyright in newer versions of Mickey Mouse, Disney needs to argue that they are different from Mickey 1.0. Having said that, it cannot then argue that Mickey 1.0 is effectively the same as the more famous Mickey and therefore enjoys the same status.</p>
	<p id="ftn"><a href="#_ftnref19" name="_ftn19" title="">[19]</a> 15 U.S.C. §1125(c)(3).</p>
	<p id="ftn"><a href="#_ftnref20" name="_ftn20" title="">[20]</a> <em>See Sony v. Fireworks</em>, 137 F.Supp.2d 1177 (C.D. Cal. 2001) (the case also included unsuccessful copyright claims). You can read more about Zorro Productions, Inc.’s tactics in Stephen Carlisle’s “<a href="http://copyright.nova.edu/zorro/" target="_blank">Will the ‘Mark’ of Zorro Defeat Court's Public Domain Ruling?</a>”.</p>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tracking Java native memory with JDK flight recorder (115 pts)]]></title>
            <link>https://www.morling.dev/blog/tracking-java-native-memory-with-jdk-flight-recorder/</link>
            <guid>38677628</guid>
            <pubDate>Sun, 17 Dec 2023 23:49:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.morling.dev/blog/tracking-java-native-memory-with-jdk-flight-recorder/">https://www.morling.dev/blog/tracking-java-native-memory-with-jdk-flight-recorder/</a>, See on <a href="https://news.ycombinator.com/item?id=38677628">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>So let’s see how NMT data is reported via JFR.
Here’s a simple example program which allocates some off heap memory,
once using a good old <a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/nio/ByteBuffer.html#direct-vs-non-direct-buffers-heading">direct byte buffer</a>,
and once using the new <a href="https://openjdk.org/jeps/454">Foreign Memory API</a>,
finalized in Java 22 with JEP 454
(it feels <em>so</em> nice to be able to allocate 4GB at once, something you couldn’t do before):</p><div>
<pre><code data-lang="java"><table><tbody><tr><td><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td><pre><span>import</span> <span>java.nio.ByteBuffer</span><span>;</span>
<span>import</span> <span>java.lang.foreign.Arena</span><span>;</span>
<span>import</span> <span>java.lang.foreign.MemorySegment</span><span>;</span>

<span>import</span> <span>static</span> <span>java</span><span>.</span><span>time</span><span>.</span><span>LocalDateTime</span><span>.</span><span>now</span><span>;</span>

<span>public</span> <span>void</span> <span>main</span><span>()</span> <span>throws</span> <span>Exception</span> <span>{</span>
  <span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>STR</span><span>.</span><span>"\{ now() } Started"</span><span>);</span>
  <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>);</span>

  <span>ByteBuffer</span> <span>buffer</span> <span>=</span> <span>ByteBuffer</span><span>.</span><span>allocateDirect</span><span>(</span><span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span><span>);</span>
  <span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>STR</span><span>.</span><span>"\{ now() } Allocated (Direct)"</span><span>);</span>
  <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>);</span>

  <span>try</span> <span>(</span><span>Arena</span> <span>arena</span> <span>=</span> <span>Arena</span><span>.</span><span>ofConfined</span><span>())</span> <span>{</span>
    <span>MemorySegment</span> <span>segment</span> <span>=</span> <span>arena</span><span>.</span><span>allocate</span><span>(</span><span>4L</span> <span>*</span> <span>1024L</span> <span>*</span> <span>1024L</span> <span>*</span> <span>1024L</span><span>);</span>
    <span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>STR</span><span>.</span><span>"\{ now() } Allocated (FMI)"</span><span>);</span>
    <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>);</span>
  <span>}</span>

  <span>buffer</span> <span>=</span> <span>null</span><span>;</span>
  <span>System</span><span>.</span><span>out</span><span>.</span><span>println</span><span>(</span><span>STR</span><span>.</span><span>"\{ now() } Deallocated"</span><span>);</span>
  <span>Thread</span><span>.</span><span>sleep</span><span>(</span><span>5000</span><span>);</span>
<span>}</span>
</pre></td></tr></tbody></table></code></pre>
</div><p>JFR records NMT events every second by default,
so I’ve sprinkled in some <code>sleep()</code> calls to make sure the program runs long enough and the different allocations are spread out a bit.
Just for the fun of it, I’m also using a top-level main method—as supported by <a href="https://openjdk.org/jeps/463">JEP 463</a>—and string templates for the log messages (<a href="https://openjdk.org/jeps/459">JEP 459</a>).</p><p>Let’s run this and see how those off-heap allocations are tracked by JFR.
Somewhat surprisingly, NMT in JFR is controlled via the <code>gc</code> setting, which must be set to a value of "normal", "detailed", "high", or "all" for recording NMT data.
This is the case for the <em>default</em> and <em>profile</em> JFR configurations which ship with the SDK,
so using either configuration will give you the NMT data.
Note though that in addition, NMT itself must be enabled using the <code>-XX:NativeMemoryTracking</code> JVM option:</p><div>
<pre><code data-lang="bash"><table><tbody><tr><td><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
</pre></td><td><pre>java <span>--enable-preview</span> <span>--source</span> 22 <span>\</span>
  <span>-XX</span>:StartFlightRecording<span>=</span><span>name</span><span>=</span>Profiling,filename<span>=</span>nmt-recording.jfr,settings<span>=</span>profile <span>\</span>
  <span>-XX</span>:NativeMemoryTracking<span>=</span>detail main.java
<span>[</span>0.316s][info][jfr,startup] Started recording 1. No limit specified, using <span>maxsize</span><span>=</span>250MB as default.
<span>[</span>0.316s][info][jfr,startup]
<span>[</span>0.316s][info][jfr,startup] Use jcmd 47194 JFR.dump <span>name</span><span>=</span>Profiling to copy recording data to file.
2023-12-17T18:31:00.475598 Started
2023-12-17T18:31:05.609319 Allocated <span>(</span>Direct<span>)</span>
2023-12-17T18:31:11.167484 Allocated <span>(</span>FMI<span>)</span>
2023-12-17T18:31:16.253059 Deallocated
</pre></td></tr></tbody></table></code></pre>
</div><p>Let’s open the recording in JDK Mission Control and see what we find.
As of version 8.3, JMC doesn’t have a bespoke view for displaying NMT data,
but the NMT events show up in the generic event browser view.
There are two event types, the first one being <a href="https://sap.github.io/SapMachine/jfrevents/#nativememoryusagetotal">"Total Native Memory Usage"</a>:</p><p>The two off-heap allocations of 1 GB (direct byte buffer) and 4 GB (Foreign Memory API) show up as expected as increases to the reserved and committed memory of the program.
We also see one of the advantages of the new Foreign Memory API:
the memory is deallocated as soon as the <code>Arena</code> object is closed,
whereas the JVM holds on to the memory of the byte buffer also after discarding the reference.
There’s no control over when this memory will be released exactly,
it will be done via a <a href="https://stackoverflow.com/questions/36077641/java-when-does-direct-buffer-released">phantom-reference-based cleaner</a> some time after the GC has removed the associated buffer object.</p><p>The second new event type, <a href="https://sap.github.io/SapMachine/jfrevents/#nativememoryusage">"Native Memory Usage Per Type"</a>, provides a more fine grained view (when setting <code>-XX:NativeMemoryTracking</code> to <code>detail</code> rather than <code>summary</code>).
The off-heap allocations show up under the "Other" category there:</p><p><em>Update Dec 18:</em> As OpenJDK developer Eric Gahlin <a href="https://twitter.com/ErikGahlin/status/1736530559231201484">pointed out</a>,
you also can take a high-level view at the NMT events of a recording using the JDK’s <em>jfr</em> tool,
which provides two <a href="https://egahlin.github.io/2023/05/30/views.html">built-in views</a> for committed and reserved memory:</p><div>
<pre><code data-lang="bash"><table><tbody><tr><td><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td><pre><span>$JAVA_HOME</span>/bin/jfr view native-memory-committed recording.jfr

                            Native Memory Committed

Memory Type                    First Observed   Average Last Observed   Maximum
<span>------------------------------</span> <span>--------------</span> <span>---------</span> <span>-------------</span> <span>---------</span>
Other                                  1,8 MB    1,7 GB        1,0 GB    5,0 GB
Java Heap                            136,0 MB  136,0 MB      136,0 MB  136,0 MB
GC                                    54,2 MB   54,2 MB       54,2 MB   54,2 MB
Metaspace                             16,0 MB   16,0 MB       16,1 MB   16,1 MB
Tracing                               15,6 MB   15,7 MB       15,7 MB   15,7 MB
Code                                  12,6 MB   12,6 MB       12,6 MB   12,6 MB
Shared class space                    12,4 MB   12,4 MB       12,4 MB   12,4 MB
Arena Chunk                            8,5 MB    2,2 MB        2,0 kB    8,5 MB
Symbol                                 5,8 MB    5,8 MB        5,8 MB    5,8 MB
Class                                  2,7 MB    2,7 MB        2,7 MB    2,7 MB
Native Memory Tracking                 1,7 MB    1,7 MB        1,7 MB    1,7 MB
Synchronization                        1,2 MB    1,2 MB        1,2 MB    1,2 MB
Internal                             563,4 kB  561,9 kB      561,7 kB  563,4 kB
Compiler                             202,9 kB  206,4 kB      205,6 kB  238,5 kB
Module                               174,1 kB  174,1 kB      174,1 kB  174,1 kB
Thread                                86,0 kB   82,5 kB       81,4 kB   86,0 kB
Safepoint                             32,0 kB   32,0 kB       32,0 kB   32,0 kB
GCCardSet                             29,5 kB   29,5 kB       29,5 kB   29,5 kB
Serviceability                        17,6 kB   17,6 kB       17,6 kB   17,6 kB
Object Monitors                        1,0 kB    1,0 kB        1,0 kB    1,0 kB
String Deduplication                608 bytes 608 bytes     608 bytes 608 bytes
Arguments                           185 bytes 185 bytes     185 bytes 185 bytes
Statistics                          128 bytes 128 bytes     128 bytes 128 bytes
Logging                              32 bytes  32 bytes      32 bytes  32 bytes
Test                                  0 bytes   0 bytes       0 bytes   0 bytes
JVMCI                                 0 bytes   0 bytes       0 bytes   0 bytes
Thread Stack                          0 bytes   0 bytes       0 bytes   0 bytes
</pre></td></tr></tbody></table></code></pre>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qatar Airways Bans YouTuber for Negative Review (220 pts)]]></title>
            <link>https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/</link>
            <guid>38677344</guid>
            <pubDate>Sun, 17 Dec 2023 23:08:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/">https://onemileatatime.com/news/qatar-airways-bans-youtuber-negative-review/</a>, See on <a href="https://news.ycombinator.com/item?id=38677344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>Qatar Airways has reportedly banned a YouTuber after he posted a negative review of a flight, and also fired the crew. The carrier’s approach here simply defies logic…</p>
<div id="ez-toc-container">

<nav><ul><li><a href="#qatar_airways_bans_josh_cahill_over_unfavorable_review" title="Qatar Airways bans Josh Cahill over unfavorable review">Qatar Airways bans Josh Cahill over unfavorable review</a></li><li><a href="#my_take_on_this_qatar_airways_situation" title="My take on this Qatar Airways situation">My take on this Qatar Airways situation</a></li><li><a href="#bottom_line" title="Bottom line">Bottom line</a></li></ul></nav></div>
<h2 id="h-qatar-airways-bans-josh-cahill-over-unfavorable-review"><span id="qatar_airways_bans_josh_cahill_over_unfavorable_review"></span>Qatar Airways bans Josh Cahill over unfavorable review<span></span></h2>
<p>Josh Cahill is a popular YouTuber who reviews flights, and on Saturday he published a roughly 20-minute video about how he was “banned and bribed” by Qatar Airways. This followed an August 2023 video review he published, entitled “the shocking decline of Qatar Airways,” about an economy class flight that he took from Colombo to Doha.</p>
<p>Let me try to summarize what happened as briefly but accurately as possible, according to Josh:</p>
<ul>
<li>Days after he published the video review, Qatar Airways reached out to ask him to hop on a call, which he agreed to, and that included the company’s VP of corporate communications</li>
<li>Once on the call, the Qatar Airways representatives immediately said that whatever is talked about is off the record, which Josh didn’t agree to; he was then asked what his motive was for such a negative review</li>
<li>They said the video reflects negatively on the airline, and they offered Josh a free flight if he’s willing to delete the video, which he rejected</li>
<li>They then asked Josh to delete the negative comments on the video from Qatar Airways employees complaining about poor working conditions at the airline, which he also refused to do</li>
<li>A week later, he received an email informing him that an upcoming booking on Qatar Airways had been canceled, and that the airline can no longer permit additional bookings from him</li>
<li>The email references <a href="https://www.qatarairways.com/en-us/legal/conditions-of-carriage.html">articles 8 &amp; 12 of the contract of carriage</a>, which require permission in order to film on an aircraft, though that was never otherwise enforced for him or any other YouTubers publishing positive reviews</li>
<li>A short time later, Josh received an email from one of the flight attendants who was working his flight, acknowledging that “I know I gave you permission to film the interaction, but I must ask you to remove me from the video,” because “I was pressured to write a statement that I didn’t give you permission to be filmed,” and “if I don’t write it, I will most likely be dismissed”</li>
<li>Since Josh wasn’t willing to voluntarily take down the video, Qatar Airways took all of these statements from crew and sent them to YouTube, demanding that the video be removed for privacy reasons </li>
<li>For those with an IP address in Qatar, Josh’s initial flight review has apparently been blocked, so that you can no longer watch it</li>
<li>While Josh can’t personally vouch for this, he claims that many sources have told him that the entire crew was terminated after the airline wasn’t able to remove the video</li>
</ul>
<p>You can watch the video for yourself below…</p>
<figure></figure>
<h2 id="h-my-take-on-this-qatar-airways-situation"><span id="my_take_on_this_qatar_airways_situation"></span>My take on this Qatar Airways situation<span></span></h2>
<p>It’s kind of unbelievable how much an airline can get in its own way. I mean, my gosh, how much more poorly could Qatar Airways have handled this? The unprofessionalism and short-sightedness here is astounding. </p>
<p>The best thing Qatar Airways could have done is to either issue an apology for the poor experience or to just ignore the video. Frankly, the video wasn’t even that negative, though Josh does tend to have rather dramatic video titles.</p>
<p>All Qatar Airways has done with its response is to draw way more attention to what happened, which seems to be the opposite of what the goal was. I mean, when he rejected the free flight and said he wasn’t willing to speak off the record, didn’t they get a sense that he would make a video about it? And did they think they would look good in it?</p>

<p>And let’s not even talk about the concept of forcing an employee to email Josh to literally admit that they were being told to lie in order to save their jobs. Some people should be fired here for incompetence, but it shouldn’t be the crew of the flight…</p>
<p>I understand in a country like Qatar, it’s easy enough to control the narrative of things, and force people to do what you’d like, if you have enough money or power. However, that approach doesn’t work when you’re trying to run a global airline. And while it’s absolutely within the carrier’s rights to ban passengers for whatever reason, it’s not a great look if you’re trying to make the airline or country have wide appeal.</p>
<p>Now, I think there’s one very important point to acknowledge, and it kind of sheds some light on how this all played out. This happened while Akbar Al Baker was still CEO of Qatar Airways, as he <a href="https://onemileatatime.com/news/qatar-airways-ceo-akbar-al-baker-resigns/" target="_blank" rel="noopener">suddenly resigned several weeks ago</a>. He was known for creating a culture of fear, and where so many decisions were driven by what “the chief” wanted, rather than actually using logic.</p>
<p>So it’s easy enough to figure out exactly how this happened. Al Baker was somehow made aware of this video, and told his minions to have the video removed. Then everyone did everything they could to get that result, rather than pointing out to him that this might not be a good idea.</p>
<p>But there’s some good news. Qatar Airways’ new CEO,&nbsp;Badr Mohammed Al Meer, has <a href="https://onemileatatime.com/news/new-qatar-airways-ceo/" target="_blank" rel="noopener">shared some refreshing new priorities</a> for the airline, including stating that creating “a culture of trust and empowerment will be the building blocks of our shared success.” </p>
<p>So far there’s actually some substance to this, as he has <a href="https://onemileatatime.com/news/qatar-airways-ends-employee-curfew/" target="_blank" rel="noopener">ended the controversial employee curfew</a> that existed for years within weeks of starting his job. Hopefully this is the first of many changes. I think it would be really smart of him to undo this and apologize for the way that Qatar Airways handled this.</p>
<figure><img fetchpriority="high" decoding="async" width="1200" height="823" src="https://cdn.onemileatatime.com/wp-content/uploads/2023/05/Doha-Hamad-Airport-Teddy-Bear.jpg" alt=""><figcaption>Qatar Airways took a short-sighted approach here</figcaption></figure>
<h2 id="h-bottom-line"><span id="bottom_line"></span>Bottom line<span></span></h2>
<p>Qatar Airways has banned a popular YouTuber after he posted a negative review of a flight back in August. The airline first tried to get him to remove the video by offering him a free flight, then had the crew email him to revoke permission to appear in the video, and then approached YouTube directly. </p>
<p>This happened under the Al Baker “regime,” so here’s to hoping under Qatar Airways’ new CEO, things change a bit… </p>
<p><strong>What do you make of this Qatar Airways and Josh Cahill situation?</strong></p>





 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[“Yes” means “no”: The language of VCs (224 pts)]]></title>
            <link>https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious</link>
            <guid>38677251</guid>
            <pubDate>Sun, 17 Dec 2023 22:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious">https://jacobbartlett.substack.com/p/yes-actually-means-no-the-curious</a>, See on <a href="https://news.ycombinator.com/item?id=38677251">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Everyone wants investment from venture capitalists.</p><p>It’s the validation that catapults you onto the cover of Forbes. Your seat at the pantheon of famed tech bros like Adam Neumann, Elizabeth Holmes, and Sam Bankman-Fried. </p><p>VCs can be a curious bunch, however. They have a special secret language — a dialect that’s borderline incomprehensible to the uninitiated.</p><p>For instance, much like the Inuit people, who have many words for snow, were you aware that venture capitalists have more than 400 words for “no”?</p><p><span>Language betrays purpose. In Orwell’s 1984, </span><em>Newspeak</em><span> was created to narrow the range of thought and, ultimately, make dissidence impossible. In the same vein, </span><em>VCspeak</em><span> has evolved to serve one key tenet:</span></p><p><em><span>“Never say no to a founder, </span><strong>just in case</strong><span>.”</span></em></p><blockquote><p><em><span>If you don’t feel like subscribing, please </span><a href="https://twitter.com/jacobs_handle" rel="">follow me on X/Twitter</a><span> if you like this post!</span></em></p></blockquote><p>VCs have no incentive to sour their relationship with you. What if you become the next Zuck and remember the deal associate who let you down gently?</p><p><span>You’ll never hear the true reason for turning you down </span><em>this time</em><span>. For a pre-seed or seed round, this is either:</span></p><ol><li><p>They don’t believe in the management team.</p></li><li><p>They don’t believe in the market opportunity.</p></li></ol><p><span>Today, I’m telling the story of my multitude of VC rejections. I’ll impart my hard-earned knowledge of </span><em>VCspeak</em><span> and explain what VCs are saying when, really, they just mean “no”. Finally, I’ll close with my top 10 survival tips for when you’re out fundraising.</span></p><p><span>My first real startup, Fixr — think Uber for car repairs — was a mess from the start. When I joined up as ‘CTO’, there were endemic power struggles between the existing founders and a chronic case of “if we build one more feature, </span><em>then</em><span> the users will flock to us”.</span></p><p>Through a Herculean effort of LinkedIn networking, we set up some calls with very junior VCs. None were brave enough to rip off the poisoned shirt we had woven for ourselves:</p><p><em>“Talk to us again when you have traction.”</em></p><p><span>In our first-time founder naïveté, we took this to heart. Our startup lived or died on whether we had </span><em>traction</em><span>.</span></p><p>Screw talking to users. Screw validation. We needed to get our product out of stealth mode and into the open. First, we’d take the market by storm. Then, we’d hoover up those promised VC dollars.</p><p><span>For our two-sided marketplace startup, this meant Android and iOS apps for customers, and </span><em>another pair of apps</em><span> for mechanics. 6 months later, we released our fully-fledged MVP to a deafening silence from customers, mechanics, and investors.</span></p><p>The junior VCs didn’t speak to us again.</p><p>My second startup fared far better.</p><p>Carbn was an app to help people develop green habits. It was a marked improvement — we were only building the iOS app and barrelled from customer validation to MVP in our first 4 months!</p><p>From here, we had a few months of bootstrapping left in us and were desperate for our first capital injection. Unfortunately, we heard another classic refrain from the venture capitalists:</p><p><em>“We think you’re too early for us, speak to us when you raise your next round.”</em></p><p>Admittedly, that was fair when we had zero revenue and low-thousands of users. We weren’t fazed — we approached various angel syndicates, accelerators and exhausted all the goodwill we could muster via our networks.</p><p><span>Through the complementary magic of a </span><a href="https://apps.apple.com/app/carbn-cut-carbon-footprint/id1533681322" rel="">strong early app store rating</a><span> and my socially hypercompetent cofounder, we landed </span><a href="https://www.crunchbase.com/organization/carbn" rel="">a cool £200k in angel funding</a><span>.</span></p><p>With a business bank account brimming with angel funding, we went on a hiring spree and maintained a consistent 10% week-on-week DAU growth — nothing to sniff at. But was it enough?</p><p>We were wrestling with a critical strategic issue: Should we remain a B2C consumer habits app or position ourselves in the B2B space as a climate-conscious employee perk? Having dipped our toes into both approaches, we’d hit our pre-seed growth targets and began to reach out to VCs for our next round.</p><p><span>We encountered the customary mix of ghosting, half-arsed lukewarm interest and genuine leads. This time, as well as the classic </span><em>“come to us with more traction”</em><span> and </span><em>“talk to us in your next round,”</em><span> we had some bigger fish biting for a second meeting.</span></p><p><span>We’d been speaking with a partner at a top-tier Silicon Valley VC. He was enthusiastic about Carbn, and asked us to present at their Monday Partner Meeting. Our presentation and subsequent conversation lasted over 2 hours. They told us they saw us becoming </span><em>the</em><span> climate B2C play. We felt it went well.</span></p><p>February 2022.</p><p>I was suffering the worst norovirus of my life. I’d been playing Elden Ring for 10 hours straight. I couldn’t tell whether I was dying more in-game or in real life.</p><p>We got the call from the VC.</p><p><em>“We like you, we want to invest, you should start hiring now.”</em></p><p>I was in too much pain to feel any satisfaction. Surely this was the VC funding I’d been waiting for my whole life? Through my fever delirium, I saw myself on the cover of Forbes, high-fiving the Fyre Festival guy.</p><p><span>My cofounder and I switched gears </span><em>hard</em><span> to focus on landing top-tier talent for our nascent engineering, product, and growth departments. In the meantime, we sent draft terms to our VC and… crickets.</span></p><p><span>The markets were spooked by some </span><a href="https://www.bbc.co.uk/news/live/world-europe-60454795" rel="">geopolitical shenanigans</a><span>, and VCs were suddenly hesitant to dole out cash at 2021 valuations. As our other leads ran cold, we held onto the hope that our deal was still in the bag.</span></p><p>Cue months of umming and ahhing from our friend in Silicon Valley. After their white-hot conviction about the B2C opportunity for a climate action app, our VC developed a newfound passion for the comfortable cashflow prospects of a B2B proposition.</p><p>Which we would have been fine with had the markets not been deteriorating further, and our borderline deal was consequently losing its lustre.</p><p>It eventually became clear that our seed round wasn’t happening. My cofounder and I agreed to call it a day.</p><p><span>Let’s delve deeper into </span><em>VCspeak</em><span>. We’ll analyze the VC rejections I’ve encountered and try to find the true meaning behind the words.</span></p><blockquote><p><strong>When they say:</strong><em><strong> </strong><span>“Talk to us again when you have traction.”</span></em></p><p><strong>They really mean: </strong><em>“If you prove there is a market opportunity and that you can execute as a management team, then we might consider you. But because I don’t believe either of those things will happen, I will not be taking a risk on you”.</em></p></blockquote><p>Passing on Fixr was the correct choice. In retrospect, there was no chance our management team was capable of successfully creating a two-sided marketplace, which is widely regarded as the hardest kind of startup to build. Don’t get me started on the abysmal decision to build 4 native apps on 2 platforms before we had a single  transaction.</p><blockquote><p><strong>When they say: </strong><em>“We think you’re too early for us, speak to us when you raise your next round.”</em></p><p><strong>They really mean: </strong><em>“We’re an early-stage fund, but generally speaking, we only invest pre-seed in second-time founders and people with executive-level industry experience in this sector. We like the opportunity but don’t believe your management team is the right one in which to invest.”</em></p></blockquote><p>Again, this is a pretty reasonable stance. While VC is inherently risky, these risks can be mitigated by backing management teams with a proven track record. Naturally, the riskiest pre-seed dollars are allocated accordingly to increase the chances of achieving a 3x ‘venture rate of return’ for a given VC fund.</p><blockquote><p><strong>When they say: </strong><em>“We like you, we want to invest, you should start hiring now.”</em></p><p><strong>They really mean: </strong><em>“We do really like your vibe as a founding team. We see your potential. We do genuinely want to invest, and want to set you up to move quickly once the deal closes. Please note however that this is neither a term sheet or a wire transfer. Perhaps we want more time to evaluate other options in the space. We lack conviction.”</em></p></blockquote><p>Venture capitalists are a famously fickle bunch. One day, they can be keen as a bean on your lean, mean, consumer business. The next day, Putin invades Ukraine and you’re a borderline B2B prospect that needs to show unit economics.</p><p>My story is typical of my fellow failed founders — some gaffes in our early ventures as we learn the ropes, a few promising steps as we gain entrepreneurial experience, and a veritable funding brick wall in 2022.</p><p>Here are some quick and dirty thoughts to keep your head screwed on right as you look for startup funding:</p><ol><li><p>If a VC gives a reason for turning you down, don’t necessarily take it at face value.</p></li><li><p>Be self-critical about your management and the market. Your team or the opportunity might just not be right.</p></li><li><p><span>For your first round, it’s </span><em>much</em><span> easier to convince a single angel investor than a roomful of VC partners.</span></p></li><li><p>When you land a lead angel investor, the rest become easier to find.</p></li><li><p>If you aren’t a big dog in your industry or haven’t had a startup exit before, the bar for pre-seed VC investment is pretty high.</p></li><li><p>In a VC firm, you need an influential internal champion who will fight hard for your startup.</p></li><li><p>Don’t play coy with VCs. Don’t stop talking to them, and don’t let a term sheet get away before the market turns.</p></li><li><p>“Yes” doesn’t always mean “yes”. Your raise isn’t over until a wire transfer hits your account.</p></li><li><p>VC funding is cool and all, but essentially, it is a waste of time if you haven’t validated your market: bootstrapping to get paying customers quickly is the most effective tactic available.</p></li><li><p><span>In all honesty, you don’t </span><em>want </em><span>to take external funding unless you have intensely strong user growth; and being unable to hire becomes your bottleneck.</span></p></li></ol><p><span>Have you encountered any rejections we could add to our </span><em>VCspeak</em><span> phrasebook? Do you have any hard-earned wisdom to append to the survival guide? Let us know in the comments!</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[0% of the phrases of the original Wikipedia "Ship of Theseus" article remain (437 pts)]]></title>
            <link>https://twitter.com/depthsofwiki/status/1735800801455419697</link>
            <guid>38677124</guid>
            <pubDate>Sun, 17 Dec 2023 22:32:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/depthsofwiki/status/1735800801455419697">https://twitter.com/depthsofwiki/status/1735800801455419697</a>, See on <a href="https://news.ycombinator.com/item?id=38677124">Hacker News</a></p>
Couldn't get https://twitter.com/depthsofwiki/status/1735800801455419697: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Fsearch, a fast file search utility for Unix-like systems (181 pts)]]></title>
            <link>https://github.com/cboxdoerfer/fsearch</link>
            <guid>38676563</guid>
            <pubDate>Sun, 17 Dec 2023 21:19:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cboxdoerfer/fsearch">https://github.com/cboxdoerfer/fsearch</a>, See on <a href="https://news.ycombinator.com/item?id=38676563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/cboxdoerfer/fsearch/actions/workflows/build_test.yml/badge.svg"><img src="https://github.com/cboxdoerfer/fsearch/actions/workflows/build_test.yml/badge.svg" alt="Build Status"></a>
<a href="https://hosted.weblate.org/engage/fsearch/?utm_source=widget" rel="nofollow"><img src="https://camo.githubusercontent.com/4090cea624534ea7440489a8805a048301c139a020d470ce5c24e3fa4ce7c81e/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f667365617263682f2d2f7376672d62616467652e737667" alt="Translation status" data-canonical-src="https://hosted.weblate.org/widgets/fsearch/-/svg-badge.svg"></a></p>
<p dir="auto">FSearch is a fast file search utility, inspired by Everything Search Engine. It's written in C and based on GTK3.</p>
<ul dir="auto">
<li>For bug reports and feature requests please use the issue tracker: <a href="https://github.com/cboxdoerfer/fsearch/issues">https://github.com/cboxdoerfer/fsearch/issues</a></li>
<li>For discussions and questions about FSearch use the discussion
forum: <a href="https://github.com/cboxdoerfer/fsearch/discussions">https://github.com/cboxdoerfer/fsearch/discussions</a></li>
<li>For everything else related to FSearch you can talk to me on Matrix: <a href="https://matrix.to/#/#fsearch:matrix.org" rel="nofollow">https://matrix.to/#/#fsearch:matrix.org</a></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/02-main_window_menubar.png"><img src="https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/02-main_window_menubar.png" alt=""></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/01-main_window_headerbar.png"><img src="https://raw.githubusercontent.com/cboxdoerfer/fsearch/master/data/screenshots/01-main_window_headerbar.png" alt=""></a></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li>Instant (as you type) results</li>
<li><a href="https://github.com/cboxdoerfer/fsearch/wiki/Search-syntax">Advanced search syntax</a></li>
<li>Wildcard support</li>
<li>RegEx support</li>
<li>Filter support (only search for files, folders or everything)</li>
<li>Include and exclude specific folders to be indexed</li>
<li>Ability to exclude certain files/folders from index using wildcard expressions</li>
<li>Fast sort by filename, path, size or modification time</li>
<li>Customizable interface (e.g., switch between traditional UI with menubar and client-side decorations)</li>
</ul>
<h2 tabindex="-1" dir="auto">Requirements</h2>
<ul dir="auto">
<li>GTK 3.18</li>
<li>GLib 2.50</li>
<li>glibc 2.19 or musl 1.1.15 (other C standard libraries might work too, those are just the ones I verified)</li>
<li>PCRE2 (libpcre2)</li>
<li>ICU 3.8</li>
</ul>
<h2 tabindex="-1" dir="auto">Download</h2>
<p dir="auto">It is recommended to install FSearch from one of the <strong>Stable</strong> packages, unless you know what you're doing.</p>
<p dir="auto">The <strong>Development</strong> packages are primarily intended for testing and adventurous users.</p>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>Stable</th>
<th>Development</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ubuntu</td>
<td><a href="https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-stable" rel="nofollow">PPA Stable</a></td>
<td><a href="https://launchpad.net/~christian-boxdoerfer/+archive/ubuntu/fsearch-daily" rel="nofollow">PPA Daily</a></td>
</tr>
<tr>
<td>Arch Linux</td>
<td><a href="https://aur.archlinux.org/packages/fsearch/" rel="nofollow">AUR</a></td>
<td><a href="https://aur.archlinux.org/packages/fsearch-git/" rel="nofollow">AUR (git)</a></td>
</tr>
<tr>
<td>Fedora/RHEL/CentOS</td>
<td><a href="https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch/" rel="nofollow">COPR Stable</a></td>
<td><a href="https://copr.fedorainfracloud.org/coprs/cboxdoerfer/fsearch_nightly/" rel="nofollow">COPR Nightly</a></td>
</tr>
<tr>
<td>Debian</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Acboxdoerfer&amp;package=fsearch#manualDebian" rel="nofollow">OpenBuildService</a></td>
<td></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://software.opensuse.org//download.html?project=home%3Acboxdoerfer&amp;package=fsearch#manualopenSUSE" rel="nofollow">OpenBuildService</a></td>
<td></td>
</tr>
<tr>
<td>Flatpak (<a href="https://github.com/cboxdoerfer/fsearch/wiki/Flatpak-version-limitations">limited features</a>)</td>
<td><a href="https://flathub.org/apps/details/io.github.cboxdoerfer.FSearch" rel="nofollow">Flathub</a></td>
<td></td>
</tr>
<tr>
<td>Solus*</td>
<td><a href="https://dev.getsol.us/source/fsearch/" rel="nofollow">Solus Repository</a></td>
<td></td>
</tr>
<tr>
<td>FreeBSD*</td>
<td><a href="https://www.freshports.org/sysutils/fsearch" rel="nofollow">FreshPorts</a></td>
<td></td>
</tr>
</tbody>
</table>
<p dir="auto">(*) Not maintained by me</p>
<h2 tabindex="-1" dir="auto">Roadmap</h2>
<p dir="auto"><a href="https://github.com/cboxdoerfer/fsearch/wiki/Roadmap">https://github.com/cboxdoerfer/fsearch/wiki/Roadmap</a></p>
<h2 tabindex="-1" dir="auto">Build Instructions</h2>
<p dir="auto"><a href="https://github.com/cboxdoerfer/fsearch/wiki/Build-instructions">https://github.com/cboxdoerfer/fsearch/wiki/Build-instructions</a></p>
<h2 tabindex="-1" dir="auto">Localization</h2>
<p dir="auto">The localization of FSearch is managed with Weblate.</p>
<p dir="auto"><a href="https://hosted.weblate.org/projects/fsearch/" rel="nofollow">https://hosted.weblate.org/projects/fsearch/</a></p>
<p dir="auto">If you want to contribute translations please submit them there, instead of opening pull requests on GitHub. This also includes any suggestions to the English texts — English isn't my first language, so there are likely errors and unusual wordings.</p>
<p dir="auto">Instructions can be found here:
<a href="https://docs.weblate.org/en/latest/user/basic.html" rel="nofollow">https://docs.weblate.org/en/latest/user/basic.html</a></p>
<p dir="auto">And of course: Thank you for taking the time to translate FSearch!</p>
<h2 tabindex="-1" dir="auto">Current Limitations</h2>
<ul dir="auto">
<li>Sorting lots of results by <em>Type</em> can be very slow, since gathering that information is expensive, and the data isn't
indexed. This also means that when the view is sorted by <em>Type</em>, searching will reset the sort order to <em>Name</em>.</li>
<li>Using the <em>Move to Trash</em> option doesn't update the database index, so trashed files/folders show up in the result
list as if nothing happened to them.</li>
</ul>
<h2 tabindex="-1" dir="auto">Why yet another search utility?</h2>
<p dir="auto">Performance. On Windows I really like to use Everything Search Engine. It provides instant results as you type for all
your files and lots of useful features (regex, filters, bookmarks, ...). On Linux I couldn't find anything that's even
remotely as fast and powerful.</p>
<p dir="auto">Before I started working on FSearch, I took a look at existing solutions. I tried MATE Search Tool (formerly GNOME
Search Tool), Recoll, Krusader (locate based search), SpaceFM File Search, Nautilus, ANGRYsearch and Catfish, to find
out whether it makes sense to improve those. However, they're not exactly what I was looking for:</p>
<ul dir="auto">
<li>standalone application (not part of a file manager)</li>
<li>written in a language with C like performance</li>
<li>no dependencies to any specific desktop environment</li>
<li>Qt5 or GTK3 based</li>
<li>small memory usage (both hard drive and RAM)</li>
<li>target audience: advanced users</li>
</ul>
<h2 tabindex="-1" dir="auto">Looking for a command line interface?</h2>
<p dir="auto">I highly recommend <a href="https://github.com/junegunn/fzf">fzf</a> or the obvious tools: find and (m)locate</p>
<h2 tabindex="-1" dir="auto">Why GTK3 and not Qt5?</h2>
<p dir="auto">I like both of them, and my long term goal is to provide console, GTK3 and Qt5 interfaces, or at least make it easy for
others to build those. However, for the time being it's only GTK3 because I like C more than C++, and I'm more familiar
with GTK development.</p>
<h2 tabindex="-1" dir="auto">Questions?</h2>
<p dir="auto">Email: christian.boxdoerfer[AT]posteo.de</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Africans are changing French, one joke, rap and book at a time (196 pts)]]></title>
            <link>https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html</link>
            <guid>38675836</guid>
            <pubDate>Sun, 17 Dec 2023 19:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html">https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html</a>, See on <a href="https://news.ycombinator.com/item?id=38675836">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2023/12/12/world/africa/africa-french-language.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[3M knew its chemicals were harmful decades ago, but didn't tell the public (433 pts)]]></title>
            <link>https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</link>
            <guid>38675616</guid>
            <pubDate>Sun, 17 Dec 2023 19:30:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/">https://minnesotareformer.com/2022/12/15/toxic-3m-knew-its-chemicals-were-harmful-decades-ago-but-didnt-tell-the-public-government/</a>, See on <a href="https://news.ycombinator.com/item?id=38675616">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="dataContent">
                                    <p><em>This is part 2 of 2. <a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">Read part 1</a>, about East Metro residents who wonder if 3M chemicals made them sick.&nbsp;</em></p>
<p><span>3M toxicologist Richard Purdy did a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1523.pdf"> <span>study</span></a><span> in 1998 to see whether any of the company’s perfluorochemicals showed up in the blood of eagles and albatrosses.</span></p>
<p><span>That seemed unlikely, given the birds’ diet consists mostly of fish. So Purdy was surprised and disturbed when he found levels in their blood similar to those found in human blood. It even showed up in bald eagle nestlings whose only food was fish their parents fed them from remote lakes.</span></p>
<p><span>That indicated what Purdy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>later called</span></a><span> “widespread environmental contamination” — the likelihood the manmade, toxic chemicals were moving through the food chain and accumulating in animals.</span></p>
<p><span>Purdy warned 3M that if wild birds’ blood contained the chemicals, then fish-eating mammals — like otters, mink, porpoise and seals —&nbsp;could have it, too. A study of rats found they had significant levels of a 3M chemical in their livers, likely from eating fishmeal.&nbsp;</span></p>
<p><span>He told company officials in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA.</span></p>
<p><span>In response, 3M managers dispersed the team collecting the data, Purdy alleged.</span></p>
<p><span>Purdy resigned in 1999 and sent his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> to the EPA, informing them that while 3M had disclosed to the EPA that a chemical called PFOS “had been found in the blood of animals,” it didn’t mention that it was found in the blood of eaglets.</span></p>
<p><span>The EPA began investigating the chemicals that year. But by then, 3M had reaped billions of dollars in profits from chemicals that the company had been warned were harming the environment and risking human health.&nbsp;</span></p>
<p><span>The per-and polyfluoroalkyl substances (PFAS) had spread — through groundwater and products like Scotchgard stain repellent,</span> <span>Teflon cookware, food wrapping and fire retardant — and were showing up in the blood of people and animals in every corner of the world. They were in nearly every living thing, from house dust to human blood, in wildlife in the Arctic circle and drinking water, rivers, streams and breast milk.&nbsp;</span></p>
<p><span>Purdy’s warnings were clear, as revealed by former Attorney General Attorney General Lori Swanson, who sued 3M in 2010, alleging the company failed for decades to report that its chemicals could be toxic to humans, animals and the environment, keeping information from regulators and scientists to protect its lucrative revenue stream.&nbsp;</span></p>
<p><span>The morning the case was set to go to trial in 2018, after 22 hours of negotiation, 3M and the state settled. 3M agreed to pay $850 million to help provide Minnesotans clean drinking water.&nbsp;</span></p>
<p><span>The settlement with Minnesota is the third largest natural resource damage settlement in U.S. history, behind the Deepwater Horizon and Exxon Valdez oil spills.&nbsp;</span></p>
<p><span>But it amounted to just 2.6% of 3M’s nearly $33 billion in revenue in 2018.&nbsp;</span></p>
<p><span>The company admitted nothing, and maintains to this day that its chemicals have no adverse health or environmental consequences.&nbsp;</span></p>
<p><span>3M spokesman Grant Thompson said in an email that 3M’s position reflects the weight of&nbsp; </span><span>scientific evidence from decades of research showing exposure to PFOA and PFOS at current and historical levels found in people and the environment has not been shown to cause adverse health effects.</span></p>
<blockquote data-secret="Vvx1EzNtcF"><p><a target="_blank" href="https://minnesotareformer.com/2022/12/14/there-must-be-something-in-the-water/">There must be something in the water</a></p></blockquote>

<p><span>Still, 3M’s settlement with the state of Minnesota is likely the beginning — not the end — of the company’s legal, regulatory and political challenges stemming from both the invention and dumping of the chemicals. 3M and other companies that made the chemicals may have to pay out billions for the damage they caused the environment and people.&nbsp;</span></p>
<p><span>During a 2019 congressional hearing, U.S. Rep. Harley Rouda of California</span><a target="_blank" href="https://www.govinfo.gov/content/pkg/CHRG-116hhrg37952/html/CHRG-116hhrg37952.htm"> <span>called</span></a><span> the contamination of Americans’ drinking water, groundwater, air and food supplies a national emergency.</span></p>
<p><span>“These companies got away with poisoning people for more than a half century,” Rouda said.</span></p>
<p><span>In August, the EPA proposed designating two perfluorochemicals as hazardous substances under the Superfund law, which would spark federal cleanup standards and could put chemical companies on the hook for billions in cleanup costs.&nbsp;</span></p>
<p><span>The EPA also published new drinking water health advisory levels for several perfluorochemical compounds and plans to propose a national drinking water perfluorochemical regulation soon.</span></p>
<p><span>A federal judge in Charleston, S.C., also dealt the company a blow in September, </span><a target="_blank" href="https://news.bloomberglaw.com/environment-and-energy/3m-loses-government-contractor-defense-in-pfas-litigation"><span>denying</span></a><span> 3M’s request for government contractor immunity in a mass tort case alleging 3M and other companies’ firefighting foam are linked to health problems.</span></p>
<p><span>Judge Richard ​​Gergel said 3M conducted over 1,000 studies of perfluorochemicals’ effect on human health and the environment, the results of which should have been disclosed to the EPA.&nbsp;</span></p>
<p><span>He wrote that 3M and other chemical manufacturers “had significantly greater knowledge than the government about the properties and risks associated with their products and knowingly withheld highly material information from the government.”</span></p>
<p><span>Closer to 3M’s Minnesota headquarters, some sickened residents in the East Metro — where groundwater was contaminated with 3M chemicals — say they’re working with attorneys on a lawsuit.&nbsp;</span></p>
<p><span>David Sunding, a University of California Berkeley professor, published a 2017 report saying Washington County residents who lived in areas where groundwater was contaminated with 3M chemicals had elevated rates of bladder, breast, kidney and prostate cancers, as well as leukemia and non-Hodgkin’s lymphoma.</span></p>
<p><span>3M disputes that, pointing to a 2018 Minnesota health department </span><a target="_blank" href="https://www.health.state.mn.us/data/mcrs/docs/rpteastmetro.pdf"><span>report</span></a><span> showing that the overall cancer rate in Washington County was “virtually identical” to the statewide average, despite chemical contamination.&nbsp;&nbsp;</span></p>
<p><span>Given the stakes of the litigation, the future of the company — which employs 7,000 people at its massive Maplewood campus and about 13,500 statewide — will hinge in part on how it confronts its own history with these toxic chemicals.&nbsp;</span></p>
<p><span>A recent </span><i><span>Bloomberg</span></i><span> analysis </span><a target="_blank" href="https://news.bloomberglaw.com/litigation/3m-combat-earplug-fight-at-crossroads-as-court-strategies-falter"><span>estimated</span></a><span> 3M liabilities for the mass torts case and another over defective earplugs&nbsp;could reach $30 billion, or nearly half of market cap.&nbsp;</span></p>
    <h4>
What they knew, when they knew it
</h4>

	
<p><span>A key problem in any 3M defense: Despite the flurry of recent legal, regulatory and political activity, the chemicals’ dangers have been known —&nbsp;and known to 3M —&nbsp;for decades.&nbsp;</span></p>
<p><span>As early as the 1950s, 3M and DuPont scientists began discovering that the chemicals were accumulating in the bodies of humans and animals.&nbsp;</span></p>
<p><span>After compiling 27 million pages of documents and deposing about 200 witnesses in seven years, Minnesota’s former attorney general, Swanson, didn’t just walk away after settling with 3M. She released </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/StatesExhibits.asp"><span>thousands of internal 3M documents</span></a><span>.&nbsp;</span></p>
<p><span>The </span><i><span>Reformer</span></i><span> reviewed the documents, which</span><span> show that company officials were repeatedly warned that the chemicals were accumulating in the environment and detected in the blood of humans and animals, while showing worrisome signs of toxicity.&nbsp;</span></p>
<p><span>Time and again, the company found reasons to delay a full accounting to government regulators, Minnesota communities, and even its own workers. Like tobacco companies’ tardy admission about its cancer-causing drug and the NFL’s approach to concussions, 3M </span><span>ignored, delayed, minimized and obscured research that raised red flags about the chemicals.&nbsp;</span></p>
<p><span>Internal 3M documents show:&nbsp;</span></p>
<ul>
<li><span> In the 1950s, 3M animal studies consistently found its PFAS chemicals were toxic.</span></li>
<li><span> By the early 1960s, 3M knew the chemicals didn’t degrade in the environment.</span></li>
<li><span> 3M knew by the 1970s its chemicals were widely present in the blood of the general U.S. population.</span></li>
<li><span> A 1970</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1083.pdf"> <span>study</span></a><span> of fish had to be abandoned “to avoid severe stream pollution” and because all the fish died. After being exposed to a chemical, the fish couldn’t stay upright and kept crashing into the fish tank and dying.</span></li>
<li><span> By 1976, 3M knew the chemicals were in its plant workers’ blood at higher levels than normal.</span></li>
<li><span> A study of a chemical’s effect on 20 rhesus monkeys in 1978</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1193.pdf"> <span>had to be aborted</span></a><span> after 20 days because all the exposed monkeys&nbsp; died.</span></li>
<li><span> In 1979, a 3M scientist warned that perfluorochemicals posed a cancer risk because they are “known to persist for a long time in the body and thereby give long-term chronic exposure.”</span></li>
<li><span> In 1979, 3M lawyers</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"> <span>advised</span></a><span> the company to conceal a 3M chemical compound found in human blood.</span></li>
<li><span> In 1983, 3M scientists</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1282.pdf"> <span>concluded</span></a><span> that concerns about its chemicals “give rise to legitimate questions about the persistence, accumulation potential, and ecotoxicity of fluorochemicals in the environment.”</span></li>
<li><span>Purdy wrote in his </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"><span>resignation letter</span></a><span> that in the 1990s, 3M told researchers not to write down their thoughts or have email discussions because of how their “speculations” might be viewed in legal discovery.</span></li>
<li><span> 3M told employees to mark documents as “attorney-client privileged” regardless of whether attorneys were involved, the state alleged, and minutes of meetings were edited to omit references to health hazards.</span></li>
<li><span> In 1997, 3M gave DuPont a “material safety data sheet” — which lays out potential hazards — for a chemical. It read, “Warning: contains a chemical which can cause cancer,” citing 1983 and 1993 studies by 3M and DuPont. But 3M removed the label that same year and continued to sell the products for decades without warning.</span></li>
</ul>
<p><span>Thompson, the 3M spokesman, said the documents released by Swanson portray an “incomplete and misleading story that distorts the full record regarding 3M’s PFAS stewardship and who we are as a company.”&nbsp;</span></p>
<p><span>He said 3M disclosed many studies to the EPA over the course of decades, including on the chemicals’ toxicity and “the materials produced and discussed with EPA addressed relevant information and issues.”&nbsp;</span></p>
	
    

	
    <h4>‘The wildest hellcat’</h4>

	
<p><span>3M’s man-made, toxic chemicals can be traced back to World War II, and the U.S. race to develop atomic weapons in the top-secret Manhattan Project.</span></p>
<p><span>Scientists used fluorine gas to separate uranium, and discovered that when fluorine weds with carbon, the bonds are almost impossible to break.</span></p>
<p><span>After the war, some of the Manhattan Project scientists were hired by the Minnesota Mining and Manufacturing Company (3M), which bought the patent to develop perfluorochemicals, according to a 3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1365.pdf"> <span>book</span></a><span> celebrating the company’s history of chemical engineering, called “A Chemical History of 3M.”&nbsp;</span></p>
<p><span>Figuring out how to handle fluorine was a major hurdle for the scientists.</span></p>
<p><span>“In its pure, uncontrolled state — fortunately never found in nature — it is one of the most active, most dangerous elements known to man,” the book says. “The greenish-yellow gas will burn steel, water and even asbestos, which earned it a nickname — the wildest hellcat. Strangely, its wildness contributes to fluorine’s unique stability when it is combined with certain compounds.”</span></p>
<p><span>When combined with carbon, the resulting fluorochemical can repel water and oil and withstand fire, which had obvious commercial potential.</span></p>
<p><span>3M began manufacturing chemicals in Minnesota in the 1950s, and for the next 50 years they were used to make stain repellents,</span> <span>Teflon and other waterproof and fireproof products.</span></p>
<figure id="attachment_19468"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" target="_blank" data-slb-active="1" data-slb-asset="355035654" data-slb-internal="0" data-slb-group="19467"><img decoding="async" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png" alt="" width="818" height="570" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM.png 818w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-300x209.png 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.05.16-PM-768x535.png 768w" sizes="(max-width: 818px) 100vw, 818px"></a><figcaption><i></i>  This 1961 3M Scotchgard ad that ran in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
<p><span>By the 1990s, the chemicals</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1386.pdf"> <span>were in many consumer products</span></a><span>, such as window cleaners, floor waxes and polishes, fabric and leather protective coatings and carpet and upholstery treatments.</span></p>
<p><span>The products were a huge success, and the company was making almost a half a billion dollars per year off them by 2000, when it began —&nbsp;at the EPA’s urging —&nbsp;to phase out production of the chemical used to make Scotchgard. Production of other chemicals continued.&nbsp;</span></p>
<p><span>But the chemicals wouldn’t go away easily: They don’t break down in the environment, and they accumulate in the human body.</span></p>
    <h4>
3M employee: We pled ignorance
</h4>

	
<p><span>In 1975, a Florida professor called 3M after he and two colleagues discovered a fluorine chemical in human blood samples from Texas and New York.</span></p>
<p><span>The scientists suspected the source might be 3M chemicals used in household items such as Teflon cookware and Scotchgard.</span></p>
<p><span>Donald Taves, a researcher at the University of Rochester, first reported in the scientific journal </span><i><span>Nature</span></i><span> in 1968 that the general population had been exposed to the compounds. Then Taves discovered his own blood contained it, according to a 3M </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"><span>document</span></a><span> marked “confidential,” obtained in the Minnesota attorney general’s lawsuit.</span></p>
<p><span>Taves was working with Warren Guy and Wallace Brey at the University of Florida on a research paper.&nbsp;</span></p>
<p><span>3M chemist G.H. Crawford took the phone call from Taves, and admitted nothing. He</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1118.pdf"> <span>wrote</span></a><span> in a confidential interoffice memo: “We (pleaded) ignorance but advised him that Scotchgard was a polymeric material not a F.C. acid.”</span></p>
<p><span>(In fact, by this point, the company knew its chemicals accumulated in the human body and were toxic, Swanson </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>told</span></a><span> a congressional committee. Moreover, Swanson added, 3M refused to identify the chemicals in its products, which for a generation thwarted the scientific community’s understanding of their health impacts.)&nbsp;</span></p>
<figure id="attachment_19466"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg" target="_blank" data-slb-active="1" data-slb-asset="1522226148" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg" alt="" width="1024" height="575" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1024x575.jpg 1024w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-300x168.jpg 300w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-768x431.jpg 768w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop-1536x863.jpg 1536w, https://minnesotareformer.com/wp-content/uploads/2022/12/3m-nocrop.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><i></i>  3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany. Photo by Chad Davis</figcaption></figure>
<p><span>Crawford, the 3M scientist, suggested Guy get blood samples from “uncivilized areas” such as New Guinea “where they don’t use too much Teflon cookware or Scotchgard.”</span></p>
<p><span>He told his colleagues that the chemical 3M sold to DuPont to make Teflon cookware was the “least unlikely” explanation, but he didn’t tell Guy that. Crawford wrote that he “adopted a position of scientific curiosity and desire to assist in any way possible” and told Guy that 3M’s people might be able to “clarify” his study findings.&nbsp;</span></p>
<p><span>Another</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>internal document</span></a><span> shows Guy, the university researcher, also talked to a 3M employee identified as J.D. LaZerte about his quest to track down the source of chemicals in human blood.</span></p>
<p><span>LaZerte</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2771.pdf"> <span>wrote</span></a><span> in an internal document that he told Guy not to speculate.</span></p>
<p><span>Taves, Guy and Brey later</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1121.pdf"> <span>discovered</span></a><span> plasma from blood banks in five cities suggested “widespread contamination of human tissues with trace amounts of organic fluorocompounds derived from commercial products” such as floor waxes, wax paper, leather and fabric conditioning agents.</span></p>
<p><span>After getting the phone calls from researchers, 3M began analyzing its fluorine compounds. Within weeks,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1123.pdf"><span> they found a compound that was </span><span>a likely match</span></a><span>.&nbsp;</span></p>
<p><span>By late 1975, 3M sent employees to see Guy and Taves at the University of Rochester, where they agreed to try to isolate and identify fluorochemicals in blood.</span></p>
<p><span>In 1976, the company began sampling employees’ blood.&nbsp;</span></p>
<p><span>Tests </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1144.pdf"><span>show</span></a><span>ed</span><span> workers at 3M’s Cottage Grove plant called Chemolite had up to 1,000 times the normal amount of fluorochemicals in their blood.</span></p>
<p><span>In plant after plant, elevated levels were found, from Decatur, Alabama, to Antwerp, Belgium.</span></p>
<p><span>Gergel, the federal judge in South Carolina, wrote in his recent ruling that although 3M helped Guy and Taves identify the compound found in blood, the company</span><span> told no one else outside 3M for nearly a <a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1691.pdf">quarter century</a>, </span><span>despite the company’s legal duty to alert the EPA about potential harm to human health and the environment.</span></p>
<p><span>The judge cited a potential culprit: 3M lawyers, who urged 3M’s lab not to release the true identity of the compound (PFOS), according to an </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2534.pdf"><span>internal 3M documen</span></a><span>t.</span></p>
<p><span>Gergel said it would be reasonable to infer that the company knowingly withheld information that PFOS was in the blood of the general population and sought to discredit independent scientific work that would have disclosed this.</span></p>
<p><span>“3M did more than simply stay silent despite the company’s knowledge that the mystery compound was PFOS,” Gergel wrote.&nbsp;</span></p>
<p><span>The company went even further in its effort to obfuscate, the judge charged. In 1981, an author of an 1976 internal 3M report that confirmed that the unidentified chemical was in fact PFOS published an article in the same scientific journal as Guy and Taves stating that the mystery compound was not man-made but was a naturally occurring substance.&nbsp;</span></p>
    <h4>
DuPont asks 3M for ‘defensive information’
</h4>

	
<p><span>One of 3M’s biggest customers was DuPont, for which it produced chemicals to make Teflon products.</span></p>
<p><span>But by late 1975, DuPont was concerned about the possible toxic effects of Teflon and asked 3M for “defensive information” after a rat study found “sub-acute toxicity,” according to a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1124.pdf"> <span>3M document</span></a><span>.&nbsp;</span></p>
<p><span>After a 1979 meeting between 3M and DuPont, a 3M committee decided its data on the chemicals in workers’ blood samples wasn’t important enough to notify the EPA. Minutes from the meeting said DuPont asked if 3M had done any “chronic studies” on fluorochemicals or planned any in the future. The answer was no, they wouldn’t do such studies unless forced to by regulators.</span></p>
<p><span>3M told DuPont that because they’d seen no adverse human health effects and no widespread potential for the chemicals to accumulate, they did not need to notify the EPA, according to a report by Philippe Grandjean, a Dutch scientist who provided expert testimony for the state of Minnesota in its case against 3M.</span></p>
<p><span>“3M either closed its eyes to the evidence, or chose purposefully not to find it, or being generous to 3M, it seems possible that 3M may have mistakenly relied on the absence of evidence, despite the old dictum that ‘the absence of evidence is not evidence of absence,’ which later became famous in U.S. politics,” Grandjean wrote.</span></p>
<figure id="attachment_19469"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" target="_blank" data-slb-active="1" data-slb-asset="31343240" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png" alt="" width="462" height="596" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM.png 462w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.00.50-PM-233x300.png 233w" sizes="(max-width: 462px) 100vw, 462px"></a><figcaption><i></i>  This 1961 Scotchgard ad in LIFE magazine was going to be an exhibit in the state’s lawsuit against 3M. Courtesy state of Minnesota</figcaption></figure>
    <h4>
Employees notified of chemicals in blood
&nbsp;
</h4>

	
<p><span>In 1978, 3M began notifying chemical workers that trace amounts of chemicals were found in the blood of employees at the Cottage Grove, Decatur and Cordova</span> <span>plants.</span></p>
<p><span>“There did not appear to be any significant grouping of abnormalities,” according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1168.pdf"><span> confidential </span><span>meeting minutes </span></a><span>of 3M’s Fluorochemicals Technical Review Committee.</span></p>
<p><span>The committee discussed the potential carcinogenicity of the chemicals, and whether to notify workers and “the appropriate government agency,” given studies showing a PFAS compound was toxic in animals and a 1979</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1199.pdf"> <span>report on toxicity studies</span></a><span> on monkeys and rats found PFOS was “certainly more toxic than anticipated.”&nbsp;&nbsp;</span></p>
<p><span>But because there was “no evidence of ill effects,” the committee decided it didn’t constitute a substantial risk based on EPA guidelines pertaining to the Toxic Substances Control Act, which regulates chemicals.</span></p>
<p><span>The committee decided to keep exposure to all fluorochemicals to a minimum in all factory operations, and look into monitoring employee urine.</span></p>
<p><span>But it was becoming increasingly clear that several of the chemicals were toxic. Soon after,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1179.pdf"> <span>a 3M study</span></a><span> of two chemicals found they were “likely to persist in the environment for extended periods.”</span></p>
<p><span>“Because of the apparent persistence of these fluorochemicals in the body, the most important question remains possible long-term effects,” the report said.</span></p>
    <h4>
Prominent toxicologist warns ‘we could have a serious problem’
</h4>

	
<p><span>In the spring of 1979, 3M officials met at the Hilton Hotel in San Francisco to talk about their fluorochemical studies and the future.</span></p>
<p><span>They also heard from toxicologist Harold Hodge, a professor from the University of California, which</span><a target="_blank" href="https://oac.cdlib.org/view?docId=hb5f59n9gs&amp;doc.view=frames&amp;chunk.id=div00015&amp;toc.depth=1&amp;toc.id="> <span>dubbed</span></a><span> him “the dean of American toxicology.”</span></p>
<p><span>An epidemiology study was being done on 3,500 people, but so far there were no “unusual” causes of death.</span></p>
<p><span>Hodge recommended the company study the carcinogenicity of its chemicals.</span></p>
<p><span>A week later, Hodge asked that 3M add to the</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1204.pdf"> <span>meeting</span></a><span> minutes that it was of “utmost importance” that the company study whether a certain chemical was present in humans, at what level, and the degree of its persistence.</span></p>
<p><span>“If the levels are high and widespread and the half-life is long, we could have a serious problem,” Hodge warned.</span></p>
<p><span>Months later, 3M scientist M.T. Case</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1212.pdf"> <span>expressed similar concerns</span></a><span> — as “responsible 3M scientists” — about the lack of chronic toxicity data more than one year after the rat studies were done.</span></p>
<p><span>“I believe it is paramount to begin now an assessment of the potential (if any) of long term (carcinogenic) effects for these compounds which are known to persist for a long time in the body and thereby give long term chronic exposure,” Case wrote in a memo.</span></p>
    <h4>‘3M will likely be embarrassed’</h4>

	
<p><span>Other 3M employees were trying to persuade the company to come clean.</span></p>
<p><span>After a California company bought firefighting foam from 3M, it later learned that 3M chemist Eric Reiner told the client that the foam wasn’t biodegradable, contrary to 3M’s advertising claims.&nbsp;</span></p>
<p><span>Furious, the client </span><a target="_blank" href="https://docs.house.gov/meetings/GO/GO28/20190910/109902/HHRG-116-GO28-Wstate-SwansonL-20190910.pdf"><span>wrote</span></a><span> to 3M in 1988, demanding an explanation.</span></p>
<p><span>Reiner </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1351.pdf"><span>implored</span></a><span> company officials to do tests on the biodegradability of the chemicals, calling out those responsible in an internal memo.</span></p>
<p><span>“I don’t think it is in 3M’s long-term interest to perpetuate the myth that these fluorochemical surfactants are biodegradable,” he wrote. “It is probable that this misconception will eventually be discovered, and when that happens, 3M will likely be embarrassed, and we and our customers may be fined and forced to immediately withdraw products from the market.”</span></p>
<p><span>Three years later, company officials were still debating whether to study the environmental effects of fluorochemicals. A</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1372.pdf"> <span>draft proposal</span></a><span> for a study of long-term effects noted the problem with previous studies was there’s rarely a single fluorochemical in the product, making generalizations difficult.</span></p>
<p><span>“Perhaps the most important conclusion from previous studies is the stability of fluorochemicals although stability is one of the most desirable properties fluorochemicals possess,” it said. “For many applications, from an environmental perspective, stability connotes persistence which can be the cause of concern especially when coupled with other properties… taken together, stability, the tendency to bioaccumulate, and biological activity are a potentially troublesome combination.”</span></p>
    <h4>
3M vice president delays reporting to EPA
</h4>

	
<p><span>By the mid-1990s, that “potentially troublesome combination” was becoming a threat to 3M.</span></p>
<p><span>The company’s Toxic Substances Control Act committee recommended in 1998 that 3M notify the EPA and FDA that the chemicals were widely found in human blood.</span></p>
<p><span>A “</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1488.pdf"><span>communications plan</span></a><span>” included steps for an “orderly exit” from the market.</span></p>
<p><span>But one month later, 3M Group Vice President Charles Reich</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1496.pdf"><span> told</span></a><span> the committee he decided instead to do a review with a “wider spectrum” of internal and external experts.&nbsp;</span></p>
<p><span>“I have concluded that 3M is not presently in possession of information that would be new to EPA and that reasonably supports a conclusion that suggests a substantial risk of injury to human health or the environment,” he wrote.</span></p>
<p><span>This, despite decades of research suggesting otherwise.</span></p>
<p><span>3M finally </span><a target="_blank" href="https://static.ewg.org/reports/2020/pfas-epa-timeline/1998_3M-Alerts-EPA.pdf"><span>notified</span></a><span> the EPA in May 1998 that a fluorochemical (PFOS) was found in the general population’s blood at “very low” levels. The company said its studies of 3M workers found “no adverse effects,” saying, “3M does not believe that any reasonable basis exists to conclude that PFOS presents a substantial risk of injury to health or the environment.”</span></p>
<p><span>Judge Gergel recently noted that despite those assurances, 3M’s manager of corporate </span><span>toxicology, John Butenhoff, urged 3M in 1998 to replace “PFOS-based chemistry as these compounds [are] VERY persistent and thus insidiously toxic.”&nbsp;</span></p>
<p><span>Butenhoff calculated a “safe” level of PFOS in human blood at a little more than 1 part per billion. But 3M’s </span><i><span>own studies</span></i><span> from roughly the same period found that PFOS concentrations in the blood of the general public were in the range of 30 parts per billion.&nbsp;</span></p>
<p><span>Gergel said Butenhoff’s findings were never reported to the EPA and were revealed only during discovery in the firefighting foam litigation.&nbsp;</span></p>
    <h4>
‘This chemical is more stable than many rocks’
</h4>

	
<p><span>By 1998, 3M toxicologist</span> <span>Richard Purdy, the one studying chemicals in eagles and albatrosses, was growing increasingly concerned about those studies of wild birds.&nbsp;</span></p>
<p><span>On Dec. 3, 1998, Purdy said in an</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1533.pdf"> <span>email</span></a><span> there was a significant risk of ecological harm, which should be reported to the EPA, warning, “The levels we are seeing in eagles and other biota is likely to climb each year.”</span></p>
<p><span>He wasn’t alone.</span></p>
<p><span>In March 1999, a 3M worker </span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"><span>emailed</span></a><span> several colleagues and 3M’s general counsel, Thomas J. DiPasquale, questioning why three months had passed since a committee had reviewed Purdy’s hypothesis on food chain contamination.</span></p>
<p><span>DiPasquale wasn’t in a hurry, though.</span></p>
<p><span>“I’m not sure there is a need to support or refute the hypothesis within any particular time frame,” he replied.</span></p>
<p><span>Purdy, who was on the email chain,</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1003.pdf"> <span>retorted</span></a><span>: “Plan! That is the same stalling technique you have been using for the last year.”</span></p>
<p><span>“There is a high probability that PFOS is killing marine mammals and you want another plan when we could have had data to support the risk assessment long ago,” Purdy wrote. “You were given a plan in 1983. Again in the early 90s. And you authorized no testing.”</span></p>
<figure id="attachment_19470"><a href="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" target="_blank" data-slb-active="1" data-slb-asset="1682286043" data-slb-internal="0" data-slb-group="19467"><img decoding="async" loading="lazy" src="http://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png" alt="" width="634" height="510" srcset="https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM.png 634w, https://minnesotareformer.com/wp-content/uploads/2022/12/Screen-Shot-2022-12-14-at-3.03.07-PM-300x241.png 300w" sizes="(max-width: 634px) 100vw, 634px"></a><figcaption><i></i>  This undated photograph showing open burning of drums in a landfill was an exhibit in the state lawsuit against 3M. Courtesy state of Minnesota.</figcaption></figure>
<p><span>Meanwhile, his preliminary research indicated adult eagles had 50 times as much PFOS in their plasma as the eaglets.</span></p>
<p><span>“For 20 years the division has been stalling the collection of data needed for evaluating the environmental impact of fluorochemicals,” Purdy wrote. “PFOS is the most onerous pollutant since PCB and you want to avoid collecting data that indicates that it is probably worse. I am outraged.”</span></p>
<p><span>Two days later, Purdy resigned, and forwarded his</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1001.pdf"> <span>resignation letter</span></a><span> to the EPA.</span></p>
<p><span>“I have continually met roadblocks, delays, and indecision. For weeks on end I have received assurances that my samples would be analyzed soon — never to see results. There are always excuses and little is accomplished,” he wrote.</span></p>
<p><span>3M continued to make the chemicals after Purdy warned they were spreading through the food chain and harming sea mammals.</span></p>
<p><span>“This chemical is more stable than many rocks,” he wrote. “And the chemicals the company is considering for replacement are just as stable and biologically available. The risk assessment I performed was simple, and not worst case.”</span></p>
<p><span>3M told the people working on the fluorochemical project not to write down their thoughts or have email discussions because of how their speculation could be viewed in potential litigation, Purdy alleged.</span></p>
<p><span>“For me it is unethical to be concerned with markets, legal defensibility and image over environmental safety,” he wrote.</span></p>
<p><span>Purdy did not respond to a request for comment, but his view of 3M’s behavior seemed to soften over time. In an interview with MPR from his Wisconsin farm in 2005, he spoke “with pride” about the company’s investment in science and chemicals.&nbsp;</span></p>
<p><span>“3M is like somebody who ran the stop sign, got through the stop sign, ‘Oh my God,’ and stopped,” he was quoted saying.</span></p>
    <h4>
3M begins working to ‘command the science’
</h4>

	
<p><span>With the EPA on notice, the agency pressured 3M to stop manufacturing the compound used in Scotchgard (PFOS) in the U.S. in 2000. Six years later, the EPA fined the company for not turning over hundreds of reports on the chemicals’ toxicity.</span></p>
<p><span>The EPA said 3M’s own data indicated its chemicals didn’t break down and could pose a long-term threat to human health and the environment.</span></p>
<p><span>Still, the Minnesota Pollution Control Agency didn’t begin investigating the chemicals for two years, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2004.pdf"> <span>MPR</span></a><span>, which reported that all the agency had on file for 3M’s Cottage Grove plant in 2001 was a press clipping headlined “Scotchgard sticks in the environment.”</span></p>
<p><span>Once 3M had finally alerted regulators, the company worked on a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1587.pdf"> <span>communications plan</span></a><span>.&nbsp;</span></p>
<p><span>The first goal: “Protect and enhance 3M’s reputation.”</span></p>
<p><span>Indeed, its primary concern seemed to be controlling the narrative around the science. The plan included a list of “high-priority” candidates to be spokespersons for the company, including Michigan State University professor John Giesy, a 3M advisor on environmental studies. 3M employee Dale Bacon said he would gauge Giesy’s interest.</span></p>
<p><span>3M wanted to get scientific papers on their chemicals published before others, according to</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>internal emails</span></a><span>.</span></p>
<p><span>A 2003</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2604.pdf"> <span>internal memo</span></a><span> showed 3M looking to fund outside research using 3M “grant” money, particularly with people who would be influential in risk assessment and “other science policy matters.”</span></p>
<p><span>Among their action items: Develop a list of 3M and “industry-preferred” nominees for science advisory panels.</span></p>
<p><span>Giesy was the ideal candidate. He was editor of more than half the academic journals about PFAS and considered an independent expert.&nbsp;</span></p>
<p><span>3M went on to pay Giesy to review and share studies with 3M before they were published, Minnesota alleged in its lawsuit against 3M.</span></p>
<p><span>It began when Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1740.pdf"> <span>emailed</span></a><span> 3M officials in August 2000 informing the company he had a draft manuscript ready and wanted to submit it to </span><i><span>Science</span></i><span> before others beat him to it.</span></p>
<p><span>“I think it is important to publish our work before theirs,” Giesy wrote. “Otherwise, it looks like we (ie 3M) was pressured into the investigations they have done and subsequent release of the data.”</span></p>
<p><span>A 3M official warned his colleagues that publishing the paper “could set off a chain reaction of speculation that could reopen the issue with the media and move it back to a health story; something up-to-now we have avoided.”</span></p>
<p><span>Instead, the 3M official wrote that the company should keep “our” scientific publications “in the right order as we had already agreed,” noting he presumed Giesy’s work was done under contract with 3M and was only publishable “if and when we agree.”</span></p>
<p><span>The official added, however: “We also can’t dilly dally around either. It will take a great deal of sensitivity and people skills to bring Dr. Giesy around to our thinking on this and to be sure he doesn’t misinterpret our position as trying to hide the winnie. We just want the winnie in the bun, complete with mustard and ketchup.”</span></p>
<p><span>3M went on to develop a campaign to “command the science” and create “defensive barriers to litigation,” the state alleged in its lawsuit, by selectively funding outside research and editing scientific papers before they were published.</span></p>
<p><span>“The company, unfortunately, engaged in a campaign to hide its own studies and to, in fact, shape the science through the funding of these other studies,” Swanson told Congress.</span></p>
<p><span>Giesy explained how it worked in a</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2204.pdf"> <span>March 2008 email</span></a><span> to 3M Laboratory Manager William Reagen: He edited a lot of PFAS papers for scientific journals, but in his 3M billings, he listed the work as “literature searches” on timesheets “so that there was no paper trail to 3M.”</span></p>
<p><span>“Some journals will allow this, but others, for conflict of interest issues, will not allow an industry to review a paper about one of their products,” he wrote. “That is where I came in for Dale (Bacon, the 3M employee).”</span></p>
<p><span>Giesy</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2209.pdf"> <span>said</span></a><span> in a later email “Dale (Bacon) had me doing things to keep a finger on the pulse of things going on around the world, especially to try to keep bad papers out of the literature.”</span></p>
<p><span>The state lawsuit alleged 3M paid Giesy at least $2 million, and that he had a net worth of about $20 million despite working at public universities most of his career.</span></p>
<p><span>3M</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2773.pdf"> <span>records</span></a><span> show he was first paid by the company in 1993. Beginning in 1998, Entrix, Inc. — Giesy’s environmental consulting company — was paid nearly $1.7 million for his work through 2009, at a rate of $275 an hour, according to one</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX2102.pdf"> <span>billing</span></a><span>.</span></p>
<p><span>By 2008, the arrangement appeared to be ending. In an email, Giesy offered some closing words:&nbsp;</span></p>
<p><span>“My personal advise (sic) is that you want to keep ‘bad’ papers out of the literature, otherwise in litigation situations they can be a large obstacle to refute,” he wrote. “Judges seem to be of the opinion that if information is in the peer-reviewed, open literature, it is accurate.”</span></p>
<p><span>Giesy — who now works at the University of Saskatchewan — did not respond to multiple requests for comment, but in the past he has</span><a target="_blank" href="https://www.cbc.ca/news/canada/saskatoon/u-of-s-professor-denies-suppressing-toxic-pollution-research-for-3m-1.4554634"> <span>denied</span></a><span> any wrongdoing. He said he was only trying to keep mistakes out of the literature — and accused Swanson of trying to smear his reputation because he refused to be an expert for the state.</span></p>
<p><span>“The documents speak for themselves,” Swanson said in an interview.</span></p>
    <h4>
Goal: ‘Sell PFCs as long and as broadly as we can’
</h4>

	
<p><span>For more than a quarter century, 3M has known its fluorochemicals could have devastating consequences for the company’s long-term financial health.&nbsp;</span></p>
<p><span>A 1995 internal</span><a target="_blank" href="https://www.ag.state.mn.us/Office/Cases/3M/docs/PTX/PTX1445.pdf"> <span>strategic planning document</span></a><span> said “obstacle No. 1” to 3M’s major vision in its chemical business was “the persistence of fluorochemicals,” and “environmental, health, safety and regulatory issues and trends that threaten to limit our business.”</span></p>
<p><span>Among the “key actions” listed: “Continue to maintain regulatory approval to sell PFCs as long and as broadly as we can.”</span></p>
<p><span>It’s easy to understand why they were so committed to the chemicals, despite the massive risks: $500 million per year in revenue, year after year after year.&nbsp;</span></p>
<p><span>“Unfortunately, it succeeded for more than 50 years,” Swanson told Congress. “And now states and local governments around the nation are grappling with the consequences.”</span></p>
<p><span>To this day, 3M still manufactures perfluorochemicals in Cottage Grove, as well as Cordova, Ill., Decatur, Ala., Zwijndrecht, Belgium, and Gendorf, Germany.</span></p>
                                </div><div>
                                        
                                        <p>Our stories may be republished online or in print under Creative Commons license CC BY-NC-ND 4.0. We ask that you edit only for style or to shorten, provide proper attribution and link to our web site. Please see our republishing guidelines for use of photos and graphics.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[So you want custom allocator support in your C library (162 pts)]]></title>
            <link>https://nullprogram.com/blog/2023/12/17/</link>
            <guid>38675379</guid>
            <pubDate>Sun, 17 Dec 2023 19:04:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nullprogram.com/blog/2023/12/17/">https://nullprogram.com/blog/2023/12/17/</a>, See on <a href="https://news.ycombinator.com/item?id=38675379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
<article>
  
  <time datetime="2023-12-17">
    December 17, 2023
  </time>
  <p>
    nullprogram.com/blog/2023/12/17/
  </p>

  <p>Users of mature C libraries conventionally get to choose how memory is
allocated — that is, when it <a href="https://nullprogram.com/blog/2018/06/10/">cannot be avoided entirely</a>. The C
standard never laid down a convention — <a href="https://nullprogram.com/blog/2023/02/11/">perhaps for the better</a> —
so each library re-invents an allocator interface. Not all are created
equal, and most repeat a few fundamental mistakes. Often the interface is
merely a token effort, to check off that it’s “supported” without actual
consideration to its use. This article describes the critical features of
a practical allocator interface, and demonstrates why they’re important.</p>

<p>Before diving into the details, here’s the checklist for library authors:</p>

<ol>
  <li>All allocation functions accept a user-defined context pointer.</li>
  <li>The “free” function accepts the original allocation size.</li>
  <li>The “realloc” function accepts both old and new size.</li>
</ol>

<h3 id="context-pointer">Context pointer</h3>

<p>The standard library allocator keeps its state in global variables. This
makes for a simple interface, but comes with significant performance and
complexity costs. These costs likely motivate custom allocator use in the
first place, in which case slavishly duplicating the standard interface is
essentially the worst possible option. Unfortunately this is typical:</p>

<div><pre><code><span>#define LIB_MALLOC  malloc
#define LIB_FREE    free
</span></code></pre></div>

<p>I could observe the library’s allocations, and I could swap in a library
functionality equivalent to the standard library allocator — jemalloc,
mimalloc, etc. — but that’s about it. Better than nothing, I suppose, but
only just so. Function pointer callbacks are slightly better:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>void</span> <span>*</span><span>(</span><span>*</span><span>malloc</span><span>)(</span><span>size_t</span><span>);</span>
    <span>void</span>  <span>(</span><span>*</span><span>free</span><span>)(</span><span>void</span> <span>*</span><span>);</span>
<span>}</span> <span>allocator</span><span>;</span>

<span>session</span> <span>*</span><span>session_new</span><span>(...,</span> <span>allocator</span><span>);</span>
</code></pre></div>

<p>At least I could use different allocators at different times, and there
are even <a href="https://nullprogram.com/blog/2017/01/08/">tricks to bind a context pointer</a> to the callback. It
also works when the library is dynamically linked.</p>

<p>Either case barely qualifies as custom allocator support, and they’re
useless when it matters most. Only a small ingredient is needed to make
these interfaces useful: a context pointer.</p>

<div><pre><code><span>// NOTE: Better, but still not great</span>
<span>typedef</span> <span>struct</span> <span>{</span>
    <span>void</span> <span>*</span><span>(</span><span>*</span><span>malloc</span><span>)(</span><span>size_t</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
    <span>void</span>  <span>(</span><span>*</span><span>free</span><span>)(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
    <span>void</span>   <span>*</span><span>ctx</span><span>;</span>
<span>}</span> <span>allocator</span><span>;</span>
</code></pre></div>

<p>Users can choose <em>from where</em> the library will allocate at at given time.
It liberates the allocator from global variables (or janky workarounds),
and multithreading woes. The default can still hook up to the standard
library through stubs that fit these interfaces.</p>

<div><pre><code><span>static</span> <span>void</span> <span>*</span><span>lib_malloc</span><span>(</span><span>size_t</span> <span>size</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>(</span><span>void</span><span>)</span><span>ctx</span><span>;</span>
    <span>return</span> <span>malloc</span><span>(</span><span>size</span><span>);</span>
<span>}</span>

<span>static</span> <span>void</span> <span>*</span><span>lib_free</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>(</span><span>void</span><span>)</span><span>ctx</span><span>;</span>
    <span>free</span><span>(</span><span>ptr</span><span>);</span>
<span>}</span>

<span>static</span> <span>allocator</span> <span>lib_allocator</span> <span>=</span> <span>{</span><span>lib_malloc</span><span>,</span> <span>lib_free</span><span>,</span> <span>0</span><span>};</span>
</code></pre></div>

<p>Note that the context pointer came after the “standard” arguments. All
things being equal, “extra” arguments should go after standard ones. But
don’t sweat it! In the most common calling conventions this allows stub
implementations to be merely an unconditional jump. It’s <em>as though</em> the
stubs are a kind of subtype of the original functions.</p>

<div><pre><code><span>lib_malloc:</span>
        <span>jmp</span> <span>malloc</span>
<span>lib_free:</span>
        <span>jmp</span> <span>free</span>
</code></pre></div>

<p>Typically the decision is completely arbitrary, and so this minutia tips
the balance.</p>

<h4 id="context-pointer-example">Context pointer example</h4>

<p>So what’s the big deal? It means we can trivially plug in, say, a <a href="https://nullprogram.com/blog/2023/09/27/">tiny
arena allocator</a>. To demonstrate, consider this fictional string
set and partial JSON API, each of which supports a custom allocator. For
simplicity — I’m attempting to balance substance and brevity — they share
an allocator interface. (Note: Because <a href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1428r0.pdf">subscripts and sizes should be
signed</a>, and we’re now breaking away from the standard library
allocator, I will use <code>ptrdiff_t</code> for the rest of the examples.)</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>void</span> <span>*</span><span>(</span><span>*</span><span>malloc</span><span>)(</span><span>ptrdiff_t</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
    <span>void</span>  <span>(</span><span>*</span><span>free</span><span>)(</span><span>void</span> <span>*</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
    <span>void</span>   <span>*</span><span>ctx</span><span>;</span>
<span>}</span> <span>allocator</span><span>;</span>

<span>typedef</span> <span>struct</span> <span>set</span> <span>set</span><span>;</span>
<span>set</span>  <span>*</span><span>set_new</span><span>(</span><span>allocator</span> <span>*</span><span>);</span>
<span>set</span>  <span>*</span><span>set_free</span><span>(</span><span>set</span> <span>*</span><span>);</span>
<span>bool</span>  <span>set_add</span><span>(</span><span>set</span> <span>*</span><span>,</span> <span>char</span> <span>*</span><span>);</span>

<span>typdef</span> <span>struct</span> <span>json</span> <span>json</span><span>;</span>
<span>json</span>     <span>*</span><span>json_load</span><span>(</span><span>char</span> <span>*</span><span>buf</span><span>,</span> <span>ptrdiff_t</span> <span>len</span><span>,</span> <span>allocator</span> <span>*</span><span>);</span>
<span>json</span>     <span>*</span><span>json_free</span><span>(</span><span>json</span> <span>*</span><span>);</span>
<span>ptrdiff_t</span> <span>json_length</span><span>(</span><span>json</span> <span>*</span><span>);</span>
<span>json</span>     <span>*</span><span>json_subscript</span><span>(</span><span>json</span> <span>*</span><span>,</span> <span>ptrdiff_t</span> <span>i</span><span>);</span>
<span>json</span>     <span>*</span><span>json_getfield</span><span>(</span><span>json</span> <span>*</span><span>,</span> <span>char</span> <span>*</span><span>field</span><span>);</span>
<span>double</span>    <span>json_getnumber</span><span>(</span><span>json</span> <span>*</span><span>);</span>
<span>char</span>     <span>*</span><span>json_getstring</span><span>(</span><span>json</span> <span>*</span><span>);</span>
</code></pre></div>

<p><code>set</code> and <code>json</code> objects retain a copy of the <code>allocator</code> object for all
allocations made through that object. Given nothing, they default to the
standard library using the pass-through definitions above. Used together
with the standard library allocator:</p>

<div><pre><code><span>typedef</span> <span>{</span>
    <span>double</span> <span>sum</span><span>;</span>
    <span>bool</span>   <span>ok</span><span>;</span>
<span>}</span> <span>sum_result</span><span>;</span>

<span>sum_result</span> <span>sum_unique</span><span>(</span><span>char</span> <span>*</span><span>json</span><span>,</span> <span>ptrdiff_t</span> <span>len</span><span>)</span>
<span>{</span>
    <span>sum_result</span> <span>r</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>json</span> <span>*</span><span>namevals</span> <span>=</span> <span>json_load</span><span>(</span><span>json</span><span>,</span> <span>len</span><span>,</span> <span>0</span><span>);</span>
    <span>if</span> <span>(</span><span>!</span><span>namevals</span><span>)</span> <span>{</span>
        <span>return</span> <span>r</span><span>;</span>  <span>// parse error</span>
    <span>}</span>

    <span>ptrdiff_t</span> <span>arraylen</span> <span>=</span> <span>json_length</span><span>(</span><span>namevals</span><span>);</span>
    <span>if</span> <span>(</span><span>arraylen</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
        <span>json_free</span><span>(</span><span>namevals</span><span>);</span>
        <span>return</span> <span>r</span><span>;</span>  <span>// not an array</span>
    <span>}</span>

    <span>set</span> <span>*</span><span>seen</span> <span>=</span> <span>set_new</span><span>(</span><span>0</span><span>);</span>
    <span>for</span> <span>(</span><span>ptrdiff_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>arraylen</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>json</span> <span>*</span><span>element</span> <span>=</span> <span>json_subscript</span><span>(</span><span>namevals</span><span>,</span> <span>i</span><span>);</span>
        <span>char</span> <span>*</span><span>name</span>    <span>=</span> <span>json_getfield</span><span>(</span><span>element</span><span>,</span> <span>"name"</span><span>);</span>
        <span>char</span> <span>*</span><span>value</span>   <span>=</span> <span>json_getfield</span><span>(</span><span>element</span><span>,</span> <span>"value"</span><span>);</span>
        <span>if</span> <span>(</span><span>!</span><span>name</span> <span>||</span> <span>!</span><span>value</span><span>)</span> <span>{</span>
            <span>set_free</span><span>(</span><span>set</span><span>);</span>
            <span>json_free</span><span>(</span><span>namevals</span><span>);</span>
            <span>return</span> <span>r</span><span>;</span>  <span>// invalid element</span>
        <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>set_add</span><span>(</span><span>set</span><span>,</span> <span>name</span><span>))</span> <span>{</span>
            <span>r</span><span>.</span><span>sum</span> <span>+=</span> <span>json_getnumber</span><span>(</span><span>value</span><span>);</span>
        <span>}</span>
    <span>}</span>

    <span>set_free</span><span>(</span><span>set</span><span>);</span>
    <span>json_free</span><span>(</span><span>namevals</span><span>);</span>
    <span>r</span><span>.</span><span>ok</span> <span>=</span> <span>1</span><span>;</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Which given as JSON input:</p>

<div><pre><code><span>[</span><span>
    </span><span>{</span><span>"name"</span><span>:</span><span> </span><span>"foo"</span><span>,</span><span> </span><span>"value"</span><span>:</span><span>  </span><span>123</span><span>},</span><span>
    </span><span>{</span><span>"name"</span><span>:</span><span> </span><span>"bar"</span><span>,</span><span> </span><span>"value"</span><span>:</span><span>  </span><span>456</span><span>},</span><span>
    </span><span>{</span><span>"name"</span><span>:</span><span> </span><span>"foo"</span><span>,</span><span> </span><span>"value"</span><span>:</span><span> </span><span>1000</span><span>}</span><span>
</span><span>]</span><span>
</span></code></pre></div>

<p>Would return <code>579.0</code>. Because it’s using standard library allocation, it
must carefully clean up before returning. There’s also no out-of-memory
handling because, in practice, programs typically do not get to observe
and respond to the standard allocator running out of memory.</p>

<p>We can improve and simplify it with an arena allocator:</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span>    <span>*</span><span>beg</span><span>;</span>
    <span>char</span>    <span>*</span><span>end</span><span>;</span>
    <span>jmp_buf</span> <span>*</span><span>oom</span><span>;</span>
<span>}</span> <span>arena</span><span>;</span>

<span>void</span> <span>*</span><span>arena_malloc</span><span>(</span><span>ptrdiff_t</span> <span>size</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>arena</span> <span>*</span><span>a</span> <span>=</span> <span>ctx</span><span>;</span>
    <span>ptrdiff_t</span> <span>available</span> <span>=</span> <span>a</span><span>-&gt;</span><span>end</span> <span>-</span> <span>a</span><span>-&gt;</span><span>beg</span><span>;</span>
    <span>ptrdiff_t</span> <span>alignment</span> <span>=</span> <span>-</span><span>size</span> <span>&amp;</span> <span>15</span><span>;</span>
    <span>if</span> <span>(</span><span>size</span> <span>&gt;</span> <span>available</span><span>-</span><span>alignment</span><span>)</span> <span>{</span>
        <span>longjmp</span><span>(</span><span>*</span><span>a</span><span>-&gt;</span><span>oom</span><span>);</span>
    <span>}</span>
    <span>return</span> <span>a</span><span>-&gt;</span><span>end</span> <span>-=</span> <span>size</span> <span>+</span> <span>alignment</span><span>;</span>
<span>}</span>

<span>void</span> <span>arena_free</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>// nothing to do (yet!)</span>
<span>}</span>
</code></pre></div>

<p>I’m allocating from the end rather than the beginning because it will make
a later change simpler. Applying that to the function:</p>

<div><pre><code><span>sum_result</span> <span>sum_unique</span><span>(</span><span>char</span> <span>*</span><span>json</span><span>,</span> <span>ptrdiff_t</span> <span>len</span><span>,</span> <span>arena</span> <span>scratch</span><span>)</span>
<span>{</span>
    <span>sum_result</span> <span>r</span> <span>=</span> <span>{</span><span>0</span><span>};</span>

    <span>allocator</span> <span>a</span> <span>=</span> <span>{</span><span>0</span><span>};</span>
    <span>a</span><span>.</span><span>malloc</span> <span>=</span> <span>arena_malloc</span><span>;</span>
    <span>a</span><span>.</span><span>free</span> <span>=</span> <span>arena_free</span><span>;</span>
    <span>a</span><span>.</span><span>ctx</span> <span>=</span> <span>&amp;</span><span>scratch</span><span>;</span>

    <span>json</span> <span>*</span><span>namevals</span> <span>=</span> <span>json_load</span><span>(</span><span>json</span><span>,</span> <span>len</span><span>,</span> <span>&amp;</span><span>a</span><span>);</span>
    <span>if</span> <span>(</span><span>!</span><span>namevals</span><span>)</span> <span>{</span>
        <span>return</span> <span>r</span><span>;</span>  <span>// parse error</span>
    <span>}</span>

    <span>ptrdiff_t</span> <span>arraylen</span> <span>=</span> <span>json_length</span><span>(</span><span>namevals</span><span>);</span>
    <span>if</span> <span>(</span><span>arraylen</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
        <span>return</span> <span>r</span><span>;</span>  <span>// not an array</span>
    <span>}</span>

    <span>set</span> <span>*</span><span>seen</span> <span>=</span> <span>set_new</span><span>(</span><span>&amp;</span><span>a</span><span>);</span>
    <span>for</span> <span>(</span><span>ptrdiff_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>arraylen</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>json</span> <span>*</span><span>element</span> <span>=</span> <span>json_subscript</span><span>(</span><span>namevals</span><span>,</span> <span>i</span><span>);</span>
        <span>char</span> <span>*</span><span>name</span>    <span>=</span> <span>json_getfield</span><span>(</span><span>element</span><span>,</span> <span>"name"</span><span>);</span>
        <span>char</span> <span>*</span><span>value</span>   <span>=</span> <span>json_getfield</span><span>(</span><span>element</span><span>,</span> <span>"value"</span><span>);</span>
        <span>if</span> <span>(</span><span>!</span><span>name</span> <span>||</span> <span>!</span><span>value</span><span>)</span> <span>{</span>
            <span>return</span> <span>r</span><span>;</span>  <span>// invalid element</span>
        <span>}</span> <span>else</span> <span>if</span> <span>(</span><span>set_add</span><span>(</span><span>set</span><span>,</span> <span>name</span><span>))</span> <span>{</span>
            <span>r</span><span>.</span><span>sum</span> <span>+=</span> <span>json_getnumber</span><span>(</span><span>value</span><span>);</span>
        <span>}</span>
    <span>}</span>
    <span>r</span><span>.</span><span>ok</span> <span>=</span> <span>1</span><span>;</span>
    <span>return</span> <span>r</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Calls to <code>set_free</code> and <code>json_free</code> are no longer necessary because the
arena automatically frees these on any return, in O(1). I almost feel bad
the library authors bothered to write them! It also handles allocation
failure without introducing it to <code>sum_unique</code>. We may even deliberately
restrict the memory available to this function — perhaps because the input
is untrusted, and we want to quickly abort denial-of-service attacks — by
giving it a small arena, relying on out-of-memory to reject pathological
inputs.</p>

<p>There are so many possibilities unlocked by the context pointer.</p>

<h3 id="provide-the-original-allocation-size-when-freeing">Provide the original allocation size when freeing</h3>

<p>When an application frees an object it always has the original, requested
allocation size on hand. After all, it’s a necessary condition to use the
object correctly. In the simplest case it’s the size of the freed object’s
type: a static quantity. If it’s an array, then it’s a multiple of the
tracked capacity: a dynamic quantity. In any case the size is either known
statically or tracked dynamically by the application.</p>

<p>Yet <code>free()</code> does not accept a size, meaning that the allocator must track
the information redundantly! That’s a needless burden on custom
allocators, and with a bit of care a library can lift it.</p>

<p>This was noticed in C++, and WG21 added <a href="https://isocpp.org/files/papers/n3778.html">sized deallocation</a> in
C++14. It’s now the default on two of the three major implementations (and
probably not the two you’d guess). In other words, object size is so
readily available that it can mostly be automated away. Notable exception:
<code>operator new[]</code> and <code>operator delete[]</code> with trivial destructors. With
non-trivial destructors, <code>operator new[]</code> must track the array length for
its its own purposes <em>on top of libc bookkeeping</em>. In other words, array
allocations have their size stored in at least three different places!</p>

<p>That means the “free” interface should look like this:</p>

<div><pre><code><span>void</span> <span>*</span><span>lib_free</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>ptrdiff_t</span> <span>len</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
</code></pre></div>

<p>And calls inside the library might look like:</p>

<div><pre><code><span>lib_free</span><span>(</span><span>p</span><span>,</span> <span>sizeof</span><span>(</span><span>*</span><span>p</span><span>));</span>
<span>lib_free</span><span>(</span><span>a</span><span>,</span> <span>sizeof</span><span>(</span><span>*</span><span>a</span><span>)</span><span>*</span><span>len</span><span>);</span>
</code></pre></div>

<p>Now that <code>arena_free</code> has size information, it can free an allocation if
it was the most recent:</p>

<div><pre><code><span>void</span> <span>arena_free</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>ptrdiff_t</span> <span>size</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>arena</span> <span>*</span><span>a</span> <span>=</span> <span>ctx</span><span>;</span>
    <span>if</span> <span>(</span><span>ptr</span> <span>==</span> <span>a</span><span>-&gt;</span><span>end</span><span>)</span> <span>{</span>
        <span>ptrdiff_t</span> <span>alignment</span> <span>+=</span> <span>-</span><span>size</span> <span>&amp;</span> <span>15</span><span>;</span>
        <span>a</span><span>-&gt;</span><span>end</span> <span>+=</span> <span>size</span> <span>+</span> <span>alignment</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>If the library allocates short-lived objects to compute some value, then
discards in reverse order, the memory can be reused. The arena doesn’t
have to do anything special. The library merely needs to share its
knowledge with the allocator.</p>

<p>Beyond arena allocation, an allocator could use the size to locate the
allocation’s size class and, say, push it onto a freelist of its size
class. <a href="https://www.youtube.com/watch?v=LIb3L4vKZ7U">Size-class freelists compose well with arenas</a>, and an
implementation is short and simple when the caller of “free” communicates
object size.</p>

<p>Another idea: During testing, use a debug allocator that tracks object
size and validates the reported size against its own bookkeeping. This can
help catch mistakes sooner.</p>

<h3 id="provide-the-old-size-when-resizing-an-allocation">Provide the old size when resizing an allocation</h3>

<p>Resizing an allocation requires a lot from an allocator, and it should be
avoided if possible. At the very least it cannot be done <em>at all</em> without
knowing the original allocation size. An allocator can’t simply no-op it
like it can with “free.” With the standard library interface, allocators
have no choice but to redundantly track object sizes when “realloc” is
required.</p>

<p>So, just as with “free,” the allocator should be given the old object
size!</p>

<div><pre><code><span>void</span> <span>*</span><span>lib_realloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>ptrdiff_t</span> <span>old</span><span>,</span> <span>ptrdiff_t</span> <span>new</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>);</span>
</code></pre></div>

<p>At the very least, an allocator could implement “realloc” with “malloc”
and <code>memcpy</code>:</p>

<div><pre><code><span>void</span> <span>arena_realloc</span><span>(</span><span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>ptrdiff_t</span> <span>old</span><span>,</span> <span>ptrdiff_t</span> <span>new</span><span>,</span> <span>void</span> <span>*</span><span>ctx</span><span>)</span>
<span>{</span>
    <span>void</span> <span>*</span><span>r</span> <span>=</span> <span>arena_malloc</span><span>(</span><span>new</span><span>,</span> <span>ctx</span><span>);</span>
    <span>return</span> <span>memcpy</span><span>(</span><span>r</span><span>,</span> <span>ptr</span><span>,</span> <span>old</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>Of the three checklist items, this is the most neglected. Exercise for the
reader: The last-allocated object <em>can</em> be resized in place, instead using
<code>memmove</code>. If this is frequently expected, allocate from the front, adjust
<code>arena_free</code> as needed, and extend the allocation in place <a href="https://nullprogram.com/blog/2023/10/05/#addendum-extend-the-last-allocation">as discussed a
previous addendum</a>, without any copying.</p>

<h3 id="real-world-examples">Real world examples</h3>

<p>Let’s examine real world examples to see how well they fit the checklist.
First up is <a href="https://troydhanson.github.io/uthash/userguide.html#_hooks">uthash</a>, a popular, easy-to-use, intrusive hash table:</p>

<div><pre><code><span>#define uthash_malloc(sz) my_malloc(sz)
#define uthash_free(ptr, sz) my_free(ptr)
</span></code></pre></div>

<p>No “realloc” so it trivially checks (3). It optionally provides the old
size to “free” which checks (2). However it misses (1) which is the most
important, greatly limiting its usefulness.</p>

<p>Next is the venerable <a href="https://www.zlib.net/manual.html">zlib</a>. It has function pointers with these
prototypes on its <code>z_stream</code> object.</p>

<div><pre><code><span>void</span> <span>*</span><span>zlib_malloc</span><span>(</span><span>void</span> <span>*</span><span>ctx</span><span>,</span> <span>unsigned</span> <span>items</span><span>,</span> <span>unsigned</span> <span>size</span><span>);</span>
<span>void</span>  <span>zlib_free</span><span>(</span><span>void</span> <span>*</span><span>ctx</span><span>,</span> <span>void</span> <span>*</span><span>ptr</span><span>);</span>
</code></pre></div>

<p>The context pointer checks (1), and I can confirm from experience that
it’s genuinely useful with a custom allocator. No “realloc” so it passes
(3) automatically. It misses (2), but in practice this hardly matters: It
allocates everything up front, and frees at the very end, meaning a no-op
“free” is quite sufficient.</p>

<p>Finally there’s the <a href="https://www.lua.org/manual/5.4/manual.html#lua_Alloc">Lua programming language</a> with this economical,
single-function interface:</p>

<div><pre><code><span>void</span> <span>*</span><span>lua_Alloc</span><span>(</span><span>void</span> <span>*</span><span>ctx</span><span>,</span> <span>void</span> <span>*</span><span>ptr</span><span>,</span> <span>size_t</span> <span>old</span><span>,</span> <span>size_t</span> <span>new</span><span>);</span>
</code></pre></div>

<p>It packs all three allocator functions into one function. It includes a
context pointer (1), a free size (2), and two realloc sizes (3). It’s a
simple allocator’s best friend!</p>



  
  <ol></ol>

  

  <nav>
  
    
  
  
  </nav>
</article>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's CDNA 3 Compute Architecture (234 pts)]]></title>
            <link>https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</link>
            <guid>38675258</guid>
            <pubDate>Sun, 17 Dec 2023 18:51:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/">https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=38675258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>AMD has a long history of vying for GPU compute market share. Ever since Nvidia got first dibs with their Tesla architecture, AMD has been playing catch up. Terascale 3 moved from VLIW5 to VLIW4 to improve execution unit utilization in compute workloads. GCN replaced Terascale and emphasized consistent performance for both GPGPU and graphics applications. Then, AMD diverged their GPU architecture development into separate CDNA and RDNA lines specialized for compute and graphics respectively. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24552"><img decoding="async" width="664" height="411" data-attachment-id="24552" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/amd_gpu_lines/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-orig-size="664,411" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amd_gpu_lines" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?fit=664%2C411&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/amd_gpu_lines.jpg?resize=664%2C411&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>CDNA 2 finally brought AMD notable success. MI250X and MI210 GPUs won several supercomputer contracts including ORNL’s Frontier, which holds first place on November 2023’s TOP500 list. But while CDNA2 delivered solid and cost efficient FP64 compute, H100 had better AI performance and offered a larger unified GPU.</p>
<p>CDNA 3 looks to close those gaps by bringing forward everything AMD has to offer. The company’s experience in advanced packaging technology is on full show, with MI300X getting a sophisticated chiplet setup. Together with Infinity Fabric components, advanced packaging lets MI300X scale to compete with Nvidia’s largest GPUs. On the memory side, Infinity Cache from the RDNA line gets pulled into the CDNA world to mitigate bandwidth issues. But that doesn’t mean MI300X is light on memory bandwidth. It still gets a massive HBM setup, giving it the best of both worlds. Finally, CDNA 3’s compute architecture gets significant generational improvements to boost throughput and utilization.</p>
<h2>GPU Layout</h2>
<p>AMD has a tradition of using chiplets to cheaply scale core counts in their Ryzen and Epyc CPUs. MI300X uses a similar strategy at a high level, with compute split off onto Accelerator Complex Dies, or XCDs. XCDs are analogous to CDNA 2 or RDNA 3’s Graphics Compute Dies (GCDs) or Ryzen’s Core Complex Dies (CCDs). AMD likely changed the naming because CDNA products lack the dedicated graphics hardware present in the RDNA line.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24592"><img decoding="async" width="688" height="384" data-attachment-id="24592" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300_xcd_slide/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-orig-size="1275,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300_xcd_slide" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=1275%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?fit=688%2C384&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=688%2C384&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300_xcd_slide.jpg?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Each XCD contains a set of cores and a shared cache. Specifically, every XCD physically has 40 CDNA 3 Compute Units, with 38 of these being enabled per XCD on the MI300X. A 4 MB L2 cache sits on the XCD as well, and serves all of the die’s CUs. MI300X has eight XCDs, giving it 304 total Compute Units. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24645"><img decoding="async" width="635" height="499" data-attachment-id="24645" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-orig-size="635,499" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?fit=635%2C499&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_drawio.jpg?resize=635%2C499&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>That’s a large increase over the MI250X’s 220 CUs. Even better, MI300X can expose all of those CUs as a single GPU. On MI250X, a programmer would have to manually split up work across the two GPUs because each has a separate pool of memory.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24647"><img loading="lazy" decoding="async" width="553" height="418" data-attachment-id="24647" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi250x_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-orig-size="553,418" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi250x_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?fit=553%2C418&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=553%2C418&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?w=553&amp;ssl=1 553w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi250x_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 553px) 100vw, 553px" data-recalc-dims="1"></a></figure></div>
<p>Nvidia’s H100 consists of 132 Streaming Multiprocessors (SMs) and also presents them to programmers as a big unified GPU. H100 takes a conventional approach by implementing all of that compute on a large monolithic die. Even with everything on the same die, H100 is too large to give all of its SMs equal access to cache. So, H100 splits the L2 into two instances. A single SM can use all 50 MB of L2, but access to more than 25 MB will incur a performance penalty.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24646"><img loading="lazy" decoding="async" width="688" height="337" data-attachment-id="24646" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/h100_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-orig-size="787,386" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=787%2C386&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?fit=688%2C337&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=688%2C337&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?w=787&amp;ssl=1 787w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/h100_drawio.jpg?resize=768%2C377&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Still, Nvidia’s strategy makes more efficient use of cache capacity than MI300X’s. A MI300X XCD doesn’t use L2 capacity on other XCDs for caching, just as CCDs on Epyc/Ryzen don’t allocate into each other’s L3 caches.</p>
<p>Intel’s Ponte Vecchio (PVC) compute GPUs make for a very interesting comparison. PVC places its basic compute building blocks in dies called Compute Tiles, which are roughly analogous to CDNA 3’s XCDs. Similarly, PVC’s Base Tile serves a similar function to CDNA 3’s IO dies. Both contain a large last level cache and HBM memory controllers. Like MI300X, a Ponte Vecchio card can be exposed as a single GPU with a unified memory pool.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24652"><img loading="lazy" decoding="async" width="451" height="339" data-attachment-id="24652" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/pvc_drawio/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-orig-size="451,339" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pvc_drawio" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?fit=451%2C339&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=451%2C339&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?w=451&amp;ssl=1 451w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=400%2C300&amp;ssl=1 400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/pvc_drawio.jpg?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 451px) 100vw, 451px" data-recalc-dims="1"></a></figure></div>
<p>However, there are important differences. Ponte Vecchio’s Compute Tiles are smaller with only eight Xe Cores, compared to 38 Compute Units on a CDNA 3 XCD. Instead of using a Compute Tile wide cache, Intel uses larger L1 caches to reduce cross-die traffic demands. Using a two-stack Ponte Vecchio part as a unified GPU presents challenges too. The EMIB bridge between the two stacks <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">only offers 230 GB/s of bandwidth</a>, which isn’t enough to fully utilize HBM bandwidth if accesses are striped across all memory controllers. To address this, Intel has APIs that can let programs work with the GPU in a NUMA configuration. </p>
<p>In terms of physical construction, PVC and CDNA 3’s designs have different challenges. CDNA 3’s ability to present a unified memory pool with HBM requires high bandwidth between the IO dies. PVC gets by with a relatively low bandwidth EMIB link. But PVC’s design gets complicated because it uses four die types with different process nodes and foundries. AMD only uses two die types in MI300X, and both nodes (6 nm and 5 nm) are from TSMC.</p>
<h2>Tackling the Bandwidth Problem</h2>
<p>Compute has been outpacing memory for decades. Like CPUs, GPUs have countered this with increasingly sophisticated caching strategies. CDNA 2 used a conventional two-level cache hierarchy with a 8 MB L2, relying on HBM2e to keep the execution units fed. But even with HBM2e, MI250X was more bandwidth starved than Nvidia’s H100. If AMD simply added more compute, bandwidth starvation could be come a serious issue. So, AMD took a leaf out of RDNA(2)’s book and added an “Infinity Cache”.</p>
<p>Much like the consumer RDNA GPUs, MI300’s Infinity Cache is what the technical documentation calls Memory Attached Last Level (MALL), which is a fancy way to say that the last level cache level is a memory side cache. Compared to L1 and L2 caches that are closer to the Compute Units, the Infinity Cache is attached to the memory controllers. All memory traffic passes through the Infinity Cache regardless of what block it’s coming from. That includes IO traffic, so communications between peer GPUs can benefit from Infinity Cache bandwidth. Because the Infinity Cache always has the most up to date view of DRAM contents, It doesn’t have to handle snoops or other cache maintenance operations.</p>
<div>
<figure><a href="https://chipsandcheese.com/polaris_l2_clients/"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24743" data-permalink="https://chipsandcheese.com/polaris_l2_clients/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-orig-size="1275,713" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="polaris_l2_clients" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=1275%2C713&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?w=1275&amp;ssl=1 1275w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/polaris_l2_clients.png?resize=1200%2C671&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From AMD’s presentation on their RDNA architecture. L2 slices may be associated with memory controllers, but the L2 is not a memory side cache because many agents can write to DRAM without going through L2</figcaption></figure></div>
<p>But because a memory side cache is farther away from compute, it generally suffers from higher latency. Therefore, AMD has multi-megabyte L2 caches on both CDNA 3 and RDNA 2 to insulate compute from the lower performance of a memory side cache.</p>
<p>Like RDNA 2, CDNA 3’s Infinity Cache is 16-way set associative. However, CDNA 3’s implementation is more optimized for bandwidth than capacity. It’s composed of 128 slices, each with 2 MB of capacity and 64 bytes per cycle of read bandwidth. All of the slices together can deliver 8192 bytes per cycle, which is good for 17.2 TB/s at 2.1 GHz.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/"><img loading="lazy" decoding="async" width="688" height="554" data-attachment-id="24577" data-permalink="https://chipsandcheese.com/cdna3_cache_hierarchy_compared/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-orig-size="920,741" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cache_hierarchy_compared" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=920%2C741&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?fit=688%2C554&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=688%2C554&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?w=920&amp;ssl=1 920w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cache_hierarchy_compared.jpg?resize=768%2C619&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>For comparison, RDNA 2’s 128 MB Infinity Cache can provide 1024 bytes per cycle across all slices, giving it 2.5 TB/s of theoretical bandwidth at 2.5 GHz.<a href="https://www.flickr.com/photos/130561288@N04/51703830661/"> Die shots</a> suggest each Infinity Cache slice has 4 MB of capacity and provides 32B/cycle. RDNA 2 therefore uses bigger slices, fewer of them and has less bandwidth from each slice.</p>
<p>MI300X’s focus on bandwidth means workloads with lower compute density can still enjoy decent performance if they can get enough Infinity Cache hits. That should make CDNA 3’s execution units easier to feed even though the main memory bandwidth to compute ratio hasn’t changed much and remains behind Nvidia’s.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_ic_roofline/"><img loading="lazy" decoding="async" width="688" height="382" data-attachment-id="24579" data-permalink="https://chipsandcheese.com/cdna3_ic_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-orig-size="796,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_ic_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=796%2C442&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?fit=688%2C382&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=688%2C382&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?w=796&amp;ssl=1 796w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_ic_roofline.png?resize=768%2C426&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>MI250X figures are for a single GCD</figcaption></figure></div>
<p>If we construct a roofline model for MI300X using Infinity Cache’s theoretical bandwidth, we can achieve full FP64 throughput with 4.75 FLOPs per byte loaded. It’s a massive improvement over DRAM, which would require 14.6 to 15 FLOPs per byte loaded.</p>
<h4>Possible Challenges with Cross-Die Bandwidth</h4>
<p>MI300X’s Infinity Fabric spans four IO dies, each of which connects to two HBM stacks and associated cache partitions. However, the bandwidth of the die to die connections may limit achieving full Infinity Cache bandwidth when MI300X operates as a single logical GPU with a unified memory pool. If memory accesses are striped evenly across the memory controllers (and thus cache partitions), as is typical for most GPU designs, the available die-to-die bandwidth may prevent applications from reaching theoretical Infinity Cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24650"><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>First, let’s focus on a single IO die partition. It has 2.7 TB/s of ingress bandwidth along two edges adjacent to other IO dies. Its two XCDs can get 4.2 TB/s of Infinity cache bandwidth. If L2 miss requests are evenly striped across the dies, 3/4 of that bandwidth, or 3.15 TB/s, must come from peer dies. Since 3.15 TB/s is greater than 2.7 TB/s, cross-die bandwidth will limit achievable cache bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24840"><img loading="lazy" decoding="async" width="688" height="404" data-attachment-id="24840" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_singlepart/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-orig-size="912,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_singlepart" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=912%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?fit=688%2C404&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=688%2C404&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_singlepart.png?resize=768%2C451&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>We can add the die in the opposite corner without any differences because all of its required die-to-die bandwidth goes in the opposite direction. MI300X has bidirectional die-to-die links.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24841"><img loading="lazy" decoding="async" width="688" height="398" data-attachment-id="24841" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_ic_bw_opposite_corners/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-orig-size="912,527" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_ic_bw_opposite_corners" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=912%2C527&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?fit=688%2C398&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=688%2C398&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?w=912&amp;ssl=1 912w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_ic_bw_opposite_corners.png?resize=768%2C444&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>If all dies demand maximum Infinity Cache bandwidth in a unified configuration, things get more complex. Extra cross-die bandwidth is consumed because transfers between dies in opposite corners require two hops, and that’ll cut into ingress bandwidth available for each die.</p>
<p>While MI300X was engineered to act like one big GPU, splitting MI300X into multiple NUMA domains could give higher combined Infinity Cache bandwidth. It’s possible that AMD will have an API that will transparently split up programs among the different IO dies. Additionally, the likelihood of bandwidth issues would be minimized by high L2 hit rates, which would help avoid those bottlenecks. And in cases where the Infinity Cache hit rate are low, the MI300X’s die-to-die links are sufficiently robust and offer ample bandwidth to smoothly handle HBM traffic.</p>
<h3> Cross-XCD Coherency</h3>
<p>Even though the Infinity Cache doesn’t have to worry about coherency, the L2 caches do. Ordinary GPU memory accesses follow a relaxed coherency model, but programmers can use atomics to enforce ordering between threads. Memory accesses on AMD GPUs can also be marked with a GLC bit (Global Level Coherent). Those mechanisms still have to work if AMD wants to expose MI300X as a single big GPU, rather than a multi-GPU configuration as MI250X had done.</p>
<div>
<figure><a href="https://chipsandcheese.com/fah_atomics/"><img loading="lazy" decoding="async" width="688" height="256" data-attachment-id="24617" data-permalink="https://chipsandcheese.com/fah_atomics/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-orig-size="1007,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fah_atomics" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=1007%2C375&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?fit=688%2C256&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=688%2C256&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?w=1007&amp;ssl=1 1007w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/fah_atomics.png?resize=768%2C286&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Snippet of RDNA 2 code from Folding at Home, showing use of global memory atomics</figcaption></figure></div>
<p>On prior AMD GPUs, atomics and coherent accesses were handled at L2. Loads with the GLC bit set would bypass L1 caches, and thus get the most up-to-date copy of data from L2. That doesn’t work with MI300X because the most up-to-date copy of a cacheline could be on another XCD’s L2 cache. AMD could make coherent accesses bypass L2, but that would lower performance. That may have worked for a gaming GPU where coherent accesses aren’t too important. But AMD wants MI300X to perform well with compute workloads, and needs MI300A (the APU variant) to efficiently share data between the CPU and GPU. That’s where Infinity Fabric comes in.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24583"><img loading="lazy" decoding="async" width="688" height="385" data-attachment-id="24583" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/cdna3_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-orig-size="1533,857" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=1533%2C857&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?fit=688%2C385&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=688%2C385&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1533&amp;ssl=1 1533w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=768%2C429&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1200%2C671&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?resize=1320%2C738&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cs.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>CM = Coherent Master. CS = Coherent Slave</figcaption></figure></div>
<p>Like Infinity Fabric on Ryzen, CDNA 3 has Coherent Masters (CMs) where the XCDs connect to the IO dies. Coherent Slaves (CS) sit at each memory controller alongside Infinity Cache (IC) slices. We can infer how these work via Ryzen documentation, which shows Coherent Slaves have a probe filter and hardware for handling atomic transactions. MI300X likely has a similar CS implementation.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24585"><img loading="lazy" decoding="async" width="688" height="501" data-attachment-id="24585" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/zen_ppr_cs/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-orig-size="736,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zen_ppr_cs" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=736%2C536&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?fit=688%2C501&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/zen_ppr_cs.png?resize=688%2C501&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From AMD’s Zen PPR, showing error reporting available at the Coherent Slave (CS).</figcaption></figure></div>
<p>If a coherent write shows up at the CS, it has to ensure any thread doing a coherent read will observe that write regardless of where that thread is running on the GPU. That means any XCD with the line cached will have to reload it from Infinity Cache to get the most up to date data. Naively, the CS would have to probe L2 caches across all XCDs because any of them could have the corresponding data cached. The probe filter helps avoid this by tracking which XCDs actually have the line cached, thus avoiding unnecessary probe traffic. CDNA 3’s whitepaper says the snoop filter (another name for a probe filter) is large enough to cover multiple XCD L2 caches. I certainly believe them because MI300X has 32 MB of L2 across all eight XCDs. Even consumer Ryzen parts can have more CCD-private cache for the probe filter to cover.</p>
<p>Thanks to CPU-like Infinity Fabric components like CS and CM, a XCD can have a private write-back L2 cache capable of handling intra-die coherent accesses without going across the IO die fabric. AMD could have gone for a naive solution where coherent operations and atomics go straight to the Infinity Cache, bypassing L2. Such a solution would save engineering effort and create a simpler design at the cost of lower performance for coherent operations. Evidently, AMD thought optimizing atomics and coherent accesses was important enough to go the extra mile.</p>
<blockquote>
<p>To ensure coherence of local memory writes of CUs in different agents a <code>buffer_wbl2 sc1</code> is required. It will writeback dirty L2 cache lines.</p>
<p>To ensure coherence of local memory reads of CUs in different agents a <code>buffer_inv sc0 sc1</code> is required. It will invalidate non-local L2 cache<br>lines if configured to have multiple L2 caches.</p>
<cite><a href="https://github.com/llvm/llvm-project/blob/main/llvm/docs/AMDGPUUsage.rst">LLVM Documentation</a> for the GFX942 Target</cite></blockquote>
<p>However, CDNA 3 within the XCD still works a lot like prior GPUs. Evidently normal memory writes will not automatically invalidate written lines from peer caches as in CPUs. Instead, code must explicitly tell the L2 to write back dirty lines and have peer L2 caches invalidate non-local L2 lines.</p>
<h3>L2 Cache</h3>
<p>Closer to the Compute Units, each MI300X XCD packs a 4 MB L2 cache. The L2 is a more traditional GPU cache, and is built from 16 slices. Each 256 KB slice can provide 128 bytes per cycle of bandwidth. At 2.1 GHz, that’s good for 4.3 TB/s. As the last level of cache on the same die as the Compute Units, the L2 plays an important role in acting as a backstop for L1 misses.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24664"><img loading="lazy" decoding="async" width="651" height="396" data-attachment-id="24664" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l2_roofline-1/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-orig-size="651,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l2_roofline-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?fit=651%2C396&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l2_roofline-1.png?resize=651%2C396&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Compared to H100 and MI250X, MI300X has a higher L2 bandwidth to compute ratio. Because each XCD comes with a L2, L2 bandwidth naturally scales as a CDNA 3 product comes with more XCDs. In other words, MI300X’s L2 arrangement avoids the problem of getting a single cache hooked up to a lot of Compute Units and maintain a ton of bandwidth.</p>
<p>PVC’s L2 is a clear contrast. As Intel adds more Compute Tiles, the Base Tile’s shared L2 gets increasing bandwidth demands. From a cache design standpoint, PVC’s configuration is simpler because the L2 acts as a single point of coherency and a backstop for L1 misses. But it can’t offer as much bandwidth as MI300X’s L2. MI300X also likely enjoys better L2 latency, making it easier for applications to utilize cache bandwidth.</p>
<h3>L1 Cache</h3>
<p>CDNA 3’s focus on high cache bandwidth continues to the L1. In a move that matches RDNA, CDNA 3 sees its L1 throughput increased from 64 to 128 bytes per cycle. CDNA 2 increased per-CU vector throughput to 4096 bits per cycle compared to 2048 in GCN, so CDNA 3’s doubled L1 throughput helps maintain the same compute to L1 bandwidth ratio as GCN.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24667"><img loading="lazy" decoding="async" width="651" height="395" data-attachment-id="24667" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_l1_roofline/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-orig-size="651,395" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_l1_roofline" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?fit=651%2C395&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_l1_roofline.png?resize=651%2C395&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Besides higher bandwidth, CDNA 3 increases L1 capacity from 16 to 32 KB. It’s a move that again mirrors developments in the RDNA line, where RDNA 3 received a similar size boost for its first level cache. Higher hitrates from the larger cache would lower average memory access latency, improving execution unit utilization. Transferring data from L2 and beyond costs power, so higher hitrate can help power efficiency too.</p>
<p>While CDNA 3 improves first level caching, Ponte Vecchio is still the champion in that category. Each Xe Core in PVC can deliver 512 bytes per cycle, giving Intel a very high L1 bandwidth to compute ratio. The L1 is large as well at 512 KB. Memory bound kernels that fit in L1 will do very well on Intel’s architecture. However, Ponte Vecchio lacks a mid-level cache at the Compute Tile level, and could face a harsh performance cliff as data spills out of L1.</p>
<h2>Scheduling and Execution Units</h2>
<p>A complex chiplet setup and modified cache hierarchy let AMD present MI300X as a single GPU, thus addressing one of MI250X’s biggest weaknesses. But AMD didn’t settle with that. They also made iterative improvements to the core Compute Unit architecture, addressing CDNA 2’s difficulties with utilizing its FP32 units.</p>
<div>
<figure><a href="https://chipsandcheese.com/cdna3_cu/"><img loading="lazy" decoding="async" width="688" height="175" data-attachment-id="24812" data-permalink="https://chipsandcheese.com/cdna3_cu/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-orig-size="692,176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cdna3_cu" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=692%2C176&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?fit=688%2C175&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/cdna3_cu.png?resize=688%2C175&amp;ssl=1" alt="" data-recalc-dims="1"></a><figcaption>From the CDNA 3 whitepaper</figcaption></figure></div>
<p>When CDNA 2 shifted to handling FP64 natively, AMD provided double rate FP32 via packed execution. The compiler would have to pack two FP32 values into adjacent registers and perform the same instruction on both. Often, the compiler struggled to pull this off unless programmers explicitly used vectors.</p>
<p>CDNA 3 gets around this with a more flexible dual issue mechanism. Most likely, this is an extension of GCN’s multi-issue capability rather than RDNA 3’s VOPD/wave64 method. Each cycle, the CU scheduler selects one of the four SIMDs and checks whether any of its threads are ready to execute. If multiple threads are ready, GCN could select up to five of them to send to execution units. Of course a GCN SIMD only has a single 16-wide vector ALU, so GCN would have to select threads with different instruction types ready to multi-issue. For example, a scalar ALU instruction can issue alongside a vector ALU one.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="280" data-attachment-id="24753" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue_occupancy/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-orig-size="1400,570" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue_occupancy" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=1400%2C570&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?fit=688%2C280&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=688%2C280&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?w=1400&amp;ssl=1 1400w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=768%2C313&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1200%2C489&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue_occupancy.jpg?resize=1320%2C537&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>An alternative approach would be to take advantage of wave64’s wider width and let a thread complete two vector instructions over four cycles. However, doing so would break GCN’s model of handling VALU instructions in multiples of 4 clock cycles. CDNA 3 is still more closely related to GCN than RDNA is, and reusing GCN’s multi-issue strategy is a sensible move. AMD also could have used RDNA 3’s VOPD mechanism, where a special instruction format can contain two operations. While that method could increase per-thread performance, relying on the compiler to find dual issue pairs could be hit or miss.</p>
<div>
<figure><a href="https://chipsandcheese.com/gcn_cu_scheduler/"><img loading="lazy" decoding="async" width="688" height="381" data-attachment-id="24655" data-permalink="https://chipsandcheese.com/gcn_cu_scheduler/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-orig-size="1032,571" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gcn_cu_scheduler" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=1032%2C571&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?fit=688%2C381&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=688%2C381&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?w=1032&amp;ssl=1 1032w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/gcn_cu_scheduler.png?resize=768%2C425&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From an old AMD presentation</figcaption></figure></div>
<p>Instead of relying on the compiler, CDNA 3’s dual issue approach likely pushes responsibility to the programmer to expose more thread level parallelism via larger dispatch sizes. If a SIMD has more threads in flight, it’ll have a better chance of finding two threads with FP32 instructions ready to execute. At minimum, a SIMD will need two threads active to achieve full FP32 throughput. In practice CDNA 3 will need much higher occupancy to achieve good FP32 utilization. GPUs use in-order execution so individual threads will often be blocked by memory or execution latency. Keeping one set of execution units fed can be difficult even at full occupancy.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=24752"><img loading="lazy" decoding="async" width="688" height="480" data-attachment-id="24752" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/mi300x_multi_issue/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-orig-size="997,695" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mi300x_multi_issue" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=997%2C695&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?fit=688%2C480&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=688%2C480&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?w=997&amp;ssl=1 997w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/mi300x_multi_issue.jpg?resize=768%2C535&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Therefore, AMD has dramatically increased the number of threads each CDNA 3 SIMD can track <a href="https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-register-pressure-readme/">from 8</a> to 24. If a programmer can take advantage of this, CDNA 3 will be better positioned to multi-issue. But this can be difficult. AMD did not mention an increase in vector register file capacity, which often limits how many threads a SIMD can have in flight. The vector register file can hold state for more threads if each thread uses fewer registers, so CDNA 3’s multi-issue capability may work best for simple kernels with few live variables.</p>
<p>Register file bandwidth presents another challenge for dual issue. CDNA 2’s packed FP32 execution didn’t require extra reads from the vector register file because it took advantage of wider register file ports needed to deliver 64-bit values. But separate instructions can reference different registers and require more reads from the register file. Adding more register file ports would be expensive, so CDNA 3 “generationally improves the source caching to provide better re-use and bandwidth amplification so that each vector register read can support more downstream vector or matrix operations”<sup>1</sup>. Most likely, AMD is using a larger register cache to mitigate port conflicts and keep the execution units fed.</p>
<h2>Matrix Operations</h2>
<p>Matrix multiplication has become increasingly important as machine learning picks up. Nvidia invested heavily in this area, adding matrix multiplication units (tensor cores) to their Volta and Turing architectures years ago. AMD’s CDNA architecture added matrix multiply support, but contemporary Nvidia architectures invested more heavily in matrix multiplication throughput. This especially applies to lower precision data types like FP16, which are often used in AI.</p>
<figure><table><tbody><tr><td></td><td>Matrix FP16 FMAs/Clk</td><td>Rate Relative to Packed FP16</td></tr><tr><td>AMD MI100 (CDNA) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI250X (CDNA 2) Compute Unit</td><td>512</td><td>4x</td></tr><tr><td>AMD MI300X (CDNA 3) Compute Unit</td><td>1024</td><td>8x</td></tr><tr><td>Nvidia V100 Streaming Multiprocessor</td><td>512</td><td>4x<sup>4</sup></td></tr><tr><td>Nvidia A100 Streaming Multiprocessor</td><td>1024</td><td>4x</td></tr><tr><td>Nvidia H100 Streaming Multiprocessor</td><td>2048</td><td>8x</td></tr></tbody></table></figure>
<p>MI300X plays catch up by doubling per-CU matrix throughput compared to prior CDNA generations. On top of that, MI300X’s chiplet design allows a massive number of CUs. But Nvidia’s higher per-SM matrix performance still makes it a force to be reckoned with. Therefore, CDNA 3 continues AMD’s trend of hitting Nvidia hard from the vector FP64 performance side while maintaining strong AI performance in isolation.</p>
<h2>Instruction Cache</h2>
<p>Besides handling memory accesses requested by instructions, a Compute Unit has to fetch the instructions themselves from memory. GPUs traditionally had an easier time with instruction delivery because GPU code tends to be simple and not occupy a lot of memory. In the DirectX 9 era, Shader Model 3.0 <a href="https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx9-graphics-reference-asm-ps-3-0">even imposed limits on code size</a>. As GPUs evolved to take on compute, AMD rolled out their GCN architecture with 32 KB instruction caches. Today, CDNA 2 and RDNA GPUs continue to use 32 KB instruction caches.</p>
<p>CDNA 3 increases instruction cache capacity to 64 KB. Associativity doubles too, from 4-way to 8-way. That means higher instruction cache hitrates for CDNA 3 with bigger, more complex kernels. I suspect AMD is targeting CPU code naively ported to GPUs. Complex CPU code can be <a href="https://streamhpc.com/blog/2018-03-14/selecting-applications-suitable-for-porting-to-the-gpu/">punishing on GPUs</a>, since they can’t hide instruction cache miss latency with long distance instruction prefetching and accurate branch prediction. Higher instruction cache capacity helps contain larger kernels, while increased associativity helps avoid conflict misses.</p>
<p>Like CDNA 2, each CDNA 3 instruction cache instance services <a href="https://elixir.bootlin.com/linux/latest/source/drivers/gpu/drm/amd/amdkfd/kfd_crat.c#L328">two Compute Units</a>. GPU kernels are usually launched with large enough work sizes to fill many Compute Units, so sharing the instruction cache is a good way to efficiently use SRAM storage. I suspect AMD didn’t share the cache across even more Compute Units because a single cache instance may struggle to satisfy instruction bandwidth demands.</p>
<h2>Final Words</h2>
<p>CDNA 3’s whitepaper says that “the greatest generational changes in the AMD CDNA 3 architecture lie in the memory hierarchy” and I would have to agree. While AMD improved the Compute Unit’s low precision math capabilities compared to CDNA 2, the real improvement was the addition of the Infinity Cache.</p>
<p>MI250X’s primary issue was that it wasn’t really one GPU. It was two GPUs sharing the same package which only has 200 Gigabyte per second per direction between the GCDs. In AMD’s assessment that 200 Gigabyte per second per direction was not enough to have the MI250X show up as one GPU which is why AMD significantly increased the die to die bandwidth.</p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="389" data-attachment-id="24650" data-permalink="https://chipsandcheese.com/?attachment_id=24650" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-orig-size="1976,1117" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=1976%2C1117&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?fit=688%2C389&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=688%2C389&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1976&amp;ssl=1 1976w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=768%2C434&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1536%2C868&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1200%2C678&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1600%2C904&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?resize=1320%2C746&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-1.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"><figcaption>For this image, I am considering North-South as the vertical axis and East-West as the horizontal axis</figcaption></figure></div>
<p>AMD increased the total East-West bandwidth to 2.4TB/sec per direction which is a 12 fold increase from MI250X. And the total North-South bandwidth is an even higher 3.0TB/sec per direction. With these massive bandwidth increases, AMD was able to make the MI300 appear as one large, unified accelerator instead of as 2 separate accelerators like MI250X. </p>
<p>4.0 TB/s of total ingress bandwidth for one die may not seem like enough if both XCD needs all available memory bandwidth. However, both XCDs combined can only access up to 4.2TB/s of bandwidth from the IO die so realistically the 4.0TB/s of ingress bandwidth is a non-issue. What the maximum of 4.0TB/s of ingress bandwidth does mean is that a single IO die can’t take advantage of all 5.3TB/s of memory bandwidth. </p>
<p>This is similar to desktop Ryzen 7000 parts where one CCD can’t take full advantage of DDR5 bandwidth due to Infinity Fabric limits. However this is likely to be a non-issue on MI300X because the bandwidth demands will be highest with all dies in play. In that case, each die will consume about 1.3 TB/s of bandwidth and getting 3/4 of that over cross-die links won’t be a problem. </p>
<div>
<figure><img loading="lazy" decoding="async" width="688" height="387" data-attachment-id="24792" data-permalink="https://chipsandcheese.com/2023/12/17/amds-cdna-3-compute-architecture/image-1-2-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-orig-size="1495,840" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1-2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=1495%2C840&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?fit=688%2C387&amp;ssl=1" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1495&amp;ssl=1 1495w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1280%2C720&amp;ssl=1 1280w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1200%2C674&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?resize=1320%2C742&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/12/image-1-2.jpg?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></figure></div>
<p>But MI300 isn’t just a GPGPU part, it also has an APU part as well, which is in my opinion is the more interesting of the two MI300 products. AMD’s first ever APU, Llano, was released in 2011 which was based on AMD’s K10.5 CPU paired with a Terascale 3 GPU. Fast forward to 2023 and for their first “big iron” APU, the MI300A, AMD paired 6 of their CDNA3 XCDs with 24 Zen 4 cores all while reusing the same base die. This allows for the CPU and the GPU to shared the same memory address space which removes the need to copy data over an external bus to keep the CPU and GPU coherent with each other. </p>
<p>We look forward to what AMD could do with future “big iron” APUs as well as their future GPGPU line up. Maybe they’ll have specialized CCDs with wider vector units or maybe they’ll have networking on their base die that can directly connect to the xGMI switches that Broadcom have said to be making. Regardless of what future Instinct products look like, we are excited to both be looking forward to those products as well as testing the MI300 series.</p>
<p>We would like to thank AMD for inviting Chips and Cheese to the MI300 launch event. We were able to ask a lot of questions and gain some extra information without which this article would have been much shorter.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>
<h2>References</h2>
<ol>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf">CDNA 3 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf">CDNA 2 Whitepaper</a></li>
<li><a href="https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf">CDNA Whitepaper</a></li>
<li><a href="https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Volta Whitepaper</a></li>
<li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">Nvidia A100 Whitepaper</a></li>
<li><a href="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper">Nvidia H100 Whitepaper</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html#gs.1p3uqo">Intel Data Center GPU Max Series Technical Overview</a></li>
</ol>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
<li>
<p><img alt="Cheese" src="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/eb262496276a5c8c0a375be578f81db9?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>

</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Misra C++:2023 (113 pts)]]></title>
            <link>https://forum.misra.org.uk/thread-1668.html</link>
            <guid>38674158</guid>
            <pubDate>Sun, 17 Dec 2023 16:52:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.misra.org.uk/thread-1668.html">https://forum.misra.org.uk/thread-1668.html</a>, See on <a href="https://news.ycombinator.com/item?id=38674158">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_3679">
<div>
	<!-- start: postbit_avatar -->
<p><a href="https://forum.misra.org.uk/user-4.html"><img src="https://forum.misra.org.uk/images/default_avatar.png" alt="" width="55" height="55"></a></p>
<!-- end: postbit_avatar -->
	
	<div>
		<!-- start: postbit_author_user --><p>

	Posts: 103<br>
	Threads: 76<br>
	Joined: May 2004
	</p><!-- start: postbit_reputation -->
<p>Reputation: </p><!-- start: postbit_reputation_formatted_link -->
<p><a href="https://forum.misra.org.uk/reputation.php?uid=4"><strong>3</strong></a></p><!-- end: postbit_reputation_formatted_link -->
<!-- end: postbit_reputation -->
<!-- end: postbit_author_user -->
	</div>
</div>
<div>
	<div>
		<!-- start: postbit_posturl -->

<!-- end: postbit_posturl -->
		
		<p><span>29-11-2023, 07:42 PM <span id="edited_by_3679"><!-- start: postbit_editedby -->
<span>(This post was last modified: 30-11-2023, 04:03 PM by <a href="https://forum.misra.org.uk/user-3.html">forum admin</a>.)</span>
<!-- end: postbit_editedby --></span></span>
		
	</p></div>
	<div id="pid_3679"><p>
		MISRA is very pleased to announce the release of the new version of MISRA C++; MISRA C++:2023 <span>Guidelines for the use C++:17 in critical systems</span></p><p>

Published in October 2023, this is the latest and current edition of MISRA C++. It is specifically targetting the 2017 language version (C++:17) as defined by ISO/IEC 14882:2017.</p><p>

The document is available in PDF form from our webstore, and you can also purchase hardcopies using a “print on demand” service.</p><p>

We will create an FAQ section shortly with answers to questions on the new document as well as a new area in this forum for discussion on its guidelines. Please wait until we have created the new forum topics before posting questions.</p><p>

Webstore purchases are for single-user individual licenses. Other uses including but not limited to corporate (shared) use, use within a tool by tool vendors and training courses require a license; details are available on request. Please use the "contact us" form on the MISRA website to get in touch.
	</p></div>
	
	<!-- start: postbit_signature -->
<p>
Dr David Ward<br>
MISRA Operations Director
</p>
<!-- end: postbit_signature -->
	
	
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Get any piece of Google Earth as a single normalized glTF 3D model (154 pts)]]></title>
            <link>https://github.com/OmarShehata/google-earth-as-gltf</link>
            <guid>38673973</guid>
            <pubDate>Sun, 17 Dec 2023 16:37:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/OmarShehata/google-earth-as-gltf">https://github.com/OmarShehata/google-earth-as-gltf</a>, See on <a href="https://news.ycombinator.com/item?id=38673973">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Google Earth as glTF models</h2>
<p dir="auto">A little web app to demonstrate (1) how to fetch 3D Tiles from the <a href="https://developers.google.com/maps/documentation/tile/3d-tiles" rel="nofollow">Google Photorealistic API</a> and (2) how to correctly normalize &amp; rotate the glTF tiles, or combine a set of them into one glTF that can be rendered in any standard engine.</p>
<p dir="auto"><em>NOTE: This is intended to be an educational tool to provide an easy way to experiment with the API and understand how to tweak different parameters like zoom &amp; screen space error. See <a href="https://developers.google.com/maps/documentation/tile/policies" rel="nofollow">Google's Map Tiles API policies</a> . Note especially that offline use is prohibited.</em></p>
<h3 tabindex="-1" dir="auto">Try it here: <a href="https://omarshehata.github.io/google-earth-as-gltf/" rel="nofollow">https://omarshehata.github.io/google-earth-as-gltf/</a></h3>
<details open="">
  <summary>
    
    <span aria-label="Video description statue_of_liberty_tile_fetching.mp4">statue_of_liberty_tile_fetching.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/1711126/291009979-03fe3b48-fa39-48ba-b83a-772454949980.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI4Nzk1MTUsIm5iZiI6MTcwMjg3OTIxNSwicGF0aCI6Ii8xNzExMTI2LzI5MTAwOTk3OS0wM2ZlM2I0OC1mYTM5LTQ4YmEtYjgzYS03NzI0NTQ5NDk5ODAubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTIxOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzEyMThUMDYwMDE1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk2NDM3NzM0ZmJiMGMzYWFkNDhlZDJmNmZlOThkN2M5NDFlNGI1ZGU2N2Q3ZWVjZWY4NGI2ZmI2ZmNhNzBkYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.4cZwo-o98ff5yTM-ooJsFAoxIJy2S_54E-dUWIwPoIE" data-canonical-src="https://private-user-images.githubusercontent.com/1711126/291009979-03fe3b48-fa39-48ba-b83a-772454949980.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDI4Nzk1MTUsIm5iZiI6MTcwMjg3OTIxNSwicGF0aCI6Ii8xNzExMTI2LzI5MTAwOTk3OS0wM2ZlM2I0OC1mYTM5LTQ4YmEtYjgzYS03NzI0NTQ5NDk5ODAubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQUlXTkpZQVg0Q1NWRUg1M0ElMkYyMDIzMTIxOCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyMzEyMThUMDYwMDE1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk2NDM3NzM0ZmJiMGMzYWFkNDhlZDJmNmZlOThkN2M5NDFlNGI1ZGU2N2Q3ZWVjZWY4NGI2ZmI2ZmNhNzBkYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.4cZwo-o98ff5yTM-ooJsFAoxIJy2S_54E-dUWIwPoIE" controls="controls" muted="muted">

  </video>
</details>

<h3 tabindex="-1" dir="auto">How it works</h3>
<ul dir="auto">
<li><code>src/index.js</code> uses <a href="https://loaders.gl/" rel="nofollow">loaders.gl</a> to take a given lat/lng/zoom level, traverse the tileset and return a list of URLs to glTF tiles</li>
<li><code>src/Viewer.js</code> takes these URLs, fetches them, normalizes the tiles from ECEF to centered around (0, 0, 0)</li>
</ul>
<p dir="auto">See <a href="https://github.com/OmarShehata/google-earth-as-gltf/blob/main/simple-node-example">simple-node-example/</a> for a minimal example of just fetching tiles for a given region, which all the extra logic the app does around filtering for tiles that match the requested screen space error.</p>
<h3 tabindex="-1" dir="auto">What is "screen space error"?</h3>
<p dir="auto">Think of it as roughly meaning "level of detail". The lowest possible SSE is 1 which is the highest quality. When you're zoomed out a lot you want to load higher SSE to get bigger tiles that cover a wider area (but that are lower quality).</p>
<p dir="auto">Screen space error is a concept defined in the 3D Tiles specification, see: <a href="https://github.com/CesiumGS/3d-tiles/tree/main/specification#geometric-error">https://github.com/CesiumGS/3d-tiles/tree/main/specification#geometric-error</a></p>
<h3 tabindex="-1" dir="auto">Running it locally</h3>
<ul dir="auto">
<li><code>npm install</code></li>
<li><code>npm run dev</code></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[microsoft/promptbase: All things prompt engineering (184 pts)]]></title>
            <link>https://github.com/microsoft/promptbase</link>
            <guid>38673954</guid>
            <pubDate>Sun, 17 Dec 2023 16:36:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/promptbase">https://github.com/microsoft/promptbase</a>, See on <a href="https://news.ycombinator.com/item?id=38673954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">promptbase</h2>
<p dir="auto"><code>promptbase</code> is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like <code>GPT-4</code>. We currently host scripts demonstrating the <a href="https://arxiv.org/abs/2311.16452" rel="nofollow"><code>Medprompt</code> methodology</a>, including examples of how we further extended this collection of prompting techniques ("<code>Medprompt+</code>") into non-medical domains:</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>GPT-4 Prompt</th>
<th>GPT-4 Results</th>
<th>Gemini Ultra Results</th>
</tr>
</thead>
<tbody>
<tr>
<td>MMLU</td>
<td>Medprompt+</td>
<td>90.10%</td>
<td>90.04%</td>
</tr>
<tr>
<td>GSM8K</td>
<td>Zero-shot</td>
<td>95.3%</td>
<td>94.4%</td>
</tr>
<tr>
<td>MATH</td>
<td>Zero-shot</td>
<td>68.4%</td>
<td>53.2%</td>
</tr>
<tr>
<td>HumanEval</td>
<td>Zero-shot</td>
<td>87.8%</td>
<td>74.4%</td>
</tr>
<tr>
<td>BIG-Bench-Hard</td>
<td>Few-shot + CoT</td>
<td>89.0%</td>
<td>83.6%</td>
</tr>
<tr>
<td>DROP</td>
<td>Zero-shot + CoT</td>
<td>83.7%</td>
<td>82.4%</td>
</tr>
<tr>
<td>HellaSwag</td>
<td>10-shot</td>
<td>95.3%</td>
<td>87.8%</td>
</tr>
</tbody>
</table>
<p dir="auto">In the near future, <code>promptbase</code> will also offer further case studies and structured interviews around the scientific process we take behind prompt engineering. We'll also offer specialized deep dives into specialized tooling that accentuates the prompt engineering process. Stay tuned!</p>
<h2 tabindex="-1" dir="auto"><code>Medprompt</code> and The Power of Prompting</h2>
<details>
<summary>
    <em>"Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine" (H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M. McKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, E. Horvitz 2023)</em>
</summary>
<br>
<pre><p dir="auto">@article{nori2023can,
title={Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine},
author={Nori, Harsha and Lee, Yin Tat and Zhang, Sheng and Carignan, Dean and Edgar, Richard and Fusi, Nicolo and King, Nicholas and Larson, Jonathan and Li, Yuanzhi and Liu, Weishung and others},
journal={arXiv preprint arXiv:2311.16452},
year={2023}
}
</p></pre>
<a href="https://arxiv.org/pdf/1909.09223.pdf" rel="nofollow">Paper link</a>
</details>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_radar.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_radar.png" alt=""></a></p>
<p dir="auto">In a recent <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">study</a>, we showed how the composition of several prompting strategies into a method that we refer to as <code>Medprompt</code> can efficiently steer generalist models like GPT-4 to achieve top performance, even when compared to models specifically finetuned for medicine. <code>Medprompt</code> composes three distinct strategies together -- including dynamic few-shot selection, self-generated chain of thought, and choice-shuffle ensembling -- to elicit specialist level performance from GPT-4. We briefly describe these strategies here:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/medprompt_sa_graphic.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/medprompt_sa_graphic.png" alt=""></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Dynamic Few Shots</strong>: Few-shot learning -- providing several examples of the task and response to a foundation model -- enables models quickly adapt to a specific domain and
learn to follow the task format. For simplicity and efficiency, the few-shot examples applied in prompting for a particular task are typically fixed; they are unchanged across test examples. This necessitates that the few-shot examples selected are broadly representative and relevant to a wide distribution of text examples. One approach to meeting these requirements is to have domain experts carefully hand-craft exemplars. Even so, this approach cannot guarantee that the curated, fixed few-shot examples will be appropriately representative of every test example. However, with enough available data, we can select <em>different</em> few-shot examples for different task inputs. We refer to this approach as employing dynamic few-shot examples. The method makes use of a mechanism to identify examples based on their similarity to the case at hand. For Medprompt, we did the following to identify representative few shot examples: Given a test example, we choose k training examples that are semantically similar using a k-NN clustering in the embedding space. Specifically, we first use OpenAI's <code>text-embedding-ada-002</code> model to embed candidate exemplars for few-shot learning. Then, for each test question x, we retrieve its nearest k neighbors x1, x2, ..., xk from the training set (according to distance in the embedding space of text-embedding-ada-002). These examples -- the ones most similar in embedding space to the test question -- are ultimately registered in the prompt.</p>
</li>
<li>
<p dir="auto"><strong>Self-Generated Chain of Thought (CoT)</strong>: Chain-of-thought (CoT) uses natural language statements, such as “Let’s think step by step,” to explicitly encourage the model to generate a series of intermediate reasoning steps. The approach has been found to significantly improve the ability of foundation models to perform complex reasoning. Most approaches to chain-of-thought center on the use of experts to manually compose few-shot examples with chains of thought for prompting. Rather than rely on human experts, we pursued
a mechanism to automate the creation of chain-of-thought examples. We found that we could simply ask GPT-4 to generate chain-of-thought for the training examples, with appropriate guardrails for reducing risk of hallucination via incorrect reasoning chains.</p>
</li>
<li>
<p dir="auto"><strong>Majority Vote Ensembling</strong>: <a href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="nofollow">Ensembling</a> refers to combining the output of several algorithms together to yield better predictive performance than any individual algorithm. Frontier models like <code>GPT-4</code> benefit from ensembling of their own outputs. A simple technique is to have a variety of prompts, or a single prompt with varied <code>temperature</code>, and report the most frequent answer amongst the ensemble constituents. For multiple choice questions, we employ a further trick that increases the diversity of the ensemble called <code>choice-shuffling</code>, where we shuffle the relative order of the answer choices before generating each reasoning
path. We then select the most consistent answer, i.e., the one that is least sensitive to choice shuffling, which increases the robustness of the answer.</p>
</li>
</ul>
<p dir="auto">The combination of these three techniques led to breakthrough performance in Medprompt for medical challenge questions. Implementation details of these techniques can be found here: <a href="https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu">https://github.com/microsoft/promptbase/tree/main/src/promptbase/mmlu</a></p>
<h2 tabindex="-1" dir="auto"><code>Medprompt+</code> | Extending the power of prompting</h2>
<p dir="auto">Here we provide some intuitive details on how we extended the <code>medprompt</code> prompting framework to elicit even stronger out-of-domain performance on the MMLU (Measuring Massive Multitask Language Understanding) benchmark.  MMLU was established as a test of general knowledge and reasoning powers of large language models.  The complete MMLU benchmark contains tens of thousands of challenge problems of different forms across 57 areas from basic mathematics to United States history, law, computer science, engineering, medicine, and more.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/promptbase/blob/main/images/mmlu_accuracy_ablation.png"><img src="https://github.com/microsoft/promptbase/raw/main/images/mmlu_accuracy_ablation.png" alt=""></a></p>
<p dir="auto">We found that applying Medprompt without modification to the whole MMLU achieved a score of 89.1%. Not bad for a single policy working across a great diversity of problems!  But could we push Medprompt to do better?  Simply scaling-up MedPrompt can yield further benefits. As a first step, we increased the number of ensembled calls from five to 20.  This boosted performance to 89.56%.</p>
<p dir="auto">On working to push further with refinement of Medprompt, we noticed that performance was relatively poor for specific topics of the MMLU. MMLU contains a great diversity of types of questions, depending on the discipline and specific benchmark at hand. How might we push GPT-4 to perform even better on MMLU given the diversity of problems?</p>
<p dir="auto">We focused on extension to a portfolio approach based on the observation that some topical areas tend to ask questions that would require multiple steps of reasoning and perhaps a scratch pad to keep track of multiple parts of a solution. Other areas seek factual answers that follow more directly from questions. Medprompt employs “chain-of-thought” (CoT) reasoning, resonating with multi-step solving.  We wondered if the sophisticated Medprompt-classic approach might do less well on very simple questions and if the system might do better if a simpler method were used for the factual queries.</p>
<p dir="auto">Following this argument, we found that we could boost the performance on MMLU by extending MedPrompt with a simple two-method prompt portfolio. We add to the classic Medprompt a set of 10 simple, direct few-shot prompts soliciting an answer directly without Chain of Thought. We then ask GPT-4 for help with deciding on the best strategy for each topic area and question. As a screening call, for each question we first ask GPT-4:</p>
<div data-snippet-clipboard-copy-content="# Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No"><pre><code># Question
{{ question }}
 
# Task
Does answering the question above require a scratch-pad?
A. Yes
B. No
</code></pre></div>
<p dir="auto">If GPT-4 thinks the question does require a scratch-pad, then the contribution of the Chain-of-Thought component of the ensemble is doubled. If it doesn't, we halve that contribution (and let the ensemble instead depend more on the direct few-shot prompts). Dynamically leveraging the appropriate prompting technique in the ensemble led to a further +0.5% performance improvement across the MMLU.</p>
<p dir="auto">We note that Medprompt+ relies on accessing confidence scores (logprobs) from GPT-4. These are not publicly available via the current API but will be enabled for all in the near future.</p>
<h2 tabindex="-1" dir="auto">Running Scripts</h2>
<blockquote>
<p dir="auto">Note: Some scripts hosted here are published for reference on methodology, but may not be immediately executable against public APIs. We're working hard on making the pipelines easier to run "out of the box" over the next few days, and appreciate your patience in the interim!</p>
</blockquote>
<p dir="auto">First, clone the repo and install the promptbase package:</p>

<p dir="auto">Next, decide which tests you'd like to run. You can choose from:</p>
<ul dir="auto">
<li>bigbench</li>
<li>drop</li>
<li>gsm8k</li>
<li>humaneval</li>
<li>math</li>
<li>mmlu</li>
</ul>
<p dir="auto">Before running the tests, you will need to download the datasets from the original sources (see below) and place them in the <code>src/promptbase/datasets</code> directory.</p>
<p dir="auto">After downloading datasets and installing the promptbase package, you can run a test with:</p>
<p dir="auto"><code>python -m promptbase dataset_name</code></p>
<p dir="auto">For example:</p>
<p dir="auto"><code>python -m promptbase gsm8k</code></p>
<h2 tabindex="-1" dir="auto">Dataset Links</h2>
<p dir="auto">To run evaluations, download these datasets and add them to /src/promptbase/datasets/</p>
<ul dir="auto">
<li>MMLU: <a href="https://github.com/hendrycks/test">https://github.com/hendrycks/test</a></li>
<li>HumanEval: <a href="https://huggingface.co/datasets/openai_humaneval" rel="nofollow">https://huggingface.co/datasets/openai_humaneval</a></li>
<li>DROP: <a href="https://allenai.org/data/drop" rel="nofollow">https://allenai.org/data/drop</a></li>
<li>GSM8K: <a href="https://github.com/openai/grade-school-math">https://github.com/openai/grade-school-math</a></li>
<li>MATH: <a href="https://huggingface.co/datasets/hendrycks/competition_math" rel="nofollow">https://huggingface.co/datasets/hendrycks/competition_math</a></li>
<li>Big-Bench-Hard: <a href="https://github.com/suzgunmirac/BIG-Bench-Hard">https://github.com/suzgunmirac/BIG-Bench-Hard</a>
The contents of this repo need to be put into a directory called <code>BigBench</code> in the <code>datasets</code> directory</li>
</ul>
<h2 tabindex="-1" dir="auto">Other Resources:</h2>
<p dir="auto">Medprompt Blog: <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/</a></p>
<p dir="auto">Medprompt Research Paper: <a href="https://arxiv.org/abs/2311.16452" rel="nofollow">https://arxiv.org/abs/2311.16452</a></p>
<p dir="auto">Medprompt+: <a href="https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/" rel="nofollow">https://www.microsoft.com/en-us/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/</a></p>
<p dir="auto">Microsoft Introduction to Prompt Engineering: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering</a></p>
<p dir="auto">Microsoft Advanced Prompt Engineering Guide: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions" rel="nofollow">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BrainGPT turns thoughts into text (323 pts)]]></title>
            <link>https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</link>
            <guid>38673854</guid>
            <pubDate>Sun, 17 Dec 2023 16:22:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054">https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054</a>, See on <a href="https://news.ycombinator.com/item?id=38673854">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><header><div><p><img alt="clock" title="clock" width="16" height="16" src="https://assets.iflscience.com/svg/_clock.svg"><span>PUBLISHED</span><time data-published-date="2023-12-17T09:56:04Z" date-time="2023-12-17T09:56:04Z"></time></p></div><h2>New Mind-Reading "BrainGPT" Turns Thoughts Into Text On Screen</h2><h2>It offers new hope to people unable to communicate in other ways.</h2></header><figure><picture><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.webp" type="image/webp"><source media="(min-width: 1000px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-l.jpg" type="image/jpeg"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.webp" type="image/webp"><source media="(max-width: 567px)" srcset="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" type="image/jpeg"><img src="https://assets.iflscience.com/assets/articleNo/72054/aImg/72811/braingpt-m.jpg" alt="Thought signals in the brain." title="&quot;BrainGPT&quot;" type="image/jpeg" loading="lazy"></picture><figcaption><p>The researchers can decode information in the brain without invasive technology.&nbsp;</p><p>Image Credit: Chaikom/Shutterstock.com</p></figcaption></figure><article><div><p id="isPasted">“Mind reading” may be about to become a reality – and in the most literal sense possible, as a new breakthrough from researchers at the University of Technology Sydney’s GrapheneX-UTS Human-centric Artificial Intelligence Centre sees thoughts transformed into words on a screen.</p><p>“This research represents a pioneering effort in translating raw EEG waves directly into language, marking a significant breakthrough in the field,” <a href="https://www.uts.edu.au/news/tech-design/portable-non-invasive-mind-reading-ai-turns-thoughts-text" target="_blank">said</a> <a href="https://profiles.uts.edu.au/Chin-Teng.Lin" target="_blank" rel="noopener noreferrer">Ching-Ten Lin</a>, Distinguished Professor at the UTS School of Computer Science and Director of the GrapheneX-UTS HAI Centre.</p><p>“It is the first to incorporate discrete encoding techniques in the brain-to-text translation process, introducing an innovative approach to neural decoding,” Lin, who led the research, explained. “The integration with large language models is also opening new frontiers in neuroscience and AI.”</p><p>In a study that has been selected as a spotlight paper at the NeurIPS conference, an annual meeting of researchers in artificial intelligence and machine learning, participants silently read passages of text while an AI model called DeWave – using only their brainwaves as input – projected those words onto a screen.&nbsp;</p><p><span contenteditable="false" draggable="true"><iframe width="560" height="315" src="https://www.youtube.com/embed/crJst7Yfzj4?si=pyhjs0xS8pSsRE65" title="YouTube video player" frameborder="0" allowfullscreen=""></iframe></span><br></p><p>While it’s <a href="https://www.iflscience.com/ai-brain-activity-decoder-can-translate-thoughts-into-written-words-68686" target="_blank">not the first</a> technology to be able to <a href="https://www.iflscience.com/write-book-your-mind-29055" target="_blank">translate brain signals into language</a>, it’s the only one so far to require neither <a href="https://www.iflscience.com/scientists-peek-inside-a-persons-minds-eye-by-reading-their-brain-waves-63037" target="_blank">brain implants</a> nor access to <a href="https://www.iflscience.com/new-brain-scanning-algorithm-can-read-your-thoughts-65945" target="_blank">a full-on MRI machine</a>. It also has an edge on predecessors that require additional input such as eye-tracking software, the researchers say, as the new technology can be used with or without such extras.</p><p>Instead, users need only to wear a cap that records their brain activity via electroencephalogram (EEG) – much more practical and convenient than an eye-tracker (not to mention an MRI machine). That meant the signal was a bit noisier than information gained from implants, the researchers admitted – though even then, the tech performed pretty well in trials. Accuracy measurements using the BLEU algorithm – a way to evaluate the similarity of an original text to a machine-translated output by giving it a score between 0 and 1 – put the new tech at about 0.4.&nbsp;</p><p>That, admittedly, isn’t as good as some of the other options that depend on these more invasive methods. “The model is more adept at matching verbs than nouns,” explained Yiqun Duan, first author on the paper accompanying the research – and “when it comes to nouns, we saw a tendency towards synonymous pairs rather than precise translations, such as ‘the man’ instead of ‘the author’.”&nbsp;</p><p>“We think [these errors are] because when the brain processes these words, semantically similar words might produce similar brain wave patterns,” Duan said.&nbsp;</p><p>But the researchers believe they can improve this accuracy up to 0.9 – a level comparable with traditional language translation programs. They already have an advantage, they suspect, due to carrying out their tests on 29 participants – it may not sound like a lot, but it’s an order of magnitude higher than many other decoding tech trials.</p><p>“Despite the challenges, our model yields meaningful results,” Duan said, “aligning keywords and forming similar sentence structures.”</p><p>The results were shown at the <a href="https://neurips.cc/virtual/2023/index.html" target="_blank">NeurIPS conference</a> and a preprint can be found on <a href="https://arxiv.org/abs/2309.14030v2" target="_blank">ArXiV</a>. It has yet to be peer reviewed.</p></div></article><div><hr><div><h3>ARTICLE POSTED IN</h3></div></div></section><br><div><header><img alt="technology" title="technology" width="16" height="16" src="https://assets.iflscience.com/svg/label/_technology.svg"><h2>More Technology Stories</h2></header></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US nuclear-fusion lab enters new era: achieving 'ignition' over and over (181 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-04045-8</link>
            <guid>38673654</guid>
            <pubDate>Sun, 17 Dec 2023 15:55:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-04045-8">https://www.nature.com/articles/d41586-023-04045-8</a>, See on <a href="https://news.ycombinator.com/item?id=38673654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>In December 2022, after <a href="https://www.nature.com/articles/d41586-022-02022-1" data-track="click" data-label="https://www.nature.com/articles/d41586-022-02022-1" data-track-category="body text link">more than a decade of effort and frustration</a>, scientists at the US National Ignition Facility (NIF) <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-label="https://www.nature.com/articles/d41586-022-04440-7" data-track-category="body text link">announced that they had set a world record</a> by producing a fusion reaction that released more energy than it consumed — a phenomenon known as ignition. They have now proved that the feat was no accident by replicating it again and again, and the administration of US President Joe Biden is looking to build on this success by establishing a trio of US research centres to help advance the science.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26526928.jpg"><p>Nuclear-fusion breakthrough: this physicist helped to achieve the first-ever energy gain</p></a>
 </article><p>The stadium-sized laser facility, housed at the Lawrence Livermore National Laboratory (LLNL) in California, has unequivocally achieved its goal of ignition in four out of its last six attempts, creating a reaction that generates pressures and temperatures greater than those that occur inside the Sun.</p><p>“I’m feeling pretty good,” says Richard Town, a physicist who heads the lab’s inertial-confinement fusion science programme at the LLNL. “I think we should all be proud of the achievement.”</p><p>The NIF was designed not as a power plant, but as a facility to recreate and study the reactions that occur during thermonuclear detonations after the United States halted underground weapons testing in 1992. The higher fusion yields are already being used to advance nuclear-weapons research, and have also fuelled enthusiasm about fusion as a limitless source of clean energy. US secretary of state John Kerry called for new international partnerships to advance fusion energy at the COP28 climate summit in Dubai last week, and the US Department of Energy (DOE), which oversees the NIF, followed up by announcing the new research hubs, to be led by Lawrence Livermore, the University of Rochester in New York and Colorado State University in Fort Collins.</p><p>Building the NIF was “a leap of faith” for many, and its success has had a real impact on the fusion community, as well as on public perception, says Saskia Mordijck, a physicist at the College of William and Mary in Willamsburg, Virginia. “In that sense, what is important is that scientists said they could do something, and then they actually did do something.”</p><h2>Hot shots</h2><p>The NIF works by firing 192 laser beams at a frozen pellet of the hydrogen isotopes deuterium and tritium that is housed in a diamond capsule suspended inside a gold cylinder. The resulting implosion causes the isotopes to fuse, creating helium and copious quantities of energy. On 5 December 2022, those fusion reactions for the first time generated more energy — roughly 54% more — than the laser beams delivered to the target.</p><p>The facility set a new record on 30 July when its beams delivered the same amount of energy to the target — 2.05 megajoules — but, this time, the implosion generated 3.88 megajoules of fusion energy, an 89% increase over the input energy. Scientists at the laboratory achieved ignition during two further attempts in October (see ‘A year of progress’). And the laboratory’s calculations suggest that two others in June and September generated slightly more energy than the lasers provided, but not enough to confirm ignition.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A year of progress: Timeline of 'ignition' experiments conducted by the US National Ignition Facility since December 2022." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_26532550.jpg">
  <figcaption>
   <p><span>Source: Lawrence Livermore National Laboratory</span></p>
  </figcaption>
 </picture>
</figure><p>For many scientists, the results confirm that the laboratory is now operating in a new regime: researchers can repeatedly hit a goal they’ve been chasing for more than a decade. Tiny variations in the laser pulses or minor defects in the diamond capsule can still allow energy to escape, making for an imperfect implosion, but the scientists now better understand the main variables at play and how to manipulate them.</p><p>“Even when we have these issues, we can still get more than a megajoule of fusion energy, which is good,” says <a href="https://www.nature.com/articles/d41586-023-03923-5" data-track="click" data-label="https://www.nature.com/articles/d41586-023-03923-5" data-track-category="body text link">Annie Kritcher</a>, the NIF’s lead designer on this series of experiments.</p><h2>New hubs</h2><p>It’s a long way from there to providing fusion energy to the power grid, however, and the NIF, although currently home to the world’s largest laser, is not well-suited for that task. The facility’s laser system is enormously inefficient, and more than 99% of the energy that goes into a single ignition attempt is lost before it can reach the target.</p><p>Developing more efficient laser systems is one goal of the DOE’s new inertial-fusion-energy research programme. This month, the agency announced US$42 million over four years to establish three new research centres — each involving a mix of national laboratories, university researchers and industry partners — that will work towards this and other advances.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-04440-7" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-04045-8/d41586-023-04045-8_25275218.jpg"><p>Nuclear-fusion lab achieves ‘ignition’: what does it mean?</p></a>
 </article><p>This investment is the first coordinated effort to develop not just the technologies, but also the workforce for a future laser-fusion industry, says Carmen Menoni, a physicist who is heading up the hub at Colorado State University.</p><p>So far, most government investments in fusion-energy research have gone towards devices known as tokamaks, which use magnetic fields inside a doughnut-shaped ‘torus’ to confine fusion reactions. This is the approach under development at ITER, an international partnership to build the world’s largest fusion facility near Saint-Paul-lez-Durance, France. Tokamaks have also been the focus of many fusion investments in the private sector, but dozens of companies are pursuing other approaches, such as laser fusion.</p><p>The timing for a dedicated laser-fusion programme is right, says Menoni, and the decision to pursue it wouldn’t have happened without the NIF’s recent success. “We now know it will work,” she says. “What will take time is to develop the technology to a level where we can build a power plant.”</p><p>Back at the NIF, Kritcher’s latest series of experiments features a 7% boost in laser energy, which should, in theory, lead to even larger yields. The first experiment in this series was one of the successful ignitions, on 30 October. Although it didn’t break the record, an input of 2.2 megajoules of laser energy yielded an output of 3.4 megajoules of fusion energy.</p><p>Kritcher chalks up the fact that it didn’t break the record for energy yield to growing pains with the new laser configuration, which is designed to squeeze more energy into the same gold cylinder. Before moving to a larger cylinder, Kritcher says her team is going to focus on changes to the laser pulse that could produce a more symmetrical implosion. “We’ve got four experiments next year,” she says. “Let’s see.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Final Speech from The Great Dictator (494 pts)]]></title>
            <link>https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</link>
            <guid>38673292</guid>
            <pubDate>Sun, 17 Dec 2023 15:03:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-">https://www.charliechaplin.com/en/articles/29-the-final-speech-from-the-great-dictator-</a>, See on <a href="https://news.ycombinator.com/item?id=38673292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>




<p>
  <iframe width="640" height="480" src="https://www.youtube.com/embed/J7GY1Xg6X20" frameborder="0" allowfullscreen=""></iframe>
</p>


<p>I’m sorry, but I don’t want to be an emperor. That’s not my business. I don’t want to rule or conquer anyone. I should like to help everyone - if possible - Jew, Gentile - black man - white. We all want to help one another. Human beings are like that. We want to live by each other’s happiness - not by each other’s misery. We don’t want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.</p>

<p>Greed has poisoned men’s souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness, hard and unkind. We think too much and feel too little. More than machinery we need humanity. More than cleverness we need kindness and gentleness. Without these qualities, life will be violent and all will be lost…</p>

<p>The aeroplane and the radio have brought us closer together. The very nature of these inventions cries out for the goodness in men - cries out for universal brotherhood - for the unity of us all. Even now my voice is reaching millions throughout the world - millions of despairing men, women, and little children - victims of a system that makes men torture and imprison innocent people.</p>

<figure><img src="https://photo.charliechaplin.com/images/photos/0000/0225/gd_p_140_big.jpg" alt=""><figcaption></figcaption></figure>

<p>To those who can hear me, I say - do not despair. The misery that is now upon us is but the passing of greed - the bitterness of men who fear the way of human progress. The hate of men will pass, and dictators die, and the power they took from the people will return to the people. And so long as men die, liberty will never perish…</p>

<p>Soldiers! don’t give yourselves to brutes - men who despise you - enslave you - who regiment your lives - tell you what to do - what to think and what to feel! Who drill you - diet you - treat you like cattle, use you as cannon fodder. Don’t give yourselves to these unnatural men - machine men with machine minds and machine hearts! You are not machines! You are not cattle! You are men! You have the love of humanity in your hearts! You don’t hate! Only the unloved hate - the unloved and the unnatural! Soldiers! Don’t fight for slavery! Fight for liberty!</p>

<p>In the 17th Chapter of St Luke it is written: “the Kingdom of God is within man” - not one man nor a group of men, but in all men! In you! You, the people have the power - the power to create machines. The power to create happiness! You, the people, have the power to make this life free and beautiful, to make this life a wonderful adventure.</p>

<p>Then - in the name of democracy - let us use that power - let us all unite. Let us fight for a new world - a decent world that will give men a chance to work - that will give youth a future and old age a security. By the promise of these things, brutes have risen to power. But they lie! They do not fulfil that promise. They never will!</p>

<p>Dictators free themselves but they enslave the people! Now let us fight to fulfil that promise! Let us fight to free the world - to do away with national barriers - to do away with greed, with hate and intolerance. Let us fight for a world of reason, a world where science and progress will lead to all men’s happiness. Soldiers! in the name of democracy, let us all unite!</p>

<p><em>Final speech from The Great Dictator Copyright © Roy Export S.A.S. All rights reserved</em></p>

<hr>

<p><a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a> was Chaplin’s first film with dialogue.    Chaplin plays both a little Jewish barber, living in the ghetto, and Hynkel, the dictator ruler of Tomainia. In his autobiography Chaplin quotes himself as having said: “One doesn’t have to be a Jew to be anti Nazi.  All one has to be is a normal decent human being.”</p>

<p>Chaplin and Hitler were born within a week of one another.  “There was something uncanny in the resemblance between the Little Tramp and Adolf Hitler, representing opposite poles of humanity, ” writes Chaplin biographer David Robinson, reproducing an unsigned article from The Spectator dated 21st April 1939:<br>
“Providence was in an ironical mood when, fifty years ago this week, it was ordained that Charles Chaplin and Adolf Hitler should make their entry into the world within four days of each other….Each in his own way has expressed the ideas, sentiments, aspirations of the millions of struggling citizens ground between the upper and the lower millstone of society. (…) Each has mirrored the same reality – the predicament of the “little man” in modern society.  Each is a distorting mirror, the one for good, the other for untold evil.”</p>

<p>Chaplin spent many months drafting and re-writing the speech for the end of the film, a call for peace from the barber who has been mistaken for Hynkel. Many people criticized the speech, and thought it was superfluous to the film.  Others found it uplifting. Regrettably Chaplin’s words are as relevant today as they were in 1940.</p>

<h4 id="transcript-of-charlie-chaplins-final-speech-in-the-great-dictatorenfilms7-the-great-dictator">Transcript of Charlie Chaplin’s Final Speech in <a href="https://www.charliechaplin.com/en/films/7-The-Great-Dictator">The Great Dictator</a>
</h4>



</article>
</div></div>]]></description>
        </item>
    </channel>
</rss>