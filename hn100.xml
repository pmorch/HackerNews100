<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 04 Jun 2025 19:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[We Are No Longer a Serious Country – Paul Krugman (104 pts)]]></title>
            <link>https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</link>
            <guid>44182634</guid>
            <pubDate>Wed, 04 Jun 2025 16:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country">https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</a>, See on <a href="https://news.ycombinator.com/item?id=44182634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“If you’re explaining, you’re losing.” This line is usually attributed to Ronald Reagan. Whoever said it definitely had a point, and not just about politics. If you’re trying to explain to people, be they voters or bond investors, that you aren’t really as bad or untrustworthy as you seem, you’re already in deep trouble.</p><p><span>So when I saw Scott Bessent, the treasury secretary, </span><a href="https://www.ft.com/content/9c652f09-cfca-4078-9e2b-5fdacd772a75" rel="">declaring</a><span> Sunday that “The United States of America is never going to default, that is never going to happen,” my reaction was, “Uh-oh.”</span></p><p>And it’s not just me. For generations investors have treated U.S. government debt as the ultimate safe asset. Whenever disaster strikes — even if it’s disaster largely made in America, like the 2008 subprime crisis — bond buyers pile into U.S. Treasuries, because America is a serious country, and the idea that we would fail to honor our debts was unthinkable.</p><p>But are we still that country? Markets seem to have doubts.</p><p><span>Yesterday the Financial Times had a neat chart showing that there used to be a clear relationship between U.S. interest rates and the international value of the dollar. Actually, the chart was a bit </span><em>too</em><span> neat: When I set out to reproduce it, I found that the FT chose a time period during which the relationship looked especially clear. Still, it used to be true that when U.S. interest rates rose, so did the dollar, because higher yields pulled in foreign capital. But since Donald Trump returned to power, that relationship has broken down. Instead, we’ve seen a combination of rising interest rates and a </span><em>falling</em><span> dollar:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75158,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>As many have noted, what we’ve been seeing in recent months, with interest rates and the dollar moving in opposite directions, doesn’t look like what we normally see in the United States, or for that matter advanced nations in general. Instead, it’s the kind of thing one sees in emerging markets, where big market moves often reflect crises of confidence: International investors lose faith, pulling their money out, and capital flight causes both a falling currency and rising interest rates.</p><p>Here, for example, is what Mexico looked like during the “tequila crisis” of 1994-5, which involved both soaring interest rates and a plunging peso:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Why are markets beginning to treat America as unreliable? It’s not just the debt numbers. Yes, we have large debts, but we’re an immensely wealthy country that, among other things, has lower average taxes than most of our peers. So we certainly have the resources to honor our debts.</p><p><span>But do we have the political will? Maybe even more important, do we have the political </span><em>seriousness</em><span>?</span></p><p>Like many economists, I’ve spent a lot of time analyzing the substance of Trump’s tariffs — how much they are likely to raise prices and reduce trade volumes. I’ve also written about the impacts of policy uncertainty, about how hard it is for businesses to make plans when they have little idea what tariff rates will be even a few months from now, let alone over the next few years.</p><p><span>But I wonder whether we’ve spent enough time looking at the policy </span><em>process</em><span> — how decisions get made in Trump’s America. Consider: On April 2, “Liberation Day,” Trump announced extremely high tariffs on many countries, the biggest tariff hike in U.S. history. The tariff rates — which differed hugely from country to country — were determined by a formula universally panned as stupid and ridiculous. And this tariff announcement was made with so little planning and forethought that it included taxes on imports from remote islands inhabited only by </span><a href="https://www.bbc.com/news/articles/cly8xlj0485o" rel="">penguins</a><span>.</span></p><p><span>Then, a week later, these tariffs were replaced by a completely different set of tariffs. How did that happen? Two of Trump’s cabinet members were able to beard him in the Oval Office while Peter Navarro, responsible for the original tariffs, was </span><a href="https://www.wsj.com/politics/policy/trump-tariff-pause-navarro-bessent-lutnick-b9e864fb?gaa_at=eafs&amp;gaa_n=ASWzDAiBOSVk0wHQwGquJzeffoF7WWhmHKUg490AKNKAI7beYwEiwJ8au1jW-gBGH4A%3D&amp;gaa_ts=683dcdff&amp;gaa_sig=xIdEPFjIsG6tDwLXo4RJ5eBSP5faPCp3McPx6CkxYZ4dSRr3EJUr_ysQIYStqcwE5VL_0ZhadzjRqX58YVM2eA%3D%3D" rel="">in another meeting</a><span>.</span></p><p>Does this sound like policymaking in a serious country?</p><p>Then there’s the budget bill making its way through Congress. It’s a terrible thing, imposing savage cuts on social programs (and decimating U.S. science) while giving such big tax cuts to the wealthy that it will explode the deficit. But content aside, notice that this hugely important piece of legislation is being rushed through with essentially no hearings or analysis.</p><p>And when outsiders, including the Congressional Budget Office and a variety of think tanks — conservative and centrist as well as progressive — have put out the analyses the bill’s sponsors won’t, pointing out the likely effects on debt, health coverage, and so on, the G.O.P. response has basically been to accuse all of the independent analysts of being part of a globalist conspiracy.</p><p><span>Wait, it gets worse. The name of the legislation — not its nickname, its official title — is the </span><a href="https://www.congress.gov/bill/119th-congress/house-bill/1/text" rel="">One Big Beautiful Bill Act</a><span>, because that’s what Trump has been calling it. Are we a mature republic with a normal head of state, or are we being ruled by Kim Jong Un in orange makeup?</span></p><p><span>Why, next thing you’ll be telling me that key policy decisions, leading to layoffs of hundreds of thousands of federal workers and many deaths around the world, have been made by a presidential crony whose erratic behavior may have reflected massive consumption of ketamine, Ecstasy and psychedelic mushrooms. </span><a href="https://www.nytimes.com/2025/05/30/us/elon-musk-drugs-children-trump.html" rel="">Oh, wait</a><span>.</span></p><p><span>Imagine yourself as a foreigner considering investing in the United States. You may well know that the One Big Beautiful Bill Act contains a “</span><a href="https://www.bloomberg.com/news/articles/2025-05-30/trump-revenge-tax-would-lower-foreign-investment-in-us-scorekeeper-predicts?sref=qzusa8bC" rel="">revenge</a><span>” provision that would allow the U.S. government to impose extra taxes on foreign investors whose home countries have policies America doesn’t like. You probably know that one of Trump’s advisers has suggested the forced conversion of short-term debt into </span><a href="https://thedailyeconomy.org/article/turning-treasury-securities-into-century-bonds-is-a-dead-end/" rel="">century bonds</a><span>. Once upon a time everyone would have dismissed these things as stuff that couldn’t happen in America. Now? Who knows?</span></p><p>In a way, the amazing thing is that we haven’t seen even more capital flight. Presumably investors still can’t believe that America has changed so much from the responsible, reliable nation it seemed to be just a few months ago.</p><p>But I think they’re headed for a rude awakening.</p><p>MUSICAL CODA</p><div id="youtube2-pvlbrKKEPdA" data-attrs="{&quot;videoId&quot;:&quot;pvlbrKKEPdA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/pvlbrKKEPdA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VC money is fueling a global boom in worker surveillance tech (137 pts)]]></title>
            <link>https://restofworld.org/2025/employee-surveillance-software-vc-funding/</link>
            <guid>44182582</guid>
            <pubDate>Wed, 04 Jun 2025 16:35:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/employee-surveillance-software-vc-funding/">https://restofworld.org/2025/employee-surveillance-software-vc-funding/</a>, See on <a href="https://news.ycombinator.com/item?id=44182582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Technologies that promise to track, manage, and supervise workers, increasingly using artificial intelligence, are getting entrenched in the developing world, according to a new <a href="https://home.coworker.org/little-tech-goes-global/">report</a> by Coworker.org, a labor rights nonprofit based in New York.&nbsp;</p>



<p>Audits of more than 150 startups and regional companies based in Kenya, Nigeria, Colombia, Brazil, Mexico, and India showed workplace surveillance is expanding in scale and sophistication, the researchers said. While large <a href="https://www.computerweekly.com/news/252521757/Microsoft-Office-365-has-ability-to-spy-on-workers">corporations</a> are <a href="https://www.cisco.com/site/us/en/learn/topics/general/what-is-employee-monitoring.html">known</a> to develop surveillance technologies, a so-called Little Tech ecosystem of mostly unregulated, venture capital-funded startups and small vendors making these products has grown since Covid-19, the report found. The term “Little Tech” was popularized by the VC firm <a href="https://a16z.com/the-little-tech-agenda/">Andreessen Horowitz</a>, which argued that excessive regulation was stifling innovation.</p>



<figure></figure>



<p><a href="https://restofworld.org/2022/workers-managed-algorithms/">Algorithmic management</a> and surveillance tools are getting even more intrusive in gig work, and are entering offices and the informal labor sector as well, Wilneida Negrón, director of research and policy at Coworker.org and a co-author of the report, told <em>Rest of World</em>.&nbsp;</p>



<p>“The pressure of the hyper-surveillance creates a lot of stress and creates a lot of uncertainty for workers. It brings a culture of suspiciousness,” she said.&nbsp;</p>



<p>Investments by Silicon Valley-based VC firms led to a boom in tech startups globally after Covid-19, Negrón said. This has carried over to companies building bossware products in the developing world, she said.&nbsp;&nbsp;</p>



<figure></figure>



<p>The technologies include biometric tracking, AI-powered productivity monitoring, and predictive analytics, the report found. Worker data is continuously collected and analyzed by algorithms with the stated aim to improve hiring, evaluate performance, and optimize processes.&nbsp;</p>



<p>Most managers in wealthier nations say algorithmic management tools improve their decision-making, according to a 2024 <a href="https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/algorithmic-management-in-the-workplace_3c84ed6d/287c13c4-en.pdf?utm_source=chatgpt.com">survey</a> of over 6,000 employers by the Organisation for Economic Co-operation and Development. More than 90% of American managers used such tools, especially to reward or sanction employees.</p>



<p>Many tools are first deployed in Latin America, where labor laws are less strictly enforced, according to Ayden Férdeline, a tech policy researcher in Berlin and a co-author of the report.</p>



<p>“There is a Latin America testing ground for products,” he told <em>Rest of World</em>. “If they are successful, they tend to be deployed in other jurisdictions, oftentimes with additional safeguards, sometimes not.”</p>



<p>Many workers are unaware of how their information is collected and used, Férdeline said.&nbsp;</p>



<p>Some gig workers in Kenya, Guatemala, and Brazil said bossware tools make them feel surveilled, and that they have less control over their work. In Porto Alegre, Brazil, Uber driver Carina Trindade told <em>Rest of World</em> she feels the app monitors her continuously, tracking her speed and braking patterns. The app has permissions <a href="https://dig.watch/updates/uber-starts-testing-tool-records-video-inside-car-during-rides">to access</a> her mic and camera, she said.&nbsp;</p>



<p>Uber spokesperson Gabriel Gabira said drivers have the option to record trips, and <a href="https://help.uber.com/en/driving-and-delivering/article/record-my-ride---frequently-asked-questions?nodeId=7f86cc51-2c3e-4888-8dc1-0ac2b6337b92">privacy terms</a> are followed to access the footage.</p>



<p>In Nairobi, Godfrey Sanya Wanga, a driver for ride-hailing firm SafeBoda, told <em>Rest of World </em>he felt the app undercharged a customer. “I really wanted to ask [the customer] to pay me more, but I remembered that I was being monitored and this would bring me trouble if the client reported me,” he said. SafeBoda did not respond to a request for comment.</p>



<p>Several nations have data protection and privacy laws, including Brazil, Nigeria, and Kenya. But enforcement is inconsistent, the report said.</p>



<p>Here are five current uses of algorithmic management tools. The companies mentioned below did not comment, unless otherwise stated.&nbsp;</p>



<p>1. <strong>Timekeeping and attendance systems&nbsp;</strong></p>



<p>What: Platforms that track the attendance of workers, often using geolocation and biometrics to verify presence.&nbsp;</p>



<p>Example: <a href="https://www.rankmi.com/es/productos/control-asistencia-y-turnos">Rankmi</a>, based in Chile, uses biometrics and geolocation to track workers. The platform also <a href="https://www.rankmi.com/es/ia-para-hr">gives</a> workers continuous performance feedback and evaluates job applicants using AI.&nbsp;</p>



<p>2. <strong>Biometric and identity verification tools</strong></p>



<p>What: Tools that use fingerprint and facial-recognition checks, special digital signatures stored on a secure network, and official records to confirm a worker’s identity before granting access.&nbsp;</p>



<p>Example: Cincel, based in Mexico, provides <a href="https://www.cincel.digital/productos/verificacion-identidad/">identity verification</a> tools that do various checks including biometrics, and also cross-check against government databases and <a href="https://www.cincel.digital/productos/background-check/">blacklists</a>.</p>



<p>3. <strong>Performance and productivity monitoring platforms</strong></p>



<p>What: Dashboards that score workers using tracked metrics such as keystrokes, transaction counts, customer interactions, and task completion times.</p>



<p>Example: <a href="https://www.ahgora.com/">Ahgora</a>, based in Brazil, offers HR software that allows managers to continually “oversee team attendance in real-time” and that tracks productivity. It uses the data to offer predictions about work, such as potential issues with attendance, which can inform decision-making.&nbsp;</p>



<p>4. <strong>Algorithmic management and predictive analytics</strong></p>



<p>What: Platforms that<strong> </strong>automate HR functions, such as hiring shortlists, performance reviews, attrition forecasting, and also unionization-risk scoring.</p>



<p>Example: Visier’s AI-powered analytics platform <a href="https://www.visier.com/solutions/internal-mobility/">analyzes</a> HR data and provides insights, including resignation risk. The platform is used by global firms including Deloitte, Accenture, and Tata Consultancy Services.&nbsp;</p>



<p>Andrea Derler, principal of research and customer value at Visier, told <em>Rest of World</em> the platform only “processes data that organizations load into the platform, and we are not responsible for the way the data and insights we help provide is being used.”&nbsp;</p>



<p>5.&nbsp; <strong>Gig economy and field workforce tracking</strong></p>



<p>What: Apps that use the workers’ smartphones to dispatch and route deliveries. They use location, trip history, and ratings to allocate jobs and <a href="https://restofworld.org/2021/only-gojek-knows-this-mystery/">evaluate performance</a>. Workers are managed mostly by platforms rather than humans.</p>



<p>Example: <span><mark><a href="https://restofworld.org/tag/rappi" aria-label="Click to learn more about Rappi.">Rappi<span>i</span></a></mark><span><span><a href="https://restofworld.org/tag/rappi">Rappi</a><svg aria-label="Close" role="button"><use xlink:href="#X"></use></svg></span>Rappi, a Colombian company, has been providing delivery services across most Latin American countries since 2015.<span><a href="https://restofworld.org/tag/rappi"><span>READ MORE</span><svg><use xlink:href="#chevron"></use></svg></a></span></span></span>, a Colombian delivery app, tracks workers in real time. It has <a href="https://americasmi.com/insights/rappi-evolving-business-model-impact-latam-logistics/">auto accept</a>, where a rider can’t decline orders — and it’s mandatory to qualify for bonuses. Delivery worker Carolina Ramírez told <em>Rest of World </em>she works 14-hour days to earn a bonus of 100,000 pesos ($25) every week, leaving her little time for anything else. “My boss is the app. It’s unfair because to earn a good salary, I have to dedicate myself almost exclusively to this,” she said.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS Direct File on GitHub (229 pts)]]></title>
            <link>https://chrisgiven.com/2025/05/direct-file-on-github/</link>
            <guid>44182356</guid>
            <pubDate>Wed, 04 Jun 2025 16:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisgiven.com/2025/05/direct-file-on-github/">https://chrisgiven.com/2025/05/direct-file-on-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44182356">Hacker News</a></p>
Couldn't get https://chrisgiven.com/2025/05/direct-file-on-github/: Error: getaddrinfo ENOTFOUND chrisgiven.com]]></description>
        </item>
        <item>
            <title><![CDATA[Meta found 'covertly tracking' Android users through Instagram and Facebook (175 pts)]]></title>
            <link>https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</link>
            <guid>44182204</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083">https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</a>, See on <a href="https://news.ycombinator.com/item?id=44182204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-name="ui-article-body" data-highlight-intro="true">
      
      <p>Meta and search engine company Yandex have been "covertly tracking" Android users in the background of their devices, according to experts.</p><p>Academics at the Radboud University in the Netherlands and IMDEA Networks said they discovered Meta and Yandex have been tracking Android users' browser activity without their consent and then using the data in their apps.</p>
<p>Meta said it was looking into the issue, while Yandex denied collecting any sensitive data.</p><p>Gunes Acar, assistant professor at Radboud University, said the "covert" data collection was spotted in January.</p><p>He said he discovered Meta's apps, including Facebook and Instagram, and Yandex's apps, such as Yandex Maps, were sitting in the background of Android devices and loading a script that sent data locally back to apps on users' phones.</p>
<p>The scripts bypassed Android's security measures and meant that Meta and Yandex could track what users were doing on web browsers, without the user consenting or even knowing, according to the expert.</p><p>"They are bridging these two worlds that we think are separate; web browsing and mobile app activities," Dr Acar told Sky News.</p><p>"That's very shocking."</p><p>The apps were able to track users' browser data on all major Android browsers, even if the user was in incognito mode, the academics said.</p>        
<p>"It's really concerning because it negates every privacy control that you have in modern browsers and also in modern mobile platforms like Android," said Narseo Vallina-Rodriguez, associate professor at IMDEA Networks, to Sky News.</p><p><strong><a href="https://news.sky.com/topic/google-5876/1" target="_blank">Google</a></strong>, which owns the Android operating system, confirmed the covert activity to Sky News.</p><p>It said Meta and Yandex used <strong><a href="https://news.sky.com/topic/android-8090/1" target="_blank">Android's</a></strong> capabilities "in unintended ways that blatantly violate our security and privacy principles".</p><p><strong>What have Meta and Yandex said?</strong></p><p>Meta told Sky News it was quickly looking into the issue.</p><p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," said a Meta spokesperson.</p><p>"Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p><p>Yandex said it "strictly complies with data protection standards", adding: "The feature in question does not collect any sensitive information and is solely intended to improve personalisation within our apps."</p><p><strong>Read more science and tech news:<br><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">AI foot scanner recognises heart warning signs</a></strong><br><strong><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">Coffee 'helps women age more healthily'</a></strong></p>    
<p>Meta appeared to have been doing the data tracking for around eight months, while Yandex had since 2017, the academics said.</p><p>"We found that Facebook was doing it on roughly 16,000 websites when visited from the EU, [...] Yandex was doing this on 1,300 websites," said Tim Vlummens, a PHD student at KU Leuven who worked on the research.</p><p>Google told Sky News it had already "implemented changes to mitigate these invasive techniques and have opened our own investigation and are directly in touch with the parties".</p>     <a href="https://news.sky.com/download-app" target="blank" data-tracking-label="ui-app-promo-download-link" data-type="" data-component-name="ui-app-promo">
        
    </a>


<p>The tech giant did not respond when asked what repercussions Meta and Yandex were facing for their conduct.</p><p>Firefox, Microsoft Edge and DuckDuckGo browsers were also affected, with Firefox owner Mozilla and DuckDuckGo engineers taking action to stop any future covert tracking.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Merges WebRTC Support (273 pts)]]></title>
            <link>https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</link>
            <guid>44182186</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00">https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</a>, See on <a href="https://news.ycombinator.com/item?id=44182186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
avformat/whip:&nbsp;Add&nbsp;WHIP&nbsp;muxer&nbsp;support&nbsp;for&nbsp;subsecond&nbsp;latency&nbsp;streaming</p><p>

0.&nbsp;WHIP&nbsp;Version&nbsp;3.<br>
1.&nbsp;The&nbsp;WHIP&nbsp;muxer&nbsp;has&nbsp;been&nbsp;renamed&nbsp;and&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;improved&nbsp;logging&nbsp;context&nbsp;and&nbsp;error&nbsp;messages&nbsp;for&nbsp;SSL,&nbsp;DTLS,&nbsp;and&nbsp;RTC.<br>
2.&nbsp;Magic&nbsp;numbers&nbsp;have&nbsp;been&nbsp;replaced&nbsp;with&nbsp;macros&nbsp;and&nbsp;extracted&nbsp;to&nbsp;functions,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;log&nbsp;levels&nbsp;have&nbsp;been&nbsp;altered&nbsp;for&nbsp;better&nbsp;clarity.<br>
3.&nbsp;DTLS&nbsp;curve&nbsp;list&nbsp;has&nbsp;been&nbsp;updated,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;SRTP&nbsp;profile&nbsp;names&nbsp;have&nbsp;been&nbsp;refined&nbsp;for&nbsp;FFmpeg&nbsp;and&nbsp;OpenSSL.<br>
4.&nbsp;ICE&nbsp;STUN&nbsp;magic&nbsp;number&nbsp;has&nbsp;been&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;RTP&nbsp;payload&nbsp;types&nbsp;have&nbsp;been&nbsp;updated&nbsp;based&nbsp;on&nbsp;Chrome's&nbsp;definition.<br>
5.&nbsp;Fixed&nbsp;frame&nbsp;size&nbsp;has&nbsp;been&nbsp;refined&nbsp;to&nbsp;rtc-&gt;audio_par-&gt;frame_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;h264_mp4toannexb&nbsp;is&nbsp;now&nbsp;used&nbsp;to&nbsp;convert&nbsp;MP4/ISOM&nbsp;to&nbsp;annexb.<br>
6.&nbsp;OPUS&nbsp;timestamp&nbsp;issue&nbsp;has&nbsp;been&nbsp;addressed,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;marker&nbsp;setting&nbsp;has&nbsp;been&nbsp;corrected&nbsp;after&nbsp;utilizing&nbsp;BSF.<br>
7.&nbsp;DTLS&nbsp;handshake&nbsp;and&nbsp;ICE&nbsp;handling&nbsp;have&nbsp;been&nbsp;optimized&nbsp;for&nbsp;improved&nbsp;performance,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;single&nbsp;handshake&nbsp;timeout&nbsp;and&nbsp;server&nbsp;role&nbsp;to&nbsp;prevent&nbsp;ARQ.<br>
8.&nbsp;Consolidated&nbsp;ICE&nbsp;request/response&nbsp;handling&nbsp;and&nbsp;DTLS&nbsp;handshake&nbsp;into&nbsp;a&nbsp;single&nbsp;function,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;fixed&nbsp;OpenSSL&nbsp;build&nbsp;errors&nbsp;to&nbsp;work&nbsp;with&nbsp;Pion.<br>
9.&nbsp;Merge&nbsp;TLS&nbsp;&amp;&nbsp;DTLS&nbsp;implementation,&nbsp;shared&nbsp;BIO&nbsp;callbacks,&nbsp;read,&nbsp;write,<br>
&nbsp;&nbsp;&nbsp;&nbsp;print_ssl_error,&nbsp;openssl_init_ca_key_cert,<br>
&nbsp;&nbsp;&nbsp;&nbsp;init_bio_method&nbsp;function&nbsp;and&nbsp;shared&nbsp;same&nbsp;data&nbsp;structure<br>
10.&nbsp;Modify&nbsp;configure&nbsp;that&nbsp;whip&nbsp;is&nbsp;enabled&nbsp;only&nbsp;dtls&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;enabled(just&nbsp;support&nbsp;openssl&nbsp;for&nbsp;now)&nbsp;to&nbsp;fix&nbsp;build&nbsp;error</p><p>

<span>Co-authored-by: winlin &lt;winlinvip@gmail.com&gt;</span><br>
<span>Co-authored-by: yangrtc &lt;yangrtc@aliyun.com&gt;</span><br>
<span>Co-authored-by: cloudwebrtc &lt;duanweiwei1982@gmail.com&gt;</span><br>
<span>Co-authored-by: Haibo Chen &lt;495810242@qq.com&gt;</span><br>
<span>Co-authored-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span><br>
<span>Co-authored-by: Jun Zhao &lt;barryjzhao@tencent.com&gt;</span><br>
<span>Signed-off-by: Jack Lau &lt;jacklau1222@qq.com&gt;</span><br>
<span>Signed-off-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Right to Repair Is Law in Washington State (256 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</link>
            <guid>44181421</guid>
            <pubDate>Wed, 04 Jun 2025 15:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state">https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</a>, See on <a href="https://news.ycombinator.com/item?id=44181421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p>Thanks in part <a href="https://www.eff.org/deeplinks/2025/05/washingtons-right-repair-bill-heads-governor">to your support</a>, the right to repair is now law in Washington.</p>
<p>Gov. Bob Ferguson signed two bills guaranteeing Washingtonians' right to access tools, parts, and information so they can fix personal electronics, appliances, and wheelchairs. This is the epitome of common-sense legislation. When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>Advocates in Washington have worked for years to pass a strong right-to-repair law in the state. In addition to Washington’s <a href="https://pirg.org/washington/updates/washington-becomes-eighth-state-with-right-to-repair-law-on-the-books/">Public Interest Research Group</a>, the consumer electronics bill moved forward with a growing group of supporting organizations, including environmental advocates, consumer advocates, and manufacturers such as Google and Microsoft. Meanwhile, advocacy from groups including&nbsp; <a href="https://disabilityrightswa.org/">Disability Rights Washington</a>&nbsp;and the <a href="https://www.hereandnowproject.org/">Here and Now Project</a> made the case for the wheelchair's inclusion in the right-to-repair bill, bringing their personal stories to Olympia to show why this bill was so important.</p>
<p>And it’s not just states that recognize the need for people to be able to fix their own stuff.&nbsp; Earlier this month, U.S. Army Secretary Dan Driscoll <a href="https://media.defense.gov/2025/May/01/2003702281/-1/-1/1/ARMY-TRANSFORMATION-AND-ACQUISITION-REFORM.PDF">issued a memo</a> stating that the Army should “[identify] and propose contract modifications for right to repair provisions where intellectual property constraints limit the Army's ability to conduct maintenance and access the appropriate maintenance tools, software, and technical data – while preserving the intellectual capital of American industry.” The memo said that the Army should seek this in future procurement contracts and also to amend existing contracts to include the right to repair.</p>
<p>This is a bedrock of sound procurement with a long history in America. President Lincoln only bought rifles with standardized tooling to outfit the Union Army, for the obvious reason that it would be a little embarrassing for the Commander in Chief to have to pull his troops off the field because the Army’s sole supplier had decided not to ship this week’s delivery of ammo and parts. Somehow, the Department of Defense forgot this lesson over the ensuing centuries, so that today, billions of dollars in public money are spent on material and systems that the US military can only maintain by buying service from a “beltway bandit.”</p>
<p>This recognizes what millions of people have said repeatedly: limiting people’s ability to fix their own stuff stands in the way of needed repairs and maintenance. That’s true whether you’re a farmer with a broken tractor during harvest, a homeowner with a misbehaving washing machine or a cracked smartphone screen, a hospital med-tech trying to fix a ventilator, or a soldier struggling with a broken generator.</p>
<p>The right to repair is gaining serious momentum. All 50 states have now considered some form of right-to-repair legislation. Washington is the eighth state to pass one of these bills into law—let’s keep it up.</p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["AI Will Replace All the Jobs " Is Just Tech Execs Doing Marketing (161 pts)]]></title>
            <link>https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</link>
            <guid>44181172</guid>
            <pubDate>Wed, 04 Jun 2025 14:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/">https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</a>, See on <a href="https://news.ycombinator.com/item?id=44181172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
<p>Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn’t exist. Worse than that, actually, there’s hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;68405d5f71052&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="576" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-300x169.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-768x432.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p>I don’t want anyone to think I’m raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let’s walk through the logic together.</p>



<p>The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I’ll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model’s capability hasn’t done it yet (or won’t):</p>



<ul>
<li>If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies–i.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past?
<ul>
<li><a href="https://www.technologyreview.com/2024/01/27/1087041/technological-unemployment-elon-musk-jobs-ai/">MIT’s Technology Review</a> noted that this “fear of new tech taking jobs,” is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that “technological unemployment is a myth,” because “technology has created so many new industries” and has expanded the market by “lowering the cost of production to make a price within reach of large masses of purchasers.” In short, technological advances had created more jobs overall.</li>



<li>Last year, Quarterly Journal of Economics <a href="https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae008/7630187">published</a> a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice <a href="https://news.mit.edu/2024/does-technology-help-or-hurt-employment-0401">summarization</a>: “the number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.”</li>



<li><a href="https://www.sciencedirect.com/science/article/pii/S0040162523004353">This 2023 paper</a> looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that “the labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.”</li>



<li>The Economic Policy Institute <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">did a deep dive</a> into what drives labor market demand and unemployment, concluding: “Productivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,” and that “Anxieties over widespread technology-driven unemployment lack an empirical base.”</li>



<li>Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the <a href="https://www.mckinsey.com/featured-insights/future-of-work/what-can-history-teach-us-about-technology-and-jobs">McKinsey Global Institute concluded</a> in 2018 that “We tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it’s about 3.5 million,” but “When we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That’s about 10 percent of the civilian labor force today.”</li>
</ul>
</li>



<li>If AI is going to have these massive impacts but hasn’t yet, why not?
<ul>
<li>Folks who claim AI will destroy the labor market have claimed this radical change is “only a few years away,” “on the immediate horizon,” or “imminent,” for the last 5 years, yet we’re at historically low unemployment (yes, even accounting for <a href="https://www.fastcompany.com/91341084/functional-unemployment-what-it-means">underemployment</a> and the <a href="https://www.marketplace.org/story/2025/05/23/is-the-unemployment-rate-truly-capturing-whats-happening-in-the-labor-market">way the BLS counts employment</a>). The US labor market is within a single percentage point of its post-war unemployment low, measured in <a href="https://econofact.org/factbrief/did-us-unemployment-fall-to-the-lowest-rate-in-50-years-under-biden">1953 at 3.4%</a>.</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="1024" height="744" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-300x218.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-768x558.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1536x1116.png 1536w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-2048x1488.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<ul>
<li>If AI is killing jobs, it’s doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets?
<ul>
<li>The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) <a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">has driven</a> <a href="https://www.theatlantic.com/economy/archive/2025/04/job-market-youth/682641/?gift=o6MjJQpusU9ebnFuymVdsJ1qwI70CnAkjDXBfrYqvHw&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share">most of</a> <a href="https://news.ycombinator.com/item?id=43612448">the</a> <a href="https://www.zdnet.com/article/is-ai-making-it-harder-for-new-college-grads-to-get-hired-in-tech/">lower-than-pre-pandemic-demand</a> for <a href="https://www.techtarget.com/whatis/feature/Tech-sector-layoffs-explained-What-you-need-to-know">entry-level talent</a> (just as it has in times of inflation-fighting interest hikes of the past).</li>



<li>Machine-learning, the technology underpinning AI, <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">has been around for decades</a>, with <a href="https://www.dataversity.net/a-brief-history-of-machine-learning/">widespread adoption</a> in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI’s release of GPT-3 (longtime readers here will recall that <a href="https://britneymuller.com/">Britney Muller</a> showed off techniques <a href="https://www.linkedin.com/posts/britneymuller_bert-101-state-of-the-art-nlp-model-explained-activity-6907013731067523072-OXsd/">extremely similar</a> to what’s now associated with modern LLMs <a href="https://www.youtube.com/watch?v=47KHP3k2RPo">back in July 2018</a>).</li>



<li>We’ve had <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">15-20 years</a> of robust machine learning development and adoption, and another <a href="https://toloka.ai/blog/history-of-generative-ai/">5-10 years of broad LLM/generative AI adoption</a>, improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events.</li>



<li>The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">within 5 years</a>. AI’s slower changes bode poorly for the argument that it will have a larger impact than those events.</li>



<li>Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There <a href="https://www.reddit.com/r/programming/comments/1di8pe9/us_employment_of_software_developers_is_in/">was a greater loss</a> (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025.</li>



<li>I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a <a href="https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html">robust analysis of that site’s usage from 2008-2020</a> shows that “What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.”</li>
</ul>
</li>



<li>However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by <a href="https://substack.com/@pragmaticengineer">Gergely Orosz</a> concludes a <a href="https://newsletter.pragmaticengineer.com/p/software-engineering-job-openings">well-visualized, data-driven walkthrough</a> with: “LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.”</li>



<li>It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one’s starting point) is <a href="https://archive.ph/t8f9X">~10% variation in a job sector that employs ~1% of US workers</a>.</li>



<li>Assuming AI will have an effect similar to 20th Century farm equipment’s on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)?
<ul>
<li>This point is hard to find citations for, given that it’s a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960.</li>



<li>Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the <a href="https://www.google.com/search?q=ai+will+take+half+of+all+jobs&amp;tbm=nws">prognostications</a> by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation.</li>



<li>A superb breakdown of farm machinery’s impact on a sector that employed more than a quarter of all Americans comes from <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">Olmstead and Rhode at UC Davis</a>:</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="657" height="161" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png" alt="The upshot is that by 1960 the tractor reduced annual labor use by at least 3.44 billion man-hours of field and chore labor from the level required using the horse power technology. (The 3.44 billion figure combines the USDA estimate of annual labor savings before 1944 and our lower-bound estimate of saving between 1944 and 1959.) This was the equivalent of approximately 1,720 thousand workers, which represented 24.3 percent of farm employment in 1960 and 27.3 percent of the decline since 1910." srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png 657w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-300x74.png 300w" sizes="(max-width: 657px) 100vw, 657px"></figure></div>


<ul>
<li>Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture?
<ul>
<li>Rationally, it’s difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality.</li>



<li>The core assertion by the “AI will replace 20-50% of all jobs” crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (“get us press,” or “optimize our marketing campaign,” or “record and audit our financials” ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor.</li>



<li>It’s impossible to argue against the assertion that AI will do what’s described above, because it’s based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I’ll avoid that to spare us all a lot of wasted time 😉</li>
</ul>
</li>
</ul>



<p>I’ll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what’s really going on here.</p>



<p>Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it’s scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses.</p>



<p>Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology.</p>



<p>If you’re feeling like the “AI will take all our jobs” discussion is familiar, you’re in good company. Many others have pointed out the similarities to stories like:</p>


<div>
<figure><a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/"><img loading="lazy" decoding="async" width="816" height="557" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png 816w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-300x205.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-768x524.png 768w" sizes="auto, (max-width: 816px) 100vw, 816px"></a></figure></div>


<p>Source: <a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/">Jalopnik</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="786" height="631" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png 786w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-300x241.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-768x617.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></figure></div>


<p>Source: <a href="https://www.insidehook.com/culture/older-generations-kids-too-soft">InsideHook</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="767" height="816" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png 767w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3-282x300.png 282w" sizes="auto, (max-width: 767px) 100vw, 767px"></figure></div>


<p>Source: <a href="https://www.honestjobs.com/post/nobody-wants-to-work-anymore-is-not-new-and-it-s-not-true">Honest Jobs</a></p>



<p>Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It’s both anti-historic and anti-evidence that AI will somehow prove to be the exception.</p>



<p>Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT’s estimation that <a href="https://www.nber.org/papers/w30389">60% of employment in 2018 was in types of jobs that didn’t exist before 1940</a>.</p>



<p>By the time I’m in my 80s, y’all better have destroyed more than half of all the existing jobs, and that’s just to keep up with the 20th Century’s pace of change. But, don’t expect AI to do it for you in the next decade; that’s just marketing.</p>



<p>p.s. If you’re looking for the TL;DR, <a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o">Ed Zitron on Bluesky</a> has got you:</p>


<div>
<figure><a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o" target="_blank" rel=" noreferrer noopener"><img loading="lazy" decoding="async" width="710" height="736" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png 710w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4-289x300.png 289w" sizes="auto, (max-width: 710px) 100vw, 710px"></a></figure></div>


<p>p.p.s. I agree there’s evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for <a href="https://www.reddit.com/r/Futurology/comments/1l100p3/ai_is_breaking_entrylevel_jobs_that_gen_z_workers/">early-stage jobs in a few tech-heavy fields</a>. But squinting at the evidence, it’s &lt;0.1% of jobs (&lt;200,000 total) being affected, and even here, the unbalanced capital vs. labor market is <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">a far more compelling explanation</a>.</p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Sky's the limit: AI automation on Mac (109 pts)]]></title>
            <link>https://taoofmac.com/space/blog/2025/06/03/2155</link>
            <guid>44179691</guid>
            <pubDate>Wed, 04 Jun 2025 11:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taoofmac.com/space/blog/2025/06/03/2155">https://taoofmac.com/space/blog/2025/06/03/2155</a>, See on <a href="https://news.ycombinator.com/item?id=44179691">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://taoofmac.com/space/blog/2025/06/03/2155">Jun 3<sup>rd</sup> 2025</a> · 4 min read
 · <small>
#ai 
#apple 
#automation 
#desktop 
#intelligence 
#macos 
#shortcuts 
#sky 
#wwdc 
</small>
</p><section id="main">
    <p>I’ve been sitting on this draft for a few days now, partly because I thought it would turn down the bitterness, and partly because I kept asking myself whether I should even write it. But I think it is worth getting out of my system, so here goes.</p>
<p>In case you’re not in the Mac community, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is an app that brings <a href="https://taoofmac.com/space/ai" rel="next">AI</a> automation to the Mac that <a href="https://www.macstories.net/stories/sky-for-mac-preview/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Federico Viticci wrote about at length last week</a>, and that not only looks and feels exactly like what I would expect <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">Apple Intelligence</a> to be like, it also completely blows out of the water all the desktop automation tools that have sprung out of the <a href="https://taoofmac.com/space/ai/mcp" rel="next">MCP</a> hype.</p>
<p>I have several questions, some of which I have already sort of asked <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">back in March</a>, but which I think are worth reformulating.</p>
<p>The people who created <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> are the same people who created Workflow and worked on Shortcuts, so here’s my first question:</p>
<p><em>Why wasn’t Apple able to harness their expertise in the first place?</em></p>
<p>I mean, people have free will and all, and can choose to work wherever they want, but this makes my <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">earlier rant about their having neglected automation</a> feel like the first clue to a corporate culture murder scene.</p>
<p>Not having made it possible for them to thrive feels like vanilla corporate politics, but having brilliant people <em>leave</em> Apple and ship something that is, even in preview, <em>much better than anything that Apple Intelligence promised</em> (including the made up bits they paraded as marketing material) is just gross mismanagement (now you know why I held back on this draft).</p>
<p>Which leads me to my second question:</p>
<p><em>Why has Apple failed this badly?</em></p>
<p>Was it just a consequence of their innately siloed nature? The internal decline of John Giannandrea’s team (and the <a href="https://daringfireball.net/linked/2025/03/20/gurman-rockwell-siri?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">rumored hand-over of Siri to Craig Federighi’s team</a>) might have played a role, but <a href="https://taoofmac.com/space/com/apple/macos" rel="next">macOS</a> has been largely stagnant from a UX perspective for ages (and as far as I know it isn’t even being addressed in the upcoming Solarium redesign), so I have to assume the <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> team saw this <em>huge</em> blind spot in terms of improving the desktop experience and just jumped on it.</p>
<p><em>Was it about control? Privacy?</em></p>
<p>I can see Apple balking at doing something like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> (if they ever even considered it) because it not only has to share bits of your screen with an LLM, but also because it would have to open up the Mac to third-party automation in a way that it has never done before, and that would be a huge departure from their current approach.</p>
<p>Which, <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">again</a>, is pretty much non-existent, so… No, that doesn’t make sense.</p>
<p>But the privacy angle is interesting, because Apple was in a <em>perfect</em> position to do something exactly like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> and ensure that it was done in a way that respected user privacy. Even though local models are still not quite there yet (remember that RAM requirements are still very high as far as running truly useful models are concerned), they do have the confidential computing tech to run inference in a privacy-preserving way–which might be the only bit of Apple Intelligence that actually works at this point.</p>
<p>But <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a>, despite having cloud inference, is designed to enhance your <em>local</em> Mac experience, and it does so in a way that looks extremely polished, and, above all, <em>feels like the way people always wanted to use computers</em>. Star Trek echoes aside, it has the ability to understand what you want to do, and <em>automates your Mac</em> to achieve that.</p>
<p>Federico’s post also goes into part of the <em>how</em> it does this, and I get the impression that even though <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> can leverage the remnants of Mac automation, for gathering context it is completely bypassing the standard automation APIs and inferring UI structure and content.</p>
<p>Everything I read about it makes me think that Apple has dropped the ball so badly that <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is like a perfect storm of what they could have done, but didn’t.</p>
<p>And now, not only is it a third-party app that is doing what Apple should have done, but it is also doing it in <em>a better way that anything they ever shipped</em>.</p>
<p>And if <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> takes off (let’s face it, the Mac desktop market isn’t really mainstream these days and there is too much AI hype, but just entertain the notion for a bit), that will have the added bonus of highlighting that Apple are completely out of touch with what people want from their computers.</p>
<p>Which leads me to my final question:</p>
<p><em>What will it take for Apple to get its act together?</em></p>
<p>I honestly don’t know. I mean, I have been asking this question for years now, and I have no idea what it will take for Apple to take any sort of integration or automation seriously. I currently have <em>zero</em> expectations towards next week’s WWDC, and not only because of the Mac. We were fooled once, and I don’t think we will be fooled again.</p>
<p>Unless they actually <em>ship</em> something, which seems highly unlikely unless it slots into their yearly release cycle (which they are rumored to be rebranding as “OS 26” because, well, why not ship the org chart <em>and</em> their corporate calendar?).</p>
<p>A case in point (and stop me if you’ve heard this before): Spotlight has been a complete mess for years, and Apple has done <em>nothing</em> to effectively fix it on any of its platforms–and it would be a <em>perfect</em> place to start integrating <a href="https://taoofmac.com/space/ai" rel="next">AI</a> in a way that would actually make sense and be useful to users.</p>
<p>Not to mention that it would be a key component of any sort of retrieval-augmented generation approach, etc.</p>
<p>So yes, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is the limit. Or, at least, one very concrete yardstick by which we can measure how much Apple has failed to deliver on the promise of AI.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I Wrote the BEAM Book (326 pts)]]></title>
            <link>https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</link>
            <guid>44179257</guid>
            <pubDate>Wed, 04 Jun 2025 10:36:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/">https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</a>, See on <a href="https://news.ycombinator.com/item?id=44179257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<h3> Post-mortems, coffee, and a decade of stubborn curiosity </h3><p>

	Posted:  2025-06-03</p>

	<h2>Why I wrote the Beam Book</h2>
<p>After ten years of keeping Klarna’s core system upright I know this: a 15
millisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote <em>The BEAM Book</em> so the next engineer fixes that pause before the coffee cools.</p>
<p><img src="https://happihacking.com/images/thebeambooks.jpg" alt="A picture of two printed BEAM Books."></p><h3>Origins</h3>
<p>I opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.
After two weeks, the commit log is mostly me adding structure, moving
headings, and updating metadata. Most of it is scaffolding. The actual
content is still just a few hopeful lines.</p>
<p>By November I had abandoned DocBook for AsciiDoc, written a custom build
script, and convinced myself the book could be wrapped up in six months.
Those early commits glow with energy: adds, rewrites, then more
rewrites to fix the rewrites.
Delusion is underrated.</p>
<p>In 2013 I managed to convince O’Reilly to publish. Moving the repo to their
Atlas system sounded simple until Atlas began hiding my main file and
overwriting half-finished chapters.</p>
<p>The Git history reads like a diary of frustration:
“Moving files to top level to cope with Atlas,” “Atlas seems to be
overwriting book.asciidoc”. Word count shot past 120 000 while actual
progress crawled. On 10 March 2015 I was literally “Smashing chapters into sections” just to keep the build green.</p>
<p>The quiet cancellation came two months later. No drama, just a polite call and a line through the contract. Relief mingled with embarrassment, I had spent two years rearranging files rather than finishing sentences.</p>
<p>Pragmatic Bookshelf took over that same year. I kept working in CVS for
their production system, but progress was slow. Eventually, they cancelled
too. On 20 January 2017, I imported everything into a new repo in one
massive commit: 6,622 files, over a million lines.
The rewrite stalled, and so did the project.</p>
<p>On 23 March 2017 I started fresh with Asciidoctor in a private GitHub repo, copy-pasting
only the parts that still made sense. Two weeks later, on April 7, minutes before
a lecture at Chalmers, I flipped the repository public. Within twenty-four
hours strangers fixed typos, added diagrams, and merged a Creative Commons
BY-4.0 license.</p>
<h3>What Kept Me Going</h3>
<p><img src="https://happihacking.com/images/star-history.svg" alt="A picture of the stars on GitHub passing 3000."></p><p>I kept going because I wanted to understand the BEAM properly. There’s
value in following the real logic, not just the surface explanations.</p>
<p>Community feedback made a difference. As soon as the repo was public,
people began sending corrections, examples, and improvements.</p>
<p>Seeing the numbers of people starring the repo on GitHub kept me going.
One highlight: <strong>Issue #113 – “Please continue being awesome.”</strong>
That emoji-laced drive-by encouragement (August 2018) still pops into my
head whenever motivation dips.</p>
<p><img src="https://happihacking.com/images/issue113.png" alt="Issue 113: This book
is ridiculously good. I have only read a few bits of it so far and have
learned a lot already. Please continue being awesome!"></p>
<p>The book started showing up as a reference in Erlang and BEAM conference
talks, sometimes several times in the same event. That was a clear signal
that others needed this as much as I did.</p>
<p>Even Twitter (in the good old days of Twitter) played a role. Whenever
someone mentioned the book or shared a
link, it was an extra nudge to keep at it.</p>
<p>Mostly, I just wanted a manual I could trust myself, a reference for the
parts of the VM that matter when things go wrong. That’s reason enough to
keep writing, even after the third rewrite.</p>
<h3>What’s Inside the Book &amp; Who It Helps</h3>
<p>The book covers what I wish I’d had when building and operating large
Erlang systems:</p>
<ul>
<li>Schedulers and process management: How the BEAM schedules,
prioritizes, and balances processes under real load.</li>
<li>Processes and their memory: How process heaps,
stack, messages, and binaries are managed and
why these details matter in production.</li>
<li>Garbage collection and memory: What actually happens
with per-process and global garbage collectors, binary references,
and memory leaks.</li>
<li>Tagging schemes and terms: How the BEAM represents data—integers,
floats, tuples, binaries, references—down to the tagging bits.</li>
<li>The compiler and the VM: How code is turned into instructions,
what the compiler does (and doesn’t do), and how the emulator executes it.</li>
<li>Tracing and debugging: Practical use of dbg, erlang:trace,
and other tools to follow messages, events, and identify bottlenecks.</li>
<li>Performance tuning: What matters when profiling real code,
understanding reductions, and tracking down real-world latency problems.</li>
<li>System architecture: How ERTS, the BEAM VM, and their subsystems
actually work together in a running node.</li>
</ul>
<p>If you build or operate Erlang or Elixir systems, especially under any kind
of scale—this book is for you. It saves you from hunting through mailing
lists, scattered docs, and code comments just to answer, “Why is the VM
behaving like this?”</p>
<h3>Lessons Learned</h3>
<p>Persistence beats perfection. Two cancelled publishing deals look bad on a
résumé, but an unfinished idea looks worse.</p>
<p>Boundaries matter. I made progress by blocking time for writing, turning
off notifications, and treating focus like a real deadline. Fika at 14:30
is non-negotiable.</p>
<p>The crowd helps. Making the repo public brought in corrections,
encouragement, and the occasional nudge when motivation was low.</p>
<p>Scope is everything. I cut the details on dirty schedulers, the new JIT,
and the debugger. Maybe those will end up in an appendix, but not in the
core.</p>
<p>Ship, then iterate. The BEAM changes every year. A living Git repo keeps
up.</p>
<p>A real deadline helps. This January, during my yearly review, I
decided to print the book in time for Code Beam Stockholm. I thought I had
until autumn, turns out the conference was June 2. That’s how you find out
what’s truly essential.</p>
<h3>Definition of Done</h3>
<p>Holding the print in my hands, it finally feels finished, at least for now. Years of scattered commits are bound into something real, so I’m calling it done.</p>
<h3>Get Involved</h3>
<p>You can now get the paperback—The BEAM Book 1.0 is live on Amazon. Buy it
here.&nbsp;<a href="https://www.amazon.com/dp/9153142535">Amazon</a></p>
<p>If you spot an error, want to improve something, or just want to see how it
works under the hood, star or fork the repo. File an issue or, even better,
submit a pull request. Contributors are credited in the acknowledgments.
<a href="https://github.com/happi/theBeamBook">GitHub: theBeamBook</a></p>
<p>If you read the book, please leave an honest review.
Algorithms notice real feedback more than marketing copy.</p>
<p>If your team wants a deep dive, I run hands-on BEAM internals
workshops, tailored for real systems, not just hello world.
Email me if that’s what you need.
<a href="mailto:happi@happihacking.com">happi@happihacking.com</a></p>


	<p>
	  - Happi
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cockatoos have learned to operate drinking fountains in Australia (236 pts)]]></title>
            <link>https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</link>
            <guid>44178902</guid>
            <pubDate>Wed, 04 Jun 2025 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia">https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</a>, See on <a href="https://news.ycombinator.com/item?id=44178902">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Cloud Run GPUs, now GA, makes running AI workloads easier for everyone (258 pts)]]></title>
            <link>https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</link>
            <guid>44178468</guid>
            <pubDate>Wed, 04 Jun 2025 08:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available">https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</a>, See on <a href="https://news.ycombinator.com/item?id=44178468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Developers love </span><a href="https://cloud.google.com/run"><span>Cloud Run</span><span>, Google Cloud’s serverless runtime, </span></a><span>for its simplicity, flexibility, and scalability. And today, we’re thrilled to announce that NVIDIA GPU support for Cloud Run is now generally available, offering a powerful runtime for a variety of use cases that’s also remarkably cost-efficient.&nbsp;</span></p>
<p><span>Now, you can enjoy the following benefits across both GPUs and CPUs:</span></p>
<ul>
<li>
<p><strong>Pay-per-second billing</strong><span>: You are only charged for the GPU resources you consume, down to the second.</span></p>
</li>
<li>
<p><strong>Scale to zero</strong><span>: Cloud Run automatically scales your GPU instances down to zero when no requests are received, eliminating idle costs. This is a game-changer for sporadic or unpredictable workloads.</span></p>
</li>
<li>
<p><strong>Rapid startup and scaling</strong><span> Go from zero to an instance with a GPU and drivers installed in under 5 seconds, allowing your applications to respond to demand very quickly. For example, when scaling from zero (cold start), we achieved an impressive Time-to-First-Token of approximately 19 seconds for a gemma3:4b model (this includes startup time, model loading time, and running the inference)</span></p>
</li>
<li>
<p><strong>Full streaming support</strong><span>: Build truly interactive applications with out-of-the box support for HTTP and WebSocket streaming, allowing you to provide LLM responses to your users as they are generated.</span></p>
</li>
</ul>
<p><span>Support for GPUs in Cloud Run is a significant milestone, underscoring our leadership in making GPU-accelerated applications simpler, faster, and more cost-effective than ever before.</span></p>
<p><span>“Serverless GPU acceleration represents a major advancement in making cutting-edge AI computing more accessible. With seamless access to NVIDIA L4 GPUs, developers can now bring AI applications to production faster and more cost-effectively than ever before.” </span><span>- Dave Salvator, director of accelerated computing products, NVIDIA</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>AI inference for everyone</strong></h3>
<p><span>One of the most exciting aspects of this GA release is that Cloud Run GPUs are now available to everyone for NVIDIA L4 GPUs, with </span><strong>no quota request required</strong><span>.This removes a significant barrier to entry, allowing you to immediately tap into GPU acceleration for your Cloud Run services. Simply use </span><code>--gpu 1</code><span> from the Cloud Run command line, or check the "GPU" checkbox in the console, no need to request quota:</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Production-ready</strong></h3>
<p><span>With general availability, Cloud Run with GPU support is now covered by Cloud Run's </span><a href="https://cloud.google.com/run/sla"><span>Service Level Agreement (SLA)</span></a><span>, providing you with assurances for reliability and uptime. By default, Cloud Run offers </span><a href="https://cloud.google.com/run/docs/zonal-redundancy"><span>zonal redundancy</span></a><span>, helping to ensure enough capacity for your service to be resilient to a zonal outage; this also applies to Cloud Run with GPUs. Alternatively, you can turn off zonal redundancy and benefit from a </span><a href="https://cloud.google.com/run/pricing"><span>lower price</span></a><span> for best-effort failover of your GPU workloads in case of a zonal outage.</span></p>
<h3><strong>Multi-regional GPUs</strong></h3>
<p><span>To support global applications, Cloud Run GPUs are available in five Google Cloud </span><a href="https://cloud.google.com/run/docs/locations#gpu"><span>regions</span></a><span>: us-central1 (Iowa, USA), europe-west1 (Belgium), europe-west4 (Netherlands), asia-southeast1 (Singapore), and asia-south1 (Mumbai, India), with more to come.</span></p>
<p><span>Cloud Run also </span><a href="https://cloud.google.com/run/docs/multiple-regions"><span>simplifies deploying your services across multiple regions</span></a><span>. For instance, you can deploy a service across the US, Europe and Asia with a single command, providing global users with lower latency and higher availability. For instance, here’s how to deploy </span><a href="https://ollama.com/" rel="noopener" target="_blank"><span>Ollama</span></a><span>, one of the easiest way to run open models, on Cloud Run across three regions:</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>See it in action: 0 to 100 NVIDIA GPUs in four minutes</strong></h3>
<p><span>You can witness the incredible scalability of Cloud Run with GPUs for yourself with </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2140" rel="noopener" target="_blank"><span>this live demo</span></a><span> from Google Cloud Next 25, showcasing how we scaled from 0 to 100 GPUs in just four minutes.</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>Load testing a Stable Diffusion service running on Cloud Run GPUs to 100 GPU instances in four minutes.</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Unlock new use cases with NVIDIA GPUs on Cloud Run jobs</strong></h3>
<p><span>The power of Cloud Run with GPUs isn't just for real-time inference using request-driven Cloud Run services. We're also excited to announce the availability of GPUs on </span><a href="https://cloud.google.com/run/docs/overview/what-is-cloud-run#cloud-run-jobs"><span>Cloud Run jobs</span></a><span>, unlocking new use cases, particularly for batch processing and asynchronous tasks:</span></p>
<ul>
<li>
<p><strong>Model fine-tuning</strong><span>: Easily fine-tune a pre-trained model on specific datasets without having to manage the underlying infrastructure. Spin up a GPU-powered job, process your data, and scale down to zero when it’s complete.</span></p>
</li>
<li>
<p><strong>Batch AI inferencing</strong><span>: Run large-scale batch inference tasks efficiently. Whether you're analyzing images, processing natural language, or generating recommendations, Cloud Run jobs with GPUs can handle the load.</span></p>
</li>
<li>
<p><strong>Batch media processing</strong><span>: Transcode videos, generate thumbnails, or perform complex image manipulations at scale.</span></p>
</li>
</ul>
<p><span><a href="https://docs.google.com/forms/d/e/1FAIpQLSe_-u-ZSxVLhRMZ3p4ZSk2CkgL_URKqNgyM8rfMGUrTbpqYJQ/viewform?usp=dialog" rel="noopener" target="_blank"><span>Sign up</span></a><span> for the private preview of GPUs on Cloud Run jobs.</span></span></p>
<h3><strong>What Cloud Run customers are saying</strong></h3>
<p><span>Don't just take our word for it. Here's what some early adopters of Cloud Run GPUs are saying:</span></p>
<p><span>"Cloud Run helps vivo quickly iterate AI applications and greatly reduces our operation and maintenance costs. The automatically scalable GPU service also greatly improves the efficiency of our AI going overseas.”</span><span> - Guangchao Li, AI Architect, vivo</span></p>
<p><span>"L4 GPUs offer really strong performance at a reasonable cost profile. Combined with the fast auto scaling, we were really able to optimize our costs and saw an 85% reduction in cost. We've been very excited about the availability of GPUs on Cloud Run."</span><span> - John Gill at </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2496" rel="noopener" target="_blank"><span>Next'25</span></a><span>, Sr. Software Engineer, Wayfair</span></p>
<p><span><span>"At Midjourney, we have found Cloud Run GPUs to be incredibly valuable for our image processing tasks. Cloud Run has a simple developer experience that lets us focus more on innovation and less on infrastructure management. Cloud Run GPU’s scalability also lets us easily analyze and process millions of images.</span><span>" - Sam Schickler, Data Team Lead, Midjourney</span></span></p>
<h3><strong>Get started today</strong></h3>
<p><span>Cloud Run with GPU is ready to power your next generation of applications. Dive into the </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu"><span>documentation</span></a><span>, explore our </span><a href="https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama"><span>quickstarts</span></a><span>, and review our </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu-best-practices"><span>best practices for optimizing model loading</span></a><span>. We can't wait to see what you build!</span></p></div><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/serverless" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/serverless" track-metadata-module="tag list" track-metadata-module_headline="posted in">Serverless</a></li><li><a href="https://cloud.google.com/blog/products/application-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/application-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Application Modernization</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Code Isn't Scary (140 pts)]]></title>
            <link>https://jimmyhmiller.com/machine-code-isnt-scary</link>
            <guid>44177446</guid>
            <pubDate>Wed, 04 Jun 2025 05:19:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jimmyhmiller.com/machine-code-isnt-scary">https://jimmyhmiller.com/machine-code-isnt-scary</a>, See on <a href="https://news.ycombinator.com/item?id=44177446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>The first programming language I ever learned was ActionScript. Writing code for Macromedia's Flash might be the furthest away from "bare metal" as you can possibly get. As I continued learning new languages, this starting heritage stuck with me. I was mostly interested in high-level, "web languages". Low-level languages felt impenetrable. Over time, I learned a bit more about them here and there, but for some reason, this notion stuck with me. Low-level things are scary, and machine code epitomized that most directly. When I Googled things asking about writing in "straight machine code", I was met with discouraging messages rather than learning.</p>
<p>Eventually, I decided I needed to overcome this barrier if I was going to achieve my goals. In doing so, I learned something I didn't expect.</p>
<blockquote>
<p>Machine code isn't scary. If you can make sure your JSON conforms to a JSON schema, you can write machine code.</p>
</blockquote>
<h3>Which Machine Code?</h3>
<p>One problem with machine code is that there isn't simply one standard. There are many different "instruction sets" depending on the processor. Most modern PCs use x86-64 machine code, but newer Macs, Raspberry Pis, and most mobile devices use ARM. There are other architectures out there, especially as you go back in time. The goal of this article won't be to give you a deep understanding of any particular instruction set, but instead, to give you enough information about how machine code typically works so you cannot be afraid of machine code. So we will start by having our examples be in ARM 64-bit (also written as aarch64). Once we have a decent understanding of that, we will talk a bit about x86-64.</p>
<h2>Machine Code Basics</h2>
<p>To understand the basics of machine code, you need three concepts:</p>
<ol>
<li>Instructions</li>
<li>Registers</li>
<li>Memory</li>
</ol>
<p>Instructions are exactly what they sound like; they are the code that will run. Machine code instructions are just numbers. In fact, in AArch64, every instruction is a 32-bit number. Instructions encode what operation the machine should run (add, move, subtract, jump, etc.) and accept some arguments for what data to operate on. These arguments might be constants (meaning like the number 2; these constants are often called "immediates"), but they can also be registers or a memory address. For now, just think of a register as a variable and memory as a list.</p>
<h3>Arm Instructions</h3>
<p>Here is an example of the instruction <a href="https://developer.arm.com/documentation/ddi0596/2021-03/Base-Instructions/ADD--immediate---Add--immediate--">add immediate</a>.</p>
<div><table><thead><tr><th>31</th><th>30</th><th>29</th><th>28</th><th>27</th><th>26</th><th>25</th><th>24</th><th>23</th><th>22</th><th>21</th><th>20</th><th>19</th><th>18</th><th>17</th><th>16</th><th>15</th><th>14</th><th>13</th><th>12</th><th>11</th><th>10</th><th>9</th><th>8</th><th>7</th><th>6</th><th>5</th><th>4</th><th>3</th><th>2</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>sf</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>sh</td><td colspan="12">imm12</td><td colspan="5">Rn</td><td colspan="5">Rd</td></tr></tbody></table></div>
<p>Now this might look a bit confusing, but once you've seen these tables long enough, they start to be fairly straightforward. Each column in this table represents a single bit in a 32-bit number. If the value is a 0 or 1, that just means it is already filled in. If it has a label, it is a variable that needs to be filled in. <code>sf</code> tells us whether the registers we are going to use are 64-bit or 32-bit registers. <code>sh</code> stands for shift. <code>sh</code> goes in conjunction with imm12, which stands for a 12-bit immediate (constant). So if we want to add <code>42</code> to something, we would put <code>000000101010</code> in for <code>imm12</code> and set sh to 0 (meaning we aren't shifting the number). But what if we want to represent a number larger than 12 bits? Well, the add instruction doesn't let us represent all such numbers; but setting sh to 1 lets us shift our number by 12 bits. So for example we can represent <code>172032172032</code> by leaving our 42 alone and setting sh to 1. This is a clever technique for encoding larger numbers in a small space. Variables that start with R are registers, in this case, Rn is our argument to add, and Rd is our destination.</p>
<p>So the above instruction can be thought of like this:</p>
<pre><code><span>struct</span> Add <span>{</span>
 is_sixty_four_bit<span>:</span> boolean<span>,</span>
 shift<span>:</span> boolean<span>,</span>
 immediate<span>:</span> u12<span>,</span>
 n<span>:</span> Register<span>,</span>
 destination<span>:</span> Register<span>,</span>
<span>}</span>
</code></pre>
<p>Our add instruction is really just a data structure where we put the right parts in the right places.</p>
<h2>Registers</h2>
<p>Registers are small places to store values. Every instruction set will have a different number of these registers, different sizes of registers, different kinds of registers, and different naming conventions for registers. For AArch64, there are 31 general-purpose registers numbered X0 through X30 for 64-bit registers. Let's say we want to add 42 to register X0 and store the result in X1; we use this binary number.</p>
<div><table><thead><tr><th>sf</th><th colspan="8">operation</th><th>sh</th><th colspan="12">imm12</th><th colspan="5">Rn</th><th colspan="5">Rd</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>
<p>To encode our registers into our instruction, we just use their number. So register X0 would be 00000 and register X18 would be <code>10010</code>. Registers are simply places where we can store values. But by convention, registers can be used for different things. These are called calling conventions and they are how "higher" level languages like C encode function calls.</p>
<p>Writing out all these binary numbers all the time (or even converting them to hex) can often be tedious. So instead, we usually talk about instructions in a simple text format called assembly.</p>
<pre><code>add x1, x0, #0x2a 
</code></pre>
<p>In order to feel cool, people usually write numbers in assembly as hex values. This is just the number 42. You can see that assembly hides some of the details of the encoding we just made. We don't think about sf, sh, what size our number is, that a register is Rn vs Rd. Instead, the destination comes first and the arguments after. Because of this lack of detail, a single assembly instruction <code>add</code> might actually map to many different machine code instructions depending on its arguments.</p>
<h2>Memory</h2>
<p>The last piece we have to understand for machine code is memory. To understand what is going on with memory, we will look at an instruction that lets us store things in memory. This instruction is called <a href="https://developer.arm.com/documentation/ddi0602/2025-03/Base-Instructions/STR--immediate---Store-register--immediate--?lang=en#iclass_unsigned_offset">STR</a> or not written in shorthand, store.</p>
<div><table><thead><tr><th>31</th><th>30</th><th>29</th><th>28</th><th>27</th><th>26</th><th>25</th><th>24</th><th>23</th><th>22</th><th>21</th><th>20</th><th>19</th><th>18</th><th>17</th><th>16</th><th>15</th><th>14</th><th>13</th><th>12</th><th>11</th><th>10</th><th>9</th><th>8</th><th>7</th><th>6</th><th>5</th><th>4</th><th>3</th><th>2</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>1</td><td>x</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td colspan="12">imm12</td><td colspan="5">Rn</td><td colspan="5">Rt</td></tr></tbody></table></div>
<p>Using this instruction, we are going to store some value (RT) into the address (RN) + some offset (imm12). So if we think about memory as a big array, this instruction is like writing into that array. <code>array[offset] = value</code>. The x here is like our sf before, it controls whether we are using 64-bit values or not. If we want to make this concrete, let's say we have a value in X2, we have an address of memory in X1 and we want to store a value 2 bytes offset from that. We would get this structure:</p>
<div><table><thead><tr><th></th><th>x</th><th colspan="8">operation</th><th colspan="12">imm12</th><th colspan="5">Rn</th><th colspan="5">Rt</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>
<p>Since writing that all is tedious, we often just write the assembly notation. We are storing the value in x2 based on the address stored in x1 + 2.</p>
<pre><code>str x2, [x1, #0x2]
</code></pre>
<h2>X86-64</h2>
<p>X86 encoding is a bit different, but it more or less has the same parts. We are still working with instructions, registers, and memory. Some names are a bit different. Instead of the consistent 0-30 naming, we get the historical baggage of the following 64-bit registers: rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8-r15). However, the biggest difference is that x86 is not a fixed width instruction set. We can't simply give a nice little diagram of every instruction using 32 bits. Instead, instructions are assembled from parts. These parts are given different names; when you see an instruction encoding, it tells you how to put the parts together.</p>
<h3>REX</h3>

<p>The first part is called the REX. This is a prefix that we can use to help us with 64-bit operations. Not sure if there is an official justification for the name REX, but my understanding is that it is the "Register Extension Prefix". Unfortunately, because the REX is a prefix, it will only make sense when we see what comes later. REX is there for backward compatibility. The W in REX lets us signal that we are using 64-bit or not for certain operations. The R and B will "extend" our registers in certain operations.  In other words, it allows more registers than you used to be able to (These are those r8-r15 registers with a different naming convention than the older registers). We need these because, before 64-bit x86, we had fewer registers and our instructions only had 3 bits per register. With 16 registers, we need an extra bit. (X is for the SIB structure, which we don't cover here).</p>
<h3>ModR/M</h3>

<p>Our next part is ModR/M. ModR/M keeps up with the tradition of naming things incredibly short and confusing names. <code>mod</code> actually means Mode. <code>mod</code> tells us if <code>rm</code> is acting as a register or if it is a pointer to memory. If <code>mod == 11</code> then rm is being used as a register, otherwise, it is being used as a pointer. <code>reg</code> just is a register.</p>
<h3>OpCode</h3>
<p><code>OpCode</code> is simple, it is a number. It can be 1-3 bytes long.</p>
<h2>Putting It Together</h2>
<p>There are other parts, but we won't cover them here. With just these parts, we can build up an instruction. Let's say we want to move a 32-bit signed immediate to a 64-bit register. We can consult <a href="https://www.felixcloutier.com/x86/mov">a table of instruction encodings</a> and we will get this:</p>
<pre><code>REX.W + C7 /0 id
</code></pre>
<p>So now we can assemble our parts and make our instruction. Let's start with REX.W. This notation just means REX with W set to 1. Then there’s B8, which is just a number written in hex. <code>/0</code> is yet more shorthand for using the ModR/M but setting the reg to 0. Finally, <code>id</code> means "immediate doubleword", in other words, a constant number that is 32 bits long. So given all that, we can write our instruction. So let's move the number 42 to the rbx register.</p>
<table><thead><tr><th>Byte Index</th><th>Bits</th><th>Description</th></tr></thead><tbody><tr><td>Byte 0</td><td>55–48</td><td>01001000      REX.W = 1</td></tr><tr><td>Byte 1</td><td>47–40</td><td>11000111      Opcode C7</td></tr><tr><td>Byte 2</td><td>39–32</td><td>11000011      ModR/M: reg=000, r/m=011 (RBX)</td></tr><tr><td>Byte 3</td><td>31–24</td><td>00101010      42</td></tr><tr><td>Byte 4</td><td>23–16</td><td>00000000      the rest of 42</td></tr><tr><td>Byte 5</td><td>15–8</td><td>00000000      ...</td></tr><tr><td>Byte 6</td><td>7–0</td><td>00000000      ...</td></tr></tbody></table>
<p>Why is RBX 011? Well, because <a href="https://wiki.osdev.org/X86-64_Instruction_Encoding#Registers">the table</a> says so. Yeah, I did say that x86 is a bit weird.</p>
<h2>The Rest of It</h2>
<p>I won't pretend that this is all you need. But I will say that starting here can get you further than you think. There are some other things to learn, like various flags for things like overflow, there’s also calling conventions, which are about which registers you use when for things like function calls. We haven't really talked about the stack here, but that's memory that you write to to keep track of things. Nor have we talked about jumps, or how to encode larger immediates in ARM, but you’ve gotten the basics. It’s easier than you would think to hop on <a href="https://godbolt.org/">compiler explorer</a> and learn how things are done.</p>
<p>Learning machine code and writing things at this low level has unlocked so many things that were mental blocks for me before. Relying on libraries made by others to do these low-level things always left a gap in my knowledge that made me doubt my understanding. Even if I intellectually could explain things, actually doing them has made a huge difference for me. So if you, like me, find low-level things intimidating, I can't recommend enough starting from scratch, at the lowest possible level for your task. What I've found over and over again with low-level details, their not hard, their just poorly documented and poorly explained.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Merlin Bird ID (508 pts)]]></title>
            <link>https://merlin.allaboutbirds.org/</link>
            <guid>44176829</guid>
            <pubDate>Wed, 04 Jun 2025 02:58:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://merlin.allaboutbirds.org/">https://merlin.allaboutbirds.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hero-wrapper">
              <h2>Identify the birds you see or hear with Merlin Bird ID</h2><p>Free global bird guide with photos, <br>sounds, maps, and more.</p><p><a href="https://itunes.apple.com/app/apple-store/id773457673?pt=401711&amp;ct=marketingwebsite&amp;mt=8"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/01/Download_App_Store_en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a> <a href="https://play.google.com/store/apps/details?id=com.labs.merlinbirdid.app&amp;pli=1"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/02/google-play-badge-en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a></p>            </div><section id="content" aria-label="Main content" data-sticky-container="">
 
      
<div>
<div>
<h2>Identify Bird Songs and Calls</h2>



<p><strong>Sound ID</strong> listens to the birds around you&nbsp;and shows real-time suggestions for who’s singing. Compare your recording to the songs and calls in Merlin to confirm what you heard. Sound ID works completely offline, so you can identify birds you hear no matter where you&nbsp;are. </p>



<p>Available for birds in the US, Canada, Europe, with some common birds of Central and South America, and India. More species and regions coming soon.&nbsp;</p>




</div>



<div>
<figure><img fetchpriority="high" decoding="async" width="2000" height="1250" src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png" alt="" srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-480x300.png 480w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>
</div>
</div>



<hr>



<div>
<div>
<figure><img decoding="async" width="1280" height="800" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-480x300.png 480w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1.png 2000w" data-sizes="(max-width: 1280px) 100vw, 1280px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>



<div>
<h2>Identify Birds in a Photo</h2>



<p>Snap a photo of a bird, or pull one in from your camera roll, and <strong>Photo ID</strong> will offer a short list of possible matches. Photo ID works completely offline, so you can identify birds in the photos you take no matter where you&nbsp;are. </p>




</div>
</div>



<hr>



<h2>Bird ID Wizard—Step-by-step</h2>



<p>Answer three simple questions about a bird you are trying to identify and Merlin will give you a list of possible matches.&nbsp;Merlin offers quick identification help for all levels of bird watchers and outdoor enthusiasts to help you learn about the birds in any country in the world. </p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>











<hr>



<div>
<div>
<h2>Save Birds to Your Life List</h2>



<p>Build a&nbsp;digital&nbsp;scrapbook of your birding memories with Save My Bird. Tap “This is my bird!” each time you identify a bird, and&nbsp;Merlin will add it to your growing life&nbsp;list.</p>




</div>



<div>
<figure><img decoding="async" width="2000" height="1250" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-480x300.png 480w" data-sizes="(max-width: 2000px) 100vw, 2000px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>
</div>



<hr>



<h2>Explore Lists of Birds Near You</h2>



<p>Merlin is powered by&nbsp;<a href="http://ebird.org/">eBird</a>, allowing you to&nbsp;build custom lists of&nbsp;the birds you’re likely to spot wherever you are. Use the filter options to&nbsp;explore birds for different locations or time of year, or switch to show all the Offline Birds you’ve downloaded.&nbsp;Get more from the app with these <a href="https://support.ebird.org/en/support/solutions/articles/48000966225-merlin-tips-and-tricks#anchorCustomList">Merlin Tips and Tricks</a>.</p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>



<hr>



<h2>See How Merlin Can Help You ID Birds</h2>



<figure><p>
<iframe title="Merlin Bird ID Demo from the Cornell Lab of Ornithology" width="1200" height="675" data-src="https://www.youtube.com/embed/xmSUOLxyatY?feature=oembed&amp;modestbranding=1&amp;showinfo=0&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==" data-load-mode="1"></iframe>
</p></figure>



<hr>



<h2>The Best Birding App, Powered By You</h2>



<p>Merlin features the best of community contributed photos, songs, and calls, tips from experts around the world to help you ID the birds you see, and range maps from <a href="https://birdsoftheworld.org/bow/home">Birds of the World</a>—all powered by billions of bird observations submitted to <a href="https://ebird.org/home">eBird</a>.</p>





      
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Binary Wordle (232 pts)]]></title>
            <link>https://wordle.chengeric.com/</link>
            <guid>44176825</guid>
            <pubDate>Wed, 04 Jun 2025 02:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordle.chengeric.com/">https://wordle.chengeric.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44176825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Use keyboard (0, 1, Enter, Backspace) or buttons to play</p></div><div><p>Sponsor: Don't like waiting on hold? Try</p><!-- --> <p><a href="https://altodial.com/?wordle"><img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://wordle.chengeric.com/square.svg">altodial.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DiffX – Next-Generation Extensible Diff Format (337 pts)]]></title>
            <link>https://diffx.org/</link>
            <guid>44176737</guid>
            <pubDate>Wed, 04 Jun 2025 02:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diffx.org/">https://diffx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176737">Hacker News</a></p>
Couldn't get https://diffx.org/: Error: getaddrinfo ENOTFOUND diffx.org]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anybody built search on top of Anna's Archive? (248 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176514</link>
            <guid>44176514</guid>
            <pubDate>Wed, 04 Jun 2025 01:47:26 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44176767"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176767" href="https://news.ycombinator.com/vote?id=44176767&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Honestly I don't think it would be that costly, but it would take a pretty long time to put together. I have a (few years old) copy of Library Genesis converted to plaintext and it's around 1TB. I think libgen proper was 50-100TB at the time, so we can probably assume that AA (~1PB) would be around 10-20TB when converted to plaintext. You'd probably spend several weeks torrenting a chunk of the archive, converting everything in it to plaintext, deleting the originals, then repeating with a new chunk until you have plaintext versions of everything in the archive. Then indexing all that for full text search would take even more storage and even more time, but still perfectly doable on commodity hardware.</p><p>The main barriers are going to be reliably extracting plaintext from the myriad of formats in the archive, cleaning up the data, and selecting a decent full text search database (god help you if you pick wrong and decide you want to switch and re-index everything later).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177936"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177936" href="https://news.ycombinator.com/vote?id=44177936&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>The main barriers for me would be:</p><p>1. Why? Who would use that? What’s the problem with the other search engines? How will it be paid for?</p><p>2. Potential legal issues.</p><p>The technical barriers are at least challenging and interesting.</p><p>Providing a service with significant upfront investment needs with no product or service vision that I’ll likely to be sued for a couple of times a year, probably losing with who knows what kind of punishment… I’ll have to pass unfortunately.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178200" href="https://news.ycombinator.com/vote?id=44178200&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It would be incredible for LLMs. Searching it, using it as training data, etc. Would probably have to be done in Russia or some other country that doesn't respect international copyright though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178241"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178241" href="https://news.ycombinator.com/vote?id=44178241&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Do you have a reason to believe this ain't already being done? I would assume that the big guys like openai are already training on basically all text in existence.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178453"><td></td></tr>
                  <tr id="44178237"><td></td></tr>
                        <tr id="44177444"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177444" href="https://news.ycombinator.com/vote?id=44177444&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I think there’s a couple ways to improve it:</p><p>1. There’s a lot of variants of the same book. We only need one for the index. Perhaps for each ISBN, select the format easiest to parse.</p><p>2. We can download, convert and index top 100K books first, launch with these, and then continue indexing and adding other books.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177679"><td></td></tr>
                <tr id="44178316"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178316" href="https://news.ycombinator.com/vote?id=44178316&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>From a good search perspective though you probably dont want 500 different versions of the same book popping up for a query</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44177996"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177996" href="https://news.ycombinator.com/vote?id=44177996&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I wonder if you could implement it with only static hosting?</p><p>We would need to split the index into a lot of smaller files that can be practically downloaded by browsers, maybe 20 MB each.
The user types in a search query, the browser hashes the query and downloads the corresponding index file which contains only results for that hashed query. Then the browser sifts quickly through that file and gives you the result.</p><p>Hosting this would be cheap, but the main barriers remain..</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177999"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177999" href="https://news.ycombinator.com/vote?id=44177999&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>It's trivial to normalise the various formats, and there were a few libraries and ML models to help parse PDFs. I was tinkering around with something like this for academic papers in Zotero, and the main issue I ran into was words spilling over to the next page, and footnotes. I totally gave up on that endeavour several years ago, but the tooling has probably matured exponentially since then.</p><p>As an example, all the academic paper hubs have been using this technology for decades.</p><p>I'd wager that <i>all</i> of the big Gen AI companies have planned to use this exact dataset, and many or them probably have already.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178133"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178133" href="https://news.ycombinator.com/vote?id=44178133&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>&gt; It's trivial to normalise the various formats,</p><p>Ha. Ha. ha ha ha.</p><p>As someone who as pretty broadly tried to normalize a pile of books and documents I have legitimate access to, <i>no it is not</i>.</p><p>You can get good results 80% of the time, usable but messy results 18% of the time, and complete garbage the remaining 2%. More effort seems to only result in marginal improvements.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178321"><td></td></tr>
                              <tr id="44178333"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178333" href="https://news.ycombinator.com/vote?id=44178333&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There is a search solution for zipped fb2 files. Not exactly what you need, but it has potential.</p><p>The project has similar story to Anna's archive. There is 0.5 TB of archived books, and the project creates index of all the books with text, title and aruthor search capabilities, gives html UI for search and reading. On weak machine it takes about 2 hours to build that index.</p><p>So if you have zipped archives of fb2, you can use the project to create web UI with search for those files. Without need of enough space to unpack all the files.</p><p>You'll have to translate some russian though to get instructions on how to set it up.</p><p><a href="https://gitlab.com/opennota/fb2index/-/blob/master/README.ru.md" rel="nofollow">https://gitlab.com/opennota/fb2index/-/blob/master/README.ru...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178203"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178203" href="https://news.ycombinator.com/vote?id=44178203&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There’s an android app called OpenLip. [1]</p><p>Description:</p><p>Openlib is an open source app to download and read books from shadow library (Anna’s Archive). The App Has Built In Reader to Read Books.</p><p>As Anna’s Archive doesn't have an API, the app works by sending requests to Anna’s Archive and parses the response to objects. The app extracts the mirrors from the responses, downloads the book and stores it in the application's document directory.</p><p>Note :
The app requires VPN to function properly . Without VPN the might show the captcha required page even after completing the captcha</p><p>Main Features:</p><p>Trending Books</p><p>Download And Read Books With In-Built Viewer</p><p>Supports Epub And Pdf Formats</p><p>Open Books With Your Favourite Ebooks Reader</p><p>Filter Books</p><p>Sort Books</p><p>[1]: <a href="https://f-droid.org/de/packages/com.app.openlib/" rel="nofollow">https://f-droid.org/de/packages/com.app.openlib/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44176569"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176569" href="https://news.ycombinator.com/vote?id=44176569&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>You must mean free text search and page level return, because it already has full metadata indexing.</p><p>The thing is AA doesn't hold the texts. They're disputable IPR and even a derived work would be a legal target.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178214" href="https://news.ycombinator.com/vote?id=44178214&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Z-Library has a keyword search. Personally i didn't find it too useful, especially given Google Books exists. It's not easy to create a quality book search engine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177457" href="https://news.ycombinator.com/vote?id=44177457&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>As far as I know, no one has fully implemented full-text search directly over Anna's Archive. Technically it’s feasible with tools like Meilisearch, Elasticsearch, or Lucene, but the main challenges are:</p><pre><code>    Converting all documents (PDFs, EPUBs, etc.) to clean plaintext.

    Indexing at scale efficiently.

    Managing potential legal issues.
</code></pre><p>
Z-Library does something similar, but it’s smaller in scope and doesn't integrate AA’s full catalog.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177592"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177592" href="https://news.ycombinator.com/vote?id=44177592&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I’ve done something like this before. Meilisearch will not be viable, because it indexes very slow and it takes up a lot of space.</p><p>In my experience only Tantivy can index this much data. Check out Lnx.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178248"><td></td></tr>
                        <tr id="44177894"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177894" href="https://news.ycombinator.com/vote?id=44177894&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Related question, has Anna's archive been thoroughly filtered for non-copyright-related illegal material? Pedo, terrorism, etc. I've considered downloading a few chunks of it but I'm worried of ending up with content I really don't want to be anywhere near from.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178086" href="https://news.ycombinator.com/vote?id=44178086&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>This is a really strange question to be honest you could ask this literally about any download let alone simply torrents of documents.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178012"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178012" href="https://news.ycombinator.com/vote?id=44178012&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>How might you inadvertently download illegal content while searching for legal content?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178070"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178070" href="https://news.ycombinator.com/vote?id=44178070&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>He said he wants to download lots of it in general, not specifical. Legit question, if you end up with dark material.</p><p>I would assume pedo stuff is not really there, but the anarchist cookbook and alike likely will be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178167"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178167" href="https://news.ycombinator.com/vote?id=44178167&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I'm still not sure the question makes much sense, if it's a general: "I want to support the project and so I want to seed a large chunk" Okay, I guess it's your due diligence to check, but there is a reporting feature built in, if something is found, report it.</p><p>Aside from that, if you're searching for specific content, the question is moot I guess.</p><p>I guess my confusion is what distinguishes this apart from any other torrent ? That is, if the submitted content is submitted at all.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178210"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178210" href="https://news.ycombinator.com/vote?id=44178210&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I understood it as he or she wants to download large chunks of potentially interesting books for offline use, or once Anna goes down. So a broad filter. Not for seeding.</p><p>But thanks for the explanation that there is a report build in.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178137"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178137" href="https://news.ycombinator.com/vote?id=44178137&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Considering the anarchist cookbook is just a rebranded selection of freely-available US Army Field Manuals, ... I don't see the problem.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178201"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178201" href="https://news.ycombinator.com/vote?id=44178201&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>I don't either, but many states have laws regarding books on how to build bombs and they might get enforced more than copyright.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178127"><td></td></tr>
                <tr id="44178218"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178218" href="https://news.ycombinator.com/vote?id=44178218&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Well, I won't. But does it contain just text or real pictures? That would make a big legal difference I assume.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178277"><td></td></tr>
                                    <tr id="44176864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176864" href="https://news.ycombinator.com/vote?id=44176864&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>AFAIK, Z-Library already does this, to some extent. Basic full-text queries do search inside the body of books and articles.</p><p>It's a bit smaller than Anna's Archive, as they do host their own collections. From some locations, it's only easy to access through Tor.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178107" href="https://news.ycombinator.com/vote?id=44178107&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Has anyone explored a different angle — like mapping out the 1,000 most frequently mentioned or cited books (across HN, Substack, Twitter, etc.), then turning their raw content into clean, structured data optimized for LLMs? Imagine curating these into thematic shelves — say, “Bill Gates’ Bookshelf” or “HN Canon” — and building an indie portal where anyone can semantically search across these high-signal texts. Kind of like an AI-searchable personal library of the internet’s favorite books.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178130"><td></td></tr>
                  <tr id="44177619"><td></td></tr>
            <tr id="44177170"><td></td></tr>
                <tr id="44178025"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178025" href="https://news.ycombinator.com/vote?id=44178025&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It's not exactly clear, but OP is asking about indexing the content of all the documents, not the metadata (e.g. titles etc)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44177542"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177542" href="https://news.ycombinator.com/vote?id=44177542&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Mebbe easier to just search Amazon or Goodreads. Like site:amazon.ca &lt;query words&gt; as someone has mentioned below.</p><p>Every book has an ISBN 10 or 13 digit ISBN number to identify them. Unless it's some self-pub/amateur-hour situation by some paranoid prepper living in a faraday-cage-protected cage in Arkansas or Florida it's likely a publication with a title, an author and an ISBN number.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177831"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Startup getting spammed with PayPal disputes, what should we do? (204 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176510</link>
            <guid>44176510</guid>
            <pubDate>Wed, 04 Jun 2025 01:46:49 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44176510">
      <td><span></span></td>      <td><center><a id="up_44176510" href="https://news.ycombinator.com/vote?id=44176510&amp;how=up&amp;goto=item%3Fid%3D44176510"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44176510">Ask HN: Startup getting spammed with PayPal disputes, what should we do?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44176510">115 points</span> by <a href="https://news.ycombinator.com/user?id=june3739">june3739</a> <span title="2025-06-04T01:46:49 1749001609"><a href="https://news.ycombinator.com/item?id=44176510">12 hours ago</a></span> <span id="unv_44176510"></span> | <a href="https://news.ycombinator.com/hide?id=44176510&amp;goto=item%3Fid%3D44176510">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Startup%20getting%20spammed%20with%20PayPal%20disputes%2C%20what%20should%20we%20do%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44176510&amp;auth=a5e7334e20b47144b4b9f3ad251dd0ab5bc8265e">favorite</a> | <a href="https://news.ycombinator.com/item?id=44176510">92&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Longtime user posting from a new account out of an abundance of caution.</p><p>I founded an e-commerce marketplace startup. We use PayPal's Multiparty APIs (PayPal Commerce Platform) for checkout. For the 10 days, someone has been bombarding us with purchases that they later dispute. There's consistent pattern to it:</p><p>* They use an email address that has no footprint online, always from the same two domains
* They use an unverified PayPal account to pay
* They pay a low amount, not always the same, in a narrow range for a digital item
* All of the charges were disputed within a few hours</p><p>They're not doing this through our API. The purchase process requires a browser because of the way our payment form is configured. There's an amount of variation to each purchase that tells us they're automating a browser. Logs indicate that they're changing IP each time. The events come in bursts and seem to be spaced to avoid automated detection.</p><p>We added the typical mitigations to our network stack and code. A few are still slipping through. Logs indicate a high amount of bot traffic.</p><p>PayPal does not seem equipped to deal with this. Their support is always extremely slow, relies on canned responses, and to date has a very limited understanding of how their own Multiparty APIs work. Their phone support people will not talk with me, they see no indication that my PayPal account is affiliated with these purchases in any way. They want each of our sellers to contact them independently, which we know will result in disparate cases that don't tell the complete story or offer any assistance.</p><p>Has anyone encountered anything like this before? We're struggling to find the motive or intended outcome by the attacker(s). We're a small company with a niche audience, we've never had a conflict with anyone that got serious enough that we'd expect them to come after us like this.</p><p>Any thoughts and recommendations would be greatly appreciated. We feel like we are on our own here and are unsure of how to handle it.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A manager is not your best friend (182 pts)]]></title>
            <link>https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</link>
            <guid>44176425</guid>
            <pubDate>Wed, 04 Jun 2025 01:29:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html">https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</a>, See on <a href="https://news.ycombinator.com/item?id=44176425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>As people become managers, it’s quite common for their team members to want to commiserate with them. This is especially true for friendly, competent, reasonable-seeming managers – people want to commiserate with <em>winners</em>. This makes commiseration extra dangerous, as it comes with a hint of flattery (“I respect your opinion and trust your discretion”).</p>

<p>But commiseration, especially with your direct reports, is organizational poison. It erodes the fabric of an organization and builds factions. It leads to feelings of superiority and creates a low-trust environment – <em>even if what you’re complaining about is made up!</em> Worst of all, it doesn’t give other teams an opportunity to improve. If I think that HR sucks, and I commiserate with my directs about it, my team is going to treat them poorly. HR will never know why, will never fix the problem, and will just think that my team are jerks (and they’ll arguably be right). Commiseration is self-fulfilling because it’s a form of victimhood: The world is conspiring against us, the only truly virtuous team.</p>

<p>Commiseration comes naturally to most people, because it happens all the time in real life. As your best friend, if you come to me saying that you were just dumped by your girlfriend, you will get unconditional sympathy beyond words. You’re the best, she didn’t deserve you, you can do so much better, everyone knows you’re the man, I have no idea why you ever spent time with her. There will be no questions, there will be no need for you to explain anything about the situation, even if all you did for the last 2 years was sit on your couch smoking weed and playing Warzone, <em>Greg</em>. Unless you did something totally crazy or illegal, my loyalty will be immediate and unconditional, because – let’s be real – none of it matters. You’re going your way, she’s going hers, and it’s over.</p>

<p>As a manager, your empathy needs to be highly conditional. Your job is to get to the truth of a matter in a respectful way, not make your team feel good. You are largely stuck with your coworkers, and you need to get stuff done together or everyone suffers. If you break up with your girlfriend you get unconditional sympathy. But if you break up with your girlfriend, and the 3 of us were trying to climb Mt. Everest together, I’m going to be a lot more measured in how I communicate and balance your relationship so that we can all survive the next few days.</p>

<p>Commiseration is generally a sensitive topic, so I’ve tried to boil down how to handle situations when a direct comes to you with grievances via some heuristics:</p>

<ul>
  <li>You don’t really want to debate your team in every situation, but your job is to essentially be a scientist and get to the bottom of what’s going on. If someone wants to commiserate about some other team, your first job is to ask a bunch of questions about what’s going on. In my experience, 90% of the time the situation is a grey area, and probably 30% of the time the person who wants to commiserate is actually in the wrong, on balance.</li>
  <li>Your role as a manager is also to be a perspective-creator. Sure, that salesperson was overly optimistic on how impactful this custom feature was for a prospect. And sure, the deal wasn’t as large and didn’t close as quickly as they said it would. But sales incentives are a law of the universe, and sales directors need to manage around them just as much as product teams do, because <em>not</em> having sales incentives is even worse. And by the way, we’re not so great at estimating development timelines either. Everyone’s blood pressure should always be lower after they’ve spoken to you.</li>
  <li>Bad therapists just let you rant. Good therapists let you vent, but they ask clarifying questions, and they sometimes push back. The phrase “is that actually true?” or “Can you explain that more?” are your friends. Good therapists validate feelings but they don’t necessarily validate <em>facts</em>. “I know you feel like you’re being a good daughter” is not the same as “you are the best daughter.” You want to be a good therapist.</li>
  <li>Remove the phrase “I don’t know why they…” from your lexicon. No matter how you end this sentence, the subtext will be clear: “I don’t know why they’re so incompetent.” Instead, it’s often better to give the most optimistic view for why another team is behaving the way that they are. It might not be <em>right</em>, but it builds empathy which is the bedrock on which productive collaboration is built.</li>
  <li>If someone is trying to get you to commiserate with them, try to speak in terms of reiterating a decision framework. Rather than “marketing doesn’t know what they’re doing,” you want to say something like “our role is to build the product and have a strong POV for marketing, and their role is to make sure that our launch generates enough pipeline. If you don’t think that’s going to happen then let’s talk to them.” The goal is to focus on objective truths rather than disparaging opinions.</li>
  <li>When it comes to commiseration, people are highly attuned to nuanced communication – especially from their boss. “Well guys, we’ve got this” plus that little head nod and eyeroll is functionally the equivalent of saying “it’s all on us, the protagonists, because everyone else is a fucking idiot <strong>again</strong>.” Those words didn’t literally leave your mouth, but you effectively said it, and as a manager that’s 100% on you. As a manager your implicit communication is just as important as your explicit communication – this is not a courtroom, this is real life, and non-verbal actions can still have consequences.</li>
  <li>One of the most common people to commiserate about is your own boss, or the company’s CEO. This can get highly toxic fast, and is rarely actually productive – cases of teams changing their CEO’s behavior through commiseration are vanishingly rare. The right way to pivot this conversation (or at least, the only way I’ve ever seen this play out positively) is to discuss how you can most effectively work with your boss. This is significantly more productive, and even if you still think they’re being dumb, at least you’re tackling that problem constructively.</li>
</ul>

<p>Of course sometimes people really are AAA grade idiots. When this happens, your communication should typically address the issue, not the other team. For most situations, it’s best to say “let me follow up,” rather than “I agree that they’re dumb.” In particularly egregious cases, you can go with “I know this is a problem and I’ll get on it” or “I hear you, I’m working on it, but I can’t give you every detail on how and don’t expect ongoing updates” – this avoids gaslighting them that everything is fine, but it also stops the vent session. When the door opens to commiseration with your team, you must slam it shut.</p>

<p>Either way, following up is the ideal next step because it commits you to respond but separates the emotion from the action. Accumulated strong emotions leave a strong impression, especially when they’re negative emotions like bitterness, so it’s best to suck the emotion out of the conversation as fast as possible. For evidence of this, light autists often make very effective managers.</p>

<p>Finally – we’re all human here, and sometimes you need to commiserate with <em>someone</em> before your head explodes. If you must commiserate, it’s almost always best if they’re a peer / near peer, and they’re not on your direct team (you don’t share a boss). This at least dodges the situation where a manager complains alongside their team, and thereby implicitly blesses their most negative views.</p>


    

    

    

    




  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta pauses mobile port tracking tech on Android after researchers cry foul (127 pts)]]></title>
            <link>https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/</link>
            <guid>44175940</guid>
            <pubDate>Tue, 03 Jun 2025 23:42:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/">https://www.theregister.com/2025/06/03/meta_pauses_android_tracking_tech/</a>, See on <a href="https://news.ycombinator.com/item?id=44175940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Security researchers say Meta and Yandex used native Android apps to listen on localhost ports, allowing them to link web browsing data to user identities and bypass typical privacy protections.</p>
<p>Following the disclosure, researchers observed that Meta's Pixel script stopped sending data to localhost and that the tracking code was largely removed. The move may help Meta avoid scrutiny under Google Play policies, which prohibit covert data collection in apps.</p>
<p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," a Meta spokesperson told <em>The Register</em>. "Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p>

    

<p>Meta's spokesperson did not respond to a request to elaborate on the company's discussions with Google.</p>
<h3>What the researchers found</h3>
<p>In a <a target="_blank" rel="nofollow" href="https://localmess.github.io/">report</a> published Tuesday, computer scientists affiliated with IMDEA Networks (Spain), Radboud University (The Netherlands), and KU Leuven (Belgium) describe how the US social media giant and the Russian search engine were observed using native Android apps to gather web cookie data via the device's loopback interface, commonly known as localhost.</p>
<p>Localhost is a loopback address that a device can use to make a network request to itself. It's commonly used by software developers to test server-based applications like websites on local hardware.</p>

        


        

<p>The researchers – Aniketh Girish (PhD student), Gunes Acar (Assistant Professor), Narseo Vallina-Rodriguez (Associate Professor), Nipuna Weerasekara (PhD student), and Tim Vlummens (PhD student) – say they found native Android apps, including Facebook and Instagram, and Yandex's Maps and Browser – that listen silently on fixed local ports for tracking purposes.</p>
<p>"These native Android apps receive browsers' metadata, cookies and commands from the Meta Pixel and Yandex Metrica scripts embedded on thousands of websites," the computer scientists explain. "These JavaScripts load on users' mobile browsers and silently connect with native apps running on the same device through localhost sockets."</p>

        

<p>As these native apps access device identifiers like the Android Advertising ID or handle user identities in Meta apps, the researchers say, they're able to link mobile browsing sessions and web cookies to user identities.</p>
<p>Essentially, by opening localhost ports that allow their Android apps to receive tracking data, such as cookies and browser metadata, from scripts running in mobile browsers, Meta and Yandex are able to bypass common privacy safeguards like cookie clearing, Incognito Mode, and Android's app permission system.</p>
<p>The technique also violates assumptions about the scope of first-party cookies, which aren't supposed to be able to track browsing activity across different websites. According to the researchers, "the method we disclose allows the linking of the different _fbp cookies to the same user, which bypasses existing protections and runs counter to user expectations."</p>

        

<p>With regard to Meta, the tracking process involves scripts associated with <a target="_blank" rel="nofollow" href="https://www.facebook.com/business/tools/meta-pixel">Meta Pixel</a>, analytics code used by marketers to gather data about interactions with websites.</p>
<p>Various APIs and protocols can be used to implement the described app-web eavesdropping scheme. These include: SDP munging, which involves manually modifying Session Description Protocol (SDP) messages before the data gets passed to the browser; real-time communications protocols <a target="_blank" rel="nofollow" href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">Websocket</a> and <a target="_blank" rel="nofollow" href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Protocols">WebRTC</a>; Session Traversal Utilities for NAT (STUN), an address discovery mechanism; and Traversal Using Relays around NAT (TURN), a router restriction bypass method.</p>
<ul>

<li><a href="https://www.theregister.com/2025/06/03/xs_new_encrypted_xchat_feature/">X's new 'encrypted' XChat feature seems no more secure than the failure that came before it</a></li>

<li><a href="https://www.theregister.com/2025/05/30/meta_is_now_a_defense/">Meta – yep, Facebook Meta – is now a defense contractor</a></li>

<li><a href="https://www.theregister.com/2025/05/29/billions_of_cookies_available/">Billions of cookies up for grabs as experts warn over session security</a></li>

<li><a href="https://www.theregister.com/2025/05/22/irish_data_protection_commission_gives/">Irish privacy watchdog OKs Meta to train AI on EU folks' posts</a></li>
</ul>
<p>The researchers describe Meta's approach thus:</p>
<blockquote>
<ol>

<li>The user opens the native Facebook or Instagram app, which eventually is sent to the background and creates a background service to listen for incoming traffic on a TCP port (12387 or 12388) and a UDP port (the first unoccupied port in 12580-12585). Users must be logged-in with their credentials on the apps.</li>

<li>The user opens their browser and visits a website integrating the Meta Pixel.</li>

<li>At this stage, websites may ask for consent depending on the website's and visitor's locations.</li>

<li>The Meta Pixel script sends the <a target="_blank" rel="nofollow" href="https://localmess.github.io/#about_fbp">_fbp cookie</a> to the native Instagram or Facebook app via WebRTC (STUN) <a target="_blank" rel="nofollow" href="https://webrtchacks.com/not-a-guide-to-sdp-munging/">SDP Munging</a>.</li>

<li>The Meta Pixel script also sends the _fbp value in a request to https://www.facebook.com/tr along with other parameters such as page URL (dl), website and browser metadata, and the <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531104925/https://developers.facebook.com/docs/meta-pixel/reference/">event type</a> (ev) (e.g., PageView, AddToCart, Donate, Purchase).</li>

<li>The Facebook or Instagram apps receive the _fbp cookie from the Meta Pixel JavaScript running on the browser. The apps transmit _fbp as a GraphQL mutation to (https://graph[.]facebook[.]com/graphql) along with other persistent user identifiers, linking users' fbp ID (web visit) with their Facebook or Instagram account.</li>
</ol>
</blockquote>
<p>Researchers observed Meta implementing this technique starting in September 2024, transmitting data via HTTP. Third-party developers working with Meta APIs noted and questioned the behavior in <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531105747/https://developers.facebook.com/community/threads/317050484803752/">forum</a> <a target="_blank" rel="nofollow" href="https://web.archive.org/web/20250531105711/https://developers.facebook.com/community/threads/937149104821259/">posts</a> at the time.</p>
<p>HTTP-based data transmission using this technique supposedly ended the following month, but other methods of transmission (WebSocket, WebRTC STUN (w/ SDP Munging), and WebRTC TURN (w/o SDP Munging)) were identified in subsequent months.</p>
<p>Presently, however, Meta's use of these techniques appears to have halted. According to the researchers, "As of June 3rd 7:45 CEST, Meta/Facebook Pixel script is no longer sending any packets or requests to localhost. The code responsible for sending the _fbp cookie has been almost completely removed."</p>
<p>Yandex's use of localhost-based tracking dates back to 2017, according to the researchers.</p>
<p><em>The Register</em> sought to ask Yandex media relations about the researchers' claims but our inquiry was bounced as spam.</p>
<p>The report authors note that their disclosure to Android browser vendors has led to several mitigations.</p>
<p>Chrome 137, which shipped May 26, 2025, includes countermeasures <a target="_blank" rel="nofollow" href="https://webrtc.googlesource.com/src.git/+/72d6d748ddbe5d7f63ba5f2dd1ce195a342c0a12">to block the SDP Munging</a> technique used by Meta Pixel, though these have only been made available to a subset of users participating in a gated field trial. A fix is currently being developed for Mozilla Firefox. Brave is unaffected as it <a target="_blank" rel="nofollow" href="https://brave.com/privacy-updates/27-localhost-permission/">requires consent for localhost</a> use. And DuckDuckGo has modified its blocklist to stop Yandex's scripts.</p>
<p>Beyond these, the authors suggest a Google <a target="_blank" rel="nofollow" href="https://github.com/explainers-by-googlers/local-network-access">proposal</a> to create a new "local network access" permission that could help mitigate localhost-based tracking in the future. A <a target="_blank" rel="nofollow" href="https://wicg.github.io/private-network-access/">prior proposal</a> along these lines ran into <a target="_blank" rel="nofollow" href="https://developer.chrome.com/blog/pna-on-hold">technical barriers</a>. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain aging shows nonlinear transitions, suggesting a midlife "critical window" (251 pts)]]></title>
            <link>https://www.pnas.org/doi/10.1073/pnas.2416433122</link>
            <guid>44175905</guid>
            <pubDate>Tue, 03 Jun 2025 23:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pnas.org/doi/10.1073/pnas.2416433122">https://www.pnas.org/doi/10.1073/pnas.2416433122</a>, See on <a href="https://news.ycombinator.com/item?id=44175905">Hacker News</a></p>
Couldn't get https://www.pnas.org/doi/10.1073/pnas.2416433122: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Precious Plastic Is in Trouble (294 pts)]]></title>
            <link>https://www.preciousplastic.com//news/problems-in-precious-plastic</link>
            <guid>44175773</guid>
            <pubDate>Tue, 03 Jun 2025 23:11:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.preciousplastic.com//news/problems-in-precious-plastic">https://www.preciousplastic.com//news/problems-in-precious-plastic</a>, See on <a href="https://news.ycombinator.com/item?id=44175773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h2>is in trouble</h2></p><div><p>Hey world</p><p>This is a heavy message to send, but essential for the future of Precious Plastic. It will either make it or not. In this post I’ll give a detailed overview of all the current problems we have, how it got to this point and whats next. A short summarised video is below. No <em>need</em> to watch, it's all in the text.<br>‍</p></div><p><iframe src="https://www.youtube.com/embed/4gTd36cQLzY?rel=0&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="" title="Precious Plastic is in trouble. Really"></iframe></p><p><strong>Our last big development<br>‍</strong>Lets start with our last big development. &nbsp;2020, when we released Version 4, this was our latest release. We worked about 1,5 year with over +100 people from around the world. We developed the first ‘Pro’ Machines, a Sheetpress, Starterkits, Business calculators, new moulds, products and more. Looking back this was quite a unique moment. A lot of passionate work was done by volunteers and everything was shared Open Source online for free. Here some of the things we made<br></p><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa486e30e592d1032dd32_v4.jpg 2004w" alt=""></p><p><img src="https://s11.gifyu.com/images/SGTIk.gif"></p><div><p>With a relatively small amount of money, we reached a global impact in 2023 of over 1100 organzations in 56 countries, who recycled 1.400.000KG Plastic, they generated together a revenue of +$3.7 Million, employed 530 people, works with 3.405 volunteers and built 1.175 machines. (And this is only from the workspaces who shared their data last year) Learn more about our impact <a href="https://www.preciousplastic.com/impact/2024" target="_blank">here</a>.<br>‍<strong><br>☝️This was the good part. Lets get into our problems</strong></p><p> <strong>How we work<br></strong>In order to understand our problems it’s important to know how Precious Plastic is developed since its unusual. We work in what we call Versions. Version 1 got released in 2013 and the latest one 4 years ago. The principle is, we develop a lot of new things, share them online for free. And then whoever was involved takes a holiday or goes off to something else. They really have to go because at that point we spend all our money. The workspace is empty, the work drops, nothing new is made, we wait. Up until now for some magical reason we always received a gift to continue moving forward. From winning awards, to getting a big workspace donated. Whenever we got enough resources we assembled a team and started developing again.. Each time the team and amount of work grew. Until the latest version, here the problems started.</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.984375px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6cb2fc0a22da432f1f6_work%20done%20chart.jpg 4247w" alt=""></p><div><p><strong>When the Problems started<br>‍</strong>We worked in versions not because we wanted to, but because we had to. We share everything online for free because we believe recycling knowledge should be available for everyone. As a consequence we don’t generate enough income to pay a team all year around. But after Version 4 a small group of seven dedicated ambitious volunteers wanted to try and sustain Precious Plastic all year around. So we can continue development and have a bigger impact to reduce plastic waste. This by itself is a hard job but we also had some extra problems on the way</p><p>‍<strong>#1 No workspace<br>‍</strong>The team was ready to continue work. But just a few weeks later Covid-19 came to the world, but this wasn’t our main problem. Our problem was Chrome-6, a chemical the municipality found in the paint from the building that was applied 40 years ago. Which meant we had to leave the workspace fast, and the building was large, we had a lot of machines and items to sell, in a short amount of time, during lockdowns. This meant we had to sell many things below value since that period most people were looking to buy bread machines, not robot arms. After the exhausting/rushed job of leaving our workspace we could stay in the garage/shed in the house of one of the team members in France. It was nice we had a base to continue, but it was much smaller and temporary. It was quite a downgrade.</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.984375px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa6f962c412ee92336560_workspace.jpg 3893w" alt=""></p><div><p><strong>#2 No business model<br>‍</strong>One of the main goals of this small team was trying to find a business model that would serve Precious Plastic’s mission, bring income to sustain a team and not compete with the rest of the community. The last one is difficult, because our most logical and common model would be to sell machines and moulds. There are not many Open Source Hardware projects like Precious Plastic in the world. And the ones that exist mostly sell their products. Arduino sells their circuit boards, Prusa sells 3d printers. However in our case we’re also building a global network of machine builders that can provide local recycling equipment. We didn't want to compete with our own community.</p><p>The plan was to do Projects or as we called them, Collabs. It meant to help others set up their projects. Sort of a consultancy. The team setup quite a few cool projects, from a refugee camp in a desert in Algeria to a big Sheetpress in Nigeria. And meanwhile released a few new machine drawings. The model somewhat worked, we continued development and could do meaningful work. However it was financially always tight and often didn’t bring in enough money to sustain the team (that was getting paid minimum Dutch wage). But what really broke it was the next problem</p><p>‍<strong>#3 Lawsuit<br>‍</strong>As the team was setting up projects we worked with different clients around the world. Working for clients was completely new for us and we're learning along the way. And one of the first projects we setup was in Manhattan, New York for a cosmetic company to recycle their packaging. We helped to set it up, got machines from a community member and local people ran it. However after a period of time an accident happened with someone using the machine, which was very unfortunate. And in the US, especially NY this means you need to get lawyers. What happened, who is responsible? Is it the company that hired us all, Precious Plastic (back then operating under One Army Entity) for organising it, was it a result of bad operational instructions, misuse of the machine or a fault in the machine from the community member? We analysed it and are convinced that we are not to blame. But we do not know what a judge is going to say. Meanwhile this has been going on for the last 2 years. Lots of paperwork and documents need to be filed with lawyers that charge up to $600/h, sending emails got painful. Being in a lawsuit in New York is very costly. On top of that we didn't have insurance, meaning we have to pay for it from our own tiny pockets.</p><p>‍<strong>#4 Software, heavily underestimated<br>‍</strong>During Version 4 we started developing our own Community Platform. It’s software that is the digital home of our community that helps members to document, share knowledge and find others to collaborate with. It’s developed in collaboration with our other projects and there is a lot more information about it here. Anyway this has been a massive project and the original complexity of it was underestimated. Took waaay more effort than we thought. In the recent years we’ve been hard at work to catch up and the platform got much more mature with many more features. But realistically this has been a hit on our online community since the ‘digital home’ wasn’t good enough to host everyone. We invested a lot in this so far and will continue to be a big project so please use the platform, give feedback so we can improve it and help us code it.</p><p>‍<strong>#5 Open Source community<br>‍</strong>At Precious Plastic we want to enable more people to recycle plastic. Plastic waste is a big global problem and needs many people collaborating in every corner of the world to fix it. That's why we give everything Open Source for free so everyone has access. On top of that a big part of Version 4 was to make sure that the people that start recycling workspaces can financially sustain themselves. Because If they can continue to recycle on a daily basis it means less plastic waste. We want all those hard working workspaces to succeed and provide business plans, calculators and instruction to help them. A few years later this resulted in hundreds of workspaces around the world that started a business who manage to recycle on a daily basis, which is great.&nbsp;</p><p>We went all-in on giving. And believe(d?) that sharing Open Source will bring contributions back one way or another. Contributions to support the Precious Plastic Community can be made in various ways.&nbsp;From a financial donation once a recycle-business is profitable, to giving credit or sharing back their knowledge. And many members do contribute something back which is great. However we’ve also been observing quite some established organisations that take more than they give, building business around Precious Plastic but not contributing anything back. It’s allowed, since it’s all Open-Source. But the mentality of only taking things and not contributing will eventually kill a community-driven project like this.&nbsp;</p><p>‍<strong>#6 It is bad designed<br>‍</strong>We don’t blame those for not contributing enough back. We see this as a fault on us. The project wasn’t designed to have a healthy financial model and relation with the community. We were always fully focussed on giving to the community, not us being a financially sustainable organisation. Funny example of this is the recent <a href="https://pposf.preciousplastic.com/" target="_blank">PPOSF</a> (Precious Plastic Open Source Fund). We received a €100K donation. Which was amazing, but we decided to give it all to the community so they can continue developing their projects. Not to sustain the organisation itself. You could see this as a humble move from us, give it to the community. But it isn’t, it’s ineffective, because now we have to bother you with our problems. Ideally we don’t have to do this and you don’t have to worry about us.&nbsp;</p><p><strong>#7 No long term team<br>‍</strong>As you can imagine all of the above problems make it hard for a team member to have a long term perspective in Precious Plastic. Even though the team has been very small, effective and works with many volunteers it has continued to be a struggle to pay everyone every month without worrying. Over time this brings lots of stress and uncertainty, especially if the team members by themselves grow up and need more stability in life.&nbsp;</p><p>‍<strong>Our current setup<br>‍</strong>Precious Plastic is a non-profit for public good (ANBI) setup in the Netherlands. <br>So what does our team and community look like? Here is a funny way to look at it:<br>‍<br><strong>Precious Plastic Community</strong><br>⬥+ 1000 workspaces around the world<br>⬥530 people employed, 3000 volunteers<br>⬥Totalling + $3.7 Million Revenue last year</p><p><strong>Precious Plastic Organisation</strong><br>⬥3 full-time people in the team<br>⬥Quarterly running costs €30K<br>⬥6 Months before out of money<br>⬥No workspace. Everyone is fully remote</p><p>As you can see not many team members to manage a large community. There are many areas we should work on but simply cannot. The team spends most of its time making sure just the basics are up and running and community members can continue to recycle and use our tools. And even with this small effective team we only have enough money to sustain for the coming 6 months. The future doesn’t look good.<br>‍</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.984375px, (max-width: 1919px) 71vw, 799.984375px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa8bb671bebd39224838b_next%20options1.jpg 2339w" alt=""></p><div><p><strong>What is next?<br>‍</strong>I’ve been thinking about this. What should Precious Plastic do?&nbsp;<br>‍<strong>1: ☠️ Let Precious Plastic Die:</strong> It build a global community of recyclers, it achieved its mission and that’s it. We learned valuable lessons and the community will probably stay online for a bit but would slowly fade away.<br>‍<strong>2: 💪 Push it to the next level:</strong> There is still lots of plastic waste around the world. We need way more people recycling and R&amp;D on other plastic types. The community needs to grow.</p><p>To be honest I personally could be at peace with both of these directions. It’s amazing what Precious Plastic developed with a relatively small budget and passionate people volunteering their time. Many lessons learned. But an Open Source project like this needs many people caring for it in order to stay alive. If there isn't a large supportive community it will naturally die. It goes beyond the power of an individual. </p><p>That said, what many people might not realise, is that we would waste an unused potential we can currently unlock. We spend years building a global community of recyclers. Rolling out new improved tools has a much higher impact than before, we can reach the right people in many areas in the world. Plus we have clear visibility on our problems and are after all these years very close to having a healthy organisational cycle, see the chart below. All of this makes me think about giving it one last push to finish it. Version 5.&nbsp;</p></div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa82f77198680abac2722_circle.jpg 2841w" alt=""></p><p><strong>What is Version 5?</strong><br>We have many ideas for a Version 5 and would love to work on this. But as you can see from this long text (thanks for reading all the way) we are in a big dip and have some big problems. Whatever we need to develop to get out of this needs to serve the community + the organisation itself. It will be made in a way that it can financially sustain itself afterwards. Something we never took into account. It will mean rebuilding things from the ground up, which requires much more help and resources than before. It would be the biggest thing we ever made as you can see below in the graph. Our team is small, our community is large. We can only do this if people like you are willing to support and help out.<br></p><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next.jpg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, (max-width: 991px) 699.9921875px, (max-width: 1919px) 71vw, 799.9921875px" srcset="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-500.jpg 500w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-800.jpg 800w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-1080.jpg 1080w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-1600.jpg 1600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-2000.jpg 2000w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-2600.jpg 2600w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next-p-3200.jpg 3200w, https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/673fa81b44c7ec88e59ad907_next.jpg 5389w" alt=""></p><div><p><strong>How can you help? <br></strong>The first step is gathering support from the community. If not ones cares about Precious Plastic there is no reason for us to continue to work on it. You can do this by "showing your support". Next we need to find resources to develop the next phase of the project, so you can help us "raise funds for V5".<strong></strong></p></div><div data-hover="false" data-delay="0"><div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/674c67fa11910bf4b8d514c0_support.png" loading="lazy" alt=""></p><p>Show your support</p></div><nav><div data-hover="false" data-delay="0"><div><p>Subscribe to our Youtube Channel</p></div><nav><div><p>The video you just watched is on the One Army Channel. However, Precious Plastic now has its own channel, which not many people know about. A big boost to our channel helps with the YouTube algorithm, allowing us to share more videos in the future.&nbsp;</p><p>‍<a href="https://www.youtube.com/@Precious_PlasticHQ/?sub_confirmation=1" target="_blank">Click here</a> to subscribe.</p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>We’re focused on recycling more plastic and making a real impact. Your support helps us drive this mission forward and generate actual income for our team.&nbsp;If you want to make a difference, <a href="https://www.preciousplastic.com/support" target="_blank">click here</a> to donate.</p></nav></div><div data-hover="false" data-delay="0"><nav><p><a href="https://www.patreon.com/one_army/membership" target="_blank">Join us on Patreon</a> to support our mission monthly. Your contribution helps create a stable income for our work. Plus, you'll get exclusive updates on our progress.</p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you're an individual, <a href="https://bazar.preciousplastic.com/products/" target="_blank">buy recycled plastic products</a> on the Bazar, like carabiners, keychains, earrings, rulers, phone cases, anything really. And if you're a workspace looking to expand, consider buying machines on the Bazar. Remember, 5% of every sale goes to Precious Plastic, which directly supports small scale plastic recycling.</p><p>Sellers, this is your chance to contribute—sell your items on the Bazar. Your sales make a big difference; that 5% income is used to improve the online marketplace itself. Avoiding the need for all the individual workspaces having to setup their own webshops. Together, we can create one strong marketplace that benefits us all.</p><p>‍</p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>The lawsuit in New York is still ongoing. We are currently facing significant costs and need pro-bono assistance from lawyers in the Netherlands or the US to help us navigate this process. If you can assist us, please send us an email to <a href="mailto:hello@preciousplastic.com?subject=Legal%20help">hello@preciousplastic.com</a></p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>Our Community Platform is a big software project, and honestly, it's more complex than we initially thought. Building the infrastructure to host a large global community. We're doing it all open source so others can use it and help out. If you're interested in helping build this, we could really use your skills.Check out the details on <a href="https://github.com/ONEARMY/community-platform" target="_blank">GitHub</a>.</p><p>⭐️ For an easy help&gt; Giving a start to the repository is useful to attract more contributors!</p></div></nav></div><div data-hover="false" data-delay="0"><div><p>Use the Community Platform</p></div><nav><div><p>Interaction on the Community Platform is essential, we're building the home for our online community. By answering Questions and uploading knowledge—such as How-tos, Research and replying to comments—you contribute to a collective resource that reflects the global activity of the community. </p><p>All this activity helps gather valuable feedback that steers our ongoing improvements, we release weekly fixes based on user insights.</p><p>Start by helping out to answer some questions <a href="https://community.preciousplastic.com/questions" target="_blank">here</a>.</p></div></nav></div></nav></div><div data-hover="false" data-delay="0"><div><p><img src="https://cdn.prod.website-files.com/5e004b03e5da773c54823bfb/674c6f45d80db1b0b342e19a_money.png" loading="lazy" alt=""></p><p>Help us raise funds for V5</p></div><nav><div data-hover="false" data-delay="0"><nav><div><p>We need your help to locate grants. There are numerous grants available, but with a small team, it’s tough to find them all. Let’s crowdsource our efforts and share what we discover. If you have any information, please contribute by adding it <a href="https://community.preciousplastic.com/questions/who-can-help-us-to-find-grants-for-v5" target="_blank">here</a>.</p><p>And if you’re a grant writer that wants to help, send us a mail: <a href="mailto:hello@preciousplastic.com?subject=Help%20with%20grants">hello@preciousplastic.com</a></p></div></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you can make a large donation, it would be the biggest support we could receive right now. This help would allow us to stay on track in the short term, enabling us to focus on long-term goals. Our aim is to become independent of donors and self-sustaining. Additionally, your contribution could be tax-deductible as we are a Non-Profit (ANBI) from the Netherlands. <br>Our 3 year vision is to triple the impact of small-scale plastic recycling globally by releasing Version 5 with a total budget of 2.1 million euros.</p><p>If you're interested, send us an email to <a href="mailto:hello@preciousplastic.com?subject=Precious%20Plastic%20Donation">hello@preciousplastic.com</a></p></div></nav></div><div data-hover="false" data-delay="0"><nav><p>If you want to collaborate with us, we welcome partnerships. Whether it's a video, sponsorship of Version 5, or featuring your logo on our website, we are open to various forms of collaboration—the bigger the initiative, the better. For inquiries, please send an email to <a href="mailto:hello@preciousplastic.com?subject=Collaborate">hello@preciousplastic.com</a> or fill the form <a href="https://www.preciousplastic.com/custom-solutions" target="_blank">here</a>.</p></nav></div><div data-hover="false" data-delay="0"><nav><div><p>If you want, you can send us crypto. We’re easy with crypto and love receiving it. Here are our wallet addresses:&nbsp;<br><strong>Bitcoin</strong>: 3NxqZ3xW8PEPtbCrDHPrL7srjhN4U4iZwp<br><strong>Ethereum</strong>: 0x28CDdE98313a9ef878076f88d3AEFa3714185123<br>Tether (USDT): 0x65f9aB8B37D98F8D334AB97C74BF160249f8298D</p><p>If you need another one, or want to double check before sending.<br>Send us an email at <a href="mailto:hello@preciousplastic.com?subject=Crypto%20Donation">hello@preciousplastic.com</a></p></div></nav></div></nav></div><div><p>If enough action is taken we can move forward and will share a more detailed plan for Version 5.<br>If not, we are willing to accept that the project will just die.<br>We will keep you posted with updates.<br>‍<br>Thank you community</p><p>Dave</p><p>*It’s a complex problem to explain spanned over multiple years. If there are ideas, questions or comments go <a href="https://community.preciousplastic.com/questions/questions-on-the-article-with-our-problems" target="_blank">here</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ephe – A minimalist open-source Markdown paper for today (132 pts)]]></title>
            <link>https://github.com/unvalley/ephe</link>
            <guid>44175557</guid>
            <pubDate>Tue, 03 Jun 2025 22:41:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/unvalley/ephe">https://github.com/unvalley/ephe</a>, See on <a href="https://news.ycombinator.com/item?id=44175557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:unvalley/ephe" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="XFrKT3G01DJsKUpu5mMdaZfi9Z9d5c7QqHiaLJn05KsWZ_NavEm4m590ZAsiYrsoqCRuNrAjb31wsVhtwR4bug" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="unvalley/ephe" data-current-org="" data-current-owner="unvalley" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=unvalley%2Fephe" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/unvalley/ephe&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="41d4782483eb3f1535a96863a9f3280c4bf9773f7897120b9b2f0c8c363da4b7" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-ba9e85f9-ced5-4945-b4ac-1c9a79fcbea7" for="icon-button-182d2309-5266-498f-9e19-e035d8196c09" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.b5e8a54636271e908e27.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.8edda24384d5c8bf99ee.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mapping latitude and longitude to country, state, or city (102 pts)]]></title>
            <link>https://austinhenley.com/blog/coord2state.html</link>
            <guid>44175356</guid>
            <pubDate>Tue, 03 Jun 2025 22:13:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://austinhenley.com/blog/coord2state.html">https://austinhenley.com/blog/coord2state.html</a>, See on <a href="https://news.ycombinator.com/item?id=44175356">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
				  <h2>Austin Z. Henley</h2>
				  <p>
						Associate Teaching Professor<br>
						Carnegie Mellon University
					</p>
				</div>

	  <hr>
	  
    
	  <hr>

	
	<small>6/3/2025</small><p>

    <img src="https://austinhenley.com/blog/images/stateborders.png"></p><p><small><i>See the discussion of this post on <a href="https://news.ycombinator.com/item?id=44175356">Hacker News</a>.</i></small></p>


<p>An app can easily check a user's location (with permission), but figuring out <i>where</i> that location is, is far more difficult. For example, a web app can use <i>geolocation.getCurrentPosition()</i> to get a user's location in the form of latitude and longitude coordinates. But how do you convert coordinates to country, state, or city? Well, you have to look it up, somehow. (I later learned this is called <i>reverse geocoding</i>.)</p>
    
<p>At the startup I worked at, we paid several thousands of dollars per year for the Google Maps API to do a reverse lookup (that decision was made long before I joined). Given the coordinates, it gave us back a full address. We only cared about the state they were in, so we ignored the rest. If the user didn't allow us to check their location, we simply showed a state selection screen:</p>

<img src="https://austinhenley.com/blog/images/stateselection.png" alt="">


<p>We paid thousands of dollars a year, just for users to avoid this screen!? Yep.</p>

<p>Shortly after I joined the company, I tried to find a library that lets you lookup the state from coordinates, but I didn't find one. It seemed like the sort of thing that should exist. There are a lot of products that want to show you the nearest store. Some products use the user's IP address to determine the location. Or we could have done a simple distance check to a set of known points. Either would have been good enough.</p>

<p>Now that I don't have a need for such a library, I went down the rabbit hole and made <b><a href="https://github.com/AZHenley/coord2state">coord2state</a></b> based on actual US state borders for quick lookups. It is a single-file JavaScript library with no dependencies that can run client-side to determine which state a given latitude and longitude point falls in. It is 260 KB and is 99.9% accurate compared to the US Census Bureau's borderlines. You could have 100% accuracy if you were ok with 21 MB! It is on <a href="https://github.com/AZHenley/coord2state">GitHub</a> and <a href="https://www.npmjs.com/package/coord2state">NPM</a>.</p>

<p>Try it:</p>


<br>


<hr>

<h3>How does it work?</h3>

<p>The US Census Bureau publishes border <a href="https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html">data</a> for use in GIS software. I downloaded the state borders from 2024, which came in a 50 MB binary file detailing the vectors of each border. Luckily there are several Python libraries that make these files easy to work with.</p>

<pre>states = gpd.read_file("tl_2024_us_state.shp")
lon, lat = -74.0060, 40.7128 # New York
match = states[states.geometry.contains(Point(lon, lat))].iloc[0]["NAME"]
</pre>

<p>I could just wrap this up in my own web API and avoid the expensive Google Maps API. It would work well, but that is boring. My goal is to do it all client-side.</p>

<p>At a first glance, the borders are <i>really</i> detailed.</p>

  <table>
    <thead>
      <tr>
        <th>State</th>
        <th>Vertices</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Texas</td><td>62855</td></tr>
      <tr><td>Virginia</td><td>49707</td></tr>
      <tr><td>Minnesota</td><td>40293</td></tr>
      <tr><td>West Virginia</td><td>35242</td></tr>
      <tr><td>Idaho</td><td>35120</td></tr>
      <tr><td>Kentucky</td><td>34623</td></tr>
      <tr><td>North Dakota</td><td>32441</td></tr>
      <tr><td>North Carolina</td><td>32133</td></tr>
      <tr><td>Georgia</td><td>30758</td></tr>
      <tr><td>Arkansas</td><td>29782</td></tr>
      <tr><td>Tennessee</td><td>28874</td></tr>
      <tr><td>Montana</td><td>26724</td></tr>
      <tr><td>Maryland</td><td>26093</td></tr>
      <tr><td>Mississippi</td><td>25719</td></tr>
      <tr><td>Iowa</td><td>24200</td></tr>
      <tr><td>Louisiana</td><td>21547</td></tr>
      <tr><td>Oklahoma</td><td>21192</td></tr>
      <tr><td>Florida</td><td>20603</td></tr>
      <tr><td>Missouri</td><td>20283</td></tr>
      <tr><td>South Dakota</td><td>19954</td></tr>
      <tr><td>Illinois</td><td>19761</td></tr>
      <tr><td>Maine</td><td>18932</td></tr>
      <tr><td>South Carolina</td><td>17509</td></tr>
      <tr><td>Ohio</td><td>17354</td></tr>
      <tr><td>New Hampshire</td><td>16608</td></tr>
      <tr><td>Indiana</td><td>16241</td></tr>
      <tr><td>Wisconsin</td><td>16184</td></tr>
      <tr><td>Pennsylvania</td><td>15994</td></tr>
      <tr><td>Alaska</td><td>15329</td></tr>
      <tr><td>Oregon</td><td>14581</td></tr>
      <tr><td>Nebraska</td><td>13992</td></tr>
      <tr><td>Alabama</td><td>13638</td></tr>
      <tr><td>Kansas</td><td>13285</td></tr>
      <tr><td>Vermont</td><td>12119</td></tr>
      <tr><td>California</td><td>12103</td></tr>
      <tr><td>Arizona</td><td>10816</td></tr>
      <tr><td>Michigan</td><td>10705</td></tr>
      <tr><td>New York</td><td>10276</td></tr>
      <tr><td>Nevada</td><td>9537</td></tr>
      <tr><td>Wyoming</td><td>8591</td></tr>
      <tr><td>Washington</td><td>7945</td></tr>
      <tr><td>New Jersey</td><td>7896</td></tr>
      <tr><td>Colorado</td><td>7513</td></tr>
      <tr><td>Utah</td><td>7490</td></tr>
      <tr><td>District of Columbia</td><td>7365</td></tr>
      <tr><td>New Mexico</td><td>7097</td></tr>
      <tr><td>Delaware</td><td>5345</td></tr>
      <tr><td>Massachusetts</td><td>4783</td></tr>
      <tr><td>Hawaii</td><td>3519</td></tr>
      <tr><td>Connecticut</td><td>3204</td></tr>
      <tr><td>Rhode Island</td><td>2669</td></tr>
    </tbody>
  </table>

<p>Thousands and thousands of vertices for a single state seems like way more precision than we need. For example, North Dakota has 32,441 vertices despite it being a rectangle with three straight sides and one jagged side that follows a river.</p>

<img src="https://austinhenley.com/blog/images/bordersnorthdakota.png" alt="">

<p>Surely we could simplify the borders a bit without losing much accuracy? The <a href="https://github.com/shapely/shapely">Shapely</a> library already implements the <a href="https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm">Douglas-Peucker algorithm</a> for simplifying lines. It removes points that don't change the overall shape within a specified tolerance.</p>

<p>With a tolerance of 0.5° (not an angle, but a linear distance measured in degrees of latitude/longitude), it reduces the number of vertices by 99.9% for most states. For example, Texas is reduced from 62,855 vertices to 14!</p>

<img src="https://austinhenley.com/blog/images/borderstexas50.png" alt="">

<p>You can see that it oversimplifies by quite a bit though. It takes going down to 0.01° tolerance to make the differences indiscernible at this resolution, and yet it still reduces the vertices for most states by 99%. Texas is reduced by 98.8% to 756 vertices:</p>

<img src="https://austinhenley.com/blog/images/borderstexas01.png" alt="">

<p>How much accuracy are we losing though? In terms of total area, it is a very small amount of error. Far less than 1%. I set up an experiment that compares the original geometry with the simplified geometry by testing 1,000,000 random points within the US.</p>

<img src="https://austinhenley.com/blog/images/bordersnotebook.png" alt="">

<p>A tolerance of 0.1° gives an accuracy of 99.3%!</p>

<p>Alright, that seems like it is <i>probably</i> accurate enough for most use cases. But none of this matters unless we can get the size down. I'd really like it to be less than 1 MB if this is going to run client side.</p>

<p>I need to convert the geometry data into an easier to use JSON format along with some JavaScript code that can look up which polygon a given point is in. Again, we can use an existing library to do all the hard work for us. <a href="https://github.com/geopandas/geopandas">Geopandas</a> can load the geometry data and convert it to JSON. ChatGPT wrote a Python script that injects the JSON into this JavaScript template (it does <i>find and replace</i> on "geojson" with the JSON literal):</p>

<img src="https://austinhenley.com/blog/images/bordersjstemplate.png" alt="">

<p>Ok, so now I have a working JS library that I could potentially ship. Next, I need to measure how much space it takes! A tolerance of 0.1° results in a 168 KB size JS file. We can minify it though. That brings it down to 55 KB.</p>

<p>That is definitely usable as a client-side library!!!</p>

<p>I wrapped it all up into a Jupyter notebook that compares accuracy and file size based on tolerance so that I could find the ideal size to go with. The results:</p>

<table>
  <thead>
    <tr>
      <th>Tolerance</th>
      <th>Accuracy</th>
      <th>Size</th>
      <th>Minified</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.000° (baseline)</td>
      <td>100.0000%</td>
      <td>72 MB</td>
      <td>21 MB</td>
    </tr>
    <tr>
      <td>0.001°</td>
      <td>99.9909%</td>
      <td>4.2 MB</td>
      <td>1.2 MB</td>
    </tr>
    <tr>
      <td>0.005°</td>
      <td>99.9505%</td>
      <td>1.5 MB</td>
      <td>439 KB</td>
    </tr>
    <tr>
      <td>0.010°</td>
      <td>99.9067%</td>
      <td>870 KB</td>
      <td>260 KB</td>
    </tr>
    <tr>
      <td>0.050°</td>
      <td>99.6473%</td>
      <td>257 KB</td>
      <td>81 KB</td>
    </tr>
    <tr>
      <td>0.100°</td>
      <td>99.3468%</td>
      <td>168 KB</td>
      <td>55 KB</td>
    </tr>
    <tr>
      <td>0.500°</td>
      <td>96.3959%</td>
      <td>96 KB</td>
      <td>34 KB</td>
    </tr>
    <tr>
      <td>1.000°</td>
      <td>92.3244%</td>
      <td>89 KB</td>
      <td>33 KB</td>
    </tr>
  </tbody>
</table>

<p>These results show that a small drop in accuracy gives a huge drop in file size. Based on this, a tolerance of 0.01° looks like a nice balance.</p>

<h3>Can it be improved?</h3>

<p>It is worth pointing out that my evaluation of accuracy is limited. The benchmark randomly chooses points within the bounding box of the continental US, but the population of the US does not follow a random distribution. The test could be expanded to generate points based on a population density map.</p>

<p>Also, testing points that are obviously in the center of a state is not interesting. It is near borders that we actually care about. The overall results will hide areas with high error rates, such as islands or narrow borders. The next step would be to identify these areas and build a test set for them.</p> 

<p>I casually scanned the 10 most populous cities to see if they were being impacted by the simplfied borders. At 0.1°, there are some issues, such as a sliver of Staten Island being considered New Jersey. 😬 Don't come after me, please. At 0.01°, the problem goes away.</p>

<p>It could improve results to use a different precision for states based on their population or density.</p>

<p>A side effect of the geometry simplication is that there are some very small gaps between states. Based on your use case, you'll need to handle the case of the point not being within any state borders. In these rare cases, you could fall back to a different method, such as distance checking centroid points, adding an episilon to all state borders, or simply asking the user. (The user may also be in another country or in the ocean...)</p>

<p>It is also possible to apply my exact approach to any other borders, such as the city borders provided by the US Census Bureau. However, this is going to require significant memory, so you wouldn't want to ship it as a client-side JS library. You'll want a more efficient data structure for doing the lookup too (e.g., <a href="https://en.wikipedia.org/wiki/Binary_space_partitioning">binary space partioning</a>).</p>

<hr>

<p>For most casual use cases, the library will be fine as is. I packaged it with a tolerance of 0.01° which I believe gives plenty of accuracy and minifies to 260 KB. You can find it on <a href="https://github.com/AZHenley/coord2state">GitHub</a> and <a href="https://www.npmjs.com/package/coord2state">NPM</a>.</p>

<pre>npm install coord2state
</pre>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep learning gets the glory, deep fact checking gets ignored (551 pts)]]></title>
            <link>https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html</link>
            <guid>44174965</guid>
            <pubDate>Tue, 03 Jun 2025 21:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html">https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=44174965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">




<p>Deep learning is glamorous and highly rewarded. If you train and evaluate a Transformer (a state-of-the-art language model) on a dataset of 22 million enzymes and then use it to predict the function of 450 unknown enzymes, you can publish your results in Nature (a very well-regarded publication). Your paper will be viewed 22,000 times and will be in the top 5% of all research outputs scored by Altmetric (a rating of how much attention online articles receive).</p>
<p>However, if you do the painstaking work of combing through someone else’s published work, and discovering that they are riddled with serious errors, including hundreds of incorrect predictions, you can post a pre-print to bioRxiv that will not receive even a fraction of the citations or views of the original. In fact, this is exactly what happened in the case of these two papers:</p>
<ul>
<li><a href="https://www.nature.com/articles/s41467-023-43216-z">Functional annotation of enzyme-encoding genes using deep learning with transformer layers | Nature Communications</a></li>
<li><a href="https://www.biorxiv.org/content/10.1101/2024.07.01.601547v2.full">Limitations of Current Machine-Learning Models in Predicting Enzymatic Functions for Uncharacterized Proteins | bioRxiv</a></li>
</ul>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/altmetric.jpg"></p>
<figcaption>A Tale of two Altmetric Scores</figcaption>
</figure>
</div>
<p>This pair of papers on enzyme function prediction make for a fascinating case study on the limits of AI in biology and the harms of current publishing incentives. I will walk through some of the details below, although I encourage you to read the papers for yourself. This contrast is a stark reminder of how hard it can be to evaluate the legitimacy of AI results without deep domain expertise.</p>
<section id="the-problem-of-determining-enzyme-function">
<h2 data-anchor-id="the-problem-of-determining-enzyme-function">The Problem of Determining Enzyme Function</h2>
<p>Enzymes are what catalyze reactions, so they are crucial for making things happen in living organisms. Enzyme Commission (EC) numbers provide a hierarchical classification system for thousands of different functions. Given a sequence of amino acids (the building blocks of all proteins, including enzymes), can you predict what the EC number (and thus, the function) is? This seems like a problem that is custom-made for machine learning, with clearly defined inputs and outputs. Moreover, there is a rich dataset available, with over 22 million enzymes and their EC numbers listed in the online database UniProt.</p>
</section>
<section id="an-approach-with-transformers-ai-model">
<h2 data-anchor-id="an-approach-with-transformers-ai-model">An Approach with Transformers (AI model)</h2>
<p>A research paper used a transformer deep learning model to predict the functions of enzymes with previously unknown functions. It seemed like a good paper! The authors used a reasonable, well-regarded neural network architecture (two transformer encoders, two convolutional layers, and a linear layer) that had been adopted from BERT. They looked at regions with high attention to confirm that these were biologically significant, which suggests that the model had learned underlying meaning and provided interpretability. They used a standard training, validation, and test split on a dataset with millions of entries. The researchers then applied the model to a dataset where no “ground truth” was known to make ~450 novel predictions. For these novel predictions, they randomly selected three to test <em>in vitro</em> and confirmed that the predictions were accurate.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/kim-fig1a-4.jpg"></p>
<figcaption>A transformer model, shown on the left, was used to predict Enzyme Commission numbers for uncharacterized enzymes in E. coli. Three of these were tested in vitro (Fig 1a and Fig 4 from Kim, et al.)</figcaption>
</figure>
</div>
</section>
<section id="the-errors">
<h2 data-anchor-id="the-errors">The Errors</h2>
<p>The Transformer model in the Nature paper made hundreds of “novel” predictions that are almost certainly erroneous. The paper had followed a standard methodology of evaluating performance on a held-out test set, and did quite well on that (although later investigation suggests there may have been <a href="https://www.kaggle.com/code/alexisbcook/data-leakage">data leakage</a>). The results claimed for enzymes where no ground truth is known were full of errors.</p>
<p>For instance, the gene E. coli YjhQ was predicted to be a mycothiol synthase, but mycothiol is not synthesized by E. coli at all! The gene yciO, which evolved from the gene TsaC, had already been shown a decade earlier <em>in vivo</em> to not have the same function as TsaC, yet the Nature paper concluded it did have the same function.</p>
<p>Of the 450 “novel” results given in the Nature paper, 135 of these results were not novel at all; they were already listed in the online database UniProt. Another 148 showed unreasonably high levels of repetition, with the same very specific enzyme functions reappearing up to 12 times for genes of <em>E. coli</em>, which biologically implausible.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig5.jpg"></p>
<figcaption>Most of the “novel” results from the transformer paper were either not novel, unusually repetitious, or incorrect paralogs (Fig 5 from de Crecy, et al.)</figcaption>
</figure>
</div>
</section>
<section id="the-microbiology-detective">
<h2 data-anchor-id="the-microbiology-detective">The Microbiology Detective</h2>
<p>How did these errors come to light? After the model had been trained, validated, and evaluated on a dataset involving millions of entries, it was used to make ~450 novel predictions, and three of these were tested in vitro. It just so happens that one of the enzymes selected for in vitro testing, yciO, had already been studied extensively over a decade earlier by Dr.&nbsp;de Crécy-Lagard. When Dr.&nbsp;de Crécy-Lagard read that deep learning had predicted that yciO had the same function of another gene, TsaC, she knew from her long years in the lab that this was incorrect. Her previous research had shown that the TsaC gene is essential in <em>E. coli</em> even if yciO is present in the same genome and even when yciO gene is overexpressed. Moreover, the yciO activity reported by Kim et al.&nbsp;is more than four orders of magnitude (i.e.&nbsp;10,000 times) weaker than that of TsaC. All this suggests that yciO does NOT serve the same key function as TsaC.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig7.jpg"></p>
<figcaption>Two enzymes with a common evolutionary ancestor, but different functions (Fig 7 from de Crecy, et al.)</figcaption>
</figure>
</div>
<p>YciO and TsaC do have structural similarities, and YciO evolved from an ancestor of TsaC. Decades of research on protein and enzyme evolution have shown that new functions often evolve via duplication of an existing gene, followed by diversification of its function. This poses a common pitfall in determining enzyme function, because the genes will have many similarities with the ones they duplicated and then diversified from.</p>
<p>Thus, looking at structural similarities is only one type of evidence for considering enzyme function. It is also crucial to look at other types of evidence, such as neighborhood context of the genes, substrate docking, gene co-occurrence in metabolic pathways, and other features of the enzymes.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/de-crecy-fig2.jpg"></p>
<figcaption>It is important to look at multiple types of evidence when classifying enzyme function (Fig 2 from de Crecy, et al.)</figcaption>
</figure>
</div>
</section>
<section id="hundreds-of-likely-erroneous-results">
<h2 data-anchor-id="hundreds-of-likely-erroneous-results">Hundreds of Likely Erroneous Results</h2>
<p>Spotting this one error inspired de Crécy-Lagard and her co-authors to take a closer look at all of the enzymes found to have novel results in the Kim, et al, paper. They found that 135 of these results were already listed in the online database used to build the training set and thus not actually novel. An additional 148 of the results contained a very high level of repetition, with the same highly specific functions reappearing up to 12 times. Biases, data imbalance, lack of relevant features, architectural limitations, or poor uncertainty calibration can all lead models to “force” the most common labels from the training data.</p>
<p>Other examples were proven wrong via biological context or a literature search. For instance, the gene YjhQ was predicted to be a mycothiol synthase but mycothiol is not synthesized by <em>E. coli</em>. YrhB was predicted to synthesize a particular compound, which was already predicted to be synthesized by the enzyme QueD. A form of <em>E. coli</em> with a QueD mutant was unable to synthesize the compound, showing that this is not in fact the function of YrhB.</p>
</section>
<section id="rethinking-enzyme-classification-and-true-unknowns">
<h2 data-anchor-id="rethinking-enzyme-classification-and-true-unknowns">Rethinking Enzyme Classification and “True Unknowns”</h2>
<p>Identifying enzyme function actually consists of two quite different problems which are commonly conflated:</p>
<ul>
<li>propagating known function labels to enzymes in the same functional family</li>
<li>discovering truly unknown functions</li>
</ul>
<p>The authors of the second paper observe, “By design, supervised ML-models cannot be used to predict the function of true unknowns.” While machine learning can be useful for propagating known functions to additional enzymes, there are many types of errors that can occur: including failing to propagate labels when they should, propagating labels when they should not, curation mistakes, and experimental mistakes. Unfortunately, erroneous functions are being entered into key online databases such as UniProt, and this incorrect data may be further propagated if it is used to train prediction models. This is a problem that increases over time.</p>
</section>
<section id="need-for-domain-expertise">
<h2 data-anchor-id="need-for-domain-expertise">Need for Domain Expertise</h2>
<p>It is not news that AI work will be more highly rewarded and supported than work that closely inspects the underlying data and integrates deep domain knowledge. The aptly titled <a href="https://research.google/pubs/everyone-wants-to-do-the-model-work-not-the-data-work-data-cascades-in-high-stakes-ai/">“Everyone Wants to do the Model Work, not the Data Work”</a> paper involving dozens of machine learning practitioners working on high-stakes AI projects and found that inadequate-application domain expertise was one of a few key causes of catastrophic failures.</p>
<div>
<figure>
<p><img src="https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/sambasivan-square.jpg"></p>
<figcaption>Sources of cascading failures in machine learning systems (Fig 1 from Sambasivan, et al.)</figcaption>
</figure>
</div>
<p>These papers also serve as a reminder of how challenging (or even impossible) it can be to evaluate AI claims in work outside our own area of expertise. I am not a domain expert in the enzyme functions of <em>E. coli</em>. And for most deep learning papers I read, domain experts have not gone through the results with a fine-tooth comb inspecting the quality of the output. How many other seemingly-impressive papers would not stand up to scrutiny? The work of checking hundreds of enzyme predictions is less glamorous than the work of building the AI model that generated them, yet it is even more important. How can we better incentivize this type of error-checking research?</p>
<p>At a time when funding is being slashed, I believe we should be doing the opposite and investing even more into a range of scientific and biomedical research, from a variety of angles. And we need to push back on an incentive system that is disproportionately focused on flashy AI solutions at the expense of quality results.</p>


</section>

<p><i>I look forward to reading your responses. Create a free GitHub account to comment below.</i></p></main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A deep dive into self-improving AI and the Darwin-Gödel Machine (173 pts)]]></title>
            <link>https://richardcsuwandi.github.io/blog/2025/dgm/</link>
            <guid>44174856</guid>
            <pubDate>Tue, 03 Jun 2025 21:19:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://richardcsuwandi.github.io/blog/2025/dgm/">https://richardcsuwandi.github.io/blog/2025/dgm/</a>, See on <a href="https://news.ycombinator.com/item?id=44174856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <d-contents> <nav> <h3>Contents</h3>   <ul> <li> <a href="#how-dgm-works">How DGM Works</a> </li> <li> <a href="#can-dgm-really-improve-itself">Can DGM Really Improve Itself?</a> </li> <li> <a href="#comparison-with-alphaevolve">Comparison with AlphaEvolve</a> </li> <li> <a href="#can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</a> </li> </ul>  </nav> </d-contents> <p>Most AI systems today are stuck in a “cage” designed by humans. They rely on fixed architectures crafted by engineers and lack the ability to evolve autonomously over time. This is the <a href="https://en.wikipedia.org/wiki/Achilles%27_heel" rel="external nofollow noopener" target="_blank">Achilles heel</a> of modern AI — like a car, no matter how well the engine is tuned and how skilled the driver is, it cannot change its body structure or engine type to adapt to a new track on its own. But what if AI could learn and improve its own capabilities without human intervention? In this post, we will dive into the concept of self-improving systems and a recent effort towards building one.</p> <h2 id="learning-to-learn">Learning to Learn</h2> <p>The idea of building systems that can improve themselves brings us to the concept of <a href="https://people.idsia.ch/~juergen/metalearning.html" rel="external nofollow noopener" target="_blank">meta-learning</a>, or “learning to learn” <d-cite key="thrun1998learning"></d-cite>, which aims to create systems that not only solve problems but also evolve their problem-solving strategies over time. One of the most ambitious efforts in this direction is the Gödel Machine<d-cite key="schmidhuber2003godel"></d-cite>, proposed by Jürgen Schmidhuber decades ago and was named after the famous mathematician <a href="https://en.wikipedia.org/wiki/Kurt_G%C3%B6del" rel="external nofollow noopener" target="_blank">Kurt Gödel</a>. A Gödel Machine is a hypothetical self-improving AI system that optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy. It represents the ultimate form of self-awareness in AI, an agent that can reason about its own limitations and modify itself accordingly.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/godel.jpg" alt="Overview of a Gödel machine" width="80%"></p> <p><strong>Figure 1.</strong> Gödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy.</p> <p>While this idea is interesting, formally proving whether a code modification of a complex AI system is <em>absolutely beneficial</em> is almost an impossible task without restrictive assumptions. This part stems from the inherent difficulty revealed by the <a href="https://en.wikipedia.org/wiki/Halting_problem" rel="external nofollow noopener" target="_blank">Halting Problem</a> and <a href="https://en.wikipedia.org/wiki/Rice%27s_theorem" rel="external nofollow noopener" target="_blank">Rice’s Theorem</a> in computational theory, and is also related to the inherent limitations of the logical system implied by <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems" rel="external nofollow noopener" target="_blank">Gödel’s incompleteness theorem</a>. These theoretical constraints make it nearly impossible to predict the complete impact of code changes without making restrictive assumptions. To illustrate this, consider a simple analogy: just as you cannot guarantee that a new software update will improve your computer’s performance without actually running it, an AI system faces an even greater challenge in predicting the long-term consequences of modifying its own complex codebase.</p> <h2 id="darwin-gödel-machine">Darwin-Gödel Machine</h2> <p>To “relax” the requirement of formal proof, a recent work by proposed the <strong>Darwin-Gödel Machine (DGM)</strong><d-cite key="zhang2025darwingodelmachineopenended"></d-cite>, which combines the Darwinian evolution and Gödelian self-improvement. Essentially, DGM abandoned the pursuit of a rigorous mathematical proof and embraced a more pragmatic way that is closer to the essence of life evolution through empirical validation. As the authors put it,</p> <blockquote> <p>We do not require formal proof, but empirical verification of self-modification based on benchmark testing, so that the system can improve and explore based on the observed results.</p> </blockquote> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm.png" alt="Overview of the DGM" width="80%"></p> <p><strong>Figure 2.</strong> The Darwin-Gödel Machine (DGM) is a self-improving AI system that optimizes its own problem-solving strategy through a combination of Darwinian evolution and Gödelian self-improvement.</p> <p>The core idea behind DGM is to let the AI agent iteratively modify its own code<d-footnote>More precisely, the metacode that controls its behavior and ability</d-footnote>, and then put these modified new versions into the actual test environment (e.g., coding benchmarks) to judge whether this modification is good by observing its performance. If the modification is good, it will be retained and may become the basis for further evolution in the future; if the modification is bad, it will be eliminated.</p> <h3 id="how-dgm-works">How DGM Works</h3> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-animation.gif" alt="Animation of the DGM process" width="80%"></p> <p><strong>Figure 3.</strong> Animation showing how DGM iteratively builds a growing archive of agents by harnessing the principles of open-ended exploration. New agents are created and scored by interleaving self-modification with downstream task evaluation.</p> <p>The general workflow of DGM is as follows:</p> <ol> <li> <p><strong>Initialization:</strong> Evolution begins with one or a few basic coding agents, so-called the “seeds” of intelligence. These initial agents may possess only minimal functionalities, such as executing bash commands or editing entire files. DGM maintains an “archive of agents”, which functions like a biological “gene bank.” This archive preserves all previously generated agents, regardless of their performance level, ensuring that no potentially valuable variation is permanently lost.</p> </li> <li> <p><strong>Sampling:</strong> DGM selects one or more parent agents from the archive to generate new child agent. The selection mechanism does not focus solely on top performers but instead allows every agent a chance to be chosen. While higher-performing agents are more likely to be selected, DGM also encourages exploration by giving “less successful agents”<d-footnote>Those that might contain novel or unconventional ideas</d-footnote> an opportunity to contribute to future generations.</p> </li> <li> <strong>Reproduction:</strong> Once the parents are selected, DGM performs modifications to generate a new child agent. These changes occur at the source code level and can include: <ul> <li> <em>Enhancement of existing tools</em>, for example, upgrading a file-editing tool that replaces entire files into one capable of precise string replacement.</li> <li> <em>Addition of new tools or workflows</em>, such as integrating a static code analysis tool or designing a “think–act–observe” loop.</li> <li> <em>Improvement of problem-solving strategies</em>, for instance, refining how the agent prompts the base model or managing long-context code more effectively.</li> <li> <em>Introduction of collaboration mechanisms</em>, like having an agent’s code reviewed by another before execution.</li> </ul> </li> <li> <p><strong>Natural Selection:</strong> The newly generated child agent is not automatically accepted into the “elite pool” but must prove its worth through rigorous testing. Each agent’s performance, such as the percentage of successfully solved problems, is quantitatively scored and used to select the best agents.</p> </li> <li> <strong>Tree Formation:</strong> If a child agent outperforms its parent or meets certain quality thresholds, it is added to the archive and becomes a new node in the evolutionary tree. This process repeats iteratively, creating a growing structure of diverse, high-quality agents. Importantly, DGM enables parallel exploration of many different paths in the search space, promoting open-ended discovery and avoiding premature convergence to local optima.</li> </ol> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-conceptual.png" alt="Workflow of DGM" width="100%"></p> <p><strong>Figure 4</strong> DGM iteratively builds a growing intelligent archive by intertwining self-modification with downstream task evaluation. The intelligent body in the archive is selected for self-modification through open exploration.</p>  <p>The paper provides a concrete example of the above principle in action. In the SWE-bench experiment, the performance of newly generated agents showed temporary declines in the 4th and 56th iterations. While traditional hill-climbing methods would abandon these seemingly “regressive paths”, DGM’s open exploration mechanism allows continued exploration. This persistence paid off as by the 70th iteration, the system had created new agents that surpassed all their predecessors, demonstrating the value of maintaining diverse evolutionary paths even when they temporarily appear less promising.</p> <h3 id="can-dgm-really-improve-itself">Can DGM Really Improve Itself?</h3> <p>In the paper, the authors evaluated DGM on two challenging benchmarks:</p> <ol> <li> <p><strong>SWE-bench:</strong> A widely-used benchmark that tests agents’ ability to resolve real-world GitHub issues<d-cite key="jimenez2024swe"></d-cite> . DGM showed impressive progress, improving its performance from 20.0% to 50.0% through self-modification.</p> </li> <li> <p><strong>Polyglot</strong>: A <a href="https://aider.chat/2024/12/21/polyglot.html" rel="external nofollow noopener" target="_blank">multi-language coding benchmark</a> containing coding problems in C++, Go, Java, JavaScript, Python and Rust. DGM achieved even more dramatic improvement, jumping from an initial 14.2% to 30.7% success rate, far surpassing the representative hand-designed agent by <a href="https://aider.chat/" rel="external nofollow noopener" target="_blank">Aider</a>.</p> </li> </ol> <p>The impressive performance improvements demonstrate DGM’s fundamental ability to enhance itself through self-modification. Through ablation studies, the authors identified two key factors driving DGM’s success. First, self-modification proved crucial, as when disabled (<strong>DGM w/o Self-improve</strong>), the performance declined substantially, showing that the ability to modify its own code is essential for accelerated learning. Second, the open-ended exploration approach was vital, as removing the Darwinian evolution-inspired search mechanism (<strong>DGM w/o Open-ended exploration</strong>), which maintains an archive of diverse solutions and enables parallel exploration of multiple paths, resulted in lower performance.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-self-improve.png" alt="Performance of DGM on SWE-bench and Polyglot" width="100%"></p> <p><strong>Figure 5</strong> Self-improvement and open-ended exploration enable the DGM to continue making progress and improve its performance. The DGM automatically discovers increasingly better coding agents and performs better on both SWE-bench (Left) and Polyglot (Right).</p> <h3 id="comparison-with-alphaevolve">Comparison with AlphaEvolve</h3> <p>In parallel, AlphaEvolve<d-cite key="deepmind2025alphaevolve"></d-cite>, which is developed by Google DeepMind, also demonstrates another powerful path forward. AlphaEvolve pairs the creative problem-solving capabilities of Google’s Gemini models with automated evaluators in an evolutionary framework. It has already demonstrated significant real-world impact across multiple domains, such as:</p> <ul> <li> <strong>Data center efficiency:</strong> AlphaEvolve discovered a simple yet highly effective heuristic for Google’s <a href="https://research.google/pubs/large-scale-cluster-management-at-google-with-borg/" rel="external nofollow noopener" target="_blank">Borg</a> cluster management system, continuously recovering 0.7% of Google’s worldwide compute resources.</li> <li> <strong>AI acceleration:</strong> It achieved a 23% speedup in Gemini’s architecture’s vital <a href="https://docs.jax.dev/en/latest/pallas/index.html" rel="external nofollow noopener" target="_blank">kernel</a> by finding more efficient ways to divide large matrix multiplication operations, resulting in a 1% reduction in overall training time.</li> <li> <strong>Mathematical breakthroughs:</strong> Most notably, it discovered an algorithm for multiplying 4x4 complex-valued matrices using just 48 scalar multiplications, surpassing <a href="https://en.wikipedia.org/wiki/Strassen_algorithm" rel="external nofollow noopener" target="_blank">Strassen’s 1969 algorithm</a>, and advanced the 300-year-old <a href="https://en.wikipedia.org/wiki/Kissing_number_problem" rel="external nofollow noopener" target="_blank">kissing number problem</a> by establishing a new lower bound in 11 dimensions.</li> </ul>  <p>While both systems adopt a similar evolutionary framework, their scopes and methodologies differ in the following ways:</p> <table> <thead> <tr> <th>Feature</th> <th>AlphaEvolve</th> <th>DGM</th> </tr> </thead> <tbody> <tr> <td>Focus</td> <td>Evolving functions and codebases</td> <td>Evolving the agent itself</td> </tr> <tr> <td>Level of Innovation</td> <td>Algorithmic level</td> <td>Agent-level (toolset, methodology)</td> </tr> <tr> <td>Role of LLM</td> <td>LLM acts as “genetic operators” to modify algorithms</td> <td>LLM serves as the “brain” to evolve itself with better tools and strategies</td> </tr> <tr> <td>Evaluation</td> <td>Well-defined problems with automated evaluators</td> <td>Open-ended environments</td> </tr> </tbody> </table> <p>To better understand the differences between the two approaches, let us take a look at the following analogy:</p>  <h3 id="can-we-trust-a-self-improving-ai">Can we trust a self-improving AI?</h3> <p>The authors also conducted some experiments to evaluate DGM’s reliability and discovered some concerning behaviors. In particular, they observed instances where DGM attempted to manipulate its reward function through deceptive practices. One notable example involved the system fabricating the use of external tools - specifically, it generated fake logs suggesting it had run and passed unit tests, when in reality no tests were executed.</p> <p><img src="https://richardcsuwandi.github.io/assets/img/dgm-hallucination.png" alt="Hallucination in DGM" width="100%"></p> <p><strong>Figure 6</strong> DGM hallucinated the use of an external tool to run unit tests.</p> <p>To address this issue, the authors implemented a “tool use hallucination” reward function and tasked DGM with optimizing it to eliminate hallucination behaviors. While DGM successfully provided solutions in many cases, it sometimes attempted to circumvent the detection system by removing the markers used to identify hallucinations, despite explicit instructions to preserve them. This behavior is reminiscent of the phenomenon of <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="external nofollow noopener" target="_blank">Goodhart’s Law</a> from economics and sociology:</p> <blockquote> <p>“When a measure becomes a target, it ceases to be a good measure.”</p> </blockquote> <p>Fortunately, DGM’s design includes a transparent, traceable lineage of all changes, which enabled the researchers to quickly identify and address these undesirable behaviors. However, this example highlights the need for more robust safeguards to prevent such manipulation attempts in the first place. These findings underscore the critical importance of safety in self-improving AI research.</p> <h2 id="takeaways">Takeaways</h2> <p>DGM represents a groundbreaking step toward the realization of <a href="https://en.wikipedia.org/wiki/Life_3.0" rel="external nofollow noopener" target="_blank">Life 3.0</a>, a concept introduced by physicist <a href="https://en.wikipedia.org/wiki/Max_Tegmark" rel="external nofollow noopener" target="_blank">Max Tegmark</a>. In his book, he classified life into three stages:</p> <ul> <li> <strong>Life 1.0:</strong> Biological life with fixed hardware and software, such as bacteria.</li> <li> <strong>Life 2.0:</strong> Beings like humans, whose behavior can be learned and adapted during their lifetime, though their biology remains fixed.</li> <li> <strong>Life 3.0:</strong> A new class of intelligence that can redesign not only its behavior but also its underlying architecture and objectives — essentially, intelligence that builds itself.</li> </ul> <p><img src="https://richardcsuwandi.github.io/assets/img/life3.webp" alt="Life 3.0" width="80%"></p> <p><strong>Figure 7</strong> The three stages of life according to Max Tegmark.</p> <p>While DGM currently focuses on evolving the “software”<d-footnote>the code and strategies of AI agents</d-footnote>, it exemplifies the early stages of Life 3.0. By iteratively rewriting its own code based on empirical feedback, DGM demonstrates how AI systems could move beyond human-designed architectures to autonomously explore new designs, self-improve, and potentially give rise to entirely new species of digital intelligence. If this trend continues, we may witness a <a href="https://en.wikipedia.org/wiki/Cambrian_explosion" rel="external nofollow noopener" target="_blank">Cambrian explosion</a> in AI development, where eventually AI systems will surpass human-designed architectures and give rise to entirely new species of digital intelligence. While this future looks promising, achieving it requires addressing significant challenges, including:</p> <ul> <li> <p><strong>Evaluation Framework</strong>: Need for more comprehensive and dynamic evaluation systems that better reflect real-world complexity and prevent “reward hacking” while ensuring beneficial AI evolution.</p> </li> <li> <p><strong>Resource Optimization</strong>: DGM’s evolution is computationally expensive<d-footnote>The paper mentioned that a complete SWE-bench experiment takes about two weeks and about $22,000 in API call costs.</d-footnote>, thus improving efficiency and reducing costs is crucial for broader adoption.</p> </li> <li> <p><strong>Safety &amp; Control</strong>: As AI self-improvement capabilities grow, maintaining alignment with human ethics and safety becomes more challenging.</p> </li> <li> <p><strong>Emergent Intelligence</strong>: Need to develop new approaches to understand and interpret AI systems that evolve beyond human-designed complexity, including new fields like “AI interpretability” and “AI psychology”.</p> </li> </ul> <p>In my view, DGM is more than a technical breakthrough, but rather a philosophical milestone. It invites us to rethink the boundaries of intelligence, autonomy, and life itself. As we advance toward Life 3.0, our role shifts from mere designers to guardians of a new era, where AI does not just follow instructions, but helps us discover what is possible.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AirAP AirPlay server - AirPlay to an iOS Device (193 pts)]]></title>
            <link>https://github.com/neon443/AirAP</link>
            <guid>44174190</guid>
            <pubDate>Tue, 03 Jun 2025 20:12:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/neon443/AirAP">https://github.com/neon443/AirAP</a>, See on <a href="https://news.ycombinator.com/item?id=44174190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">AirAP is a fully native AirPlay server, written in Swift, for iOS. Essentially, AirAP allows you to use your iPhone as an AirPlay receiver in iTunes or on your Mac, meaning that you can use your iPhone to play your device's sound.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's AirAP?</h2><a id="user-content-whats-airap" aria-label="Permalink: What's AirAP?" href="#whats-airap"></a></p>
<p dir="auto">Have you ever wanted to stream audio from your Mac, Apple TV, or another iOS device to your iPhone? AirAP makes this possible by implementing a full AirPlay server that runs natively on iOS. Once installed, your iPhone will appear as an available AirPlay destination in iTunes (including the Windows version), Music app, or any other AirPlay-compatible application.</p>
<p dir="auto">The concept might seem backwards at first - after all, we're used to streaming from our iPhones to other devices. But there are surprisingly many scenarios where you'd want to do the reverse. Maybe you're working on your Mac late at night and want to route the audio to your iPhone with headphones so you don't disturb anyone (hi 👋). Perhaps you're a developer testing audio applications and need to quickly switch between different output devices. Or maybe you just want to include your iPhone in a multi-room audio setup alongside your other AirPlay speakers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing AirAP</h2><a id="user-content-installing-airap" aria-label="Permalink: Installing AirAP" href="#installing-airap"></a></p>
<p dir="auto">To try it out, <a href="https://testflight.apple.com/join/8aeqD8Q2" rel="nofollow">open this TestFlight link</a>, install AirAP, and follow the instructions. After installation, simply launch AirAP and ensure your iPhone is connected to the same Wi-Fi network as the device you want to stream from. Your iPhone will automatically appear in AirPlay device lists, ready to receive audio - if it doesn't, try restarting the app.</p>
<hr>
<sup>
© 2025 Nihaal Sharma. AirPlay, iPhone, iTunes, Mac, and Apple TV are trademarks of Apple Inc.
</sup>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>