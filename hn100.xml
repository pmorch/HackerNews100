<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 12 Oct 2024 00:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Working from home is powering productivity (232 pts)]]></title>
            <link>https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom</link>
            <guid>41813304</guid>
            <pubDate>Fri, 11 Oct 2024 20:18:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom">https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom</a>, See on <a href="https://news.ycombinator.com/item?id=41813304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>                 
                    
                    <!-- DOWNLOAD PDF LINK -->
                    <p>
                        <a href="https://www.imf.org/-/media/Files/Publications/Fandd/Article/2024/09/Bloom.ashx">Download PDF <i></i></a>
                    </p>
                    <!-- END DOWNLOAD PDF LINK -->
<!-- BEGIN INTRO TEXT -->
<p>A fivefold increase in remote work since the pandemic could boost economic growth and bring wider benefits</p>

<!-- END INTRO TEXT --><p>Economics is famous for being the dismal science. Sadly, <a href="https://web.stanford.edu/~chadj/IdeaPF.pdf">recent work</a> highlighting the slowdown in productivity growth stretching back to the 1950s is no exception. But I take a more cheerful view because of the great productivity gains promised by the pandemic-induced jump in working from home.&nbsp;&nbsp;</p>
<p>Working from home (WFH) increased about tenfold following the outbreak of the pandemic and has settled in at about five times its prepandemic level (see Chart 1). This could counter slowing productivity and deliver a surge in economic growth over the next few decades. If AI yields additional output, the era of slow growth could be over.&nbsp;&nbsp;</p>
<p><img src="https://www.imf.org/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart1-v3.ashx?w=950" alt="Bloom 1" data-featherlight="/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart1-v3.ashx"></p>
<p>The decomposition of economic growth by Nobel laureate Robert Solow, one of the most famous economists of all time, guides my analysis. Solow’s <a href="https://www.jstor.org/stable/1926047">1957 classic paper</a> highlights how growth comes from both the increase in factor inputs like labor and capital and from raw productivity growth. I hang my&nbsp;analysis on his framework by highlighting in turn how each of these factors will promote faster growth.&nbsp;</p>
<h6>Labor</h6>
<p>The easiest way to see labor’s impact is the survey evidence from across the United States, Europe, and Asia that shows hybrid work is worth about an <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.37.4.23">8 percent increase in salary</a>. Hybrid work is the typical pattern for office workers, managers, and other professionals, involving usually two or three days a week away from the office. To understand why employees would consider this to be worth 8 percent of their salary, note that typical workers spend about 45 hours a week in the office, yet they spend close to another 8 hours a week commuting. So working from home three days a week saves them about five hours a week, about 10 percent of their total weekly work and commute time.&nbsp;&nbsp;</p>
<p>Most people really dislike commuting, and so place even greater value on this time savings. See, for example, <a href="https://www.science.org/doi/10.1126/science.1103572">another famous paper</a>, by the Nobel Prize winner <a href="https://www.imf.org/external/pubs/ft/fandd/2009/09/people.htm">Daniel Kahneman</a>. This research found that commuting is the most detested activity in the day, disliked even more than work itself. This makes it easy to understand why the average employee values working from home so much—with its ability to save hours of painful weekly commuting, alongside the flexibility of being able to live farther from work.&nbsp;&nbsp;</p>
<p>This value of working from home has a powerful impact on labor supply. In the global economy there are tens of millions of people who are on the edge of the workforce. So small changes in the attractiveness of work can bring many millions of them into employment. This marginal labor force includes those with childcare or eldercare responsibilities, those close to retirement, and some folks in rural areas.&nbsp;&nbsp;</p>
<p>One example of this WFH impact on labor supply is the <a href="https://fred.stlouisfed.org/series/LNU02074597">approximately 2 million more employees</a> with a disability who are working in the US following the pandemic. These increases in disability employment have occurred primarily in high-WFH occupations. Employees with a disability benefit in two ways: first, by avoiding long commutes and second, by the ability to control their work environment at home.&nbsp;&nbsp;</p>
<p>Another example is prime-age female employment in the US, which has risen about 2 percent faster than prime-age male employment since the pandemic. Women’s larger role in childcare could be driving this rise in female labor force participation via WFH, according to <a href="https://drive.google.com/file/d/1Z1J2GHZjqkWzRV5ygA02yzSemwkAmzne/view">recent research</a>.&nbsp;</p>
<p>Collectively these effects could increase labor supply by several percent.&nbsp;&nbsp;</p>
<p>Of course, this calculation takes the current population as given. In the longer run, WFH could also increase fertility rates. One story I’ve heard repeatedly from talking to hundreds of employees and managers is how working remotely makes it easier to parent. This is perhaps most salient in East Asia, where long workdays, punishing commutes, and intense parenting pressures have led to rapidly dropping fertility. If parents are able to work two or three days a week at home, particularly with flexible schedules that allow them to share parenting responsibilities, this could increase birth rates. <a href="http://www.wfhresearch.com/">Preliminary analysis based on US survey data</a> suggests perhaps 0.3 to 0.5 more desired children per couple when both work from home one day or more a week.&nbsp;</p>
<h6>Capital</h6>
<p>The beneficial impact of WFH on capital comes from the longer-term release of office space for other uses, like residential and retail. If employees are based at home two or three days a week, society needs less office space, and that space can be used for other activities. It also reduces commuting traffic, curbing the need for additional transportation infrastructure. More intensive use of our home capital—the space and equipment in our houses and apartments—can allow society to save on the use of transportation and office capital, which can be redeployed to other uses. In major city centers about half of the land is covered in office space, and given that office occupancy is now 50 percent below prepandemic levels, there is great potential for office space reduction.&nbsp;&nbsp;</p>
<p><a href="https://nbloom.people.stanford.edu/sites/g/files/sbiybj24291/files/media/file/supercommuters_final.pdf">Recent data on driving speeds</a> show that traffic is now moving about 2 or 3 miles per hour faster during the morning commute, which reduces the need for additional transportation infrastructure and saves the typical commuter a few minutes a day.&nbsp;&nbsp;</p>
<p>Over the longer term, allowing employees to work partially or fully remotely also opens up currently underused land for housing, effectively <a href="https://www.kansascityfed.org/research/economic-review/hybrid-working-commuting-time-and-the-coming-long-term-boom-in-home-construction/">increasing the usable land supply</a>. Many major cities are heavily congested because most employees do not want to live more than a one-hour commute from the center. If they are required at work only a couple of days a week, longer commutes become possible, opening up space farther outside city centers for housing use.&nbsp;</p>
<p>Collectively, these capital contributions could also raise output a few percent over the coming decades.&nbsp;</p>
<h6>Productivity</h6>
<p>Classic firm and individual micro studies <a href="https://www.nature.com/articles/s41586-024-07500-2">typically find</a> that hybrid work, the usual pattern for about 30 percent of the US, European, and Asian labor forces, has a roughly flat impact on productivity. WFH benefits workers by saving them from exhausting commutes and typically provides a quieter working environment. But by reducing time at the office, it can also reduce employees’ ability to learn, to innovate, and to communicate. These positive and negative effects roughly offset each other, generating no net productivity impact of hybrid WFH, research suggests.&nbsp;</p>
<p>The impact of <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.37.4.23">fully remote working</a>, which has been adopted by about 10 percent of employees, is highly dependent on how well it’s managed. Some studies that examined fully remote working during the early days of the pandemic found large negative impacts, potentially because of the chaos of the early lockdowns. Other studies found large positive impacts, typically in more self-directed activities, such as call center or data entry work with well-managed firms.&nbsp;&nbsp;</p>
<p>In summary, the impact of fully remote work is perhaps neutral, because firms tend to adopt it only when such work arrangements match the work activity—often tasks such as coding or IT support, carried out by trained employees in a managed environment. But while the micro productivity impacts on any individual firm may be neutral, the huge power of labor market inclusion means that the aggregate macro impact is likely to be positive.&nbsp;&nbsp;</p>
<p>To explain the benefits of labor market inclusion, consider that fully in-person jobs can be filled only by nearby employees. A human resources or information technology position in New York can, for example, be filled only by a local resident. Even if there are people in Bulgaria, Brazil, or Belize who would be a better fit, they cannot do the job if they are not there in person. But as soon as positions can be filled remotely, employers go from taking the best local employee to taking the best regional employee for hybrid and the best global employee for fully remote work.&nbsp;&nbsp;</p>
<p><a href="https://web.stanford.edu/~chadj/HHJK.pdf">Recent studies</a> of work discrimination and reallocation highlight how expanding labor markets to a wider pool of potential employees can have massive productivity benefits. Going from 10 to 10,000 qualified candidates for a position allows a far more productive match, particularly if AI can help screen applicants. Remote work enables global matching between employees and firms, boosting labor productivity.&nbsp;</p>
<p>An additional macro productivity benefit from working from home is its positive impact on pollution from transportation. The WFH surge has curbed commuting traffic volumes across the US and Europe by an estimated 10 percent. This has <a href="https://www.pnas.org/doi/10.1073/pnas.2304099120">reduced pollution</a>, particularly emissions of low-level heavy particulates. Health studies have linked pollution to cognitive and productivity damage. Lowering pollution not only improves our quality of life but can also increase growth.&nbsp;</p>
<h6>Positive feedback loop</h6>
<p>A positive feedback loop—from working from home to faster growth and back—boosts these impacts. A long history of market-size effects in economics highlights how firms strive to innovate to serve larger, more lucrative markets. When you go from 5 million to 50 million people working from home every day, major hardware and software companies, start-ups, and funders take notice. This leads to an acceleration of new technologies to serve those markets, improving their productivity and growth.&nbsp;&nbsp;</p>
<p>That feedback loop has already begun. The share of <a href="https://drive.google.com/file/d/1ydRlxKc1ss17WuvlJSqXDj2vd1F12KeW/view">new patent applications</a> at the US Patent and Trademark Office that repeatedly use “remote work,” “working from home,” or similar words was flat until 2020 but has started to rise (see Chart 2). This highlights the improvement in technologies. Better cameras, screens, and software and technologies such as augmented and virtual reality and holograms will increase the productivity of hybrid and remote work in the future. This will generate a positive feedback loop between growth and working from home.&nbsp;</p>
<p><img src="https://www.imf.org/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart2-v3.ashx?w=950" alt="Bloom 2" data-featherlight="/-/media/Images/IMF/FANDD/Charts/2024/09/bloom-chart2-v3.ashx"></p>
<p>One critique of the boom in working from home is the damage to city centers. It’s true that retail spending has fallen in city centers, but this activity has relocated to the suburbs, and overall consumption expenditure has resumed its prepandemic trend. Perhaps more problematic is the large reduction in valuations of commercial office space. Although this represents a loss of valuation for investors in the office sector, the release of city center space for residential use will in the long run make downtown living more affordable. The cost of living in the city rose dramatically in the 1990s and 2000s, pricing many middle- and lower-income employees out of city centers. This is especially problematic as many of these workers provide essential services, such as firefighting, policing, teaching, health care, food, transportation, and other work that can only be done in person. Cutting the amount of space for office use in city centers and converting it to residential use would make housing more affordable for these essential workers.&nbsp;</p>
<p>The 2020 surge in working from home has helped offset the prepandemic productivity slowdown overall and is boosting present and future growth. Being an economist usually means balancing winners and losers. Analyzing changes in technology, trade, prices, and regulations usually has mixed effects, with large groups of winners and losers. When it comes to working from home, the winners massively outweigh the losers. Firms, employees, and society in general have all reaped huge benefits. In my lifetime as an economist I have never seen a change that is so broadly beneficial.&nbsp;&nbsp;</p>
<p>This leaves me in the unusual place of being an optimistic “dismal scientist.” But it’s a place I’m happy to be as I write this while working from home. </p>
                <div>
                    <h5>Podcast</h5>
                    

                            <p><a href="https://www.imf.org/en/News/Podcasts/All-Podcasts/2024/09/03/nicholas-bloom">
                                <img src="https://www.imf.org/-/media/Images/IMF/News/Podcasts/2024/800x545-nick-bloom-sndwv.ashx?h=545&amp;w=800&amp;la=en" alt="">
                            </a></p>
                            <p>
                                Working from home was not an option for most people before March 11, 2020, when work and home life suddenly collided. Stanford University's Nicholas Bloom was studying the potential impact of remote work long before the pandemic launched it into the mainstream and now has data to suggest businesses should stick to the hybrid working model.
                            </p>



                </div>

            <!-- BEGIN AUTHORS -->
            
            <!-- END AUTHORS -->
<!-- BEGIN FOOTNOTE -->
<p>Opinions expressed in articles and other materials are those of the authors; they do not necessarily reflect IMF policy.</p>
<!-- END FOOTNOTE -->
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve says Steam users don't own a thing, GOG says its games can't be taken away (142 pts)]]></title>
            <link>https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/</link>
            <guid>41812813</guid>
            <pubDate>Fri, 11 Oct 2024 19:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/">https://www.gamesradar.com/games/valve-reminds-steam-users-they-dont-actually-own-a-darn-thing-they-buy-gog-pounces-and-says-its-games-cannot-be-taken-away-from-you-thanks-to-offline-installers/</a>, See on <a href="https://news.ycombinator.com/item?id=41812813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg" alt="An orange car in The Crew." srcset="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/6mtmmTG9AavwwxkJsiMn6m.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Ubisoft)</span>
</figcaption>
</div>

<div id="article-body">
<p>A subtle change has arrived to the Steam shopping experience to drive home the fact that you're buying a game <em>license </em>rather than a copy of a game that you'll definitely own forever, and rival storefront GOG already seems to be weighing in on the matter.</p><p>As <a data-analytics-id="inline-link" href="https://www.engadget.com/gaming/steam-now-tells-gamers-up-front-that-theyre-buying-a-license-not-a-game-085106522.html" target="_blank" data-url="https://www.engadget.com/gaming/steam-now-tells-gamers-up-front-that-theyre-buying-a-license-not-a-game-085106522.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>reported by Engadget</u></a>, if you're about to buy a new game on Steam, you'll now notice a new message pop up, which reads: "A purchase of a digital product grants a license for the product on Steam." This disclaimer appears as though it's likely related to a California law set to come into effect next year, which'll stop digital storefronts from using words like "buy" in relation to things like game licenses unless it's obvious what people are spending their money on, as part of a move to make it clearer to consumers what they actually own (or rather, what they <em>don't</em>).&nbsp;</p><p>Needless to say, this new message on Steam has already caused quite a stir, so much so that it's seemingly reached the ears of rival storefront GOG. GOG is famously free of digital rights management (DRM), and offers its customers offline installers for the games it sells which you can download onto your PC where they can remain safe forever, so it's understandable that the site might have some thoughts on all this.&nbsp;</p><p>"Since checkout banners are trending, we're thinking of putting one up ourselves," a <a data-analytics-id="inline-link" href="https://x.com/GOGcom/status/1844752098145038435" target="_blank" data-url="https://x.com/GOGcom/status/1844752098145038435" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>tweet posted on the official GOG Twitter</u></a> account reads. "Thoughts on this one?" The concept banner in question says: "A purchase of a digital product on GOG grants you its Offline Installers, which cannot be taken away from you."</p><p>As was pointed out as recently as September when GOG weighed in on the upcoming California law, however, it's worth noting that GOG does, in fact, sell licenses to games much like other storefronts. With that said, <a data-analytics-id="inline-link" href="https://www.gamesradar.com/platforms/pc-gaming/as-california-forces-stores-to-admit-you-dont-own-digital-games-gog-reminds-pc-gamers-you-can-keep-drm-free-games-your-gaming-legacy-is-always-in-your-hands/" data-before-rewrite-localise="https://www.gamesradar.com/platforms/pc-gaming/as-california-forces-stores-to-admit-you-dont-own-digital-games-gog-reminds-pc-gamers-you-can-keep-drm-free-games-your-gaming-legacy-is-always-in-your-hands/"><u>it clarified at the time</u></a>: "When we said we let you 'own' your games, we meant that no matter what happens – whether it's licensing issues, storefronts shutting down, or even a zombie apocalypse cutting off your Internet – you'll still be able to play them thanks to our offline installers. We want to ensure your gaming legacy is always in your hands, not ours." With that in mind, there's no doubt many would argue that it's a better deal than what Steam offers.&nbsp;</p><p><em>Be sure to check out our recommendations for the </em><a data-analytics-id="inline-link" href="https://www.gamesradar.com/best-pc-games/" data-before-rewrite-localise="https://www.gamesradar.com/best-pc-games/"><u><em>best PC games</em></u></a><em>.</em></p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-amWrMVdqPwh7TxvGcdx7NS"><section><p>Weekly digests, tales from the communities you love, and more</p></section></div>
</div>
<div id="slice-container-authorBio-amWrMVdqPwh7TxvGcdx7NS"><p>I'm one of GamesRadar+'s news writers, who works alongside the rest of the news team to deliver cool gaming stories that we love. After spending more hours than I can count filling The University of Sheffield's student newspaper with Pokemon and indie game content, and picking up a degree in Journalism Studies, I started my career at GAMINGbible where I worked as a journalist for over a year and a half. I then became TechRadar Gaming's news writer, where I sourced stories and wrote about all sorts of intriguing topics. In my spare time, you're sure to find me on my Nintendo Switch or PS5 playing through story-driven RPGs like Xenoblade Chronicles and Persona 5 Royal, nuzlocking old Pokemon games, or going for a Victory Royale in Fortnite.</p></div>





</section>


<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs don't do formal reasoning (111 pts)]]></title>
            <link>https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and</link>
            <guid>41812523</guid>
            <pubDate>Fri, 11 Oct 2024 19:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and">https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and</a>, See on <a href="https://news.ycombinator.com/item?id=41812523">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A superb </span><a href="https://arxiv.org/pdf/2410.05229" rel="">new article</a><span> on LLMs from six AI researchers at Apple who were brave enough to challenge the dominant paradigm has just come out.</span></p><p><em>Everyone</em><span> actively working with AI should read it, or at least this </span><a href="https://x.com/mfarajtabar/status/1844456880971858028?s=61" rel="">terrific X thread</a><span> by senior author, Mehrdad Farajtabar, that summarizes what they observed. One key passage: </span></p><blockquote><p>“we found no evidence of formal reasoning in language models …. Their behavior is better explained by sophisticated pattern matching—so fragile, in fact, that changing names can alter results by ~10%!” </p></blockquote><p>One particularly damning result was a new task the Apple team developed, called GSM-NoOp</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png" width="1456" height="967" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:967,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1517763,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3c84b2f-cb58-4890-acc4-08a5a157e5e6_2644x1756.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>§</p><p><span>This kind of flaw, in which reasoning fails in light of distracting material, is not new. Robin Jia Percy Liang of Stanford ran a similar study, with similar results, back in 2017 (which Ernest Davis and I quoted in </span><em>Rebooting AI</em><span>, in 2019:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png" width="935" height="638" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:638,&quot;width&quot;:935,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:171487,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee1a4735-af08-4618-8fc7-cd0120e17c04_935x638.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>§</p><p><strong>𝗧𝗵𝗲𝗿𝗲 𝗶𝘀 𝗷𝘂𝘀𝘁 𝗻𝗼 𝘄𝗮𝘆 𝗰𝗮𝗻 𝘆𝗼𝘂 𝗯𝘂𝗶𝗹𝗱 𝗿𝗲𝗹𝗶𝗮𝗯𝗹𝗲 𝗮𝗴𝗲𝗻𝘁𝘀 𝗼𝗻 𝘁𝗵𝗶𝘀 𝗳𝗼𝘂𝗻𝗱𝗮𝘁𝗶𝗼𝗻, where changing a</strong><span> word or two in irrelevant ways or adding a few bit of irrelevant info can give you a different answer.</span></p><p>§</p><p><span>Another manifestation of the lack of sufficiently abstract, formal reasoning in LLMs is the way in which performance often fall apart as problems are made bigger.  This comes from </span><a href="https://www.arxiv.org/pdf/2409.13373" rel="">a recent analysis of GPT o1</a><span> by Subbarao Kambhapati’s team:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png" width="888" height="532" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:532,&quot;width&quot;:888,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:127278,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f374410-3d43-4775-b8de-7ad0608053a7_888x532.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Performance is ok on small problems, but quickly tails off.</p><p>§</p><p>We can see the same thing on integer arithmetic. Fall off on increasingly large multiplication problems has repeatedly been observed, both in older models and newer models. (Compare with a calculator which would be at 100%.)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg" width="1200" height="396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:396,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:67405,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ebb3fc-c0ff-4421-a4e8-1928d8838b74_1200x396.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even o1 suffers from this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png" width="1301" height="1078" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad644058-8cbe-429a-9298-21318e200efe_1301x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1078,&quot;width&quot;:1301,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1781277,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad644058-8cbe-429a-9298-21318e200efe_1301x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Failure to follow the rules of chess is another continuing failure of formal reasoning:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg" width="1244" height="1248" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1248,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:319035,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa0c3102b-ff64-4e1e-8ba1-bd20618a8d8f_1244x1248.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>§</p><p>Elon Musk’s putative robotaxis are likely to suffer from a similar affliction: they may well work safely for the most common situations, but are also likely struggle to reason abstractly enough in some circumstances. (We are, however, unlikely ever to get systematic data on this, since the company isn’t transparent about what it has done or what the results are.) </p><p>§</p><p>The refuge of the LLM fan is always to write off any individual error. The patterns we see here, in the new Apple study, and the other recent work on math and planning (which fits with many previous studies), and even the anecdotal data on chess, are too broad and systematic for that.</p><p>§</p><p><span>The inability of standard neural network architectures to reliably extrapolate — and reason formally — has been </span><em>the</em><span> central theme of my own work back to </span><a href="https://www.sciencedirect.com/science/article/pii/S0010028598906946" rel="">1998</a><span> and </span><a href="https://mitpress.mit.edu/9780262133791" rel="">2001</a><span>, and has been a theme in all of my challenges to deep learning, going back to 2012, and LLMs in 2019. </span></p><p><span>I strongly believe the current results are robust. After a quarter century of “</span><a href="http://wikibin.org/articles/real-soon-now.html" rel="">real soon now</a><span>” promissory notes I would want a lot more than hand-waving to be convinced than at an LLM-compatible solution is in reach. </span></p><p><span>What I argued in 2001, in </span><em>The Algebraic Mind</em><span>, still holds: </span><a href="https://mitpress.mit.edu/9780262632683/the-algebraic-mind/" rel="">symbol manipulation</a><span>, in which some knowledge is represented truly abstractly in terms of variables and operations over those variables, much as we see in algebra and traditional computer programming, must be part of the mix.  Neurosymbolic AI —  combining such machinery with neural networks – is likely a necessary condition for going forward. </span></p><p><em><strong>Gary Marcus</strong><span> is the author of The Algebraic Mind, a 2001 MIT Press Book that foresaw the Achilles’ Heel of current models. In his most recent book, Taming Silicon Valley (also MIT Press), in Chapter 17, he discusses the need for alternative research strategies.</span></em></p><p data-attrs="{&quot;url&quot;:&quot;https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://garymarcus.substack.com/p/llms-dont-do-formal-reasoning-and?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Started a guide to writing FUSE filesystems in Python (128 pts)]]></title>
            <link>https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html</link>
            <guid>41811983</guid>
            <pubDate>Fri, 11 Oct 2024 18:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html">https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html</a>, See on <a href="https://news.ycombinator.com/item?id=41811983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	
	
<!-- Blog entry follows -->

<p>As DebConf22 was coming to an end, in Kosovo, talking with <em>Eeveelweezel</em> they
invited me to prepare a talk to give for the <a href="https://chipy.org/">Chicago Python User
Group</a>. I replied that I’m not really that much of a Python
guy… But would think about a topic. Two years passed. I meet <em>Eeveelweezel</em>
again for DebConf24 in Busan, South Korea. And the topic came up again. I had
thought of some ideas, but none really pleased me. Again, I do write some Python
when needed, and I <em>teach</em> using Python, as it’s the language I find my students
can best cope with. But <em>delivering a talk</em> to ChiPy?</p>

<p>On the other hand, I have long used a very simplistic and limited filesystem
I’ve designed as an implementation project at class:
<a href="https://github.com/unamfi/sistop-2024-2/blob/main/proyectos/1/README.org">FIUnamFS</a>
(for “Facultad de Ingeniería, Universidad Nacional Autónoma de México”: the
Engineering Faculty for Mexico’s National University, where I teach. Sorry, the
link is in Spanish — but you will find several implementations of it from the
students 😉). It is a toy filesystem, with as many bad characteristics you can
think of, but easy to specify and implement. It is based on contiguous file
allocation, has no support for sub-directories, and is often limited to the size
of a 1.44MB floppy disk.</p>

<p>As I give this filesystem as a <em>project</em> to my students (and not as a mere
<em>homework</em>), I always ask them to try and provide a good, polished, professional
interface, not just the simplistic menu I often get. And I tell them the best
possible interface would be if they provide support for FIUnamFS transparently,
usable by the user without thinking too much about it. With high probability,
that would mean: Use FUSE.</p>

<p><a href="https://gwolf.org/files/2024-10/python-fuse.png"><img src="https://gwolf.org/files/2024-10/python-fuse.400.png" alt="Python FUSE"></a></p>

<p>But, in the six semesters I’ve used this project (with 30-40 students per
semester group), <em>only one student</em> has bitten the bullet and presented a FUSE
implementation.</p>

<p>Maybe this is because it’s not easy to understand how to build a FUSE-based
filesystem from a high-level language such as Python? Yes, I’ve seen several
implementation examples and even nice web pages (i.e. <a href="https://github.com/libfuse/python-fuse/tree/master/example">the examples shipped with
the<code>python-fuse</code>
module</a> <a href="https://www.stavros.io/posts/python-fuse-filesystem/">Stavros’
passthrough filesystem</a>,
<a href="https://thepythoncorner.com/posts/2017-02-27-writing-a-fuse-filesystem-in-python/">Dave Filesystem based upon, and further explaining,
Stavros’</a>,
and several others) explaining how to provide basic functionality. I found a
<a href="https://speakerdeck.com/matteobertozzi/python-fuse-pycon4">particularly useful presentation by Matteo
Bertozzi</a> presented
~15 years ago at PyCon4… But none of those is IMO followable enough by
itself. Also, most of them are <em>very</em> old (maybe the world is telling me
something that I refuse to understand?).</p>

<p>And of course, there isn’t a single interface to work from. In Python only, we
can find
<a href="https://github.com/libfuse/python-fuse/tree/master/example">python-fuse</a>,
<a href="https://github.com/nrclark/pyfuse">Pyfuse</a>,
<a href="https://github.com/fusepy/fusepy">Fusepy</a>… Where to start from?</p>

<p>…So I setup to try and help.</p>

<p>Over the past couple of weeks, I have been slowly working on my own version, and
presenting it as a <em>progressive set of tasks</em>, adding filesystem calls, and
being careful to thoroughly document what I write (but… maybe my documentation
ends up <em>obfuscating</em> the intent? I hope not — and, read on, I’ve provided some
remediation).</p>

<p>I registered a GitLab project for <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide">a hand-holding guide to writing FUSE-based
filesystems in Python</a>. This
is a project where I present several working FUSE filesystem implementations,
some of them RAM-based, some passthrough-based, and I intend to add to this also
filesystems backed on pseudo-block-devices (for implementations such as my
FIUnamFS).</p>

<p>So far, I have added five stepwise pieces, starting from the barest possible
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/1._emptyfs.py">empty
filesystem</a>,
and adding system calls (and functionality) until (so far) either a <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/4._basic_stat_info.py">read-write
filesystem in RAM with basic<code>stat()</code>
support</a>
or a <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/5._read_only_passthrough.py">read-only passthrough
filesystem</a>.</p>

<p>I think providing fun or useful examples is also a good way to get students to
use what I’m teaching, so I’ve added some ideas I’ve had: <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/dnsfs.py">DNS
Filesystem</a>,
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/markdown_compiling_fs.py">on-the-fly markdown compiling
filesystem</a>,
<a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/unzipfs.py">unzip
filesystem</a>
and <a href="https://gitlab.com/gunnarwolf/fuse_in_python_guide/-/blob/main/useful/uncommentfs.py">uncomment
filesystem</a>.</p>

<p>They all provide something that could be seen as useful, in a way that’s easy to
teach, in just some tens of lines. And, in case my comments/documentation are
too long to read, <code>uncommentfs</code> will happily strip all comments and whitespace
automatically! 😉</p>

<p>So… I will be delivering my talk <a href="https://www.chipy.org/meetings/288/">tomorrow (2024.10.10, 18:30 GMT-6) at
ChiPy</a> (virtually). I am also presenting
this talk virtually at <a href="https://eventol.flisol.org.ar/events/jrsl-2024-santa-fe/">Jornadas Regionales de Software
Libre</a> in Santa Fe,
Argentina, next week (virtually as well). And also in November, in person, at
<a href="https://nerdear.la/">nerdear.la</a>, that will be held in Mexico City for the
first time.</p>

<p>Of course, I will also share this project with my students in the next couple of
weeks… And hope it manages to lure them into implementing FUSE in Python. At
some point, I shall report!</p>




      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How long til we're all on Ozempic? (247 pts)]]></title>
            <link>https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic</link>
            <guid>41811263</guid>
            <pubDate>Fri, 11 Oct 2024 17:06:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic">https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic</a>, See on <a href="https://news.ycombinator.com/item?id=41811263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<div data-mode="add-marker">
		<p><img id="marker" src="https://asteriskmag.com/assets/img/asterisk_mark.png" title="save highlight"></p><!-- <a href="https://asteriskmag.com/about/#highlights"><img id="help" src="https://asteriskmag.com/assets/img/asterisk_help.png" title="about highlights"></a> -->
		
	</div>

	<section>
					<p>forecast</p>
				
		 			<h2>
				   
					<span>Greg Justice</span>
							</h2>
			</section>
	
			<section id="rangyscope">
					<p>Over 100 million Americans, and possibly many more, could benefit from GLP-1 drugs. When can they expect to get them?</p>
				<div>
											<div><p>Obesity medication has something of a troubled past. Fen-phen, a weight-loss drug combination popular in the 1990s, was pulled after it was found to cause heart valve problems. Sibutramine, sold under the brand name Meridia, was prescribed until it was discovered to lead to adverse cardiovascular events including strokes in 2010. &nbsp;</p><p>But the market for an effective weight-loss drug is too big and the potential profits too high for pharmaceutical companies to give up. More than one in eight people around the world live with obesity. In the United States, it’s more than two in five. Though many clinical trials of weight-loss drugs over the past decade ended in failure, it was only a matter of time until a successful drug emerged.&nbsp;</p><p>GLP-1 medications<sup>
    <!-- <a id="fnref-1" href="#fn-1"> -->
    <span id="fnref-1">
        1    </span>
    <!-- </a> -->
</sup>
 like Ozempic appear to be that drug. Estimates suggest GLP-1s can reduce body weight by at least 15% when taken regularly — and perhaps even more as newer drugs come to market. And though evidence is still being gathered, they may have benefits beyond weight loss: potentially curbing drinking, treating sleep apnea, and reducing risk of stroke. They’ve been called, in many places, a miracle drug, and as such, the category is poised for massive growth. Gallup&nbsp;<a href="https://news.gallup.com/poll/644861/injectable-weight-loss-drugs-uses-work.aspx" rel="noopener noreferrer">estimated</a> that 15.5 million Americans have tried them, and half as many are currently using them.</p><p>But to date, the supply has been plagued by shortages so severe that there’s a website that tracks which pharmacies have it in stock. Though new products continue to come to market, and companies are doing what they can to increase supply, it’s extremely unlikely that demand will be met in the near term. So what does growth for GLP-1s really look like, and how many people stand to benefit?&nbsp;</p><p>In this forecast, I’m going to look at this over the next six years.</p><p><strong>Question: How many GLP-1 agonist medications will be sold in 2030 in the United States? </strong>Since a substantial portion of people discontinue their use, we’ll measure this by the number of one-year supplies sold.</p></div>
											<p><h2><strong>Estimating current usage</strong></h2>
</p>
											<div><p>Our projections for the future will be based on growth relative to the present, so it’s important to have a good idea of what GLP-1 sales look like today, which we’ll approach in a few different ways.</p><p>Polling is one approach. A Gallup poll from March estimated that 3% of American adults (approximately 7.75 million people) are currently taking GLP-1s. The poll has some issues for our purposes,<sup>
    <!-- <a id="fnref-2" href="#fn-2"> -->
    <span id="fnref-2">
        2    </span>
    <!-- </a> -->
</sup>
 but its flaws bias the results in opposing directions, so approximately eight million is a fine starting point.<sup>
    <!-- <a id="fnref-3" href="#fn-3"> -->
    <span id="fnref-3">
        3    </span>
    <!-- </a> -->
</sup>
</p><p>Another approach is to take company revenue for GLP-1s and divide that by the <a href="https://www.aei.org/wp-content/uploads/2023/09/Estimating-the-Cost-of-New-Treatments-for-Diabetes-and-Obesity.pdf?x91208" rel="noopener noreferrer">estimated net price</a> to get monthly supply. Almost the entire supply of GLP-1s in the United States is manufactured by either Eli Lilly or Novo Nordisk. Looking at their revenue for 2023 implies enough supply for 5.4 million patients each month: 3.5 million for Novo Nordisk and 1.9 million for Eli Lilly.&nbsp;</p><p>But sales are growing fast, and we want to base our estimate as much as we can on 2024 data. Novo Nordisk expects approximately <a href="https://www.morningstar.co.uk/uk/news/248948/novo-nordisk-earnings-beat-estimates-on-weight-loss-sales.aspx" rel="noopener noreferrer">23% sales growth for 2024</a>, or $7.8 billion at current exchange rates. Lilly expects revenue growth of about $8.9 billion. If those projections are right, and we assume 100% of Novo’s growth and 90% of Lilly’s<sup>
    <!-- <a id="fnref-4" href="#fn-4"> -->
    <span id="fnref-4">
        4    </span>
    <!-- </a> -->
</sup>
 is from GLP-1s, that gives us growth of $15.8 billion (54%) in the GLP-1 market, for a total of $45 billion.&nbsp;</p><p>The United States represented 72% of GLP-1 sales globally in 2023 — if that stays roughly constant, we should expect $32 billion in US sales for 2024. If, for simplicity, growth is also evenly spread between brands, then implied supply for 2024 would be 5.4 million * 154% = 8.3 million — similar to the poll results.&nbsp;</p><p>So enough supply for eight million people on average in 2024 seems like a safe base to work from.</p></div>
											<p><h2><strong>The outside view of obesity drugs</strong></h2>
</p>
											<div><p>I’ll begin by taking an “outside view” approach — that is, ignoring the details of specific GLP-1 drugs like Ozempic and coming up with an estimate based on other drugs in roughly the same reference class. This approach is less accurate than a more detailed “inside view” model — which we’ll get to— but it also requires much less information. Taking the outside view allows me to come up with a rough estimate that I can adjust as I do additional research. It also helps me account for factors I might not explicitly think to include when building a detailed model of GLP-1s in particular. From there, I can tweak only the factors that I think are significant and different for Ozempic, rather than trying to establish each and every one from scratch.&nbsp;</p><p>So what’s the right reference class for GLP-1 medications? I chose to look at the uptake of other recent multibillion-dollar “blockbuster” pharmaceutical drug categories. These are new classes of drugs created in the past 40 years or so that, like GLP-1s, experienced rapid expansions in sales and production. The drug categories I used are:</p></div>
											<div><p>Statins (cholesterol-lowering medications) — Lipitor, Crestor, etc.</p><p>TNF-α inhibitors (treatments for inflammatory conditions like rheumatoid arthritis) — Humira, Enbrel, Remicade.</p><p>PD-1/PD-L1 inhibitors (treatments for specific types of cancer) — Keytruda, Opdivo, Tecentriq.</p><p>Direct oral anticoagulants (DOACs, which prevent blood clotting) — Eliquis, Xarelto, Pradaxa.</p></div>
											<div><p>There are hundreds of other types of drugs I could’ve used. I want to address some drugs I didn’t use, and why.</p><p>The first omitted drug is insulin. Insulin is similar to obesity drugs in many ways. It addresses a large patient population, it’s a biologic (or drug made from a living organism), it’s an injection used on an ongoing basis, and it treats similar indications — Ozempic was initially approved as a treatment for diabetes. But market conditions now are very different from those of 1923, when commercial insulin was first introduced. Furthermore, when insulin was discovered and commercialized, it had to be extracted from pig and cow pancreases. Scaling production meant building an operation to collect animal pancreases and developing processes to better refine the pancreatic extract — a much different process than building a modern factory.&nbsp;</p><p>I also left out Hepatitis C drugs like sofosbuvir, which came out in the 2010s. Though this is another blockbuster release that had several billion dollars in sales, it is a cure, rather than an ongoing treatment, and so its sales trend downward rather than up.&nbsp;</p><p>That leaves the four drug classes listed above, which have their own limitations. First, obesity drugs have a larger potential market than any of them. Almost 150 million American adults have diabetes or obesity. About 45 million are eligible for statins,<sup>
    <!-- <a id="fnref-5" href="#fn-5"> -->
    <span id="fnref-5">
        5    </span>
    <!-- </a> -->
</sup>
 the next largest population, and the markets for DOACs and TNF-α inhibitors are likely in the single-digit millions. The US government also has more incentive to make GLP-1 drugs available — both because promoting weight loss would lead to savings for Medicare down the line and because it would (probably) please voters. Both factors suggest faster growth.</p><p>On the other hand, GLP-1s currently receive different insurance treatment in the United States when prescribed for weight loss, and they currently have lower patient adherence rates when compared with comparable drugs. Those factors point in the other direction.</p><p>Finally, and most importantly, obesity drugs have already shown a different growth path than our base rate drugs. The other drugs gained popularity very quickly after launch. But GLP-1s were initially prescribed as a treatment for diabetes and then gained popularity for weight loss later on thanks to off-label use. Novo Nordisk then underestimated demand for Wegovy and ran into supply problems, which meant sales didn’t truly take off until late 2022 or 2023.&nbsp;</p><p>There are two different benchmarks we can get from the reference class: sales and prescriptions. Each approach has its benefits and drawbacks, but both should hopefully give similar estimates. If they do, then we’ll be more confident that our base rate forecast is robust.</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/2592186f22-1720207926/web_2.svg" alt="">
  </p>
  </figure>
</div>
											<p><h2><strong>Calculating growth in sales</strong></h2>
</p>
											<div><p>For each category, I collected annual net sales data for each drug from published research and drugmakers’ annual reports.<sup>
    <!-- <a id="fnref-6" href="#fn-6"> -->
    <span id="fnref-6">
        6    </span>
    <!-- </a> -->
</sup>
 I then calculated year-over-year changes in sales relative to the drug class’s first full year of significant sales. The growth rates over time for the drugs selected follow&nbsp; similar exponential decay patterns. Growth rates start high when sales are small. PD-1/PD-L1 sales grew by about 75% in their second year, and DOACs more than doubled. But by their eighth year on the market, sales for all classes grew between only 9% and 18%.</p><p>Looking at cumulative growth relative to the fourth year of sales, we can see very roughly the type of growth we should expect for GLP-1s over the next few years. If 2024 is year four, we expect somewhere between 180% and 240% growth in total sales by 2030.</p><p>To implement the sales model, I built a Monte Carlo model and ran it 10,000 times. The goal of a Monte Carlo simulation is to model future sales while accounting for uncertainty and variability in the possible growth trajectory of GLP-1s. This works by setting some parameters ourselves and randomly sampling others for each run. The cumulative result of this process should tell us which outcomes are most likely. To get a forecast for GLP-1s using that information, we’ll need to first answer one question: What year of sales are we in for GLP-1s?</p><p>This matters because, while sales grow each year, the rate at which they grow slows down. Picking the wrong year would give us the wrong rate of growth, and ultimately very different results. We could take the simple approach and define year one as the first full year in which a modern GLP-1 drug, Ozempic, was widely sold (2019),<sup>
    <!-- <a id="fnref-7" href="#fn-7"> -->
    <span id="fnref-7">
        7    </span>
    <!-- </a> -->
</sup>
 making 2024 what we call “index year” six. This isn’t a bad approach. In 2019, Ozempic was approved only as a treatment for diabetes, but that’s still a patient population of 38 million Americans. But GLP-1s have exploded into the larger obesity market much more recently. Indeed, if we look at growth in revenue for Novo Nordisk and Eli Lilly from 2023 to 2024 and compare it with our growth rate model, it suggests that 2024 should really be seen as approximately index year 3.5. I handled this discrepancy by making it one of the parameters I randomly varied between runs of my model.</p><p>After converting sales numbers to patient supply, this produces the following distribution:</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/c44a55175b-1720207979/web_2-20.svg" alt="">
  </p>
  </figure>
</div>
											<div><p>Our median estimate at this point suggests a patient-year supply of around 18 million.</p><p>That’s what looking at sales gets us, but we can also look at prescription volume directly. Data on prescription fills is not widely publicly available, especially going back multiple decades. However, there is some limited data from the long-running Medical Expenditure Panel Survey. The survey measures 18 respondents reporting a TNF-α prescription in year four and 38 in year 10. In the same period, DOAC respondents went from 132 to 527. Data doesn’t go back far enough to include the early years of statins.</p><p>Still, this is a data point worth considering. Our outside view using sales predicts approximately 100% growth in US supply. Prescription growth is higher than those estimates, so we may want to adjust them upward.&nbsp;</p></div>
											<p><h2><strong>Incorporating the inside view</strong></h2>
</p>
											<div><p>The outside view approaches above are a useful starting point. Now we’ll consider information specific to GLP-1s.&nbsp;</p><p>We should start with supply constraints. After that, there are three other considerations that, in my opinion, are both significant to and different from the reference class: insurance treatment, patient adherence, and pipeline developments.&nbsp;</p><p><strong>Supply constraints and manufacturing expansion</strong></p><p>Demand for GLP-1s is sky-high. A major limiting factor for sales right now, according to both Eli Lilly and Novo Nordisk, is supply. So why is supply so hard to scale up?&nbsp;</p><p>Right now, the constraint appears to be in production facilities. Entirely new facilities need to be bought and repurposed or even built to meet demand. At baseline, much of pharmaceutical manufacturing is contracted out to third-party companies. Normally, this system would provide some flexibility for Novo and Lilly to expand production, but current capacity is too small to meet the historic demand for GLP-1s. Novo Nordisk recently agreed to acquire three factories by purchasing Catalent, a contract manufacturer which also produces products for Eli Lilly, in order to maximize production of its own products. Additionally, injector pens for Mounjaro are apparently unusually complex and may not be able to use existing assembly lines from other factories, which <a href="https://www.bloomberg.com/news/articles/2024-05-02/why-weight-loss-drugs-wegovy-zepbound-are-facing-shortages" rel="noopener noreferrer">further limits</a> Lilly’s options.&nbsp;</p><p>There are also geopolitical concerns. The United States is <a href="https://www.bloomberg.com/news/articles/2024-03-06/weight-loss-drugs-threatened-by-us-effort-to-contain-china" rel="noopener noreferrer">actively cracking down</a> on Chinese contract manufacturers, including WuXi AppTec, who reportedly produces much of the active ingredient in Mounjaro. All of this suggests that Novo and Lilly will need to invest in building new facilities or expanding existing ones, rather than utilizing existing sites. This isn’t like building a new Costco. Such advanced manufacturing facilities are multiyear, multibillion-dollar projects.&nbsp;</p><p>What’s especially notable about the current situation is that it’s very distinct from the reference class drugs. We didn’t see large spikes in capital expenditures after the launch of drug classes in our base rate. However, we see it very clearly already for Novo and Lilly. Lilly recently committed at least $13 billion for factories in North Carolina, Wisconsin, Indiana, and Germany, which are expected to come online at varying points by 2028. That’s a nearly 70% increase relative to their gross plant, property, and equipment (PP&amp;E) holdings in 2021 of about $19 billion. Novo Nordisk has similarly announced at least $19 billion in investments. $11 billion of that is for Catalent’s factories, with the rest going to facilities in Kalundborg, Denmark, and Chartres, France. This would more than double their gross PP&amp;E holdings of about $14 billion in 2021.</p><p>How much should we update based on this information?&nbsp;</p><p>Not as much it may seem we should at first glance. First, some investments would have happened regardless. Lilly averaged approximately $1.2 billion annually in PP&amp;E purchases from 2015 to 2021, and Novo Nordisk averaged approximately $1.1 billion. The large sums announced recently likely include some investments they would have made anyway.&nbsp;</p><p>It’s also already partly priced in. Our model predicts doubling sales, and while it doesn’t explain how, that capacity has to come from somewhere. We already expected that sales would be much larger than TNF-α inhibitors and DOACs, so unusually large investments are reasonable.</p><p>There are a lot of unknowns here that make it very difficult to rigorously factor this information in. It’s clearly important, but I don’t know, for example, to what degree the base rate factors this type of growth in, and I don’t know how much these investments actually increase production, whether they’ll be completed on time or in budget, what level of growth would be possible without them, or what additional investments are yet to come.&nbsp;</p><p>In the end, I tried to estimate the GLP-1-relevant manufacturing assets Novo and Lilly have now and then estimated how much of an increase their new investments might represent.&nbsp;</p><p>The announced investments across both companies total $32 billion.<sup>
    <!-- <a id="fnref-8" href="#fn-8"> -->
    <span id="fnref-8">
        8    </span>
    <!-- </a> -->
</sup>
 GLP-1s were 71% of Novo’s revenue in 2023, 16% of Lilly’s in 2023, and 26% of Lilly’s in 2024Q1. If these sales are proportional to the manufacturing capacity used to create those drugs, then about 40% of Novo and Lilly’s combined estimate of $45 billion in gross PP&amp;E is for GLP-1s, for a total of $18 billion; $25 billion would then mean a 140% increase in GLP-1-relevant PP&amp;E. However, a 140% increase in company-owned PP&amp;E does not necessarily mean a 140% increase in production capacity, because much capacity currently is contracted out. If company-owned production grows 140% but contracted production is unchanged, then total growth will be much lower. Contractors may scale up in parallel, or the new company-owned facilities may displace them, which seems likely for WuXi AppTec. Lastly, we’re also likely to see even more investment by Novo and Lilly announced between now and 2030.</p><p>Ultimately, the companies are investing more in PP&amp;E than I would’ve expected, which means I need to update. Increased investment is significant both in itself and for what it tells us about the companies’ future intentions. In light of this, I increase the base rate estimates for US volume by a pretty sizable 30%, with a standard deviation of 5%. This means I’m estimating most likely a 20%–40% increase in volume in light of this information.</p><p>This would tentatively bring median estimated patient-year supply up from 18 million to around 24 million.</p><p><strong>Insurance treatment</strong></p><p>Next let’s consider insurance treatment. In the United States, insurers are typically required to cover all FDA-approved medications that are “medically necessary.” While diabetes falls into that category, obesity does not. Medicare is actually prohibited by law from covering medication prescribed for weight loss. This makes the drugs much more expensive for patients when used only for weight loss, which should lead to lower use.&nbsp;</p><p>About 140 million Americans are obese. 38 million have diabetes, but most of them — approximately 31 million — are obese as well. This makes the total patient pool for GLP-1s approximately 147 million people, a bit over a quarter of whom can get GLP-1s prescribed for diabetes and covered through their insurance.</p><p>And insurers have a strong incentive to keep coverage limitations in place. Right now, an annual supply of Ozempic or Mounjaro costs roughly <a href="https://www.aei.org/wp-content/uploads/2023/09/Estimating-the-Cost-of-New-Treatments-for-Diabetes-and-Obesity.pdf?x91208" rel="noopener noreferrer">$3,000</a>. If our supply estimate of 22 million patient-years is accurate, that would mean a total annual spend of $66 billion — more than 10% of current spending on&nbsp;<a href="https://pubmed.ncbi.nlm.nih.gov/37094296/" rel="noopener noreferrer">all pharmaceuticals in the United States</a> combined. Insurance companies and Medicare may seek to restrict access to these drugs to keep premiums down.</p><p>However, expanding coverage for these drugs is a popular idea. In one poll, <a href="https://www.kff.org/health-costs/poll-finding/kff-health-tracking-poll-may-2024-the-publics-use-and-views-of-glp-1-drugs/" rel="noopener noreferrer">61%</a> of people stated Medicare should cover GLP-1s for weight loss. There’s also the chance that the government repeals Medicare’s ban on weight-loss drugs, which increasingly feels antiquated now that we have safe and effective treatments. And finally, we shouldn’t underestimate pharma’s famously strong lobby — or advocacy from the millions of patients who stand to benefit.&nbsp;</p><p>So currently, insurance coverage is quite restricted for GLP-1s, which should limit sales even as shortages become less severe. However, it seems like the desires of patients and most lobbyists are aligned in wanting to remove those limits. Medicare could also be able to negotiate the price of Ozempic as soon as 2025, and lower prices may make coverage more palatable. However, it’s still possible that prices will remain stubbornly high in the commercial (non-Medicare) market and that insurers will be able to restrict coverage enough to slow down demand.&nbsp;</p><p>Combining all of those pieces, I’d estimate (very roughly) that there’s a 50% chance that nothing changes, which reduces volume by 10% relative to the base rate drugs with no restrictions, a 40% chance of GLP-1s getting coverage parity, so no change from the base rate, and 10% chance that insurers restrict coverage even more, reducing volume 20%. On average, this is a 7% adjustment downward relative to the base rate.</p><p><strong>Patient adherence</strong></p><p>The second consideration is that so far, GLP-1 drugs have had low adherence rates. One study showed <a href="https://www.primetherapeutics.com/news/real-world-analysis-of-glp-1a-drugs-for-weight-loss-finds-low-adherence-and-increased-cost-in-first-year/" rel="noopener noreferrer">68%</a> of patients who started taking a GLP-1 drug for weight loss weren’t taking it within a year. Some percentage of this is due to supply shortages — there’s simply not enough to go around. But there are concerns that people are discontinuing the drugs once their weight loss has plateaued, once they’re no longer able to afford it, or due to side effects like nausea, diarrhea, vomiting, and loss of muscle mass. This discontinuation rate is much higher than those of the base rate drugs. <a href="https://pubmed.ncbi.nlm.nih.gov/28958039/" rel="noopener noreferrer">Statins</a> and <a href="https://pubmed.ncbi.nlm.nih.gov/26490106/" rel="noopener noreferrer">TNFa inhibitors</a> have one-year adherence rates of 59% and 73% respectively.&nbsp;</p><p>GLP-1s’ net turnover rate<sup>
    <!-- <a id="fnref-9" href="#fn-9"> -->
    <span id="fnref-9">
        9    </span>
    <!-- </a> -->
</sup>
 may be considerably higher than that of comparable drugs, meaning it may be harder to reach new GLP-1 patients. However, if turnover is driven by seasonality or people hitting weight-loss plateaus, then lapsed patients may become eligible again in the near future. On the other hand, if side effects become more well known as time goes on, there’s a chance that puts a dent in demand. Still, demand is so high relative to supply, though, that this probably won’t be a major issue. I’d say there’s a 20% chance of an 8% decrease in volume resulting from these concerns.</p><p><strong>Pipeline drugs</strong></p><p>The last factor I considered is other obesity drugs currently in development. There are several <a href="https://www.fiercebiotech.com/biotech/late-breaking-obesity-glp-1-wegovy-zepbound-novo-lilly-pipeline-rd-landscape" rel="noopener noreferrer">ongoing phase 3 clinical trials</a> for obesity drugs that could lead to subsequent FDA approval if successful. It’s likely though not certain that these will be roughly comparable to existing treatments in terms of efficacy. And most of them are produced by Eli Lilly or Novo Nordisk, so we should expect their introduction to mostly cannibalize sales and manufacturing capacity from existing drugs, rather than widen the market.&nbsp;</p><p>However, innovations in the pipeline might help raise adherence or improve production efficiencies. Orforglipron, for example, is a daily oral pill, as opposed to a weekly injection. Rybelsus, which is daily oral semaglutide made by Novo Nordisk, showed up to <a href="https://www.novonordisk.com/news-and-media/news-and-ir-materials/news-details.html?id=166110" rel="noopener noreferrer">17.4% weight loss</a> at 68 weeks in a Phase 3 trial but is <a href="https://www.reuters.com/business/healthcare-pharmaceuticals/novo-nordisk-says-obesity-pill-leads-15-weight-loss-availability-be-determined-2023-06-26/" rel="noopener noreferrer">not yet approved</a> for weight loss. Orals don’t depend on the complex injection pen manufacturing process, but they also use much larger doses of active ingredients than injectables do. A typical maintenance dose of the injectable Ozempic is 0.5 mg per week. The equivalent dose for Rybelsus is 7 mg per <em>day</em>. This means a manufacturer can create many more doses of injectables than orals with the same ingredients. Daily oral alternatives are likely to be available by 2030. But given their comparable efficacy, higher ingredient requirements, and the fact that the manufacturers overlap, I don’t believe that the current pipeline merits an adjustment.</p><p>In sum, there’s three adjustments to be added. Supply investments: a 30% increase with standard deviation of 5%. Insurance coverage: 50% chance of 10% decrease, and a&nbsp; 10% chance of a 20% decrease. Side effects and adherence: a 20% chance of an 8% decrease.&nbsp;</p><p>Applying those adjustments to the original estimate, we get our final prediction:</p></div>
											<div><figure>
      <p><img src="https://asteriskmag.com/media/pages/issues/07/how-long-til-were-all-on-ozempic/4899f3d74b-1720208054/web_2-21.svg" alt="">
  </p>
  </figure>
</div>
											<div><p><strong>My forecast predicts that the supply of GLP-1s will increase from eight million patient-years to roughly enough for approximately 23 million Americans by 2030. </strong>Still, this is only enough supply for about 15% of the 147 million Americans with diabetes or obesity.&nbsp;</p><p>Most patients who want them will not have access to these drugs in the near future. The need is vast, especially abroad, where rollouts are being delayed until demand in the United States can be met. And as indications continue to be added for conditions ranging from heart disease to <a href="https://www.bloomberg.com/news/newsletters/2023-12-08/can-ozempic-wegovy-treat-alcoholism">potentially even alcoholism</a>, demand will only grow. It will likely be many years before these drugs become widely available to the patients who need them. It takes a long time to make a miracle.</p></div>
										 
				</div>
		
	</section>
	 	<section>
		 		 <p><strong>Greg Justice</strong> is a member of the Samotsvety forecasting group. He has worked as an analyst and project manager in the healthcare industry, and is completing his MBA at Chicago Booth.</p>		 		 		 </section>
	 	<section>            
		<p>
			Published July 2024		</p>
		
		<p>Have something to say? Email us at <a href="mailto:letters@asteriskmag.com">letters@asteriskmag.com</a>.</p>		                        
	</section>	
	
	
	<!--end published content, not coming soon-->

	<!--tags-->
		<section>
		<h4>Further Reading</h4>                
			<p>
				More:  
									<span data-no="tag-1">health</span>
									<span data-no="tag-2">forecasting</span>
							</p>
			<!--related articles-->
			             
	</section>
	 
	
	  
	<p>Subscribe</p>
	<div id="signup-article-popup">
			
<p><img src="https://asteriskmag.com/assets/img/asterisk_x.png">
		</p></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lm.rs: Minimal CPU LLM inference in Rust with no dependency (177 pts)]]></title>
            <link>https://github.com/samuel-vitorino/lm.rs</link>
            <guid>41811078</guid>
            <pubDate>Fri, 11 Oct 2024 16:46:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/samuel-vitorino/lm.rs">https://github.com/samuel-vitorino/lm.rs</a>, See on <a href="https://news.ycombinator.com/item?id=41811078">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
    <img alt="lmrs logo" src="https://github.com/samuel-vitorino/lm.rs/raw/main/repo_cover.svg">
</picture></themed-picture>
<p dir="auto">lm.rs: run inference on Language Models locally on the CPU with Rust</p>

</div>
<hr>
<p dir="auto"><strong>🌃 Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.</strong></p>
<p dir="auto">Inspired by Karpathy's <a href="https://github.com/karpathy/llama2.c">llama2.c</a> and <a href="https://github.com/karpathy/llm.c">llm.c</a> I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google's Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.</p>
<p dir="auto">Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn't it incredible that in a few years, we could have AGI running in a few lines of poorly written Rust code?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prepared models</h2><a id="user-content-prepared-models" aria-label="Permalink: Prepared models" href="#prepared-models"></a></p>
<p dir="auto">Some benchmarks and download links for the models and tokenizers. I recommend using Q8_0, Q4_0 quantization still being improved. Speed measured on a 16-core AMD Epyc.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Size</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q4_0-LMRS" rel="nofollow">Gemma 2 2B IT Q4_0</a></td>
<td>1.39G</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-2b-it-q8_0-LMRS" rel="nofollow">Gemma 2 2B IT Q8_0</a></td>
<td>2.66GB</td>
<td>18 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q4_0-LMRS" rel="nofollow">Gemma 2 9B IT Q4_0</a></td>
<td>4.91GB</td>
<td>7 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/gemma2-9b-it-q8_0-LMRS" rel="nofollow">Gemma 2 9B IT Q8_0</a></td>
<td>9.53GB</td>
<td>8 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-LMRS" rel="nofollow">Llama 3.2 1B IT</a></td>
<td>4.94GB</td>
<td>20 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 1B IT Q8_0</a></td>
<td>1.27GB</td>
<td>35 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q4_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q4_0</a></td>
<td>1.71GB</td>
<td>17 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Llama-3.2-3B-Instruct-Q8_0-LMRS" rel="nofollow">Llama 3.2 3B IT Q8_0</a></td>
<td>3.31GB</td>
<td>16 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-vision-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Vision Q8_0</a></td>
<td>4.28GB</td>
<td>15 tok/s</td>
</tr>
<tr>
<td><a href="https://huggingface.co/samuel-vitorino/Phi-3.5-mini-instruct-q8_0-LMRS" rel="nofollow">PHI 3.5 IT Mini Q8_0</a></td>
<td>3.94GB</td>
<td>16 tok/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instructions</h2><a id="user-content-instructions" aria-label="Permalink: Instructions" href="#instructions"></a></p>
<p dir="auto">You can download the prepared quantized model and tokenizer model files in the lmrs format from huggingface. If you'd prefer to convert the models published by Google/Meta on huggingface yourself, please refer to the following section. Otherwise, you can skip ahead to the build section.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Model Conversion</h3><a id="user-content-model-conversion" aria-label="Permalink: Model Conversion" href="#model-conversion"></a></p>
<p dir="auto">Install additional python dependencies (assuming you already have pytorch installed) used in export.py and tokenizer.py:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre>pip install -r requirements.txt</pre></div>
<p dir="auto">Download the <strong>.safetensors</strong> and <strong>config.json</strong> files from the original model's page on huggingface (So we don't have to clone the pytorch repo). For multimodal models (PHI3.5 Vision), we also need the CLIP <strong>.config</strong> <a href="https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json" rel="nofollow">file</a>.</p>
<p dir="auto">Use the export.py script to convert the model bfloat16 weights into the LMRS format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]"><pre>python export.py --files [ordered .safetensor files] --config [model config.json] --save-path [name and path to save] --type [model type (GEMMA/LLAMA/PHI)]</pre></div>
<p dir="auto">To export the quantized version use the <strong>--quantize</strong> and <strong>--quantize-type</strong> flags. The int8 quantized model size should be 4X smaller (from ~9.8G to ~2.5G, depending on the group size). For multimodal models include the <strong>--vision-config</strong> argument.</p>
<p dir="auto">Use the tokenizer.py script to convert the tokenizer model into the LMRS tokenizer format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]"><pre>python tokenizer.py --model-id [huggingface model_id] --tokenizer-type [type of the tokenizer (GEMMA/LLAMA/PHI)]</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build</h3><a id="user-content-build" aria-label="Permalink: Build" href="#build"></a></p>
<p dir="auto">Compile the rust code with cargo (make sure to pass the target-cpu flag):</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&quot;-C target-cpu=native&quot; cargo build --release --bin chat"><pre><span>RUSTFLAGS</span>=<span><span>"</span>-C target-cpu=native<span>"</span></span> cargo build --release --bin chat</pre></div>
<p dir="auto">To enable multimodality, include the multimodal feature by passing the <strong>--features multimodal</strong> argument.</p>
<p dir="auto">And you are good to go:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/chat --model [model weights file]"><pre>./target/release/chat --model [model weights file]</pre></div>
<p dir="auto">Other arguments include tokenizer, temperature, top-p, show-metrics etc. To check available arguments run with --help. For multimodal models use the <strong>--image</strong> argument with the image path.</p>
<hr>
<p dir="auto">To run the backend for the <a href="https://github.com/samuel-vitorino/lm.rs-webui">WebUI</a>, first compile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="RUSTFLAGS=&quot;-C target-cpu=native&quot; cargo build --release --features backend --bin backend"><pre><span>RUSTFLAGS</span>=<span><span>"</span>-C target-cpu=native<span>"</span></span> cargo build --release --features backend --bin backend</pre></div>
<p dir="auto">For multimodality enable the <strong>backend-multimodal</strong> feature.</p>
<p dir="auto">Then run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./target/release/backend --model [model weights file]"><pre>./target/release/backend --model [model weights file]</pre></div>
<p dir="auto">You can change the ip and port with --ip and --port. Other flags such as temperature, etc. are also available. For multimodal compatibility use the <strong>--multimodal</strong> flag. You can now connect via the web interface.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODOs</h2><a id="user-content-todos" aria-label="Permalink: TODOs" href="#todos"></a></p>
<p dir="auto">Some things to do in the future:</p>
<ul>
<li> Add other sampling methods.</li>
<li> Test the 9B and 27B models (tested the 9B, 27B would be too slow).</li>
<li> Parallelize the multi head attention loop.</li>
<li> Add performance metrics.</li>
<li> Ability to give a system prompt</li>
<li> Quantization support (int8, int4).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Dead man's switch without reliance on your infra (117 pts)]]></title>
            <link>https://github.com/adamdecaf/deadcheck</link>
            <guid>41809879</guid>
            <pubDate>Fri, 11 Oct 2024 14:40:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/adamdecaf/deadcheck">https://github.com/adamdecaf/deadcheck</a>, See on <a href="https://news.ycombinator.com/item?id=41809879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">deadcheck</h2><a id="user-content-deadcheck" aria-label="Permalink: deadcheck" href="#deadcheck"></a></p>
<p dir="auto"><a href="https://godoc.org/github.com/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/50f96480adc71d0679655bcdeaaf1d17bd1a1a1d200cf0912e0cb575fff6200c/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f6164616d64656361662f64656164636865636b3f7374617475732e737667" alt="GoDoc" data-canonical-src="https://godoc.org/github.com/adamdecaf/deadcheck?status.svg"></a>
<a href="https://github.com/adamdecaf/deadcheck/actions"><img src="https://github.com/adamdecaf/deadcheck/workflows/Go/badge.svg" alt="Build Status"></a>
<a href="https://codecov.io/gh/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/075df6433861f25f7b1de1d009bb28cc1b46383fda6243f46f81bf154f8bd598/68747470733a2f2f636f6465636f762e696f2f67682f6164616d64656361662f64656164636865636b2f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="Coverage Status" data-canonical-src="https://codecov.io/gh/adamdecaf/deadcheck/branch/master/graph/badge.svg"></a>
<a href="https://goreportcard.com/report/github.com/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/769326ffc5972c7cde59a002222232ad9a33639bf3ab487e510dda34d38a1257/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6164616d64656361662f64656164636865636b" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/adamdecaf/deadcheck"></a>
<a href="https://raw.githubusercontent.com/adamdecaf/deadcheck/master/LICENSE" rel="nofollow"><img src="https://camo.githubusercontent.com/29ba86e33d6c73f556fd517b07ad76acf76b1352e87e1e681e9c51c9501e7dba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d417061636865322d626c75652e737667" alt="Apache 2 License" data-canonical-src="https://img.shields.io/badge/license-Apache2-blue.svg"></a>
<a href="https://hub.docker.com/r/adamdecaf/deadcheck" rel="nofollow"><img src="https://camo.githubusercontent.com/f4b22cf0558a58274c54e8f8684097ff975c5a640ed87029fd4ce13844d5fccd/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6164616d64656361662f64656164636865636b" alt="Docker Pulls" data-canonical-src="https://img.shields.io/docker/pulls/adamdecaf/deadcheck"></a></p>
<p dir="auto">Deadcheck is a versatile <a href="https://en.wikipedia.org/wiki/Dead_man's_switch" rel="nofollow">dead man's switch</a> designed to be independent of the infrastructure hosting it. The project allows users to set up checks that must be periodically "confirmed" to indicate that everything is fine. If a check isn't confirmed within the specified time, Deadcheck triggers a set of actions, such as sending alerts or executing tasks to ensure that the necessary steps are taken in the event you're no longer able to do so.</p>
<p dir="auto">Deadcheck is an automated dead man's switch that <strong>doesn't rely on its own uptime</strong>. Instead, it uses external services for final triggers, ensuring alerts and actions occur even if the hosting infrastructure is down. Deadcheck relies on third parties (e.g., PagerDuty) to handle alerts when a check is missed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Decoupled from Hosting Infrastructure: Deadcheck ensures that the actual execution of alerts or incidents happens independently from the infrastructure hosting it. Even if the Deadcheck server goes down, the check-in process will still trigger events using external services.</li>
<li>Configurable Check Intervals: Flexible check intervals allow you to set up switches ranging from short-term (hours) to long-term (months).</li>
<li>Provider-Agnostic Setup: Deadcheck is designed to integrate with a variety of external systems, allowing for a wide range of customization in how your dead man's switch operates.</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/adamdecaf/deadcheck/blob/master/docs/images/timeline.png"><img src="https://github.com/adamdecaf/deadcheck/raw/master/docs/images/timeline.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">Download the <a href="https://github.com/adamdecaf/deadcheck/releases/latest">latest release for your architecture</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="checks:
  - id: &quot;hourly-sync&quot;
    name: &quot;Upload data every hour&quot;
    description: &quot;<string>&quot;
    schedule:
      every:
        interval: &quot;1h&quot;
        start: &quot;14:00&quot;
        end: &quot;18:00&quot;
    # Override alert for one check
    alert:
      pagerduty:
        apiKey: &quot;<string>&quot;
        escalationPolicy: &quot;<string>&quot;

  - id: &quot;2pm-checkin&quot;
    name: &quot;Reports Finalized&quot;
    schedule:
      weekdays:
        timezone: &quot;America/New_York&quot;
        times:
          - &quot;14:00&quot;
        # Only allow check-ins between 13:55 and 14:05
        tolerance: &quot;5m&quot;

  - id: &quot;5pm-close&quot;
    name: &quot;Close out for the day&quot;
    schedule:
      bankingDays:
        timezone: &quot;America/New_York&quot;
        times:
          - &quot;17:00&quot;
        # Only allow check-ins between 16:55 and 17:05
        tolerance: &quot;5m&quot;

# Global alert configuration
alert:
  pagerduty:
    apiKey: &quot;<string>&quot;
    escalationPolicy: &quot;<string>&quot;"><pre><span>checks</span>:
  - <span>id</span>: <span><span>"</span>hourly-sync<span>"</span></span>
    <span>name</span>: <span><span>"</span>Upload data every hour<span>"</span></span>
    <span>description</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
    <span>schedule</span>:
      <span>every</span>:
        <span>interval</span>: <span><span>"</span>1h<span>"</span></span>
        <span>start</span>: <span><span>"</span>14:00<span>"</span></span>
        <span>end</span>: <span><span>"</span>18:00<span>"</span></span>
    <span><span>#</span> Override alert for one check</span>
    <span>alert</span>:
      <span>pagerduty</span>:
        <span>apiKey</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
        <span>escalationPolicy</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>

  - <span>id</span>: <span><span>"</span>2pm-checkin<span>"</span></span>
    <span>name</span>: <span><span>"</span>Reports Finalized<span>"</span></span>
    <span>schedule</span>:
      <span>weekdays</span>:
        <span>timezone</span>: <span><span>"</span>America/New_York<span>"</span></span>
        <span>times</span>:
          - <span><span>"</span>14:00<span>"</span></span>
        <span><span>#</span> Only allow check-ins between 13:55 and 14:05</span>
        <span>tolerance</span>: <span><span>"</span>5m<span>"</span></span>

  - <span>id</span>: <span><span>"</span>5pm-close<span>"</span></span>
    <span>name</span>: <span><span>"</span>Close out for the day<span>"</span></span>
    <span>schedule</span>:
      <span>bankingDays</span>:
        <span>timezone</span>: <span><span>"</span>America/New_York<span>"</span></span>
        <span>times</span>:
          - <span><span>"</span>17:00<span>"</span></span>
        <span><span>#</span> Only allow check-ins between 16:55 and 17:05</span>
        <span>tolerance</span>: <span><span>"</span>5m<span>"</span></span>

<span><span>#</span> Global alert configuration</span>
<span>alert</span>:
  <span>pagerduty</span>:
    <span>apiKey</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span>
    <span>escalationPolicy</span>: <span><span>"</span>&lt;string&gt;<span>"</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="PUT /v1/checks/{id}/check-in"><pre><code>PUT /v1/checks/{id}/check-in
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="{&quot;nextExpectedCheckIn&quot;:&quot;2024-10-09T21:05:00Z&quot;}"><pre>{<span>"nextExpectedCheckIn"</span>:<span><span>"</span>2024-10-09T21:05:00Z<span>"</span></span>}</pre></div>
<p dir="auto">Successful response, or failure in the response.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Integrations</h2><a id="user-content-integrations" aria-label="Permalink: Integrations" href="#integrations"></a></p>
<ul dir="auto">
<li>PagerDuty: A service is used and incident created but snoozed preventing notifications. Each successful check-in pushes the snooze out into the future until the next expected check-in.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported and tested platforms</h2><a id="user-content-supported-and-tested-platforms" aria-label="Permalink: Supported and tested platforms" href="#supported-and-tested-platforms"></a></p>
<ul dir="auto">
<li>64-bit Linux (Ubuntu, Debian), macOS, and Windows</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Apache License 2.0 - See <a href="https://github.com/adamdecaf/deadcheck/blob/master/LICENSE">LICENSE</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Manifest v2 is now removed from Chrome canary (333 pts)]]></title>
            <link>https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline</link>
            <guid>41809698</guid>
            <pubDate>Fri, 11 Oct 2024 14:20:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline">https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline</a>, See on <a href="https://news.ycombinator.com/item?id=41809698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<p>Understand when Manifest V2 will stop working for extensions</p>

<h2 id="latest" data-text="Latest" tabindex="-1">Latest</h2>

<h3 id="october_9th_2024_an_update_on_manifest_v2_phase-out" data-text="October 9th 2024: an update on Manifest V2 phase-out." tabindex="-1">October 9th 2024: an update on Manifest V2 phase-out.</h3>

<p>Over the last few months, we have continued with the Manifest V2 phase-out.
Currently the chrome://extensions page displays a warning banner for all users
of Manifest V2 extensions. Additionally, we have started disabling Manifest V2
extensions on pre-stable channels.</p>

<p>We will now begin disabling installed extensions still using Manifest V2 in
Chrome stable. This change will be slowly rolled out over the following weeks.
Users will be directed to the Chrome Web Store, where they will be recommended
Manifest V3 alternatives for their disabled extension. For a short time, users
will still be able to turn their Manifest V2 extensions back on. Enterprises
using the
<a href="https://chromeenterprise.google/policies/#ExtensionManifestV2Availability">ExtensionManifestV2Availability</a>
policy will be exempt from any browser changes until June 2025. See our <a href="https://blog.chromium.org/2024/05/manifest-v2-phase-out-begins.html">May
2024 blog</a>
for more context.</p>

<h3 id="june_3rd_2024_the_manifest_v2_phase-out_begins" data-text="June 3rd 2024: the Manifest V2 phase-out begins." tabindex="-1">June 3rd 2024: the Manifest V2 phase-out begins.</h3>

<p>Starting on June 3rd on the Chrome Beta, Dev and Canary channels, if users still
have Manifest V2 extensions installed, some will start to see a warning banner
when visiting their extension management page - chrome://extensions - informing
them that some (Manifest V2) extensions they have installed will soon no longer
be supported. At the same time, extensions with the Featured badge that are
still using Manifest V2 will lose their badge.</p>

<h2 id="upcoming" data-text="Upcoming" tabindex="-1">Upcoming</h2>

<h3 id="june_2025_chrome_mv2_deprecation_enterprise_rollout" data-text="June 2025: Chrome MV2 deprecation enterprise rollout" tabindex="-1">June 2025: Chrome MV2 deprecation enterprise rollout</h3>

<p>Enterprises using the
<a href="https://chromeenterprise.google/policies/#ExtensionManifestV2Availability">ExtensionManifestV2Availability</a>
policy to ensure the continued functioning of Manifest V2 extensions in their
organization will have one additional year - until June 2025 - to migrate the
Manifest V2 extensions in their organization. Browsers with the policy enabled
won't be impacted by the rollout of the deprecation until that time.</p>

<h2 id="past" data-text="Past" tabindex="-1">Past</h2>

<h3 id="june_2022_chrome_web_store_-_no_new_private_extensions" data-text="June 2022: Chrome Web Store -  no new private extensions" tabindex="-1">June 2022: Chrome Web Store -  no new private extensions</h3>

<p>Chrome Web Store stopped accepting new Manifest V2 extensions with visibility
set to "Private".</p>

<h3 id="january_2022_chrome_web_store_-_no_new_public_unlisted_extensions" data-text="January 2022: Chrome Web Store - no new public / unlisted extensions" tabindex="-1">January 2022: Chrome Web Store - no new public / unlisted extensions</h3>

<p>Chrome Web Store stopped accepting new Manifest V2 extensions with visibility
set to "Public" or "Unlisted". The ability to change Manifest V2 extensions from
"Private" to "Public" or "Unlisted" was removed.</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: NotesHub: cross-platform, Markdown-based note-taking app (121 pts)]]></title>
            <link>https://about.noteshub.app</link>
            <guid>41808943</guid>
            <pubDate>Fri, 11 Oct 2024 12:37:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://about.noteshub.app">https://about.noteshub.app</a>, See on <a href="https://news.ycombinator.com/item?id=41808943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you like the sharing and security advantages of storing your notes in GitHub repositories, you likely will feel at home in this versatile notetaker/organizer/keeper. From there you can use your notes to feed a public website or blog. For those who prefer iCloud Drive, that option is available as well.</p><p>You’ll find yourself quickly and easily setting up a hierarchical folder structure and navigating it by simply tapping oval icons in a horizontal row at the top of your screen. Couldn’t be more intuitive. In that structure you can store almost anything: text, images and photos, names, addresses, phone numbers, geolocations, dates and times, etc. You could even set up a zettelkasten, a knowledge management database in hierarchical form—for business record keeping, research, study, or just personal reference.</p><p>The markdown editor is fully featured and included is a primer that you can refer to as you write. Tables, code blocks, html tags, even footnotes are all supported.</p><p>What attracted me initially was the kanban board feature. I experimented by setting up an archplot structure “template” for novel plotting using the “Save the Cat” method. I simply brainstorm and add event summaries or setting descriptions or dialogue or whatever in vertical columns below each horizontal plot element. Don’t know how much that will help me yet, but so far I’m pleased with the results.</p><p>Lots of potential with this NotesHub app, and the one-time purchase price can’t be beat.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Play killed my game and won't tell me why (125 pts)]]></title>
            <link>https://antiidlereborn.com/news/</link>
            <guid>41808917</guid>
            <pubDate>Fri, 11 Oct 2024 12:34:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antiidlereborn.com/news/">https://antiidlereborn.com/news/</a>, See on <a href="https://news.ycombinator.com/item?id=41808917">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hello everyone – this is Tukkun (of course) and this is my first real post on this website! I know you guys are all excited for the release of my upcoming game “Anti-Idle: Reborn” and it is my intention to keep this website updated with contents related to the game – its development, gameplay sneak peeks, and so on. But before that, today I’d like to write this post to explain the current situation.</p>







<h2>In short…</h2>



<ul>
<li>I have submitted the game “Anti-Idle: Reborn” to both Google and Apple for review. Both Google and Apple have reviewed the game and approved it for production.</li>



<li>Closed Beta of the game has been ongoing for about a month. People have found lots of weird bugs, and I have fixed many of them.</li>



<li>However, on October 7, 2024, without any prior warnings, Google decided to terminate my account due to “prior violations” and “High Risk Behavior”. I’ve re-read the policies, I’ve checked everything I can think of, and I still can’t figure out why.</li>



<li>I sent an appeal but it seems like they haven’t looked into it yet (as of October 11, 2024). I have gathered any information I can think of and sent it to the Google Play Team, but I only receive the same (possibly automated) response.</li>



<li>I have developed games for many years, and “Anti-Idle: Reborn” is more than just my passion. It’s over one year of full time work and dedication, and it is my future source of income. With the Google Play Developer account terminated, I can no longer continue work.</li>



<li>A quick search showed me that I am not the only one in this situation. Lots of other developers have had their account terminated for vague reasons, possibly by bots and automated algorithms, and received nothing other than automated messages when appealing.</li>



<li>I would like to emphasize that I understand the need for thorough app reviews and termination of accounts that violate the rules. However, this shouldn’t come at a cost of many good faith developers’ accounts being wrongly terminated.</li>
</ul>







<h2>The full story</h2>



<p>While I think most of you are familiar with my works and are just here to check for new information about my game <strong>Anti-Idle: Reborn</strong>, I understand that some of you might have gotten here from other pages and have no idea who I am. In that case, or in the rare case that a Google employee is somehow reading this, let me introduce myself.</p>



<p>I am <strong>Tukkun</strong>, an indie game developer making games since 2008. My most significant work is a PC Flash game I made back in 2009 called <a href="http://www.kongregate.com/games/tukkun/anti-idle-the-game" data-type="link" data-id="http://www.kongregate.com/games/tukkun/anti-idle-the-game"><strong>Anti-Idle: The Game</strong></a>, uploaded to the website Kongregate. As of the writing of this post, the game has been played 16,392,188 times (and this is not counting plays of the offline version). I know I shouldn’t say too much good stuff about my own projects (just like anything, my game had imperfections), but <strong>Anti-Idle: The Game</strong> is often said to have pioneered the idle game genre. It is one of the first games to popularize many mechanics often seen in modern idle games and is a source of inspiration for the development of many popular idle games, including several games on the Play Store (Android) and App Store (iOS). It is even mentioned in the <a href="https://en.wikipedia.org/wiki/Incremental_game">Wikipedia page for Incremental game</a>: </p>



<p><em>The early pioneers of idle games also saw some games parodying the genre, such as Anti-Idle (2009, by tukkun) which has elements of both active and idle games. The game was extremely complicated, content-rich, and constantly updated, and it helped popularize the genre.</em></p>


<div>
<figure><img decoding="async" loading="lazy" width="646" height="646" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image.png 646w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-300x300.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-150x150.png 150w" sizes="(max-width: 646px) 100vw, 646px"></figure></div>


<p><em>A screenshot of “Anti-Idle: The Game”</em> </p>



<p>Following the success of <strong>Anti-Idle: The Game</strong>, I decided to develop the mobile sequel <strong>Anti-Idle: Reborn</strong>. I started doing serious design works on the game since 2023, and started programming it in Unity since the beginning of 2024, with the target of releasing it to Android and iOS late 2024.</p>



<p>The idea of developing a mobile sequel started as early as around 2020, with Flash no longer being supported by browsers, and lots of people in the community asked for a “mobile version” of the game. However, like many grown up adults, I had a day job and didn’t have enough free time to develop a mobile game. Despite that, I’ve released a few updates for the original Anti-Idle: The Game – 13 years after its initial release, the game still has a nice active community.</p>



<p><strong>That’s why I decided to follow my passion</strong> – I went as far as quitting my day job some time ago to fully dedicate myself to game development, and to make Anti-Idle live on. I’ve decided to announce and work on <strong>Anti-Idle: Reborn</strong> as my first mobile game. Creating a game from scratch feels great – when I managed to get the game design work on paper, when I opened Unity and created a simple loading screen that worked, when I got a prototype running… everything felt like a huge milestone.</p>


<div>
<figure><img decoding="async" loading="lazy" width="188" height="300" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-188x300.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-188x300.png 188w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-640x1024.png 640w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-768x1229.png 768w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-960x1536.png 960w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1-1280x2048.png 1280w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-1.png 1440w" sizes="(max-width: 188px) 100vw, 188px"></figure></div>


<p><em>A screenshot of “Anti-Idle: Reborn” (under development)</em></p>



<p>The day eventually came when I created enough features to launch the first version of the game and decided that <strong>Closed Beta</strong> can finally begin. I made a Google Forms so people can voluntarily register for Closed Beta and shared it with the community that still played my original game after over 14 years. To my surprise, they shared it to many other communities, including <a href="https://www.reddit.com/r/incremental_games/comments/1etrj2e/antiidles_mobile_sequel_closed_beta_opening_and/">this post on Reddit r/incremental_games</a>, and in total <strong>over 1000 people have applied</strong>!</p>



<p>Then, of course, in order to start Closed Beta, I would have to upload my game to the stores: Play Store (Android) and App Store (iOS). Little did I know, this is only the beginning of the story.</p>







<h3>Uploading the game to iOS</h3>



<p>I’ve always been under the impression that it is really difficult to upload a game to the App Store of iOS. They have always set high quality standards and from what I’ve heard, they seem to review apps really thoroughly.</p>



<p>And I guess I was right. Within a few hours of uploading my game to the App Store for review, it got rejected.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1024" height="626" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-1024x626.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-1024x626.png 1024w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-300x183.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2-768x470.png 768w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-2.png 1248w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p><em>Apple’s initial rejection</em></p>



<p>Oh, great, I knew it. Of course “Anti-Idle” is a world-renowned intellectual property and it is natural for Apple to think I have no permission to use it (sigh).</p>



<p>I attempted to convince Apple that I am the real Tukkun (because that’s who I am), and I submitted some proof to the Apple review team, including a screenshot of the source code of the original <strong>Anti-Idle: The Game</strong> and a link from my profile on Kongregate to this website of <strong>Anti-Idle: Reborn</strong>. I told Apple that I would provide any other information if necessary. A few hours later, they reviewed the game again, pointed out a bug and even sent screenshots as evidence. I fixed it, and they approved the game. All of the subsequent updates also passed through Apple’s review just fine. All good.</p>


<div>
<figure><img decoding="async" loading="lazy" width="836" height="279" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-5.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-5.png 836w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-5-300x100.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-5-768x256.png 768w" sizes="(max-width: 836px) 100vw, 836px"></figure></div>


<p><em>iOS version has been approved! Hooray!</em></p>







<h3>Uploading the game to Android</h3>



<p>Onto the next fun part – I entered the necessary information and uploaded the game to the Play Store. It seemed to get through review pretty quickly. I was able to get the Closed Beta up and running in no time. I think initial review for my first closed testing version took around half a day. My first impressions with the Google Play Console were pretty good, it’s easy and intuitive to use.</p>



<p>However, for the Play Store, there is a policy that before applying for production (which is required to start open testing and put the game on pre-registration) you need to run closed testing for at least 14 days with at least 20 testers (this seems like a new policy since 2023). All good, that sounds like it will increase the quality of apps uploaded to the store. And I didn’t have problem finding testers at all – as I said, I had over 1000 people applying so I just randomly picked 40 of them for the first phase of testing.</p>



<p>During testing, the testers have found a lot of bugs, ranging from minor ones to game-breaking ones, like microtransactions not working correctly, user data sometimes being rolled back and every action within the game causing serious lag. I fixed the game-breaking ones pretty quickly (as a result though, I didn’t have much time creating new features or writing progress updates). And with the game-breaking bugs gone, I have also fulfilled the requirements of “20 testers in 14 days” so I figured I should apply for production. So I did.</p>



<p>There was a questionnaire about how I found testers, what the testers did, how I incorporated the testers’ feedback, what makes the game stand out, and why I think the game is ready for production. I just answered the questions truthfully. And after Google’s review, on October 4, they approved my request for production! Look, I even received a congratulations email.</p>


<div>
<figure><img decoding="async" loading="lazy" width="631" height="386" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-3.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-3.png 631w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-3-300x184.png 300w" sizes="(max-width: 631px) 100vw, 631px"></figure></div>


<p><em>My game has been granted Google Play production access!</em></p>



<p>Great! Now all that’s left is testing some more, improving the game quality and then publishing to production, right? That’s Google’s recommendation as well.</p>



<p>Unfortunately, before I could do that, three days after the above email, on October 7, testers started reporting that in-app purchases suddenly stopped working and the URL to download the game doesn’t work anymore. In a hurry, I checked and was shocked to find out that <strong>my account has been terminated</strong>.</p>


<div>
<figure><img decoding="async" loading="lazy" width="998" height="870" src="https://antiidlereborn.com/wp-content/uploads/2024/10/image-4.png" alt="" srcset="https://antiidlereborn.com/wp-content/uploads/2024/10/image-4.png 998w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-4-300x262.png 300w, https://antiidlereborn.com/wp-content/uploads/2024/10/image-4-768x669.png 768w" sizes="(max-width: 998px) 100vw, 998px"></figure></div>


<p><em>My account has been terminated… Wait, what?</em></p>



<p>I did a quick search for the issue and learned that a termination for this reason is most likely related to <strong>prior violations, possibly prior violations of associated accounts</strong>. Which is weird. My app got two policy warnings from Google Play before, but both times I fixed it promptly, and according to Google’s <a href="https://support.google.com/googleplay/android-developer/answer/9899234?hl=en" data-type="link" data-id="https://support.google.com/googleplay/android-developer/answer/9899234?hl=en">Enforcement Process</a>, app rejections or violations of this level shouldn’t affect the standing of my account. And my game was even approved for production. Which leads me think of prior violations of associated accounts. But what’s an associated account exactly? I am an indie developer and this is my first and only account. Well, I did add some trusted developers and testers into the internal testing track, but I’ve checked with them and they insist that their accounts are in good standing. Do the 40 testers I added for closed testing count? And why should I be responsible for their prior violations (how do I even know whether they made any violations in the first place)?</p>



<p>I even took extra precaution steps: I used my Google Play Console account on only one device that I use for releasing the game, I didn’t use VPN, I didn’t use the same network with other people, and ensured I didn’t accidentally connect to some public Wi-Fi. And yet Google still decided that my account has a “high risk of abuse” and terminated it.</p>



<p>I have heard a lot of stories about other Android developers having their accounts terminated, but I never thought it would happen to me. I re-read their policies once again just to be sure (by now, I think I’ve read through Google’s policies at least five times). Believing that I did nothing wrong, I sent an appeal.</p>







<h3>Appealing</h3>



<p>The appeal form only allowed me to enter 1000 characters, so this is what I wrote.</p>



<hr>



<p><em>After thoroughly checking the Developer Program Policies and Developer Distribution Agreement, as well as the Policy Coverage policy, I don’t believe I have made any violations that could have led to account termination. I have promptly resolved violations in the past, and my app was even approved for production a few days ago. I am also working closely with my testers in Closed Testing to fix bugs, improve app performance, and ensure that my game “Anti-Idle: Reborn” meets all of Google’s standards and meets user expectations prior to production. I am new to the Developer Program, this is my first account and my first app. I don’t know about “associated accounts” but if this includes the testers’ emails I have added, they are users who volunteered to test my game and I’m not associated with any of their violations (if any). I am always thriving to improve app quality and would greatly appreciate it if you could tell me what is wrong with my app or account so that I could resolve it.</em></p>



<hr>







<p>In hindsight, that was probably not the most useful information that could have fit into 1000 characters, but that’s all I could think of at the time.</p>



<p>I received a system email saying that my appeal would be reviewed by a specialist, subsequently followed by an email with the name of a person at Google (presumably the “specialist”).</p>



<hr>



<p><em>Hi developers at Tukkun,</em></p>



<p><em>Thanks for contacting the Google Play team.</em></p>



<p><em>I’ve received your appeal and I appreciate your patience while I look into it.</em></p>



<p><em>I’ll let you know as soon as I have any additional information to share. Please let me know if you have any questions in the meantime.</em></p>



<p><em>[…]</em></p>



<p><em>Regards,<br>[Name of Google specialist]</em></p>



<hr>







<p>I subsequently sent some additional information in relatively lengthy emails, basically everything that I can think of.</p>



<ul>
<li>Any information I know about my prior violations (I’ve promptly resolved them anyway)</li>



<li>How I am “the real Tukkun” and have rights to the things I’m using within the game (basically the same things I’ve sent Apple)</li>



<li>How I’ve put my heart and soul into the development of Anti-Idle: Reborn and that it is a very anticipated release. I’ve even sent screenshots of the game’s design files</li>



<li>Anything I know about what’s possibly considered “associated accounts”</li>



<li>Anything else that I think might be the problem</li>
</ul>



<p>I just said everything I can know of with all of my honesty, and said that whatever the problem is I am committed to resolving it. I still have no idea what the exact problem is though. Of course, in my emails, I tried asking for more information too.</p>



<p>However, both times I contacted them, they responded with the exact same email:</p>



<hr>



<p><em>Hi developers at Tukkun,</em></p>



<p><em>Thanks again for contacting the Google Play team.</em></p>



<p><em>I’ve received your appeal and I appreciate your patience while I look into it.</em></p>



<p><em>I’ll let you know as soon as I have any additional information to share.</em></p>



<p><em>Regards,<br>[Name of Google specialist]</em></p>



<hr>







<p>They didn’t respond instantly, but several hours after I sent the information. And to be fair they did say “thanks <span>again</span>” (they know it’s not the first time I contacted them), but there’s no other useful information in the email. At this point I’m not even sure if that’s an actual human or just an automated email delayed to feel human. I wouldn’t even be surprised if Google started giving AI unique “names” to make them sound like human specialists.</p>



<p>According to Google, it can take up to 7 days for them to make a decision. As of the writing of this post, it is the 4th day. There’s still time and I guess “I appreciate your patience” is still better than a rejection, but I am beginning to get impatient and this is affecting my future plans for the development of Anti-Idle: Reborn. And the Closed Beta testers are just as impatient as I am.</p>



<p>I still believe that I have done nothing wrong, and I hoped it would be easy to show my good faith (just like how Apple immediately re-reviewed my app after I sent the evidence that I am indeed Tukkun), but I’m starting to get more and more worried as each day passes without any new information. And from what I’ve read about these appeals, most of the time they just get rejected for vague reasons.</p>



<p>And that’s it. Over 15 years of game development, first app on Android with over 1 year of development, and my career as an Android game developer is at stake for no reason, even before the game is released.</p>



<p>Dear everyone who is looking forward to the release of Anti-Idle: Reborn on Android, thank you for your continued support and your interest in the game. While I can’t make any promises under the current situation, I will keep you updated with any new information.</p>



<p>Dear Google, thank you for providing a trustworthy place for app developers to provide apps to billions of users. Once again, I would like to emphasize that I understand the need for thorough app reviews and termination of accounts that violate the rules. It is what allows users to trust the Google Play Store to download and use apps. However, it also needs trust from developers so they can confidently develop great apps without the fear of everything being erased without prior warning and for no reason.</p>



<p>Please, tell me what I am doing wrong and what I can do to have my account and my game restored. Anti-Idle: Reborn is my hopes and dreams, and a large community is waiting for it to become a reality.</p>



<hr>



<p><em>In case anyone at Google is reading this, my appeal ticket number is <strong>6-1733000037134</strong>, and my game’s package ID (before it was removed from Google Play) is <strong>com.tukkun.anti.idle.reborn</strong></em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Lisp compiler to RISC-V written in Lisp (214 pts)]]></title>
            <link>http://www.ulisp.com/show?4Y20</link>
            <guid>41808696</guid>
            <pubDate>Fri, 11 Oct 2024 11:56:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.ulisp.com/show?4Y20">http://www.ulisp.com/show?4Y20</a>, See on <a href="https://news.ycombinator.com/item?id=41808696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<p>11th October 2024</p>
<p>This is a simple experimental Lisp compiler, written in uLisp, that will compile a Lisp function into RISC-V machine code. You can run the compiler on the RISC-V core of a Raspberry Pi Pico 2 (or another RP2350-based board):</p>
<p><img src="http://www.ulisp.com/pictures/3j/raspberrypipico2.jpg" alt="RaspberryPiPico2.jpg" width="600" height="275"></p>
<p>It's based on my earlier project&nbsp;<a href="http://www.ulisp.com/show?4W2I">A Lisp compiler to ARM written in Lisp</a>.</p>
<h4><span>Introduction</span></h4>
<p>When I added the facility of executing machine code to uLisp I had in mind the eventual goal of being able to compile uLisp functions into machine code, and this is a first step in that direction.</p>
<p>The nice thing about compiling Lisp is that you don't have to write a tokeniser or parser, because Lisp programs are already in a consistent structure that can be processed by another Lisp program.</p>
<p>The compiler program is written in the subset of Common Lisp supported by uLisp, and will run on the RISC-V core of a RP2350-based board; I used a Raspberry Pi Pico 2. You can also run it using Common Lisp on a laptop or desktop computer, and display the code it generates, but of course you won't be able to run the RISC-V machine code because Common Lisp doesn't have uLisp's <strong>defcode</strong> command.</p>
<p>I got my initial inspiration for this compiler from Peter Norvig's book "Paradigms of Artificial Intelligence Programming" <sup id="cite_ref1"><a href="#cite_note1">[1]</a></sup>.</p>
<h4>Resources</h4>
<p>To use the compiler you first need to load the RISC-V assembler from:&nbsp;<a href="http://www.ulisp.com/list?31OE">RISC-V assembler in uLisp</a>.</p>
<p>Get the full source of the compiler here:&nbsp;<a href="http://www.ulisp.com/list?4Y4Q">Lisp compiler for RISC-V</a>.</p>
<p>Or from GitHub here:&nbsp;<a href="https://github.com/technoblogy/lisp-arm-compiler" target="_blank">https://github.com/technoblogy/lisp-arm-compiler</a>.</p>
<p>For information about setting up uLisp on a Raspberry Pi Pico 2 see:&nbsp;<a href="http://www.ulisp.com/show?4X21">Raspberry Pi Pico 2</a>.</p>
<h4>Using the compiler</h4>
<p>To run the compiler you simply call <strong>compile</strong> on a Lisp function; for example:</p>
<pre>(compile 'fibonacci)</pre>
<p>The function will be compiled into a machine code function, replacing the original Lisp code, so that calling <strong>fibonacci</strong> will now execute the RISC-V machine-code version.</p>
<p>You can also display the code generated for an expression by calling <strong>comp</strong> on the expression; for example:</p>
<pre>(pprint (comp '(* 13 17)))

(:integer
  ($li 'a0 13)
  ($addi 'sp 'sp -4)
  ($sw 'a0 0 '(sp))
  ($li 'a0 17)
  ($lw 'a1 0 '(sp))
  ($addi 'sp 'sp 4)
  ($mul 'a0 'a1 'a0))</pre>
<p>The <strong>:integer</strong> prefix shows that the result is an integer; see below.</p>
<p>For examples of several simple Lisp programs that it will successfully compile see <a href="#examples">Examples</a> below. These also give a comparison of the speed of the Lisp and machine-code versions.</p>
<h4>Specification</h4>
<p>The compiler understands the following Lisp objects:</p>
<p><strong>Defining variables and functions:</strong> defun, setq</p>
<p><strong>Symbols:</strong> nil, t</p>
<p><strong>List functions:</strong> car, cdr</p>
<p><strong>Arithmetic functions:</strong> +, -, *, /, mod, 1+, 1-</p>
<p><strong>Arithmetic comparisons:</strong> =, &lt;, &lt;=, &gt;, &gt;=, /=</p>
<p><strong>Conditionals:</strong> if, and, or</p>
<h4>Tail-call optimisation</h4>
<p>Although the compiler doesn't include any iteration constructs, it does provide tail-call optimisation which can make recursive programs as efficient as iterative ones. Consider this recursive program to add two positive numbers:</p>
<pre>(defun add (a b)
  (if (= b 0) a
    (add (+ a 1) (- b 1))))</pre>
<p>On a system without tail-call optimisation, evaluating:</p>
<pre>(add 10000 10000)</pre>
<p>will probably fail, because it requires 10000 stack frames to store the intermediate results. This compiler recognises that the recursive call to <strong>add</strong> can be replaced by a jump to the start of the program, and so it has no problem evaluating it. For a more sensible example see <strong><a href="#factor">factor</a></strong> below.</p>
<h3>How the compiler works</h3>
<h4>Register usage</h4>
<p>To avoid needing to keep track of register usage the compiler makes use of the stack to pass values to an expression, and store the value returned by an expression.</p>
<p>The following table shows how the RISC-V registers are used within the compiler:</p>
<table>
<thead>
<tr>
<td><strong>Registers</strong></td>
<td><strong>Use</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>a0 a1 a2 a3</td>
<td>Used to pass the parameters to the main function's arguments.</td>
</tr>
<tr>
<td>a0</td>
<td>Contains the value returned by the main function.</td>
</tr>
<tr>
<td>a4 a5 a6 a7</td>
<td>Contain copies of the function arguments within the function.</td>
</tr>
<tr>
<td>a0 a1</td>
<td>Used to pass the arguments to each operator.</td>
</tr>
<tr>
<td>a0</td>
<td>Used to return the value from each operator.</td>
</tr>
<tr>
<td>s0 to s11</td>
<td>Local variables.</td>
</tr>
</tbody>
</table>
<h4>Compiling an expression</h4>
<p>The following steps show the sequence of compiling an expression, such as:</p>
<pre>(* x 13)</pre>
<ul>
<li>Code is generated to evaluate each of the arguments, in this case <strong>x</strong> and 13, and each result is pushed onto the stack, apart from the last which is left in <strong>a0</strong>.</li>
<li>The first value is popped from the stack into register&nbsp;<strong>a1.</strong></li>
<li>The function, in this case *, is then evaluated for <strong>a1</strong> and <strong>a0</strong>, with the result in <strong>a0</strong>.</li>
</ul>
<p>This stack-based approach ensures that a more complex expression, such as:</p>
<pre>(* (- x 1) (+ x 13))</pre>
<p>will also compile into correct code, without conflicts between registers.</p>
<h4>Calling the function recursively</h4>
<p>The compiler supports calling a function recursively from within the function itself. Because the registers corresponding to the parameters and local variables would be overwritten by the recursive call they are stored on the stack around the function call.</p>
<p>There are several recursive functions in the examples below.</p>
<h4>Types</h4>
<p>For boolean operations I decided to represent <strong>nil</strong> as 0, and <strong>t</strong> as 1. A problem I hadn't anticipated was that I would need to keep track of what type of object each function returned, integer or boolean. For example, consider the problem of compiling the statement:</p>
<pre>(and x y)</pre>
<p>If <strong>x</strong> has the value 0 and <strong>y</strong> has the value 7 this should return 7. However, if <strong>x</strong> has the value <strong>nil</strong>&nbsp;and&nbsp;<strong>y</strong>&nbsp;has the value 7 this should return <strong>nil</strong>. Representing <strong>nil</strong> as zero leads to an ambiguity.</p>
<p>I solved this by returning a type, <strong>:integer</strong> or <strong>:boolean</strong>, with each compiled expression, according to the following rules:</p>
<ul>
<li>Predicates, and <strong>t</strong> or <strong>nil</strong>, always return a <strong>:boolean</strong>.</li>
<li>Arithmetic operations always return an <strong>:integer</strong>.</li>
<li>An <strong>if</strong> form requires a <strong>:boolean</strong> test form and returns an <strong>:integer</strong>.</li>
<li>A <strong>progn</strong>&nbsp;or <strong>let</strong> block returns the type of its last expression.</li>
</ul>
<p>An item with an ambiguous type returns the type <strong>nil</strong>.</p>
<h4>Running the examples</h4>
<p>I used the following simple examples to test the compiler. Before compiling a new function you might want to remove the previous one from memory using&nbsp;<strong>makunbound</strong>&nbsp;to free up the code memory before compiling the next function; for example:</p>
<pre>(makunbound 'fibonacci)</pre>
<p>Alternatively you could increase the amount of memory available for machine code by editing the directive such as:</p>
<pre>#define CODESIZE 256&nbsp;</pre>
<p>before uploading uLisp to your board.</p>
<h3 id="examples">Examples</h3>
<p>The following examples take integer arguments and return an integer result.</p>
<h4 id="factor">Factor</h4>
<p>This function takes a simple approach to finding the least prime factor of a number:</p>
<pre>(defun factor (n d)
  (if (&gt; (* d d) n) n
   (if (= 0 (mod n d)) d
     (factor n (1+ d)))))</pre>
<p>It should be called with n equal to the number to be factorized, and d=2. It takes advantage of the compiler's tail-call optimisation, which makes it as efficient as an iterative solution. If the number is prime,&nbsp;factor&nbsp;will print the number itself.</p>
<p>To find the least prime factor of&nbsp;2146654199 (46327 x 46337):</p>
<p>Lisp version:</p>
<pre>&gt; (time (factor 2146654199 2))
46327
Time: 5.4 s</pre>
<p>Compiled version:</p>
<pre>&gt; (time (factor 2146654199 2))
46327
Time: 19 ms</pre>
<p>You can use the above&nbsp;function as the basis for a simple recursive routine to factorize a number into a list of its prime factors:</p>
<pre>(defun factorize (n)
  (let ((f (factor n 2)))
    (if (= n f) (list n) (cons f (factorize (/ n f))))))</pre>
<p>For example:</p>
<pre>&gt; (factorize 731731731)
(3 17 43 333667)</pre>
<h4>Takeuchi function</h4>
<p>This is a version of the highly-recursive benchmark I use for comparing versions of Lisp <sup id="cite_ref2"><a href="#cite_note2">[2]</a></sup>:</p>
<pre>(defun tak (x y z)
  (if (&gt;= y x) z
    (tak
     (tak (1- x) y z)
     (tak (1- y) z x)
     (tak (1- z) x y))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (tak 18 12 6))
7
Time: 4.1 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (tak 18 12 6))
7
Time: 16 ms</pre>
<h4>Factorial</h4>
<p>This is a recursive implementation of the factorial function:</p>
<pre>(defun fact (n)
  (if (&lt;= n 1) 1
    (* n (fact (- n 1)))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (fact 12))
479001600
Time: 1 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (fact 12))
479001600
Time: 0 ms</pre>
<h4>Fibonacci</h4>
<p>This is a recursive implementation of the Fibonacci series:</p>
<pre>(defun fibonacci (n)
  (if (&lt; n 3) 1
    (+ (fibonacci (- n 1)) (fibonacci (- n 2)))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (fibonacci 27))<br>196418
Time: 50.5 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (fibonacci 27))
196418
Time: 80 ms</pre>
<h4>Greatest Common Divisor</h4>
<p>A recursive algorithm to calculate the greatest common divisor of two integers.</p>
<pre>(defun gcd (a b)
  (if (= b 0) a
   (gcd b (mod a b))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (gcd 2032460032 2056252672))
256
Time: 1 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (gcd 2032460032 2056252672))
256
Time: 0 ms</pre>
<h4>Hofstadter Q sequence</h4>
<p>This is one of several recursive sequences described in Douglas Hofstadter's book "Gödel, Escher, Bach: an Eternal Golden Braid". It is defined as follows:</p>
<pre>(defun q (n)
  (if (&lt;= n 2) 1
    (+
     (q (- n (q (- n 1))))
     (q (- n (q (- n 2)))))))</pre>
<p>It is related to the Fibonacci sequence, except that in this case&nbsp;the two preceding terms specify how far to go back in the sequence to find the two terms to be summed.</p>
<p>Lisp version:</p>
<pre>&gt; (time (q 21))
12
Time: 8.6 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (q 21))
12
Time: 25 ms</pre>
<h4>Two-dimensional recursive function Q2</h4>
<p>This function Q2 is my two-dimensional extension of the Hofstadter Q sequence <sup id="cite_ref3"><a href="#cite_note3">[3]</a></sup>:</p>
<pre>(defun q2 (x y)
  (if (or (&lt; x 1) (&lt; y 1)) 1
    (+ (q2 (- x (q2 (1- x) y)) y)
       (q2 x (- y (q2 x (1- y)))))))</pre>
<p>Lisp version:</p>
<pre>&gt; (time (q2 7 8))
31
Time: 13.8 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (q2 7 8))
31
Time: 50 ms</pre>
<h4>Number of combinations - nCr</h4>
<p>This is a simple but very inefficient way of recursively calculating nCr, based on Pascal's Triangle:</p>
<pre>(defun ncr (n r)
  (if (or (= r 0) (= r n)) 1
    (+ (ncr (1- n) (1- r)) (ncr (1- n) r))))</pre>
<p>For example, to calculate the number of possible poker hands from a pack of cards:</p>
<p>Lisp version:</p>
<pre>&gt; (time (ncr 52 5))
2598960
Time: 615.5 s
</pre>
<p>Compiled version</p>
<pre>&gt; (time (ncr 52 5))
2598960
Time: 1.7 s</pre>
<h3 id="listexamples">List examples</h3>
<p>Any of the arguments to a machine-code function can be a list, in which case the address of the list is passed to the routine in the corresponding parameter. You can then use the functions <strong>car</strong> and <strong>cdr</strong> to process the elements in the list.</p>
<h4>Dot product</h4>
<p>This recursive function calculates the dot product of two vectors:</p>
<pre>(defun dot-product (a b)
  (if (and a b)
      (+ (* (car a) (car b)) (dot-product (cdr a) (cdr b)))<br>    0))</pre>
<p>It can handle two vectors of arbitrary length provided they are the same length.</p>
<p>For example, to calculate the following dot product:</p>
<p>(987 654 321)&nbsp;•&nbsp;(963 852 741) = 987&nbsp;×&nbsp;963 + 654&nbsp;×&nbsp;852 + 321&nbsp;×&nbsp;741 = 1745550</p>
<p>Lisp version:</p>
<pre>&gt; (time (dot-product '(987 654 321) '(963 852 741)))
1745550
Time: 0 ms
</pre>
<p>Compiled version</p>
<pre>&gt; (time (dot-product '(987 654 321) '(963 852 741)))
1745550
Time: 0 ms</pre>
<h3>Compiler source</h3>
<p>Here's a description of the source of the compiler.</p>
<h4>Invoking the compiler</h4>
<p>To compile a Lisp function you simply give the command compile followed by the name of the function; for example, to compile the <strong>fibonacci</strong> function:</p>
<pre>(compile 'fibonacci)</pre>
<p>Here's the definition of the command <strong>compile</strong>:</p>
<pre>(defun compile (name)
  (if (eq (car (eval name)) 'lambda)
    (eval (comp (cons 'defun (cons name (cdr (eval name))))))
 "Not a Lisp function"))</pre>
<h4>Main compiler function</h4>
<p>The main function&nbsp;<strong>comp</strong> returns the compiled code for an expression or form, as a list of assembler instructions prefixed by the type, <strong>:integer</strong> or <strong>:boolean</strong>:</p>
<pre>(defun comp (x &amp;optional env tail)
  (cond
   ((null x) (type-code :boolean '(($li 'a0 0))))
   ((eq x t) (type-code :boolean '(($li 'a0 1))))
   ((symbolp x) (comp-symbol x env tail))
   ((atom x) (type-code :integer (list (list '$li ''a0 x))))
   (t (let ((fn (first x)) (args (rest x)))
        (case fn
          (defun (setq *label-num* 0)
                 (setq env (mapcar #'(lambda (x y) (cons x y)) (second args) *locals*))
                 (comp-defun (first args) (second args) (cddr args) env))
          (progn (comp-progn args env tail))
          (if    (comp-if (first args) (second args) (third args) env tail))
          (setq  (comp-setq args env tail))
          (t     (comp-funcall fn args env tail)))))))</pre>
<p>The function <strong>comp</strong> takes the item <strong>x</strong> to be compiled,&nbsp;the current environment <strong>env</strong> associating each local variable with a register, and a flag <strong>tail</strong>&nbsp;which is true if the item has no successors.</p>
<p>Each of the different types of form are handled by separate functions such as <strong>comp-defun</strong>, <strong>comp-if</strong>, and <strong>comp-progn</strong>.</p>
<h4>Utilities</h4>
<p>The compiler uses the following utility functions:</p>
<p>The functions <strong>push-regs</strong> and <strong>pop-regs</strong> generate instructions to push a list of registers to the stack, and pop a list of registers from the stack:</p>
<pre>(defun push-regs (&amp;rest regs)
  (let ((n -4))
  (append
   (list (list '$addi ''sp ''sp (* -4 (length regs))))
   (mapcar #'(lambda (reg) (list '$sw (list 'quote reg) (incf n 4) ''(sp))) regs))))

(defun pop-regs (&amp;rest regs)
  (let ((n (* 4 (length regs))))
  (append
   (mapcar #'(lambda (reg) (list '$lw (list 'quote reg) (decf n 4) ''(sp))) regs)
   (list (list '$addi ''sp ''sp (* 4 (length regs)))))))</pre>
<p>The function <strong>mappend</strong> applies a function to the elements of a list, and the results, which should be lists, are appended together:</p>
<pre>(defun mappend (fn lst)
  (apply #'append (mapcar fn lst)))</pre>
<p>The function <strong>type-code</strong> adds a code type label to the front of a list of assembler instructions, and the functions code-type and code return the code type label, and the code list, respectively:</p>
<pre>(defun type-code (type code) (cons type code))

(defun code-type (type-code) (car type-code))

(defun code (type-code) (cdr type-code))</pre>
<p>The function <strong>checktype</strong> gives an error if the value returned is not the correct type:</p>
<pre>(defun checktype (fn type check)
  (unless (or (null type) (null check) (eq type check))
    (error "Argument to '~a' must be ~a not ~a~%" fn check type)))</pre>
<p>The lists <strong>*params*</strong> and <strong>*locals*</strong> list the registers available for use in the compiler:</p>
<pre>(defvar *params* '(a0 a1 a2 a3))

(defvar *locals* '(a4 a5 s0 s1 a6 a7 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11))</pre>
<p>Finally, <strong>gen-label</strong> generates a new label for use in branches and jumps:</p>
<pre>(defvar *label-num* 0)

(defun gen-label ()
  (read-from-string (format nil "lab~d" (incf *label-num*))))</pre>
<p>The remaining functions handle the compiling of specific types of Lisp form:</p>
<h4>Symbols</h4>
<p>The environment is represented by an association list giving the register associated with each variable, such as:</p>
<pre>((y . r5) (x . r4))</pre>
<p>The function <strong>comp-symbol</strong> looks up a symbol in the association list and returns the appropriate register:</p>
<pre>(defun comp-symbol (x env)
  (let ((reg (cdr (assoc x env))))
    (type-code nil (list (list '$mv ''a0 (list 'quote reg))))))</pre>
<h4>Assignment</h4>
<p>The function <strong>comp-setq</strong> handles assignment to a variable:</p>
<pre>(defun comp-setq (args env tail)
  (let ((value (comp (second args) env tail))
        (reg (cdr (assoc (first args) env))))
    (type-code 
     (code-type value) 
     (append (code value) (list (list '$mv (list 'quote reg) ''a0))))))</pre>
<h4>Function definition</h4>
<p>The definition of the function being compiled is handled by <strong>comp-defun</strong>:</p>
<pre>(defun comp-defun (name args body env)
  (setq *used-params* (subseq *locals* 0 (length args)))
  (append 
   (list 'defcode name args)
   (list name)
   (apply #'append 
          (mapcar #'(lambda (x y) (list (list '$mv (list 'quote x) (list 'quote y))))
                  *used-params* *params*))
   (code (comp-progn body env t))))</pre>
<h4>Progn special form</h4>
<p>The function <strong>comp-progn</strong> compiles a <strong>progn</strong> form:</p>
<pre>(defun comp-progn (exps env tail)
  (let* ((len (1- (length exps)))
         (nlast (subseq exps 0 len))
         (last1 (nth len exps))
         (start (mappend #'(lambda (x) (append (code (comp x env t)))) nlast))
         (end (comp last1 env tail)))
    (type-code (code-type end) (append start (code end)))))</pre>
<p>It compiles code to evaluate each expression in the body of the <strong>progn</strong>, discarding all but the last results, and returns the type of the last form as the type of the whole block.</p>
<h4>If special form</h4>
<p>The function <strong>comp-if</strong> compiles the code for an <strong>if</strong> special form:</p>
<pre>(defun comp-if (pred then else env tail)
  (let ((lab1 (gen-label))
        (lab2 (gen-label))
        (test (comp pred env nil)))
    (checktype 'if (car test) :boolean)
    (type-code :integer
               (append
                (code test) (list (list '$beqz ''a0 lab1))
                (code (comp then env t)) (list (list '$j lab2) lab1)
                (code (comp else env tail)) (list lab2)
                (when tail '(($ret)))))))</pre>
<h4>Function calls</h4>
<p>Finally, <strong>comp-funcall</strong> compiles code for function calls to the built-in functions, or a recursive call to the main function:</p>
<pre>(defun comp-funcall (f args env tail)
  (let ((test (assoc f '((&lt; . $slt) (&gt; . $sgt))))
        (teste (assoc f '((= . $seqz) (/= . $snez))))
        (testn (assoc f '((&gt;= . $slt) (&lt;= . $sgt))))
        (logical (assoc f '((and . $and) (or . $or))))
        (arith1 (assoc f '((1+ . 1) (1- . -1))))
        (arith (assoc f '((+ . $add) (- . $sub) (* . $mul) (/ . $div) (mod . $rem)))))
    (cond
     ((or test teste testn)
      (type-code :boolean
                   (append
                    (comp-args f args 2 :integer env)
                    (pop-regs 'a1)
                    (cond
                     (test (list (list (cdr test) ''a0 ''a1 ''a0)))
                     (teste (list '($sub 'a0 'a1 'a0) (list (cdr teste) ''a0 ''a0)))
                     (testn (list (list (cdr testn) ''a0 ''a1 ''a0) '($xori 'a0 'a0 1))))
                    (when tail '(($ret))))))
     (logical 
      (type-code :boolean
                 (append
                  (comp-args f args 2 :boolean env)
                  (pop-regs 'a1)
                  (list (list (cdr logical) ''a0 ''a0 ''a1))
                  (when tail '(($ret))))))
     (arith1
      (type-code :integer
                 (append
                  (comp-args f args 1 :integer env)
                  (list (list '$addi ''a0 ''a0 (cdr arith1)))
                  (when tail '(($ret))))))
     (arith
      (type-code :integer 
                 (append
                  (comp-args f args 2 :integer env)
                  (pop-regs 'a1)
                  (list (list (cdr arith) ''a0 ''a1 ''a0))
                  (when tail '(($ret))))))
     ((member f '(car cdr))
      (type-code :integer
                 (append
                  (comp-args f args 1 :integer env)
                  (if (eq f 'cdr) (list '($lw 'a0 4 '(a0)))
                    (list '($lw 'a0 0 '(a0)) '($lw 'a0 4 '(a0))))
                  (when tail '(($ret))))))
     (t ; function call
      (type-code :integer 
                 (append
                  (comp-args f args nil :integer env)
                  (when (&gt; (length args) 1)
                    (append
                     (list (list '$mv (list 'quote (nth (1- (length args)) *params*)) ''a0))
                     (apply #'pop-regs (subseq *params* 0 (1- (length args))))))
                  (cond
                   (tail (list (list '$j f)))
                   (t (append
                       (apply #'push-regs (cons 'ra (reverse *used-params*)))
                       (list (list '$jal f))
                       (apply 'pop-regs (append *used-params* (list 'ra))))))))))))</pre>
<p>The arithmetic comparisons take advantage of the RISC-V instructions such as <strong>slt</strong>&nbsp;(Set if less than), which set the destination register to 0 if the comparison is false, and to 1 if it's true; this provides the required boolean result without needing a branch.</p>
<p>The function <strong>comp-funcall</strong> uses the routine&nbsp;<strong>comp-args</strong>&nbsp;to generate code to compile each of the arguments to a function:</p>
<pre>(defun comp-args (fn args n type env)
  (unless (or (null n) (= (length args) n))
    (error "Incorrect number of arguments to '~a'" fn))
  (let ((n (length args)))
    (mappend #'(lambda (y)
                 (let ((c (comp y env nil)))
                   (decf n)
                   (checktype fn type (code-type c))
                   (if (zerop n) (code c) (append (code c) (push-regs 'a0)))))
             args)))</pre>
<h3>Appendix</h3>
<p>The following example shows the code generated by a simple function,&nbsp;<strong>rec</strong>, a recursive function related to the factorial function:</p>
<pre>(defun rec (n)
  (1+ (* n (if (= n 0) 0 (rec (1- n))))))</pre>
<p>Compiling this gives the following RISC-V machine code:</p>
<pre>&gt; (compiler 'rec)
0000      rec
0000 872a ($mv 'a4 'a0)
0002 853a ($mv 'a0 'a4)
0004 1171 ($addi 'sp 'sp -4)
0006 c02a ($sw 'a0 0 '(sp))
0008 853a ($mv 'a0 'a4)
000a 1171 ($addi 'sp 'sp -4)
000c c02a ($sw 'a0 0 '(sp))
000e 4501 ($li 'a0 0)
0010 4582 ($lw 'a1 0 '(sp))
0012 0111 ($addi 'sp 'sp 4)
0014 8533 ($sub 'a0 'a1 'a0)
0016 40a5 
0018 3513 ($seqz 'a0 'a0)
001a 0015 
001c c119 ($beqz 'a0 lab1)
001e 4501 ($li 'a0 0)
0020 a819 ($j lab2)
0022      lab1
0022 853a ($mv 'a0 'a4)
0024 157d ($addi 'a0 'a0 -1)
0026 1161 ($addi 'sp 'sp -8)
0028 c006 ($sw 'ra 0 '(sp))
002a c23a ($sw 'a4 4 '(sp))
002c f0ef ($jal rec)
002e fd5f 
0030 4712 ($lw 'a4 4 '(sp))
0032 4082 ($lw 'ra 0 '(sp))
0034 0121 ($addi 'sp 'sp 8)
0036      lab2
0036 4582 ($lw 'a1 0 '(sp))
0038 0111 ($addi 'sp 'sp 4)
003a 8533 ($mul 'a0 'a1 'a0)
003c 02a5 
003e 0505 ($addi 'a0 'a0 1)
0040 8082 ($ret)</pre>
<p>Trying it out:</p>
<pre>&gt; (rec 12)
1302061345</pre>
<p>This example demonstrates how the RISC-V Lisp assembler takes advantage of 16-bit compressed instructions where possible, instead of the equivalent full 32-bit instructions.</p><hr>
<ol>
<li id="cite_note1"><a href="#cite_ref1">^</a> Norvig, Peter "Paradigms of Artificial Intelligence Programming" Morgan Kaufmann Publishers, Inc, San Francisco, 1992, pp 784-833, available as a PDF <a href="https://github.com/norvig/paip-lisp" target="_blank">paip-lisp</a> on GitHub.</li>
<li id="cite_note2"><a href="#cite_ref2">^</a> <a href="http://www.ulisp.com/show?1EO1#tak">Benchmarks - Takeuchi function</a>.</li>
<li id="cite_note3"><a href="#cite_ref3">^</a> <a href="http://www.ulisp.com/show?1EO1#q2">Benchmarks - Two-dimensional recursive function Q2</a>.</li>
</ol>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding the Limitations of Mathematical Reasoning in Large Language Models (154 pts)]]></title>
            <link>https://arxiv.org/abs/2410.05229</link>
            <guid>41808683</guid>
            <pubDate>Fri, 11 Oct 2024 11:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.05229">https://arxiv.org/abs/2410.05229</a>, See on <a href="https://news.ycombinator.com/item?id=41808683">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.05229">View PDF</a>
    <a href="https://arxiv.org/html/2410.05229v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of <a href="http://models.our/" rel="external noopener nofollow">this http URL</a> findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Seyed Iman Mirzadeh [<a href="https://arxiv.org/show-email/d6cde35e/2410.05229">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 7 Oct 2024 17:36:37 UTC (5,949 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Initial CUDA Performance Lessons (144 pts)]]></title>
            <link>https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/</link>
            <guid>41808013</guid>
            <pubDate>Fri, 11 Oct 2024 10:01:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/">https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/</a>, See on <a href="https://news.ycombinator.com/item?id=41808013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>I am somehow very late to learning CUDA. I didn’t even know until recently that CUDA is just C++ with a small amount of extra stuff. If I had known that there is so little friction to learning it, I would have checked it out much earlier. But if you come in with C++ habits, you’ll write suboptimal code, so here are some lessons I had to learn to get things to run fast.</p>



<h2>Memory Coalescing</h2>



<p>If you have multiple threads operating on an array in C++, you probably want to iterate like this:</p>


<div><pre title="">std::vector&lt;T&gt; vec = ...;
size_t per_thread = vec.size() / num_threads;
T * my_slice = vec.data() + per_thread * my_thread_i;
for (size_t i = 0; i &lt; per_thread; ++i) {
    do_something(my_slice[i]);
}
</pre></div>


<p>Meaning each thread iterates over a contiguous chunk of memory. In CUDA this is going to be slow because you want the threads to load memory together. So if thread 0 loads bytes 0 to 15, then you want thread 1 to load bytes 16 to 31 and thread 2 to load bytes 32 to 47 etc. So the loop instead has to look like this:</p>


<div><pre title="">T * data = ...;
size_t num_elements = ...;
for (int i = my_thread_i; i &lt; num_elements; i += num_threads) {
    do_something(data[i]);
}
</pre></div>


<p>This is called “memory coalescing” where adjacent threads use adjacent memory. On a loop with a small body (dot product) this is 3x faster.</p>



<h2>Most of the Performance is now in Specialized Hardware</h2>



<p>Many years ago Sean Parent presented a graph that breaks down where the performance is in a modern PC. I’m reproducing it with current numbers here:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png"><img data-attachment-id="11747" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/pc_performance-2/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png" data-orig-size="1480,290" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pc_performance" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png?w=650" tabindex="0" role="button" width="1480" height="290" src="https://probablydance.com/wp-content/uploads/2024/10/pc_performance-2.png" alt=""></a></figure>



<p>What we see here is the breakdown of theoretical performance in a PC with a Ryzen 9950X and a RTX 4090. The overall theoretical performance is ~95 TFLOPS. These are theoretical, so for example the single-threaded CPU performance is just “5.7 Ghz * 4 instructions per cycle = 22.8 GFLOPS”. That’s the blue line that you can’t see because it’s such a tiny fraction. If you use all 32 threads and AVX 512 you can multiply that performance by 32*16 = 512 to fill up the red and yellow parts of the graph. But if you really want performance, you need to use the GPU which gives you the green part of the graph.</p>



<p>But while these are current numbers, it’s missing most of the GPU performance. The GPU now has specialized hardware for machine learning and for raytracing. When you add those to the graph you get current performance.</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png"><img data-attachment-id="11750" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/pc_performance_with_specialized_hardware-2/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png" data-orig-size="1472,340" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pc_performance_with_specialized_hardware" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png?w=650" tabindex="0" role="button" width="1472" height="340" src="https://probablydance.com/wp-content/uploads/2024/10/pc_performance_with_specialized_hardware-1.png" alt=""></a></figure>



<p>This is the same graph plus specialized hardware. For the tensor core I chose the TFLOPS when doing BF16 matrix multiplies. Meaning it’s not exactly a fair comparison because it operates on lower precision (the output is in 32 bits though) but everyone uses this for matrix multiplies and thinks it’s fine.</p>



<p>The point is that now most of the performance in your PC is in specialized chips. If you’re just writing straightforward CUDA code, you’re leaving most of the performance on the table. The graph gets even more lopsided when looking at a deep learning GPU like the H100:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png"><img data-attachment-id="11755" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/h100_performance/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png" data-orig-size="1440,347" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="h100_performance" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png?w=650" tabindex="0" role="button" width="1440" height="347" src="https://probablydance.com/wp-content/uploads/2024/10/h100_performance.png" alt=""></a></figure>



<p>Note how the x-axis now goes above 2000 TFLOPS. If you’re not using tensor cores, the GPU is sitting &gt;90% idle. This is changing the algorithms that are used in deep learning. If algorithm A can just do bigger matrix multiplications to get higher quality results, and algorithm B can achieve better quality results by cleverly doing lots of little pieces of work, people will choose algorithm A.</p>



<h2>Different Kinds of Memory</h2>



<p>Memory is more complicated in CUDA, but with my limited understanding so far I think of CUDA as having three different types of memory:</p>



<ol>
<li>Normal memory</li>



<li>Shared memory (faster)</li>



<li>Registers (fastest)</li>
</ol>



<p>Registers are particularly weird. One thread block has 65536 registers, meaning you can store 256k bytes of data in registers. Which is more than you can store in shared memory. I was trying to understand how some cuDNN kernel could possibly be as fast as it was, when I realized that they keep a particular matrix entirely in registers where each thread holds a small part of the matrix.</p>



<p>You get some control over how many registers you have. You can have up to 1024 threads per thread block, meaning you get 64 registers per thread by default. But you could launch fewer threads and get proportionally more registers per thread. If you need, say 150 registers because you want to cache some data, you divide 65536/150 which tells you that you can use 436 threads.</p>



<p>But you’re still just writing in C++ which doesn’t make it easy to say “keep this data in registers.” The best way I found to do this is to keep a fixed-size array on the stack and then use “#pragma unroll” in every single loop that uses that array. The loop needs to be unrolled because every unrolled iteration of the loop needs to refer to different registers.</p>



<p>Shared memory was straightforward in comparison. It allows you to dedicate some cache space for a specific purpose, and the data is shared between threads. So you can use it for two purposes:</p>



<ol>
<li>To communicate between threads</li>



<li>To load data more quickly: If you want to load 512 floats and you have 512 threads, every thread can load one float into shared memory. So you don’t even have to loop.</li>
</ol>



<h2>Sharing is ~Free Within a Warp</h2>



<p>This one was a delight when I saw code doing this for the first time: A warp is 32 threads that share one instruction pointer. They all do the same thing at the same time. So if you e.g. parallelize a dot product, the 32 threads of the warp can sum their results to get the overall result in five steps, using a parallel sum algorithm:</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png"><img data-attachment-id="11736" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/parallel_sum/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png" data-orig-size="2298,804" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="parallel_sum" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png?w=650" tabindex="0" role="button" loading="lazy" width="2298" height="804" src="https://probablydance.com/wp-content/uploads/2024/09/parallel_sum.png" alt=""></a></figure>



<p>On a CPU this algorithm is impractical because the overhead of keeping the threads in sync is too high. But on a GPU they just are in sync, so sharing is literally five steps:</p>


<div><pre title="">__device__ float add_warp(float x) {
    static constexpr const unsigned all = 0xffffffff;
    x += __shfl_xor_sync(all, x, 1);
    x += __shfl_xor_sync(all, x, 2);
    x += __shfl_xor_sync(all, x, 4);
    x += __shfl_xor_sync(all, x, 8);
    x += __shfl_xor_sync(all, x, 16);
    return x;
}
</pre></div>


<p>I verified that this compiles down to two instructions each. This compiles to 5 SHFL.BFLY instructions plus 5 FADD instructions for the addition. There are no secret locks or barriers here.</p>



<p>This only works within a warp (32 threads). For a thread block, up to 1024 threads, you can use shared memory, which requires using barriers because the threads won’t automatically be in sync. If you need more threads than that and want to share data between them, don’t. (you’ll often want many more threads, you just can’t share data. You need to write out the result to memory and then launch a new thread to work on the new data)</p>



<h2>Parallelism First</h2>



<p>My intuition for how many threads to use was wrong by a lot. If you’re iterating over some data and have to do several non-trivial things to it, it’s probably best to launch one thread for each of the things you want to do. It’s tempting to say “this thread already loaded all the relevant data, it can just do a bit of extra work” but in CUDA it’s better to launch a separate thread for that extra work, even if they both have to load the same data. It’s much cheaper for them to synchronize and share their data than it would be on a CPU.</p>



<p>When I ran Nsight Compute on the first couple versions of my code, the feedback that came back could always be summarized as “you’re barely using the GPU, make it more parallel.”</p>



<p>This also means that you often want to pull your algorithm apart. If there is one part that can run massively parallel (across tens of thousands of threads) and one part that has limited parallelism (say only a few hundred threads) then it’s probably worth to launch those as separate kernels to benefit from the massive parallelism on part of your problem, even if that part is only a small part.</p>



<p>So whenever you try to solve a problem, the first question should not be “how can I make this fast?” but “how can I run this in parallel?” After you solve that, worry about making the parallel code fast.</p>



<h2>Conclusion</h2>



<p>Writing CUDA definitely has a different feeling. It feels more puzzly because it’s so easy to accidentally only use 1% of your GPU. It actually reminds me of TIS-100, especially the trick of distributing data in the registers of multiple threads. But instead of managing a small number of chips you have to figure out how to generate work for tens of thousands of threads. My mental model is that you’ve got a bunch of container ships that can travel at 10% of the speed of light. You’re using them to ship goods around the world. They’re very fast so most of the work is in setting up your harbors so that you can load and unload these container-ships in fractions of a second so that it can sail to do the next thing. It’s not easy to feed these beasts, but if you do it right you can do huge chunks of work in almost no time.</p>



<figure><a href="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png"><img data-attachment-id="11765" data-permalink="https://probablydance.com/2024/10/07/initial-cuda-performance-lessons/comfyui_00091_/" data-orig-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png" data-orig-size="1024,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ComfyUI_00091_" data-image-description="" data-image-caption="" data-medium-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png?w=300" data-large-file="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png?w=650" tabindex="0" role="button" loading="lazy" width="1024" height="1024" src="https://probablydance.com/wp-content/uploads/2024/10/comfyui_00091_.png" alt=""></a></figure>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nobel Peace Prize for 2024 awarded to Nihon Hidankyo (233 pts)]]></title>
            <link>https://www.nobelprize.org/press-release-peace-2024/</link>
            <guid>41807681</guid>
            <pubDate>Fri, 11 Oct 2024 09:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/press-release-peace-2024/">https://www.nobelprize.org/press-release-peace-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=41807681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
<section>

<article>
<header>
<h2>
Press release </h2>
</header>

<h5><b>English</b><br><a href="https://www.nobelprize.org/prizes/peace/2024/222035-press-release-norwegian/">Norwegian</a></h5>
<figure><img decoding="async" width="226" height="121" src="https://www.nobelprize.org/uploads/2022/10/nobel-peace-prize-logo.jpg" alt="The Nobel Peace Prize logo"></figure>
<h2>Announcement</h2>
<p><a href="https://www.nobelpeaceprize.org/" target="_blank" rel="noreferrer noopener">The Norwegian Nobel Committee</a> has decided to award the Nobel Peace Prize for 2024 to the Japanese organisation Nihon Hidankyo. This grassroots movement of atomic bomb survivors from Hiroshima and Nagasaki, also known as Hibakusha, is receiving the Peace Prize for its efforts to achieve a world free of nuclear weapons and for demonstrating through witness testimony that nuclear weapons must never be used again.</p>
<p>In response to the atomic bomb attacks of August 1945, a global movement arose whose members have worked tirelessly to raise awareness about the catastrophic humanitarian consequences of using nuclear weapons. Gradually, a powerful international norm developed, stigmatising the use of nuclear weapons as morally unacceptable. This norm has become known as “the nuclear taboo”.</p>
<p>The testimony of the Hibakusha – the survivors of Hiroshima and Nagasaki – is unique in this larger context.</p>
<p>These historical witnesses have helped to generate and consolidate widespread opposition to nuclear weapons around the world by drawing on personal stories, creating educational campaigns based on their own experience, and issuing urgent warnings against the spread and use of nuclear weapons. The Hibakusha help us to describe the indescribable, to think the unthinkable, and to somehow grasp the incomprehensible pain and suffering caused by nuclear weapons.</p>
<p>The Norwegian Nobel Committee wishes nevertheless to acknowledge one encouraging fact: No nuclear weapon has been used in war in nearly 80 years. The extraordinary efforts of Nihon Hidankyo and other representatives of the Hibakusha have contributed greatly to the establishment of the nuclear taboo. It is therefore alarming that today this taboo against the use of nuclear weapons is under pressure.</p>
<p>The nuclear powers are modernising and upgrading their arsenals; new countries appear to be preparing to acquire nuclear weapons; and threats are being made to use nuclear weapons in ongoing warfare. At this moment in human history, it is worth reminding ourselves what nuclear weapons are: the most destructive weapons the world has ever seen.</p>
<p>Next year will mark 80 years since two American atomic bombs killed an estimated 120&nbsp;000 inhabitants of Hiroshima and Nagasaki. A comparable number died of burn and radiation injuries in the months and years that followed. Today’s nuclear weapons have far greater destructive power. They can kill millions and would impact the climate catastrophically. A nuclear war could destroy our civilisation.</p>
<p>The fates of those who survived the infernos of Hiroshima and Nagasaki were long concealed and neglected. In 1956, local Hibakusha associations along with victims of nuclear weapons tests in the Pacific formed the Japan Confederation of A- and H-Bomb Sufferers Organisations. This name was shortened in Japanese to Nihon Hidankyo. It would become the largest and most influential Hibakusha organisation in Japan.</p>
<p>The core of Alfred Nobel’s vision was the belief that committed individuals can make a difference. In awarding this year’s Nobel Peace Prize to Nihon Hidankyo, the Norwegian Nobel Committee wishes to honour all survivors who, despite physical suffering and painful memories, have chosen to use their costly experience to cultivate hope and engagement for peace.</p>
<p>Nihon Hidankyo has provided thousands of witness accounts, issued resolutions and public appeals, and sent annual delegations to the United Nations and a variety of peace conferences to remind the world of the pressing need for nuclear disarmament.</p>
<p>One day, the Hibakusha will no longer be among us as witnesses to history. But with a strong culture of remembrance and continued commitment, new generations in Japan are carrying forward the experience and the message of the witnesses. They are inspiring and educating people around the world. In this way they are helping to maintain the nuclear taboo – a precondition of a peaceful future for humanity.</p>
<p>The decision to award the Nobel Peace Prize for 2024 to Nihon Hidankyo is securely anchored in Alfred Nobel’s will. This year’s prize joins a distinguished list of Peace Prizes that the Committee has previously awarded to champions of nuclear disarmament and arms control.</p>
<p>The Nobel Peace Prize for 2024 fulfils Alfred Nobel’s desire to recognise efforts of the greatest benefit to humankind.</p>
<p>Oslo, 11 October 2024</p>

<div>
<p><a href="#content">
Back to top </a></p><svg width="18px" height="15px" viewBox="0 0 20 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" role="image" aria-labelledby="back-to-top-title  back-to-top-desc">
<title id="back-to-top-title">Back To Top</title>
<desc id="back-to-top-desc">Takes users back to the top of the page</desc>
<g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g transform="translate(-474.000000, -9998.000000)" fill="#2E2A25">
<g transform="translate(474.000000, 9998.000000)">
<g transform="translate(10.000000, 10.000000) rotate(45.000000) translate(-10.000000, -10.000000) translate(3.000000, 3.000000)">
<rect x="0" y="0" width="2" height="14"></rect>
<rect x="0" y="0" width="14" height="2"></rect>
</g>
<rect x="9" y="3" width="2" height="14"></rect>
</g>
</g>
</g>
</svg>
</div>
</article>

</section>
<section>

<article>
<div>
<header>
<p>
<h2>
<a href="https://www.nobelprize.org/">
Coming up </a>
</h2>
</p>
</header>
<div><p>
Don't miss the Nobel Prize announcements 7-14 October!</p><p>Watch the live stream of the announcements. </p></div>
</div>
<figure>
<a href="https://www.nobelprize.org/"><picture><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-992x656.jpg" media="(min-width: 220px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1520x1008.jpg" media="(min-width: 900px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live.jpg" media="(min-width: 1400px)"><img src="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1024x676.jpg" alt="Illustration" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></picture></a> </figure>
</article>
</section>
<section>

<form id="67090ae8419c5" method="GET" action="">
<p><label for="mobile-dropdown">
Select the category or categories you would like to filter by </label>

</p>
<div>
<p><label>Select the category or categories you would like to filter by</label></p><p><label for="physics">

<span>
Physics </span>
</label>
</p>
<p><label for="chemistry">

<span>
Chemistry </span>
</label>
</p>
<p><label for="medicine">

<span>
Medicine </span>
</label>
</p>
<p><label for="literature">

<span>
Literature </span>
</label>
</p>
<p><label for="peace">

<span>
Peace </span>
</label>
</p>
<p><label for="economic-sciences">

<span>
Economic Sciences </span>
</label>
</p>
</div>
<p><label for="increment-down">
Decrease the year by one </label>

<label for="increment-input">
Choose a year you would like to search in </label>

<label for="increment-up">
Increase the year by one </label>

</p>

</form>
</section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nurdle Patrol (123 pts)]]></title>
            <link>https://www.nurdlepatrol.org/app/</link>
            <guid>41806629</guid>
            <pubDate>Fri, 11 Oct 2024 06:00:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nurdlepatrol.org/app/">https://www.nurdlepatrol.org/app/</a>, See on <a href="https://news.ycombinator.com/item?id=41806629">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla Robotaxi (256 pts)]]></title>
            <link>https://www.tesla.com/we-robot</link>
            <guid>41805706</guid>
            <pubDate>Fri, 11 Oct 2024 03:24:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tesla.com/we-robot">https://www.tesla.com/we-robot</a>, See on <a href="https://news.ycombinator.com/item?id=41805706">Hacker News</a></p>
Couldn't get https://www.tesla.com/we-robot: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[$2 H100s: How the GPU Rental Bubble Burst (365 pts)]]></title>
            <link>https://www.latent.space/p/gpu-bubble</link>
            <guid>41805446</guid>
            <pubDate>Fri, 11 Oct 2024 02:19:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latent.space/p/gpu-bubble">https://www.latent.space/p/gpu-bubble</a>, See on <a href="https://news.ycombinator.com/item?id=41805446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><strong>Swyx’s note:</strong><span> we’re on a roll catching up with former guests! Apart from our recent guest spot on </span><a href="https://www.listennotes.com/podcasts/high-agency-the/why-your-ai-product-needs-ALy02ewNtDC/" rel="">Raza Habib’s chat with Hamel Husain</a><span> (see </span><a href="https://www.latent.space/p/humanloop" rel="">Raza’s first pod here</a><span>). </span></em></p><p><em><span>We’re delighted to welcome Eugene Cheah (see </span><a href="https://www.latent.space/p/rwkv" rel="">his first pod on RWKV last year</a><span>) as a rare guest </span><strong>writer </strong><span>for our newsletter</span><strong>.</strong><span> Eugene has now cofounded </span><a href="https://featherless.ai/" rel="">Featherless.AI</a><span>, an inference platform with the world’s largest collection of open source models (~2,000) instantly accessible via a single API for a </span><strong>flat rate</strong><span> ($10-$75+ a month).</span></em></p><p><em><span>Recently there has been a lot of excitement with NVIDIA’s new Blackwell series rolling out to OpenAI, with the company saying it is </span><a href="https://x.com/firstadopter/status/1844417947277852925" rel="">sold out for the next year</a><span> and Jensen noting that it could be the “</span><a href="https://x.com/The_AI_Investor/status/1844080690046058843" rel="">most successful product in the history of the industry</a><span>”. With cousin Lisa hot on his heels </span><a href="https://www.cnbc.com/2024/10/10/amd-launches-mi325x-ai-chip-to-rival-nvidias-blackwell-.html" rel="">announcing the MI3 25 X</a><span> and </span><a href="https://news.ycombinator.com/item?id=41702789" rel="">Cerebras filing for IPO</a><span>, it is time to dive deep on the GPU market again (see also </span><a href="https://www.latent.space/p/semianalysis" rel="">former guest</a><span> </span><a href="https://www.dwarkeshpatel.com/p/dylan-jon" rel="">Dylan Patel’s pod</a><span> for his trademark candid take on the industry of course): </span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png" width="421" height="497.04655493482306" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1268,&quot;width&quot;:1074,&quot;resizeWidth&quot;:421,&quot;bytes&quot;:1493562,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f05a21e-05b3-4691-a2bb-44547e0acd7e_1074x1268.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em><span>Do we yet have an answer to </span><a href="https://www.latent.space/p/mar-jun-2024" rel="">the $600bn question</a><span>? It is now consensus that the capex on foundation model training is the “</span><a href="https://x.com/GavinSBaker/status/1720819375517716610" rel="">fastest depreciating asset in history</a><span>”, but the jury on GPU infra spend is still out and </span><a href="https://www.latent.space/i/140396949/mixtral-sparks-a-gpuinference-race-to-the-bottom" rel="">the GPU Rich Wars are raging</a><span>.</span></em></p><p><em><span>What follows is Eugene’s take on GPU economics as he is now an inference provider, diving deep on the H100 market, as a possible read for what is to come for the Blackwell generation. Not financial advice! We also recommend </span><a href="https://blog.lepton.ai/the-missing-guide-to-the-h100-gpu-market-91ebfed34516" rel="">Yangqing Jia’s guide</a><span>.</span></em></p><p><em><strong>TLDR: Don’t buy H100s. The market has flipped from shortage ($8/hr) to oversupplied ($2/hr), because of reserved compute resales, open model finetuning, and decline in new foundation model co’s. Rent instead.</strong><p><span>(Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers, or you have billions and need a super large cluster for frontier model training)</span></p><p><span>For the general market, it makes little sense to be investing in new H100s today, when </span><strong>you can rent it at near cost, when you need it</strong><span>, with the current oversupply.</span></p></em></p><p><span>ChatGPT was launched in November 2022, built on the A100 series. The H100s arrived in March 2023. </span><strong>The pitch to investors and founders was simple: </strong><span>Compared to A100s, </span><strong>the new H100s were 3x more powerful, but only 2x the sticker price</strong><span>.</span></p><p>If you were faster to ramp up on H100s, you too, can build a bigger, better model, and maybe even leapfrog OpenAI to Artificial General Intelligence - If you have the capital to match their wallet! </p><p>With this desire, $10-100’s billions of dollars were invested into GPU-rich AI startups to build this next revolution. Which lead to ….</p><p><strong>The sudden surge in H100 demand</strong></p><p><span>Market prices shot through the roof, the original rental rates of H100 started at approximately </span><em><strong>$4.70 an hour</strong></em><span> but were going for </span><em><strong>over $8</strong></em><span>. For all the desperate founders rushing to train their models to convince their investors for their next $100 million round.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png" width="404" height="227.52747252747253" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:404,&quot;bytes&quot;:265694,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d1b8c6-8c88-4579-a65d-3ce677c98d88_2798x1576.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Nvidia, literally pitched to their investors &amp; datacenter customers, in their 2023 investor presentation - the “market opportunity” on renting H100s at $4/hr</figcaption></figure></div><p><span>For GPU farms, it felt like free money - if you can get these founders to rent your H100 SXMGPUs at $4.70 an hour or more, or even get them to pay it upfront, </span><strong>the payback period was &lt;1.5 years</strong><span>. From then on, it was free-flowing cash of over $100k per GPU, per year.</span></p><p>With no end to the GPU demand in sight, their investors agreed, with even larger investments…</p><p><span>Physical goods, unlike digital goods, suffer from lag time. Especially when there are </span><a href="https://www.ft.com/content/c7e9cfa9-3f68-47d3-92fc-7cf85bcb73b3" rel="">multiple shipment delays</a><span>.</span></p><p>For most of 2023, the H100 prices felt like they would forever be above $4.70 (unless you were willing to do a huge upfront downpayment)</p><p>At the start of 2024, the H100 prices reached approximately $2.85 across multiple providers.</p><p>As more providers come online, however… I started to get emails like this:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png" width="1456" height="825" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:825,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:413942,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14ef9576-25d7-4b63-b70b-57c442789fe3_2202x1248.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>While I have not been successful with acquiring H100 nodes (8xH100) at $4/hour, I have confirmed multiple times, that you can do so at $8 - $16/hour</figcaption></figure></div><p>In Aug 2024, if you're willing to auction for a small slice of H100 time (days to weeks), you can start finding H100 GPUs for $1 to $2 an hour.</p><p><strong>We are looking at a &gt;= 40% price drop per year</strong><span>, especially for small clusters. NVIDIA’s marketing projection of $4 per GPU hour across 4 years, has evaporated away in under 1.5 years.</span></p><p><span>And that is horrifying because it means someone out there is potentially </span><a href="https://en.wikipedia.org/wiki/Bagholder" rel="">left holding the bag</a><span> - especially so if they just bought it as a new GPUs. So what is going on?</span></p><blockquote><p><em>This will be focusing on the economical cost, and the ROI on leasing, against various market rates. Not the opportunity cost, or buisness value.</em></p></blockquote><p>The average H100 SXM GPU in a data center costs $50k or more to set up, maintain, and operate (aka most of the CAPEX). Excluding electricity and cooling OPEX cost. More details on the calculation are provided later in this article.</p><p><span>But what does that mean for unit economics today, as an investment?</span><br><span>Especially if we assume a 5-year lifespan on the GPUs itself today.</span></p><p>Generally, there are two business models for leasing H100, which we would cover.</p><ul><li><p>Short on-demand leases (by the hour - by the week - or the month)</p></li><li><p>Longterm reservation (3-5 years)</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png" width="1456" height="765" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:765,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:618581,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7815afb5-d4f8-4444-804d-5fc96fa094c6_1884x990.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>In summary, for an on-demand workload</strong></p><ul><li><p><strong>&gt;$2.85</strong><span> : Beat stock market IRR</span></p></li><li><p><strong>&lt;$2.85</strong><span> : Loses to stock market IRR</span></p></li><li><p><strong>&lt;$1.65</strong><span> : Expect loss in investment</span></p></li></ul><p>For the above ROI and revenue forecast projection, we introduced “blended price”, where we assume a gradual drop to 50% in the rental price across 5 years.</p><p>This is arguably a conservative/optimistic estimate given the &gt;= 40% price drop per year we see now. But it’s a means of projecting an ROI while taking into account a certain % of price drop.</p><p>At $4.50/hour, even when blended, we get to see the original pitch for data center providers from NVIDIA, where they practically print money after 2 years. Giving an IRR (Internal rate of return) of 20+%.</p><p>However, at $2.85/hour, this is where it starts to be barely above 10% IRR.</p><p>Meaning, if you are buying a new H100 server today, and if the market price is less than $2.85/hour, you can barely beat the market, assuming 100% allocation (which is an unreasonable assumption). Anything, below that price, and you're better off with the stock market, instead of a H100 infrastructure company, as an investment.</p><p><strong>And if the price falls below $1.65/hour, you are doomed to make losses on the H100 over the 5 years, as an infra provider</strong><span>. Especially, if you just bought the nodes and cluster this year.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png" width="1456" height="760" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:642212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb3453721-463c-4de2-9bda-cd2fa3133ada_1884x984.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Many infrastructure providers, especially the older ones - were not naive about this - Because they had been burnt firsthand by GPU massive rental price drops, after a major price pump, from the crypto days - they had seen this cycle before.</p><p><strong>So for this cycle, last year, they pushed heavily for a 3-5 year upfront commitment and/or payment at the $4+ price range. </strong><span>(typically with 50% to 100% upfront)</span><strong>. </strong><span>Today, they push the $2.85+ price range - locking in their profits.</span></p><p>This happened aggressively during the 2023 AI peak with various foundation model companies, especially in the image generation space, indirectly forced into high-priced 3-5 year contracts, just so to get to the front-of-the-line of a new cluster, and be first to make their target model, to help close the next round.</p><p>It may not be the most economical move, but it lets them move faster than the competition.</p><p>This, however, has led to some interesting market dynamics - if you are paying $3 or $4 per hour for your H100, for the next 3 years, locked into a contract.</p><p><span>When a model creator is done training a model, you have no more use for the cluster. </span><strong>What would they do? - they resell and start recouping some of the costs.</strong></p><p>From hardware to AI inference / finetune, it can be broadly viewed as the following</p><ul><li><p>Hardware vendors partnered with Nvidia (one-time purchase cost)</p></li><li><p>Datacenter Infrastructure providers &amp; partners (selling long-term reservations, on facility space and/or H100 nodes)</p></li><li><p><span>VC Funds, Large Companies, and Startups: that plann</span><em>ed</em><span> to build foundation models (or have already finished building their models)</span></p></li><li><p><strong>Resellers of capacity: Runpod, SFCompute, Together.ai, Vast.ai, GPUlist.ai</strong></p></li><li><p>Managed AI Inference / Finetune providers: who use a combination of the above</p></li></ul><p><span>While any layer down the stack may be vertically integrated (skipping the infra players for example), the key drivers here are the </span><strong>“Resellers of unused capacity” </strong><span>and the rise of “good enough” open weights models like </span><a href="https://www.latent.space/p/llama-3" rel="">Llama 3</a><span>, as they are all major influencing factors in the current H100 economical pressures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png" width="1456" height="871" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:871,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:932212,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F333cc1d8-8e8b-47dc-b9f5-1a02e0eefd7b_2140x1280.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em><strong><span>The rise of open weights models, on-par with closed-source models.</span><br><span>Is resulting in a fundamental shift in the market</span></strong></em></p><blockquote><p><em><strong>↑↑ Increased demand for AI inference &amp; fine-tuning</strong><p><span>Because many “open” models, lack proper “open source” licenses, but are being distributed freely, and used widely, even commercially. We will refer to them collectively as “open-weights” or “open” models instead here.</span></p></em></p></blockquote><p>In general, with multiple open-weights models of various sizes being built, so has the growth in demand for inference and fine-tuning them. This is largely driven by two major events</p><ul><li><p>The arrival of GPT4 class open models (eg. 405B LLaMA3, DeepSeek-v2)</p></li><li><p>The maturity and adoption of small (~8B) and medium (~70B) fine-tuned models</p></li></ul><p>Today, for the vast majority of use cases, enterprises may need, there are already off-the-shelf open-weights models. Which might be a small step behind proprietary models in certain benchmarks.</p><p>Provides an advantage with the following</p><ul><li><p><strong>Flexibility</strong><span>: Domain / Task specific finetunes</span></p></li><li><p><strong>Reliability</strong><span>: No more minor model updates, breaking use case (there is currently low community trust that model weights are not quietly changed without notification in public API endpoints, causing inexplicable regressions)</span></p></li><li><p><strong>Security &amp; Privacy</strong><span>: Assurance that their prompts and customer data are safe.</span></p></li></ul><p>All of this leads to the current continuous growth and adoption of open models, with the growth in demand for inference and finetunes.</p><p>But it does cause another problem…</p><blockquote><p><em><strong>↓↓ Shrinking foundation model creator market (Small &amp; Medium)</strong><p><span>We used “model creators” to collectively refer to organization that create models from scratch. For fine-tuners, we refer to them as “model finetuners”</span></p></em></p></blockquote><p>Many enterprises, and multiple small &amp; medium foundation model creator startups - especially those who raised on the pitch of “smaller, specialized domain-specific models”, are groups who had no long-term plans / goals for training large foundation models from scratch ( &gt;= 70B ).</p><p>For both groups, they both came to the realization that it is more economical and effective to fine-tune existing Open Weights models, instead of “training on their own”.</p><p><strong>This ended up creating a triple whammy in reducing the demand for H100s!</strong></p><ol><li><p><strong>Finetuning is significantly cheaper than training from scratch.</strong></p><ol><li><p>Because the demands for fine-tuning are significantly less in compute requirements (typically 4 nodes or less, usually a single node), compared to training from scratch (from 16 nodes, usually more, for 7B and up models).</p></li><li><p>This industry-wide switch essentially killed a large part of smaller cluster demands.</p></li></ol></li><li><p><strong>Scaling back on foundation model investment (at small/mid-tier)</strong></p><ol><li><p>In 2023, there was a huge wave of small and medium foundation models, within the text and image space.</p></li><li><p>Today, however, unless you are absolutely confident you can surpass llama3, or you are bringing something new to the table (eg. new architecture, 100x lower inference, 100+ languages, etc), there are ~no more foundation model cos being founded from scratch.</p></li><li><p>In general, the small &amp; medium, open models created by the bigger players (Facebook, etc), make it hard for smaller players to justify training foundation models - unless they have a strong differentiator to do so (tech or data) - or have plans to scale to larger models.</p></li><li><p>And this has been reflected lately with investors as well, as there has been a sharp decline in new foundation model creators’ funding. With the vast majority of smaller groups having switched over to finetuning. (this sentiment is combined with the recent less than desired exits for multiple companies).</p></li><li><p>Presently today, there is approximately worldwide by my estimate:</p><ul><li><p>&lt;20 Large model creator teams (aka 70B++, may create small models as well)</p></li><li><p>&lt;30 Small / Medium model creator teams (7B - 70B)</p></li></ul></li><li><p>Collectively there are less than &lt;50 teams worldwide who would be in the market for 16 nodes of H100s (or much more), at any point in time, to do foundation model training.</p></li><li><p>There are more than 50 clusters of H100 worldwide with more than 16 nodes.</p></li></ol></li><li><p><strong>Excess capacity from reserved nodes is coming online</strong></p><ol><li><p>For the cluster owners, especially the various foundation model startups and VCs, who made long reservations, in the initial “land grab” of the year 2023.</p></li><li><p><span>With the switch to finetuning, and the very long wait times of the H100’s</span><br><span>(it peaked at &gt;= 6 months), it is very well possible that many of these groups had already made the upfront payment before they made the change, essentially making their prepaid hardware “obsolete on arrival”.</span></p></li><li><p>Alternatively, those who had the hardware arrive on time, to train their first few models, had come to the same realization it would be better to fine-tune their next iteration of models. Instead of building on their own.</p></li><li><p><span>In both cases, they would have unused capacity, which comes online via </span><strong>“Compute Resellers”</strong><span> joining the market supply….</span></p></li></ol></li></ol><p>Another major factor, is how all the major Model Creators, such as Facebook, X.AI, and arguably OpenAI (if you count them as part of Microsoft) are moving away from an existing public provider, and building their own billion-dollar clusters, removing the demand that the existing clusters depend on.</p><p>The move is happening mostly for the following reasons:</p><ul><li><p>Existing ~1k node clusters (which costs &gt;$50M to build), is no longer big enough for them, to train bigger models</p></li><li><p>At a billion-dollar scale, it is better for accounting to purchase assets (of servers, land, etc), which has booked value (part of company valuation and assets), instead of pure expenses leasing.</p></li><li><p>If you do not have the people (they do), you could straight up buy small datacenters companies, who have the expertise to build this for you.</p></li></ul><p>With the demand gradually weaning away in stages. These clusters are coming online to the public cloud market instead.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png" width="1456" height="974" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1268863,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F23dbc794-e567-4b34-a6e7-761fafef2ccc_2710x1812.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Vast.ai essentially does a free market system, where providers from all over the world, are forced to compete with each other</figcaption></figure></div><p>Recall all the H100 large shipment delays in 2023, or 6 months or more? They are coming online, now - along with the H200, B200, etc.</p><p>This is alongside, the various unused compute, coming online (from existing startups, enterprises or VCs as covered earlier).</p><p><span>The bulk of this is done via </span><strong>Compute Resellers</strong><span>, such as : together.ai, sfcompute, runpod, vast.ai, etc</span></p><p>In most cases, cluster owners have a small or medium cluster, (typically 8-64 nodes), that is underutilized. With the money already “spent” for the cluster.</p><p>With the primary goal is to recoup as much of the cost as possible, they rather undercut the market and guarantee an allocation, instead of competing with the main providers, and possibly have no allocation.</p><p>This is typically done either via a fixed rate, an auction system, or just a free market listing, etc. With the later 2 driving the market price downwards.</p><p>Another major factor, is once your outside of the training / fine-tune space. The inference space is filled with alternatives, especially if your running smaller models.</p><p>One do not need to pay for the premium invoked by H100’s Infiniband and/or nvidia.</p><p>H100 premium for training is priced into the hardware. For example nvidia themselves recommend the L40S, which is the more price competitive alternative for inference.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png" width="1456" height="439" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/df705375-9942-4ea5-86b9-b41d3661096b_1842x556.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:439,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:414838,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf705375-9942-4ea5-86b9-b41d3661096b_1842x556.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Which Is 1/3rd the performance, at 1/5th the price. But does not work well with multi-node training. Undercutting their very own H100 for this segment.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png" width="1456" height="461" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:461,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2982780,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55aee33d-e501-4923-9fd2-39740f781ed5_2610x826.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Both AMD and Intel may be late into the game with their MX300, and Gaudi 3 respectively.</p><p>This has been tested and verified by us, having used these systems. They are generally:</p><ul><li><p>Cheaper than a H100 in purchase cost</p></li><li><p>Have more memory and compute than a H100, and outperforms on a single node.</p></li><li><p>Overall, they are great hardware!</p></li></ul><p>The catch? They have minor driver issues in training and are entirely unproven in large multi-node cluster training.</p><p>Which as we covered is largely irrelevant to the current landscape. To anyone but &lt;50 teams. The market for H100 has been moving towards inference and single or small cluster fine-tuning.</p><p>All of which these GPUs have been proven to work at. For the use cases, the vast majority of the market is asking for.</p><p>These 2 competitors are full drop-in replacements. With working off-the-shelf inference code (eg. VLLM) or finetuning code for most common model architectures (primarily LLaMA3, followed by others).</p><p>So, if you have compatibility sorted out. Its highly recommended to have a look.</p><p>With Ethereum moving towards proof of stake, ASIC dominating the bitcoin mining race, and the general crypto market condition.</p><p>GPU usage in mining for crypto has been a downward trend, and in several cases unprofitable. And has since been flooding the GPU public cloud market.</p><p>And while the vast majority of these GPUs are unusable for training, or even for inference, due to hardware constraints (low PCIe bandwidth, network, etc). The hardware has been flooding the market and has been repurposed for AI inference workloads.</p><p>In most cases if you are under &lt;10B, you can get decent performance with these GPUs, out of the box, for really low prices.</p><p>If you optimize it further (though various tricks), you can even get large 405B models to run on a small cluster of this hardware, cheaper then an H100 node (which is what is typically used)</p><p><em><span>H100 Prices are becoming commodity-prices cheap.</span><br><span>Or even being rented at a loss - if so, what now?</span></em></p><p>On a high level, it is expected that big clusters still get to charge a premium (&gt;=$2.90 / hour) because there is no other option. For those who truly need it.</p><p>We are starting to see this trend for example with Voltage Park:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png" width="1456" height="741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:647709,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1dd60f1-1e51-4ec7-ac64-d5a2b506f2f0_2118x1078.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Where clusters with Infiniband are charged at a premium.</p><p>While the Ethernet-based instances, which are perfectly fine for inference are priced at a lower rate. Adjusting the prices for the respective use case/availability.</p><p>While there’s been a general decline in foundation model creator teams, it is hard to predict if there will be a resurgence, with the growth in open weights, and/or alternative architectures.</p><p>It is also, expected that in the future, we will see further segmentation by cluster sizes. Where a large 512-node cluster with Infiniband may be billed higher per GPU than a 16-node cluster.</p><p>There is a lot against you, if you price it below $2.25, depending on your OPEX, you risk potentially being unprofitable.</p><p>If you price it too high &gt;= $3, you might not be able to get sufficient buyers to fill capacity.</p><p>If you're late, you could not recoup the cost in the early $4/hour days.</p><p>Overall, these cluster investments will be rough for the key stakeholders and investors.</p><p>While I doubt it’s the case, if new clusters, make a large segment of the AI portfolio investments. We may see additional rippling effects in the funding ecosystem from burnt investors.</p><p>Instead of a negative outlook, a neutral outlook would be some of the unused compute foundation model creators, coming online, are already paid for.</p><p>The funding market has already priced in and paid for this cluster and its model training. And “extracted its value” which they used for their current and next funding round.</p><p><span>Most of these purchases were made before the popularity of </span><strong>Compute Resellers</strong><span>, the cost was already priced in.</span></p><p>If anything, the current revenue they get from their excess H100 compute, and the lowered prices we get, are beneficial to both parties</p><p>If so the negative market impact is minimal, while overall it’s a net positive win for the ecosystem.</p><p>Given that the open-weights model has entered the GPT-4 class arena. Falling H100 prices will be the multiplier unlock for open-weights AI adoption.</p><p>It will be more affordable, for hobbyists, AI developers, and engineers, to run, fine-tune, and tinker with these open models.</p><p><span>Especially if there is no major leap for GPT5++,</span><strong> </strong><span>because it will mean that the gap between open-weights and closed-source models will blur.</span></p><p>This is strongly needed, as the market is currently not sustainable. As there lacks the value capture on the application layer for paying users (which trickles down the platform, models, and infra layers)</p><p>In a way, if everyone is building shovels (including us), and applications with paying users are not being built (and collecting revenue and value).</p><p>But when AI inference and fine-tuning becomes cheaper than ever.</p><p>It can potentially kick off the AI application wave. If it has not already slowly started so.</p><p><em><strong>Spending on new H100’s hardware is likely a loss-maker</strong><p><span>Unless you have some combination of discounted H100s, discounted electricity, or a Sovereign AI angle where the location of your GPU is critical to your customers. Or you have billions and need a super large cluster.</span></p><p><span>If you're investing, consider investing elsewhere.</span><br><span>Or the stock market index itself for a better rate of returns. IMO</span></p></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png" width="1456" height="813" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:813,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:664297,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e858927-de4c-499b-9ba0-261dd470a88b_1948x1088.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At Featherless.AI - We currently host the world’s largest collection of OpenSource AI models, instantly accessible, serverlessly, with unlimited requests from $10 a month, at a fixed price.</p><p><span>We have indexed and made over 2,000 models ready for inference today. This is 10x the catalog of openrouter.ai, the largest model provider </span><em>aggregator</em><span>, </span><em>and </em><span>is the world’s largest collection of Open Weights models available serverlessly for instant inference. Without the need for any expensive dedicated GPUs</span></p><p>And our platform makes this possible, as it’s able to dynamically hot-swap between models in seconds.</p><p>It’s designed to be easy to use, with full OpenAI API compatibility, so you can just plug our platform in as a replacement to your existing AI API for your AI agents. Running in the background</p><p>And we do all of this; As we believe that AI should be easily accessible to everyone, regardless of language or social status.</p><p>On the technical side of things, related to this article.</p><p>It is a challenge having PetaBytes’s worth of AI models, and growing, running 24/7 - while being hardware profitable (we are), because we needed to optimize every layer of our platform, down to how we choose the GPU hardware.</p><p>In an industry, where the typical inference provider pitch is typically along the lines of winning with their, special data center advantages, and CUDA optimization that they perform on their own hardware. Hardware is CAPEX intensive. (Which is being pitched and funded even today)</p><p>We were saying the opposite, which defied most investors’ sensibilities - we were saying we would be avoiding buying new hardware like the plague.</p><p>We came to a realization, that most investors, their analysts, and founders failed to realize, thanks to the billions in hardware investments to date. GPUs are commodity hardware. Faster than all of us expected.</p><p>Few investors have even realized we have reached commodity-level prices at $2.85 in certain places, let alone loss-making prices of a dollar. Because most providers (ignoring certain exceptions), only show their full prices after quotation or after login.</p><p>And that was the trigger, which got me to write this article.</p><p>While we do optimize our inference CUDA and kernels as well. On the hardware side; We’ve bet on hardware commoditizing and have focussed instead on the orchestration layer above.</p><p>So for us, this is a mix of sources from, AWS spot (preferred), to various data center grade providers (eg. Tensordock, Runpod) with security and networking compliances that meet our standards.</p><p>Leveraging them with our own proprietary model hot swapping, which boots new models up in under a second. Keeping our fleet of GPUs right-sized to our workload, while using a custom version of our RWKV foundation model as a low-cost speculative decoder. All of which allows us to take full advantage of this market trend, and future GPU price drops, as newer (and older) GPUs come online to replace the H100s. And scale aggressively.</p><p><em>PS: If you are looking at building the world's largest inference platform, and are aligned with our goals - to make AI accessible to everyone, regardless of language or status. Reach out to us at: hello@featherless.ai</em></p><p><em><span>Head over to Eugene’s Blog </span></em></p><p><em><span> for </span><a href="https://substack.tech-talk-cto.com/p/d4ffab7a-3f0d-4e6e-ade0-e74409770196?postPreview=paid&amp;updated=2024-08-25T03%3A36%3A59.886Z&amp;audience=everyone&amp;free_preview=false&amp;freemail=true" rel="">more footnotes on xAI’s H100 cluster</a><span> we cut from this piece.</span></em><span> </span></p><p><strong>Additional Sources:</strong></p><ul><li><p><span>GPU data: </span><a href="https://www.techpowerup.com/gpu-specs/h100-sxm5-80-gb.c3900" rel="">Tech Power Up Database</a><span>. The A100 SXM had 624 bf16 TFlops, the H100 SXM was 1,979 bf16 TFlops</span></p></li><li><p><span>Microsoft &amp; AWS allocated over $40 billion in AI infra alone: </span><a href="https://www.wsj.com/tech/ai/big-tech-moves-more-ai-spending-abroad-088988de" rel="">Wall Street Journal</a></p></li><li><p><span>“600 Billion Dollars “ is about: </span><a href="https://www.sequoiacap.com/article/ais-600b-question/" rel="">Sequoia’s AI article</a></p></li><li><p><span>Nvidia investor slides for Oct 2014: </span><a href="https://s201.q4cdn.com/141608511/files/doc_presentations/2023/Oct/01/ndr_presentation_oct_2023_final.pdf" rel="">page 14 has the pitch for “data centers”</a></p></li><li><p><span>Semi Analysis: </span><a href="https://www.semianalysis.com/p/100000-h100-clusters-power-network" rel="">deepdive for H100 clusters, w/ 5 year lifespan approx for components</a></p></li><li><p><span>Spreadsheet for : </span><a href="https://docs.google.com/spreadsheets/d/1kZosZmvaecG6P4-yCPzMN7Ha3ubMcTmF9AeJNDKeo98/edit?usp=sharing" rel="">new H100 ROI (Aug 2024)</a></p></li><li><p><span>Spreadsheet for: </span><a href="https://docs.google.com/spreadsheets/d/1Ft3RbeZ-w43kYSiLfYc1vxO41mK5lmJpcPC9GOYHAWc/edit?usp=sharing" rel="">H100 Infiniband Cluster math (Aug 2024)</a></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress Alternatives (145 pts)]]></title>
            <link>https://darn.es/wordpress-alternatives/</link>
            <guid>41805391</guid>
            <pubDate>Fri, 11 Oct 2024 02:03:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darn.es/wordpress-alternatives/">https://darn.es/wordpress-alternatives/</a>, See on <a href="https://news.ycombinator.com/item?id=41805391">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <heading-anchors>
        <div><p>📝</p><p>Editor note: Due to this article's unexpected attention, I've included a few more alternatives that people suggested. I've also added some contextual notes you should know before diving into these options.</p></div><p>Due to <em>gestures vaguely, </em>everything going on <a href="https://css-tricks.com/catching-up-on-the-wordpress-wp-engine-sitch/" rel="noreferrer">right now with WordPress</a>, I thought I'd put together a list of alternative CMSs that better fit the criteria someone might have for their website. The modern CMS landscape is super broad, with the very definition of "Content Management System" being stretched. Some see it as a full-package website platform, and some see it as just UI for their content stored elsewhere.</p><p>The criteria for this list are "Can it be downloaded, dropped onto a server, and you'll have a website?" This eliminates API and git-based CMSs, which I enjoy using; however, wiring a daisy chain of tools is just not viable for many.</p><figure><a href="https://ghost.org/"><div><p>Ghost: The best open source blog &amp; newsletter platform</p><p>Beautiful, modern publishing with email newsletters and paid subscriptions built-in. Used by Platformer, 404Media, Lever News, Tangle, The Browser, and thousands more.</p><p><img src="https://darn.es/img/Z24D4b6-favicon-1.ico" alt=""></p></div><p><img src="https://darn.es/img/Z1hjw9Q-ghost.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>People will already know I have a soft spot for Ghost. But what you might not know is what I'd recommend for hosting.</p><figure><a href="https://www.magicpages.co/"><div><p>Magic Pages</p><p>Get your Ghost CMS publication up and running in no time with Magic Pages’ Ghost CMS web hosting – starting at $4/month!</p><p><img src="https://darn.es/img/iczFH-favicon-196x196-1.jpg" alt=""><span>Magic Pages</span></p></div><p><img src="https://darn.es/img/Z1EYPIA-MagicPages.co-3.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Magic Pages is what I'm using for <a href="https://designsystems.wtf/" rel="noreferrer">Design Systems WTF</a>, and it's been great! The uptime is good, the price is very reasonable, and <a href="https://www.jannis.io/" rel="noreferrer">Jannis</a> provides a personal touch with support. In addition, this sidesteps Ghost's own hosting option (Ghost Pro), which I would be wary of due to <a href="https://x.com/amyhoy/status/1449482190224384000">past experiences with other customers</a>.</p><figure><a href="https://getkirby.com/"><div><p>Kirby is the CMS that adapts to you</p><p>Kirby is the content management system that adapts to any project. Made for developers, designers, creators and clients.</p><p><img src="https://darn.es/img/1CTUoB-favicon.1704303350.svg" alt=""><span>Kirby CMS</span></p></div><p><img src="https://darn.es/img/1RRwiG-opengraph.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I have not used Kirby in client work, but I hear only good things. It's file-based, which seems super appealing to someone like myself who gets cold sweats when opening a database.</p><figure><a href="https://getindiekit.com/"><div><p>Indiekit</p><p>The little server that connects your website to the independent web.</p><p><img src="https://darn.es/img/1Py0ge-icon.svg" alt=""><span>Get Started</span></p></div><p><img src="https://darn.es/img/Z1QWsfN-opengraph-image.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Indiekit seems like an interesting option; it's also file-based but needs a database to manage existing content.</p><figure><a href="https://craftcms.com/"><div><p>Craft CMS</p><p>Craft is a flexible, user-friendly CMS for creating custom digital experiences on the web and beyond.</p><p><img src="https://darn.es/img/1o1WvD-apple-touch-icon.png" alt=""><span>Craft CMS</span></p></div><p><img src="https://darn.es/img/ZXh3Gs-social-craft-cms.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It's a bit more of a commercial option with Craft CMS, but it does offer a free option for solo creators. Warning, though, as you'll need to spend time architecting your content structure by the looks of it.</p><figure><a href="https://www.classicpress.net/"><div><p>ClassicPress | Stable. Lightweight. Instantly Familiar.</p><p>ClassicPress is a community-led open source content management system. A fork of WordPress 4.9, it retains the WordPress classic editor as the default option.</p><p><img src="https://darn.es/img/1MLpdH-cropped-icon-gradient-500x500.png" alt=""><span>ClassicPress</span></p></div><p><img src="https://darn.es/img/1lFgeF-classicpress-cms-for-creators.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>ClassicPress appears to be a direct fork of WordPress but at version 6.2.3. It seems perfect for anyone looking for "the good old days." However, it uses the official WordPress plugin API, so it's not a 100% clean break if that's what you're going for.  Thanks to <a href="https://voxpelli.com/" rel="noreferrer">Pelle Wessman</a> for this suggestion.</p><figure><a href="https://statamic.com/"><div><p>Statamic is a powerful, highly scalable CMS built on Laravel.</p><p>The open source, flat-first, Laravel + Git powered CMS designed for building easy to manage websites.</p><p><img src="https://darn.es/img/Z2ddyQl-favicon-196x196.png" alt=""><span>Statamic</span></p></div><p><img src="https://darn.es/img/Z1MKo0C-card-2023.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I've had several people suggest Statamic. It does look pretty good, plus they have a free solo plan (similar to Craft CMS). I think if the cofounder hadn't brazenly <a href="https://jaygeorge.co.uk/blog/intolerance">endorsed a horrendously damaging politician</a>, I'd have tried it.</p><p>I was going to suggest <a href="https://grabaperch.com/" rel="noreferrer">Perch</a> and <a href="http://buckets.io/" rel="noreferrer">Buckets</a> on this list, but public activity seems low for both. The Perch website even has SSL certificate issues, which isn't a good sign. Check them out if you're interested, but you have been warned.</p><h3 id="honourable-mention">Honourable mention</h3><figure><a href="http://anchorcms.com/"><div><p>Lifting Anchor</p><p>Help on how to use Anchor</p><p><img src="https://darn.es/img/p9SGH-1433533.png" alt=""><span>Anchor CMS</span></p></div><p><img src="https://darn.es/img/Z2kqVUC-screenshot.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Many years ago, I contributed to Anchor, a humble PHP-based CMS that grew a little community around itself. Sadly, the creator, Charlotte, passed away in 2020, and the remaining core team couldn't keep the project going while juggling other responsibilities. I think of it fondly and wish we could give it the time it deserves. </p><p>The theming and custom types aspects were wonderfully simple; heck, I even made a whole site dedicated to themes and sites built with it:</p><figure><a href="https://anchorthemes.com/"><div><p>Welcome - Anchor Themes</p><p>Themes and sites built for &lt;a href=“https://anchorcms.com”&gt;Anchor&lt;/a&gt;, obviously</p><p><img src="https://darn.es/img/xtffw-link-icon.svg" alt=""><span>Anchor Themes</span><span>David Darnes</span></p></div><p><img src="https://darn.es/img/Z1nRKSm-facebook.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>I'll try to keep this list up to date if I recall any others I've used in the past. Hopefully, you find this useful if you're seeking alternative CMSs.</p>
      </heading-anchors>
    </article></div>]]></description>
        </item>
    </channel>
</rss>