<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Jun 2025 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Apple Notes Will Gain Markdown Export at WWDC, and, I Have Thoughts (108 pts)]]></title>
            <link>https://daringfireball.net/linked/2025/06/04/apple-notes-markdown</link>
            <guid>44191558</guid>
            <pubDate>Thu, 05 Jun 2025 13:32:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/linked/2025/06/04/apple-notes-markdown">https://daringfireball.net/linked/2025/06/04/apple-notes-markdown</a>, See on <a href="https://news.ycombinator.com/item?id=44191558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">


<dl>
<dt><a href="https://9to5mac.com/2025/06/03/exclusive-ios-26-messages-carplay-more/">9to5Mac Reports Apple Notes Will Gain Markdown Export at WWDC, and, You’ll Be Unsurprised to Know, I Have Thoughts</a></dt>
<dd>
<p>Marcus Mendes, in a piece at 9to5Mac with multiple spoilers for next week’s keynote:</p>

<blockquote>
  <p>Apple is working on supporting the ability to export notes in
Markdown from Apple Notes, which is something third-party apps
have supported for years. Granted, this is a niche feature, but as
a fierce participant in the niche, I can confirm: this is huge.</p>
</blockquote>

<p>When this story first started spreading this morning, it was getting repeated as Notes “gaining Markdown support”, which implied something like <a href="https://bear.app/">Bear</a> or <a href="https://obsidian.md/">Obsidian</a>, where you can type Markdown syntax characters while editing, and perhaps optionally see the Markdown syntax in your notes. “Markdown notes app” is really like a class of notes apps unto itself.</p>

<p>Some people find this surprising, but I personally don’t want to use a Markdown notes app. <a href="https://daringfireball.net/2004/03/dive_into_markdown">I created Markdown two decades ago</a> and have used it ever since for one thing and one thing only: writing for the web at Daring Fireball. My <a href="https://daringfireball.net/projects/markdown/">original description</a> of what it is still stands: “Markdown is a text-to-HTML conversion tool for web writers.” Perhaps an even better description of Markdown is Matthew Butterick’s, <a href="https://docs.racket-lang.org/pollen/quick-tour.html#%28part._.Markdown_mode%29">from the documentation for Pollen</a>: “Markdown is a simplified notation system for HTML.”</p>

<p>The other great use case for Markdown is in a context where you either <em>need</em> or just <em>want</em> to be saving to a plain text file or database field. That’s not what Apple Notes is or should be. I can see why many technically-minded people want to use Markdown “everywhere”. It’s quite gratifying that Markdown has not only become so popular, but <a href="https://daringfireball.net/2004/03/introducing_markdown">after 21 years</a>, seemingly continues to grow in popularity, to the point now where there clearly are a lot of people who seemingly enjoy writing in Markdown more than even I do. But I think it would be a huge mistake for Apple to make Apple Notes a “Markdown editor”, even as an option. It’s trivial to create malformed Markdown syntax; it shouldn’t be possible to have a malformed note in Apple Notes. I <em>craft</em> posts for Daring Fireball; I dash off notes in Apple Notes.</p>

<p>Apple Notes offers a great WYSIWYG rich text editing interface that works great on an iPhone and even better on a Mac, which I think is exactly appropriate. Particularly clever are the limited formatting options, where you don’t pick a font per se, but rather only from a set of predefined styles, like headings, lists, and block quote. It’s not nerdy at all. That’s the Macintosh way. (But that’s why I think Apple Notes’s use of hashtags, rather than real tokenized tags <a href="https://support.apple.com/guide/mac-help/tag-files-and-folders-mchlp15236/mac">like in the Finder</a>, was an enormous mistake on Apple’s part. Real tokenized tags can contain spaces (so a multi-word tag can just be “Words Written Naturally” not “#WordsCrammedTogether”) and don’t need to be prefixed with an ugly, nerdy-looking <code>#</code> character. Notes using hashtags is like if the Finder disallowed spaces and uppercase letters in filenames.)</p>

<p>But Markdown <em>export</em> from Notes? That sounds awesome. Frankly, perhaps the biggest problem with Apple Notes is that its export functionality is rather crude — <a href="https://support.apple.com/guide/iphone/export-or-print-notes-iphdf551cfa2/ios">PDF and, of all formats, Pages</a>. Exporting and/or copying the selected text as Markdown would be pretty cool. Very curious to see how they handle images though, if this rumor is true.</p>

<p>★ <em>Wednesday, 4 June 2025</em></p>
</dd>
</dl>




<!-- Google Analytics -->

<!-- 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-593949-1']);
  _gaq.push (['_gat._anonymizeIp']);
  _gaq.push(['_trackPageview']);
  (function() {
	var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
	ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
	var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
 -->

<!-- Asynchronously load Mint -->
<!-- No, screw mint
<script type="text/javascript">
(function () {
	var ma = document.createElement('script');
	ma.type = 'text/javascript';
	ma.src = '/mint/?js';
	ma.async = true;
	var s = document.getElementsByTagName('script')[0];
	s.parentNode.insertBefore(ma, s);
})();
</script>
-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[10 Years of Betting on Rust (105 pts)]]></title>
            <link>https://tably.com/tably/10-years-of-betting-on-rust</link>
            <guid>44190190</guid>
            <pubDate>Thu, 05 Jun 2025 10:19:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tably.com/tably/10-years-of-betting-on-rust">https://tably.com/tably/10-years-of-betting-on-rust</a>, See on <a href="https://news.ycombinator.com/item?id=44190190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="textbox" translate="no" spellcheck="false"><h2>10 years of betting on Rust <span>and what I’m looking forward to next.</span></h2><p><span>By Alec Mocatta, Founder<br>1 Jun, 2025</span></p><hr data-tably-img="fwAoAAAAaHR0cHM6Ly90YWJseS5jb20vc3RhdGljL2pFVzNMWjNEWGNPLnN2ZwEIBoID"><p>I wrote my first line of Rust in June 2015, <a href="https://hn.algolia.com/?dateRange=custom&amp;dateStart=0&amp;dateEnd=1437004800&amp;query=rust&amp;sort=byPopularity&amp;type=story" rel="noopener noreferrer" target="_blank">a month after the buzz of Rust 1.0 landing</a>. Coming from C, Python, and JavaScript, I never looked back. Two Rust-based startups and 500k lines of Rust later, here are some reflections on the milestone.</p><h2>The early days were painful</h2><p>Version compatibility was poor—both between crates and with the compiler itself. A bug-fix update could force a compiler bump, which in turn dragged in unstable crates like <a href="https://github.com/serde-deprecated/syntex/tree/master" rel="noopener noreferrer" target="_blank"><code>syntex</code></a> (<a href="https://blog.rust-lang.org/2017/02/02/Rust-1.15/" rel="noopener noreferrer" target="_blank">once</a> a key <code>serde</code> dependency) and their dependents. In practice we “updated the world” and temporally pinned when a critical bug fix or feature demanded it—early on this was as often as every 6-week compiler release cycle. An awful lot of time was wasted binary-searching for compatible version combinations.</p><p>“Fighting the borrow checker” was real for me. Traits came naturally given background in C++, Java and Objective-C, but lifetimes and the idea of “proofs” that (<a href="https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/#cant-we-just-rule-out-bad-specializations" rel="noopener noreferrer" target="_blank">*mostly</a>) don’t affect codegen took a while to grok. Patient friends and colleagues made this easier!</p><p>After a couple years of our codebase and team growing, compile times became painful. Large types were a <a href="https://github.com/rust-lang/rust/issues/38528" rel="noopener noreferrer" target="_blank">recurring</a> issue (and <a href="https://github.com/rust-lang/rust/issues/140944" rel="noopener noreferrer" target="_blank">still are</a>, occasionally), requiring diagnosis and mitigation. Investments brought it down for us, but iteration cycles still took a hit and rapid prototyping generally required effort to set up.</p><h2>The people were and are exceptional</h2><p>The Rust ecosystem has an impressive amount of programming “taste”, manifesting in dependencies with relatively simple builds, elegant implementations, and fast and robust performance. Reaching for TypeScript or Python is a relative exercise in frustration. There’s a reason Rust has taken the <a href="https://survey.stackoverflow.co/2024/technology#admired-and-desired" rel="noopener noreferrer" target="_blank">“most loved/admired language”</a> title for nine years now!</p><p>“What went right” is an essay in itself, but the evolving cadre of dedicated, opinionated and earnest volunteers, with strong mores around saying “no” and “not yet”, are I think the crux of it.</p><p>I reaped the benefits as an employer. We’ve been fortunate to be one of few Rust opportunities in London, with a pipeline of talented engineers keen to work in their favourite language. That your average Rust programmer is better than your average &lt;most other languages&gt; programmer has been a bonus.</p><h2>Rust has become a safe bet (in some domains)</h2><p>The early days necessitated a lot of <a href="https://softwareengineering.stackexchange.com/questions/388092/what-exactly-is-yak-shaving/388236#388236" rel="noopener noreferrer" target="_blank">yak shaving</a>. Omissions in <code>std</code>, for example, led to our evolving a library of workarounds, hacks and extensions: code that simplified or optimised our application code but wasn’t ready or wasn’t justifiable to merge upstream. Over time the rate of additions to it has slowed, and we regularly find ourselves now removing from it. <a href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.extract_if" rel="noopener noreferrer" target="_blank">Our</a> <a href="https://doc.rust-lang.org/std/primitive.u64.html#method.midpoint" rel="noopener noreferrer" target="_blank">custom</a> <a href="https://doc.rust-lang.org/stable/std/thread/struct.Builder.html#method.spawn_unchecked" rel="noopener noreferrer" target="_blank">implementations</a> <a href="https://github.com/rust-lang/rust/issues/133125" rel="noopener noreferrer" target="_blank">begone</a>! Nowadays being able to rely upon <code>std</code>’s primitives having even relatively <a href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html#method.rotate_left" rel="noopener noreferrer" target="_blank">uncommon</a> or <a href="https://github.com/rust-lang/rust/pull/128261" rel="noopener noreferrer" target="_blank">abstract</a> implementations, and them be well-optimised, is a joy.</p><p>This reliability now extends to much of the Rust experience. Building and upgrading is wildly more predictable, with fewer non-Rust dependencies, ~no surprise compiletime/codegen/inlining blowups, less ecosystem reliance on nightly features and greater respecting of semver. <a href="https://blog.rust-lang.org/2018/12/06/Rust-1.31-and-rust-2018/#non-lexical-lifetimes" rel="noopener noreferrer" target="_blank">Inference</a> <a href="https://blog.rust-lang.org/2021/10/21/Rust-1.56.0/#disjoint-capture-in-closures" rel="noopener noreferrer" target="_blank">improvements</a> have made the borrow checker friendlier to newcomers. And far fewer ICEs on nightly! We used to see new ICEs or link errors almost weekly, now it’s quarterly at most.</p><p>The crate ecosystem is more predictable too: newer crates like <a href="https://github.com/BurntSushi/jiff" rel="noopener noreferrer" target="_blank"><code>jiff</code></a>, <a href="https://github.com/pola-rs/polars" rel="noopener noreferrer" target="_blank"><code>polars</code></a> and <a href="https://github.com/tauri-apps/tauri" rel="noopener noreferrer" target="_blank"><code>tauri</code></a> build on the hard-won lessons of earlier projects, while stalwarts like <a href="https://github.com/tokio-rs/tokio" rel="noopener noreferrer" target="_blank"><code>tokio</code></a>, <a href="https://github.com/hyperium/hyper" rel="noopener noreferrer" target="_blank"><code>hyper</code></a> and <a href="https://github.com/rust-lang/regex" rel="noopener noreferrer" target="_blank"><code>regex</code></a> have earned robustness through years of heavy production use.</p><p>Ten years ago choosing Rust for production opted you into reinventing wheels and working around known and unknown issues. While still the case for some domains—namely Rust in the browser for us—for general systems and backend engineering that is a thing of the past. Rust empowers us to focus on business logic while producing remarkably fast and robust applications.</p><h2>Rust today feels like what programming should be</h2><p>More than a safe bet, Rust has a degree of programmer empathy that is unprecedented in large software projects: simple and robust builds, the best <a href="https://kobzol.github.io/rust/rustc/2025/05/16/evolution-of-rustc-errors.html" rel="noopener noreferrer" target="_blank">error messages</a> and linting around, great docs and IDE integration, and strong <a href="https://kobzol.github.io/rust/rustc/2023/07/30/optimizing-rust-ci-2023.html" rel="noopener noreferrer" target="_blank">CI</a> and <a href="https://rustc-dev-guide.rust-lang.org/tests/crater.html" rel="noopener noreferrer" target="_blank">regression testing</a>. Rust feels like a passion project, a labour of love by and for programmers.</p><p>10 years ago could we have predicted this? Well, I think some did. Pained by the status quo, some saw the potential of a language, an ecosystem, designed <i>well</i> by people like them. They volunteered their time and made Rust what it is. As Graydon Hoare, one of Rust’s original authors, <a href="https://rustfoundation.org/media/10-years-of-stable-rust-an-infrastructure-story/" rel="noopener noreferrer" target="_blank">puts it</a>:</p><blockquote><p>Rust today is the result of significant investments made by forward-looking institutions and the efforts of thousands of individuals who shared a belief in the long-term payoff of what they were building.</p></blockquote><p>I bet my time and (my investors’) money on Rust 10 years ago because I believed in this payoff. The enthusiasm was infections, the potential palpable, and smart minds were converging. I’m so glad that has only grown, and so grateful to <a href="https://github.com/topics/rust" rel="noopener noreferrer" target="_blank">everyone</a> who’s <a href="https://thanks.rust-lang.org/rust/all-time/" rel="noopener noreferrer" target="_blank">contributed</a> to it.</p><h2>What I’m looking forward to over the next 10 years</h2><h3>Simpler and faster builds</h3><p>With growing engineering and testing bandwidth we can continue to replace battle-tested but complex or slow dependencies with simpler and faster ones. I’ve directly or indirectly benefitted from this with <a href="https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux/#benefits" rel="noopener noreferrer" target="_blank">linking</a>, <a href="https://github.com/rust-lang/rust/pull/95035#issuecomment-1073966631" rel="noopener noreferrer" target="_blank">locks</a>, <a href="https://github.com/rust-lang/rust/pull/73441" rel="noopener noreferrer" target="_blank">backtraces</a>, <a href="https://github.com/rust-lang/rust/issues/35437" rel="noopener noreferrer" target="_blank">platform-optimised routines</a>, <a href="https://www.memorysafety.org/blog/rustls-server-perf/" rel="noopener noreferrer" target="_blank">TLS</a>, <a href="https://github.com/rust-lang/rustup/issues/3790" rel="noopener noreferrer" target="_blank">HTTP</a>, <a href="https://github.com/rust-lang/cargo/issues/11813" rel="noopener noreferrer" target="_blank">git</a>, <a href="https://github.com/rust-lang/cargo/pull/15417" rel="noopener noreferrer" target="_blank">compression</a>, <a href="https://github.com/rust-lang/rust/pull/37817" rel="noopener noreferrer" target="_blank">building</a> and <a href="https://github.com/rust-lang/rustup" rel="noopener noreferrer" target="_blank">switching</a> toolchains, and <a href="https://github.com/rust-lang/cargo/pull/9992" rel="noopener noreferrer" target="_blank">build automation</a>. A few I’m particularly looking forward to gaining more attention are a <a href="https://github.com/sunfishcode/eyra" rel="noopener noreferrer" target="_blank">pure-Rust</a> and <a href="https://github.com/rust-lang/wg-cargo-std-aware" rel="noopener noreferrer" target="_blank">less special</a> <code>std</code>, reduced reliance on system <a href="https://github.com/rust-lang/rust/pull/140525" rel="noopener noreferrer" target="_blank">linkers</a> and <a href="https://doc.rust-lang.org/rustc/codegen-options/index.html#link-self-contained" rel="noopener noreferrer" target="_blank">libs</a>, <a href="https://github.com/ctz/graviola" rel="noopener noreferrer" target="_blank">pure-Rust crypto</a>, a <a href="https://github.com/cberner/redb" rel="noopener noreferrer" target="_blank">durable BTreeMap</a>, and a Rust <a href="https://github.com/bevyengine/bevy" rel="noopener noreferrer" target="_blank">game engine</a>. The latter not for me to use but because game devs bring some of the most <a href="https://www.reddit.com/r/rust/comments/lcsek8/any_thoughts_on_panic_unreachable_unchecked/" rel="noopener noreferrer" target="_blank">aggressive optimisation</a> I’ve seen and I’d love to benefit from it.</p><hr data-tably-img="fwAoAAAAaHR0cHM6Ly90YWJseS5jb20vc3RhdGljL2xwbkFraUxJRGl1LnBuZwEmCYQC"><p><a href="https://hn.algolia.com/?query=How%20to%20speed%20up%20the%20Rust%20compiler" rel="noopener noreferrer" target="_blank">Significant expertise and effort</a> is going into performance work. In the last few months Tably saw a 60% compile time improvement across frontend and backend. Many of the biggest wins have come from meta changes that aren’t visible on this chart, including <a href="https://github.com/rust-lang/rust/issues/45320#issuecomment-337957101" rel="noopener noreferrer" target="_blank">ThinLTO and backend parallelism</a>, <a href="https://blog.rust-lang.org/2016/09/08/incremental/" rel="noopener noreferrer" target="_blank">incremental</a>, <a href="https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199" rel="noopener noreferrer" target="_blank">metadata pipelining</a>, <a href="https://blog.rust-lang.org/2023/11/09/parallel-rustc/" rel="noopener noreferrer" target="_blank">front-end parallelism</a>. Meta changes that might bear further fruit are: <a href="https://github.com/rust-lang/rustc_codegen_cranelift" rel="noopener noreferrer" target="_blank">Rust-oriented codegen</a>, incremental <a href="https://github.com/davidlattimore/wild" rel="noopener noreferrer" target="_blank">linkers</a>, <a href="https://github.com/rust-lang/rust/issues/127634#issuecomment-2224274508" rel="noopener noreferrer" target="_blank">dead-code elimination</a>, <a href="https://blog.rust-lang.org/inside-rust/2024/12/04/trait-system-refactor-initiative/" rel="noopener noreferrer" target="_blank">a new solver</a>, and <a href="https://github.com/rust-lang/cargo/issues/10673" rel="noopener noreferrer" target="_blank"><code>cargo test</code> caching results</a>.</p><h3>Improved portability and less <code>#[cfg()]</code></h3><p>Testing the power set of valid <code>#[cfg()]</code> options, targets and features on CI is generally prohibitive. This creates untested code paths and potential compile failures, alongside annoyances like fragmented and incomplete documentation, impaired IDE functionality and <code>unused_imports</code> warnings.</p><p>A <a href="https://github.com/rust-lang/rfcs/blob/master/text/1868-portability-lint.md" rel="noopener noreferrer" target="_blank">lint</a> was proposed to mitigate this. A mooted better approach is <a href="https://github.com/rust-lang/rust/issues/41619#issuecomment-2056902943" rel="noopener noreferrer" target="_blank">moving <code>#[cfg()]</code> into the trait system</a> (<a href="https://www.reddit.com/r/rust/comments/lkgoyk/comment/gnjufff/" rel="noopener noreferrer" target="_blank">cc</a>) with bounds like <code>where Platform: Unix</code> and <code>where Platform: Avx2</code>. With this the compiler naturally guarantees compilation under all supported options, targets and features. Specializing items with OS-specific or architecture-specific implementations can be done via, appropriately, <a href="https://github.com/rust-lang/rust/issues/31844" rel="noopener noreferrer" target="_blank">specialization</a>. Metadata can be shared between dev/test/release, different feature combination and different platform builds, resulting in less recompilation, faster CI, and bringing a crates.io MIR <a href="https://www.reddit.com/r/rust/comments/5hav0q/towards_a_worldwide_precompiled_crate_cache_for/" rel="noopener noreferrer" target="_blank">cache</a> closer to reality.</p><h3>Everything being <code>const</code></h3><p>Constant evaluation reduces dependence on macros and build scripts, and brings forward run-time work and panics to compile-time. Currently we only scratch the surface of this as only a small subset of Rust code is supported (no traits, no allocation), and that which is supported runs <a href="https://docs.rs/hex-literal" rel="noopener noreferrer" target="_blank">too slowly</a> for nontrivial use cases.</p><p>Assuming portability all code can be evaluated during compilation (<a href="https://docs.rs/crabtime/latest/crabtime/#-one-shot-evaluation" rel="noopener noreferrer" target="_blank">cc</a>). Unportable code like FFI or assembly without a generic fallback is getting less common. <a href="https://github.com/rust-lang/const-eval/issues/1" rel="noopener noreferrer" target="_blank">Various</a> effects-inspired attempts to enable ubiquitous <code>const</code> have been <a href="https://github.com/rust-lang/rust/issues/67792" rel="noopener noreferrer" target="_blank">made</a> and <a href="https://github.com/rust-lang/rust/issues/110395" rel="noopener noreferrer" target="_blank">reverted</a> and <a href="https://github.com/rust-lang/rfcs/pull/3762" rel="noopener noreferrer" target="_blank">attempted again</a>. I hope we dodge this extra language complexity with the simple assumption that “everything can be executed in a <code>const</code> context”. There are <a href="https://github.com/rust-lang/const-eval/issues/20#issuecomment-464124376" rel="noopener noreferrer" target="_blank">tricky</a> <a href="https://github.com/rust-lang/unsafe-code-guidelines/issues/522" rel="noopener noreferrer" target="_blank">issues</a> to overcome but the language simplicity will be worth it.</p><h3>Simpler concurrency</h3><p><code>async</code> has a relatively high complexity cost for us due to the <a href="https://docs.rs/tokio-util/0.7.15/tokio_util/task/struct.LocalPoolHandle.html#method.spawn_pinned" rel="noopener noreferrer" target="_blank"><code>'static</code> bound</a> (<a href="https://maciej.codes/2022-06-09-local-async.html" rel="noopener noreferrer" target="_blank">cc</a>), <a href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/cancellation.html" rel="noopener noreferrer" target="_blank">cancellation-safety</a>, and <a href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/async-traits.html" rel="noopener noreferrer" target="_blank">restrictions</a> relating to <a href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#is-it-okay-to-use-async-fn-in-traits-what-are-the-limitations" rel="noopener noreferrer" target="_blank">traits</a> and <a href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#dynamic-dispatch" rel="noopener noreferrer" target="_blank"><code>dyn</code></a>. These currently <a href="https://github.com/tokio-rs/tokio/issues/2596#issuecomment-708441238" rel="noopener noreferrer" target="_blank">seem</a> <a href="https://www.reddit.com/r/rust/comments/1gvblzb/comment/ly0o3hi/" rel="noopener noreferrer" target="_blank">insoluble</a>. Bifurcation between <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" rel="noopener noreferrer" target="_blank">sync</a> and <a href="https://docs.rs/futures/latest/futures/prelude/stream/trait.StreamExt.html" rel="noopener noreferrer" target="_blank">async</a> primitives and <a href="https://github.com/rust-lang/futures-rs/issues/2109#issuecomment-608413396" rel="noopener noreferrer" target="_blank">ecosystem</a> <a href="https://github.com/tokio-rs/tokio/issues/1529#issuecomment-2149262072" rel="noopener noreferrer" target="_blank">idiosyncrasies</a> further increase <a href="https://eta.st/2021/03/08/async-rust-2.html" rel="noopener noreferrer" target="_blank">the</a> <a href="https://trouble.mataroa.blog/blog/asyncawait-is-real-and-can-hurt-you/" rel="noopener noreferrer" target="_blank">async</a> <a href="https://bitbashing.io/async-rust.html" rel="noopener noreferrer" target="_blank">tax</a>. <a href="https://blog.yoshuawuyts.com/extending-rusts-effect-system/" rel="noopener noreferrer" target="_blank">Effects-inspired</a> solutions <a href="https://www.reddit.com/r/rust/comments/119y8ex/comment/j9pt1h3/" rel="noopener noreferrer" target="_blank">seem unpromising</a>.</p><p>Pre-1.0 Rust had a solution: <a href="https://stackoverflow.com/questions/29791031/what-happened-to-libgreen" rel="noopener noreferrer" target="_blank"><code>libgreen</code></a>. It achieved concurrency in user-space without this bifurcation, but at <a href="https://github.com/rust-lang/rfcs/blob/master/text/0230-remove-runtime.md#the-problems" rel="noopener noreferrer" target="_blank">significant performance, portability and maintenance cost</a> and so was <a href="https://github.com/rust-lang/rust/pull/18967" rel="noopener noreferrer" target="_blank">removed</a>. With increased engineering bandwidth this may be worth revisiting. Again the language simplicity would be worth it. (One day I’ll create a PoC of zero-cost wrapping of <code>std::{fs, net}</code> plus <code>fiber::{spawn, select}</code> using <a href="https://github.com/Xudong-Huang/generator-rs" rel="noopener noreferrer" target="_blank"><code>generator</code></a>!)</p><h3>Excelling in more domains</h3><p>Rust in the browser feels under-explored: we bump into issues that suggest there <a href="https://github.com/rustwasm/wasm-bindgen/issues/2388" rel="noopener noreferrer" target="_blank">aren’t</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/2392" rel="noopener noreferrer" target="_blank">many</a> <a href="https://github.com/rustwasm/wasm-bindgen/pull/2598" rel="noopener noreferrer" target="_blank">others</a> <a href="https://github.com/rustwasm/wasm-bindgen/pull/3899" rel="noopener noreferrer" target="_blank">using</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/3801" rel="noopener noreferrer" target="_blank">it</a> <a href="https://github.com/rustwasm/wasm-bindgen/issues/3687#issuecomment-1806486879" rel="noopener noreferrer" target="_blank">seriously</a>. Table stakes like <a href="https://github.com/rustwasm/wasm-bindgen/issues/3687" rel="noopener noreferrer" target="_blank">avoiding UB</a> and getting stack traces <a href="https://github.com/getsentry/sentry-javascript/issues/9968" rel="noopener noreferrer" target="_blank">cross-browser</a> took our time and effort. <a href="https://github.com/leptos-rs/leptos" rel="noopener noreferrer" target="_blank"><code>leptos</code></a>—the web framework behind this site—has come a long way (much like our previous choice <a href="https://github.com/sycamore-rs/sycamore" rel="noopener noreferrer" target="_blank"><code>sycamore</code></a>), yet rough edges remain.</p><p>Some domains like rapid prototyping and <a href="https://www.bartoszsypytkowski.com/is-rust-a-good-fit-for-business-apps/" rel="noopener noreferrer" target="_blank">business logic</a> may remain outside of Rust’s reach due to deliberate design decisions that trade off iteration speed. For most domains though I think it’s a matter of time. Adoption hurdles remain for GUIs, machine learning, and <a href="https://loglog.games/blog/leaving-rust-gamedev/" rel="noopener noreferrer" target="_blank">game development</a> due to entrenched ecosystems, high switching costs, cultural inertia, and established tooling. But those barriers aren’t permanent. Rust continues to mature and steadily break into these domains, thanks to relentless community efforts and innovation.</p><h2>Conclusion</h2><p>Looking forward the trajectory is clear and exciting. The positive feedback loop of adoption boosting engineering and testing bandwidth, which then fuels further adoption, is accelerating. The upcoming decade promises further refinement, faster compile times, broader domain adoption, and an increasingly seamless developer experience. At Tably, we remain committed to pushing the boundaries of what’s possible with Rust, excited to witness—and contribute to—the language’s next chapter. Here’s to another 10 years of betting on Rust!</p></div><div><div><h2>10 years of betting on Rust <span>and what I’m looking forward to next.</span><br></h2></div><p><span></span><span></span><span>By Alec Mocatta, Founder
1 Jun, 2025</span><br></p><p><img src="https://tably.com/static/jEW3LZ3DXcO.svg" width="1544" height="898"><span></span><span></span></p><p><span></span><span></span>I wrote my first line of Rust in June 2015, <a target="_blank" rel="noopener noreferrer" href="https://hn.algolia.com/?dateRange=custom&amp;dateStart=0&amp;dateEnd=1437004800&amp;query=rust&amp;sort=byPopularity&amp;type=story">a month after the buzz of Rust 1.0 landing</a>. Coming from C, Python, and JavaScript, I never looked back. Two Rust-based startups and 500k lines of Rust later, here are some reflections on the milestone.<br></p><h2><span></span><span></span>The early days were painful<br></h2><p><span></span><span></span>Version compatibility was poor—both between crates and with the compiler itself. A bug-fix update could force a compiler bump, which in turn dragged in unstable crates like <a target="_blank" rel="noopener noreferrer" href="https://github.com/serde-deprecated/syntex/tree/master"><code>syntex</code></a> (<a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2017/02/02/Rust-1.15/">once</a> a key <code>serde</code> dependency) and their dependents. In practice we “updated the world” and temporally pinned when a critical bug fix or feature demanded it—early on this was as often as every 6-week compiler release cycle. An awful lot of time was wasted binary-searching for compatible version combinations.<br></p><p><span></span><span></span>“Fighting the borrow checker” was real for me. Traits came naturally given background in C++, Java and Objective-C, but lifetimes and the idea of “proofs” that (<a target="_blank" rel="noopener noreferrer" href="https://aturon.github.io/blog/2017/07/08/lifetime-dispatch/#cant-we-just-rule-out-bad-specializations">*mostly</a>) don’t affect codegen took a while to grok. Patient friends and colleagues made this easier!<br></p><p><span></span><span></span>After a couple years of our codebase and team growing, compile times became painful. Large types were a <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/38528">recurring</a> issue (and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/140944">still are</a>, occasionally), requiring diagnosis and mitigation. Investments brought it down for us, but iteration cycles still took a hit and rapid prototyping generally required effort to set up.<br></p><h2><span></span><span></span>The people were and are exceptional<br></h2><p><span></span><span></span>The Rust ecosystem has an impressive amount of programming “taste”, manifesting in dependencies with relatively simple builds, elegant implementations, and fast and robust performance. Reaching for TypeScript or Python is a relative exercise in frustration. There’s a reason Rust has taken the <a target="_blank" rel="noopener noreferrer" href="https://survey.stackoverflow.co/2024/technology#admired-and-desired">“most loved/admired language”</a> title for nine years now!<br></p><p><span></span><span></span>“What went right” is an essay in itself, but the evolving cadre of dedicated, opinionated and earnest volunteers, with strong mores around saying “no” and “not yet”, are I think the crux of it.<br></p><p><span></span><span></span>I reaped the benefits as an employer. We’ve been fortunate to be one of few Rust opportunities in London, with a pipeline of talented engineers keen to work in their favourite language. That your average Rust programmer is better than your average &lt;most other languages&gt; programmer has been a bonus.<br></p><h2><span></span><span></span>Rust has become a safe bet (in some domains)<br></h2><p><span></span><span></span>The early days necessitated a lot of <a target="_blank" rel="noopener noreferrer" href="https://softwareengineering.stackexchange.com/questions/388092/what-exactly-is-yak-shaving/388236#388236">yak shaving</a>. Omissions in <code>std</code>, for example, led to our evolving a library of workarounds, hacks and extensions: code that simplified or optimised our application code but wasn’t ready or wasn’t justifiable to merge upstream. Over time the rate of additions to it has slowed, and we regularly find ourselves now removing from it. <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html#method.extract_if">Our</a> <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/primitive.u64.html#method.midpoint">custom</a> <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/stable/std/thread/struct.Builder.html#method.spawn_unchecked">implementations</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/133125">begone</a>! Nowadays being able to rely upon <code>std</code>’s primitives having even relatively <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/collections/struct.VecDeque.html#method.rotate_left">uncommon</a> or <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/128261">abstract</a> implementations, and them be well-optimised, is a joy.<br></p><p><span></span><span></span>This reliability now extends to much of the Rust experience. Building and upgrading is wildly more predictable, with fewer non-Rust dependencies, ~no surprise compiletime/codegen/inlining blowups, less ecosystem reliance on nightly features and greater respecting of semver. <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2018/12/06/Rust-1.31-and-rust-2018/#non-lexical-lifetimes">Inference</a> <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2021/10/21/Rust-1.56.0/#disjoint-capture-in-closures">improvements</a> have made the borrow checker friendlier to newcomers. And far fewer ICEs on nightly! We used to see new ICEs or link errors almost weekly, now it’s quarterly at most.<br></p><p><span></span><span></span>The crate ecosystem is more predictable too: newer crates like <a target="_blank" rel="noopener noreferrer" href="https://github.com/BurntSushi/jiff"><code>jiff</code></a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/pola-rs/polars"><code>polars</code></a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/tauri-apps/tauri"><code>tauri</code></a> build on the hard-won lessons of earlier projects, while stalwarts like <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio"><code>tokio</code></a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperium/hyper"><code>hyper</code></a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/regex"><code>regex</code></a> have earned robustness through years of heavy production use.<br></p><p><span></span><span></span>Ten years ago choosing Rust for production opted you into reinventing wheels and working around known and unknown issues. While still the case for some domains—namely Rust in the browser for us—for general systems and backend engineering that is a thing of the past. Rust empowers us to focus on business logic while producing remarkably fast and robust applications.<br></p><h2><span></span><span></span>Rust today feels like what programming should be<br></h2><p><span></span><span></span>More than a safe bet, Rust has a degree of programmer empathy that is unprecedented in large software projects: simple and robust builds, the best <a target="_blank" rel="noopener noreferrer" href="https://kobzol.github.io/rust/rustc/2025/05/16/evolution-of-rustc-errors.html">error messages</a> and linting around, great docs and IDE integration, and strong <a target="_blank" rel="noopener noreferrer" href="https://kobzol.github.io/rust/rustc/2023/07/30/optimizing-rust-ci-2023.html">CI</a> and <a target="_blank" rel="noopener noreferrer" href="https://rustc-dev-guide.rust-lang.org/tests/crater.html">regression testing</a>. Rust feels like a passion project, a labour of love by and for programmers.<br></p><p><span></span><span></span>10 years ago could we have predicted this? Well, I think some did. Pained by the status quo, some saw the potential of a language, an ecosystem, designed <i>well</i> by people like them. They volunteered their time and made Rust what it is. As Graydon Hoare, one of Rust’s original authors, <a target="_blank" rel="noopener noreferrer" href="https://rustfoundation.org/media/10-years-of-stable-rust-an-infrastructure-story/">puts it</a>:<br></p><blockquote><p>Rust today is the result of significant investments made by forward-looking institutions and the efforts of thousands of individuals who shared a belief in the long-term payoff of what they were building.<br></p><span></span><span></span></blockquote><p><span></span><span></span>I bet my time and (my investors’) money on Rust 10 years ago because I believed in this payoff. The enthusiasm was infections, the potential palpable, and smart minds were converging. I’m so glad that has only grown, and so grateful to <a target="_blank" rel="noopener noreferrer" href="https://github.com/topics/rust">everyone</a> who’s <a target="_blank" rel="noopener noreferrer" href="https://thanks.rust-lang.org/rust/all-time/">contributed</a> to it.<br></p><h2><span></span><span></span>What I’m looking forward to over the next 10 years<br></h2><h3><span></span><span></span>Simpler and faster builds<br></h3><p><span></span><span></span>With growing engineering and testing bandwidth we can continue to replace battle-tested but complex or slow dependencies with simpler and faster ones. I’ve directly or indirectly benefitted from this with <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux/#benefits">linking</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/95035#issuecomment-1073966631">locks</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/73441">backtraces</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/35437">platform-optimised routines</a>, <a target="_blank" rel="noopener noreferrer" href="https://www.memorysafety.org/blog/rustls-server-perf/">TLS</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustup/issues/3790">HTTP</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/issues/11813">git</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/pull/15417">compression</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/37817">building</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustup">switching</a> toolchains, and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/pull/9992">build automation</a>. A few I’m particularly looking forward to gaining more attention are a <a target="_blank" rel="noopener noreferrer" href="https://github.com/sunfishcode/eyra">pure-Rust</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/wg-cargo-std-aware">less special</a> <code>std</code>, reduced reliance on system <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/140525">linkers</a> and <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/rustc/codegen-options/index.html#link-self-contained">libs</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/ctz/graviola">pure-Rust crypto</a>, a <a target="_blank" rel="noopener noreferrer" href="https://github.com/cberner/redb">durable BTreeMap</a>, and a Rust <a target="_blank" rel="noopener noreferrer" href="https://github.com/bevyengine/bevy">game engine</a>. The latter not for me to use but because game devs bring some of the most <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/lcsek8/any_thoughts_on_panic_unreachable_unchecked/">aggressive optimisation</a> I’ve seen and I’d love to benefit from it.<br></p><p><img src="https://tably.com/static/lpnAkiLIDiu.png" width="2342" height="644"><span></span><span></span></p><p><span></span><span></span><a target="_blank" rel="noopener noreferrer" href="https://hn.algolia.com/?query=How%20to%20speed%20up%20the%20Rust%20compiler">Significant expertise and effort</a> is going into performance work. In the last few months Tably saw a 60% compile time improvement across frontend and backend. Many of the biggest wins have come from meta changes that aren’t visible on this chart, including <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/45320#issuecomment-337957101">ThinLTO and backend parallelism</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2016/09/08/incremental/">incremental</a>, <a target="_blank" rel="noopener noreferrer" href="https://internals.rust-lang.org/t/evaluating-pipelined-rustc-compilation/10199">metadata pipelining</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/11/09/parallel-rustc/">front-end parallelism</a>. Meta changes that might bear further fruit are: <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rustc_codegen_cranelift">Rust-oriented codegen</a>, incremental <a target="_blank" rel="noopener noreferrer" href="https://github.com/davidlattimore/wild">linkers</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/127634#issuecomment-2224274508">dead-code elimination</a>, <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/inside-rust/2024/12/04/trait-system-refactor-initiative/">a new solver</a>, and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/cargo/issues/10673"><code>cargo test</code> caching results</a>.<br></p><h3><span></span><span></span>Improved portability and less <code>#[cfg()]</code><br></h3><p><span></span><span></span>Testing the power set of valid <code>#[cfg()]</code> options, targets and features on CI is generally prohibitive. This creates untested code paths and potential compile failures, alongside annoyances like fragmented and incomplete documentation, impaired IDE functionality and <code>unused_imports</code> warnings.<br></p><p><span></span><span></span>A <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/blob/master/text/1868-portability-lint.md">lint</a> was proposed to mitigate this. A mooted better approach is <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/41619#issuecomment-2056902943">moving <code>#[cfg()]</code> into the trait system</a> (<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/lkgoyk/comment/gnjufff/">cc</a>) with bounds like <code>where Platform: Unix</code> and <code>where Platform: Avx2</code>. With this the compiler naturally guarantees compilation under all supported options, targets and features. Specializing items with OS-specific or architecture-specific implementations can be done via, appropriately, <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/31844">specialization</a>. Metadata can be shared between dev/test/release, different feature combination and different platform builds, resulting in less recompilation, faster CI, and bringing a crates.io MIR <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/5hav0q/towards_a_worldwide_precompiled_crate_cache_for/">cache</a> closer to reality.<br></p><h3><span></span><span></span>Everything being <code>const</code><br></h3><p><span></span><span></span>Constant evaluation reduces dependence on macros and build scripts, and brings forward run-time work and panics to compile-time. Currently we only scratch the surface of this as only a small subset of Rust code is supported (no traits, no allocation), and that which is supported runs <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/hex-literal">too slowly</a> for nontrivial use cases.<br></p><p><span></span><span></span>Assuming portability all code can be evaluated during compilation (<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/crabtime/latest/crabtime/#-one-shot-evaluation">cc</a>). Unportable code like FFI or assembly without a generic fallback is getting less common. <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/const-eval/issues/1">Various</a> effects-inspired attempts to enable ubiquitous <code>const</code> have been <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/67792">made</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/110395">reverted</a> and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/pull/3762">attempted again</a>. I hope we dodge this extra language complexity with the simple assumption that “everything can be executed in a <code>const</code> context”. There are <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/const-eval/issues/20#issuecomment-464124376">tricky</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/unsafe-code-guidelines/issues/522">issues</a> to overcome but the language simplicity will be worth it.<br></p><h3><span></span><span></span>Simpler concurrency<br></h3><p><span></span><span></span><code>async</code> has a relatively high complexity cost for us due to the <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/tokio-util/0.7.15/tokio_util/task/struct.LocalPoolHandle.html#method.spawn_pinned"><code>'static</code> bound</a> (<a target="_blank" rel="noopener noreferrer" href="https://maciej.codes/2022-06-09-local-async.html">cc</a>), <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/cancellation.html">cancellation-safety</a>, and <a target="_blank" rel="noopener noreferrer" href="https://google.github.io/comprehensive-rust/concurrency/async-pitfalls/async-traits.html">restrictions</a> relating to <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#is-it-okay-to-use-async-fn-in-traits-what-are-the-limitations">traits</a> and <a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2023/12/21/async-fn-rpit-in-traits/#dynamic-dispatch"><code>dyn</code></a>. These currently <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio/issues/2596#issuecomment-708441238">seem</a> <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/1gvblzb/comment/ly0o3hi/">insoluble</a>. Bifurcation between <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/iter/trait.Iterator.html">sync</a> and <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/futures/latest/futures/prelude/stream/trait.StreamExt.html">async</a> primitives and <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/futures-rs/issues/2109#issuecomment-608413396">ecosystem</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/tokio-rs/tokio/issues/1529#issuecomment-2149262072">idiosyncrasies</a> further increase <a target="_blank" rel="noopener noreferrer" href="https://eta.st/2021/03/08/async-rust-2.html">the</a> <a target="_blank" rel="noopener noreferrer" href="https://trouble.mataroa.blog/blog/asyncawait-is-real-and-can-hurt-you/">async</a> <a target="_blank" rel="noopener noreferrer" href="https://bitbashing.io/async-rust.html">tax</a>. <a target="_blank" rel="noopener noreferrer" href="https://blog.yoshuawuyts.com/extending-rusts-effect-system/">Effects-inspired</a> solutions <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/119y8ex/comment/j9pt1h3/">seem unpromising</a>.<br></p><p><span></span><span></span>Pre-1.0 Rust had a solution: <a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/29791031/what-happened-to-libgreen"><code>libgreen</code></a>. It achieved concurrency in user-space without this bifurcation, but at <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rfcs/blob/master/text/0230-remove-runtime.md#the-problems">significant performance, portability and maintenance cost</a> and so was <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/pull/18967">removed</a>. With increased engineering bandwidth this may be worth revisiting. Again the language simplicity would be worth it. (One day I’ll create a PoC of zero-cost wrapping of <code>std::{fs, net}</code> plus <code>fiber::{spawn, select}</code> using <a target="_blank" rel="noopener noreferrer" href="https://github.com/Xudong-Huang/generator-rs"><code>generator</code></a>!)<br></p><h3><span></span><span></span>Excelling in more domains<br></h3><p><span></span><span></span>Rust in the browser feels under-explored: we bump into issues that suggest there <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/2388">aren’t</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/2392">many</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/pull/2598">others</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/pull/3899">using</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3801">it</a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3687#issuecomment-1806486879">seriously</a>. Table stakes like <a target="_blank" rel="noopener noreferrer" href="https://github.com/rustwasm/wasm-bindgen/issues/3687">avoiding UB</a> and getting stack traces <a target="_blank" rel="noopener noreferrer" href="https://github.com/getsentry/sentry-javascript/issues/9968">cross-browser</a> took our time and effort. <a target="_blank" rel="noopener noreferrer" href="https://github.com/leptos-rs/leptos"><code>leptos</code></a>—the web framework behind this site—has come a long way (much like our previous choice <a target="_blank" rel="noopener noreferrer" href="https://github.com/sycamore-rs/sycamore"><code>sycamore</code></a>), yet rough edges remain.<br></p><p><span></span><span></span>Some domains like rapid prototyping and <a target="_blank" rel="noopener noreferrer" href="https://www.bartoszsypytkowski.com/is-rust-a-good-fit-for-business-apps/">business logic</a> may remain outside of Rust’s reach due to deliberate design decisions that trade off iteration speed. For most domains though I think it’s a matter of time. Adoption hurdles remain for GUIs, machine learning, and <a target="_blank" rel="noopener noreferrer" href="https://loglog.games/blog/leaving-rust-gamedev/">game development</a> due to entrenched ecosystems, high switching costs, cultural inertia, and established tooling. But those barriers aren’t permanent. Rust continues to mature and steadily break into these domains, thanks to relentless community efforts and innovation.<br></p><h2><span></span><span></span>Conclusion<br></h2><p><span></span><span></span>Looking forward the trajectory is clear and exciting. The positive feedback loop of adoption boosting engineering and testing bandwidth, which then fuels further adoption, is accelerating. The upcoming decade promises further refinement, faster compile times, broader domain adoption, and an increasingly seamless developer experience. At Tably, we remain committed to pushing the boundaries of what’s possible with Rust, excited to witness—and contribute to—the language’s next chapter. Here’s to another 10 years of betting on Rust!<br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Air Lab – A portable and open air quality measuring device (220 pts)]]></title>
            <link>https://networkedartifacts.com/airlab/simulator</link>
            <guid>44189329</guid>
            <pubDate>Thu, 05 Jun 2025 07:42:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://networkedartifacts.com/airlab/simulator">https://networkedartifacts.com/airlab/simulator</a>, See on <a href="https://news.ycombinator.com/item?id=44189329">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Old payphones get new life, thanks to Vermont engineer (113 pts)]]></title>
            <link>https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public</link>
            <guid>44188204</guid>
            <pubDate>Thu, 05 Jun 2025 04:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public">https://www.core77.com/posts/137183/Engineer-Fixes-and-Re-Installs-Old-Payphones-Provides-Free-Calls-to-the-Public</a>, See on <a href="https://news.ycombinator.com/item?id=44188204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <div id="post_header">
        

            <ul>
                


                            </ul>
        

            <h2>Engineer Fixes and Re-Installs Old Payphones, Provides Free Calls to the Public </h2>
                    <h2>Patrick Schlott's RandTel</h2>
                    

        
    </div>

    




            <section id="post">




<p id="e6b1c2_1150" data-ic-marker="cba9c1_2864">Payphones "were the only things that were built to last for decades and be out in the elements," says electrical engineer Patrick Schlott. He should know; as a hobby, he buys secondhand payphones, rewires them, then asks local businesses in rural Vermont if they'd let him install them. His goal is to offer, for free, public telephone service. (Schlott foots the bill himself.)</p><p id="c5e647_2917" data-ic-marker="7cf2fe_2466"><img src="https://s3files.core77.com/blog/images/1723601_81_137183_kpeo47g3E.jpg"></p><p id="9a6bcd_3553" data-ic-marker="dba584_3342"><img src="https://s3files.core77.com/blog/images/1723605_81_137183_vA5MEq9TD.jpg"> </p><p id="4bf8c5_5474" data-ic-marker="ae6d05_1832"><img src="https://s3files.core77.com/blog/images/1723606_81_137183_qJB0s8tAa.jpg">  </p><p id="5e9bc_4785" data-ic-marker="4cc280_1160">"It's assumed most folks own cell phones," writes Schlott. "Well, not everyone does, sometimes they don't work out on dirt roads, sometimes you forget your charger, and sometimes you just really need to make a phone call. We aim to provide a valuable public service to the community while teaching people about the US telephone system that has over a century of history behind it."</p><p id="80e5b4_741" data-ic-marker="e1b546_1909"><img src="https://s3files.core77.com/blog/images/1723598_81_137183_3jwvCPXrg.jpg"> </p><p id="df568f_1321" data-ic-marker="fde2a4_1693"><img data-image-width="880" data-image-height="879" data-image-id="1723604" src="https://s3files.core77.com/blog/images/1723604_81_137183_7Ov2GElFL.jpg"></p><p id="d88795_6843" data-ic-marker="5fc854_3489">Schlott's company, <a href="https://randtel.co/" rel="">RandTel</a>, currently operates three phones in his neck of Vermont: One at the North Tunbridge General Store in Tunbridge, one at the Latham Library and a third—a rotary model from the 1950s--at the town of Randolph's information booth. He's particularly proud of that last one, as "This installation is 100% solar-powered, provided graciously by Catamount Solar," he writes. "Many thanks to the White River Valley Chamber of Commerce for hosting!"</p><p id="54cf2d_185" data-ic-marker="307d76_3913"><img src="https://s3files.core77.com/blog/images/1723595_81_137183_gT0mw7bIA.jpg"></p><p id="8e0ad6_568" data-ic-marker="ec336b_1835"><img src="https://s3files.core77.com/blog/images/1723596_81_137183_uRqLcCL_e.jpg"> </p><p id="6b31de_964" data-ic-marker="6c8ffe_1656"><img src="https://s3files.core77.com/blog/images/1723610_81_137183_Vg4976xOm.jpg"></p><p id="147407_1071" data-ic-marker="92d262_1337"><img src="https://s3files.core77.com/blog/images/1723603_81_137183_OOm_XuQld.jpg"> </p><p id="da9100_451" data-ic-marker="746ec4_3951"><img src="https://s3files.core77.com/blog/images/1723597_81_137183_8ao8X9lBJ.jpg"> </p><p id="32c173_36" data-ic-marker="74d4b0_2910"><img src="https://s3files.core77.com/blog/images/1723599_81_137183_gyZ4iO7wV.jpg"></p><p id="2ac73e_107" data-ic-marker="3b6999_3940"><img src="https://s3files.core77.com/blog/images/1723600_81_137183_P67Na8vKw.jpg"> </p><p id="1cc4a4_5093" data-ic-marker="1d346b_3959">Here's a look at what Schlott does:</p><p id="406a7e_2513"><iframe width="560" height="315" src="https://www.youtube.com/embed/DwXTUM2Y7L0?si=LQ_geDiPeryisVer" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><em id="aa17fa_4828">Enter a caption (optional)</em></p>
        

            </section>

                


                


<div>
    <ul>
        <li data-this-post-id="137183" data-this-author-id="0">
            
            
            <p>Favorite This</p>
        </li>
       <li data-this-post-title="Engineer Fixes and Re-Installs Old Payphones, Provides Free Calls to the Public " data-this-post-id="137183" data-this-author-id="0">
            
            
            <p>Comment</p>
        </li>
        
    </ul> 
</div>






           


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a 3D SVG Renderer that projects textures without rasterization (173 pts)]]></title>
            <link>https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects</link>
            <guid>44187645</guid>
            <pubDate>Thu, 05 Jun 2025 02:05:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects">https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects</a>, See on <a href="https://news.ycombinator.com/item?id=44187645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>I’ve been building a </span><a href="https://github.com/tscircuit/simple-3d-svg" rel="">vanilla 3D object to SVG renderer in Typescript</a><span> to help render </span><a href="https://tscircuit.com/seveibar/usb-c-flashlight#3dhttps://tscircuit.com/seveibar/usb-c-flashlight#3d" rel="">circuit boards that are made in React</a><span> and discovered an interesting trick to keep the SVGs small while getting approximately-correct looking perspective transformations with image textures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png" width="1456" height="708" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:708,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:115620,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>An example circuit board rendered with our vanilla Typescript 3D Renderer. Great for checking sizing! You can see we were able to project the “texture” containing the PCB traces!</figcaption></figure></div><p>SVGs don’t support perspective transforms like CSS (or at least they’re not guaranteed to work in image viewers), so we need a way to simulate this perspective transformation without creating a massive SVG. It’s easy to draw the box below, you can just project the face of each side of the cube into a polygon, but mapping the texture to that perspective transform isn’t natively possible!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png" width="1456" height="738" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1456w" sizes="100vw"></picture></div></a><figcaption>Perspective transforms in CSS using the transform attribute (from MDN)</figcaption></figure></div><p><span>So SVGs don’t support perspective transforms, what do they support? SVGs support this nice little transform called an affine transform. This 6 number transform is what you get when you do </span><code>transform: matrix(a,b,c,d,e,f)</code><span> in CSS. They are super useful for 2D transformations, like panning/scaling/dragging, but can’t really project into 3d.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png" width="1456" height="621" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:621,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:284621,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1456w" sizes="100vw"></picture></div></a><figcaption>This projection isn’t possible with a single affine transform. You also can’t combine affine transformation matrices together to create this shape.</figcaption></figure></div><p>How can we approximate the transform? Here are some ideas I mulled over that could achieve a good result:</p><ul><li><p>Redraw the image with the distortion. This is potentially expensive and means that we can’t use SVGs as the images without converting them to bitmaps. It also means that things might look “fuzzy”</p></li><li><p>Ray trace everything! By projecting a ray to compute each pixel for the image, I could get a very conventional 3d renderer. This doesn’t achieve my goal of lightweight SVGs though</p></li><li><p>Subdivide the image and project each subdivision in the most locally-correct affine transformation. Use projected polygon clip paths to cut off the edges of regions.</p></li></ul><p>I was really curious how the last bullet point could work, and I could think of no other ideas that didn’t require rasterization. So with OpenAI O3’s help I implemented it into the vanilla Typescript 3D renderer. To test it, we’re going to project a checkerboard pattern onto a cube.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png" width="1456" height="1009" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1009,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:94975,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Cube with no texture</figcaption></figure></div><p>Ok here’s our starting point, 2 subdivisions, the 2 checkboard images with affine transformations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png" width="1456" height="1033" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1033,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Two subdivisions with a different affine transformation applied to each image</figcaption></figure></div><p>Not too bad, let’s see it with 4 subdivisions!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png" width="1244" height="900" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:900,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Four images with different affine transformations</figcaption></figure></div><p>Looks a bit rough (literally, it looks like it is not a flat surface). Let’s keep going!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png" width="1394" height="886" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:886,&quot;width&quot;:1394,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137833,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Eight images, eight clip paths, still not very smooth!!</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png" width="1426" height="960" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1426,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:147523,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>16 images, 16 clip paths</figcaption></figure></div><p>Alright let’s go to the end, we want it to look flat!!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png" width="1456" height="891" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:891,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:187807,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>512 Images &amp; Clip Paths</figcaption></figure></div><p>At around 512 images, it’s really hard to tell the difference. Awesome! We did image projection without any rasterization!</p><p>Now for the exciting part: The SVG isn’t that big! Because we can use the `defs` of the SVG to avoid repeating the image, we only need to define each clip path!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png" width="1456" height="655" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:655,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:339186,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This is the full file for the subdivision-2 cube. The polygons are the sides of the cube. The groups and clip paths are the only things that you need for each subregion.</figcaption></figure></div><p>Here’s a table of the file size as you increase subdivisions. I _think_ some differences in the matrix calculation account for the somewhat weird scaling. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png" width="1456" height="622" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:622,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75912,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The math is fun but in the age of AI, below our pay-grade! </span><a href="https://github.com/tscircuit/simple-3d-svg" rel="">You can check out the full source code here</a><span>!</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png" width="546" height="490.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1308,&quot;width&quot;:1456,&quot;resizeWidth&quot;:546,&quot;bytes&quot;:322256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Relevant snippet from the source where we subdivide and use bilinear interpolation to find the relevant quads</figcaption></figure></div><p><span>I’m excited to flesh out this 3D renderer because 3D SVGs can make great artifacts on GitHub, we want to make it so that people can easily review changes to circuit boards made with </span><a href="https://github.com/tscircuit/tscircuit" rel="">tscircuit</a><span> in pull requests.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png" width="662" height="471.49313186813185" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1037,&quot;width&quot;:1456,&quot;resizeWidth&quot;:662,&quot;bytes&quot;:97884,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A visual snapshot test with a 3D SVG</figcaption></figure></div><p><span>I hope you enjoyed this neat little 3D trick! </span><em>Back to coding…</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla seeks to guard crash data from public disclosure (434 pts)]]></title>
            <link>https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/</link>
            <guid>44186780</guid>
            <pubDate>Wed, 04 Jun 2025 23:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/">https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/</a>, See on <a href="https://news.ycombinator.com/item?id=44186780">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[A Spiral Structure in the Inner Oort Cloud (108 pts)]]></title>
            <link>https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b</link>
            <guid>44186660</guid>
            <pubDate>Wed, 04 Jun 2025 23:22:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b">https://iopscience.iop.org/article/10.3847/1538-4357/adbf9b</a>, See on <a href="https://news.ycombinator.com/item?id=44186660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div xmlns:book="http://api.iop.org/Book/1.0/" xmlns:c="http://ns.iop.org/namespaces/content" itemprop="articleBody"><div data-mobile-collapse=""><p>The Oort cloud is a large shell of icy bodies surrounding the solar system at heliocentric distances 1000 ≲ <em>r</em> ≲ 100,000 au. These bodies are faint and not directly observed, but their existence is inferred from observations of long-period comets (LPCs; J. H. Oort <a href="#apjadbf9bbib28" id="fnref-apjadbf9bbib28">1950</a>). The so-called new LPCs, which are observed during their first perihelion passage through the inner solar system, often have the semimajor axes between 20,000 and 100,000 au.<sup><a href="#apjadbf9bfn1">5</a></sup>
 They are thought to have relatively recently evolved, due to the effects of the Galactic tide (J. Heisler &amp; S. Tremaine <a href="#apjadbf9bbib13" id="fnref-apjadbf9bbib13">1986</a>; Section <a href="#apjadbf9bs3">3</a> here), onto high-eccentricity/low-perihelion orbits. The new LPCs have a nearly isotropic distribution of orbital inclinations, suggesting that the outer Oort cloud at <em>r</em> &gt; 10,000 au is roughly spherical (see L. Dones et al. <a href="#apjadbf9bbib7" id="fnref-apjadbf9bbib7">2015</a> for a review).</p><p>Oort cloud formation dates back to early stages of the solar system some 4.6 Gyr ago (M. Duncan et al. <a href="#apjadbf9bbib9" id="fnref-apjadbf9bbib9">1987</a>; L. Dones et al. <a href="#apjadbf9bbib8" id="fnref-apjadbf9bbib8">2004</a>; D. Vokrouhlický et al. <a href="#apjadbf9bbib34" id="fnref-apjadbf9bbib34">2019</a>). First, as the outer planets cleared their orbital neighborhood, small bodies were scattered onto very eccentric orbits with perihelion distances <em>q</em> ≲ 30 au and semimajor axes <em>a</em> ≳ 1​​​​​000 au (orbital eccentricities <em>e</em> ≳ 0.97). Second, the Galactic tide raised the perihelion distances of these bodies, effectively decoupling them from planetary perturbations, and tilted their orbits. Third, encounters of the Sun with stars in the Galactic neighborhood thoroughly mixed the orbits in the outer Oort cloud, producing a relatively homogeneous and isotropic source for LPCs.<sup><a href="#apjadbf9bfn2">6</a></sup>

</p><p>Dynamical simulations reveal formation of the inner Oort cloud at 1000 &lt; <em>r</em> &lt; 10,000 au (M. Duncan et al. <a href="#apjadbf9bbib9" id="fnref-apjadbf9bbib9">1987</a>; H. F. Levison et al. <a href="#apjadbf9bbib21" id="fnref-apjadbf9bbib21">2001</a>; D. Vokrouhlický et al. <a href="#apjadbf9bbib34" id="fnref-apjadbf9bbib34">2019</a>).<sup><a href="#apjadbf9bfn3">7</a></sup>
 The inner Oort cloud forms in much the same way as the outer Oort cloud, except that the timescale on which the Galactic tide changes orbits at 1000 &lt; <em>r</em> &lt; 10,000 au is long, comparable to the age of the solar system. This explains why the inner Oort cloud is not a dominant source of LPCs (D. Vokrouhlický et al. <a href="#apjadbf9bbib34" id="fnref-apjadbf9bbib34">2019</a>): bodies from this region evolve too slowly and are ejected by planets before they can reach <em>q</em> ≲ 3 au, heat up, and become active comets (J. G. Hills <a href="#apjadbf9bbib16" id="fnref-apjadbf9bbib16">1981</a>, but see N. A. Kaib &amp; T. Quinn <a href="#apjadbf9bbib17" id="fnref-apjadbf9bbib17">2009</a>).<sup><a href="#apjadbf9bfn4">8</a></sup>
 In addition, the orbits with 1​​​​000 &lt; <em>a</em> &lt; 10,000 au, which are more strongly bound to the Sun, are less affected by stellar encounters. The inner Oort cloud is therefore often portrayed as a relatively flat disk, roughly aligned with the ecliptic (H. F. Levison et al. <a href="#apjadbf9bbib21" id="fnref-apjadbf9bbib21">2001</a>), that retained memory of its initial conditions (e.g., M. Fouchard et al. <a href="#apjadbf9bbib10" id="fnref-apjadbf9bbib10">2018</a>).</p><p>In the inner Oort cloud at 1​​​​000 &lt; <em>r</em> &lt; 10,000 au, structures in the spatial distribution of bodies can form and "freeze" over timescales comparable to the age of the solar system. This raises the question of how the inner Oort cloud would look to a distant observer and/or whether there are any diagnostic features that would facilitate its detection. Here we show that the inner Oort cloud is a slightly warped disk, roughly 15,000 au across, inclined <em>i</em> ∼ 30° to the ecliptic (nearly polar in the Galactic reference system; Galactic inclination <em>i</em><sub>G</sub> ∼ 90°). The disk, when viewed from a distance, would appear as a spiral structure with two twisted arms. The spiral structure was first identified by examining the simulation in the Hayden Planetarium in preparation for a new space show that describes and visualizes the Oort cloud.<sup><a href="#apjadbf9bfn5">9</a></sup>
 In Section <a href="#apjadbf9bs2">2</a>, we discuss the results of dynamical simulations—described in the <a href="#apjadbf9bapp1">Appendix</a>—to illustrate the inner Oort cloud structure. The analytic model of S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>; also see A. Higuchi et al. <a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>) is employed to interpret these results (Section <a href="#apjadbf9bs3">3</a>). Observational detectability is discussed in Section <a href="#apjadbf9bs4">4</a>.</p></div><div data-mobile-collapse=""><p>Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a> shows the Oort spiral as it should appear at the present epoch. The distribution of bodies in the inner Oort cloud was extracted from the Galaxy simulation (D. Nesvorný et al. <a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>; see the <a href="#apjadbf9bapp1">Appendix</a> here for a description of the simulation setup) and was plotted from a viewpoint of a distant observer. The observer is located at the intersection of the Galactic and ecliptic planes, with the Galactic plane running horizontally across the plot and the ecliptic plane tilted 60<span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn1.gif" alt="$\mathop{.}\limits^{\unicode{x000B0}}$"></span></span></span>2 to it along the main axis of the spiral. The Sun is in the plot's center. The main axis of the spiral is aligned with the ecliptic—the ends of the spiral are twisted away from the ecliptic. We note that Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a> does not incorporate any information about the actual detectability of the inner Oort cloud (for that, see Section <a href="#apjadbf9bs4">4</a>).</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf1" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf1" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf1_lr.jpg" data-toolbar-title="Figure 1."><figure><div><p><img alt="Figure 1. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf1_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf1_lr.jpg"></p></div><figcaption><p><strong>Figure 1.</strong>&nbsp;The spiral structure in the inner Oort cloud viewed by a distant observer along the Galactic node direction (intersection of the Galactic and ecliptic planes). The distribution of bodies was obtained from the Galaxy simulation in D. Nesvorný et al. (<a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>). To show things clearly, here we isolated the inner Oort cloud from the more spherical outer Oort cloud by plotting bodies, 34,000 in total, with <em>a</em>&nbsp;&lt;&nbsp;5000 au. If the outer Oort cloud were plotted in the figure, it would appear as a large, roughly spherical envelope of the spiral. The classical Kuiper Belt with <em>r</em>&nbsp;&lt;&nbsp;100 au is not shown here—it would appear as a doughnut-like central concentration of bodies aligned with the ecliptic plane.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf1" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf1_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf1" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf1_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>We verified that the spiral exists in all our previous simulations with the Galactic tide independently of whether the effects of the stellar cluster were included (D. Nesvorný et al. <a href="#apjadbf9bbib27" id="fnref-apjadbf9bbib27">2017</a>, <a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>; D. Vokrouhlický et al. <a href="#apjadbf9bbib34" id="fnref-apjadbf9bbib34">2019</a>). The spiral is long-lived: it emerges in the first hundreds of megayears after the formation of the solar system and persists over billions of years. The same spiral structure appears in simulations with different sequences of stellar encounters, indicating that the spiral is not related to stellar encounters. Instead, as we establish below, the spiral is a consequence of the Galactic tide. To simulate the Galactic tide, the Galaxy simulation in D. Nesvorný et al. (<a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>) adopted the mass density of 0.15 <em>M</em><sub>Sun</sub> pc<sup>−3</sup> in the solar neighborhood. By comparing the results of simulations with different mass densities, we found that the spiral becomes smaller (larger) for higher (lower) stellar densities in the solar neighborhood. This suggests a direct involvement of the Galactic tide.</p><p>We studied formation of the Oort spiral by inspecting orbital histories of bodies contributing to the spiral. They were first scattered to 1000 &lt; <em>a</em> &lt; 10,000 au by migrating planets (Neptune, Uranus, and Saturn). The scattered orbits had relatively low orbital inclinations with respect to the ecliptic (<em>i</em> ≲ 30°), low perihelion distances (<em>q</em> ≲ 30 au), and high eccentricities (<em>e</em> ≳ 0.97). The initial nodal longitude and initial perihelion argument of orbits in the ecliptic frame were chosen to be uniformly random. Consequently, the population of scattered bodies initially appeared as a relatively thin disk near the ecliptic, and this—when looked at by an observer in the ecliptic node<sup><a href="#apjadbf9bfn6">10</a></sup>
—formed the main axis of the Oort spiral in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>.</p><p>The Galactic tide is important for orbits with <em>a </em>&gt; 1000 au. As the ecliptic plane is inclined ≃60<span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn2.gif" alt="$\mathop{.}\limits^{\unicode{x000B0}}$"></span></span></span>2 with respect to the Galactic plane, bodies scattered near the ecliptic to <em>a</em> &gt; 1​​​​​​000 au had large orbital inclinations in the Galactic frame (<em>i</em><sub>G</sub> ∼ 60°) and were subject to Kozai cycles (Y. Kozai <a href="#apjadbf9bbib19" id="fnref-apjadbf9bbib19">1962</a>; we discuss the Kozai cycles in detail in Section <a href="#apjadbf9bs3">3</a>). The Kozai cycles produce anticorrelated oscillations of <em>e</em> and <em>i</em><sub>G</sub> that are accompanied by a slow evolution of perihelion argument <em>ω</em><sub>G</sub> (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf2">2</a>). The fate of an orbit then depends on the initial value of <em>ω</em><sub>G</sub>. If 0° &lt; <em>ω</em><sub>G</sub> &lt; 90° or 180° &lt; <em>ω</em><sub>G</sub> &lt; 270°, the orbital eccentricity increases, the perihelion distance drops, and the body tends to be scattered by planets away from the 1​​​​000 &lt; <em>a</em> &lt; 10,000 au region (often to interstellar space).</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf2" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf2" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf2_lr.jpg" data-toolbar-title="Figure 2."><figure><div><p><img alt="Figure 2. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf2_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf2_lr.jpg"></p></div><figcaption><p><strong>Figure 2.</strong>&nbsp;The orbital elements of bodies in the inner Oort cloud (<em>a</em>&nbsp;∼&nbsp;3000 au). Most orbits in the inner Oort cloud are expected to have <em>ω</em><sub>G</sub>&nbsp;=&nbsp;70°–180° or <em>ω</em><sub>G</sub>&nbsp;=&nbsp;250°–360°. These two broad concentrations in <em>ω</em><sub>G</sub> appear as two spiral arms in Figures <a href="#apjadbf9bf1">1</a> and <a href="#apjadbf9bf9">9</a>. The red curves are the evolutionary tracks of <em>e</em> and <em>ω</em><sub>G</sub> computed from the analytic model for <em>a</em> = 3000 au and <em>J</em><sub><em>z</em></sub>&nbsp;=&nbsp;0.070534 (see Section <a href="#apjadbf9bs3">3</a>).</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf2" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf2_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf2" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf2_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>If, instead, 90° &lt; <em>ω</em><sub>G</sub> &lt; 180° or 270° &lt; <em>ω</em><sub>G</sub> &lt; 360°, the orbital eccentricity initially decreases, and the orbit can decouple from planetary perturbations (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf2">2</a>). These orbits can be stable over long timescales. They stay in the inner Oort cloud and continue to evolve by Kozai cycles. As <em>e</em> decreases in a Kozai cycle, <em>i</em><sub>G</sub> increases from the initial value of <em>i</em><sub>G</sub> ∼ 60°, and the orbit becomes nearly polar in the Galactic frame. This is when the orbital changes due to Galactic tide become exceedingly slow and orbits freeze (Section <a href="#apjadbf9bs3">3</a>). This explains the twisted arms of the Oort spiral in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a> that reach away from the ecliptic plane toward the Galactic poles.</p><p>The ascending node of the ecliptic on the Galactic plane is located near the Galactic longitude <em>l</em> = 186° (and, by definition, at the Galactic latitude <em>b</em> = 0). In the Galaxy simulation, we observe that orbits in the inner Oort cloud (1000 &lt; <em>a</em> &lt; 10,000 au) start with the nodal longitude Ω<sub>G</sub> ≃ 186°. Subsequently, if <em>ω</em><sub>G</sub> has the right value (see above) and the orbit decouples from planetary perturbations, Ω<sub>G</sub> slowly rotates in the retrograde sense (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>). Thus, over time, orbits move away from the ecliptic plane, and their initially nearly perfect alignment is (slightly) smeared. The observer looking at the structure along the ecliptic node will then not see the disk exactly edge on, which is the situation shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>. For example, for <em>a</em> ∼ 3000 au, the characteristic rotation of Ω<sub>G</sub> over 4.6 Gyr is broadly centered at ∼30° (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>).</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf3" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf3" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf3_lr.jpg" data-toolbar-title="Figure 3."><figure><div><p><img alt="Figure 3. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf3_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf3_lr.jpg"></p></div><figcaption><p><strong>Figure 3.</strong>&nbsp;The orbital elements of bodies in the inner Oort cloud (<em>a</em>&nbsp;∼&nbsp;3000 au). The figure shows that bodies in the inner Oort cloud are expected to have nearly polar orbits in the Galactic frame (<em>i</em><sub>G</sub>&nbsp;=&nbsp;75°–90°) and orbital planes that are only slightly rotated away from the ecliptic (Ω<sub>G</sub>&nbsp;=&nbsp;120°–180°; the ascending node of the ecliptic is at the Galactic longitude <em>l</em>&nbsp;=&nbsp;186°—the vertical red line). This implies a disk-like structure in physical space. The blue star is the initial location of bodies scattered by planets near the ecliptic plane.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf3" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf3_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf3" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf3_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure></div><div data-mobile-collapse=""><p>Here we adopt the analytic model from S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>), S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib6" id="fnref-apjadbf9bbib6">2006</a>), and A. Higuchi et al. (<a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>). The model starts with the Galactic tidal potential from J. Heisler &amp; S. Tremaine (<a href="#apjadbf9bbib13" id="fnref-apjadbf9bbib13">1986</a>) and neglects all components of the tide except the (largest) one that is perpendicular to the Galactic plane. The gravitational potential of planets is neglected as well. This is an appropriate simplification for <em>a</em> &gt; 1000 au, where the effect of Galactic tide is more important than planets (M. Saillenfest et al. <a href="#apjadbf9bbib31" id="fnref-apjadbf9bbib31">2019</a>). The corresponding Hamiltonian is averaged over the orbital period. Consequently, the semimajor axis <em>a</em> and the <em>z</em> component of (scaled) angular momentum </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn1.gif" alt="Equation (1)"></span></span></p><p>where <em>i</em><sub>G</sub> is the inclination with respect to the Galactic frame, are constant. As <em>J</em><sub><em>z</em></sub> remains constant during orbital evolution, <em>e</em> and <em>i</em><sub>G</sub> show anticorrelated oscillations defined by Equation (<a href="#apjadbf9beqn1">1</a>). The simplified Hamiltonian becomes </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn2.gif" alt="Equation (2)"></span></span></p><p>where <em>ω</em><sub>G</sub> is the argument of perihelion in the Galactic reference system. For any values <em>J</em><sub><em>z</em></sub> =  const. and <em>C</em> = const., as defined by initial conditions, the evolution of <em>e</em>, <em>i</em><sub>G</sub>, and <em>ω</em><sub>G</sub> can be obtained from Equations (<a href="#apjadbf9beqn1">1</a>) and (<a href="#apjadbf9beqn2">2</a>).</p><p>The model of S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>) and A. Higuchi et al. (<a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>) gives explicit expressions, in terms of special functions, for the time evolution of <em>e</em>, <em>i</em><sub>G</sub>, <em>ω</em><sub>G</sub>, and Ω<sub>G</sub>. For example, the evolution of eccentricity is </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn3.gif" alt="Equation (3)"></span></span></p><p>where <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn3.gif" alt="${e}_{{\rm{\min }}}$"></span></span></span> and <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn4.gif" alt="${e}_{{\rm{\max }}}$"></span></span></span> are the minimum and maximum values (both are computed from the initial conditions in S. Breiter &amp; R. Ratajczak <a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>), <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn5.gif" alt="${\rm{cn}}(\theta ,k)$"></span></span></span> is the Jacobian elliptic function, <em>θ</em>(<em>t</em>) is a linear function of time and depends on initial conditions, and the modulus <em>k</em> is a constant obtained from the initial conditions. Once <em>e</em>(<em>t</em>) is solved for, the inclination evolution can be obtained from </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn4.gif" alt="Equation (4)"></span></span></p><p>The evolution of <em>ω</em><sub>G</sub>(<em>t</em>) can then be computed from from Equation (<a href="#apjadbf9beqn2">2</a>). Finally, the evolution of the longitude of node Ω<sub>G</sub>(<em>t</em>) is </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn5.gif" alt="Equation (5)"></span></span></p><p>where Ω<sub>G,<em>i</em></sub> is the initial value of Ω<sub>G</sub>, <em>A</em><sub>2</sub> and <em>α</em> are constants that can be obtained from initial conditions (S. Breiter &amp; R. Ratajczak <a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>; A. Higuchi et al. <a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>), and Π is an incomplete elliptic integral of the third kind.</p><p>We evaluated <em>e</em>(<em>t</em>), <em>i</em><sub>G</sub>(<em>t</em>), <em>ω</em><sub>G</sub>(<em>t</em>), and Ω<sub>G</sub>(<em>t</em>) from S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>) and used it to understand the results of the more complete simulation discussed in Section <a href="#apjadbf9bs2">2</a>. Before we proceed with this interpretation, note that the Galaxy simulation in Section <a href="#apjadbf9bs2">2</a> accounted for (1) the gravitational scattering from migrating outer planets, (2) all three components of the Galactic tide, and (3) stellar encounters—none of these effects is included in the analytic model. At least some of the differences between our Galaxy simulation and the analytic model arise from the neglected effects.</p><p>We first checked on the results shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>. For that, we generated 34,000 bodies, which is the same as the number of bodies shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>, and propagated their orbits with the analytic model for 4.6 Gyr. As a proxy for bodies scattered by planets to the inner Oort cloud distances, the initial orbits were given uniform distributions with 2000 &lt; <em>a</em> &lt; 5000 au, 10 &lt; <em>q</em> &lt; 30 au, <em>i</em> &lt; 30°, and random orbital angles. The spatial distribution of bodies at <em>t</em> = 4.6 Gyr, viewed from the same direction as in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>, is shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf4">4</a>. The two plots are similar in that they show the same spiral structure. The one in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf4">4</a> is less fuzzy, with the two arms standing out more clearly. There is also a concentration of orbits with <em>i</em><sub>G</sub> ≃ 90° in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf4">4</a>, forming a vertical line that runs through the plot's center.<sup><a href="#apjadbf9bfn7">11</a></sup>

</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf4" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf4" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf4_lr.jpg" data-toolbar-title="Figure 4."><figure><div><p><img alt="Figure 4. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf4_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf4_lr.jpg"></p></div><figcaption><p><strong>Figure 4.</strong>&nbsp;The spiral structure reconstructed by analytical means from S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>). We placed 34,000 bodies onto initial orbits with 2000&nbsp;&lt;&nbsp;<em>a</em>&nbsp;&lt;&nbsp;5000 au, 10&nbsp;&lt;&nbsp;<em>q</em>&nbsp;&lt;&nbsp;30 au, and <em>i</em>&nbsp;&lt;&nbsp;30° (inclination with respect to the ecliptic frame). The initial orbital longitudes were chosen at random. The analytic formulas from S. Breiter &amp; R. Ratajczak (<a href="#apjadbf9bbib5" id="fnref-apjadbf9bbib5">2005</a>; also see A. Higuchi et al. <a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>) were used to compute the orbits at <em>t</em> = 4.6 Gyr, corresponding to the present epoch. The distribution of bodies at the present epoch was then plotted in the same way as in Figure <a href="#apjadbf9bf1">1</a>.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf4" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf4_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf4" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf4_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>To set up a simple analytic case, we adopted fixed <em>a</em> = 3000 au, <em>e</em> = 0.99, and <em>q</em> = 30 au. We assumed that scattering from the outer planets does not produce very large orbital inclinations with respect to the ecliptic, as indicated by our numerical simulations (<em>i</em> ≲ 30°; D. Nesvorný et al. <a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>). For simplicity we therefore set <em>i</em> = 0. This implies <em>i</em><sub>G</sub> = 60<span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn6.gif" alt="$\mathop{.}\limits^{\unicode{x000B0}}$"></span></span></span>2 and Ω<sub>G</sub> = 186°. With these choices, the orbital evolution only depends on the initial value of <em>ω</em><sub>G</sub> (Figures <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf5">5</a> and <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf6">6</a>). The results are closely aligned with those discussed in Section <a href="#apjadbf9bs2">2</a>. If 0° &lt; <em>ω</em><sub>G</sub> &lt; 90° or 180° &lt; <em>ω</em><sub>G</sub> &lt; 270°, the orbital eccentricity increases (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf6">6</a>), the perihelion distance drops, and the subsequent evolution would be affected by the scattering encounters with planets (not included in the analytic model). This explains why orbits with 0° &lt; <em>ω</em><sub>G</sub> &lt; 90° or 180° &lt; <em>ω</em><sub>G</sub> &lt; 270° are underrepresented in the Oort spiral.</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf5" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf5" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf5_lr.jpg" data-toolbar-title="Figure 5."><figure><div><p><img alt="Figure 5. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf5_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf5_lr.jpg"></p></div><figcaption><p><strong>Figure 5.</strong>&nbsp;The Galactic-tide-driven evolution of orbital inclination <em>i</em><sub>G</sub> and argument of perihelion <em>ω</em><sub>G</sub>, both in the Galactic reference system. The thin lines show trajectories for different values of the <em>C</em> constant (Equation (<a href="#apjadbf9beqn2">2</a>)) and <em>J</em><sub><em>z</em></sub>&nbsp;=&nbsp;0.070534 (Equation (<a href="#apjadbf9beqn1">1</a>)). To illustrate how bodies in the inner Oort cloud end up on nearly polar orbits in the Galactic frame, we started 18 bodies with <em>i</em><sub>G</sub>&nbsp;=&nbsp;60° and Ω<sub>G</sub>&nbsp;=&nbsp;186° (i.e., the initial orbits in the ecliptic plane) and different values of <em>ω</em><sub>G</sub> (equally spaced in 20° intervals; green dots on the horizontal dashed line) and let them evolve for 4.6 Gyr (thick trajectories). The red dots label the final orbits. The initial orbits had <em>a</em> = 3000 au, <em>e</em> = 0.99, and <em>q</em> = 30 au.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf5" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf5_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf5" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf5_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf6" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf6" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf6_lr.jpg" data-toolbar-title="Figure 6."><figure><div><p><img alt="Figure 6. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf6_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf6_lr.jpg"></p></div><figcaption><p><strong>Figure 6.</strong>&nbsp;The Galactic-tide-driven evolution of orbital eccentricity <em>e</em> and argument of perihelion <em>ω</em><sub>G</sub>. The thin lines show trajectories for different values of the <em>C</em> constant (Equation (<a href="#apjadbf9beqn2">2</a>)) and <em>J</em> = 0.070534 (Equation (<a href="#apjadbf9beqn1">1</a>)). The green and red dots label the initial and final orbits, respectively. See Figure <a href="#apjadbf9bf5">5</a> for additional information.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf6" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf6_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf6" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf6_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>For 90° &lt; <em>ω</em><sub>G</sub> &lt; 180° or 270° &lt; <em>ω</em><sub>G</sub> &lt; 360°, the orbital eccentricity initially decreases, so the orbit can decouple from planetary perturbations and become stable. At the same time, as <em>i</em><sub>G</sub> of the orbit increases (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf5">5</a>), the orbit becomes nearly polar and practically freezes (red dots in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf5">5</a>). This is a consequence of Equation (<a href="#apjadbf9beqn2">2</a>), where <em>C</em> → 0 when <em>i</em><sub>G</sub> → 90°, and the orbit evolution becomes exceedingly slow. Two examples of this are shown in Figure 3 in A. Higuchi et al. (<a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>). The analytic model therefore explains why orbits in the Oort spiral often have <em>i</em><sub>G</sub> = 75°–90° (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>). The slow retrograde circulation of Ω<sub>G</sub> that starts near Ω<sub>G</sub> = 186° and stalls when <em>i</em><sub>G</sub> ≃ 90° complements the picture. It explains why orbits in the inner Oort cloud often have Ω<sub>G</sub> = 120°–180° (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>).<sup><a href="#apjadbf9bfn8">12</a></sup>

</p></div><div data-mobile-collapse=""><p>The observational detection of the Oort spiral is difficult. The reflected light from large bodies in the inner Oort cloud can be detected by large telescopes. For example, S. S. Sheppard et al. (<a href="#apjadbf9bbib32" id="fnref-apjadbf9bbib32">2019</a>) used the 8.2 m Subaru telescope to discover 541132 Leleākūhonua with <em>a</em> = 1085 au, <em>e</em> = 0.94, and <em>i</em> = 11<span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn7.gif" alt="$\mathop{.}\limits^{\unicode{x000B0}}$"></span></span></span>7 (original barycentric elements). In the Galactic reference system, we have <em>i</em><sub>G</sub> = 50<span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn8.gif" alt="$\mathop{.}\limits^{\unicode{x000B0}}$"></span></span></span>4, Ω<sub>G</sub> = 166°, and <em>ω</em><sub>G</sub> = 333°, which would allow us to project 541132 Leleākūhonua in plots like those shown in Figures <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf2">2</a> and <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a> (note that the semimajor axis of 541132 is roughly 2–5 times smaller than that of the bodies shown in these figures). We infer that the orbit of 541132 Leleākūhonua must have had a more complex history than simple planet scattering plus Kozai cycles. This is because 541132 Leleākūhonua does not have nearly as polar an orbit in the Galactic frame as the bulk of inner Oort cloud objects in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>. M. Saillenfest et al. (<a href="#apjadbf9bbib31" id="fnref-apjadbf9bbib31">2019</a>) already showed that objects with <em>a</em> ∼ 1000 au are in a transition region where the effects of Galactic and planetary potentials combine to produce dynamical chaos.<sup><a href="#apjadbf9bfn9">13</a></sup>

</p><p>It may have been scattered away from the ecliptic such that the initial inclination was <em>i</em><sub>G</sub> &lt; 50°. The Kozai cycles would then plausibly produce the current orbit by decreasing <em>e</em> and increasing <em>i</em><sub>G</sub>. Complicating factors include the effects of stellar encounters, stellar cluster, and the possible planet 9 (S. S. Sheppard et al. <a href="#apjadbf9bbib32" id="fnref-apjadbf9bbib32">2019</a>). In any case, the Oort spiral is mainly contributed by bodies with large perihelion distances and <em>a</em> &gt; 2000 au. The telescopic observations would therefore need to detect objects on even more extreme orbits than 541132 Leleākūhonua, and obtain sufficient statistics for these bodies, to be able to piece together their spatial distribution. This task will have to wait for the next generation of telescopic surveys (e.g., the Vera C. Rubin Observatory).</p><p>Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a> shows the distribution of inner Oort cloud bodies on the sky as seen from the perspective of a terrestrial observer. The two clouds in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a> correspond to the two spiral arms in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a>. The maximum density occurs near the Galactic coordinates <em>l</em> = 340°, <em>b</em> = 30° and <em>l</em> = 160°, <em>b</em> = −20°. We used the multipole expansion to highlight the large-scale features. The multipole expansion of function <em>f</em> is </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn6.gif" alt="Equation (6)"></span></span></p><p>where <em>θ</em> = 90° − <em>b</em> is the colatitude and <em>ϕ</em> is the longitude, <em>a</em><sub><em>l</em>,<em>m</em></sub> is coefficients, and <em>Y</em><sub><em>l</em>,<em>m</em></sub> is the spherical harmonics. The coefficients are computed by integration, </p><p><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9beqn7.gif" alt="Equation (7)"></span></span></p><p>over the celestial sphere <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn10.gif" alt="${ \mathcal S }$"></span></span></span>, where <em>f</em>(<em>θ</em>, <em>ϕ</em>) is the number density of objects on the sky, <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn11.gif" alt="${Y}_{l,m}^{* }$"></span></span></span> is the complex conjugate of <em>Y</em><sub><em>l</em>,<em>m</em></sub>, and <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn12.gif" alt="$d{\rm{\Omega }}=\sin \theta d\theta d\phi $"></span></span></span> is the infinitesimal solid angle.</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf7" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf7" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf7_lr.jpg" data-toolbar-title="Figure 7."><figure><div><p><img alt="Figure 7. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf7_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf7_lr.jpg"></p></div><figcaption><p><strong>Figure 7.</strong>&nbsp;The distribution of inner Oort cloud objects on the sky. We collected bodies with 1000&nbsp;&lt;&nbsp;<em>a</em>&nbsp;&lt;&nbsp;3000 au in the Galaxy simulation (Section <a href="#apjadbf9bs2">2</a>) and show them here from the viewpoint of a terrestrial observer (upper panel). The red lines in the upper panel are lines of constant ecliptic latitudes <em>β</em>&nbsp;=&nbsp;−20°, 0°, and 20°. The bottom panel shows the multipole expansion for <em>l</em> ≤ 3, with the warmer colors indicating higher number densities. The scaling is arbitrary.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf7" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf7_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf7" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf7_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>We find that the inner Oort cloud distribution is dominated by the quadratic term with <em>l</em> = 2 and <em>m</em> = 2 (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a>, bottom panel). For comparison, the distribution of Kuiper Belt objects (KBOs) and scattered disk objects (SDOs) with <em>a</em> &lt; 1​​​​​​000 au is more continuous along the ecliptic (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf8">8</a>). When represented by the multipole expansion, the distribution of KBOs/SDOs appears to be dominated by two quadratic terms with <em>l</em> = 2: <em>m</em> = −1 and <em>m</em> = 2. This offers a criterion for distinguishing the inner Oort cloud objects from KBOs/SDOs: for them the multipole expansion should be dominated by the <em>a</em><sub>2,2</sub> term (∣<em>a</em><sub>2,2</sub>∣ ∼ 2∣<em>a</em><sub>2,−1</sub>∣). For a more continuous distribution such as the one shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf8">8</a>, ∣<em>a</em><sub>2,−1</sub>∣ ≳ ∣<em>a</em><sub>2,2</sub>∣.</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf8" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf8" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf8_lr.jpg" data-toolbar-title="Figure 8."><figure><div><p><img alt="Figure 8. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf8_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf8_lr.jpg"></p></div><figcaption><p><strong>Figure 8.</strong>&nbsp;The distribution of KBOs/SDOs on the sky. We collected bodies with 30&nbsp;&lt;&nbsp;<em>a</em>&nbsp;&lt;&nbsp;1000 au in the Galaxy simulation (Section <a href="#apjadbf9bs2">2</a>) and show them here from the viewpoint of a terrestrial observer (upper panel). The red lines in the upper panel are lines of constant ecliptic latitudes <em>β</em>&nbsp;=&nbsp;−20°, 0°, and 20°. The bottom panel shows the multipole expansion for <em>l</em> ≤ 3, with the warmer colors indicating higher number densities. The scaling is arbitrary.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf8" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf8_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf8" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf8_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure><p>Detecting thermal emission from small dust particles in the inner Oort cloud is similarly difficult. For shorter wavelengths, <em>λ</em> ≲ 100 <em>μ</em>m, the large-scale thermal emission is dominated by the zodiacal light (e.g., D. Nesvorný et al. <a href="#apjadbf9bbib24" id="fnref-apjadbf9bbib24">2010</a>; Planck Collaboration et al. <a href="#apjadbf9bbib29" id="fnref-apjadbf9bbib29">2014</a>). For longer wavelengths, <em>λ</em> ≳ 500 <em>μ</em>m, the thermal emission is dominated by the cosmic microwave background (CMB) and Galactic sources (e.g., Planck Collaboration et al. <a href="#apjadbf9bbib30" id="fnref-apjadbf9bbib30">2020</a>). The CMB shows an anomalous quadrupole term that is somewhat smaller than the expectations from the best-fit cosmological model (D. N. Spergel et al. <a href="#apjadbf9bbib33" id="fnref-apjadbf9bbib33">2003</a>; Planck Collaboration et al. <a href="#apjadbf9bbib30" id="fnref-apjadbf9bbib30">2020</a>). The quadrupole moment expected from the inner Oort cloud emission has nearly the opposite orientation on the sky (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a>) to that of the CMB quadrupole and would therefore—at least in principle—act to decrease the CMB quadrupole. In practice, however, we estimate that the amplitude of the inner Oort quadrupole should be some 3 orders below that of the CMB quadrupole (∼10 Jy sr<sup>−1</sup> for Oort versus​​​​​ ∼10 kJy sr<sup>−1</sup> for CMB at frequencies <em>ν</em> ∼ 160 GHz) and therefore negligible.<sup><a href="#apjadbf9bfn10">14</a></sup>

</p><p>The best chances to detect thermal emission from the inner Oort cloud are near wavelengths ∼300–400 <em>μ</em>m (E. J. Baxter et al. <a href="#apjadbf9bbib1" id="fnref-apjadbf9bbib1">2018</a>). This is where the spectral radiance of both the zodiacal and CMB emissions is reduced by a factor of ≳10<sup>3</sup> from their peak values. The small particles in the inner Oort cloud, ∼10–100 <em>μ</em>m, would have temperatures <em>T</em> ∼ 10 K and efficiently emit at these wavelengths. To isolate this component from the zodiacal cloud, one would have to search for an anomalous quadrupole signature corresponding to a distribution that is not completely smooth in the ecliptic longitude and peaks near the expected directions (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a>). To separate it from the CMB, one would have to demonstrate that the amplitude of the CMB quadrupole moment decreases—relative to lower and higher multipole moments—for these intermediate wavelengths.</p><p>The high-frequency instrument on board the Planck satellite observed at 857 GHz, which translates to <em>λ</em> ≃ 350 <em>μ</em>m. A careful analysis of these observations could perhaps unveil some interesting features. But even here the prospects of the inner Oort cloud detection are feeble. A complication arises because the thermal signal from the inner Oort particles scales with 1/<em>r</em><sup>2</sup> and temperature <span xmlns:xlink="http://www.w3.org/1999/xlink"><span><span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=" data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bieqn13.gif" alt="$T\simeq 280/\sqrt{r}$"></span></span></span>. Thus, even though these particles spend more time near aphelion, where they contribute to the Oort spiral, they are much more easily detectable when they approach the terrestrial observer during their perihelion passage. This creates a strong bias in detectability of particles with small perihelion distances; these particles have high eccentricities and a different distribution on the sky than the one shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf7">7</a>. Unfortunately, we do not have enough statistics in the Galaxy simulation to accurately predict the expected signature from this population.</p><p>Finally, we briefly comment on E. J. Baxter et al. (<a href="#apjadbf9bbib1" id="fnref-apjadbf9bbib1">2018</a>), who used the high-frequency Planck detector (545 and 857 GHz) to search for signatures of exo-Oort clouds around nearby (hot) stars. They found several candidates but pointed out that the rate of false positives in these observations is expected to be relatively high. They argued that future CMB surveys and targeted observations with far-infrared and millimeter wavelength telescopes have the potential to detect exo-Oort clouds or other extended sources of thermal emission beyond ∼1000 au from the parent stars. Here we point out that resolving a spiral-like signature such as the one shown in Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a> or Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf9">9</a> would potentially represent more definitive evidence for the existence of the (inner) exo-Oort cloud around a nearby star. For the exo-Oort spiral to form, there needs to be a planetary system capable of ejecting small bodies to ∼1000–10,000 au from its host star, and the orbital plane of the ejected bodies needs to be significantly inclined to the Galactic plane for the Kozai cycles to happen.</p><figure xmlns:xlink="http://www.w3.org/1999/xlink" id="apjadbf9bf9" tabindex="-1" role="group" data-toolbar-type="figure" data-toolbar-link="apjadbf9bf9" data-toolbar-img="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf9_lr.jpg" data-toolbar-title="Figure 9."><figure><div><p><img alt="Figure 9. Refer to the following caption and surrounding text." data-src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf9_lr.jpg" src="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf9_lr.jpg"></p></div><figcaption><p><strong>Figure 9.</strong>&nbsp;A top view of the Oort spiral from the perspective of a distant observer at the Galactic plane. The plot shows the same bodies as in Figure <a href="#apjadbf9bf1">1</a>, but the view is rotated by 90° around the Galactic pole. The yellow curves highlight the two spiral arms.</p><p>Download figure:</p><span><a id="wd-jnl-art-btn-std-img-apjadbf9bf9" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf9_lr.jpg">Standard image
					</a><a id="wd-jnl-art-btn-hires-img-apjadbf9bf9" href="https://content.cld.iop.org/journals/0004-637X/983/1/74/revision1/apjadbf9bf9_hr.jpg">High-resolution image
					</a></span></figcaption></figure></figure></div><div data-mobile-collapse=""><p>We used numerical simulations and analytical methods to demonstrate that the inner Oort cloud is a warped disk with a spiral structure roughly 15,000 au in length (Figures <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf1">1</a> and <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf9">9</a>). The spiral forms when small bodies are scattered by planets to 1000–10,000 au and orbitally evolve by the Galactic tide. At 1000–10,000 au, where the dynamical timescales are comparable to the age of the solar system, the Galactic tide acts to (a) raise perihelion distances and decouple bodies from planetary perturbations, (b) rotate orbital planes such that they evolve to become nearly perpendicular to the Galactic plane (and Ω<sub>G</sub> = 120°–180°; Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf3">3</a>), and (c) favor long-term stability of orbits with 90° &lt; <em>ω</em><sub>G</sub> &lt; 180° and 270° &lt; <em>ω</em><sub>G</sub> &lt; 360°. Item (b) implies that the inner Oort cloud would look like a disk to a distant observer. Item (c) implies two preferred directions for orbits distributed in the disk. As the preferred directions depend on the period of Kozai oscillations (Section <a href="#apjadbf9bs3">3</a> and A. Higuchi et al. <a href="#apjadbf9bbib15" id="fnref-apjadbf9bbib15">2007</a>), which in turn depends on the semimajor axis, the inner Oort cloud bodies appear to be concentrated in two spiral arms (Figure <a xmlns:xlink="http://www.w3.org/1999/xlink" href="#apjadbf9bf9">9</a>).</p><p>In retrospect, the existence of the Oort cloud spiral could have been inferred from Figure 3 in M. Fouchard et al. (<a href="#apjadbf9bbib10" id="fnref-apjadbf9bbib10">2018</a>), where the nonuniformity of orbital angles corresponds to the spiral structure reported here. The structure was not present in Figure 1 of M. Fouchard et al. (<a href="#apjadbf9bbib10" id="fnref-apjadbf9bbib10">2018</a>), which corresponds to an initially isotropic Oort cloud, showing that the spiral structure is indeed linked to the initial state of the Oort cloud (i.e., orbital scattering of bodies from the outer planet region along the ecliptic). Additionally, M. Fouchard et al. (<a href="#apjadbf9bbib10" id="fnref-apjadbf9bbib10">2018</a>, <a href="#apjadbf9bbib11" id="fnref-apjadbf9bbib11">2023</a>) identified a related group of LPCs (B3) with properties that would reflect the structure of the inner Oort cloud—the so-called "empty ecliptic" feature; this feature was identified in LPC catalogs (M. Fouchard et al. <a href="#apjadbf9bbib11" id="fnref-apjadbf9bbib11">2023</a>). In this sense, the Oort cloud spiral has (indirectly) been detected.</p><p>Direct observational detection of the Oort spiral is difficult. Either (i) this structure can be pieced together from detection of a large number of objects with <em>a</em> &gt; 1000 au and <em>q</em> &gt; 30 au, or (ii) the thermal emission from small particles in the Oort spiral will be separated from various foreground and background sources. As for (ii), perhaps the best chance is to scrutinize emission at intermediate wavelengths (<em>λ</em> ∼ 100–500 <em>μ</em>m; e.g., Planck's 857 GHz detector) and demonstrate excess emission that is not fully continuous in ecliptic longitude (i.e., not from the zodiacal cloud) and deviates in some significant way from the low-degree multipoles observed at longer wavelengths (i.e., not a CMB quadrupole). Observational detection of Oort spirals around Milky Way stars is similarly challenging (E. J. Baxter et al. <a href="#apjadbf9bbib1" id="fnref-apjadbf9bbib1">2018</a>).</p></div><p>The Galaxy simulation was performed on the NASA Pleiades Supercomputer. We thank the NASA NAS computing division for continued support. The work of DN was funded by the NASA EW program. D.V. acknowledges support from the grant 25-16507S of the Czech Science Foundation. J.F., J.P., and C.E. acknowledge the support from NASA grant #80NSSC23K0602.</p><div data-mobile-collapse=""><p>Here we make use of the dynamical model from D. Nesvorný et al. (<a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>). To start with, we disregard their cases with the stellar cluster and focus on the simulation called Galaxy. This simulation included the (1) migration model for the outer planets, (2) effects of planet scattering on disk planetesimals, and (3) Galactic potential and stellar encounters. The model results were calibrated on Dark Energy Survey (DES) detections of objects in the trans-Neptunian region (P. H. Bernardinelli et al. <a href="#apjadbf9bbib3" id="fnref-apjadbf9bbib3">2022</a>). Here is a brief description of these components:</p><p>(1)<em> Migration model.</em> The numerical simulations consisted of tracking the orbits of the four giant planets (Jupiter to Neptune) and a large number of planetesimals. Uranus and Neptune were initially placed inside of their current orbits and were migrated outward. The <tt>swift_rmvs4</tt> code, part of the Swift <em>N</em>-body integration package (H. F. Levison &amp; M. J. Duncan <a href="#apjadbf9bbib22" id="fnref-apjadbf9bbib22">1994</a>), was used to follow all orbits. The code was modified to include artificial forces that mimic the radial migration and damping of planetary orbits. The migration history of planets was informed by the best models of planetary migration/instability. Specifically, they adopted the migration model s10/30j from D. Nesvorný et al. (<a href="#apjadbf9bbib26" id="fnref-apjadbf9bbib26">2020</a>) that worked well to satisfy many constraints. See that work for a detailed description of the migration parameters (e.g., migration <em>e</em>-fold timescale <em>τ</em> = 10 Myr for <em>t</em> &lt; 10 Myr and instability at <em>t</em> = 10 Myr). The migration model also accounted for the jitter that Neptune's orbit experienced due to close encounters with massive bodies (D. Nesvorný &amp; D. Vokrouhlický <a href="#apjadbf9bbib25" id="fnref-apjadbf9bbib25">2016</a>).</p><p>(2)<em> Planetesimal Disk.</em> The simulations included one million disk planetesimals distributed from 4 au to beyond 30 au. Such a high resolution was needed to obtain good statistics for the Oort cloud. The initial surface density of disk planetesimals was assumed to follow the truncated power-law profile from D. Nesvorný et al. (<a href="#apjadbf9bbib26" id="fnref-apjadbf9bbib26">2020</a>; also see R. S. Gomes et al. <a href="#apjadbf9bbib12" id="fnref-apjadbf9bbib12">2004</a>). The step in the surface density at 30 au was parameterized by the contrast parameter <em>c</em> ∼ 10<sup>3</sup>, which is simply the ratio of surface densities on either side of 30 au (the planetesimals with initial <em>a</em> &gt; 30 au are not an important source for the Oort cloud). The initial eccentricities and inclinations of orbits were set according to the Rayleigh distribution with scale parameters <em>σ</em><sub><em>e</em></sub> = 0.1 and <em>σ</em><sub><em>i</em></sub> = 0.05. The disk bodies were assumed to be massless such that their gravity did not interfere with the migration/damping routines.</p><p>(3)<em> Galactic potential and stellar encounters.</em> The Galaxy was assumed to be axisymmetric, and the Sun followed a circular orbit in the Galactic midplane (the Sun's migration in the Galaxy was not included; N. A. Kaib et al. <a href="#apjadbf9bbib18" id="fnref-apjadbf9bbib18">2011</a>). The Galactic tidal acceleration was taken from J. Heisler &amp; S. Tremaine (<a href="#apjadbf9bbib13" id="fnref-apjadbf9bbib13">1986</a>; see also P. Wiegert &amp; S. Tremaine <a href="#apjadbf9bbib35" id="fnref-apjadbf9bbib35">1999</a>; H. F. Levison et al. <a href="#apjadbf9bbib21" id="fnref-apjadbf9bbib21">2001</a>). The stellar mass density in the solar neighborhood was set to <em>ρ</em><sub>0</sub> = 0.15 <em>M</em><sub>⊙</sub> pc<sup>−3</sup>. The simulations accounted for the effect of stellar encounters. The stellar mass and number density of different stellar species were computed from J. Heisler et al. (<a href="#apjadbf9bbib14" id="fnref-apjadbf9bbib14">1987</a>). The stars were released and removed at the heliocentric distance of 1 pc (206,000 au). For each species, the velocity distribution was approximated by the isotropic Maxwell–Boltzmann distribution. The dynamical effect of passing molecular clouds was ignored.</p><p>(4)<em> Calibration on observations.</em> The Galaxy simulation was run over 4.6 Gyr, at which point the orbital distribution of bodies in the trans-Neptunian region was compared with DES observations (P. H. Bernardinelli et al. <a href="#apjadbf9bbib3" id="fnref-apjadbf9bbib3">2022</a>). DES covered a contiguous 5000 deg<sup>2</sup> of the southern sky between 2013 and 2019, with the majority of the imaged area being at high ecliptic latitudes. The search for outer solar system objects yielded 812 KBOs with well-characterized discovery biases, including over 200 SDOs with <em>a</em> &gt; 50 au. The DES survey simulator<sup><a href="#apjadbf9bfn11">15</a></sup>
 (P. H. Bernardinelli et al. <a href="#apjadbf9bbib3" id="fnref-apjadbf9bbib3">2022</a>) was used to bias the model in the same way as the data. D. Nesvorný et al. (<a href="#apjadbf9bbib23" id="fnref-apjadbf9bbib23">2023</a>) made use of DES observations to calibrate the magnitude distribution of trans-Neptunian objects and establish that the dynamical model results were consistent with DES observations. The model fidelity was previously tested from observations of short-period comets (D. Nesvorný et al. <a href="#apjadbf9bbib27" id="fnref-apjadbf9bbib27">2017</a>) and LPCs (D. Vokrouhlický et al. <a href="#apjadbf9bbib34" id="fnref-apjadbf9bbib34">2019</a>).</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[parrot.live (156 pts)]]></title>
            <link>https://github.com/hugomd/parrot.live</link>
            <guid>44186536</guid>
            <pubDate>Wed, 04 Jun 2025 23:05:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hugomd/parrot.live">https://github.com/hugomd/parrot.live</a>, See on <a href="https://news.ycombinator.com/item?id=44186536">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:hugomd/parrot.live" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="1TOfisRGBuMIxwqNhD6YyAfQ8mgo92Mfkb5Pz7O4AYqxsAMVi9O9LuK2-7p_y3VqSginL0Sr9jdn-IluwNf__g" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="hugomd/parrot.live" data-current-org="" data-current-owner="hugomd" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=hugomd%2Fparrot.live" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/hugomd/parrot.live&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="7ead61428bf63824eeebfc03ae5a424d0d3faab812d2b9084df85a94d7e3620f" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-0f2cf06b-dc7e-47c4-a05d-b3309a4a540a" for="icon-button-2bdf1e9c-622e-4583-b00e-413f92ec1482" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.8edda24384d5c8bf99ee.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs and Elixir: Windfall or Deathblow? (151 pts)]]></title>
            <link>https://www.zachdaniel.dev/p/llms-and-elixir-windfall-or-deathblow</link>
            <guid>44186496</guid>
            <pubDate>Wed, 04 Jun 2025 23:00:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zachdaniel.dev/p/llms-and-elixir-windfall-or-deathblow">https://www.zachdaniel.dev/p/llms-and-elixir-windfall-or-deathblow</a>, See on <a href="https://news.ycombinator.com/item?id=44186496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Will “AI” take our jobs? Will it </span><a href="https://en.wikipedia.org/wiki/Enshittification" rel="">enshittify</a><span> our profession, transitioning us into managers for stupid robots? Does it dream of electric sheep? Today I’d like to introduce a new fear and, perhaps, throw you a rope with which to pull yourself out. </span></p><p>If LLMs turn out to be a fad and you’re reading this after they jump the shark, please accept this article as a historical artifact from a time when we entertained the idea that fancy autocomplete might eat the world.</p><p><span>Aside, for those who don’t know what </span><a href="http://elixir-lang.org/" rel="">Elixir</a><span> is: it’s the </span><a href="https://discord.com/blog/how-discord-handles-push-request-bursts-of-over-a-million-per-minute-with-elixirs-genstage" rel="">best</a><span> </span><a href="https://elixir-lang.org/blog/2023/03/09/embedded-and-cloud-elixir-at-sparkmeter/" rel="">general</a><span> </span><a href="https://underjord.io/elixir-as-competitive-advantage.html?utm_source=chatgpt.com" rel="">purpose</a><span> </span><a href="https://medium.com/coryodaniel/from-erverless-to-elixir-48752db4d7bc" rel="">language</a><span> </span><a href="https://moz.com/devblog/moz-analytics-db-free" rel="">in</a><span> </span><a href="https://www.zachdaniel.dev/p/serialization-is-the-secret?r=f3smb&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">the</a><span> </span><a href="https://curiosum.com/blog/elixir-programming-language-guide#:~:text=What%20is%20Elixir%20and%20why,%2C%20and%20fault%2Dtolerant%20applications." rel="">world</a><span> </span><a href="https://paraxial.io/blog/elixir-savings" rel="">and</a><span> </span><a href="https://dockyard.com/blog/2025/04/24/elixir-success-story-clarus-r-d-rebuilds-smarter?utm_source=chatgpt.com" rel="">if</a><span> </span><a href="https://elixir-lang.org/blog/2021/11/10/embracing-open-data-with-elixir-at-the-ministry-of-ecological-transition-in-france/" rel="">you</a><span> </span><a href="https://elixir-lang.org/blog/2025/03/25/cyanview-elixir-case/" rel="">haven’t</a><span> </span><a href="https://www.thegreatcodeadventure.com/an-elixir-adoption-success-story/?utm_source=chatgpt.com" rel="">heard</a><span> </span><a href="https://www.erlang-solutions.com/blog/why-elixir-is-the-programming-language-you-should-learn-in-2024/" rel="">of</a><span> </span><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ" rel="">it</a><span> </span><a href="https://elixir-lang.org/blog/2025/01/21/remote-elixir-case/?utm_source=chatgpt.com" rel="">wake</a><span> </span><a href="https://elixir-lang.org/blog/2020/11/17/real-time-collaboration-with-elixir-at-slab/" rel="">up</a><span> </span><a href="https://www.monterail.com/blog/famous-companies-using-elixir?utm_source=chatgpt.com" rel="">and</a><span> </span><a href="https://elixir-lang.org/blog/2021/04/02/marketing-and-sales-intelligence-with-elixir-at-pepsico/" rel="">smell</a><span> </span><a href="https://alembic.com.au/case-studies/transforming-healthcare-communication-with-elixir-and-ash-framework" rel="">the</a><span> </span><a href="http://ash-hq.org/" rel="">bacon</a><span>.</span></p><p>The argument goes like so: Now that you have a bunch of folks who are “vibe coding”, they will just do whatever ChatGPT or Claude tell them to do. They will also be evaluating things almost solely against “how well did my ‘AI’ do with it” for making decisions going forward. For Elixir, this sounds like bad news bears. Here is a portion of a very long-winded response from ChatGPT when I asked it how I could “build a website like Amazon”.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png" width="510" height="586.7179487179487" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1346,&quot;width&quot;:1170,&quot;resizeWidth&quot;:510,&quot;bytes&quot;:224964,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdbc118a7-036d-4a73-a1bb-cf677f00cd1a_1170x1346.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Oh no! I in my ultimate wisdom know that this is nowhere close to the best stack for the job. But the noobs won’t know that at all! Let’s say instead they’ve heard of Elixir on a blog post, or they saw this article topping the hacker news charts 🤞 (hi mom!). </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png" width="1318" height="198" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:198,&quot;width&quot;:1318,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29090,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc936c91f-a0d4-4d4d-8c4d-00ef464c2a08_1318x198.png 1456w" sizes="100vw"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png" width="1456" height="287" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:287,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:104325,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe3a2b6e-9eac-490b-b270-ac1830dfda98_1854x366.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I’m in this unique situation where time-to-market happens to matter. Also what do you mean “learning curve”, you’re supposed to do the learning for me. Thanks Chat, you really helped me dodge a bullet here, guess I’ll stick with Node &amp; Next.js, thanks! 🥰</p><p><span>Mixing metaphors like a boss. While I’m sure the above scenario will play out countless times, at the end of the day this </span><strong>isn’t new</strong><span>. Theoretically this all goes one of two ways.</span></p><ol><li><p>“AI” is able to overcome the fundamental issues with any tool, thus equalizing effectively all programming languages.</p></li><li><p>Vibe coders &amp; new programmers working with LLMs hit the same walls that drove Elixir to exist in the first place.</p></li></ol><p>In my opinion, both of these realities are “good news” in a way. In scenario 1, we’ve got magical mystery robots and they’ll just create the one programming language to rule them all and now we all have infinite automation in our pockets. Let’s talk about scenario 2, the one I find far more likely.</p><p>It’s no surprise that these things will mirror popular opinion. We can finagle our way there. This is a fresh prompt.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png" width="1286" height="204" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:204,&quot;width&quot;:1286,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:115220,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc659d9be-a56c-4b5f-8ff1-67278bee1ee7_1286x204.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>After giving me a bunch of sensible recommendations like using a CDN, horizontally scaling, using background jobs, migrating from Vercel to ECS, and scrolling past the recommendations to rewrite in Go and Rust, we finally found what we were looking for 🥹.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png" width="880" height="374" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:374,&quot;width&quot;:880,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:68622,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6340b5db-0b37-42e8-a7c8-41ce1aebd703_880x374.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This brings us to a crucial realization: if LLMs can successfully guide someone toward Elixir when the use case demands it, then perhaps the real challenge isn't LLM bias toward mainstream tools - it's making sure LLMs can work effectively with our tools to enable developers to make that choice.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png" width="958" height="140" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:140,&quot;width&quot;:958,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17370,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3af91a5c-7911-4b0c-8898-50eccae49074_958x140.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Alright, I’m up and running. Not sure how this would cost someone $10k on Vercel but I’m sure someone could find a way.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png" width="1456" height="625" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:625,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:166315,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a85c36-18dd-4096-ba11-131ff5285276_1802x774.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So let's test this theory. If I take Claude's advice and decide to explore Elixir, how well can it help me actually build something?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png" width="960" height="128" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:128,&quot;width&quot;:960,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:15759,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc8be0157-ed49-4c12-89a3-e687ff8cdde2_960x128.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Not only does it work, but it does it effectively the same way that I personally would have done it if given the same task.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png" width="890" height="662" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:662,&quot;width&quot;:890,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:66249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F063f4386-fa9e-4507-abe7-b33644e3e5d5_890x662.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>LLMs are quite good at translation. Where they often fall down with invention, this kind of thing is their bread and butter. It even kept it compatible with the node server I started with.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png" width="942" height="470" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:470,&quot;width&quot;:942,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109130,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47877ae0-f55a-4967-8cdd-9edbdda1580e_942x470.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Plus some goodies:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png" width="946" height="598" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:598,&quot;width&quot;:946,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:135589,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad686c31-965d-4776-a6ea-a6be74a3c582_946x598.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Now, this doesn’t necessarily mean that non-mainstream technologies will be naturally surfaced over time, but, well…</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif" width="480" height="192" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:192,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:867759,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a18203e-9f17-4ea2-bb42-e6ac6eca7469_480x192.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If LLMs never get good enough to do things like troubleshoot esoteric technology and learn new things that weren’t in their training set, then they’ll fade into irrelevance anyway. So let’s extend this line of thinking out. If you think (like we probably all do) that your soup du jour is better than the other guy’s, then guess what? Either the LLMs will get “smart” enough to </span><strong>realize it too</strong><span>, or you’ll </span><strong>eat all the vibe coder’s lunch because they never figure it out.</strong></p><p><span>Yes and no. I know there are a lot of people who will hate what I’m about to say. People I respect, and whose opinions I value despite how incorrect they are on this front. “AI” is going to significantly change what it looks like to build software. I’m not worried about how I’m going to stack up against the “</span><strong>Giving into the vibes, embrace exponentials, and forget that code even exists</strong><span>” folks, but I </span><strong>am</strong><span> aware that there are significant new tools &amp; variables at play here. Not factoring them in is how you become a dinosaur. Which, for what it's worth, is fully your prerogative. </span></p><p>With that in mind, if we want our tools to succeed in this space, we need to invest in it as well. Not exclusively, maybe even not significantly. But being relevant here is non-optional IMHO.</p><p><span>I’m the author of an application framework for Elixir called </span><a href="https://ash-hq.org/" rel="">Ash Framework</a><span>. This article is not about Ash, but my work on it and the support that I provide for it have given me some potentially unique insights. Over the last few months I’ve observed:</span></p><ul><li><p>a new item on the rubric for tools: the quality of answers from Claude &amp; ChatGPT about it</p></li><li><p>a huge increase in fully off the wall and weird questions driven by LLM hallucinations</p></li><li><p>methodologies for working with LLM agents that are making me wish *more* of my users used them, not *less*</p></li></ul><p>People are finding that the magic word smusher gives surprisingly good results for things like Typescript &amp; Rust. All of a sudden, they have a new thing to factor in when making their decisions. FOMO is a powerful thing, so I think this plays a bigger factor than we’d like. Folks are worrying even more than ever about the future-proofing of their knowledge in the context of a major industry shift. If I invest my time to learn something that “AI” can’t help me with, will I be left behind? It’s a very real and important question, and I think to succeed these days, you have to alleviate the concern in some way.</p><p>Fairly often these days I get questions about code that no one would ever have had a reason to think should work. These cases are clearly coming from folks asking LLMs how to do something, or using an agentic assistant. The first knee-jerk response here is to tell people that we don’t answer questions about “AI” generated code. And in some cases, if it’s clear that the person asking the question did zero research on their own, they’ll get a response like that. But what if there was another way?</p><p><span>Ultimately you should not be relying on an LLM as a source of “knowledge”. This is true effectively </span><strong>always</strong><span>, even for popular tools, because LLMs won’t “know” about recent changes, but is magnified when asking questions about things that are more esoteric. </span></p><p>The key to succeeding with LLMs is to treat them as if they are only capable of doing things like summarization and pattern transformation. Assume that, before extracting an “answer” to your question, you must first do something that would *place* that answer into its context window, making its job the summarization and/or reformatting of the information into some desired result/effect. Here are some real things you can do to this effect.</p><p>Here is a concrete example of using LLMs “badly”.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png" width="568" height="562.5384615384615" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1442,&quot;width&quot;:1456,&quot;resizeWidth&quot;:568,&quot;bytes&quot;:295003,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2cd5ed29-49ee-4d89-9b3c-bb98d11b3f47_1488x1474.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Looks plausible enough right? But as you can guess, it’s just flat out wrong. Let’s try again, but better this time. I’ll cheat a bit because I know where the docs that it needs are.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png" width="518" height="462.1442307692308" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1299,&quot;width&quot;:1456,&quot;resizeWidth&quot;:518,&quot;bytes&quot;:259003,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbca41d7-629d-446b-8552-458eb020e7b0_1614x1440.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Not only is this </span><strong>correct</strong><span>, but it continues on to provide advice that would likely be easy to miss if skimming the docs. See the </span><a href="https://claude.ai/share/80ff75b1-77ab-4d0c-8db4-b0ac0ad91e97" rel="">full answer here</a><span>. All of a sudden, I’m not in the world of “please stop using LLMs to answer your questions”, I’m in the world of “please stop using LLMs </span><strong>badly</strong><span> to answer your questions”. I knew where to find this documentation, but what if I didn’t?</span></p><p><span>Have a look at </span><a href="https://claude.ai/share/ec8b0563-d80b-4882-82e8-5504e3229a17" rel="">another way of asking this question</a><span>. First, I ask it to find the relevant docs. It does a great job. If you read the linked conversation, I then follow that up with the same question as before, and I get a correct answer without ever leaving Claude’s GUI.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png" width="626" height="639.0235783633842" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1472,&quot;width&quot;:1442,&quot;resizeWidth&quot;:626,&quot;bytes&quot;:397906,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3422f22c-60ae-453d-856f-fbc6c05aff51_1442x1472.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Tidewave is a server that speaks MCP (Model Context Protocol) that you can serve directly from your application. Think of it as giving your LLM a direct hotline to your running code - not just static documentation, but your actual living application.</p><p>Here's what this looks like in practice. Instead of asking Claude "How do I create a user in this app?" and getting a generic answer that might be outdated, Tidewave lets the LLM use tools like:</p><ul><li><p><code>project_eval</code><span> - runs code snippets directly in your application context</span></p></li><li><p><code>run_sql_query</code><span> - executes database queries using your app's actual schema and tooling</span></p></li><li><p><code>search_hex_docs</code><span> - searches the documentation of your specific package versions</span></p></li></ul><p>This means your agent might do things like search the hex docs of your dependencies to see how Ash works, use the `project_eval` tool to poke around in your running app, try creating a user using your app to see if it works, and then run a SQL query after the fact to make sure the user exists in the database. Wildly powerful stuff.</p><p>This one is easy, and you can try it on your own apps today. Often when you end up building new features, they bear resemblance to one or more of the other things in your application. LLMs are very good at synthesis, summary, and categorization. So put them to work at it! A really excellent prompt to kick off an agent’s task often looks like this:</p><pre><code>I'd like to make a feature to track user activity. Let's start with "last_login_at" and "last_seen_at". To start, we'll need a new plug in our router, kind of like @/path/to/similar/plug.ex, except instead of X it does Y. For the fields, take a look at `last_updated` for an example of the kind of field we want, except this one should be accepted as input when updating a user.</code></pre><p><span>I'm trying to push a new pattern for Elixir packages that I think could be game-changing. The concept is simple: any library can include a </span><code>usage-rules.md</code><span> file - essentially very terse documentation designed specifically for LLM context windows, explaining what to and what not to do when using the library.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png" width="598" height="257.51785714285717" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:627,&quot;width&quot;:1456,&quot;resizeWidth&quot;:598,&quot;bytes&quot;:64317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.zachdaniel.dev/i/164927613?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ab20e2c-906b-483c-9c9f-cd7452e8ced4_2080x896.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We've done this for the main Ash packages, and the transformation is remarkable. We went from LLM agents being practically useless for Ash development to being able to generate idiomatic, production-ready code. The dichotomy of "meet my standards" versus "let the LLM do it" simply disappeared.</p><p>This isn't just about Ash or even Elixir. This is a template for how any technology community can turn LLMs from a threat into their most effective growth tool.</p><p>We can go even further. Instead of just helping LLMs use Elixir better, we can help train them to be better at Elixir in the first place. This is something we discussed at the contributors summit, so it is the product of many minds, not just mine.</p><p>The Elixir community could build evaluation datasets that test real-world scenarios: GenServer supervision trees, OTP fault tolerance patterns, Phoenix LiveView reactivity, writing Ecto queries, building applications with Ash. When companies like Anthropic and OpenAI train their models, they need diverse, high-quality benchmarks - and right now, they're mostly testing on Python and JavaScript.</p><p><span>If we get these evaluations to a degree of quality and scale, we get a bunch of benefits. We can determine which models are the best at writing Elixir. If done well enough, companies like OpenAI and Anthropic may even opt to use it to train their private models (something we should encourage in this scenario). You can find collections of benchmarks in various places, like: </span><a href="https://huggingface.co/" rel="">Hugging Face</a><span> and </span><a href="https://github.com/leobeeson/llm_benchmarks" rel="">GitHub</a><span>. I see plenty of python stuff there, but not no Elixir 🤔.</span></p><p>Here's the kicker: Elixir's runtime makes it perfect for performance-based evals. We can build harnesses that don't just check if code compiles, but whether it actually handles concurrent load properly, recovers from crashes gracefully, or manages memory efficiently.</p><p>As models get better at Elixir through these evaluations, they become better advocates for it. We would be going beyond just making LLMs write better Elixir - we'd be making them recommend Elixir when it's the right choice.</p><p>If I can make LLMs a force to be reckoned with not only for Elixir but for the niche-within-a-niche that is Ash Framework, I think it's safe to say that LLMs are not, in-and-of-themselves, an existential threat to Elixir.</p><p>In fact, I think we're looking at this backwards. The real opportunity isn't surviving the LLM revolution - it's using it as a force multiplier. Better technologies have lost adoption battles because they had steeper learning curves or less accessible documentation, but maybe LLMs can flatten that learning curve if we do the work to make our tools competitive in this space.</p><p>The question isn't whether LLMs will change how we build software. They already have. The question is whether we'll shape that change or let it shape us.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After court order, OpenAI is now preserving all ChatGPT user logs (924 pts)]]></title>
            <link>https://mastodon.laurenweinstein.org/@lauren/114627064774788581</link>
            <guid>44185913</guid>
            <pubDate>Wed, 04 Jun 2025 21:47:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.laurenweinstein.org/@lauren/114627064774788581">https://mastodon.laurenweinstein.org/@lauren/114627064774788581</a>, See on <a href="https://news.ycombinator.com/item?id=44185913">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor 1.0 (510 pts)]]></title>
            <link>https://www.cursor.com/en/changelog/1-0</link>
            <guid>44185256</guid>
            <pubDate>Wed, 04 Jun 2025 20:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cursor.com/en/changelog/1-0">https://www.cursor.com/en/changelog/1-0</a>, See on <a href="https://news.ycombinator.com/item?id=44185256">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2>BugBot, Background Agent access to everyone and one-click MCP install</h2><p>Cursor 1.0 is here!</p>
<p>This release brings BugBot for code review, a first look at memories, one-click MCP setup, Jupyter support and general availability of Background Agent.</p>
<h3 id="automatic-code-review-with-bugbot"><span>Automatic code review with BugBot</span></h3>
<p>BugBot automatically reviews your PRs and catches potential bugs and issues.</p>
<p>When an issue is found, BugBot leaves a comment on your PRs in GitHub. You can click "<em><strong>Fix in Cursor</strong></em>" to move back to the editor with a pre-filled prompt to fix the issue.</p>
<p>To set it up, follow instructions in our <a target="_blank" href="https://docs.cursor.com/bugbot">BugBot docs</a></p>

<h3 id="background-agent-for-everyone"><span>Background Agent for everyone</span></h3>
<p>Since we released Background Agent, our remote coding agent, in early access a few weeks ago, the early signals we've been seeing have been positive.</p>
<p>We're now excited to expand Background Agent to all users! You can start using it right away by clicking the cloud icon in chat or hitting <code>Cmd/Ctrl+E</code> if you have privacy mode disabled. For users with privacy mode enabled - we'll soon have a way to enable it for you too!</p>

<!-- -->
<h3 id="agent-in-jupyter-notebooks"><span>Agent in Jupyter Notebooks</span></h3>
<p>Cursor can now implement changes in Jupyter Notebooks!</p>
<p>Agent will now create and edit multiple cells directly inside of Jupyter, a significant improvement for research and data science tasks. Only supported with Sonnet models to start.</p>

<h3 id="memories"><span>Memories</span></h3>
<p>With Memories, Cursor can remember facts from conversations and reference them in the future. Memories are stored per project on an individual level, and can be managed from Settings.</p>
<p>We're rolling out Memories as a beta feature. To get started, enable from Settings → Rules.</p>

<h3 id="mcp-one-click-install-and-oauth-support"><span>MCP one-click install and OAuth support</span></h3>
<p>You can now set up MCP servers in Cursor with one click and together with OAuth support, you can easily authenticate servers that support it.</p>
<p>We've curated a short list of official MCP servers you can add to Cursor at <a target="_blank" href="https://docs.cursor.com/tools">docs.cursor.com/tools</a>.</p>
<p>If you're an MCP developer, you can easily make your server available to developers by adding a <em>Add to Cursor</em> button in your documentation and READMEs. Generate one at <a target="_blank" href="https://docs.cursor.com/deeplinks">docs.cursor.com/deeplinks</a></p>

<h3 id="richer-chat-responses"><span>Richer Chat responses</span></h3>
<p>Cursor can now render visualizations inside of a conversation. In particular, Mermaid diagrams and Markdown tables can now be generated and viewed in the same place!</p>

<h3 id="new-settings-and-dashboard"><span>New Settings and Dashboard</span></h3>
<p>The setting and dashboard page have gotten some polish with this release.</p>
<p>With the new Dashboard, you can view your individual or team's usage analytics, update your display name, and view detailed statistics broken down by tool or model.</p>

<details><summary><span>Keyboard<!-- --> <span>(<!-- -->1<!-- -->)</span></span></summary><div><ul>
<li><span>Open Background Agent control panel with <kbd><code>Cmd/Ctrl+E</code></kbd></span></li>
</ul></div></details>
<details><summary><span>Improvements<!-- --> <span>(<!-- -->4<!-- -->)</span></span></summary><div><ul>
<li><span><code>@Link</code> and web search can now parse PDFs and include in context</span></li>
<li><span>Network diagnostics in settings to verify connectivity</span></li>
<li><span>Faster responses with parallel tool calls</span></li>
<li><span>Collapsable tool calls in Chat</span></li>
</ul></div></details>
<details><summary><span>Account<!-- --> <span>(<!-- -->3<!-- -->)</span></span></summary><div><ul>
<li><span>Enterprise users can only access stable release (no pre-release)</span></li>
<li><span>Team admins can now disable Privacy Mode</span></li>
<li><span><a target="_blank" href="https://docs.cursor.com/account/teams/admin-api">Admin API for teams</a> to access usage metrics and spend data</span></li>
</ul></div></details></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autonomous drone defeats human champions in racing first (242 pts)]]></title>
            <link>https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</link>
            <guid>44184900</guid>
            <pubDate>Wed, 04 Jun 2025 20:03:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first">https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</a>, See on <a href="https://news.ycombinator.com/item?id=44184900">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

                    
                            
                                    
    
    
    <p>
        News
        -
        15 April 2025
        
    </p>

                                
                        

                    <!--TYPO3SEARCH_begin-->
                    

    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580269">
        
            <p><strong>A team of scientists and students from TU Delft has taken first place at the A2RL Drone Championship in Abu Dhabi - an international race that pushes the limits of physical artificial intelligence, challenging teams to fly fully autonomous drones using only a single camera. The TU Delft drone competed against 13 autonomous drones and even human drone racing champions, using innovative methods to train deep neural networks for high-performance control. The gained knowledge on highly-efficient robust AI &nbsp;will contribute to many robotics applications, from self-driving cars to humanoid robots.</strong></p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580270">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" data-imagezoom="" data-width="1248" data-height="1173">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" width="1248" height="1173" alt="Group photo winners WK Drone race">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The TU Delft team: Anton Lang, Quentin Missine, Aderik Verraest, Erin Lucassen, Till Blaha, Robin Ferede, Stavrow Bahnam, Christophe De Wagter and Guido de Croon.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580271">
        
            <h3>
                Beating human pilots
            </h3>
        



            
        



            



        
    

    




        
        

    <p>For the first time, a drone has beaten human pilots in an international drone racing competition, marking a new milestone in the development of artificial intelligence. On Saturday April 14, 2025, two drone racing events took place simultaneously: The Falcon Cup Finals for human pilots and the A2RL Drone Championship for AI-powered, autonomous drones. As a climax, the best AI drones also competed against the best human pilots. The AI drone developed by TU Delft first won the A2RL Grand Challenge. It then went on to win the knockout tournament against human pilots, beating three former DCL world champions and reaching flight speeds up to 95.8 km/h on the very winding track.</p>
<p>The team of scientists and students from TU Delft achieved this by developing an efficient and robust AI system, capable of split-second, high-performance control. Whereas earlier breakthroughs, like AI defeating world champions at chess or Go, have taken place in virtual settings, this achievement happened in the real world. Two years ago, the Robotics and Perception Group at the University of Zürich was the first to beat human drone racing champions with an autonomous drone. However, that impressive achievement occurred in a flight lab environment, where conditions, hardware, and the track were still controlled by the researchers – a very different situation from this world championship, where the hardware and track were fully designed and managed by the competition organisers.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580272">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" data-imagezoom="" data-width="4032" data-height="2395">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" width="4032" height="2395" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The drone designed by the organizers, A2RL and DCL for use by the AI teams and human pilots.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580274">
        
            <h3>
                Pushing the frontiers of physical AI
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The goal of the 2025 A2RL Drone Championship in Abu Dhabi was to push the frontier of physical AI, by stimulating research on robotic AI under extreme time pressure and with very limited computational and sensory resources. The drone had access to just one forward-looking camera, a major difference from previous autonomous drone races. This is more similar to how human FPV pilots fly, and leads to additional perception challenges for the AI.</p>
<p>The AI that won against the three former DCL world champions was developed by a team of scientists and students from the MAVLab at TU Delft’s Faculty of Aerospace Engineering. Team lead Christophe De Wagter is both exhausted and exhilarated.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580275">
        <blockquote>
            <p>I always wondered when AI would be able to compete with human drone racing pilots in real competitions. I’m extremely proud of the team that we were able to make it happen already this year. I hope that this achievement and this type of competition in general forms a springboard for real-world robot applications.</p>
            
            <cite>
                
                
                    Christophe De Wagter 
                
                
            </cite>
        </blockquote>
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580283">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" data-imagezoom="" data-width="1920" data-height="974">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" width="1919" height="974" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>Timelapse of the TU Delft drone flying through a gate of the racing track.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580284">
        
            <h3>
                AI that directly commands the motors
            </h3>
        



            
        



            



        
    

    




        
        

    <p>One of the core new elements of the drone’s AI is the use of a deep neural network that doesn’t send control commands to a traditional human controller, but directly to the motors. These networks were originally developed by the Advanced Concepts Team at the European Space Agency (ESA) under the name of “Guidance and Control Nets”. Traditional, human-engineered algorithms for optimal control were computationally so expensive that they would never be able to run onboard resource-constrained systems such as drones or satellites. ESA found that deep neural networks were able to mimick the outcomes of traditional algorithms, while requiring orders of magnitude less processing time. As it was hard to test whether the networks would perform well on real hardware in space, a collaboration was formed with the MAVLab at TU Delft.</p>
<p>“We now train the deep neural networks with reinforcement learning, a form of learning by trial and error. ”, says Christophe De Wagter. “This allows the drone to more closely approach the physical limits of the system. To get there, though, we had to redesign not only the training procedure for the control, but also how we can learn about the drone’s dynamics from its own onboard sensory data.”</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580286">
        
            <h3>
                Optimising robotic applications
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The highly efficient AI developed for robust perception and optimal control are not only vital to autonomous racing drones but will extend to other robots. Christophe De Wagter: “Robot AI is limited by the required computational and energy resources. Autonomous drone racing is an ideal test case for developing and demonstrating highly-efficient, robust AI. Flying drones faster will be important for many economic and societal applications, ranging from delivering blood samples and defibrillators in time to finding people in natural disaster scenarios. Moreover, we can use the developed methods to strive not for optimal time but for other criteria such as optimal energy or safety. This will have an impact on many other applications, from vacuum robots to self-driving cars”.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    
        
    

    
        
        
    

    <div id="c1580458">
        
            <h3>
                Watch the video of the drone race
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <p id="c1580288">
        
        
            



        
        
            


        
    
        
            

    
            
                

    
            <h3>
                Contact
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </p>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    
    
        



    

    

    


                    <!--TYPO3SEARCH_end-->

                    



                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ada and SPARK enter the automotive ISO-26262 market with Nvidia (106 pts)]]></title>
            <link>https://www.adacore.com/press/ada-and-spark-enter-the-automotive-iso-26262-market-with-nvidia</link>
            <guid>44184861</guid>
            <pubDate>Wed, 04 Jun 2025 19:59:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adacore.com/press/ada-and-spark-enter-the-automotive-iso-26262-market-with-nvidia">https://www.adacore.com/press/ada-and-spark-enter-the-automotive-iso-26262-market-with-nvidia</a>, See on <a href="https://news.ycombinator.com/item?id=44184861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr">High-integrity software tooling experts, <a href="https://www.adacore.com/">AdaCore</a>, are delighted to announce the introduction of the Ada and SPARK programming languages into the automotive market. Together with their partner NVIDIA, they are set to publish an off-the-shelf reference process, allowing others to follow their lead.</p><p dir="ltr">NVIDIA developed Drive® OS, the reference operating system and associated software stack designed specifically for developing and deploying autonomous vehicle applications on DRIVE AGX-based hardware.</p><p dir="ltr">This system includes software components that comply with the highest levels of integrity of the automotive certification standard ISO-26262. To achieve that effort, NVIDIA selected these languages to develop some of the most critical components of its software stack. This required establishing a development process that takes advantage of formal methods and other safety characteristics of Ada and SPARK, thus fully leveraging their capabilities.</p><p dir="ltr">AdaCore and NVIDIA have decided to publish this reference process freely as an open-source and evolving document, allowing the industry at large to adopt Ada and SPARK.</p><p dir="ltr"><em>“As the added value of the automotive industry turns more and more from mechanical to software features, achieving software safety at the highest levels becomes one of the most critical challenges of modern car development,</em>” says Quentin Ochem, Chief Product and Revenue Officer at AdaCore. <em>“NVIDIA demonstrates truly remarkable technical leadership in the domain by introducing Ada and SPARK into its development process and allowing the rest of the community to follow its path.”</em></p><p dir="ltr">The ISO-26262 reference process is available on <a href="https://nvidia.github.io/spark-process/">https://nvidia.github.io/spark-process/</a>&nbsp;or <a target="_blank" href="https://github.com/NVIDIA/spark-process" rel="noreferrer noopener">https://github.com/NVIDIA/spark-process</a>&nbsp;and can be used or customized freely by anyone interested in adopting these languages.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redesigned Swift.org is now live (159 pts)]]></title>
            <link>https://swift.org/</link>
            <guid>44184542</guid>
            <pubDate>Wed, 04 Jun 2025 19:26:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swift.org/">https://swift.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44184542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
    
  
    <!-- mobile-navigation -->
    
  </header>
   
<section id="what-is-swift">
    <div>
        <h2>Swift is the powerful, flexible,<br> multiplatform programming language.</h2>
        <p><h2>Fast. Expressive. Safe.</h2></p>
        <p><a href="https://swift.org/install/" data-text="Install">Install</a></p><p>Tools for Linux, macOS, and Windows</p>
        <h2>Create using Swift</h2>
    </div>
    <nav aria-label="Get started with Swift">
        <ul>
            
            <li>
                <a href="https://swift.org/get-started/cloud-services" data-text="Cloud Services">
                    <i></i>
                    <div>
                        <h3>Cloud Services</h3>
                        <p>Run performant services on Linux and deploy to the cloud.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/command-line-tools" data-text="Command Line">
                    <i></i>
                    <div>
                        <h3>Command Line</h3>
                        <p>Create powerful CLI tools that are fast and memory safe.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/embedded" data-text="Embedded">
                    <i></i>
                    <div>
                        <h3>Embedded</h3>
                        <p>Develop efficient, reliable firmware for devices like microcontrollers.</p>
                    </div>
                </a>
            </li>
            
        </ul>
        <ul>
            
            <li>
                <a href="https://swift.org/getting-started/swiftui/" data-text="iOS apps">
                    <h4>iOS apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/swift-everywhere-windows-interop/" data-text="Windows apps">
                    <h4>Windows apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/mlx-swift/" data-text="Machine learning and AI">
                    <h4>Machine Learning &amp; AI</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/getting-started/library-swiftpm/" data-text="Packages">
                    <h4>Packages</h4>
                </a>
            </li>
            
        </ul>
    </nav>
    
</section>

<section id="pillar-1">
    <div>
        <p>
            Swift is the only language that scales from embedded devices and kernels to apps and cloud infrastructure. It’s simple, and expressive, with incredible performance and safety. And it has unmatched interoperability with C and C++.
        </p>
        
        <p>
            It's the combination of approachability, speed, safety, and all of<br> Swift’s strengths that make it so unique.
        </p>
    </div>
    
 
<div>
  <div>
    
    <h3>Fast</h3>
     
    <p>Build with speed and performance.</p>
     
    <p>Swift meets the most performance-critical needs, while allowing your code to remain expressive and approachable. Swift compiles directly to native code and provides predictable memory management.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Vectorized check that a utf8 buffer is all ASCII</span>
<span>func</span> <span>isASCII</span><span>(</span><span>utf8</span><span>:</span> <span>Span</span><span>&lt;</span><span>SIMD16</span><span>&lt;</span><span>UInt8</span><span>&gt;&gt;</span><span>)</span> <span>-&gt;</span> <span>Bool</span> <span>{</span>
  <span>// combine all the code units into a single entry</span>
  <span>utf8</span><span>.</span><span>indices</span><span>.</span><span>reduce</span><span>(</span><span>into</span><span>:</span> <span>SIMD16</span><span>())</span> <span>{</span>
    <span>// fold each set of code units into the result</span>
    <span>$0</span> <span>|=</span> <span>utf8</span><span>[</span><span>$1</span><span>]</span>
  <span>}</span>
  <span>// check that every entry is in the ASCII range</span>
  <span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>0x80</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Expressive</h3>
     
    <p>Concise code. Powerful results.</p>
     
    <p>Swift empowers you to write advanced code in a concise, readable syntax that even a beginner can understand. Swift supports object-oriented, functional, and generic programming patterns that experienced developers are familiar with. Its progressive disclosure allows you to pick up the language quickly, taking advantage of power-user features as you need them.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>ArgumentParser</span>

<span>// Complete implementation of a command line tool</span>
<span>@main</span> <span>struct</span> <span>Describe</span><span>:</span> <span>ParsableCommand</span> <span>{</span>
  <span>@Argument</span><span>(</span><span>help</span><span>:</span> <span>"The values to describe."</span><span>)</span>
  <span>var</span> <span>values</span><span>:</span> <span>[</span><span>Double</span><span>]</span> <span>=</span> <span>[]</span>

  <span>mutating</span> <span>func</span> <span>run</span><span>()</span> <span>{</span>
    <span>values</span><span>.</span><span>sort</span><span>()</span>
    <span>let</span> <span>total</span> <span>=</span> <span>values</span><span>.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>+</span><span>)</span>

    <span>print</span><span>(</span>
      <span>"""
      Smallest: </span><span>\(</span><span>values</span><span>.</span><span>first</span><span>,</span> <span>default</span><span>:</span> <span>"No value"</span><span>)</span><span>
      Total:    </span><span>\(</span><span>total</span><span>)</span><span>
      Mean:     </span><span>\(</span><span>total</span> <span>/</span> <span>Double</span><span>(</span><span>values</span><span>.</span><span>count</span><span>)</span><span>)</span><span>
      """</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Safe</h3>
     
    <p>Protect memory safety.</p>
     
    <p>Swift prioritizes safety and eliminates entire classes of bugs and vulnerabilities by its design. Memory safety and data race safety are core features of the language, making them straightforward to integrate into your codebase. Safety is required at compile time, before your applications are ever run.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>let</span> <span>transform</span> <span>=</span> <span>Affine2DTransformBuilder</span><span>()</span>
    <span>.</span><span>translate</span><span>([</span><span>10.0</span><span>,</span> <span>20.0</span><span>]</span><span>.</span><span>span</span><span>)</span>
    <span>.</span><span>rotate</span><span>(</span><span>30.0</span><span>)</span>
    <span>.</span><span>build</span><span>()</span>

<span>let</span> <span>v</span> <span>=</span> <span>[</span><span>11.0</span><span>,</span> <span>22.0</span><span>,</span> <span>1.0</span><span>]</span>

<span>// Call C functions safely with Swift types</span>
<span>let</span> <span>u</span> <span>=</span> <span>mat_vec_mul</span><span>(</span>
  <span>transform</span><span>,</span> <span>rowCount</span><span>,</span> <span>colCount</span><span>,</span> <span>v</span><span>.</span><span>span</span><span>,</span> <span>allocator</span><span>)</span>
<span>let</span> <span>uMagnitude</span> <span>=</span> <span>vec_mag</span><span>(</span><span>u</span><span>.</span><span>span</span><span>)</span>
</code></pre></div>
    
</div>

    

    

</section>

<section id="pillar-2">
    
 
<div>
  <div>
    
    <h3>Interoperable</h3>
     
    <p>Adopt in existing code incrementally.</p>
     
    <p>Swift provides unmatched interoperability with its combination of natively understanding C and C++ types without the need for foreign function interfaces, and by providing bridging for bi-directional access. Swift’s interoperability features allow you to incrementally adopt the language into existing codebases without requiring a full code rewrite.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>CxxStdlib</span>

<span>// Use types from C++, like std::string, directly</span>
<span>let</span> <span>beverages</span><span>:</span> <span>[</span><span>std</span><span>.</span><span>string</span><span>]</span> <span>=</span> <span>[</span>
  <span>"apple juice"</span><span>,</span> <span>"grape juice"</span><span>,</span> <span>"green tea"</span>
<span>]</span>

<span>let</span> <span>juices</span> <span>=</span> <span>beverages</span><span>.</span><span>filter</span> <span>{</span> <span>cppstring</span> <span>in</span>
  <span>// and call methods directly on C++ types</span>
  <span>cppstring</span><span>.</span><span>find</span><span>(</span><span>.</span><span>init</span><span>(</span><span>"juice"</span><span>))</span> <span>!=</span> <span>std</span><span>.</span><span>string</span><span>.</span><span>npos</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Adaptable</h3>
     
    <p>From microcontrollers to servers.</p>
     
    <p>The only language that can span from embedded and kernel, to server and apps. Swift excels no matter where it’s used: from constrained environments like firmware where every byte counts, to cloud services handling billions of requests a day.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Configure UART by direct register manipulation </span>
<span>// using Swift MMIO. Enables RX and TX, and sets</span>
<span>// baud rate to 115,200. Compiles down to an</span>
<span>// optimal assembly sequence with no overhead.</span>

<span>usart1</span><span>.</span><span>brr</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>raw</span><span>.</span><span>brr_field</span> <span>=</span> <span>16_000_000</span> <span>/</span> <span>115_200</span>
<span>}</span>

<span>usart1</span><span>.</span><span>cr1</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>ue</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>re</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>te</span> <span>=</span> <span>.</span><span>Enabled</span>
<span>}</span>
</code></pre></div>
    
</div>

    
    
</section>

<div id="pillar-3">
    
    <h3>Open Source</h3>
     
    <p>Contribute and get involved.</p>
     
  </div>
 

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curtis Yarvin's Plot Against America (271 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</link>
            <guid>44184305</guid>
            <pubDate>Wed, 04 Jun 2025 19:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile">https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</a>, See on <a href="https://news.ycombinator.com/item?id=44184305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure></figure><p>In the spring and summer of 2008, when <a href="https://www.newyorker.com/tag/donald-trump">Donald Trump</a> was still a registered Democrat, an anonymous blogger known as Mencius Moldbug posted a serial manifesto under the heading “An Open Letter to Open-Minded Progressives.” Written with the sneering disaffection of an ex-believer, the hundred-and-twenty-thousand-word letter argued that egalitarianism, far from improving the world, was actually responsible for most of its ills. That his bien-pensant readers thought otherwise, Moldbug contended, was due to the influence of the media and the academy, which worked together, however unwittingly, to perpetuate a left-liberal consensus. To this nefarious alliance he gave the name the Cathedral. Moldbug called for nothing less than its destruction and a total “reboot” of the social order. He proposed “the liquidation of democracy, the Constitution, and the rule of law,” and the eventual transfer of power to a C.E.O.-in-chief (someone like Steve Jobs or Marc Andreessen, he suggested), who would transform the government into “a heavily-armed, ultra-profitable corporation.” This new regime would sell off public schools, destroy universities, abolish the press, and imprison “decivilized populations.” It would also fire civil servants en masse (a policy Moldbug later called <em>RAGE</em>—Retire All Government Employees) and discontinue international relations, including “security guarantees, foreign aid, and mass immigration.”</p><p>Moldbug acknowledged that his vision depended on the sanity of his chief executive: “Clearly, if he or she turns out to be Hitler or Stalin, we have just recreated Nazism or Stalinism.” Yet he dismissed the failures of twentieth-century dictators, whom he saw as too reliant on popular support. For Moldbug, any system that sought legitimacy in the passions of the mob was doomed to instability. Though critics labelled him a techno-fascist, he preferred to call himself a royalist or a Jacobite—a nod to partisans of James II and his descendants, who, in the seventeenth and eighteenth centuries, opposed Britain’s parliamentary system and upheld the divine right of kings. Never mind the French Revolution, the bête noire of reactionary thinkers: Moldbug believed that the English and American Revolutions had gone too far.</p><p>If Moldbug’s “Open Letter” showed little affection for the masses, it intimated that they might still have a use. “Communism was not overthrown by <a href="https://www.newyorker.com/news/our-columnists/fifty-years-later-andrei-sakharovs-most-famous-essay-is-a-powerful-model-of-writing-for-social-change">Andrei Sakharov</a>, <a href="https://www.newyorker.com/magazine/1996/02/12/perfect-pitch-2">Joseph Brodsky</a>, and <a href="https://www.newyorker.com/books/page-turner/vaclav-havels-lessons-on-how-to-create-a-parallel-polis">Václav Havel</a>,” he wrote. “What was needed was the combination of philosopher and crowd.” The best place to recruit this crowd, he said, was on the internet—a shrewd intuition. Before long, links to Moldbug’s blog, “Unqualified Reservations,” were being passed around by libertarian techies, disgruntled bureaucrats, and self-styled rationalists—many of whom formed the shock troops of an online intellectual movement that came to be known as neo-reaction, or the Dark Enlightenment. While few turned into outright monarchists, their contempt for Obama-era uplift seemed to find voice in Moldbug’s heresies. In his most influential coinage, which quickly gained currency among the nascent alt-right, Moldbug urged his readers to rouse themselves from their ideological slumber by taking the “red pill,” like Keanu Reeves’s character in “The Matrix,” who chooses daunting truth over contented ignorance.</p><p>In 2013, an article on the news site <em>TechCrunch</em>, titled “Geeks for Monarchy,” revealed that Mencius Moldbug was the cyber alias of a forty-year-old programmer in San Francisco named Curtis Yarvin. At the same time that he was trying to redesign the U.S. government, Yarvin was also dreaming up a new computer operating system that he hoped would serve as a “digital republic.” He founded a company that he named Tlon, for the <a href="https://www.newyorker.com/magazine/1970/09/19/jorge-luis-borges-profile-autobiographical-notes">Borges</a> story “Tlön, Uqbar, Orbis Tertius,” in which a secret society describes an elaborate parallel world that begins to overtake reality. As he raised money for his startup, Yarvin became a kind of Machiavelli to his big-tech benefactors, who shared his view that the world would be better off if they were in charge. Tlon’s investors included the venture-capital firms Andreessen Horowitz and Founders Fund, the latter of which was started by the billionaire <a href="https://www.newyorker.com/news/letter-from-silicon-valley/what-is-it-about-peter-thiel">Peter Thiel</a>. Both Thiel and Balaji Srinivasan, then a general partner at Andreessen Horowitz, had become friends with Yarvin after reading his blog, though e-mails shared with me revealed that neither was thrilled to be publicly associated with him at the time. “How dangerous is it that we are being linked?” Thiel wrote to Yarvin in 2014. “One reassuring thought: one of our hidden advantages is that these people”—social-justice warriors—“wouldn’t believe in a conspiracy if it hit them over the head (this is perhaps the best measure of the decline of the Left). Linkages make them sound really crazy, and they kinda know it.”</p><p>A decade on, with the Trumpian right embracing strongman rule, Yarvin’s links to élites in Silicon Valley and Washington are no longer a secret. In a 2021 appearance on a far-right podcast, Vice-President J.&nbsp;D. Vance, a former employee of one of Thiel’s venture-capital firms, cited Yarvin when suggesting that a future Trump Administration “fire every single mid-level bureaucrat, every civil servant in the administrative state, replace them with our people,” and ignore the courts if they objected. Marc Andreessen, one of the heads of Andreessen Horowitz and an informal adviser to the so-called Department of Government Efficiency (<em>DOGE</em>), has started quoting his “good friend” Yarvin about the need for a founder-like figure to take charge of our “out of control” bureaucracy. Andrew Kloster, the new general counsel at the government’s Office of Personnel Management, has said that replacing civil servants with loyalists could help Trump defeat “the Cathedral.”</p><p>“There are figures who channel a Zeitgeist—Nietzsche calls them timely men—and Curtis is definitely a timely man,” a State Department official who has been reading Yarvin since the Moldbug era told me. Back in 2011, Yarvin said that Trump was one of two figures who seemed “biologically suited” to be an American monarch. (The other was Chris Christie.) In 2022, he recommended that Trump, if reëlected, appoint Elon Musk to run the executive branch. On a podcast with his friend Michael Anton, now the director of policy planning at the State Department, Yarvin argued that the institutions of civil society, such as Harvard, would need to be shut down. “The idea that you’re going to be a Caesar&nbsp;.&nbsp;.&nbsp;. with someone else’s Department of Reality in operation is just manifestly&nbsp;absurd,” he said.</p><p>In another timeline, Yarvin might have remained an obscure and ineffectual internet crank, a digital de Maistre. Instead, he has become one of America’s most influential illiberal thinkers, an engineer of the intellectual source code for the second Trump Administration. “Yarvin has pushed the Overton window,” Nikhil Pal Singh, a history professor at N.Y.U., told me. His work has revived ideas that once seemed outside the bounds of polite society, Singh said, and created a road map for the dismantling of “the administrative state and the global postwar order.”</p><p>As his ideas have been surrealized in <em>DOGE</em> and Trump has taken to self-identifying as a king, one might expect to find Yarvin in an exultant mood. In fact, he has spent the past few months fretting that the moment will go to waste. “If you have a Trump boner right now, enjoy it,” he wrote two days after the election. “It’s as hard as you’ll ever get.” What many see as the most dangerous assault on American democracy in the nation’s history Yarvin dismisses as woefully insufficient—a “vibes coup.” Without a full-blown autocratic takeover, he believes, a backlash is sure to follow. When I spoke to him recently, he quoted the words of Louis de Saint-Just, the French philosopher who championed the Reign of Terror: “He who makes half a revolution digs his own grave.”</p><p>Earlier this year, Yarvin and I had lunch in Washington, D.C., where he had come to celebrate the regime change. He was in his usual getup: bluejeans, Chelsea boots, a rumpled dress shirt under a motorcycle jacket. After taking a few bites of a cheeseburger topped with crispy onions, he pushed his plate away. Last year, he explained, he’d decided to start taking an Ozempic-like drug after a debate with the right-wing commentator Richard Hanania about the relative merits of monarchy and democracy. “I destroyed him in almost every way,” Yarvin said, nudging a tomato with his fork. “But he had one huge advantage, which was that I was fat and he was not.”</p><p>The injections seemed to be working. As I ate, Yarvin’s phone filled with messages, some of them complimenting his glow-up. That morning, the <em>Times Magazine</em> had published an interview with him, accompanied by a moody black-and-white portrait. Until recently, Yarvin, with his frazzled curtain of shoulder-length hair and ill-fitting wardrobe, had seemed indifferent to his appearance. Now, wearing his leather jacket, he glared out at the reader through stylishly tousled hair. His friend Steve Sailer, a writer for white-nationalist websites, said he looked like “the fifth Ramone.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61101&quot;}" href="https://www.newyorker.com/cartoon/a61101" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“How can a hunter-gatherer and a rock designer afford such a nice cave?”</span></p><p><span>Cartoon by Enrico Pinto</span></p></div></span></p></figure><p>In person, as in print, Yarvin expresses himself with imperious self-assurance. He is nearly impossible to interrupt. “When the rabbi is speaking, you let the rabbi speak,” Razib Khan, a right-wing science blogger and a close friend of Yarvin’s, told me. Even his friends and family, however, acknowledge that he has room to grow as a communicator. He talks in a halting monotone, rarely answers questions directly, and is prone to disorienting asides. In the middle of saying one thing, he is always getting distracted by something else he could be saying, like a G.P.S. that keeps suggesting faster routes.</p><p>Yarvin, for his part, was relieved at how the interview with the <em>Times</em> had gone. “My main goal was, how do I not damage any of my relationships?” he said. For years, Yarvin was best known, to the extent that he was known at all, as the court philosopher of the Thiel-verse, the network of heterodox entrepreneurs, intellectuals, and hangers-on surrounding the tech mogul. He mentioned that a businessman he knew had once complained to a journalist that Thiel had not invested enough money in his company. “That’s one strike and you’re out, and he was out,” Yarvin said, sighing theatrically. His second goal, he said, was to reach the <em>Times</em> audience. This seemed surprising: he has called for the government to shut down the paper. “I tend to be more interested in outreach to people who share my own cultural background,” Yarvin explained.</p><p>He likes to tell the story of his paternal grandparents, Jewish Communists from Brooklyn who met at a leftist gathering in the thirties. (He has less to say about his maternal grandparents, Tarrytown Wasps with a cottage on Nantucket.) “The vibe of American communism was ‘We’ve got thirty I.Q. points on these people, and we’re going to win,’&nbsp;” he said. “It’s like, what if all the gifted kids formed a political party and tried to take over the world?” Yarvin’s parents met at Brown, where his father, Herbert, was pursuing a Ph.D. in philosophy. After finishing school and failing to get tenure (“too arrogant,” Yarvin said), Herbert tried his hand at writing the Great American Novel, then joined the Foreign Service as a diplomat. In the following years, the family lived in the Dominican Republic and Cyprus. Herbert was cynical about working for the government, and Yarvin seems to have inherited his disdain: he has repeatedly proposed closing America’s embassies, a prospect the State Department is now considering in parts of Europe and Africa.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin is reticent on the subject of his childhood, but friends and family suggested to me that his father could be harsh, domineering, and impossible to please. “He controlled their life with an iron fist,” someone with close knowledge of the family told me. “It was absolutely his domain.” (Yarvin vehemently rejected this view, saying that people who are controlling tend to be insecure, “and that is very much <em>not</em> the way of my father.” Better words to describe him, he said, would be “stubborn,” “intense,” and “formidable”—like “a good manager.”)</p><p>Growing up, Yarvin was sometimes homeschooled by his mother, and skipped three grades. (His older brother, Norman, skipped four.) The family eventually moved to Columbia, Maryland, where Yarvin entered high school as a twelve-year-old sophomore. “When you’re much younger than your classmates, you’re either an adorable mascot or a weird, threatening, disturbing alien,” Yarvin said, adding that he was the latter. Yarvin was selected to participate in a Johns Hopkins study of math prodigies. He attended the university’s Center for Talented Youth, a summer camp for gifted children, and was a Baltimore-area champion on “It’s Academic,” a television trivia show. Andrew Cone, a software engineer who currently lives in a spare room in Yarvin’s home, told me that Yarvin’s childhood seems to have left him with a lifelong feeling of inadequacy. “I think he has this sense of being not good enough, that he’s seen as ridiculous or small, and that the only way out is to perform,” Cone said.</p><p>Yarvin went to Brown, graduated at eighteen, and then entered a Ph.D. program in computer science at the University of California, Berkeley. Former peers told me that he wore a bicycle helmet in class and seemed eager to show off his knowledge to the professor. “Oh, you mean helmet-head?” one said when I asked about Yarvin. The joke among some of his classmates was that the helmet prevented new ideas from penetrating his mind. He found more of a community on Usenet, a precursor to today’s online forums. But even in groups like talk.bizarre, where intellectual peacocking was the norm, he stood out for his desire to dominate. Along with posting jokes, advice, light verse, and “flames” (blistering takedowns of other users), he maintained a “kill file,” a list of members he had blocked because he found their posts uninteresting. “He wanted to be viewed as the smart guy—that was really, really important to him,” his first girlfriend, Meredith Tanner, told me. She was drawn to Yarvin after reading one of his virtuosic flames, and the pair dated for a few years. “Don’t get involved with someone just because you’re impressed by how creatively they insult people,” she warned. “They will turn that skill on you.”</p><p>Friends from Yarvin’s twenties described him as a reflexive contrarian who revelled in provocation. “He wasn’t a sweet kid, and he could sometimes be nasty,&nbsp;but he wasn’t Moldbug,” one said. Politically and culturally, Yarvin was a liberal—“a big old hippie,” as Tanner put it. He had a ponytail, wore a silver hoop earring, dropped acid at raves, and wrote poetry. Tanner recalled that when she once questioned the value of affirmative action in college admissions, it was Yarvin who convinced her of its necessity.</p><p>After a year and a half of doctoral work, Yarvin left academia to seek his fortune in the tech industry. He helped design an early version of a mobile web browser for a company that came to be known as Phone.com. In 2001, he began dating Jennifer Kollmer, a playwright he met on Craigslist, whom he later married and had two children with. Phone.com had gone public, leaving him with a windfall of a million dollars. He used some of the money to buy a condo near the Haight-Ashbury neighborhood of San Francisco and the rest to fund a self-directed study of computer science and political theory. “I was used to getting pats on the head for being smart,” he said of his decision to leave the <em>cursus honorum</em> of the gifted child. “Diverging from the pat-on-the-head economy was a strange and scary choice.”</p><p>Out in the wilderness, Yarvin delved into recondite history and economics texts, many of them newly accessible through Google Books. He read Thomas Carlyle, James Burnham, and Albert Jay Nock, alongside an early-aughts profusion of political blogs. Yarvin traces his own red-pill moment to the Presidential election of 2004. As many of his peers were being driven to the left by lies about weapons of mass destruction in Iraq, Yarvin was pulled in the opposite direction by fabrications of a different sort: the Swift Boat conspiracy theory pushed by veterans allied with the George&nbsp;W. Bush campaign, who claimed that the Democratic candidate, John Kerry, had lied about his service in Vietnam. It seemed obvious to Yarvin, who believed the accusations, that once the truth emerged Kerry would be forced to drop out of the race. When that didn’t happen, he began to question what else he’d naïvely taken on trust. Facts no longer felt stable. How could he be confident in what he’d been told about Joseph McCarthy, the Civil War, or global warming? What about democracy itself? After years of energetic debates in the comments sections of other people’s blogs, he decided to start his own. It did not lack for ambition. The first post began, “The other day I was tinkering around in my garage and I decided to build a new ideology.”</p><p>The German academic Hans-Hermann Hoppe is sometimes described as an intellectual gateway to the far right. A retired economics professor at the University of Nevada, Las Vegas, Hoppe argues that universal suffrage has supplanted rule by a “natural élite”; advocates for breaking nations into smaller, homogenous communities; and calls for communists, homosexuals, and others who oppose this rigid social order to be “physically removed.” (Some white nationalists have made memes pairing Hoppe’s face with a helicopter—an allusion to the Chilean dictator Augusto Pinochet’s practice of executing opponents by throwing them from aircraft.) Though Hoppe favors a minimal state, he believes that freedom is better preserved by monarchy than by democracy.</p><p>Yarvin nearly ended up a libertarian. As a Bay Area coder and a devotee of Austrian-school economists in his late twenties, he exhibited all the risk factors. Then he discovered Hoppe’s book “<a data-offer-url="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684&quot;}" href="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" rel="nofollow noopener" target="_blank">Democracy: The God That Failed</a>” (2001) and changed his mind. Yarvin soon adopted Hoppe’s imago of a benevolent strongman—someone who would govern efficiently, avoid senseless wars, and prioritize the well-being of his subjects. “It’s not copy-and-pasted, but it is such a direct influence that it’s kind of obscene,” Julian Waller, a scholar of authoritarianism at George Washington University, said. (Over e-mail, Hoppe recalled that he met Yarvin once at an exclusive gathering at Peter Thiel’s home, where Hoppe had been invited to speak. He acknowledged his influence on Yarvin, but added, “For my taste his writing has always been a bit too flowery and rambling.”) Hoppe argues that, unlike democratically elected officials, a monarch has a long-term incentive to safeguard his subjects and the state, because both belong to him. Anyone familiar with the history of dictatorships might find this idea disingenuous. Not Yarvin.</p><p>“You don’t ransack your own house,” he told me one afternoon, at an open-air café in Venice Beach. I’d asked him what would stop his C.E.O.-monarch from plundering the country—or enslaving his people—for personal gain. “For Louis XIV, when he says, ‘<em>L’état, c’est moi</em>,’ ransacking the state holds no meaning because it’s all his anyway.” Following Hoppe, Yarvin proposes that nations should eventually be broken up into a “patchwork” of statelets, like Singapore or Dubai, each with its own sovereign ruler. The eternal political problems of legitimacy, accountability, and succession would be solved by a secret board with the power to select and recall the otherwise all-powerful C.E.O. of each sovereign corporation, or SovCorp. (How the board itself would be selected is unclear, but Yarvin has suggested that airline pilots—“a fraternity of intelligent, practical, and careful people who are already trusted on a regular basis with the lives of others. What’s not to like?”—could manage the transition between regimes.) To prevent a C.E.O. from staging a military coup, the board members would have access to cryptographic keys that would allow them to disarm all government weapons, from nuclear missiles down to small arms, with the push of a button.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Mass political participation would cease, and the only way that people could vote would be with their feet, by moving from one SovCorp to another if they became dissatisfied with the terms of service, like switching from X to Bluesky. The irony that dissenters like Yarvin would probably be repressed in such a state appears not to concern him. In his imagined polity, he insists, there would still be freedom of speech. “You can think, say, or write whatever you want,” he has promised. “Because the state has no reason to care.”</p><p>Yarvin’s congenital cynicism about governance disappears as soon as he starts talking about dictatorial regimes. He has kind words for El Salvador’s strongman, <a href="https://www.newyorker.com/magazine/2022/09/12/the-rise-of-nayib-bukele-el-salvadors-authoritarian-president">Nayib Bukele</a>, and has encouraged Trump to let Putin end the liberal order “not just in Russian-speaking territories—but all the way to the English Channel.” Picking at a plate of fried calamari, Yarvin praised China and Rwanda (neither of which he has visited) for having strong governments that insured both public safety and personal liberty. In China, he told me, “you can think and pretty much say whatever you want.” He may have sensed my skepticism, given the country’s record of imprisoning critics and detaining ethnic minorities in concentration camps. “If you want to organize against the government, you’re gonna have problems,” he admitted. Then he returned to his airbrush: “Not Stalin problems. You’ll just, like, be cancelled.”</p><p>For certain people, like meth addicts or four-year-olds, Yarvin said, too much freedom could be deadly. Then, gesturing to the homeless population camped in the neighborhood, he suddenly began to cry. “The idea that this represents success, or this represents the ‘worst of all systems, except for all the others’&nbsp;”—he was referencing Churchill’s famous comment about democracy, which I’d paraphrased earlier—“is highly delusional,” he said, wiping away the tears. (A few weeks later, on a trip to London, I watched him break down while giving a similar speech to a member of the House of Lords. It was less affecting the second time around.)</p><p>Presumably, Yarvin’s monarch would act decisively to safeguard his wards. At the Venice café, Yarvin lauded the Delancey Street Foundation, a nonprofit rehab organization, whose strict program he has characterized as exerting “fascist-parent-level control.” Some of his own proposals go further. On his blog, he once joked about converting San Francisco’s underclasses into biodiesel to power the city’s buses. Then he suggested another idea: putting them in solitary confinement, hooked up to a virtual-reality interface. Whatever the exact solution, he has written, it is crucial to find “a humane alternative to genocide,” an outcome that “achieves the same result as mass murder (the removal of undesirable elements from society) but without any of the moral stigma.”</p><p>Yarvin’s call for an American strongman is often treated as an eccentric provocation. In fact, he considers it the only answer to a world in which most people are unfit for democracy. An “African country today,” he told me, has “enough smart people in the country to run it—you just don’t have enough smart people to have a democratic election in which everyone is smart.” Because of such remarks, Yarvin is sometimes identified as a white nationalist, a label he delicately resists. In a 2007 blog post titled “Why I Am Not a White Nationalist,” he explained that, though he is “not exactly allergic to the stuff,” he finds both whiteness and nationalism to be unhelpful political concepts. During lunch, he told me that he feels a rueful sympathy for the bigots of the past, who had some of the right intuitions but lacked the proper science. Neo-reactionaries tend to subscribe to what they call “human biodiversity,” a set of fringe beliefs which holds, among other things, that not all racial or population groups are equally intelligent. As Yarvin came to see it from his online research, these genetic differences contributed to (and, conveniently, helped explain away) demographic differences in poverty, crime, and educational attainment. “In this house, we believe in science—<em>race</em> science,” he wrote last year.</p><p>For several hours, Yarvin shuffled through his pitches for strongman rule, like an auctioneer desperate to clinch a sale. I listened patiently, though I was often puzzled by his factual distortions and peculiar asides. “What is the right policy in a completely new-from-scratch regime for African Americans?” he wondered aloud at one point. At first, this seemed like a non sequitur: I’d been pressing him on how he would define success in the second Trump Administration. Answering himself, he said that the “obvious solution” to problems of inner-city drug abuse and poverty would be to “put the church Blacks in charge of the ghetto Blacks.” Yarvin, who is an atheist, is not particularly interested in theocratic rule, but he advocates creating different legal codes to govern different populations. (He has cited the Ottoman <em>millet</em> system, which granted religious communities a measure of autonomy.) To keep the “ghetto Blacks” in line, he went on, they should be forced to live in a “traditional way,” like Orthodox Jews or the Amish. “The approach that the twentieth century took is, if we could just make the schools good enough, they would all turn into Unitarians,” he said. “If you’ve seen ‘The Wire’ and lived in Baltimore, both of which I have, that does not seem to work at all.” It wasn’t until he reached the end of his speech, ten minutes later, that I realized he was, in his own way, addressing my initial question. “Unless we can totally reëngineer DNA to change what a human being is, there are many people who should not live in a modern way but in a traditional way,” he concluded. “And <em>that</em> is a level of revolution that is so far beyond anything the Trump-Vance regime is doing.”</p><p>Yarvin is not known for his discretion. He has a habit of sharing private correspondence, as I discovered when he started sending me unsolicited screenshots of text messages and e-mails he’d exchanged with his wife, his friends, a fact checker at the <em>Times Magazine</em>, and someone nominated to the new Administration. He seemed troubled by the thought that the wit and wisdom they contained might be lost to posterity. He was more guarded about his friendship with Thiel, but he did mention a conversation they’d privately filmed together last year and boasted about a fortieth-birthday gift he’d received from the billionaire: Francis Neilson’s “The Tragedy of Europe,” a contemporaneous commentary on the Second World War, though not the first edition that Yarvin had been hoping for.</p><p>Thiel has always had a prophetic touch. He co-founded PayPal, became the first outside investor in Facebook, and created Palantir, a data-mining firm that has just received a new contract to help Immigration and Customs Enforcement officers carry out deportations. Thiel supported Trump back when doing so still made one a pariah in Silicon Valley. In 2022, he donated fifteen million dollars to J.&nbsp;D. Vance’s Senate campaign, the largest amount given to a single candidate in congressional history. A longtime libertarian, Thiel appears to have taken a Yarvinian turn around 2009, when, in a widely quoted essay published online by the Cato Institute, he wrote, “I no longer believe that freedom and democracy are compatible.” Yarvin linked to it approvingly in a blog post titled “Democraphobia Goes (Slightly) Viral.” They soon met for the first time, at Thiel’s house in San Francisco, and, according to private messages I reviewed, struck up a confiding correspondence. Yarvin’s e-mails were long and homiletic, full of precepts gleaned from pickup-artist blogs; Thiel’s were straightforward and concise. Both men seemed to take for granted that America was a communist country, that journalists acted like the Stasi, and that tech C.E.O.s were their prey.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In the fall of 2014, Thiel published “Zero to One,” a best-selling treatise on startups, with Blake Masters, his employee and a longtime Moldbug fan. Before the book tour, Thiel asked Yarvin for advice on fielding questions he might get on how to steer more women into tech. The premise appeared to strike them both as misguided, since women, in their view, were less likely to have men’s aptitude for computer science. As Yarvin put it in one e-mail, “There’s simply no way short of becoming a farce for Google, YC”—Y Combinator, the startup accelerator—“etc, etc, to ‘look like America.’&nbsp;” Yarvin suggested that Thiel deploy a pickup-artist tactic called “agree and amplify”—that is, ask a journalist, who probably had no solution in mind, what she would do to tackle the problem. “The purpose here is not to get the interlocutor to sleep with you, but to get her to fear this issue and run away from it—and ditto for future interviewers,” he wrote. Once, at a dinner, Thiel quizzed Yarvin on how one might go about taking down <em>Gawker</em>. (As it turned out, Thiel had already decided to secretly bankroll Hulk Hogan’s defamation lawsuit against the online publication, which eventually bankrupted it, in 2016.) In e-mails obtained by <em>BuzzFeed</em>, Yarvin bragged to Milo Yiannopoulos, the <em>Breitbart</em> editor, that he’d watched Trump’s first election at Thiel’s house and had been “coaching” him. “Peter needs guidance on politics for sure,” Yiannopoulos replied. Yarvin wrote back, “Less than you might think!&nbsp;.&nbsp;.&nbsp;. He’s fully enlightened, just plays it very carefully.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60750&quot;}" href="https://www.newyorker.com/cartoon/a60750" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“Slowest to hit Skip has to watch the whole ad.”</span></p><p><span>Cartoon by Juan Astasio and Colin Mills</span></p></div></span></p></figure><p>When I recently visited Yarvin’s Craftsman home, in Berkeley, I noticed a painting that Thiel had given him: a portrait of Yarvin in the style of a role-playing-game character card, bearing the legend “Philosopher.” As I sipped tea from a novelty mug featuring an image of Yarvin with a cartoon crown, he told me that it would be “cringe” for him to broadcast his relationship with Thiel—or with Vance, for that matter, whom he met through Thiel around 2015. “Does a normal Ohio voter read&nbsp;.&nbsp;.&nbsp;. Mencius Moldbug? No,” Vance reportedly said one night at a bar during the 2021 National Conservatism Conference. “But do they agree with the broad thrust of where we think American public policy should go? Absolutely.” “He’s a really cool guy,” Yarvin said of the Vice-President, who followed him on X earlier this year. (The White House did not respond to requests for comment.)</p><p>Although Yarvin tried to be discreet, he mentioned that Thiel has a bit of a “weirdo edge” and described Andreessen, the venture capitalist, as someone who, “apart from the bizarre and possibly even nonhuman shape of his head, would seem much more normal than Peter.” After Andreessen invested in Yarvin’s startup, Tlon, the two got to know each other; they texted and went to brunch long before Andreessen came out as a Trump supporter, last year. Andreessen has been known to urge his associates to read Yarvin’s blog. “Tech people are not interested in appeals to virtue or beauty or tradition, like most conservatives,” the State Department official said. “They are more like right-wing progressives, and for a long time Moldbug was the only person speaking to them this way.” (Andreessen and Thiel declined to comment.) Apropos of his relationships with powerful men, Yarvin paraphrased to me “a wonderful piece of advice for courtiers” that he’d picked up from Lord Chesterfield’s “Letters to His Son,” an eighteenth-century etiquette manual addressed to the author’s illegitimate child: “Never bug them. And never let them forget you exist.”</p><p>Yarvin has had more success as a courtier to startup founders than as a founder himself. He launched Tlon in 2013, with a twentysomething former Thiel fellow. Yarvin approached computer science the same way he approached the U.S. government—with, as he put it, “utopian megalomania.” Yarvin’s visionary goal was to build a peer-to-peer computer network, named Urbit, that would allow users to control their own data, free from scolds, spies, and monopolies. Each user on the Urbit network is identified with an N.F.T. that acts like a digital passport. Even though Urbit promotes decentralization, the system is designed around a hierarchical model of virtual real estate, with users owning “planets,” “stars,” or “galaxies.”</p><p>In an early sketch of the system, Yarvin named himself its “prince,” but he struggled to attract subjects to his imaginary kingdom. Like Yarvin’s political theory, his programming language, which he wrote himself, was daring, abstruse, and sometimes mistaken for a hoax. Ever the contrarian, he reversed the meaning of zeros and ones. After decades of work and an estimated thirty million dollars of investment, Urbit seems to function less like a feudal society and more like the Usenet forums of Yarvin’s youth. (The trade publication <em>CoinDesk</em> has called it “a slower version of AOL Instant Messenger.”) “It doesn’t work the way it’s supposed to,” a former Urbit employee told me, describing Yarvin as “the world’s first computer-science crank.” Yarvin left the company in 2019.</p><p>No longer needing to worry about spooking investors, Yarvin threw himself into the life style of a self-described “rogue intellectual.” Under his own name, he launched a Substack newsletter, “Gray Mirror of the Nihilist Prince.” (Today, it is the platform’s third most popular “history” publication.) He became a fixture on the right-wing podcast circuit and seemed never to turn down an invitation to party. On his travels, he often hosted “office hours”—informal, freewheeling discussions with readers, many of them thoughtful young men, alienated by liberal guilt and groupthink. What wins Yarvin converts is less the soundness of his arguments than the transgressive energy they exude: he makes his listeners feel that he is granting them access to forbidden knowledge—about racial hierarchy, historical conspiracies, and the perfidy of democratic rule—that progressive culture is at pains to suppress. His approach seizes on the reality that most Americans have never learned how to defend democracy; they were simply brought up to believe in it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin advises his followers to avoid culture-war battles over issues like D.E.I. and abortion. It is wiser, he argues, to let the democratic system collapse on its own. In the meantime, dissidents should focus on becoming “fashionable” by building a reactionary subculture—a counter-Cathedral. Sam Kriss, a left-wing writer who has debated Yarvin, said of his work, “It flatters people who believe they can change the world simply by having weird ideas on the Internet and decadent parties in Manhattan.”</p><p>Such people have come to be known as the “dissident right,” a loose constellation of artists and strivers clustered around the Bay Area, Miami, and the Lower East Side micro-neighborhood Dimes Square. The milieu was drawn together by a frustration with electoral politics, <em>Covid</em> lockdowns, and the strictures of “wokeness.” Vice signalling has been central to the scene’s countercultural allure: instead of sharing pronouns and employing the approved nomenclature (“unhoused,” “Latinx,” “justice-involved person”), its members have revived insults like “gay” and “retarded.” Dasha Nekrasova and Anna Khachiyan, the hosts of the “Red Scare” podcast, are among the most prominent avatars of the scene. In 2021, Thiel helped to fund an anti-woke film festival in New York, and Yarvin read his poetry at one of its packed events. Urbit now hosts a literary magazine designed to look like <em>The New York Review of Books</em>. “If you are an intelligent Jewish-American urbanite who wants to play around with certain Nietzschean and eugenic themes, you aren’t going to join tiki-torch-bearing marchers chanting that ‘the Jews will not replace us,’&nbsp;” the conservative commentator Sohrab Ahmari observed in an essay last year. “No, you turn to the dissident right.”</p><p>Yarvin has emerged as a veteran edgelord of this crowd, which he compared to San Francisco’s gay subculture in the seventies and to the Lost Generation of literary modernists—tight-knit communities whose members bonded over their sense of being outsiders. James Joyce, he said, sold few copies of “Ulysses,” but his friends, like Ezra Pound and T.&nbsp;S. Eliot, “knew that what he was doing was good.” So it was with the creatives of the dissident right, whose endeavors, he felt, had been overlooked by the intolerant Cathedral. This past April, Yarvin pitched Darren Beattie, the acting Under-Secretary of State for Public Diplomacy, on a plan for “dissident-right art hos” to take over the American pavilion at the Venice Biennale.</p><p>Lately, Yarvin has been trying to flip some of his newly acquired cultural capital into the real thing. Last year, he returned to Urbit as a “wartime C.E.O.,” after which several top employees resigned, and in February he raised more money from Andreessen Horowitz. According to a draft of an unpublished Substack post, his newest plan is to promote Urbit as an élite private club whose members, he believes, are destined to become “the stars of the new public sphere—a new Usenet, a new digital Athens built to last forever.”</p><p>The night before Trump’s Inauguration, I drove Yarvin to a black-tie “Coronation Ball” at the Watergate Hotel, in Washington, D.C. The event was organized by a neo-reactionary publishing house, Passage Press, which recently released Yarvin’s book “<a data-offer-url="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P&quot;}" href="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" rel="nofollow noopener" target="_blank">Gray Mirror, Fascicle I: Disturbance</a>,” the first of a planned four-part cycle outlining his vision for a new political regime. Its endnotes predominantly consist of QR-code links to Wikipedia pages: “Denazification,” “L’État, c’est moi,” “Presentism (historical analysis).” As I negotiated the icy streets, Yarvin explained that during the Elizabethan era the finest minds in the arts and sciences were to be found at court. When I asked if he saw a parallel with Trump’s inner circle, he burst out laughing. “Oh, no,” he said. “My God.”</p><p>Like most journalists, I had been denied entry to the ball, so I ordered a drink at a bar in the lobby. Standing next to me was a man wearing a cowboy hat and a burgundy velour suit—a Yarvin enthusiast, it turned out, named Alex Maxa. He ran a party-bus company in San Francisco, and in his free time he made memes featuring Yarvin’s likeness. He said that he was drawn to Yarvin’s work because “it makes me feel like I’ve got something that people in Washington who think they’re really smart can’t actually make a compelling argument against.” He’d wanted to go to the ball but tickets, whose price had surged to twenty thousand dollars, were now sold out. Not long afterward, I met two of Yarvin’s friends, who encouraged me, and another journalist I was with, to confidently walk into the party with them. Maxa was already inside, having taken a similar approach. “Lol I just waltzed right in by asking where the coat check was,” he texted.</p><p>Passage Press had billed the event as “<em>MAGA</em> meets the Tech Right.” It was not false advertising. In a banquet hall awash in pink and purple light, Anton, from the State Department, Laura Loomer, a Trump whisperer known for her anti-Muslim bigotry, and Jack Posobiec, who popularized the Pizzagate conspiracy theory, mingled with venture capitalists, crypto accelerationists, and Substack all-stars. Earlier that evening, as guests dined on seared scallops and filet mignon, Steve Bannon, the ball’s keynote speaker, called for mass deportations, the “Götterdämmerung” of the administrative state, and Mark Zuckerberg’s imprisonment.</p><p>Eight years ago, Mike Cernovich, a first-gen alt-right influencer, had co-hosted an inaugural party known as the DeploraBall, a winking reference to Hillary Clinton’s unfortunate crack about half of Trump’s supporters belonging in a “basket of deplorables.” It was, by all accounts, a shambolic affair, plagued by journalists and protesters. One of Cernovich’s co-organizers, Tim Gionet, who goes by the online pseudonym Baked Alaska, was removed from his role after posting antisemitic content on Twitter. Now, at the Coronation Ball, Baked Alaska was served for dessert—a nod, it seemed, to Gionet, who was then on probation for participating in the January 6th insurrection. (He was pardoned by Trump the next day.) Cernovich pushed a baby around in a stroller and marvelled, like a proud father, at how&nbsp;far the movement had come. “I was one of the oldest guys in the place!” he tweeted the following afternoon. “Real right wing. High energy and high IQ.” In 2008, Yarvin, in his “Open Letter,” had called for a reactionary vanguard to form an underground political party. The Coronation Ball made it clear that this was no longer necessary. His web-addled counter-élite was now the establishment.</p><p>Yarvin was dressed in the same tuxedo, including a bright-red cummerbund, that he’d worn to a party at Thiel’s house in D.C. the night before, where, as <em>Politico</em> reported, Vance had amiably greeted him with “You reactionary fascist!” He’d also worn the tux to his wedding last year. Yarvin’s first wife died in 2021, from a hereditary heart disease, at the age of fifty. At the ball, he was accompanied by his second wife, Kristine Militello. A former Bernie Sanders supporter and an aspiring novelist, Kristine described herself as having been “red-pilled” during the pandemic, after losing her customer-service job at an online wine retailer. She first encountered Yarvin on YouTube, where she watched a video of him arguing against the legitimacy of the American Revolution, and proceeded to read everything he’d written. She sent him an admiring e-mail in 2022, seeking advice on how to break into New York’s dissident-right literary scene, and they met for drinks a few weeks later.</p><p>Recently, Yarvin has taken to describing himself as a “dark elf” whose role is to seduce “high elves”—blue-state élites—by planting “acorns of dark doubt in their high golden minds.” (In this Tolkien-inspired metaphor, red-state conservatives are “hobbits” who should submit to the “absolute power” of a new ruling class made up, unsurprisingly, of dark elves.) He didn’t always express himself so quaintly. In 2011, the day after the far-right terrorist Anders Behring Breivik killed sixty-nine people, many of them teen-agers, at a summer camp in Norway, Yarvin wrote, “If you’re going to change Norway into something new, you need the present ruling class of Norway to <em>join</em> and <em>follow</em> you. Or at least, you’ll need their children.” He praised Breivik for targeting the right group (“communists, not Muslims”), but condemned his methods: “Rape is beta. Seduction is alpha. Don’t slaughter the youth camp—<em>recruit</em> the youth camp.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin’s own recruitment efforts seemed to be working. Near the open bar, I spoke to Stevie Miller, a sprightly sophomore at Carnegie Mellon who has been reading Yarvin since the seventh grade. (Yarvin told me that he’d encountered several gifted Zoomers who’d read him as preteens because his “high-I.Q. style” served as a “high-I.Q. magnet.”) Two years ago, Miller hung out with Yarvin at Vibecamp, a gathering for nerds and techies in rural Maryland. Yarvin, who left early, asked Miller to help him throw his own party in D.C., which came to be known as Vibekampf. Afterward, Miller became Yarvin’s first personal intern. “My parents, New York Jewish liberals who I love, were totally mystified,” he said.</p><p>After half an hour, I was escorted out of the party, as were other reporters throughout the evening. Security mistook Maxa, my friend from the lobby, for one of our kind, and he was ejected, too, though not before pressing through the crowd to get his photo taken with the dark elf.</p><p>Even Trump’s most pessimistic critics have been startled by the speed with which the President, in his second term, has moved to impose autocracy on America, concentrating power in the executive branch—and often enough in the hands of the richest men on earth. Elon Musk, an unelected citizen, has led a squadron of twentysomethings on a spree through the federal government, laying off tens of thousands of civil servants, shuttering the U.S. Agency for International Development, and seizing control of the Treasury Department’s payment system. Meanwhile, the Administration has launched an assault on civil society, revoking funding at Harvard and other universities that it claims are bastions of ideological indoctrination and punishing law firms that have represented Trump’s opponents. It has expanded the machinery of immigration enforcement, deporting three U.S.-born children to Honduras, a group of Asian and Latin American immigrants to Africa, and more than two hundred Venezuelan migrants to a maximum-security prison in El Salvador, where they may remain until the end of their lives. U.S. citizens now find themselves with a government that claims the right to disappear them without due process: as Trump told Bukele, the President of El Salvador, during an Oval Office meeting, “Homegrowns are next.” Without a vigorous system of checks and balances, one man’s crank ideas—like starting an incoherent trade war that upends the global economy—don’t get filtered out. They become policies that enrich his family and his allies.</p><p>Since January, a cottage industry has arisen online to trace links between the government’s chaotic blitz of actions and Yarvin’s writings. Yarvin is hardly the Rasputin-like figure with Oval Office access that certain Bluesky users imagine him to be, but it isn’t difficult to see why some people may have come to this view. Last month, an anonymous <em>DOGE</em> adviser told the Washington <em>Post</em> that it was “an open secret that everyone in policymaking roles has read Yarvin.” Stephen Miller, the President’s deputy chief of staff, recently quote-tweeted him. Vance has called for the U.S. to retrench from Europe, a longtime Yarvin desideratum. Last spring, Yarvin proposed expelling all Palestinians from the Gaza Strip and turning it into a luxury resort. “Did I hear someone say ‘beachfront?’&nbsp;” he wrote on Substack. “The new Gaza—developed, of course, by Jared Kushner—is the LA of the Mediterranean, an entirely new charter city on humanity’s oldest ocean, sublime real estate with an absolutely perfect, Apple-quality government.” This February, during a joint press conference with Benjamin Netanyahu, the Israeli Prime Minister, Trump surprised his advisers when he made a nearly identical proposal, describing his redeveloped Gaza as “the Riviera of the Middle East.”</p><p>Whenever I asked Yarvin about resonances between his writing and real-world events, his response was nonchalant. He seemed to see himself as a conduit for pure reason—the only mystery, to him, was why it had taken others so long to catch up. “You can invent a lie, but you can only discover the truth,” he told me. We were in London, where he was attending the Alliance for Responsible Citizenship, a conservative conference co-founded by the psychologist Jordan Peterson. (Yarvin described Peterson to me as “a dandy” with “a weird narcissistic energy coming off of him.”) Accompanying Yarvin on his travels were Eduardo Giralt Brun and Alonso Esquinca Díaz, two millennial filmmakers who were shooting a documentary about his life. Their goal was to make a naturalistic character study in the style of “Grey Gardens,” in which, as Brun put it, “the camera just happens to be around.” It wasn’t going to plan. Yarvin kept repeating the same monologues, which meant that much of the footage was the same. The filmmakers worried that his racist remarks would turn viewers off. One afternoon in London, Díaz had filmed Yarvin getting his portrait painted with Lord Maurice Glasman, a post-liberal political theorist who has been called “Labour’s <em>MAGA</em> Lord,” for his support of Brexit and his ongoing dialogue with figures like Steve Bannon. At one point in their discussion, Yarvin had pulled out his iPhone to show Glasman that he’d hacked the chatbot Claude to get it to call him by the N-word.</p><p>Some thinkers would envy the attention Yarvin is receiving. But he dismissed his influence as a “fraudulent currency” since it has yet to cash out in the revolution he desires. He poured scorn on <em>DOGE</em> (“so much libertarian DNA”) and Trump’s tariff plan (not mercantilist enough). In a recent essay on Substack, he criticized the decision to dispatch plainclothes <em>ICE</em> officers to jail college students and professors for political speech—not on moral grounds, but because the thuggish optics were likely to provoke resistance. Yarvin’s oracular pronouncements and bottomless disdain for actually existing politics have inspired a viral post: his face under the words “Your anti-regime actions work well in practice. But do they work in theory?” The conservative activist Christopher Rufo has compared Yarvin to “a sullen teenager who insists that everything is pointless.” I came to think of him as a reactionary Goldilocks who would be satisfied with nothing less than the inch-perfect autocracy that he’d constructed in his mind.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61050&quot;}" href="https://www.newyorker.com/cartoon/a61050" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Vi-An Nguyen</span></p></div></span></p></figure><p>This apparent desire for control also shows up in some of his relationships. Not long ago, I visited Lydia Laurenson, Yarvin’s ex-fiancée, in Berkeley. The two began dating in September, 2021, after Yarvin posted a personal ad on Substack, explaining that he’d recently lost his “widower virginity” and was looking to meet someone of “childbearing age.” Laurenson, a freelance writer and editor, replied the same day: “I have historically been a liberal but my IQ is really high, I want kids, and I’m incredibly curious to talk to you.” Yarvin went on Zoom dates with other women who answered the post—among them, Caroline Ellison, the ex-girlfriend of the now imprisoned crypto entrepreneur Sam Bankman-Fried—but he and Laurenson soon found themselves in an all-consuming romance. She told me that the ethos of her relationship with Yarvin was “&nbsp;‘We’re going to be geniuses together and have genius babies.’ I’m making fun of it a little bit, but that really was it.”</p><p>Like Yarvin, Laurenson had been a precocious child who went to college early. She’d also maintained a blog with a cult following, where, under the pseudonym Clarisse Thorn, she wrote about sex-positive feminism, B.D.S.M., and pickup artistry. She and Yarvin fought often, sometimes about politics. Laurenson had moved away from the left, but she hadn’t fully embraced neo-reaction. When I asked her if she’d ever changed Yarvin’s mind about anything, she said she’d gotten him to stop using the N-word, at least around her. (He later told this magazine that he was not using the word in the spirit of “a Southern plantation owner.”)</p><p>The bigger source of tension, according to Laurenson, was Yarvin’s autocratic attachment style. When they fought, Laurenson said, he insisted that she provide a rational justification for ending hostilities. She felt that Yarvin’s slippery personal attacks resembled his manner in public debates. “He makes up explanations that seem reasonable, but are actually false; he attacks the character of the person who is trying to point out what he’s doing; it’s like a DDOS attack of the soul,” she told me in an e-mail, referencing the cyberattack strategy of overwhelming a server with traffic from multiple sources. James Dama, a friend of Laurenson’s who had his own falling out with Yarvin, recalled, “He would make a coarse joke about Lydia’s weight or looks, not get a laugh, and then get angry at Lydia for being too stuck up.” (Tanner, Yarvin’s first girlfriend, described a similar pattern of insults and demands.)</p><p>Laurenson and Yarvin broke up in the summer of 2022, while Laurenson was pregnant. He told me that his desire for closeness might have struck Laurenson as “overbearing and stifling,” and that he had a bad habit of making “a joke that’s sort of a barb,” but he denied that he was ever purposefully cruel during the relationship. (He added that, after the relationship ended, “my natural instinct was, I’m going to cut her down to size every time I can”—something, he noted, he was “very good at.”) A few weeks after their son was born, that December, Yarvin sued for partial custody, which he received. An ongoing family-court case remains acrimonious. “The parents are in disagreement about nearly every issue,” their mediator observed last year.</p><p>Now that they share a toddler, Laurenson spends a lot of time thinking about Yarvin’s own childhood. “He has this class-clown thing going on, where he very much craves attention,” she said. To her, it seemed that his embrace of a provocative ideology was a kind of “repetition compulsion,” a psychological defense that allowed him to reframe the ostracization he experienced growing up. As America’s most famous living monarchist, he could tell himself that people were rejecting him for his outré ideas, not for his personality. She wondered if he’d first adopted “the monarchist thing” as a kind of intellectual sport, a bit from Usenet, and then, like the parallel world in the Borges story, it had slowly taken on a reality of its own. “Is it just like you found this place where people admire you and allow you to troll as much as you want, and then you just live in that world?” she asked.</p><p>In the past decade, liberalism has taken a beating from both sides of the political spectrum. Its critics to the left view its measured gradualism as incommensurate to the present’s multiple emergencies: climate change, inequality, the rise of an ethno-nationalist right. Conservatives, by contrast, paint liberalism as a cultural leviathan that has trampled traditional values underfoot. In “<a data-offer-url="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447&quot;}" href="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" rel="nofollow noopener" target="_blank">Why Liberalism Failed</a>” (2018), the Notre Dame political scientist Patrick Deneen argues that the contemporary American emphasis on individual freedom has come at the expense of family, faith, and community, turning us into “increasingly separate, autonomous, non-relational selves replete with rights and defined by our liberty, but insecure, powerless, afraid, and alone.” Other post-liberal theorists, including Adrian Vermeule, have proposed that the state curtail certain rights in the service of an explicitly Catholic “common good.”</p><p>Yarvin is calling for something simpler and more libidinally satisfying: to burn it all down and start again from scratch. Since the advent of neoliberalism in the late seventies, political leaders have increasingly treated governance like corporate management, turning citizens into customers and privatizing services. The result has been greater inequality, a weakened social safety net, and the widespread perception that democracy itself is to blame for these ills, creating an appetite for exactly the kind of autocratic efficiency Yarvin now extolls. “A Yarvin program might seem seductive during a period of neoliberal rule, where efforts to change things, whether it is global warming or the war machine, feel futile,” the historian Suzanne Schneider told me. “You can sit back, not give a fuck, and let someone else run the show.” Yarvin has little to say on the question of human flourishing, or about humans in general, who appear in his work as sheep to be herded, idiots to be corrected, or marionettes controlled by leftist puppeteers.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Whatever gift Yarvin has for attracting attention, his work does not survive scrutiny. It is full of spurious syllogisms and arguments retconned to match his jaundiced intuitions. He has read widely, but he uses his knowledge merely as grist for the same reactionary fairy tale: once upon a time, people knew their place and lived in harmony; then along came the Enlightenment, with its “noble lie” of egalitarianism, plunging the world into disorder. Yarvin often criticizes academics for treating history like a Marvel movie, with oversimplified heroes and villains, but it’s unclear what he adds to the picture by calling Napoleon a “startup guy.” (He has favored the revisionist theories that Shakespeare’s plays were really written by the seventeenth Earl of Oxford and that the American Civil War, which he calls the War of Secession, worsened living conditions for Black Americans.) “The neat thing about primary sources is that often, it takes only one to prove your point,” he has proclaimed, which would come as news to historians.</p><p>Some of his most thoroughgoing critics are on the right. Rufo, the conservative activist, has written that Yarvin is a “sophist” whose debating style consists of “childish insults, bouts of paranoia, heavy italics, pointless digressions, competitive bibliography, and allusions to cartoons.” He added, “When one tries to locate what it is that you actually think, he cannot help but discover that there really isn’t much substance there.” The most generous engagement with Yarvin’s ideas has come from bloggers associated with the rationalist movement, which prides itself on weighing evidence for even seemingly far-fetched claims. Their formidable patience, however, has also worn thin. “He never addressed me as an equal, only as a brainwashed person,” Scott Aaronson, an eminent computer scientist, said of their conversations. “He seemed to think that if he just gave me one more reading assignment about happy slaves singing or one more monologue about F.D.R., I’d finally see the light.”</p><p>Intellectual seriousness may not be the point. Yarvin’s polemics have proved useful for those on the right in search of a rationale for nerd ressentiment and plutocratic will to power. “The guy does not have a coherent theory of the case,” the Democratic senator Chris Murphy, from Connecticut, told me. “He just happens to be saying something out loud that a lot of Republicans are eager to hear.”</p><p>It is not difficult to anticipate the totalitarian endgame of a world view that marries power worship with a contempt for human dignity—fascism, as some might call it. Like his ideological nemeses the Bolsheviks, Yarvin seems to believe that the only thing standing in the way of Utopia is an unwillingness to use every means possible to achieve it. He claims that the transition to his regime will be peaceful, even joyous, but fantasies of violence flicker throughout his work. “Unless the monarch is ready to actually <em>genocide</em> the nobility or the masses, he has to capture their loyalty,” he wrote in a Substack post in March. “You’re not going to <em>foam</em> these people, like turkeys with bird flu. Right?”</p><p>Yarvin’s strong opinions on how the world ought to work extended to this profile. Some of his suggestions were intriguing: he floated the idea of staging a debate with one of his ex-girlfriends, and invited me to follow him to Doha for a meeting with Omar bin Laden, one of Osama’s sons. Others were officious. At one point, he sent me nine texts objecting to my use of the word “extreme”—“a hostile pejorative,” he explained, which my article would be better off without. (He’d previously boasted several times in our taped conversations that he was more “extreme” than anyone in the current Administration.) A few days after the Coronation Ball at the Watergate Hotel, he wrote to <em>The New Yorker</em> to complain that I’d walked in without his publisher’s permission; he said that he hoped the incident would not turn into “Watergate 2,” and referred to himself as “certainly the most media-friendly person in the scene!” (Jonathan Keeperman, his publisher at Passage Press and the host of the ball, once suggested that the Republican Party should “lamppost”—that is, lynch—“the journos,” so this was not a particularly high bar to clear.)</p><p>One morning this winter, I woke up to twenty-eight texts from Yarvin expressing concerns about my reporting technique. “The problem is that your process is slack and I can feel it generating low-quality content—because it’s not adversarial enough,” he wrote. “When the process is not adversarial, I don’t know what I am contending against.” He briefly considered whether I was “too dumb to understand the ideas,” or whether I’d succumbed to the mental self-censorship that Orwell called “crimestop.” He urged me to watch “The Lives of Others,” an Oscar-winning film that depicts the relationship between an East German playwright and a Stasi agent who is tasked with surveilling him. The Stasi agent, he wrote, “can actually write up the ideas of the playwright, *without even thinking them* It is not even that he is ‘opposed’ to the dissident ideas. It is that he does not even let them touch his brain.” In the film, the Stasi agent eventually “cracks,” after he comes to sympathize with the playwright’s views. Yarvin, presumably, was the playwright.</p><p>He said that he was coming to see me, on the other hand, as an “NPC,” or non-player character. He proposed giving me a Voight-Kampff test, the fictional exam in “Blade Runner” used to distinguish androids from humans. His version would involve the two of us debating “the ‘blank slate theory’ versus ‘racism’&nbsp;” and recording the conversation. (“By ‘racism’ I mean of course human biodiversity,” he elaborated.) When I explained that my reporting process did not include submitting to on-demand tests, Yarvin sent me a screenshot of “August 1968,” W.&nbsp;H. Auden’s poem about the Soviet-led invasion of Czechoslovakia to suppress the Prague Spring:</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>The Ogre does what ogres can<br>Deeds quite impossible for Man,<br>But one prize is beyond his reach,<br>The Ogre cannot master Speech</p></blockquote><p>He went on to say that although he’d agreed to participate in this story because “no publicity is bad publicity,” he would now try to kill it if he could.</p><p>I was struck by the contrast between his messages and the coolheaded tone he’d recommended that Thiel and other friends deploy when handling the media. After the 2013 <em>TechCrunch</em> article identifying Yarvin came out, Balaji Srinivasan, the entrepreneur, proposed in an e-mail “to sic the Dark Enlightenment audience on a single vulnerable hostile reporter to dox them.” Yarvin dissuaded him. “What would Heartiste say?” Yarvin asked, referring to the white-nationalist pickup-artist blog “Chateau Heartiste.” “Almost always, the right alpha answer is ‘nothing.’ Say nothing. Do nothing.”</p><p>On a balmy afternoon in late February, Yarvin and his wife, Kristine, were driving down a country road in the South of France. They were accompanied by the documentarians, Brun and Díaz. “Where are we going, Kristine?” Brun asked from the passenger seat, turning the camera around to film her in the back beside me.</p><p>She said that she had only the vaguest notion. “Honestly, he just tells me everything last minute,” she explained. “It’s kind of like being a dog. You just know that you’re going in the car, and you don’t know if you’re gonna go to the dog park, or you’re gonna go to the vet, and you’ll find out when you get there.”</p><p>“Spontaneity,” Yarvin chimed in.</p><p>“That’s a word for it,” Kristine teased.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22211&quot;}" href="https://www.newyorker.com/cartoon/a22211" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by John O’Brien</span></p></div></span></p></figure><p>We were on our way to meet Renaud Camus, a seventy-eight-year-old novelist and pamphleteer, who, in 2011, published “The Great Replacement,” an incendiary manifesto that argued that liberal élites were behind a conspiracy to replace white Europeans with migrants from Africa and the Middle East. The title phrase has since become a rallying cry for white nationalists around the world, from Charlottesville, Virginia, where, in 2017, marchers chanted, “You will not replace us,” to Christchurch, New Zealand, where, two years later, a man who’d published a manifesto with the same title as Camus’s killed fifty-one Muslims.</p><p>As we crested a hill, the walls of Camus’s castle, Château de Plieux, loomed into view. “Does anyone know if he’s related to Albert Camus?” Yarvin asked. “I think he’s not related to Albert, but he’s a lovely, old, gay, literary Frenchman.”</p><p>Brun, who is Venezuelan, wondered what he would do if Camus “has a sign that says ‘No Foreigners Allowed.’&nbsp;”</p><p>“Well, are you here to replace us?” Kristine joked. Nobody replied.</p><p>Yarvin rang an impressive metal bell beside the door, and we were soon ushered inside by Pierre Jolibert, Camus’s partner. Upstairs, Camus was waiting for us with a bottle of champagne. With his manicured white beard and brown corduroy jacket, complete with a bow tie and gold pocket-watch chain, he looked like a nineteenth-century man of letters. Speaking perfect English, with an English accent, he made it sound as though he’d had no choice but to buy the castle, which dated from the early thirteen-hundreds, after his library grew too large for his small Parisian flat. That was thirty-five years ago. Now, acknowledging the stacks of books that were overtaking his cavernous study, he said that he was running into the same problem here.</p><p>Over several glasses of champagne, Yarvin fired a series of questions at Camus, though he rarely waited long enough for his host to give a full answer. What did Camus think of Philippe Pétain? Charles de Gaulle? Napoleon III? Napoleon I? Ernst Jünger? Ernst von Salomon? Ezra Pound? Basil Bunting? More than an interaction, Yarvin, the former trivia champion, seemed to want a pat on the head for his display of learning.</p><p>After we headed downstairs for lunch—strips of sizzling duck, a quiche Lorraine, red wine—Yarvin resumed his cross-examination. Did Camus rate Thomas Carlyle? Michel Houellebecq? Louis XIV? What would he say to Charles Maurras if he were alive today? What would Dostoyevsky have thought about the <em>Covid</em> lab-leak theory?</p><p>Camus let out a high-pitched giggle whenever Yarvin asked a particularly odd question, but he was baffled by his guest’s repeated inquiries about Brigitte Macron, the French First Lady, who Yarvin suspected was actually a man. “We are dealing with the most important thing in the history of the Continent,” Camus exclaimed, referring to the rise of nonwhite immigration to Europe. “What does it matter if Mrs. Macron is a man or woman?”</p><p>Brun asked the men to move to a window so that he could shoot them from outside. As Yarvin gazed at the patchwork of neatly tended fields below, he spoke about the Great Replacement as “one of the greatest crimes” in history. “Is it greater than the Holocaust? I don’t know.&nbsp;.&nbsp;.&nbsp;. We haven’t seen it play out yet.” He’d been drinking since his arrival and seemed to be in an emotional state. “I have three children,” he told Camus. “Will they be basically lined up and marched into mass graves?” They had been discussing Jean Raspail’s apocalyptic novel, “The Camp of the Saints” (1973), which depicts an invasion of Indian migrants destroying European nations. Sobbing now, he continued, “I want my children to die in the twenty-second century. I don’t want them to experience some kind of insane post-colonial Holocaust.”</p><p>After dessert, coffee, and a rum from Guadeloupe, it was time for an evening stroll. Carrying a wooden cane, Camus led Yarvin through the small town of Plieux. Spring had arrived early: a cherry tree was blossoming with little flowers. As they passed the local church, Yarvin took out his phone to show Camus a photo of the toddler he shares with Laurenson. “The mother of that child was not my wife,” he said confidingly. A moment later, he was reading a poem by C.&nbsp;P. Cavafy, in tears once again.</p><p>When Yarvin and Camus went on ahead, the filmmakers paused to assess the day’s shoot. Brun said that Yarvin reminded him of the long-winded character in “Airplane!” who talks so incessantly that it drives his seatmates to kill themselves. We wondered what Camus was making of the afternoon. It wasn’t long before we found out. “If intellectual exchanges were commercial exchanges—which they are, to a certain extent—the amount of my exports would not reach one per cent of that of my imports,” Camus wrote in his diary, which he posted online the following day. “The visitor spoke without interruption from his arrival to his departure, for five hours, very quickly and very loudly, interrupting himself only for curious fits of tears, when he spoke of his deceased wife, but also, more strangely, certain political situations.”</p><p>It was dark by the time we all returned to the château. “Thank you so much for your hospitality and your duck and your castle,” Yarvin said, looking around. “How much money did you spend on it?”</p><p>Lovingly squeezing Yarvin’s arm, Kristine said, “You can’t just ask people that!”</p><p>Camus gave Yarvin some of his books as souvenirs, but Yarvin’s mind already seemed elsewhere. Tomorrow, he would fly to Paris to meet with a group of red-pilled Zoomers and Éric Zemmour, a far-right polemicist who once ran to be the President of France.</p><p>As we headed to the car, Yarvin was buzzing with boyish excitement about his performance. He turned to me and the filmmakers. “Was that good?” he asked. “Was that good?”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Notes Expected to Gain Markdown Support in iOS 26 (272 pts)]]></title>
            <link>https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</link>
            <guid>44183923</guid>
            <pubDate>Wed, 04 Jun 2025 18:29:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/">https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</a>, See on <a href="https://news.ycombinator.com/item?id=44183923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/06/04/apple-notes-rumored-markdown-support-ios-26/"><p>Apple's Notes app is rumored to be getting limited Markdown support in iOS 26 and macOS 26, according to <em><a href="https://9to5mac.com/2025/06/03/exclusive-ios-26-messages-carplay-more/">9to5Mac</a></em>. The feature would allow users to export text in the markdown format.</p>
<p><img src="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy" srcset="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 400w,https://images.macrumors.com/t/rNrX5sZI2CoLmeluvBSY9PV3oIQ=/800x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 800w,https://images.macrumors.com/t/MFdPb4DjJL7dZ3MuK5-yqgdRmro=/1600x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 1600w,https://images.macrumors.com/t/_Eb67huucTUo6d63cc3KL-IEMjQ=/2500x0/filters:no_upscale()/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 26 Mock Rainbow Feature" width="2500" height="1406"><br>Markdown is a lightweight markup language that some writers prefer to use over rich text. Rather than using HTML for bold, italics, links, and headers, Markdown uses quick character shortcuts like **bold** or #header. It sounds like the feature will only add support for exporting text with markdown formatting and not writing in markdown directly. </p>
<p>If the rumor holds up, it's likely to be unveiled at next week's Worldwide Developers Conference alongside other iOS 26 improvements, including <a href="https://www.macrumors.com/2025/06/03/ios-26-messages-app-rumors/">automatic translation and polls in Messages</a>, not to mention a <a href="https://www.macrumors.com/2025/03/17/apple-believes-users-love-major-ios-19-redesign/">major visual redesign</a>.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/06/01/new-icloud-plus-perk-this-year/">iPhone Users Who Pay for iCloud Storage Received a New Perk This Year</a></h3><p>If you pay for iCloud storage on your iPhone, Apple introduced an additional perk for you this year, at no additional cost.
The perk is the ability to create invitations in the Apple Invites app for the iPhone, which was released in the App Store in February.
In the Apple Invites app, iCloud+ subscribers can create invitations for any occasion, such as birthday parties, graduations, baby...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/apple-sleek-peek/">Apple Shares New 'Sleek Peek' Teaser Ahead of WWDC 2025 Next Week</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time. Ahead of the annual developer conference, Apple updated its WWDC page today with a new "Sleek peek" tagline, which replaces the original "On the horizon" tagline that it used over the past few weeks.
The graphic for WWDC 2025 has also been updated. It is now a...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/what-to-expect-from-ios-18-6/">What to Expect From iOS 18.6 as One of the Final Updates Before iOS 26</a></h3><p>It has been three weeks as of today since Apple released iOS 18.5, and we are still waiting for the first iOS 18.6 beta to follow.
Below, we outline everything we know about iOS 18.6 so far.
Timing
Apple's software engineers have been internally testing iOS 18.6 since late March, according to the MacRumors visitors logs.
The first betas of iOS 13.6 through iOS 16.6 were all released...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ios-26-to-upgrade-carplay-in-two-ways/">iOS 26 to Upgrade CarPlay in Two Ways</a></h3><p>While the spotlight has been on CarPlay Ultra lately, the regular version of CarPlay is set to receive some enhancements alongside iOS 26.
Apple will announce iOS 26 at WWDC 2025 next week, and the software update is expected to upgrade the CarPlay experience in at least two ways.
The first iOS 26 beta should be seeded to developers shortly after Apple's keynote, and the update will...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/macos-26-tahoe-what-to-expect/">WWDC 2025: What to Expect From macOS 26 Tahoe</a></h3><p>Monday June 2, 2025 4:17 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>WWDC is less than a week away, and as we ramp up to the big announcement, we're going to share details on what we know about each operating system. We're starting with the next-generation version of macOS, which Apple is apparently going to call macOS Tahoe.
Name
Since the current version of macOS is macOS 15, it would normally be followed by macOS 16, but Apple is changing its naming...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/ios-26-rumored-features/">iOS 26 Will Add These New Features to Your iPhone</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time.
Apple will announce its latest software updates, including iOS 26, iPadOS 26, macOS 26, tvOS 26, watchOS 26, and visionOS 26, which are all rumored to feature a sleek new glass-like design. There might not be any hardware announcements, however, as Bloomberg's Mark...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ex-apple-designer-living-glass-ios-concepts/">Ex-Apple Designer Reveals 'Living Glass' iOS 26 Concepts</a></h3><p>Wednesday June 4, 2025 4:17 am PDT by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Designer Sebastiaan de With has published an impressive preview of what Apple's rumored iOS redesign might look like, complete with detailed mockups and a design philosophy that he believes could reshape how users interact with their devices.
With WWDC just days away, de With – co-founder of photography app maker Lux and former Apple designer – has created what he calls "Living Glass"...</p></div><div><h3><a href="https://www.macrumors.com/2025/05/27/iphone-17-pro-rumors-list/">iPhone 17 Pro Launching Later This Year With These 12 New Features</a></h3><p>While the iPhone 17 Pro and iPhone 17 Pro Max are not expected to launch until September, there are already plenty of rumors about the devices.
Below, we recap key changes rumored for the iPhone 17 Pro models as of May 2025:
Aluminum frame: iPhone 17 Pro models are rumored to have an aluminum frame, whereas the iPhone 15 Pro and iPhone 16 Pro models have a titanium frame, and the iPhone X ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A proposal to restrict sites from accessing a users’ local network (499 pts)]]></title>
            <link>https://github.com/explainers-by-googlers/local-network-access</link>
            <guid>44183799</guid>
            <pubDate>Wed, 04 Jun 2025 18:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/explainers-by-googlers/local-network-access">https://github.com/explainers-by-googlers/local-network-access</a>, See on <a href="https://news.ycombinator.com/item?id=44183799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Explainer for Local Network Access</h2><a id="user-content-explainer-for-local-network-access" aria-label="Permalink: Explainer for Local Network Access" href="#explainer-for-local-network-access"></a></p>
<p dir="auto">This proposal is an early design sketch by the Chrome Secure Web and Network team to describe the problem below and solicit feedback on the proposed solution. It has not been approved to ship in Chrome.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proponents</h2><a id="user-content-proponents" aria-label="Permalink: Proponents" href="#proponents"></a></p>
<ul dir="auto">
<li>Chrome Secure Web and Network team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Participate</h2><a id="user-content-participate" aria-label="Permalink: Participate" href="#participate"></a></p>
<ul dir="auto">
<li><a href="https://github.com/explainers-by-googlers/local-network-access/issues">https://github.com/explainers-by-googlers/local-network-access/issues</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Currently public websites can probe a user's local network, perform CSRF attacks against vulnerable local devices, and generally abuse the user's browser as a "confused deputy" that has access inside the user's local network or software on their local machine. For example, if you visit <code>evil.com</code> it can use your browser as a springboard to attack your printer (given an HTTP accessible printer exploit).</p>
<p dir="auto">Local Network Access aims to prevent these undesired requests to insecure devices on the local network. This is achieved by deprecating direct access to private IP addresses from public websites, and instead requiring that the user grants permission to the initiating website to make connections to their local network.</p>
<p dir="auto"><em>Note:</em> This proposal builds on top of Chrome's previously paused <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access (PNA) work</a>, but differs by gating access on a permission rather than via preflight requests. This increases the level of user control (at the expense of new permissions that have to be explained to the user) but removes the explicit "device opt-in" that the preflight design achieved. We believe this simpler design will be easier to ship, in order to mitigate the real risks of local network access today. Unlike the previous Private Network Access proposal, which required changes to devices on local networks, this approach should only require changes to sites that need to access the local network. Sites are much easier to update than devices, and so this approach should be much more straightforward to roll out.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goals</h2><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<ul dir="auto">
<li><strong>Stop exploitation of vulnerable devices and servers from the drive-by web.</strong></li>
<li>Allow public websites to communicate to private network devices when the user expects it and explicitly allows it.</li>
</ul>
<p dir="auto">An adjacent goal is that we want a path for browsers to be good stewards of OS-level local network access permissions. These OS-level permissions are increasingly common (<a href="https://developer.apple.com/documentation/technotes/tn3179-understanding-local-network-privacy" rel="nofollow">on iOS, and more recently on macOS</a>) -- simply because the <em>browser</em> has been granted the permission (for legitimate browser functionality the user may want to use, like mirroring the contents of a tab on a local device) should not expose users' local devices to the risks of the open web.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Non-goals</h2><a id="user-content-non-goals" aria-label="Permalink: Non-goals" href="#non-goals"></a></p>
<ul dir="auto">
<li>Break existing workflows and services that rely on a public web frontend that can control local network devices.
<ul dir="auto">
<li>As long as there is <em>some</em> path forward we should be okay with breaking some use cases (e.g., iframe and HTML subresources that aren't explicitly sourced from local hostnames), but overall we want to minimize breakage.</li>
</ul>
</li>
<li>Solve the local network HTTPS problem.
<ul dir="auto">
<li>As stated in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#non-goals">original Private Network Access explainer</a>: <em>Provide a secure mechanism for initiating HTTPS connections to services running on the local network or the user's machine. This piece is missing to allow secure public websites to embed non-public resources without running into mixed content violations, with the exception of <a href="http://localhost/" rel="nofollow">http://localhost</a> which is embeddable. While a useful goal, and maybe even a necessary one in order to deploy Private Network Access more widely, it is out of scope of this specification.</em></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 1</h3><a id="user-content-use-case-1" aria-label="Permalink: Use case 1" href="#use-case-1"></a></p>
<p dir="auto">The most common case is for users who don't have any services or devices on their local network that expect connections from websites. Today, browsers freely allow JavaScript and subresource requests to these devices without any indication to the end user. Unless this behavior is expected by the user, users should not be exposed to this risk by default.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 2</h3><a id="user-content-use-case-2" aria-label="Permalink: Use case 2" href="#use-case-2"></a></p>
<p dir="auto">Public web frontend for controlling or setting up local devices (such as an IoT device, home router, etc.).</p>
<p dir="auto">Device manufacturers want to be able to give users an easy process for setting up a new device, and one method that is used is to have a page hosted on the manufacturer's public website which then communicates with the device via the user's browser, explicitly relying on the browser's vantage point inside the user's network.</p>
<p dir="auto">This also reduces the complexity needed on the device itself -- for example, a smart toothbrush does not need to support a full webserver. Additionally, by being a public webpage under the control of the manufacturer, the setup page is always up to date.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proposed Solution</h2><a id="user-content-proposed-solution" aria-label="Permalink: Proposed Solution" href="#proposed-solution"></a></p>
<p dir="auto">We propose gating the ability for a site to make requests to the users' local network behind a new "local network access" permission. Any origin that has not been granted this permission would be blocked from making such requests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Address spaces</h3><a id="user-content-address-spaces" aria-label="Permalink: Address spaces" href="#address-spaces"></a></p>
<p dir="auto">As defined in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#address-spaces">original Private Network Access proposal</a>, we organize an IP network into three layers from the point of view of a node, from most to least private:</p>
<ul dir="auto">
<li>Localhost: accessible only to the node itself, by default</li>
<li>Private IP addresses: accessible only to the members of the local network (e.g. RFC1918)</li>
<li>Public IP addresses: accessible to anyone</li>
</ul>
<p dir="auto">We call these layers <strong>address spaces</strong>: <code>loopback</code>, <code>local</code>, and <code>public</code>.</p>
<p dir="auto"><em>(Note: The original PNA proposal called these <code>local</code>, <code>private</code>, and <code>public</code>. Changing this was considered in <a data-error-text="Failed to load title" data-id="1459537487" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/91" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/91/hovercard" href="https://github.com/WICG/private-network-access/issues/91">WICG/private-network-access#91</a> but reverted due to already using the "private network access" name and values in headers implemented by sites and device manufacturers.)</em></p>
<p dir="auto">We note that <code>local</code> includes <a href="https://datatracker.ietf.org/doc/html/rfc1918" rel="nofollow">RFC 1918</a>/<a href="https://datatracker.ietf.org/doc/html/rfc4193" rel="nofollow">RFC 4193</a> private/local IP addresses and <a href="https://datatracker.ietf.org/doc/html/rfc6762" rel="nofollow">RFC 6762</a> link-local names (<code>.local</code> hostnames). (See <a href="https://github.com/WICG/private-network-access/issues/4" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/4/hovercard">discussion on the original PNA proposal repository</a>.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Local network requests</h3><a id="user-content-local-network-requests" aria-label="Permalink: Local network requests" href="#local-network-requests"></a></p>
<p dir="auto">We define a <strong>local network request</strong> as a request crossing an address space boundary to a more-private address space. That is, any of the following are considered to be local network requests:</p>
<ol dir="auto">
<li><code>public</code> -&gt; <code>local</code></li>
<li><code>public</code> -&gt; <code>loopback</code></li>
<li><code>local</code> -&gt; <code>loopback</code></li>
</ol>
<p dir="auto">Note that <code>local</code> -&gt; <code>local</code> is not a local network request, as well as <code>loopback</code> -&gt; anything. (See "cross-origin requests" below for a discussion on potentially expanding this definition in the future.)</p>
<p dir="auto">A request is considered to be going to a <code>local</code> space if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC 1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain (per RFC 6762), or</li>
<li>The <code>fetch()</code> call is annotated with <code>targetAddressSpace="local"</code> (see "Integration with Fetch" below).</li>
</ul>
<p dir="auto">In these cases we know <em>a priori</em> that the request is <code>local</code>.</p>
<p dir="auto">Similarly, if a request is to a loopback IP literal (e.g., 127.0.0.1), <code>localhost</code>, or the <code>fetch()</code> call is annotated with <code>targetAddressSpace="loopback"</code>, then the request is <code>loopback</code>.</p>
<p dir="auto">Separately, a request may eventually end up being considered a local network request if the request's hostname resolves to a private or loopback IP address. (We do not know this a priori however, and so cannot exempt these requests from mixed content blocking -- see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Permission prompts</h3><a id="user-content-permission-prompts" aria-label="Permalink: Permission prompts" href="#permission-prompts"></a></p>
<p dir="auto">When a site makes a local network request, the UA should check if the origin has already been granted the "local network access" permission. If not, the request should be blocked while the UA displays a prompt to the user asking whether they want to allow the origin to make requests to their local network. If the user denies the permission prompt, the request fails. If the user accepts the permission prompt, the request continues.</p>
<p dir="auto">To reduce breakage (due to the lack of local network HTTPS), the permission also exempts requests that are known to be <code>local</code> or <code>loopback</code> from mixed content blocking (see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How this solution would solve the use cases</h3><a id="user-content-how-this-solution-would-solve-the-use-cases" aria-label="Permalink: How this solution would solve the use cases" href="#how-this-solution-would-solve-the-use-cases"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 1: unexpected usage</h4><a id="user-content-use-case-1-unexpected-usage" aria-label="Permalink: Use case 1: unexpected usage" href="#use-case-1-unexpected-usage"></a></p>
<p dir="auto">For a user who is not expecting a site to connect to their local network, when <code>example.com</code> tries to call <code>fetch("http://192.168.0.1/routerstatus")</code>, the user's browser will ask whether or not to allow <code>example.com</code> to make connections to the local network. Since the user is not expecting this behavior, they can deny the permission request, and <code>example.com</code> is blocked from making these connections.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 2: controlling local devices</h4><a id="user-content-use-case-2-controlling-local-devices" aria-label="Permalink: Use case 2: controlling local devices" href="#use-case-2-controlling-local-devices"></a></p>
<p dir="auto">An existing site run by a device manufacturer that talks to a local device by making <code>fetch()</code> requests would potentially make minor modifications (for example, ensuring that they either use a private IP address or <code>.local</code> name when referring to the device, or adding the <code>targetAddressSpace="local"</code> property to their <code>fetch()</code> calls). When the site first tries to make a request to the device, the user sees a permission prompt. If the user expects the site to be communicating with devices on their local network, they can choose to grant the permission and the site will continue to function. If the user does not expect this or does not want to grant the permission to the site, they choose to not grant the permission to the site, and no local network requests from the site will be allowed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed design discussion</h2><a id="user-content-detailed-design-discussion" aria-label="Permalink: Detailed design discussion" href="#detailed-design-discussion"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Fetch</h3><a id="user-content-integration-with-fetch" aria-label="Permalink: Integration with Fetch" href="#integration-with-fetch"></a></p>
<p dir="auto">The Fetch spec does not integrate the details of DNS resolution, only defining an <strong>obtain a connection</strong> algorithm, thus Local Network Access checks are applied to the newly-obtained connection. Given complexities such as Happy Eyeballs (<a href="https://datatracker.ietf.org/doc/html/rfc6555" rel="nofollow">RFC6555</a>, <a href="https://datatracker.ietf.org/doc/html/rfc8305" rel="nofollow">RFC8305</a>), these checks might pass or fail non-deterministically for hosts with multiple IP addresses that straddle IP address space boundaries.</p>
<p dir="auto">After we have obtained a connection, if we detect a local network request:</p>
<ul dir="auto">
<li>If the client is not in a secure context, block the request.</li>
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the request will proceed.</li>
</ul>
<p dir="auto">However, the requirement that local network requests be made from secure contexts means that any insecure request will be blocked as mixed content unless we can know ahead of time that the request should be considered a local network request.</p>
<p dir="auto">To make this easier for developers, we propose adding a new parameter to the <code>fetch()</code> options bag to explicitly tag the address space of the request. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fetch(&quot;http://router.com/ping&quot;, {
  targetAddressSpace: &quot;local&quot;,
});"><pre><span>fetch</span><span>(</span><span>"http://router.com/ping"</span><span>,</span> <span>{</span>
  <span>targetAddressSpace</span>: <span>"local"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto">This would instruct the browser to allow the fetch even though the scheme is non-secure and obtain a connection to the target server. The <code>targetAddressSpace</code> should be either <code>local</code> or <code>loopback</code>.</p>
<p dir="auto">If the remote IP address does not belong to the IP address space specified as the targetAddressSpace option value, then the request is failed. This ensures the feature cannot be abused to bypass mixed content in general. See "Mixed content" below for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mixed content</h3><a id="user-content-mixed-content" aria-label="Permalink: Mixed content" href="#mixed-content"></a></p>
<p dir="auto">There is a challenge in the combination of (1) requiring secure contexts in order to make PNA requests, (2) trying to load PNA subresources over HTTP (due to the lack of local network HTTPS), and (3) applying mixed content blocking. The Fetch specification applies mixed content upgrading and blocking steps well before we have obtained a connection.</p>
<p dir="auto">We can know ahead of the mixed content checks whether a request has a local target address if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain, or</li>
<li>The <code>fetch()</code> call is annotated with the <code>{ targetAddressSpace: "local" }</code> option.</li>
</ul>
<p dir="auto">If the request meets any of those requirements, we skip steps 6 and 7 of <strong>main fetch</strong> (upgrading mixed content and blocking mixed content) and mark the request as a local network request. Then, after obtaining the connection the local network access checks can fully run, and if the origin is not granted the local network access permission the request will be blocked. Additionally, if the request ends up not resolving to a local network endpoint, we need to block the request (as it should have been blocked as mixed content).</p>
<p dir="auto">A new parameter <code>targetAddressSpace</code> will be added as a <code>fetch()</code> API option, allowing a developer to specify that the request should be treated as going to a <code>local</code> or <code>loopback</code> address space. This allows for HTTP local network requests that are not private IP literals or .local domains as long as they are explicitly tagged with the target address space. This also allows the developer to deterministically request the permission.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with HTML</h3><a id="user-content-integration-with-html" aria-label="Permalink: Integration with HTML" href="#integration-with-html"></a></p>
<p dir="auto"><code>Document</code>s and <code>WorkerGlobalScope</code>s store an additional <strong>address space</strong> value. This is initialized from the IP address the document or worker was sourced from.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with WebRTC</h3><a id="user-content-integration-with-webrtc" aria-label="Permalink: Integration with WebRTC" href="#integration-with-webrtc"></a></p>
<p dir="auto">Local connection attempts that use WebRTC should also be gated behind the Local Network Access permission prompt.</p>
<p dir="auto">In the WebRTC spec, algorithms that add candidates to the ICE Agent already have steps that ensure <a href="https://www.w3.org/TR/webrtc/#dfn-administratively-prohibited" rel="nofollow">“administratively prohibited”</a> addresses are not used. We can modify these algorithms to perform the following steps if the candidate has a loopback or local address:</p>
<ul dir="auto">
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the algorithm will continue.</li>
<li>If the user denies the permission, we won’t add the candidate to the ICE Agent and it won’t be used when establishing a connection.</li>
</ul>
<p dir="auto">Note that these checks are done asynchronously and don’t block resolving the methods where they are used i.e. setRemoteDescription() and addIceCandidate().</p>
<p dir="auto">The same checks should also be performed when connecting to STUN/TURN servers with loopback or local addresses.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions Policy</h3><a id="user-content-integration-with-permissions-policy" aria-label="Permalink: Integration with Permissions Policy" href="#integration-with-permissions-policy"></a></p>
<p dir="auto">By default the ability to make local network requests will be limited to top-level documents that are secure contexts. There are use cases where a site needs to be able to delegate this permission into a subframe. To support these use cases, a new policy-controlled feature ("local-network-access") will be added that will allow top-level documents to delegate access to this feature to subframes.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions API</h3><a id="user-content-integration-with-permissions-api" aria-label="Permalink: Integration with Permissions API" href="#integration-with-permissions-api"></a></p>
<p dir="auto">The permission will be integrated with the Permissions API, which will allow sites to query the status of the permission grant.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">HTML subresources</h3><a id="user-content-html-subresources" aria-label="Permalink: HTML subresources" href="#html-subresources"></a></p>
<p dir="auto">HTML subresource fetches go through the standard Fetch algorithm, but will not have the ability to specify an explicit <code>targetAddressSpace</code>. This includes subframe navigations.</p>
<p dir="auto">For HTTP subresources, only "local names" (i.e., private IP literals or .local hostnames) are allowed for local network requests. This is required for resolving the mixed content problem (see "Mixed content" above, and see "Considered alternatives" for more involved methods that have been discussed).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Websockets</h3><a id="user-content-websockets" aria-label="Permalink: Websockets" href="#websockets"></a></p>
<p dir="auto">The <strong>establish a WebSocket connection</strong> algorithm depends on the Fetch algorithm (<a href="https://websockets.spec.whatwg.org/#websocket-protocol" rel="nofollow">in the updated WHATWG spec</a>), so Websockets should behave like other Fetch requests and trigger local network access prompts without additional work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Service Workers</h3><a id="user-content-service-workers" aria-label="Permalink: Service Workers" href="#service-workers"></a></p>
<p dir="auto">Requests from a service worker go through the Fetch algorithm, and will be included in local network access restrictions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Considered alternatives</h2><a id="user-content-considered-alternatives" aria-label="Permalink: Considered alternatives" href="#considered-alternatives"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Private Network Access (PNA)</h3><a id="user-content-private-network-access-pna" aria-label="Permalink: Private Network Access (PNA)" href="#private-network-access-pna"></a></p>
<p dir="auto">The previously proposed <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access work</a> (PNA for short, also previously referred to as CORS-RFC1918) required a secure CORS preflight response from the private subresource server. If the preflight failed, the request would be blocked. If the request was for an insecure resource (e.g., due to the lack of trusted local HTTPS), they <a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md">proposed a separate permission prompt</a> to allow the connection to a specific endpoint device <em>and</em> relax the mixed content restrictions on the connection.</p>
<p dir="auto">A lot of effort and developer outreach went into this effort, but it was never able to ship. Chrome currently has an opt-in mode behind an enterprise policy, and for a while Chrome shipped a restriction where private network access was restricted to secure contexts only (with a deprecation trial for developers who could not yet meet this requirement due to having HTTP-only private network endpoints). The previous plan was to build and ship the "mixed content permission prompt" to get these remaining developers out of the reverse origin trial.</p>
<p dir="auto">PNA met a lot of different developer and user needs, and in the "good case" (secure website talking to a local network device that had a publicly trusted TLS certificate and a "PNA-aware" server) could be quite seamless, since it required no user intervention. In the non-ideal cases, PNA accumulated a lot of workarounds to address use cases that would result in a permission prompt in many cases (a device chooser style prompt for insecure devices). For example:</p>
<ul dir="auto">
<li>Some routers filter out DNS responses mapping public domain names to private IP addresses -- this means that <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to direct (insecure) private IP addresses is required</a> (and thus a permission prompt to bypass mixed content blocking), even for developers who provision publicly trusted certs to local network servers.</li>
<li>Some devices could not update to supply preflight, so the permission prompt was relaxed to allow this (<a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md#ephemeral-permission">but only grant an ephemeral permission</a>).</li>
</ul>
<p dir="auto">Even if the local device has opted in to connections from a top level site, we believe there is value in user awareness and control over this exchange.</p>
<p dir="auto">The use of preflights (without any user consent speed bump) also exposed its own risks. For example, timing attacks could be used to determine valid IP addresses on the network (<a href="https://crbug.com/40051437" rel="nofollow">crbug.com/40051437</a>, <a data-error-text="Failed to load title" data-id="822980731" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/41" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/41/hovercard" href="https://github.com/WICG/private-network-access/issues/41">WICG/private-network-access#41</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Block all local network requests</h3><a id="user-content-block-all-local-network-requests" aria-label="Permalink: Block all local network requests" href="#block-all-local-network-requests"></a></p>
<p dir="auto">Given the risks of allowing sites to use the browser as an access point into the user's local network, we could simply block all local network requests (or just any requests that cross from a public address space to a more private one). This would be simplest, but would break many existing use cases, or require expensive workarounds.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Do nothing</h3><a id="user-content-do-nothing" aria-label="Permalink: Do nothing" href="#do-nothing"></a></p>
<p dir="auto">As more operating systems are implementing local network access permissions at the application level, we believe it is the duty of user agents to broker access to that privilege in order to be good stewards of it. A user may have legitimate reasons to grant access for their <em>browser</em> (e.g., Chrome's "cast this tab" functionality) but never want a <em>site</em> to be able to access their local network.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternatives for the mixed content problem</h3><a id="user-content-alternatives-for-the-mixed-content-problem" aria-label="Permalink: Alternatives for the mixed content problem" href="#alternatives-for-the-mixed-content-problem"></a></p>
<p dir="auto">In our proposal above we recommend restricting local network HTML subresources to "assumed local" hostnames (such as <code>.local</code> domains) as a middle ground that meets developer needs, is relatively easy to deploy, and doesn't require complex technical or specification work to accomplish.</p>
<p dir="auto">Below are some alternatives that have been considered for addressing the mixed content problem.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Require secure local network subresources</h4><a id="user-content-require-secure-local-network-subresources" aria-label="Permalink: Require secure local network subresources" href="#require-secure-local-network-subresources"></a></p>
<p dir="auto">In order to restrict local network access to secure contexts (which is necessary in order for a permission prompt to make sense), we need some resolution of the mixed content problem. The Fetch specification orders the "upgrade or block mixed content" steps <em>before</em> we have obtained a connection, and thus before we can know the IP address.</p>
<p dir="auto">Currently, some developers can work around this by getting publicly trusted certificates for their servers running on local networks (e.g., Plex getting Let's Encrypt certs under a different subdomain for every install) but it is a substantial engineering and maintenance burden. Even for developers that go to the trouble of using publicly trusted certificates, <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to HTTP is required in some network circumstances</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">List subresource address space details in a header or meta tag</h4><a id="user-content-list-subresource-address-space-details-in-a-header-or-meta-tag" aria-label="Permalink: List subresource address space details in a header or meta tag" href="#list-subresource-address-space-details-in-a-header-or-meta-tag"></a></p>
<p dir="auto">This could be a "treat hostname as public" / "treat hostname as local" property that could be specified in a response header or in a meta tag (or a response header meta equiv).</p>
<p dir="auto">An initial idea was to add this on to <a href="https://www.w3.org/TR/CSP3/" rel="nofollow">CSP</a>, however that was rejected due to CSP already being a bit overloaded and this not being a great conceptual fit for it. A separate "Sec-Treat-Origin-As-Private" header (or something along those lines) could be used to list origins that should be assumed to resolve to private IP addresses.</p>
<p dir="auto">Additionally, it might be useful if such a header could be specified via an http-equiv meta tag (like one can do with CSP).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Potential future changes</h3><a id="user-content-potential-future-changes" aria-label="Permalink: Potential future changes" href="#potential-future-changes"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Top-level navigations to local network</h4><a id="user-content-top-level-navigations-to-local-network" aria-label="Permalink: Top-level navigations to local network" href="#top-level-navigations-to-local-network"></a></p>
<p dir="auto">Top-level navigations remain a risk after restrictions on subresource local network requests are in place. For an attacker, main frame navigations are noisier (compared to subresource requests and iframe navigations), although popunder techniques could potentially be used to hide navigations from the user.</p>
<p dir="auto">To prevent these, we could block or show an interstitial warning when a public page navigates to a local one. To avoid too much breakage or over-warning, we could maybe scope protections to just these cases. We might consider that "complex" requests such as POST navigations and GET requests with URL parameters are particularly risky. We might also be able to "defang" navigations by stripping the URL parameters to reduce the risk of exploitation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Consider all cross-origin requests to private addresses as "local network requests"</h4><a id="user-content-consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests" aria-label="Permalink: Consider all cross-origin requests to private addresses as &quot;local network requests&quot;" href="#consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests"></a></p>
<p dir="auto">We could instead define a <strong>local network request</strong> as "any request targeting an IP address in the local or loopback address space, regardless of the requestor's address space", while maintaining the exception for not blocking same-origin requests. This is a stricter / broader definition, and would likely cause more widespread breakage than our proposal to only consider requests that cross from one address space class to a more private one.</p>
<p dir="auto">This would also allow the mixed content relaxation that is granted when an origin is given the local network access permission to apply to <code>local</code> -&gt; <code>local</code> or <code>loopback</code> -&gt; <code>local</code> requests. This has been raised as a concern by developers in <a data-error-text="Failed to load title" data-id="1795130611" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/109" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/109/hovercard" href="https://github.com/WICG/private-network-access/issues/109">WICG/private-network-access#109</a>. Note that the status quo could continue here for now -- i.e., the top level page can get around mixed content checks by remaining on HTTP.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">More granular permission grants</h4><a id="user-content-more-granular-permission-grants" aria-label="Permalink: More granular permission grants" href="#more-granular-permission-grants"></a></p>
<p dir="auto">A <a href="https://martina.lindorfer.in/files/papers/nwscanning_oakland25.pdf" rel="nofollow">recent study</a> (Schmidt et al., S&amp;P 2025) found that the <em>transitivity</em> of the local networks permission on iOS was the hardest for users to understand. It might be beneficial to scope the permission grant to an origin to the specific local network the user is currently connected to. Should the user grant permission to example.com on their home network, it may be reasonable for a UA to re-prompt the user if they bring their device to a different location and visit example.com again.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add address space properties to HTML</h4><a id="user-content-add-address-space-properties-to-html" aria-label="Permalink: Add address space properties to HTML" href="#add-address-space-properties-to-html"></a></p>
<p dir="auto">PNA 1.0 worked to add a new <code>targetAddressSpace</code> parameter to <code>fetch()</code> to label the target address space of the request, so that mixed content checks could be relaxed (and then enforced if that connection ended up being public, to avoid it being a mixed content blocking bypass). The challenge is how to handle HTML subresources (e.g., img, iframe, etc.). We could add a new property to these HTML elements allowing developers to "label" them as public/local/loopback. This would function similarly to the parameter on <code>fetch()</code> and allow the user agent to initially bypass mixed content checks (and to know it should trigger the permission prompt). Currently we don't think this is necessary, as most use cases that require explicitly marking the <code>targetAddressSpace</code> should be able to switch to using <code>fetch()</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security &amp; Privacy Considerations</h2><a id="user-content-security--privacy-considerations" aria-label="Permalink: Security &amp; Privacy Considerations" href="#security--privacy-considerations"></a></p>
<p dir="auto"><strong>Security</strong></p>
<ul dir="auto">
<li>While the local network access permission exempts requests to a priori known local endpoints from mixed content blocking, the page should still be considered to have loaded mixed content if such a request is made. This means that, for example, browsers can choose to show a different security UI for pages that make insecure connections to the local network.</li>
<li>Compared to the original PNA proposal, a site granted the local network access permission has more power to probe and connect to devices on the local network, regardless of whether those devices expect it.</li>
<li>There is some risk of users accepting the permission without understanding it (which couldn't happen with preflights).</li>
</ul>
<p dir="auto"><strong>Privacy</strong></p>
<ul dir="auto">
<li>Compared to the original PNA proposal, no local network connections are allowed until the user has explicitly granted permission to a site.</li>
<li>Compared to the original PNA proposal, there are no preflights (and thus no risk of timing/probing attacks from them).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stakeholder Feedback / Opposition</h2><a id="user-content-stakeholder-feedback--opposition" aria-label="Permalink: Stakeholder Feedback / Opposition" href="#stakeholder-feedback--opposition"></a></p>
<p dir="auto">The previous PNA proposal (using preflights) was positively received by Mozilla (<a data-error-text="Failed to load title" data-id="419961868" data-permission-text="Title is private" data-url="https://github.com/mozilla/standards-positions/issues/143" data-hovercard-type="issue" data-hovercard-url="/mozilla/standards-positions/issues/143/hovercard" href="https://github.com/mozilla/standards-positions/issues/143">mozilla/standards-positions#143</a>) and WebKit (<a data-error-text="Failed to load title" data-id="1656934582" data-permission-text="Title is private" data-url="https://github.com/WebKit/standards-positions/issues/163" data-hovercard-type="issue" data-hovercard-url="/WebKit/standards-positions/issues/163/hovercard" href="https://github.com/WebKit/standards-positions/issues/163">WebKit/standards-positions#163</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References &amp; acknowledgements</h2><a id="user-content-references--acknowledgements" aria-label="Permalink: References &amp; acknowledgements" href="#references--acknowledgements"></a></p>
<p dir="auto">Many thanks for valuable feedback and advice from:</p>
<ul dir="auto">
<li>Titouan Rigoudy, Jonathan Hao, and Yifan Luo who worked on the original PNA proposals and specification, and generously discussed their work with me and helped brainstorm paths forward.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The iPhone 15 Pro's Depth Maps (308 pts)]]></title>
            <link>https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</link>
            <guid>44183591</guid>
            <pubDate>Wed, 04 Jun 2025 17:57:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html">https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</a>, See on <a href="https://news.ycombinator.com/item?id=44183591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Since 2017, Apple have supported depth maps in the images its iPhones capture either via LiDAR scanners, 3D <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">time-of-flight</a> scanner-less LIDAR or <a href="https://en.wikipedia.org/wiki/Structured-light_3D_scanner">structured-light</a> 3D scanning.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
</a></p><p>These depth maps, along with other imagery, are stored in High Efficiency Image File Format (HEIF) container files. These files can contain multiple images and vast amounts of metadata. This format was originally designed between 2013 and 2015 and Apple adopted its HEIC variant in 2017.</p>
<p>Since then, HEIC files are the default storage container format for images captured on the iPhone. But with that said, it is possible to use the JPEG format instead if things like depth maps and HDR are not of interest.</p>
<p>Finn Jaeger, who is the head of VFX at Replayboys, a film production firm in Hamburg, Germany, <a href="https://www.linkedin.com/posts/finn-j%C3%A4ger-b44058176_did-you-know-your-iphone-heic-files-are-activity-7319460880750940162-Rn23/">posted</a> a screenshot a few weeks ago showing how multiple depth maps were being produced by his iPhone.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
</a></p><p>He announced he was working on a project called <a href="https://github.com/finnschi/heic-shenanigans">HEIC Shenanigans</a>. This project contains scripts to separate out images and their metadata from HEIC containers as well as convert them into EXR Files. As of this writing, the project contains 374 lines of Python.</p>
<p>In this post, I'll walk through Finn's codebase with an example image from an iPhone 15 Pro.</p>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.</p>
<p>I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll use Python 3.12.3 and a few other tools in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>add-apt-repository<span> </span>ppa:deadsnakes/ppa
$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>openexr<span> </span><span>\</span>
<span>    </span>libimage-exiftool-perl<span> </span><span>\</span>
<span>    </span>libopenexr-dev<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3.12-venv
</pre></div>
<p>Note, the <tt><span>libimage-exiftool-perl</span></tt> package installs <tt>exiftool</tt> version 12.76+dfsg-1 which was released at the end of January 2024. Since then, there have been at least ten releases that have addressed issues or enhanced exiftool's HEIC support.</p>
<p>The above version should work fine for the steps in this post but be mindful that any issues you encounter going forward might be resolved with a more up to date version of exiftool.</p>
<p>I'll be using <a href="https://github.com/kellyjonbrazil/jc?tab=readme-ov-file">JSON Convert</a> (jc) to convert the output of various CLI tools into JSON.</p>
<div><pre><span></span>$<span> </span>wget<span> </span>https://github.com/kellyjonbrazil/jc/releases/download/v1.25.2/jc_1.25.2-1_amd64.deb
$<span> </span>sudo<span> </span>dpkg<span> </span>-i<span> </span>jc_1.25.2-1_amd64.deb
</pre></div>
<p>I'll clone Finn Jaeger's HEIC Shenanigans repo.</p>
<div><pre><span></span>$<span> </span>git<span> </span>clone<span> </span>https://github.com/finnschi/heic-shenanigans<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans
</pre></div>
<p>I'll set up a Python Virtual Environment and install a few dependencies.</p>
<div><pre><span></span>$<span> </span>python3<span> </span>-m<span> </span>venv<span> </span>~/.iphone_depth
$<span> </span><span>source</span><span> </span>~/.iphone_depth/bin/activate

$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span>-r<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans/requirements.txt
$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>OpenImageIO
</pre></div>
<p>I'll use <a href="https://github.com/darbyjohnston/DJV/releases">DJV</a> v2.0.8 to view EXR imagery produced in this post.</p>
</div>
<div id="an-iphone-15-pro-image">
<h2>An iPhone 15 Pro Image</h2>
<p><a href="https://www.linkedin.com/in/joel-joseph-b865981ab/">Joel Joseph</a>, a subject matter expert on the ArcGIS Desktop Products Suite in Mumbai, India, was kind enough to send me an HEIC-contained image he took on his iPhone 15 Pro. I'll use this image in this post. Below is a screenshot.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
</a>
</p></div>

<div id="converting-an-heic-into-exr">
<h2>Converting an HEIC into EXR</h2>
<p>The <a href="https://github.com/AcademySoftwareFoundation">Academy Software Foundation</a> fosters various <a href="https://www.aswf.io/projects/">open source projects</a> and standards used in film, television and other creative industries. Its <a href="https://www.aswf.io/members/">members</a> include the Academy of Motion Picture Arts and Sciences, Disney, Nvidia and Netflix to name a few.</p>
<p>Below is a screenshot of their <a href="https://landscape.aswf.io/">landscape</a> of projects.</p>
<p><a href="https://landscape.aswf.io/">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_g5ZWZ7GWLE.png">
</a></p><p>One of their projects is <a href="https://github.com/AcademySoftwareFoundation/openexr">OpenEXR</a>, a high-dynamic range (HDR) image file format. It was originally developed by Industrial Light and Magic (ILM) in 1999 and was later made open source in 2003. It's used in producing visual effects and 3D renderings.</p>
<p>Below I'll convert iPhone 15 Pro image into an OpenEXR file.</p>
<div><pre><span></span>$<span> </span>python<span> </span>~/heic-shenanigans/heic_to_exr.py<span> </span><span>\</span>
<span>    </span>IMG_E2153.HEIC
</pre></div>
<p>The resulting file is 468 MB. Below is a screenshot of it in DJV.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
</a></p><p>The above file was produced by making several calls to <tt>oiiotool</tt>, an image processing tool from the <a href="https://github.com/AcademySoftwareFoundation/OpenImageIO">OpenImageIO</a> project, which is also apart of the Academy Software Foundation.</p>
<p>Below I've annotated the calls to oiiotool made by the above script.</p>
<p>The script will first get the dimensions of the source image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>--info<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>It will then produce a base image, converting the source image from an sRGB curve through Linear P3 and onto ACEScg.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_base.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'sRGB - Texture'</span><span> </span><span>'Linear Rec.709 (sRGB)'</span><span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'Linear P3-D65'</span><span>  </span><span>'ACES - ACEScg'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/base.exr
</pre></div>
<p>The above OCIO file is an <a href="https://opencolorio.org/">OpenColorIO</a> colour profile file. OpenColorIO, also under the Academy Software Foundation, is a colour management suite. The project provides several <a href="https://github.com/AcademySoftwareFoundation/OpenColorIO-Config-ACES/releases">reference configurations</a> including one from the <a href="https://en.wikipedia.org/wiki/Academy_Color_Encoding_System">Academy Color Encoding System (ACES)</a>. The OCIO file is text-based and contains 1,242 lines of content.</p>
<div><pre><span></span>$<span> </span>head<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio
</pre></div>
<div><pre><span></span>ocio_profile_version: 2.1

environment:
  {}
search_path: ""
strictparsing: true
luma: [0.2126, 0.7152, 0.0722]
name: studio-config-v1.0.0_aces-v1.3_ocio-v2.1
description: |
  Academy Color Encoding System - Studio Config [COLORSPACES v1.0.0] [ACES v1.3] [OCIO v2.1]
</pre></div>
<p>Then, an EXR-based gain map will be produced by converting the source HDR gain map from Rec709 curve to Linear.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_hdrgainmap_50.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--ocionamedtransform<span> </span><span>'Rec.709 - Curve'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap.exr
</pre></div>
<p>That gain map will then be converted into RGB by duplicating the Y channel three times.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>gainmap.Y,gainmap.Y,gainmap.Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr
</pre></div>
<p>Then, the HDR gain map's headroom value will be extracted.</p>
<div><pre><span></span>$<span> </span>exiftool<span> </span>-HDRGainMapHeadroom<span> </span>-b<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>The gain map will then be scaled using the inverse of the HDR headroom value captured above.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>    </span>--mulc<span> </span>-0.12135654640000004<span> </span><span>\</span>
<span>    </span>--addc<span> </span><span>1</span>.0<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr
</pre></div>
<p>Then, the base image will be multiplied by the scaled gain map to create an HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr<span> </span><span>\</span>
<span>           </span>--mul<span> </span><span>\</span>
<span>           </span>--chnames<span> </span>R,G,B<span> </span><span>\</span>
</pre></div>
<p>Then, the Y channel from the depth map will be used to create an EXR-formatted depth map.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_depth_0.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>depth.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/depth.exr
</pre></div>
<p>If the source image contained mattes they would be processed at this stage.</p>
<p>The first step in constructing the final EXR file starts by copying in the RGB channels from the EXR-formatted HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/hdr_base.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the SDR channels are added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the gain map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the depth map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/depth.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>depth.Y<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>If there were any matte layers, they would be added here.</p>
<p>The <tt>final.exr</tt> file is then moved to a <tt>&lt;prefix&gt;_acesCG.exr</tt> file alongside the source imagery.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Code (193 pts)]]></title>
            <link>https://mistral.ai/products/mistral-code</link>
            <guid>44183515</guid>
            <pubDate>Wed, 04 Jun 2025 17:50:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/products/mistral-code">https://mistral.ai/products/mistral-code</a>, See on <a href="https://news.ycombinator.com/item?id=44183515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><nav aria-label="Main" data-orientation="horizontal" dir="ltr"><div><a rel="home" aria-label="Home" href="https://mistral.ai/"></a><div><ul data-orientation="horizontal" dir="ltr"><li></li><li></li><li></li><li></li><li><a target="_self" href="https://mistral.ai/pricing">Pricing</a></li><li></li></ul></div></div></nav></div><div><div><p>Lightning-fast completions, deep code understanding, and agentic software engineering—right where you work.</p></div><div><p><img alt="744a3c97-aba4-4cbd-9697-a4a4ead168c1" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75"></p></div></div><div id=""><div><p>Code with intelligence, control with confidence.</p><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><div><h3><p>Code with intelligence, control with confidence.</p></h3><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-«Rm6fdbnb»-trigger-Intelligent coding assistance" id="radix-«Rm6fdbnb»-content-Intelligent coding assistance" tabindex="0" dir="ltr"><div><p>Elevate your coding with AI that understands, completes, and optimizes your code with frontier, purpose-built software engineering models.</p></div><div><p><img alt="ac37ae27-1d08-49aa-b191-26758baca735" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75"></p></div></div></div></div><div id=""><div><h3><p>Why Mistral Code?</p></h3></div><div><div><div><p><img alt="icon-key-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75"></p></div><div><p><span><p>State-of-the art AI-powered software development, in your control.</p></span></p><p>10X developer productivity and efficiency without sacrificing the privacy and safety of your codebase. Mistral Code can be deployed where your most important code resides.</p></div></div><div><div><p><img alt="icon-pencil-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Frontier coding models, fully customizable and tunable to your codebase.</p></span></p><p>Advanced models such as Codestral and Devstral under the hood, further customizable with fine-tuning, post-training, and distillation.</p></div></div><div><p><span><p>One platform across the full AI-powered software stack.</p></span></p><p>End‑to‑end tooling from models to IDE in one first-party platform. Unified SLA, faster support, and simpler compliance posture.</p></div><div><div><p><img alt="icon-lightning-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Deep code understanding, generation, and completion.</p></span></p><p>Advanced understanding of files and integrations with agent scaffolds for benchmark-setting performance on agentic tasks, along with lightning-fast code generation and completion.</p></div></div></div></div><div id=""><h4>
<center>Frontier intelligence, in your favorite IDE.</center>
</h4><div>
<center>Our unique mix of purpose-built models delivers an unmatched combination of lightning-fast completions and deep code understanding.</center>
</div></div><div id=""><p><img alt="c7ab7262-be13-45c1-8498-e37551db9527" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75"></p></div><div id=""><p>Speed development time.</p><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div></div><section><div><h3><p>How teams use Mistral Code today.</p></h3></div><div><div><div><p>Code completion</p><div><p><img alt="check-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75"></p><p>Suggests completions for code as you type, helping to speed up the coding process and reduce errors.</p></div></div><div><p>Code debugging and refactoring</p><div><p><img alt="glasses-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75"></p><p>Identifies syntax and logical errors, typos in real-time and provides suggestions for improving code structure.</p></div></div><div><p>Code documentation</p><div><p><img alt="icon-copy-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75"></p><p>Automatically generates documentation for code, including comments and API documentation, to improve code maintainability.</p></div></div></div><div><div><p>Code testing</p><div><p><img alt="icon-computer-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75"></p><p>Generates unit and system tests to ensure that the code generated is fully functional.</p></div></div><div><p>Code migration</p><div><p><img alt="icon-earth-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75"></p><p>Generate code snippets in the target language, helping to translate and adapt existing codebases to new languages or frameworks.</p></div></div><div><p>Code performance</p><div><p><img alt="icon-lightbulb" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75"></p><p>Analyzes code performance and identifies bottlenecks, helping developers optimize their code for better speed and efficiency.</p></div></div></div></div></section><div id=""><p>Powering the world's pioneers.</p></div><div id=""><h2><p>Your AI, from models to outputs.</p></h2><p>Experience the power of Mistral Code directly in your favorite IDE.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When memory was measured in kilobytes: The art of efficient vision (121 pts)]]></title>
            <link>https://www.softwareheritage.org/2025/06/04/history_computer_vision/</link>
            <guid>44182698</guid>
            <pubDate>Wed, 04 Jun 2025 16:46:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.softwareheritage.org/2025/06/04/history_computer_vision/">https://www.softwareheritage.org/2025/06/04/history_computer_vision/</a>, See on <a href="https://news.ycombinator.com/item?id=44182698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<p><em>By Mathilde Fichen</em></p>



<p>In the early days of computer vision, when memory was scarce and every byte counted, innovation thrived under constraint. “An Efficient Chain-Linking Algorithm,” developed at Inria in the late 1980s, is a brilliant example of this spirit. Now preserved and shared by Software Heritage, this compact yet powerful piece of C code showcases how elegance and efficiency went hand in hand in outlining the future of image processing—one pixel chain at a time.</p>



<p>The code resulted from research work carried out between 1985 and 1991 at Inria, by Gérard Giraudon (research and principal investigator), Philippe Garnesson (a PhD student), and Patrick Cipière (software engineer). Down in sunny Sophia Antipolis, a <a href="https://en.wikipedia.org/wiki/Sophia_Antipolis">tech park</a> 20 minutes inland from Antibes, the team tackled computer vision with a distinctly local flavor. They called themselves PASTIS, a playful nod to the anise drink. Still, the acronym – Scene Analysis and Symbolic Image Processing Project (Projet d’Analyse de Scène et de Traitement d’Image Symbolique) – hinted at their serious mission.</p>





<h2>Preserving Inria legacy software</h2>



<p>The effort to preserve this source code is part of a broader initiative to preserve the legacy codes of <a href="https://www.inria.fr/en">Inria</a> (France’s National Institute for Research in Digital Science and Technology), launched in 2023 in a joint effort of Software Heritage, Inria Alumni, and Inria. The project to preserve Inria’s legacy software started by reaching out to the institute’s community, past and present, through a survey (as detailed in our <a href="https://ipres2024.pubpub.org/pub/hdap1420/release/1?readingCollection=21e62c05">iPres article</a>.)  This initial outreach informed a dedicated, <a href="https://www.softwareheritage.org/2024/12/10/preserving-inrias-legacy-software/">hands-on workshop in 2024</a>, which kicked off the practical work of exploring these historical codes. The current focus is on securely archiving the important legacy software we’ve identified within Software Heritage. Sharing the stories behind these codes with the broader community is just as vital.</p>



<p>The recovery of some code has been surprisingly straightforward. For instance, the code for “An Efficient Chain-Linking Algorithm” was readily accessible thanks to Gérard Giraudon’s personal preservation efforts on a local drive. That small success story is a reminder of how important individual initiative is for preserving digital work. Each piece of software recovered isn’t just code; it’s a piece of research history, carrying the stories of the people who created it.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-scaled.jpg"><img decoding="async" width="1024" height="576" src="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1024x576.jpg" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1024x576.jpg 1024w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-300x169.jpg 300w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-768x432.jpg 768w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1536x864.jpg 1536w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><em>Inria alumni workshop, October 2024, Inria Paris.</em></figcaption></figure></div>


<h2>An algorithm for outlining images&nbsp;</h2>



<p>The chain-linking algorithm processes a 2D pixel matrix, typically the output of a contour detection step.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png"><img loading="lazy" decoding="async" width="325" height="307" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png 325w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image5-300x283.png 300w" sizes="auto, (max-width: 325px) 100vw, 325px"></a></figure></div>


<p><em>Raw image used as input for the algorithm. Source: Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon</em></p>



<p>The output is a list of contour chains. Each chain in this list is a sequence of pixel coordinates that define a continuous boundary, ready for further steps like polygonal approximation or shape analysis..</p>



<p>Processed image. Source : Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon</p>



<p>Basically, the chain-linking connects the edge pixels to create smooth outlines, just like tracing the shape with a pencil.</p>



<h2>Computer vision in the 80s</h2>



<p>The mid-1980s aspiration of seeing robots hit a speed bump: computer vision algorithms weren’t fast enough. The core challenge? Real-time performance – the essential ingredient for giving robots camera “eyes” that could truly see and react. At the time, such a system was conceived as a pipeline of operations from image acquisition (one shot or video) to a decision-making system.&nbsp;</p>



<p>The goal was to extract meaningful information from raw image data, typically represented as an 8-bit matrix of pixel intensities, and convert it into a graph in list form (i.e., from matrix to list). This process begins with detecting and recognizing shapes within the image, either in 2D or 3D, depending on the available data. From there, the system identifies objects and analyzes their spatial relationships, ultimately constructing a graph of semantic connections between them. This graph captures not just what the objects are, but how they relate to one another within the observed scene. In some cases, the analysis extends over time (3D + t), allowing for the interpretation of motion and dynamic interactions in a sequence of frames.</p>



<h2>Solving key memory issues</h2>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png"><img loading="lazy" decoding="async" width="786" height="412" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png 786w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image3-300x157.png 300w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image3-768x403.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></a></figure></div>


<p><em>The algorithm was first developed on a PerkinElmer Model 3250 computer, seen above in a brochure <a href="https://www.1000bit.it/ad/bro/perkin/PerkinElmer-3250.pdf">via 1000bit.&nbsp;</a></em></p>



<p>Another challenge in early image processing was the limited amount of short-term memory (RAM) available in computers. This made it essential to focus on reducing the amount of data stored and processed at any given time, while preserving important information.</p>



<p>Due to these constraints, the Efficient Chain-Linking Algorithm could only store three lines of the image at a time while reading it line by line. As each new line was read, the algorithm would build and extend chains of connected pixels on the fly, without knowing how those chains might continue in future lines. Once the entire image was scanned, a final processing stage merged pixel chains belonging to the same contour and resolved junctions or branching points. Importantly, this was done using just one full pass over the data, making it memory-efficient.</p>



<p>But the algorithm wasn’t the only clever bit. From a programming standpoint, the code’s true ingenuity lay in its dynamic memory allocation for storing the chain lists. Back then, predicting memory needs upfront was impossible, making Patrick Cipiere’s approach an elegant solution to an unpredictable challenge.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png"><img loading="lazy" decoding="async" width="425" height="339" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png 425w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image1-300x239.png 300w" sizes="auto, (max-width: 425px) 100vw, 425px"></a><figcaption><em>Source code for memory allocation</em> <em>(excerpt)</em></figcaption></figure></div>


<h2>Computer vision today</h2>



<p>With the dramatic advancements in computer vision, fueled by deep learning and the prevalence of large memory capacities, contemporary methods often involve storing the full image and constructing contour chains sequentially, possibly necessitating multiple passes over the data, one for each chain. Yet, even with today’s abundant memory, this algorithm retains its power: remarkable efficiency when every byte counts. By processing the image sequentially, storing only a few lines at a time, and building pixel chains incrementally without looking back, it offers a lean and effective alternative to the memory-hungry approaches now common.</p>



<h2>Links and references</h2>



<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png"><img loading="lazy" decoding="async" width="1024" height="522" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png 1024w, https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision-300x153.png 300w, https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision-768x392.png 768w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>The preserved source code can be found on Software Heritage archive: <a href="https://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://github.com/mathfichen/chainage_de_contour">https://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://github.com/mathfichen/chainage_de_contour</a></p>



<p>The original 79-page paper in French, “Chaînage efficace de contour, n° 605, Février 1987, Gerard Giraudon” <a href="https://inria.hal.science/inria-00075949/document">https://inria.hal.science/inria-00075949/document</a></p>



<p>An Efficient Chain-Linking Algorithm, G. Giraudon, in The 5th Scandinavian Conference on Image Analysis, Stockholm, June 1987.</p>



<p>A Real Time Parallel Edge Following in Single Pass, GG, in Workshop on Computer Vision, Miami, 1987</p>



<p>Chaînage efficace de contour, G. Giraudon, 3ème Colloque Image-Cesta, Paris, Mai 1987</p>



<p>Un standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.</p>



<p>Un standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.<br><a href="https://inria.hal.science/inria-00070061v1">https://inria.hal.science/inria-00070061v1</a></p>



<p>« L’approximation polygonale, bilans et perspectives », Rapport de recherche n°1621 INRIA, juin 1991&nbsp; Garnesson P. and Giraudon G.&nbsp;<br><a href="https://inria.hal.science/inria-00074940v1/document">https://inria.hal.science/inria-00074940v1/document</a></p>



<p>This work has resulted in citations and a hardware implementation presented at the 16th GRETSI COLLOQUIUM — SEPTEMBER 15-19, 1997</p>




	
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Are No Longer a Serious Country – Paul Krugman (153 pts)]]></title>
            <link>https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</link>
            <guid>44182634</guid>
            <pubDate>Wed, 04 Jun 2025 16:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country">https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</a>, See on <a href="https://news.ycombinator.com/item?id=44182634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“If you’re explaining, you’re losing.” This line is usually attributed to Ronald Reagan. Whoever said it definitely had a point, and not just about politics. If you’re trying to explain to people, be they voters or bond investors, that you aren’t really as bad or untrustworthy as you seem, you’re already in deep trouble.</p><p><span>So when I saw Scott Bessent, the treasury secretary, </span><a href="https://www.ft.com/content/9c652f09-cfca-4078-9e2b-5fdacd772a75" rel="">declaring</a><span> Sunday that “The United States of America is never going to default, that is never going to happen,” my reaction was, “Uh-oh.”</span></p><p>And it’s not just me. For generations investors have treated U.S. government debt as the ultimate safe asset. Whenever disaster strikes — even if it’s disaster largely made in America, like the 2008 subprime crisis — bond buyers pile into U.S. Treasuries, because America is a serious country, and the idea that we would fail to honor our debts was unthinkable.</p><p>But are we still that country? Markets seem to have doubts.</p><p><span>Yesterday the Financial Times had a neat chart showing that there used to be a clear relationship between U.S. interest rates and the international value of the dollar. Actually, the chart was a bit </span><em>too</em><span> neat: When I set out to reproduce it, I found that the FT chose a time period during which the relationship looked especially clear. Still, it used to be true that when U.S. interest rates rose, so did the dollar, because higher yields pulled in foreign capital. But since Donald Trump returned to power, that relationship has broken down. Instead, we’ve seen a combination of rising interest rates and a </span><em>falling</em><span> dollar:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75158,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>As many have noted, what we’ve been seeing in recent months, with interest rates and the dollar moving in opposite directions, doesn’t look like what we normally see in the United States, or for that matter advanced nations in general. Instead, it’s the kind of thing one sees in emerging markets, where big market moves often reflect crises of confidence: International investors lose faith, pulling their money out, and capital flight causes both a falling currency and rising interest rates.</p><p>Here, for example, is what Mexico looked like during the “tequila crisis” of 1994-5, which involved both soaring interest rates and a plunging peso:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Why are markets beginning to treat America as unreliable? It’s not just the debt numbers. Yes, we have large debts, but we’re an immensely wealthy country that, among other things, has lower average taxes than most of our peers. So we certainly have the resources to honor our debts.</p><p><span>But do we have the political will? Maybe even more important, do we have the political </span><em>seriousness</em><span>?</span></p><p>Like many economists, I’ve spent a lot of time analyzing the substance of Trump’s tariffs — how much they are likely to raise prices and reduce trade volumes. I’ve also written about the impacts of policy uncertainty, about how hard it is for businesses to make plans when they have little idea what tariff rates will be even a few months from now, let alone over the next few years.</p><p><span>But I wonder whether we’ve spent enough time looking at the policy </span><em>process</em><span> — how decisions get made in Trump’s America. Consider: On April 2, “Liberation Day,” Trump announced extremely high tariffs on many countries, the biggest tariff hike in U.S. history. The tariff rates — which differed hugely from country to country — were determined by a formula universally panned as stupid and ridiculous. And this tariff announcement was made with so little planning and forethought that it included taxes on imports from remote islands inhabited only by </span><a href="https://www.bbc.com/news/articles/cly8xlj0485o" rel="">penguins</a><span>.</span></p><p><span>Then, a week later, these tariffs were replaced by a completely different set of tariffs. How did that happen? Two of Trump’s cabinet members were able to beard him in the Oval Office while Peter Navarro, responsible for the original tariffs, was </span><a href="https://www.wsj.com/politics/policy/trump-tariff-pause-navarro-bessent-lutnick-b9e864fb?gaa_at=eafs&amp;gaa_n=ASWzDAiBOSVk0wHQwGquJzeffoF7WWhmHKUg490AKNKAI7beYwEiwJ8au1jW-gBGH4A%3D&amp;gaa_ts=683dcdff&amp;gaa_sig=xIdEPFjIsG6tDwLXo4RJ5eBSP5faPCp3McPx6CkxYZ4dSRr3EJUr_ysQIYStqcwE5VL_0ZhadzjRqX58YVM2eA%3D%3D" rel="">in another meeting</a><span>.</span></p><p>Does this sound like policymaking in a serious country?</p><p>Then there’s the budget bill making its way through Congress. It’s a terrible thing, imposing savage cuts on social programs (and decimating U.S. science) while giving such big tax cuts to the wealthy that it will explode the deficit. But content aside, notice that this hugely important piece of legislation is being rushed through with essentially no hearings or analysis.</p><p>And when outsiders, including the Congressional Budget Office and a variety of think tanks — conservative and centrist as well as progressive — have put out the analyses the bill’s sponsors won’t, pointing out the likely effects on debt, health coverage, and so on, the G.O.P. response has basically been to accuse all of the independent analysts of being part of a globalist conspiracy.</p><p><span>Wait, it gets worse. The name of the legislation — not its nickname, its official title — is the </span><a href="https://www.congress.gov/bill/119th-congress/house-bill/1/text" rel="">One Big Beautiful Bill Act</a><span>, because that’s what Trump has been calling it. Are we a mature republic with a normal head of state, or are we being ruled by Kim Jong Un in orange makeup?</span></p><p><span>Why, next thing you’ll be telling me that key policy decisions, leading to layoffs of hundreds of thousands of federal workers and many deaths around the world, have been made by a presidential crony whose erratic behavior may have reflected massive consumption of ketamine, Ecstasy and psychedelic mushrooms. </span><a href="https://www.nytimes.com/2025/05/30/us/elon-musk-drugs-children-trump.html" rel="">Oh, wait</a><span>.</span></p><p><span>Imagine yourself as a foreigner considering investing in the United States. You may well know that the One Big Beautiful Bill Act contains a “</span><a href="https://www.bloomberg.com/news/articles/2025-05-30/trump-revenge-tax-would-lower-foreign-investment-in-us-scorekeeper-predicts?sref=qzusa8bC" rel="">revenge</a><span>” provision that would allow the U.S. government to impose extra taxes on foreign investors whose home countries have policies America doesn’t like. You probably know that one of Trump’s advisers has suggested the forced conversion of short-term debt into </span><a href="https://thedailyeconomy.org/article/turning-treasury-securities-into-century-bonds-is-a-dead-end/" rel="">century bonds</a><span>. Once upon a time everyone would have dismissed these things as stuff that couldn’t happen in America. Now? Who knows?</span></p><p>In a way, the amazing thing is that we haven’t seen even more capital flight. Presumably investors still can’t believe that America has changed so much from the responsible, reliable nation it seemed to be just a few months ago.</p><p>But I think they’re headed for a rude awakening.</p><p>MUSICAL CODA</p><div id="youtube2-pvlbrKKEPdA" data-attrs="{&quot;videoId&quot;:&quot;pvlbrKKEPdA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/pvlbrKKEPdA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VC money is fueling a global boom in worker surveillance tech (225 pts)]]></title>
            <link>https://restofworld.org/2025/employee-surveillance-software-vc-funding/</link>
            <guid>44182582</guid>
            <pubDate>Wed, 04 Jun 2025 16:35:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/employee-surveillance-software-vc-funding/">https://restofworld.org/2025/employee-surveillance-software-vc-funding/</a>, See on <a href="https://news.ycombinator.com/item?id=44182582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Technologies that promise to track, manage, and supervise workers, increasingly using artificial intelligence, are getting entrenched in the developing world, according to a new <a href="https://home.coworker.org/little-tech-goes-global/">report</a> by Coworker.org, a labor rights nonprofit based in New York.&nbsp;</p>



<p>Audits of more than 150 startups and regional companies based in Kenya, Nigeria, Colombia, Brazil, Mexico, and India showed workplace surveillance is expanding in scale and sophistication, the researchers said. While large <a href="https://www.computerweekly.com/news/252521757/Microsoft-Office-365-has-ability-to-spy-on-workers">corporations</a> are <a href="https://www.cisco.com/site/us/en/learn/topics/general/what-is-employee-monitoring.html">known</a> to develop surveillance technologies, a so-called Little Tech ecosystem of mostly unregulated, venture capital-funded startups and small vendors making these products has grown since Covid-19, the report found. The term “Little Tech” was popularized by the VC firm <a href="https://a16z.com/the-little-tech-agenda/">Andreessen Horowitz</a>, which argued that excessive regulation was stifling innovation.</p>



<figure></figure>



<p><a href="https://restofworld.org/2022/workers-managed-algorithms/">Algorithmic management</a> and surveillance tools are getting even more intrusive in gig work, and are entering offices and the informal labor sector as well, Wilneida Negrón, director of research and policy at Coworker.org and a co-author of the report, told <em>Rest of World</em>.&nbsp;</p>



<p>“The pressure of the hyper-surveillance creates a lot of stress and creates a lot of uncertainty for workers. It brings a culture of suspiciousness,” she said.&nbsp;</p>



<p>Investments by Silicon Valley-based VC firms led to a boom in tech startups globally after Covid-19, Negrón said. This has carried over to companies building bossware products in the developing world, she said.&nbsp;&nbsp;</p>



<figure></figure>



<p>The technologies include biometric tracking, AI-powered productivity monitoring, and predictive analytics, the report found. Worker data is continuously collected and analyzed by algorithms with the stated aim to improve hiring, evaluate performance, and optimize processes.&nbsp;</p>



<p>Most managers in wealthier nations say algorithmic management tools improve their decision-making, according to a 2024 <a href="https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/algorithmic-management-in-the-workplace_3c84ed6d/287c13c4-en.pdf?utm_source=chatgpt.com">survey</a> of over 6,000 employers by the Organisation for Economic Co-operation and Development. More than 90% of American managers used such tools, especially to reward or sanction employees.</p>



<p>Many tools are first deployed in Latin America, where labor laws are less strictly enforced, according to Ayden Férdeline, a tech policy researcher in Berlin and a co-author of the report.</p>



<p>“There is a Latin America testing ground for products,” he told <em>Rest of World</em>. “If they are successful, they tend to be deployed in other jurisdictions, oftentimes with additional safeguards, sometimes not.”</p>



<p>Many workers are unaware of how their information is collected and used, Férdeline said.&nbsp;</p>



<p>Some gig workers in Kenya, Guatemala, and Brazil said bossware tools make them feel surveilled, and that they have less control over their work. In Porto Alegre, Brazil, Uber driver Carina Trindade told <em>Rest of World</em> she feels the app monitors her continuously, tracking her speed and braking patterns. The app has permissions <a href="https://dig.watch/updates/uber-starts-testing-tool-records-video-inside-car-during-rides">to access</a> her mic and camera, she said.&nbsp;</p>



<p>Uber spokesperson Gabriel Gabira said drivers have the option to record trips, and <a href="https://help.uber.com/en/driving-and-delivering/article/record-my-ride---frequently-asked-questions?nodeId=7f86cc51-2c3e-4888-8dc1-0ac2b6337b92">privacy terms</a> are followed to access the footage.</p>



<p>In Nairobi, Godfrey Sanya Wanga, a driver for ride-hailing firm SafeBoda, told <em>Rest of World </em>he felt the app undercharged a customer. “I really wanted to ask [the customer] to pay me more, but I remembered that I was being monitored and this would bring me trouble if the client reported me,” he said. SafeBoda did not respond to a request for comment.</p>



<p>Several nations have data protection and privacy laws, including Brazil, Nigeria, and Kenya. But enforcement is inconsistent, the report said.</p>



<p>Here are five current uses of algorithmic management tools. The companies mentioned below did not comment, unless otherwise stated.&nbsp;</p>



<p>1. <strong>Timekeeping and attendance systems&nbsp;</strong></p>



<p>What: Platforms that track the attendance of workers, often using geolocation and biometrics to verify presence.&nbsp;</p>



<p>Example: <a href="https://www.rankmi.com/es/productos/control-asistencia-y-turnos">Rankmi</a>, based in Chile, uses biometrics and geolocation to track workers. The platform also <a href="https://www.rankmi.com/es/ia-para-hr">gives</a> workers continuous performance feedback and evaluates job applicants using AI.&nbsp;</p>



<p>2. <strong>Biometric and identity verification tools</strong></p>



<p>What: Tools that use fingerprint and facial-recognition checks, special digital signatures stored on a secure network, and official records to confirm a worker’s identity before granting access.&nbsp;</p>



<p>Example: Cincel, based in Mexico, provides <a href="https://www.cincel.digital/productos/verificacion-identidad/">identity verification</a> tools that do various checks including biometrics, and also cross-check against government databases and <a href="https://www.cincel.digital/productos/background-check/">blacklists</a>.</p>



<p>3. <strong>Performance and productivity monitoring platforms</strong></p>



<p>What: Dashboards that score workers using tracked metrics such as keystrokes, transaction counts, customer interactions, and task completion times.</p>



<p>Example: <a href="https://www.ahgora.com/">Ahgora</a>, based in Brazil, offers HR software that allows managers to continually “oversee team attendance in real-time” and that tracks productivity. It uses the data to offer predictions about work, such as potential issues with attendance, which can inform decision-making.&nbsp;</p>



<p>4. <strong>Algorithmic management and predictive analytics</strong></p>



<p>What: Platforms that<strong> </strong>automate HR functions, such as hiring shortlists, performance reviews, attrition forecasting, and also unionization-risk scoring.</p>



<p>Example: Visier’s AI-powered analytics platform <a href="https://www.visier.com/solutions/internal-mobility/">analyzes</a> HR data and provides insights, including resignation risk. The platform is used by global firms including Deloitte, Accenture, and Tata Consultancy Services.&nbsp;</p>



<p>Andrea Derler, principal of research and customer value at Visier, told <em>Rest of World</em> the platform only “processes data that organizations load into the platform, and we are not responsible for the way the data and insights we help provide is being used.”&nbsp;</p>



<p>5.&nbsp; <strong>Gig economy and field workforce tracking</strong></p>



<p>What: Apps that use the workers’ smartphones to dispatch and route deliveries. They use location, trip history, and ratings to allocate jobs and <a href="https://restofworld.org/2021/only-gojek-knows-this-mystery/">evaluate performance</a>. Workers are managed mostly by platforms rather than humans.</p>



<p>Example: <span><mark><a href="https://restofworld.org/tag/rappi" aria-label="Click to learn more about Rappi.">Rappi<span>i</span></a></mark><span><span><a href="https://restofworld.org/tag/rappi">Rappi</a><svg aria-label="Close" role="button"><use xlink:href="#X"></use></svg></span>Rappi, a Colombian company, has been providing delivery services across most Latin American countries since 2015.<span><a href="https://restofworld.org/tag/rappi"><span>READ MORE</span><svg><use xlink:href="#chevron"></use></svg></a></span></span></span>, a Colombian delivery app, tracks workers in real time. It has <a href="https://americasmi.com/insights/rappi-evolving-business-model-impact-latam-logistics/">auto accept</a>, where a rider can’t decline orders — and it’s mandatory to qualify for bonuses. Delivery worker Carolina Ramírez told <em>Rest of World </em>she works 14-hour days to earn a bonus of 100,000 pesos ($25) every week, leaving her little time for anything else. “My boss is the app. It’s unfair because to earn a good salary, I have to dedicate myself almost exclusively to this,” she said.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS Direct File on GitHub (605 pts)]]></title>
            <link>https://chrisgiven.com/2025/05/direct-file-on-github/</link>
            <guid>44182356</guid>
            <pubDate>Wed, 04 Jun 2025 16:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisgiven.com/2025/05/direct-file-on-github/">https://chrisgiven.com/2025/05/direct-file-on-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44182356">Hacker News</a></p>
Couldn't get https://chrisgiven.com/2025/05/direct-file-on-github/: Error: getaddrinfo ENOTFOUND chrisgiven.com]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: GPT image editing, but for 3D models (149 pts)]]></title>
            <link>https://www.adamcad.com/</link>
            <guid>44182206</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adamcad.com/">https://www.adamcad.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44182206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-root="" id="main" data-framer-hydrate-v2="{&quot;routeId&quot;:&quot;TsrMrhCtf&quot;,&quot;localeId&quot;:&quot;default&quot;,&quot;breakpoints&quot;:[{&quot;hash&quot;:&quot;7rp3o1&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1200px)&quot;},{&quot;hash&quot;:&quot;4ud97w&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1199px)&quot;},{&quot;hash&quot;:&quot;43sqdp&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809px)&quot;}]}" data-framer-ssr-released-at="2025-05-14T07:55:41.954Z" data-framer-page-optimized-at="2025-05-16T23:26:25.868Z"><div data-border="true" data-framer-name="Nav Section"><!--$--><!--$--><a data-framer-name="Adam-Selected" aria-label="Adam CAD Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><div data-framer-name="Btn"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="68.4683px"></p></div></a><!--/$--><!--/$--></div><div data-framer-name="Hero" id="hero"><div data-framer-name="Text + Signup" id="hero-textsignup"><div data-framer-name="Content" id="heading-content"><div><p data-framer-name="Heading" data-framer-component-type="RichTextContainer"><h2>Speak anything into <span>existence</span></h2></p></div><div><p data-framer-name="Heading" data-framer-component-type="RichTextContainer"><h2>Speak anything into <span>existence</span></h2></p></div><p>AdamCAD is an AI Powered CAD platform that generates 3D designs in seconds</p></div><div aria-label="Y Combinator Logo" data-framer-name="Back-by-yc"><p><img decoding="async" width="716" height="267" sizes="(min-width: 1200px) 169px, (min-width: 810px) and (max-width: 1199px) 169px, (max-width: 809px) 169px" srcset="https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png?scale-down-to=512 512w,https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png 716w" src="https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png?scale-down-to=512" alt="" data-framer-original-sizes="169px"></p></div></div><div aria-label="Adam CAD Preview" data-border="true" data-framer-name="Graphics" id="product-demo"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div></div><div data-framer-name="How it works"><div data-framer-name="Row 1"><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=512 512w,https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png 1788w" src="https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Text to CAD</h3></p><div data-framer-component-type="RichTextContainer"><p>Use prompts in AdamCAD to describe and edit the 3D</p><p>model you want to create.</p></div></div></div><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=512 512w,https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png 1788w" src="https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Refine &amp; Export</h3></p><p>AdamCAD generates the 3D Model and a list of parameters based on your design to refine it further.</p></div></div></div><div data-framer-name="Row 2"><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=512 512w,https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png 1788w" src="https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Image to 3D</h3></p><p>AdamCAD's creative mode turns any image into a 3D generation in seconds.</p></div></div><div data-framer-name="Card"><p data-framer-component-type="RichTextContainer"><h3>AdamCAD AI Co-Pilot</h3></p><p>AdamCAD is built to integrate with the CAD software professionals rely on.</p></div></div></div><div data-framer-name="Collapsed"><!--$--><a data-framer-name="Landing Nav Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="81px"></p></a><!--/$--></div><div data-framer-name="Collapsed"><!--$--><a data-framer-name="Landing Nav Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="81px"></p></a><!--/$--></div><div data-framer-name="Footer"><p>© 2025  All Rights Reserved AdamCAD</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta found 'covertly tracking' Android users through Instagram and Facebook (202 pts)]]></title>
            <link>https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</link>
            <guid>44182204</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083">https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</a>, See on <a href="https://news.ycombinator.com/item?id=44182204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-name="ui-article-body" data-highlight-intro="true">
      
      <p>Meta and search engine company Yandex have been "covertly tracking" Android users in the background of their devices, according to experts.</p><p>Academics at the Radboud University in the Netherlands and IMDEA Networks said they discovered Meta and Yandex have been tracking Android users' browser activity without their consent and then using the data in their apps.</p>
<p>Meta said it was looking into the issue, while Yandex denied collecting any sensitive data.</p><p>Gunes Acar, assistant professor at Radboud University, said the "covert" data collection was spotted in January.</p><p>He said he discovered Meta's apps, including Facebook and Instagram, and Yandex's apps, such as Yandex Maps, were sitting in the background of Android devices and loading a script that sent data locally back to apps on users' phones.</p>
<p>The scripts bypassed Android's security measures and meant that Meta and Yandex could track what users were doing on web browsers, without the user consenting or even knowing, according to the expert.</p><p>"They are bridging these two worlds that we think are separate; web browsing and mobile app activities," Dr Acar told Sky News.</p><p>"That's very shocking."</p><p>The apps were able to track users' browser data on all major Android browsers, even if the user was in incognito mode, the academics said.</p>        
<p>"It's really concerning because it negates every privacy control that you have in modern browsers and also in modern mobile platforms like Android," said Narseo Vallina-Rodriguez, associate professor at IMDEA Networks, to Sky News.</p><p><strong><a href="https://news.sky.com/topic/google-5876/1" target="_blank">Google</a></strong>, which owns the Android operating system, confirmed the covert activity to Sky News.</p><p>It said Meta and Yandex used <strong><a href="https://news.sky.com/topic/android-8090/1" target="_blank">Android's</a></strong> capabilities "in unintended ways that blatantly violate our security and privacy principles".</p><p><strong>What have Meta and Yandex said?</strong></p><p>Meta told Sky News it was quickly looking into the issue.</p><p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," said a Meta spokesperson.</p><p>"Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p><p>Yandex said it "strictly complies with data protection standards", adding: "The feature in question does not collect any sensitive information and is solely intended to improve personalisation within our apps."</p><p><strong>Read more science and tech news:<br><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">AI foot scanner recognises heart warning signs</a></strong><br><strong><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">Coffee 'helps women age more healthily'</a></strong></p>    
<p>Meta appeared to have been doing the data tracking for around eight months, while Yandex had since 2017, the academics said.</p><p>"We found that Facebook was doing it on roughly 16,000 websites when visited from the EU, [...] Yandex was doing this on 1,300 websites," said Tim Vlummens, a PHD student at KU Leuven who worked on the research.</p><p>Google told Sky News it had already "implemented changes to mitigate these invasive techniques and have opened our own investigation and are directly in touch with the parties".</p>     <a href="https://news.sky.com/download-app" target="blank" data-tracking-label="ui-app-promo-download-link" data-type="" data-component-name="ui-app-promo">
        
    </a>


<p>The tech giant did not respond when asked what repercussions Meta and Yandex were facing for their conduct.</p><p>Firefox, Microsoft Edge and DuckDuckGo browsers were also affected, with Firefox owner Mozilla and DuckDuckGo engineers taking action to stop any future covert tracking.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Prompt Engineering Playbook for Programmers (270 pts)]]></title>
            <link>https://addyo.substack.com/p/the-prompt-engineering-playbook-for</link>
            <guid>44182188</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addyo.substack.com/p/the-prompt-engineering-playbook-for">https://addyo.substack.com/p/the-prompt-engineering-playbook-for</a>, See on <a href="https://news.ycombinator.com/item?id=44182188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Developers are increasingly relying on AI coding assistants to accelerate our daily workflows. These tools can autocomplete functions, suggest bug fixes, and even generate entire modules or MVPs. Yet, as many of us have learned, the </span><em>quality</em><span> of the AI’s output depends largely on the </span><em>quality of the prompt</em><span> you provide. In other words, </span><strong>prompt engineering</strong><span> has become an essential skill. A poorly phrased request can yield irrelevant or generic answers, while a well-crafted prompt can produce thoughtful, accurate, and even creative code solutions. This write-up takes a practical look at how to systematically craft effective prompts for common development tasks.</span></p><p><span>AI pair programmers are powerful but not magical – they have no prior knowledge of your specific project or intent beyond what you tell them or include as context. The more information you provide, the better the output. We’ll distill key prompt patterns, </span><strong>repeatable frameworks</strong><span>, and memorable examples that have resonated with developers. You’ll see side-by-side comparisons of </span><strong>good vs. bad prompts</strong><span> with actual AI responses, along with commentary to understand why one succeeds where the other falters. </span><strong>Here’s a cheat sheet to get started:</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" width="1456" height="1915" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1915,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:520317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Prompting an AI coding tool is somewhat like communicating with a very literal, </span><em>sometimes</em><span> knowledgeable collaborator. To get useful results, you need to set the stage clearly and guide the AI on </span><em>what</em><span> you want and </span><em>how</em><span> you want it. </span></p><p>Below are foundational principles that underpin all examples in this playbook:</p><ul><li><p><strong>Provide rich context.</strong><span> Always assume the AI knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is </span><em>supposed</em><span> to do. </span><strong>Specificity</strong><span> and </span><strong>context</strong><span> make the difference between vague suggestions and precise, actionable solutions . In practice, this means your prompt might include a brief setup like: “I have a Node.js function using Express and Mongoose that should fetch a user by ID, but it throws a TypeError. Here’s the code and error…”. The more setup you give, the less the AI has to guess.</span></p></li><li><p><strong>Be specific about your goal or question.</strong><span> Vague queries lead to vague answers. Instead of asking something like “Why isn’t my code working?”, pinpoint what insight you need. For example: “This JavaScript function is returning undefined instead of the expected result. Given the code below, can you help identify why and how to fix it?” is far more likely to yield a helpful answer. One prompt formula for debugging is: </span><em>“It’s expected to do [expected behavior] but instead it’s doing [current behavior] when given [example input]. Where is the bug?”</em><span> . Similarly, if you want an optimization, ask for a </span><em>specific kind</em><span> of optimization (e.g. </span><em>“How can I improve the runtime performance of this sorting function for 10k items?”</em><span>). Specificity guides the AI’s focus .</span></p></li><li><p><strong>Break down complex tasks.</strong><span> When implementing a new feature or tackling a multi-step problem, don’t feed the entire problem in one gigantic prompt. It’s often more effective to split the work into smaller chunks and iterate. For instance, </span><em>“First, generate a React component skeleton for a product list page. Next, we’ll add state management. Then, we’ll integrate the API call.”</em><span> Each prompt builds on the previous. It’s often not advised to ask for a whole large feature in one go; instead, start with a high-level goal and then iteratively ask for each piece . This approach not only keeps the AI’s responses focused and manageable, but also mirrors how a human would incrementally build a solution.</span></p></li><li><p><strong>Include examples of inputs/outputs or expected behavior.</strong><span> If you can illustrate what you want with an example, do it. For example, </span><em>“Given the array [3,1,4], this function should return [1,3,4].”</em><span> Providing a concrete example in the prompt helps the AI understand your intent and reduces ambiguity . It’s akin to giving a junior developer a quick test case – it clarifies the requirements. In prompt engineering terms, this is sometimes called “</span><strong>few-shot prompting</strong><span>,” where you show the AI a pattern to follow. Even one example of correct behavior can guide the model’s response significantly.</span></p></li><li><p><strong>Leverage roles or personas.</strong><span> A powerful technique popularized in many viral prompt examples is to ask the AI to “act as” a certain persona or role. This can influence the style and depth of the answer. For instance, </span><em>“Act as a senior React developer and review my code for potential bugs”</em><span> or </span><em>“You are a JavaScript performance expert. Optimize the following function.”</em><span> By setting a role, you prime the assistant to adopt the relevant tone – whether it’s being a strict code reviewer, a helpful teacher for a junior dev, or a security analyst looking for vulnerabilities. Community-shared prompts have shown success with this method, such as </span><em>“Act as a JavaScript error handler and debug this function for me. The data isn’t rendering properly from the API call.”</em><span> . In our own usage, we must still provide the code and problem details, but the </span><strong>role-play</strong><span> prompt can yield more structured and expert-level guidance.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3413086,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"></picture></div></a></figure></div><ul><li><p><strong>Iterate and refine the conversation.</strong><span> Prompt engineering is an </span><em>interactive</em><span> process, not a one-shot deal. Developers often need to review the AI’s first answer and then ask follow-up questions or make corrections. If the solution isn’t quite right, you might say, </span><em>“That solution uses recursion, but I’d prefer an iterative approach – can you try again without recursion?”</em><span> Or, </span><em>“Great, now can you improve the variable names and add comments?”</em><span> The AI remembers the context in a chat session, so you can progressively steer it to the desired outcome. The key is to view the AI as a partner you can coach – </span><strong>progress over perfection</strong><span> on the first try .</span></p></li><li><p><strong>Maintain code clarity and consistency.</strong><span> This last principle is a bit indirect but very important for tools that work on your code context. Write clean, well-structured code and comments, even before the AI comes into play. Meaningful function and variable names, consistent formatting, and docstrings not only make your code easier to understand for humans, but also give the AI stronger clues about what you’re doing. If you show a consistent pattern or style, the AI will continue it . Treat these tools as extremely attentive junior developers – they take every cue from your code and comments.</span></p></li></ul><p><span>With these foundational principles in mind, let’s dive into specific scenarios. We’ll start with </span><strong>debugging</strong><span>, perhaps the most immediate use-case: you have code that’s misbehaving, and you want the AI to help figure out why.</span></p><p>Debugging is a natural fit for an AI assistant. It’s like having a rubber-duck that not only listens, but actually talks back with suggestions. However, success largely depends on how you present the problem to the AI. Here’s how to systematically prompt for help in finding and fixing bugs:</p><p><strong>1. Clearly describe the problem and symptoms.</strong><span> Begin your prompt by describing what is going wrong and what the code is supposed to do. Always include the exact error message or incorrect behavior. For example, instead of just saying “My code doesn’t work,” you might prompt: </span><em>“I have a function in JavaScript that should calculate the sum of an array of numbers, but it’s returning NaN (Not a Number) instead of the actual sum. Here is the code: [include code]. It should output a number (the sum) for an array of numbers like [1,2,3], but I’m getting NaN. What could be the cause of this bug?”</em><span> This prompt specifies the language, the intended behavior, the observed wrong output, and provides the code context – all crucial information. Providing a structured context (code + error + expected outcome + what you’ve tried) gives the AI a solid starting point . By contrast, a generic question like “Why isn’t my function working?” yields meager results – the model can only offer the most general guesses without context.</span></p><p><strong>2. Use a step-by-step or line-by-line approach for tricky bugs.</strong><span> For more complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance: </span><em>“Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly – where does the logic go wrong?”</em><span> This is an example of a </span><strong>rubber duck debugging prompt</strong><span> – you’re essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in: </span><em>“Explain what the filter call is doing here, and if it might be excluding more items than it should.”</em><span> Engaging the AI in an explanatory role can surface the bug in the process of explanation.</span></p><p><strong>3. Provide minimal reproducible examples when possible.</strong><span> Sometimes your actual codebase is large, but the bug can be demonstrated in a small snippet. If you can extract or simplify the code that still reproduces the issue, do so and feed that to the AI. This not only makes it easier for the AI to focus, but also forces you to clarify the problem (often a useful exercise in itself). For example, if you’re getting a TypeError in a deeply nested function call, try to reproduce it with a few lines that you can share. Aim to isolate the bug with the minimum code, make an assumption about what’s wrong, test it, and iterate . You can involve the AI in this by saying: </span><em>“Here’s a pared-down example that still triggers the error [include snippet]. Why does this error occur?”</em><span> By simplifying, you remove noise and help the AI pinpoint the issue. (This technique mirrors the advice of many senior engineers: if you can’t immediately find a bug, simplify the problem space. The AI can assist in that analysis if you present a smaller case to it.)</span></p><p><strong>4. Ask focused questions and follow-ups.</strong><span> After providing context, it’s often effective to directly ask what you need, for example: </span><em>“What might be causing this issue, and how can I fix it?”</em><span> . This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up. You could say, </span><em>“That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.”</em><span> In a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VS Code or Cursor without a chat, you might instead write a comment above the code like // BUG: returns NaN, fix this function and see how it autocompletes – but in general, the interactive chat yields more thorough explanations. Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask </span><em>“Can you explain why that change solves the problem?”</em><span> This way you learn for next time, and you double-check that the AI’s reasoning is sound.</span></p><p><span>Now, let’s illustrate these debugging prompt principles with a concrete example, showing a </span><strong>poor prompt vs. improved prompt</strong><span> and the difference in AI responses:</span></p><p>Imagine we have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:</p><pre><code><code>// Buggy function: converts array of users to a map by ID
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    // Note: using &lt;= will go out of bounds on the last iteration
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}

// Example usage:
const result = mapUsersById([{ id: 1, name: "Alice" }]);</code></code></pre><p>This code is supposed to iterate through the users array and build an object where keys are user IDs. However, there is a bug: the loop uses i &lt;= users.length instead of i &lt; users.length. This will cause users[i] to be undefined on the last iteration, likely leading to a runtime error (e.g., attempting to access property id of undefined). Let’s see how an AI assistant might respond to two different prompts about this issue.</p><p><strong>❌ Poor Prompt:</strong><span> “Why isn’t my mapUsersById function working?”</span></p><p><em>AI’s response to the poor prompt (simulated):</em></p><blockquote><p><em>It’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.</em></p></blockquote><p><strong>Analysis:</strong><span> This answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“maybe the array is empty or not an array”, “make sure each user has an id”). It did not identify the actual bug. This is the kind of response you get when the question is underspecified – the model can only regurgitate common debugging tips. As a developer, this isn’t very helpful because it doesn’t address the real mistake. The </span><strong>lack of context</strong><span> and specificity in the prompt yields a generic answer (essentially the AI equivalent of “have you tried turning it off and on again?”). This aligns with the community advice that vague prompts lead to irrelevant or generic answers .</span></p><p><strong>✅ Improved Prompt:</strong><span> “I have a JavaScript function mapUsersById that should convert an array of user objects into a map (object) keyed by user ID. However, it throws an error when I run it. For example, when I pass [ {id: 1, name: "Alice"} ], I get TypeError: Cannot read property 'id' of undefined. Here is the function code:</span></p><pre><code><code>function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}</code></code></pre><p>It should return { "1": {id: 1, name: "Alice"} }. What is the bug and how can I fix it?”</p><p><em>AI’s response to the improved prompt (simulated):</em></p><blockquote><p>*The error Cannot read property 'id' of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using i &lt;= users.length. If users.length is 1 (for one user in the array), the loop runs with i = 0 and i = 1. When i is 1, users[1] is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and user.id throws a TypeError. The fix is to change the loop condition to i &lt; users.length so that i goes from 0 to users.length-1. For example:</p></blockquote><pre><code><code>for (let i = 0; i &lt; users.length; i++) {
  const user = users[i];
  userMap[user.id] = user;
}</code></code></pre><blockquote><p>This way, i will be 0 for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning {"1": {id: 1, name: "Alice"}} for the given input.*</p></blockquote><p><strong>Analysis:</strong><span> This response directly pinpoints the bug (using &lt;= in the loop) and explains why it causes user to be undefined. It then provides the correct code solution (i &lt; users.length) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question. Notice how the AI used the error message as a clue to focus on the loop bounds – a targeted prompt enabled the AI to engage in true problem-solving, effectively simulating how a human debugger would think: “where could undefined come from? likely from the loop indexing”. This is a concrete demonstration of the benefit of detailed prompts. </span></p><p><strong>Additional Debugging Tactics:</strong><span> Beyond identifying obvious bugs, you can use prompt engineering for deeper debugging assistance:</span></p><ul><li><p><em>Ask for potential causes.</em><span> If you’re truly stumped, you can broaden the question slightly: </span><em>“What are some possible reasons for a TypeError: cannot read property 'foo' of undefined in this code?”</em><span> along with the code. The model might list a few scenarios (e.g. the object wasn’t initialized, a race condition, wrong variable scoping, etc.). This can give you angles to investigate that you hadn’t considered. It’s like brainstorming with a colleague.</span></p></li><li><p><em>“Ask the Rubber Duck”</em><span> – i.e., explain your code to the AI. This may sound counterintuitive (why explain to the assistant?), but the act of writing an explanation can clarify your own understanding, and you can then have the AI verify or critique it. For example: </span><em>“I will explain what this function is doing: [your explanation]. Given that, is my reasoning correct and does it reveal where the bug is?”</em><span> The AI might catch a flaw in your explanation that points to the actual bug. This technique leverages the AI as an active rubber duck that not only listens but responds.</span></p></li><li><p><em>Have the AI create test cases.</em><span> You can ask: </span><em>“Can you provide a couple of test cases (inputs) that might break this function?”</em><span> The assistant might come up with edge cases you didn’t think of (empty array, extremely large numbers, null values, etc.). This is useful both for debugging and for generating tests for future robustness.</span></p></li><li><p><em>Role-play a code reviewer.</em><span> As an alternative to a direct “debug this” prompt, you can say: </span><em>“Act as a code reviewer. Here’s a snippet that isn’t working as expected. Review it and point out any mistakes or bad practices that could be causing issues: [code]”.</em><span> This sets the AI into a critical mode. Many developers find that phrasing the request as a code review yields a very thorough analysis, because the model will comment on each part of the code (and often, in doing so, it spots the bug). In fact, one prompt engineering tip is to explicitly request the AI to behave like a meticulous reviewer . This can surface not only the bug at hand but also other issues (e.g. potential null checks missing) which might be useful.</span></p></li></ul><p><span>In summary, when debugging with an AI assistant, </span><strong>detail and direction are your friends</strong><span>. Provide the scenario, the symptoms, and then ask pointed questions. The difference between a flailing “it doesn’t work, help!” prompt and a surgical debugging prompt is night and day, as we saw above. Next, we’ll move on to another major use case: refactoring and improving existing code.</span></p><p><span>Refactoring code – making it cleaner, faster, or more idiomatic without changing its functionality – is an area where AI assistants can shine. They’ve been trained on vast amounts of code, which includes many examples of well-structured, optimized solutions. However, to tap into that knowledge effectively, </span><strong>your prompt must clarify what “better” means for your situation</strong><span>. Here’s how to prompt for refactoring tasks:</span></p><p><strong>1. State your refactoring goals explicitly.</strong><span> “Refactor this code” on its own is too open-ended. Do you want to improve readability? Reduce complexity? Optimize performance? Use a different paradigm or library? The AI needs a target. A good prompt frames the task, for example: </span><em>“Refactor the following function to improve its readability and maintainability (reduce repetition, use clearer variable names).”</em><span> Or </span><em>“Optimize this algorithm for speed – it’s too slow on large inputs.”</em><span> By stating </span><strong>specific goals</strong><span>, you help the model decide which transformations to apply . For instance, telling it you care about performance might lead it to use a more efficient sorting algorithm or caching, whereas focusing on readability might lead it to break a function into smaller ones or add comments. If you have multiple goals, list them out. A prompt template from the Strapi guide suggests even enumerating issues: </span><em>“Issues I’d like to address: 1) [performance issue], 2) [code duplication], 3) [outdated API usage].”</em><span> . This way, the AI knows exactly what to fix. Remember, it will not inherently know </span><em>what you consider a problem</em><span> in the code – you must tell it.</span></p><p><strong>2. Provide the necessary code context.</strong><span> When refactoring, you’ll typically include the code snippet that needs improvement in the prompt. It’s important to include the full function or section that you want to be refactored, and sometimes a bit of surrounding context if relevant (like the function’s usage or related code, which could affect how you refactor). Also mention the language and framework, because “idiomatic” code varies between, say, idiomatic Node.js vs. idiomatic Deno, or React class components vs. functional components. For example: </span><em>“I have a React component written as a class. Please refactor it to a functional component using Hooks.”</em><span> The AI will then apply the typical steps (using useState, useEffect, etc.). If you just said “refactor this React component” without clarifying the style, the AI might not know you specifically wanted Hooks.</span></p><ul><li><p><strong>Include version or environment details if relevant.</strong><span> For instance, </span><em>“This is a Node.js v14 codebase”</em><span> or </span><em>“We’re using ES6 modules”</em><span>. This can influence whether the AI uses certain syntax (like import/export vs. require), which is part of a correct refactoring. If you want to ensure it doesn’t introduce something incompatible, mention your constraints.</span></p></li></ul><p><strong>3. Encourage explanations along with the code.</strong><span> A great way to learn from an AI-led refactor (and to verify its correctness) is to ask for an explanation of the changes. For example: </span><em>“Please suggest a refactored version of the code, and explain the improvements you made.”</em><span> This was even built into the prompt template we referenced: </span><em>“…suggest refactored code with explanations for your changes.”</em><span> . When the AI provides an explanation, you can assess if it understood the code and met your objectives. The explanation might say: “I combined two similar loops into one to reduce duplication, and I used a dictionary for faster lookups,” etc. If something sounds off in the explanation, that’s a red flag to examine the code carefully. In short, </span><em>use the AI’s ability to explain as a safeguard</em><span> – it’s like having the AI perform a code review on its own refactor.</span></p><p><strong>4. Use role-play to set a high standard.</strong><span> As mentioned earlier, asking the AI to act as a code reviewer or senior engineer can be very effective. For refactoring, you might say: </span><em>“Act as a seasoned TypeScript expert and refactor this code to align with best practices and modern standards.”</em><span> This often yields not just superficial changes, but more insightful improvements because the AI tries to live up to the “expert” persona. A popular example from a prompt guide is having the AI role-play a mentor: </span><em>“Act like an experienced Python developer mentoring a junior. Provide explanations and write docstrings. Rewrite the code to optimize it.”</em><span> . The result in that case was that the AI used a more efficient data structure (set to remove duplicates) and provided a one-line solution for a function that originally used a loop . The role-play helped it not only refactor but also explain </span><em>why</em><span> the new approach is better (in that case, using a set is a well-known optimization for uniqueness).</span></p><p>Now, let’s walk through an example of refactoring to see how a prompt can influence the outcome. We will use a scenario in JavaScript (Node.js) where we have some less-than-ideal code and we want it improved.</p><p>Suppose we have a function that makes two database calls and does some processing. It works, but it’s not pretty – there’s duplicated code and it’s hard to read. We’d like to refactor it for clarity and efficiency. Here’s the original code:</p><pre><code><code>// Original function: Fetches two lists and processes them (needs refactoring)
async function getCombinedData(apiClient) {
  // Fetch list of users
  const usersResponse = await apiClient.fetch('/users');
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  const users = await usersResponse.json();
  // Fetch list of orders
  const ordersResponse = await apiClient.fetch('/orders');
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }
  const orders = await ordersResponse.json();
  // Combine data (match user to orders)
  const result = [];
  for (let user of users) {
    const userOrders = orders.filter(o =&gt; o.userId === user.id);
    result.push({ user, orders: userOrders });
  }
  return result;
}</code></code></pre><p>Issues in this code: We see duplicated fetch logic for users and orders (could be refactored into a helper or loop), lack of error detail (just a generic error message), and it could perhaps run the fetches in parallel to improve speed. Let’s see two prompting approaches:</p><p><strong>❌ Prompt 1 (Underspecified):</strong><span> “Refactor the above getCombinedData function.”</span></p><p><em>AI’s response to Prompt 1:</em></p><blockquote><p>*Sure. Here’s a refactored version of getCombinedData:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // Fetch users and orders in parallel
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);
  if (!usersResponse.ok || !ordersResponse.ok) {
    throw new Error('Failed to fetch data');
  }
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);
  // Map orders by userId for efficient lookup
  const ordersByUser = {};
  for (const order of orders) {
    if (!ordersByUser[order.userId]) {
      ordersByUser[order.userId] = [];
    }
    ordersByUser[order.userId].push(order);
  }
  // Combine data
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []
  }));
}</code></code></pre><blockquote><p>I fetched users and orders in parallel to improve performance. I also consolidated error handling and used a map (ordersByUser) to group orders by user, which makes the combination step cleaner and potentially faster.*</p></blockquote><p><strong>Analysis:</strong><span> This is actually a pretty good refactor, hitting several improvements: parallel fetching, grouping orders by user for efficiency, cleaner combination using .map and defaulting to an empty array. The AI also explained what it did. So why do we label this prompt as underspecified or “poor”? In this case, the AI happened to make good choices, but it </span><em>assumed</em><span> a lot because the prompt was open-ended. It assumed that performance should be improved (parallel requests, which is a smart change but what if parallel wasn’t safe here?), and it changed error handling to a generic message “Failed to fetch data,” which in combining the errors lost the specificity of which call failed. </span></p><p>The risk with a generic “Refactor this” prompt is that the AI might make changes that alter behavior in unintended ways. Here, combining error handling might be acceptable or not, depending on requirements. The assistant also made a structural change (using a dictionary to group orders) – that’s fine and probably for the better, but if we had specific constraints (like wanting to stream results or keep memory usage low), we didn’t inform it. So while the result looks good, it was somewhat lucky that the AI’s own interpretation of “refactor” aligned with what we wanted. This illustrates that an underspecified prompt can sometimes get good results on simple cases, but for complex code, it might go off in an undesired direction or miss certain improvements you care about.</p><p><strong>✅ Prompt 2 (Goal-Oriented):</strong><span> “Refactor the above getCombinedData function to eliminate duplicate code and improve performance. Specifically: (1) Avoid repeating the fetch logic for users and orders – maybe use a helper or fetch them together. (2) Fetch both lists in parallel if possible. (3) Keep the error handling for each fetch (we want to know which call failed). (4) Improve the combination of data, possibly by using a more efficient structure for lookup instead of a nested loop. Provide the refactored code with comments explaining the changes.”</span></p><p><em>AI’s response to Prompt 2:</em></p><blockquote><p>*Sure! Here’s an improved version of getCombinedData with the requested refactoring:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // 1. Fetch users and orders in parallel to improve performance
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);

  // 2. Handle errors separately to know which call failed
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }

  // Parse JSON responses in parallel
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);

  // 3. Use a lookup map for orders by userId for efficient matching
  const ordersByUser = orders.reduce((map, order) =&gt; {
    if (!map[order.userId]) {
      map[order.userId] = [];
    }
    map[order.userId].push(order);
    return map;
  }, {});

  // 4. Combine data using the lookup map
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []  // empty array if no orders for user
  }));
}</code></code></pre><blockquote><p><strong>Explanation of changes:</strong></p></blockquote><ul><li><p><em>Parallel fetching:</em><span> I used Promise.all to fetch users and orders at the same time (point 1).</span></p></li><li><p><em>Preserved individual error handling:</em><span> I kept separate checks for usersResponse.ok and ordersResponse.ok so that any error clearly indicates which fetch failed (point 2).</span></p></li><li><p><em>Parallel JSON parsing:</em><span> Also used Promise.all for response.json() calls, which is a minor improvement.</span></p></li><li><p><em>Efficient combination:</em><span> Instead of a nested loop, I created an ordersByUser map (point 3). This groups orders by userId in a single pass. Then I map over users to build the result, pulling each user’s orders from the map in O(1) time (point 4). This is more efficient and concise.*</span></p></li></ul><p><strong>Analysis:</strong><span> This response directly addressed all the specified goals. The code is refactored to be cleaner and faster, and it maintained separate error messages as requested. The AI’s explanation confirms each point we listed, which shows it carefully followed the prompt instructions. This is a great outcome because we, as the prompter, defined what “refactor” meant in this context. By doing so, we guided the AI to produce a solution that matches our needs with minimal back-and-forth. If the AI had overlooked one of the points (say it still merged the error handling), we could easily prompt again: </span><em>“Looks good, but please ensure the error messages remain distinct for users vs orders.”</em><span> – however, in this case it wasn’t needed because our prompt was thorough.</span></p><p><span>This example demonstrates a key lesson: </span><strong>when you know what you want improved, spell it out.</strong><span> AI is good at following instructions, but it won’t read your mind. A broad “make this better” might work for simple things, but for non-trivial code, you’ll get the best results by enumerating what “better” means to you. This aligns with community insights that clear, structured prompts yield significantly improved results .</span></p><p><strong>Additional Refactoring Tips:</strong></p><ul><li><p><em>Refactor in steps:</em><span> If the code is very large or you have a long list of changes, you can tackle them one at a time. For example, first ask the AI to “refactor for readability” (focus on renaming, splitting functions), then later “optimize the algorithm in this function.” This prevents overwhelming the model with too many instructions at once and lets you verify each change stepwise.</span></p></li><li><p><em>Ask for alternative approaches:</em><span> Maybe the AI’s first refactor works but you’re curious about a different angle. You can ask, </span><em>“Can you refactor it in another way, perhaps using functional programming style (e.g. array methods instead of loops)?”</em><span> or </span><em>“How about using recursion here instead of iterative approach, just to compare?”</em><span> This way, you can evaluate different solutions. It’s like brainstorming multiple refactoring options with a colleague.</span></p></li><li><p><em>Combine refactoring with explanation to learn patterns:</em><span> We touched on this, but it’s worth emphasizing – use the AI as a learning tool. If it refactors code in a clever way, study the output and explanation. You might discover a new API or technique (like using reduce to build a map) that you hadn’t used before. This is one reason to ask for explanations: it turns an answer into a mini-tutorial, reinforcing your understanding of best practices.</span></p></li><li><p><em>Validation and testing:</em><span> After any AI-generated refactor, always run your tests or try the code with sample inputs. AI might inadvertently introduce subtle bugs, especially if the prompt didn’t specify an important constraint. For example, in our refactor, if the original code intentionally separated fetch errors for logging but we didn’t mention logging, the combined error might be less useful. It’s our job to catch that in review. The AI can help by writing tests too – you could ask </span><em>“Generate a few unit tests for the refactored function”</em><span> to ensure it behaves the same as before on expected inputs.</span></p></li></ul><p><span>At this point, we’ve covered debugging and refactoring – improving existing code. The next logical step is to use AI assistance for </span><strong>implementing new features</strong><span> or generating new code. We’ll explore how to prompt for that scenario effectively.</span></p><p><strong>❌ Poor Prompt:</strong><span> "My useEffect isn't working right"</span></p><p><strong>✅ Enhanced Prompt:</strong></p><pre><code><code>I have a React component that fetches user data, but it's causing infinite re-renders. Here's my code:


const UserProfile = ({ userId }) =&gt; {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(true);
  
  useEffect(() =&gt; {
    fetchUser(userId).then(setUser).finally(() =&gt; setLoading(false));
  }, [userId, setUser, setLoading]); // Problem is here
  
  return loading ? &lt;div&gt;Loading...&lt;/div&gt; : &lt;div&gt;{user?.name}&lt;/div&gt;;
};</code></code></pre><p><strong>Expected behavior:</strong><span> Should fetch user data once when userId changes Actual behavior: Component re-renders infinitely Error in console: "Warning: Maximum update depth exceeded"</span></p><p>What's causing this infinite loop and how do I fix the dependency array?</p><p><strong>Why this works:</strong><span> Provides exact code, error message, expected vs actual behavior, and focuses on a specific React pattern that's commonly misunderstood.</span></p><p><strong>❌ Poor Prompt:</strong><span> "Build the state management for my Next.js ecommerce app”</span></p><p>✅ Enhanced Prompt:</p><p>I'm building a Next.js 14 e-commerce app and need to design the state management architecture. Here are my requirements:</p><p>Components:</p><ul><li><p>Product listing page (needs: products[], filters, pagination)</p></li><li><p>Shopping cart (needs: cart items, totals, shipping info)</p></li><li><p>User auth (needs: user profile, auth status, preferences)</p></li><li><p>Real-time notifications (needs: toast messages, error states)</p></li></ul><p>Technical constraints:</p><ul><li><p>Next.js 14 with App Router and Server Components</p></li><li><p>TypeScript strict mode</p></li><li><p>Server-side data fetching for SEO</p></li><li><p>Client-side interactivity for cart/user actions</p></li><li><p>State should persist across navigation</p></li></ul><p>Should I use:</p><ol><li><p>Zustand stores for each domain (cart, auth, notifications)</p></li><li><p>React Query/TanStack Query for server state + Zustand for client state</p></li><li><p>A single Zustand store with slices</p></li></ol><p>Please provide a recommended architecture with code examples showing how to structure stores and integrate with Next.js App Router patterns.</p><p><strong>Why this works: </strong><span>Real-world scenario with specific tech stack, clear requirements, and asks for architectural guidance with implementation details.</span></p><p>One of the most exciting uses of AI code assistants is to help you write new code from scratch or integrate a new feature into an existing codebase. This could range from generating a boilerplate for a React component to writing a new API endpoint in an Express app. The challenge here is often that these tasks are open-ended – there are many ways to implement a feature. Prompt engineering for code generation is about guiding the AI to produce code that fits your needs and style. Here are strategies to do that:</p><p><strong>1. Start with high-level instructions, then drill down.</strong><span> Begin by outlining what you want to build in plain language, possibly breaking it into smaller tasks (similar to our advice on breaking down complex tasks earlier). For example, say you want to add a </span><strong>search bar feature</strong><span> to an existing web app. You might first prompt: </span><em>“Outline a plan to add a search feature that filters a list of products by name in my React app. The products are fetched from an API.”</em><span> </span></p><p>The AI might give you a step-by-step plan: “1. Add an input field for the search query. 2. Add state to hold the query. 3. Filter the products list based on the query. 4. Ensure it’s case-insensitive, etc.” Once you have this plan (which you can refine with the AI’s help), you can tackle each bullet with focused prompts. </p><p><span>For instance: </span><em>“Okay, implement step 1: create a SearchBar component with an input that updates a searchQuery state.”</em><span> After that, </span><em>“Implement step 3: given the searchQuery and an array of products, filter the products (case-insensitive match on name).”</em><span> By dividing the feature, you ensure each prompt is specific and the responses are manageable. This also mirrors iterative development – you can test each piece as it’s built.</span></p><p><strong>2. Provide relevant context or reference code.</strong><span> If you’re adding a feature to an existing project, it helps tremendously to show the AI how similar things are done in that project. For example, if you already have a component that is similar to what you want, you can say: </span><em>“Here is an existing UserList component (code…). Now create a ProductList component that is similar but includes a search bar.”</em><span> </span></p><p><span>The AI will see the patterns (maybe you use certain libraries or style conventions) and apply them. Having relevant files open or referencing them in your prompt provides context that leads to more project-specific and consistent code suggestions . Another trick: if your project uses a particular coding style or architecture (say Redux for state or a certain CSS framework), mention that. </span><em>“We use Redux for state management – integrate the search state into Redux store.”</em><span> </span></p><p><span>A well-trained model will then generate code consistent with Redux patterns, etc. Essentially, you are </span><strong>teaching the AI about your project’s environment</strong><span> so it can tailor the output. Some assistants can even use your entire repository as context to draw from; if using those, ensure you point it to similar modules or documentation in your repo.</span></p><ul><li><p><span>If starting something new but you have a preferred approach, you can also mention that: </span><em>“I’d like to implement this using functional programming style (no external state, using array methods).”</em><span> Or, </span><em>“Ensure to follow the MVC pattern and put logic in the controller, not the view.”</em><span> These are the kind of details a senior engineer might remind a junior about, and here </span><strong>you are the senior telling the AI</strong><span>.</span></p></li></ul><p><strong>3. Use comments and TODOs as inline prompts.</strong><span> When working directly in an IDE with Copilot, one effective workflow is writing a comment that describes the next chunk of code you need, then letting the AI autocomplete it. For example, in a Node.js backend, you might write: // TODO: Validate the request payload (ensure name and email are provided) and then start the next line. Copilot often picks up on the intent and generates a block of code performing that validation. This works because your comment is effectively a natural language prompt. However, be prepared to edit the generated code if the AI misinterprets – as always, verify its correctness.</span></p><p><strong>4. Provide examples of expected input/output or usage.</strong><span> Similar to what we discussed before, if you’re asking the AI to implement a new function, include a quick example of how it will be used or a simple test case. For instance: </span><em>“Implement a function formatPrice(amount) in JavaScript that takes a number (like 2.5) and returns a string formatted in USD (like $2.50). For example, formatPrice(2.5) should return '$2.50'.”</em><span> </span></p><p><span>By giving that example, you constrain the AI to produce a function consistent with it. Without the example, the AI might assume some other formatting or currency. The difference could be subtle but important. Another example in a web context: </span><em>“Implement an Express middleware that logs requests. For instance, a GET request to /users should log ‘GET /users’ to the console.”</em><span> This makes it clear what the output should look like. Including expected behavior in the prompt acts as a test the AI will try to satisfy.</span></p><p><strong>5. When the result isn’t what you want, rewrite the prompt with more detail or constraints.</strong><span> It’s common that the first attempt at generating a new feature doesn’t nail it. Maybe the code runs but is not idiomatic, or it missed a requirement. Instead of getting frustrated, treat the AI like a junior dev who gave a first draft – now you need to give feedback. For example, </span><em>“The solution works but I’d prefer if you used the built-in array filter method instead of a for loop.”</em><span> Or, </span><em>“Can you refactor the generated component to use React Hooks for state instead of a class component? Our codebase is all functional components.”</em><span> You can also add new constraints: </span><em>“Also, ensure the function runs in O(n) time or better, because n could be large.”</em><span> This iterative prompting is powerful. A real-world scenario: one developer asked an LLM to generate code to draw an ice cream cone using a JS canvas library, but it kept giving irrelevant output until they refined the prompt with more specifics and context . The lesson is, </span><em>don’t give up after one try</em><span>. Figure out what was lacking or misunderstood in the prompt and clarify it. This is the essence of prompt engineering – each tweak can guide the model closer to what you envision.</span></p><p>Let’s illustrate feature implementation prompting with an example scenario:</p><p>Suppose we have a React application and we want to add a new component: a product list with a search filter and a sort dropdown. This is a fairly typical feature in a web app. We’ll focus on prompting for the search functionality within this component for brevity.</p><p><strong>Initial Prompt (Feature request):</strong><span> “Create a React functional component called ProductList that displays a list of products and includes a text input to filter the products by name. The component should: fetch an array of products from /api/products (assume it returns JSON array of {id, name, ...}), store them in state, and allow the user to type in a search box to filter the displayed products by name (case-insensitive match). Display the filtered list in a simple &lt;ul&gt; with product names. Include basic error handling (e.g. show an error message if the API call fails) and a loading state while fetching.”</span></p><p>That’s a long prompt but it’s specific. Let’s assume we give this to an LLM in a chat.</p><p><em>AI’s response (summary of what it might produce):</em><span> The AI would likely output a ProductList component something like:</span></p><pre><code><code>import React, { useState, useEffect } from 'react';

function ProductList() {
  const [products, setProducts] = useState([]);
  const [search, setSearch] = useState('');
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);

  useEffect(() =&gt; {
    async function fetchProducts() {
      try {
        const res = await fetch('/api/products');
        if (!res.ok) throw new Error('Network response was not ok');
        const data = await res.json();
        setProducts(data);
      } catch (err) {
        console.error('Failed to fetch products:', err);
        setError('Failed to load products.');
      } finally {
        setLoading(false);
      }
    }
    fetchProducts();
  }, []);

  const handleSearchChange = (e) =&gt; {
    setSearch(e.target.value);
  };

  // Filter products by search query (case-insensitive)
  const filteredProducts = products.filter(product =&gt;
    product.name.toLowerCase().includes(search.toLowerCase())
  );

  if (loading) return &lt;div&gt;Loading products...&lt;/div&gt;;
  if (error) return &lt;div&gt;{error}&lt;/div&gt;;

  return (
    &lt;div className="product-list"&gt;
      &lt;input 
        type="text" 
        placeholder="Search products" 
        value={search} 
        onChange={handleSearchChange} 
      /&gt;
      &lt;ul&gt;
        {filteredProducts.map(prod =&gt; (
          &lt;li key={prod.id}&gt;{prod.name}&lt;/li&gt;
        ))}
      &lt;/ul&gt;
    &lt;/div&gt;
  );
}

export default ProductList;</code></code></pre><p><em>(Plus possibly some comments in the code explaining each part, if we asked for them.)</em></p><p>Now, let’s analyze this result and consider prompt refinements:</p><ul><li><p><span>Suppose the above code came back, but our project uses a context or a global store for data fetching instead of fetching inside the component. This is something we didn’t specify. The AI made a choice to fetch in useEffect. If we prefer using a custom hook or context, we should have hinted at that. So we could refine our prompt: </span><em>“Actually, in our app we use a custom hook useProducts() that already handles fetching. Please refactor the component to use useProducts hook instead of directly calling fetch.”</em><span> The AI would then adjust, perhaps assuming useProducts returns { products, loading, error } and simplify the component accordingly.</span></p></li><li><p><span>Another refinement: maybe we realize we also want a sort dropdown (which we didn’t mention initially). We can now extend the conversation: </span><em>“Great, now add a dropdown to sort the products by name (A-Z or Z-A). The dropdown should let the user choose ascending or descending, and the list should sort accordingly in addition to the filtering.”</em><span> Because the AI has the context of the existing code, it can insert a sort state and adjust the rendering. We provided a clear new requirement, and it will attempt to fulfill it, likely by adding something like:</span></p></li></ul><pre><code><code>const [sortOrder, setSortOrder] = useState('asc');
// ... a select input for sortOrder ...
// and sort the filteredProducts before rendering:
const sortedProducts = [...filteredProducts].sort((a, b) =&gt; {
  if (sortOrder === 'asc') return a.name.localeCompare(b.name);
  else return b.name.localeCompare(a.name);
});</code></code></pre><ul><li><p>(plus the dropdown UI).</p><p>By iterating like this, feature by feature, we simulate a development cycle with the AI. This is far more effective than trying to prompt for the entire, complex component with all features in one go initially. It reduces mistakes and allows mid-course corrections as requirements become clearer.</p></li><li><p><span>If the AI makes a subtle mistake (say it forgot to make the search filter case-insensitive), we just point that out: </span><em>“Make the search case-insensitive.”</em><span> It will adjust the filter to use lowercase comparison (which in our pseudo-output it already did, but if not it would fix it).</span></p></li></ul><p><span>This example shows that implementing features with AI is all about </span><strong>incremental development and prompt refinement</strong><span>. A Twitter thread might exclaim how someone built a small app by continually prompting an LLM for each part – that’s essentially the approach: build, review, refine, extend. Each prompt is like a commit in your development process.</span></p><p><strong>Additional tips for feature implementation:</strong></p><ul><li><p><em>Let the AI scaffold, then you fill in specifics:</em><span> Sometimes it’s useful to have the AI generate a rough structure, then you tweak it. For example, </span><em>“Generate the skeleton of a Node.js Express route for user registration with validation and error handling.”</em><span> It might produce a generic route with placeholders. You can then fill in the actual validation rules or database calls which are specific to your app. The AI saves you from writing boilerplate, and you handle the custom logic if it’s sensitive.</span></p></li><li><p><em>Ask for edge case handling:</em><span> When generating a feature, you might prompt the AI to think of edge cases: </span><em>“What edge cases should we consider for this feature (and can you handle them in the code)?”</em><span> For instance, in the search example, an edge case might be “what if the products haven’t loaded yet when the user types?” (though our code handles that via loading state) or “what if two products have the same name” (not a big issue but maybe mention it). The AI could mention things like empty result handling, very large lists (maybe needing debounce for search input), etc. This is a way to leverage the AI’s training on common pitfalls.</span></p></li><li><p><em>Documentation-driven development:</em><span> A nifty approach some have taken is writing a docstring or usage example first and having the AI implement the function to match. For example:</span></p></li></ul><pre><code><code>/**
 * Returns the nth Fibonacci number.
 * @param {number} n - The position in Fibonacci sequence (0-indexed).
 * @returns {number} The nth Fibonacci number.
 * 
 * Example: fibonacci(5) -&gt; 5  (sequence: 0,1,1,2,3,5,…)
 */
function fibonacci(n) {
  // ... implementation
}</code></code></pre><ul><li><p>If you write the above comment and function signature, an LLM might fill in the implementation correctly because the comment describes exactly what to do and even gives an example. This technique ensures you clarify the feature in words first (which is a good practice generally), and then the AI uses that as the spec to write the code.</p></li></ul><p><span>Having covered prompting strategies for debugging, refactoring, and new code generation, let’s turn our attention to some </span><strong>common pitfalls and anti-patterns</strong><span> in prompt engineering for coding. Understanding these will help you avoid wasting time on unproductive interactions and quickly adjust when the AI isn’t giving you what you need.</span></p><p><span>Not all prompts are created equal. By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize </span><strong>anti-patterns</strong><span> – common mistakes that lead to poor AI responses. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" width="1024" height="1077" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1077,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2173076,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here are some frequent prompt failures and how to fix them:</p><ul><li><p><strong>Anti-Pattern: The Vague Prompt.</strong><span> This is the classic </span><em>“It doesn’t work, please fix it”</em><span> or </span><em>“Write something that does X”</em><span> without enough detail. We saw an example of this when the question “Why isn’t my function working?” got a useless answer . Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code. The fix is straightforward: </span><strong>add context and specifics</strong><span>. If you find yourself asking a question and the answer feels like a Magic 8-ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected vs actual outcome, etc.). A good practice is to read your prompt and ask, </span><em>“Could this question apply to dozens of different scenarios?”</em><span> If yes, it’s too vague. Make it so specific that it could </span><em>only</em><span> apply to your scenario.</span></p></li><li><p><strong>Anti-Pattern: The Overloaded Prompt.</strong><span> This is the opposite issue: asking the AI to do too many things at once. For instance, </span><em>“Generate a complete Node.js app with authentication, a front-end in React, and deployment scripts.”</em><span> Or even on a smaller scale, </span><em>“Fix these 5 bugs and also add these 3 features in one go.”</em><span> The AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify. The remedy is to </span><strong>split the tasks</strong><span>. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph with multiple “and” in the instructions, consider breaking it into separate prompts or sequential steps.</span></p></li><li><p><strong>Anti-Pattern: Missing the Question.</strong><span> Sometimes users will present a lot of information but never clearly ask a question or specify what they need. For example, dumping a large code snippet and just saying “Here’s my code.” This can confuse the AI – it doesn’t know what you want. Always include a clear ask, such as </span><em>“Identify any bugs in the above code”</em><span>, </span><em>“Explain what this code does”</em><span>, or </span><em>“Complete the TODOs in the code”</em><span>. A prompt should have a </span><em>purpose</em><span>. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows </span><em>why</em><span> you showed it some code. Even a simple addition like, </span><em>“What’s wrong with this code?”</em><span> or </span><em>“Please continue implementing this function.”</em><span> gives it direction.</span></p></li><li><p><strong>Anti-Pattern: Vague Success Criteria.</strong><span> This is a subtle one – sometimes you might ask for an optimization or improvement, but you don’t define what success looks like. For example, </span><em>“Make this function faster.”</em><span> Faster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or </span><em>“make this code cleaner”</em><span> – “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names” etc. The fix: </span><strong>quantify or qualify the improvement</strong><span>. E.g., “optimize this function to run in linear time (current version is quadratic)” or “refactor this to remove global variables and use a class instead.” Basically, </span><em>be explicit about what problem you’re solving with the refactor or feature</em><span>. If you leave it too open, the AI might solve a different problem than the one you care about.</span></p></li><li><p><strong>Anti-Pattern: Ignoring AI’s Clarification or Output.</strong><span> Sometimes the AI might respond with a clarifying question or an assumption. For instance: </span><em>“Are you using React class components or functional components?”</em><span> or </span><em>“I assume the input is a string – please confirm.”</em><span> If you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details. Additionally, if the AI’s output is clearly off (like it misunderstood the question), </span><em>don’t just retry the same prompt verbatim</em><span>. Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation – if a human misunderstood, you’d explain differently; do the same for the AI.</span></p></li><li><p><strong>Anti-Pattern: Varying Style or Inconsistency.</strong><span> If you keep changing how you ask or mixing different formats in one go, the model can get confused. For example, switching between first-person and third-person in instructions, or mixing pseudocode with actual code in a confusing way. Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 vs ES5 syntax), consistently mention it, otherwise the model might suggest one way in one prompt and another way later.</span></p></li><li><p><strong>Anti-Pattern: Vague references like “above code”.</strong><span> When using chat, if you say “the above function” or “the previous output”, be sure the reference is clear. If the conversation is long and you say “refactor the above code”, the AI might lose track or pick the wrong code snippet to refactor. It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.</span></p></li></ul><p><span>Finally, here’s a </span><strong>tactical approach to rewriting prompts</strong><span> when things go wrong:</span></p><ul><li><p><strong>Identify what was missing or incorrect in the AI’s response.</strong><span> Did it solve a different problem? Did it produce an error or a solution that doesn’t fit? For example, maybe you asked for a solution in TypeScript but it gave plain JavaScript. Or it wrote a recursive solution when you explicitly wanted iterative. Pinpoint the discrepancy.</span></p></li><li><p><strong>Add or emphasize that requirement in a new prompt.</strong><span> You might say, </span><em>“The solution should be in TypeScript, not JavaScript. Please include type annotations.”</em><span> Or, </span><em>“I mentioned I wanted an iterative solution – please avoid recursion and use a loop instead.”</em><span> Sometimes it helps to literally use phrases like </span><em>“Note:”</em><span> or </span><em>“Important:”</em><span> in your prompt to highlight key constraints (the model doesn’t have emotions, but it does weigh certain phrasing as indicating importance). For instance: </span><em><span>“</span><strong>Important:</strong><span> Do not use any external libraries for this.”</span></em><span> or </span><em><span>“</span><strong>Note:</strong><span> The code must run in the browser, so no Node-specific APIs.”</span></em><span>.</span></p></li><li><p><strong>Break down the request further if needed.</strong><span> If the AI repeatedly fails on a complex request, try asking for a smaller piece first. Or ask a question that might enlighten the situation: </span><em>“Do you understand what I mean by X?”</em><span> The model might then paraphrase what it thinks you mean, and you can correct it if it’s wrong. This is meta-prompting – discussing the prompt itself – and can sometimes resolve misunderstandings.</span></p></li><li><p><strong>Consider starting fresh if the thread is stuck.</strong><span> Sometimes after multiple tries, the conversation may reach a confused state. It can help to start a new session (or clear the chat history for a moment) and prompt from scratch with a more refined ask that you’ve formulated based on previous failures. The model doesn’t mind repetition, and a fresh context can eliminate any accumulated confusion from prior messages.</span></p></li></ul><p>By being aware of these anti-patterns and their solutions, you’ll become much faster at adjusting your prompts on the fly. Prompt engineering for developers is very much an iterative, feedback-driven process (as any programming task is!). The good news is, you now have a lot of patterns and examples in your toolkit to draw from.</p><p><span>Prompt engineering is a bit of an art and a bit of a science – and as we’ve seen, it’s quickly becoming a must-have skill for developers working with AI code assistants. By crafting clear, context-rich prompts, you essentially </span><em>teach</em><span> the AI what you need, just as you would onboard a human team member or explain a problem to a peer. Throughout this article, we explored how to systematically approach prompts for debugging, refactoring, and feature implementation:</span></p><ul><li><p>We learned to feed the AI the same information you’d give a colleague when asking for help: what the code is supposed to do, how it’s misbehaving, relevant code snippets, and so on – thereby getting much more targeted help .</p></li><li><p>We saw the power of iterating with the AI, whether it’s stepping through a function’s logic line by line, or refining a solution through multiple prompts (like turning a recursive solution into an iterative one, then improving variable names) . Patience and iteration turn the AI into a true pair programmer rather than a one-shot code generator.</p></li><li><p>We utilized role-playing and personas to up-level the responses – treating the AI as a code reviewer, a mentor, or an expert in a certain stack . This often produces more rigorous and explanation-rich outputs, which not only solve the problem but educate us in the process.</p></li><li><p>For refactoring and optimization, we emphasized defining what “good” looks like (be it faster, cleaner, more idiomatic, etc.) , and the AI showed that it can apply known best practices when guided (like parallelizing calls, removing duplication, handling errors properly). It’s like having access to the collective wisdom of countless code reviewers – but you have to ask the right questions to tap into it.</p></li><li><p>We also demonstrated building new features step by step with AI assistance, showing that even complex tasks can be decomposed and tackled one prompt at a time. The AI can scaffold boilerplate, suggest implementations, and even highlight edge cases if prompted – acting as a knowledgeable co-developer who’s always available.</p></li><li><p>Along the way, we identified pitfalls to avoid: keeping prompts neither too vague nor too overloaded, always specifying our intent and constraints, and being ready to adjust when the AI’s output isn’t on target. We cited concrete examples of bad prompts and saw how minor changes (like including an error message or expected output) can dramatically improve the outcome.</p></li></ul><p><span>As you incorporate these techniques into your workflow, you’ll likely find that working with AI becomes more intuitive. You’ll develop a feel for what phrasing gets the best results and how to guide the model when it goes off course. Remember that the AI is a product of its training data – it has seen many examples of code and problem-solving, but it’s </span><em>you</em><span> who provides direction on which of those examples are relevant now. In essence, </span><strong>you set the context, and the AI follows through</strong><span>.</span></p><p><strong>It’s also worth noting that prompt engineering is an evolving practice.</strong><span> The community of developers is constantly discovering new tricks – a clever one-liner prompt or a structured template can suddenly go viral on social media because it unlocks a capability people didn’t realize was there. Stay tuned to those discussions (on Hacker News, Twitter, etc.) because they can inspire your own techniques. But also, don’t be afraid to experiment yourself. Treat the AI as a flexible tool – if you have an idea (“what if I ask it to draw an ASCII diagram of my architecture?”), just try it. You might be surprised at the results, and if it fails, no harm done – you’ve learned something about the model’s limits or needs.</span></p><p><strong>In summary, prompt engineering empowers developers to get more out of AI assistants.</strong><span> It’s the difference between a frustrating experience (“this tool is useless, it gave me nonsense”) and a productive one (“this feels like pair programming with an expert who writes boilerplate for me”). By applying the playbook of strategies we’ve covered – from providing exhaustive context to nudging the AI’s style and thinking – you can turn these code-focused AI tools into true extensions of your development workflow. The end result is not only that you code faster, but often you pick up new insights and patterns along the way (as the AI explains things or suggests alternatives), leveling up your own skillset.</span></p><p><span>As a final takeaway, remember that </span><strong>prompting is an iterative dialogue</strong><span>. Approach it with the same clarity, patience, and thoroughness you’d use when communicating with another engineer. Do that, and you’ll find that AI assistants can significantly amplify your abilities – helping you debug quicker, refactor smarter, and implement features with greater ease. </span></p><p><strong>Happy prompting, and happy coding!</strong></p><p><strong>Further reading:</strong></p><ul><li><p><em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">How to write better prompts for GitHub Copilot</a></em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">. GitHub Blog</a></p></li><li><p><em><a href="https://strapi.io/blog/ChatGPT-Prompt-Engineering-for-Developers#:~:text=1" rel="">ChatGPT Prompt Engineering for Developers: 13 Best Examples</a></em></p></li><li><p><em><a href="https://medium.com/data-science/using-chatgpt-for-efficient-debugging-fc9e065b7856#:~:text=If%20nothing%20comes%20to%20mind%2C,don%E2%80%99t%20be%20afraid%20to%20experiment" rel="">Using ChatGPT for Efficient Debugging</a></em></p></li><li><p><em><a href="https://dev.to/jamesbright/prompt-engineering-for-lazy-programmers-getting-exactly-the-code-you-want-and-even-more-out-of-chatgpt-3plf#:~:text=The%20Trick%3A%20,rewrite%20the%20function%20if%20necessary" rel="">Prompt Engineering for Lazy Programmers: Getting Exactly the Code You Want</a></em></p></li><li><p><em><a href="https://www.linkedin.com/pulse/best-practices-prompting-github-copilot-vs-code-pamela-fox#:~:text=In%20this%20post%2C%20I%27m%20going,provide%20context%20and%20be%20predictable" rel="">Best practices for prompting GitHub Copilot in VS Code</a></em></p></li><li><p><em><a href="https://medium.com/@shamawali/chatgpt-a-new-age-debugger-10-prompts-20ee3e9c63aa#:~:text=2,Debug%20This%20Function%20for%20Me" rel="">ChatGPT: A new-age Debugger, 10 Prompts</a></em></p></li><li><p><em><a href="https://dev.to/techiesdiary/chatgpt-prompts-for-code-review-and-debugging-48j#:~:text=5%20Debug%20Debug%20the%20given,Enter%20your%20code%20here" rel="">ChatGPT Prompts for Code Review and Debugging</a></em></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:624626,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Merges WebRTC Support (731 pts)]]></title>
            <link>https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</link>
            <guid>44182186</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00">https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</a>, See on <a href="https://news.ycombinator.com/item?id=44182186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
avformat/whip:&nbsp;Add&nbsp;WHIP&nbsp;muxer&nbsp;support&nbsp;for&nbsp;subsecond&nbsp;latency&nbsp;streaming</p><p>

0.&nbsp;WHIP&nbsp;Version&nbsp;3.<br>
1.&nbsp;The&nbsp;WHIP&nbsp;muxer&nbsp;has&nbsp;been&nbsp;renamed&nbsp;and&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;improved&nbsp;logging&nbsp;context&nbsp;and&nbsp;error&nbsp;messages&nbsp;for&nbsp;SSL,&nbsp;DTLS,&nbsp;and&nbsp;RTC.<br>
2.&nbsp;Magic&nbsp;numbers&nbsp;have&nbsp;been&nbsp;replaced&nbsp;with&nbsp;macros&nbsp;and&nbsp;extracted&nbsp;to&nbsp;functions,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;log&nbsp;levels&nbsp;have&nbsp;been&nbsp;altered&nbsp;for&nbsp;better&nbsp;clarity.<br>
3.&nbsp;DTLS&nbsp;curve&nbsp;list&nbsp;has&nbsp;been&nbsp;updated,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;SRTP&nbsp;profile&nbsp;names&nbsp;have&nbsp;been&nbsp;refined&nbsp;for&nbsp;FFmpeg&nbsp;and&nbsp;OpenSSL.<br>
4.&nbsp;ICE&nbsp;STUN&nbsp;magic&nbsp;number&nbsp;has&nbsp;been&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;RTP&nbsp;payload&nbsp;types&nbsp;have&nbsp;been&nbsp;updated&nbsp;based&nbsp;on&nbsp;Chrome's&nbsp;definition.<br>
5.&nbsp;Fixed&nbsp;frame&nbsp;size&nbsp;has&nbsp;been&nbsp;refined&nbsp;to&nbsp;rtc-&gt;audio_par-&gt;frame_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;h264_mp4toannexb&nbsp;is&nbsp;now&nbsp;used&nbsp;to&nbsp;convert&nbsp;MP4/ISOM&nbsp;to&nbsp;annexb.<br>
6.&nbsp;OPUS&nbsp;timestamp&nbsp;issue&nbsp;has&nbsp;been&nbsp;addressed,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;marker&nbsp;setting&nbsp;has&nbsp;been&nbsp;corrected&nbsp;after&nbsp;utilizing&nbsp;BSF.<br>
7.&nbsp;DTLS&nbsp;handshake&nbsp;and&nbsp;ICE&nbsp;handling&nbsp;have&nbsp;been&nbsp;optimized&nbsp;for&nbsp;improved&nbsp;performance,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;single&nbsp;handshake&nbsp;timeout&nbsp;and&nbsp;server&nbsp;role&nbsp;to&nbsp;prevent&nbsp;ARQ.<br>
8.&nbsp;Consolidated&nbsp;ICE&nbsp;request/response&nbsp;handling&nbsp;and&nbsp;DTLS&nbsp;handshake&nbsp;into&nbsp;a&nbsp;single&nbsp;function,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;fixed&nbsp;OpenSSL&nbsp;build&nbsp;errors&nbsp;to&nbsp;work&nbsp;with&nbsp;Pion.<br>
9.&nbsp;Merge&nbsp;TLS&nbsp;&amp;&nbsp;DTLS&nbsp;implementation,&nbsp;shared&nbsp;BIO&nbsp;callbacks,&nbsp;read,&nbsp;write,<br>
&nbsp;&nbsp;&nbsp;&nbsp;print_ssl_error,&nbsp;openssl_init_ca_key_cert,<br>
&nbsp;&nbsp;&nbsp;&nbsp;init_bio_method&nbsp;function&nbsp;and&nbsp;shared&nbsp;same&nbsp;data&nbsp;structure<br>
10.&nbsp;Modify&nbsp;configure&nbsp;that&nbsp;whip&nbsp;is&nbsp;enabled&nbsp;only&nbsp;dtls&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;enabled(just&nbsp;support&nbsp;openssl&nbsp;for&nbsp;now)&nbsp;to&nbsp;fix&nbsp;build&nbsp;error</p><p>

<span>Co-authored-by: winlin &lt;winlinvip@gmail.com&gt;</span><br>
<span>Co-authored-by: yangrtc &lt;yangrtc@aliyun.com&gt;</span><br>
<span>Co-authored-by: cloudwebrtc &lt;duanweiwei1982@gmail.com&gt;</span><br>
<span>Co-authored-by: Haibo Chen &lt;495810242@qq.com&gt;</span><br>
<span>Co-authored-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span><br>
<span>Co-authored-by: Jun Zhao &lt;barryjzhao@tencent.com&gt;</span><br>
<span>Signed-off-by: Jack Lau &lt;jacklau1222@qq.com&gt;</span><br>
<span>Signed-off-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we reduced the impact of zombie clients (110 pts)]]></title>
            <link>https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/</link>
            <guid>44182184</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/">https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/</a>, See on <a href="https://news.ycombinator.com/item?id=44182184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Every night, right around midnight (mainly <a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">UTC</a>), a horde of zombies wakes up and clamors for … digital certificates!</p>
<p>The zombies in question are abandoned or misconfigured Internet servers and ACME clients that have been set to request certificates from Let’s Encrypt. As our certificates <a href="https://letsencrypt.org/2015/11/09/why-90-days/">last for at most 90 days</a>, these zombie clients’ software knows that their certificates are out-of-date and need to be replaced. What they don’t realize is that their quest for new certificates is doomed! These devices are cursed to seek certificates again and again, never receiving them.</p>
<p>But they do use up a lot of certificate authority resources in the process.</p>
<h3 id="the-zombie-client-problem">The Zombie Client Problem</h3>
<p>Unlike a human being, software doesn’t give up in frustration, or try to modify its approach, when it repeatedly fails at the same task. Our emphasis on automation means that the vast majority of Let’s Encrypt certificate renewals are performed by automated software. This is great when those renewals succeed, but it also means that forgotten clients and devices can continue requesting renewals unsuccessfully for months, or even years.</p>
<p>How might that happen? Most often, it happens when a device no longer has a domain name pointed to it. The device itself doesn’t know that this has changed, so it treats renewal failures as transient even though they are actually permanent. For instance:</p>
<ul>
<li>An organization may have allowed a domain name registration to lapse because it is no longer needed, but its servers are still configured to request certs for it.</li>
<li>Or, a home user stopped using a particular dynamic-DNS domain with a network-attached storage device, but is still using that device at home. The device doesn’t realize that the user no longer expects to use the name, so it keeps requesting certs for it.</li>
<li>Or, a web hosting or CDN customer migrated to a different service provider, but never informed the old service provider. The old service provider’s servers keep requesting certs unsuccessfully. If the customer was in a free service tier, there might not be invoices or charges reminding the customer to cancel the service.</li>
<li>Or any number of other, subtler changes in a subscriber’s infrastructure, such as changing a firewall rule or some webserver configuration.</li>
</ul>
<p>At the scale of Let’s Encrypt, which now covers <a href="https://letsencrypt.org/stats/">hundreds of millions of names</a>, scenarios like these have become common, and their impact has become substantial. In 2024, we noticed that about half of all certificate requests to the Let’s Encrypt ACME API came from about a million accounts that never successfully complete any validations. Many of these had completed validations and issued certificates sometime in the past, but nowadays every single one of their validation attempts fails, and they show no signs that this will change anytime soon.</p>
<p>Unfortunately, trying to validate those futile requests still uses resources. Our CA software has to generate challenges, reach out and attempt to validate them over the Internet, detect and report failures, and record all of the associated information in our databases and audit logs. And over time, we’ve seen more and more recurring failures: accounts that always fail their issuance requests have been growing at around 18% per year.</p>
<p>In January, we mentioned that we had been addressing the zombie client problem <a href="https://letsencrypt.org/2025/01/30/scaling-rate-limits/">through our rate limit system</a>. This post provides more detail on that progress.&nbsp;</p>
<h3 id="our-rate-limit-philosophy">Our Rate Limit Philosophy</h3>
<p>If you’ve used Let’s Encrypt as a subscriber, you may have run into one of our <a href="https://letsencrypt.org/docs/rate-limits/">rate limits</a> at some point, maybe during your initial setup process. We have eight different kinds of rate limits in place now; as our January post describes, they’ve become more algorithmically sophisticated and grown to address a wider range of problems. A key principle for Let’s Encrypt is that our rate limiting is not a punishment. We don’t think of rate limits as a way of retaliating against a client for misbehavior. Rate limits are simply a tool to maximize the efficient use of our limited resources and prevent people and programs from using up those resources for no constructive purpose.</p>
<p>We’ve consistently tried to design our rate limit mechanisms in line with that philosophy. So if a misconfiguration or misunderstanding has caused excessive requests in the past, we’re still happy to welcome the user in question back and start issuing them certificates again—once the problem has been addressed. We want the rate limits to put a brake on wasteful use of our systems, but not to frustrate users who are actively trying to make Let’s Encrypt work for them.</p>
<p>In addition, we’ve always implemented our rate limits to err on the side of permissiveness. For example, if the Redis instances where rate limits are tracked have an outage or lose data, the system is designed to permit more issuance rather than less issuance as a result.</p>
<p>We wanted to create additional limits that would target zombie clients, but in a correspondingly non-punitive way that would avoid any disruption to valid issuance, and welcome subscribers back quickly if they happened to notice and fix a long-time problem with their setups.</p>
<h3 id="our-zombie-related-rate-limits-and-their-impact">Our Zombie-Related Rate Limits and Their Impact</h3>
<p>In planning a new zombie-specific response, we decided on a “pausing” approach, which can temporarily limit an account’s ability to proceed with certificate requests. The core idea is that, if a particular account consistently fails to complete validation for a particular hostname, we’ll pause that account-hostname pair. The pause means that any new order requests from that account for that hostname will be rejected immediately, before we get to the resource-intensive validation phase.</p>
<p>This approach is more finely targeted than pausing an entire account. Pausing account-hostname pairs means that your ability to issue certs for a specific name could be paused due to repeated failures, but you can still get all of your other certs like normal. So a large hosting provider doesn’t have to fear that its certificate issuance on behalf of one customer will be affected by renewal failures related to a problem with a different customer’s domain name. The account-specificity of the pause, in turn, means that validation failures from one subscriber or device won’t prevent a different subscriber or device from attempting to validate the same name, as long as the devices in question don’t share a single Let’s Encrypt account.</p>
<p>In September 2024, we began applying our zombie rate limits manually by pausing about 21,000 of the most recurrently-failing account-hostname pairs, those which were consistently repeating the same failed requests many times per day, every day. After implementing that first round of pauses, we immediately saw a significant impact on our failed request rates. As we announced at that time, we also began <a href="https://community.letsencrypt.org/t/automatic-pausing-of-zombie-clients/229642">using a formula to automatically pause other zombie client account-hostname pairs from December 2024 onward</a>. The associated new rate limit is called “<a href="https://letsencrypt.org/docs/rate-limits/#consecutive-authorization-failures-per-hostname-per-account">Consecutive Authorization Failures per Hostname Per Account</a>” (and is independent of the existing “Authorization Failures per Hostname Per Account” limit, which resets every hour).</p>
<p>This formula relates to the frequency of successive failed issuance requests for the same domain name by the same Let’s Encrypt account. It applies only to failures that happen again and again, with no successful issuances at all in between: a single successful validation immediately resets the rate limit all the way to zero. Like all of our rate limits, this is not a punitive measure but is simply intended to reduce the waste of resources. So, we decided to set the thresholds rather high in the expectation that we would catch only the most disruptive zombie clients, and ultimately only those clients that were extremely unlikely to succeed in the future based on their substantial history of failed requests. We don’t hurry to block requesters as zombies: according to our current formula, client software following the default established by EFF’s <a href="https://certbot.eff.org/">Certbot</a> (two renewal attempts per day) would be paused as a zombie only after about ten years of constant failures. More aggressive failed issuance attempts will get a client paused sooner, but clients will generally have to fail hundreds or thousands of attempts in a row before they are paused.</p>
<p>Most subscribers using mainstream client applications with default configurations will never encounter this rate limit, even if they forget to deactivate renewal attempts for domains that are no longer pointed at their servers. As described below, our current limit is already providing noticeable benefits with minimal disruption, and we’re likely to tighten it a bit in the near future, so it will trigger after somewhat fewer consecutive failures.</p>
<h3 id="self-service-unpausing">Self-Service Unpausing</h3>
<p>A key feature in our zombie issuance pausing mechanism is self-service unpausing. Whenever an account-hostname pair is paused, any new certificate requests for that hostname submitted by that account are immediately rejected. But this means that the “one successful validation immediately resets the rate limit counter” feature can no longer come into effect: once they’re paused, they can’t even attempt validation anymore.</p>
<p>So every rejection comes with an error message explaining what has happened and a custom link that can be used to immediately unpause that account-hostname pair and remove any other pauses on the same account at the same time. The point of this is that subscribers who notice at some point that issuance is failing and want to intervene to get it working again have a straightforward option to let Let’s Encrypt know that they’re aware of the recurring failures and are still planning to use a particular account. As soon as subscribers notify us via the self-service link, they’ll be able to issue certificates again.</p>
<p>Currently, the user interface for an affected subscriber looks like this:</p>
<p><img src="https://letsencrypt.org/images/blog/blog-2025-06-04--image1.jpg" alt="Let's Encrypt unpause interface"></p><p>This link would be provided via an ACME error message in response to any request that was blocked due to a pause account-hostname pair.</p>
<p>As it’s turned out, the unpause option shown above has only been used by about 3% of affected accounts! This goes to show that most of the zombies we’ve paused were, in fact, well and truly forgotten about.</p>
<p>However, the unpause feature is there for whenever it’s needed, and there may be cases when it will become more important. A very large integration could trigger the zombie-related rate limits if a newly-introduced software bug causes what looks like a very high volume of zombie requests in a very short time. In that case, once that bug has been noticed and fixed, an integrator may need to unpause its issuance on behalf of lots of customers at once. Our unpause feature permits unpausing 50,000 domain names on a single account at a time, so even the largest integrators can get themselves unpaused expeditiously in this situation.</p>
<h3 id="conclusion">Conclusion</h3>
<p>We’ve been very happy with the results of our zombie mitigation measures, and, as far as we can tell, there’s been almost no impact for subscribers! Our statistics indicate that we’ve managed to reduce the load on our infrastructure while causing no detectable harm or inconvenience to subscribers’ valid issuance requests.</p>
<p>Since implementing the manual pauses in September and the automated pauses in December, we’ve seen:</p>
<ul>
<li>Over 100,000 account-hostname pairs have been paused for excessive failures.</li>
<li>We received zero (!) associated complaints or support requests.</li>
<li>About 3,200 people manually unpaused issuance.</li>
<li>Failed certificate orders fell by about 30% so far, and should continue to fall over time as we fine-tune the rate limit formula and catch more zombie clients.</li>
</ul>
<p>The new rate limit and the self-service unpause system are also ready to deal with circumstances that might produce more zombie clients in the future. For instance, we’ve announced that <a href="https://letsencrypt.org/2025/01/22/ending-expiration-emails/">we’re going to be discontinuing renewal reminder emails</a> soon. If some subscribers overlook failed renewals in the future, we might see more paused clients that result from unintentional renewal failures. We think taking advantage of the existing self-service unpause feature will be straightforward in that case. But it’s much better to notice problems and get them fixed up front, so please remember to <a href="https://letsencrypt.org/docs/monitoring-options/">set up your own monitoring</a> to avoid unnoticed renewal failures in the future.</p>
<p>If you’re a subscriber who’s had occasion to use the self-service unpause feature, we’d love your feedback on the <a href="https://community.letsencrypt.org/">Community Forum</a> about your experience using the feature and the circumstances that surrounded your account’s getting paused.</p>
<p>Also, if you’re a Let’s Encrypt client developer, please remember to make renewal requests at a random time (not precisely at midnight) so that the load on our infrastructure is smoothed out. You can also reduce the impact of zombie renewals by repeating failed requests somewhat less frequently over time (a “back-off” strategy), especially if the failure reason makes it look like a domain name may no longer be in use at all.</p>

      

    </div></div>]]></description>
        </item>
    </channel>
</rss>