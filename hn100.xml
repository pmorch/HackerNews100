<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Sep 2023 14:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[All the ways to capture changes in Postgres (110 pts)]]></title>
            <link>https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/</link>
            <guid>37610899</guid>
            <pubDate>Fri, 22 Sep 2023 12:06:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/">https://blog.sequin.io/all-the-ways-to-capture-changes-in-postgres/</a>, See on <a href="https://news.ycombinator.com/item?id=37610899">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <!--kg-card-begin: markdown--><p>Working with data at rest is where Postgres shines. But what about when you need data in motion? What about when you need to trigger a workflow based on changes to a table? Or you need to stream the data in Postgres to another data store, system, or service in real-time?</p>
<p>Fortunately, Postgres comes with a lot of options to make this happen. In this post, I’ll lay them all out. I’ll also give you an idea of which are easy to do, which are more robust, and how to make the right choice for you.</p>
<h2 id="listennotify">Listen/Notify</h2>
<p>Perhaps the simplest approach is to use Postgres' interprocess communication feature, Listen/Notify. Listen/Notify is an implementation of the publish-subscribe pattern.</p>
<p>With Listen/Notify, a Postgres session (or connection) can "listen" to a particular channel for notifications. Activity in the database or other sessions can "notify" that channel. Whenever a notification is sent to a channel, all sessions listening to that channel receive the notification instantly.</p>
<p>You can see Listen/Notify for yourself by opening two <code>psql</code> sessions.</p>
<p>In session 1, you can setup your listener:</p>
<pre><code>&gt; listen my_channel;
LISTEN
</code></pre>
<p>And in session 2, you can publish to that channel with a message:</p>
<pre><code>&gt; notify my_channel, 'hey there!';
NOTIFY
&gt; notify my_channel, 'is this thing on?';
NOTIFY
</code></pre>
<p>While the listener process received the message right away, <code>psql</code> won't print the message automatically. To get it to print out the messages it's received so far, you just need to run any query. For example, you can just send an empty query like this:</p>
<pre><code>&gt; listen my_channel;
LISTEN
&gt; ;
Asynchronous notification "my_channel" with payload "hey there!" received from server process with PID 80019.
Asynchronous notification "my_channel" with payload "is this thing on?" received from server process with PID 80019.
</code></pre>
<p>(Naturally, this isn't how the Postgres client library in your preferred programming language will work. Libraries will deliver messages to your subscriber immediately without requiring a query.)</p>
<p>To use Listen/Notify to capture changes, you can set up a trigger. For example, here's an <code>after</code> trigger that sends along the payload of the record that changed as JSON via Notify:</p>
<pre><code>create or replace function notify_trigger() returns trigger as $$
declare
  payload json;
begin
  payload := json_build_object('table', TG_TABLE_NAME, 'id', NEW.id, 'action', TG_OP);
  perform pg_notify('table_changes', payload::text);
  return new;
end;
$$ language plpgsql;

create trigger my_trigger
after insert or update or delete on my_table
for each row execute function notify_trigger();
</code></pre>
<h3 id="downsides">Downsides</h3>
<p>Listen/Notify is simple and powerful, but has some notable downsides.</p>
<p>First, as a pub-sub mechanism, it has "at most once" delivery semantics. Notifications are transient; a listener needs to be listening to a channel when notifications are published. When a listener subscribes to a channel, it will only receive notifications from that moment forward. This also means that if there are network issues that cause a listening session to disconnect even briefly, it won't receive the notification.</p>
<p>Second, the payload size limit is 8000 bytes. If the message exceeds this size, the <code>notify</code> command will fail. <sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>As such, Listen/Notify is solid for basic change detection needs, but you'll probably find it does not serve more sophisticated needs well. However, it can complement other strategies (like "poll the table") nicely.</p>
<h2 id="poll-the-table">Poll the table</h2>
<p>The simplest <em>robust</em> way to capture changes is to poll the table directly. Here, you need each table to have an <code>updated_at</code> column or similar that updates whenever the row updates. (You can use a trigger for this.) A combination of <code>updated_at</code> <a href="https://blog.sequin.io/whats-changed-in-your-api/">and <code>id</code></a> serve as your cursor. In this setup, your application logic that polls the table handles storing and maintaining the cursor.</p>
<p>In addition to polling the table, you can use a Notify subscription to inform your application that a record has been inserted or modified. Postgres' notifications are ephemeral, so this should only serve as an optimization on top of polling.</p>
<h3 id="downsides">Downsides</h3>
<p>This approach has two downsides.</p>
<p>The first is that you can't detect when a row is deleted. There's no way to "see" the missing row in the table.</p>
<p>One remediation is to have a Postgres trigger fire on deletes, and store the <code>id</code> (and whatever other columns you want) in a separate table: e.g. <code>deleted_contacts</code>. Then, your application can poll that table to discover deletes instead.</p>
<p>The second downside is that you don't get diffs. You know this record was updated since you last polled the table, but you don't know <em>what</em> was updated on the record.</p>
<p>Maybe deletes aren't a big deal for your use case or you don't care about diffs. If so, polling the table is a reasonable and simple solution for tracking changes.</p>
<h2 id="replication-wal">Replication (WAL)</h2>
<p>Postgres supports streaming replication to other Postgres databases. In streaming replication, Postgres sends the WAL stream over a network connection from the primary to a replica. The standby servers pull these WAL records and replay them to keep their database in sync with the primary database.</p>
<p>Streaming replication was built for streaming changes to other Postgres servers. But you can use it to capture changes for your application too.</p>
<p>You first create a replication slot, like this:</p>
<pre><code>select * from
pg_create_logical_replication_slot('&lt;your_slot_name&gt;', '&lt;output_plugin&gt;');
</code></pre>
<p><code>output_plugin</code> is a parameter which specifies which plugin Postgres should use to decode WAL changes. Postgres comes with a few built-in plugins. <code>pgoutput</code> is the default. It formats the output in the binary expected by client servers. <code>test_decoding</code> is a simple output plugin that provides human-readable output of the changes to the WAL.</p>
<p>The most popular output plugin not built-in to Postgres is <code>wal2json</code>. It does what it says on the tin. JSON will be a lot easier for you to consume from an application than Postgres' binary format.</p>
<p>After creating your replication slot, you can start it and consume from it. Working with replication slots uses a different part of the Postgres protocol than standard queries. But many client libraries have functions that help you work with replication slots.</p>
<p>For example, this is how you consume WAL messages in the <code>psycopg2</code> library:</p>
<pre><code>cursor.start_replication(slot_name='your_slot_name', decode=True)
cursor.consume_stream(lambda msg: acknowledge_to_server(cursor, msg))

def acknowledge_to_server(cursor, msg):
    # Process the message (msg) here
    # ...
    # Acknowledge the message
    cursor.send_feedback(flush_lsn=msg.wal_end)
</code></pre>
<p>Note that the client is responsible for ack'ing WAL messages that it has received. So the replication slot behaves like event buses such as SQS.</p>
<p>Instead of consuming from the WAL directly, you can use tools like Debezium to do this for you. Debezium will consume the WAL from Postgres and stream those changes to a variety of sinks, including Kafka or NATS.</p>
<h3 id="downsides">Downsides</h3>
<p>Using Postgres' replication facilities to capture changes is a robust solution. The biggest downside is complexity. Replication slots and the replication protocol are less familiar to most developers than the "standard" parts (i.e. tables and queries).</p>
<p>Along with this complexity is a decrease in clarity. If something with replication breaks or if there's a lag or things aren't working as expected, it can be a bit trickier to debug than the other solutions outlined here.</p>
<p>Another aspect worth mentioning is that replication slots may require tweaking <code>postgresql.conf</code>. For example, you may need to tweak parameters like <code>max_wal_senders</code> and <code>max_replication_slots</code>. So you'll need total access to the database to implement this solution.</p>
<h2 id="capture-changes-in-an-audit-table">Capture changes in an audit table</h2>
<p>In this approach, you set up a separate table for logging changes, e.g. <code>changelog</code>. That table contains column related to the record's modification, such as:</p>
<ul>
<li><code>action</code>: Was this an <code>insert</code>, <code>update</code>, or <code>delete</code>?</li>
<li><code>old</code>: A jsonb of the record before the mutation. Blank for inserts.</li>
<li><code>values</code>: A jsonb of the change fields. Blank for deletes.</li>
<li><code>inserted_at</code>: Time the change occurred.</li>
</ul>
<p>To set this up, you need to create a trigger function that inserts into this table every time a change occurs. Then, you need to create triggers on all the tables you care about to invoke that trigger function.</p>
<p>Here's an example of what that trigger function might look like:</p>
<pre><code>create or replace function changelog_trigger() returns trigger as $$
declare
  action text;
  table_name text;
  transaction_id bigint;
  timestamp timestamp;
  old_data jsonb;
  new_data jsonb;
begin
  action := lower(TG_OP::text);
  table_name := TG_TABLE_NAME::text;
  transaction_id := txid_current();
  timestamp := current_timestamp;

  if TG_OP = 'DELETE' then
    old_data := to_jsonb(OLD.*);
  elseif TG_OP = 'INSERT' then
    new_data := to_jsonb(NEW.*);
  elseif TG_OP = 'UPDATE' then
    old_data := to_jsonb(OLD.*);
    new_data := to_jsonb(NEW.*);
  end if;

  insert into changelog (action, table_name, transaction_id, timestamp, old_data, new_data) 
  values (action, table_name, transaction_id, timestamp, old_data, new_data);

  return null;
end;
$$ language plpgsql;
</code></pre>
<p>After setting up a way to capture changes, you need to figure out how to consume them.</p>
<p>There's a lot of different ways you can do this. One way is to treat the <code>changelog</code> as a queue. Your application workers can pull changes from this table. You'll probably want to ensure that changes are processed ~exactly once. You can use the <code>for update skip locked</code> feature in Postgres to do this. For example, your workers can open a transaction and grab a chunk of <code>changelog</code> entries:</p>
<pre><code>begin;

select * 
from changelog 
order by timestamp 
limit 100 
for update skip locked;
</code></pre>
<p>Now, other workers running that query will not receive this "locked" block of rows. After your worker processes the records, it can delete them:</p>
<pre><code>delete from changelog 
where id in (list_of_processed_record_ids);

commit;
</code></pre>
<h3 id="downsides">Downsides</h3>
<p>This approach is similar to using a replication slot, but more manual. The trigger function and table design I've outlined might work to start. But you'd likely need to make tweaks before deploying at scale in production. <sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>The advantage over replication slots is that it's all "standard" Postgres. Instead of an opaque replication slot, you have an easy to query Postgres table. And you don't need access to <code>postgresql.conf</code> to make this work.</p>
<h2 id="foreign-data-wrappers">Foreign data wrappers</h2>
<p>Foreign data wrappers (FDWs) are a Postgres feature that allow you to both read from and write to external data sources from your Postgres database.</p>
<p>The most notable and widely supported extension built on FDWs is <code>postgres_fdw</code>. With <code>postgres_fdw</code>, you can connect two Postgres databases and create something like a <a href="https://www.postgresql.org/docs/current/sql-createview.html?ref=blog.sequin.io">view</a> in one Postgres database that references a table in another Postgres database. Under the hood, you're turning one Postgres database into a client and the other into a server. When you make queries against foreign tables, the client database sends the queries to the server database via Postgres' <a href="https://www.postgresql.org/docs/current/protocol-flow.html?ref=blog.sequin.io">wire protocol</a>.</p>
<p>Using FDWs to capture changes is an unusual strategy. I wouldn't recommend it outside very specific situations.</p>
<p>One situation where FDWs could make sense is if you're capturing changes in one Postgres database in order to write them to another Postgres database. Perhaps you use one database for accounting and another for your application. You can skip the intermediary change capture steps and use <code>postgres_fdw</code> to go from database to database.</p>
<p>Here's an example trigger that ensures the status for a given account (identified by <code>email</code>) is in-sync across two databases. This assumes the foreign table has already been declared as <code>foreign_app_database</code>:</p>
<pre><code>create or replace function cancel_subscription()
  returns trigger as $$
declare
  account_status text;
begin
  if (new.status = 'cancelled' or new.status = 'suspended') then
    account_status := 'cancelled';

    update foreign_app_database.account
    set status = account_status
    where email = new.email;
  end if;

  return new;
end;
$$ language plpgsql;
</code></pre>
<p>In addition to <code>postgres_fdw</code>, you can create and load your own foreign data wrappers into your Postgres database.</p>
<p>That means you could create a foreign data wrapper that posts changes to an internal API. Unlike the other change detection strategies in this list, because you'd write to the API inside your commit, your API would have the ability to reject the change and roll back the commit.</p>
<h3 id="downsides">Downsides</h3>
<p>Foreign data wrappers are a fun and powerful Postgres feature. But they'll rarely be your best option for capturing changes. You're probably not trying to replicate changes from one Postgres database to another. And while writing your own foreign data wrapper from scratch <a href="https://github.com/supabase/wrappers?ref=blog.sequin.io">has gotten easier</a>, writing your own FDW is probably the biggest lift in this list for capturing changes.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There are lots of options for capturing changes in Postgres. Depending on your use case, some options are clearly better than others. In sum:</p>
<ul>
<li>Listen/Notify is great for non-critical event capture, prototyping, or optimizing polling.</li>
<li>Polling for changes is a fine, straightforward solution for simple use cases.</li>
<li>Replication is probably your best bet for a robust solution. If that’s too difficult or opaque, then perhaps the audit table is a good middle-ground.</li>
<li>Finally, foreign data wrappers solve a need you’re unlikely to have.</li>
</ul>
<p>We examined all of these options for our own change capture requirements, and unfortunately none of them met our complex (and niche) needs. So, we ended up needing to build a Postgres proxy 😅 You can <a href="https://blog.sequin.io/we-had-no-choice-but-to-build-a-postgres-proxy/">read more about that here</a>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>Note the payload size includes the channel name, which like all Postgres identifiers can be up to 64 bytes in size. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>One example issue that comes to mind: should there be a timeout for how long workers can have changes checked out? <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Age of the Grift Shift (164 pts)]]></title>
            <link>https://tante.cc/2023/09/21/the-age-of-the-grift-shift/</link>
            <guid>37610238</guid>
            <pubDate>Fri, 22 Sep 2023 10:25:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tante.cc/2023/09/21/the-age-of-the-grift-shift/">https://tante.cc/2023/09/21/the-age-of-the-grift-shift/</a>, See on <a href="https://news.ycombinator.com/item?id=37610238">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-4631">

	<div>

			<p>For a book proposal I am currently working on (German, no proposal isn’t done yet because I keep reworking stuff, my agent hates me) I am thinking a lot about late stage capitalism and technologies, about how the kinda terminal economic system shapes the technologies it brings forward etc. And there are of course a lot of ways one can tackle that topic: You can do very abstract theoretical approaches or rather argue from examples or mix and match etc.</p>



<p>So a few weeks ago while going through my “to read pile” I stumbled on <a href="https://www.institutionalinvestor.com/article/2c4fad0w6irk838pca3gg/portfolio/money-is-pouring-into-ai-skeptics-say-its-a-grift-shift">an article</a> that itself isn’t super relevant or special but it gave me a very useful phrase to chew on for the last weeks: The “grift shift”. In that article the term is used in reference to a whole bunch of VC firms who – after crypto/web3 had imploded – seamlessly rebranded themselves as “AI” firms. And if you’ve heard me speak in the last few months you know me often opening with jokingly referencing the same dynamic on Linkedin where the crypto influencers turned into Metaverse influencers and now have turned into “AI” influencers.</p>



<p>Very obviously I agree with the analysis: There is a massive amount of people and institutions jumping from one grift to the next to the next. But that’s not necessarily new: Especially when it comes to investments it’s just natural to follow the hype, to try to ride the wave wherever it leads you to get the next payday. </p>



<p>But I think there’s more going on here. Because it’s not just investors or their little helpers from the consulting firms: The whole tech discourse has shifted a bit in my subjective observation. So let’s try to define the “Grift Shift” in a more structural way. </p>



<blockquote>
<p>The Grift Shift is a new paradigm of debating technologies within a society that is based a lot less on the actual realistic use cases or properties of a certain technology but a surface level fascination with technologies but even more their narratives of future deliverance. Within the Grift Shift paradigm the topics and technologies addressed are mere material for public personalities to continuously claim expertise and “thought leadership” in every cycle of the shift regardless of what specific technologies are being talked about.</p>
</blockquote>



<p>Everybody can probably name a few people who have embraced that paradigm fully. People who effortlessly shift from “web3 is the future” to “I will explain to you why ‘AI’ will replace you”, people who get fame by talking about self driving cars and jump to superconductors the next week depending on whatever is sticky in the news. Again, it’s not like we never had these people but their relevance, their position of power, their influence has changed.</p>



<p>So I kept thinking about why that is. Like: Are these people just willing or clueless PR people for whatever capital wants to push on people (usually through their employers or sometimes even governments)? But while that is a function they have, I think that’s not the “why”. And I think the “why” is to a certain degree YouTube’s fault.</p>



<h3>Content</h3>



<p>A few years ago YouTube (of course others joined in and followed but I think YouTube was a leading force here) established the term “content”. It was no longer about the actual qualities of the medium, not about videos or music or stories or essays etc. Everything one made was just <em>content</em>.</p>



<p>This was an interesting development that underlined how strong our culture is already shaped by technological terminology: “Content” is an absolute abstraction, shedding everything specific about the objects it references. All meaning (and everything that goes beyond the most literal output, the most simplistic view on a created artifact) is stripped. No longer are people writers or filmmakers or musicians or whatever: They are “content creators”. Lumped together in spite of having radically different processes, subcultures, communities, values, traditions, etc. </p>



<p>“Content” is how programmers think about media when designing software systems to manage it. WordPress, the system I use to write and publish this website, is a so-called “content management system”. Which is technically correct but strips all the process of writing, editing, formatting from the narrative. But that is how you build a system to manage “articles”. You don’t care too much about how articles emerge from experience, thinking and sometimes pure accident. You care about the words and paragraphs. The long string of characters you need to store in a database. And that string could also be the binary representation of an image or a movie or whatnot. Who gives a shit?</p>



<p>“Content” is less about the creation. It’s about delivery and managing audience attention. YouTube’s algorithms reward certain behaviors (and when we look at how YouTube works, that seems to be mostly “be a right wing shithead yelling in the microphone about trans people”) and certain formal structures (as in length, structure of the preview image, title) but they don’t really care about anything. Just that whenever a person is on the site they get something to look at. And something else afterwards. And something else afterwards. It doesn’t matter. Just don’t leave or at least let the tab open so whe can autoplay some more ads.</p>



<p>There was a somewhat big scandal recently about a popular YouTube tech show doing shoddy work, abusing their workers and after trying to ignore it one excuse was the murderous schedule: “We can’t do good work because we need to put out a video every day.” Whether that explanation holds any water (in my experience people who abuse their workers and coworkers do that regardless of the stress) it’s telling how putting anything out every day has priority over what it is or means. “I need to say something every day, but I don’t have something to say every day” is a shitty position that pretty mush crushes a human soul after a while but the millions that channel made probably help to dry some tears. </p>



<p>Why am I talking about YouTube when we were talking about the grift shift? It’s not because everyone on YouTube is a grifter or that it is worse than many other platforms (TBH I feel like Linkedin is a bit worse and let’s not even talk about Twitter), it is because YouTube’s way of thinking about “content” has reshaped our tech discourse structurally. Which is a bit of the problem given how especially in western countries we basically outsourced “Future” to tech instead of also thinking about maybe for example political visions.</p>



<p>I think the Grift Shift is a symptom of the shift from actual media to content.</p>



<h3>Why so unserious?</h3>



<p>Our lives, our economies everything is increasingly shaped by digital technologies. Even activities within the physical realm require digital tools to participate. So talking about technology, explaining new technologies to the public who needs to form an opinion on them is gravely important. </p>



<p>But we’ve seemingly lost a bit of our ability to be  serious. Now I don’t mean that talking about technology (or politics or social issues for that matter) cannot be entertaining or have entertaining aspects to it. But we are not taking the subject matter at hand serious. </p>



<p>Technological development, potential innovations are published through advertising / PR into the world and as soon as a bit of momentum forms they get reduced to materials to generate content. Because there needs to be a video every day. And everyone else says that <s>crypto</s> <s>metaverse</s> AI is the future and you need to ride that wave. Click subscribe please.</p>



<p>The Grift Shift keeps going because we are not taking technological developments seriously as political forces. Which they are or are at least very aligned with. You can’t talk about “AI” and the wonders of whatever the newest paper or startup promises without looking at environmental impacts, about deskilling, about how these tools affect the real economy that we’re relying on to get people the money to pay for food and housing (granted: I don’t think that that basic idea is kinda dumb because every human being has a right to food, shelter, healthcare, community and participation, has a right to have their needs met in order for them to thrive but that is a whole different article I guess).</p>



<p>And that is what is so sad about it. The grift shift and all the people and organizations who embody it are just a sign that we’ve all developed “morbus contentcreatoris” or “content creator brain”. And that is very depressing.</p>



<p>Because basically nobody doing anything interesting is “creating content”. The things we write, the paintings we draw, the music we play isn’t just about what the <em>output </em>is. Every creative activity is in itself a value, with the output sometimes being almost secondary. This article isn’t about the words I typed or even the process of typing them. It’s about me thinking through things I see in the world through the lense of my understanding of it, my values, my feelings and wishes. This isn’t content.</p>



<p>I keep thinking about the grift shifters. But maybe I shouldn’t. Maybe even that is taking them to seriously. And why should I? Because they surely don’t seem to be very serious about anything. </p>
<div><p>Liked it? Take a second to support tante on Patreon!</p><p><a rel="nofollow" href="https://www.patreon.com/tante?utm_content=post_button&amp;utm_medium=patron_button_and_widgets_plugin&amp;utm_campaign=&amp;utm_term=&amp;utm_source=https://tante.cc/2023/09/21/the-age-of-the-grift-shift/" aria-label="Click to become a patron at Patreon!"><img decoding="async" src="https://tante.cc/wp-content/plugins/patron-button-and-widgets-by-codebard/images/become_a_patron_button.png" alt="Become a patron at Patreon!"></a></p></div>
			
		

	
		</div><!-- .container.container-small -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AWS Customers Cannot Escape IPv4 (131 pts)]]></title>
            <link>https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html</link>
            <guid>37608900</guid>
            <pubDate>Fri, 22 Sep 2023 07:15:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html">https://tty.neveragain.de/2023/09/21/aws-cannot-escape-ipv4.html</a>, See on <a href="https://news.ycombinator.com/item?id=37608900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<hr>

<p>This is the first part of a blog series. Other pieces will be linked here when published.</p>

<hr>

<h2 id="introduction">Introduction</h2>

<p>AWS has announced that they
<a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/">will charge for public IPv4 addresses</a>
soon. Surprisingly, most reactions to this announcement were welcoming, as it’s believed to bring
a big push for IPv6 adoption.</p>

<p>The AWS blog had this to say:</p>

<blockquote>
  <p>This change … is also intended to encourage you to be a bit more frugal with your use of public IPv4 addresses and to think about accelerating your adoption of IPv6 as a modernization and conservation measure.</p>
</blockquote>

<p>I will explain why this charge will not affect IPv6 adoption in any meaningful way.</p>

<p>I will also show that it is pretty bold for AWS to use IPv6 in trying to soften the blow of this charge: AWS IPv6 support has
<em>huge</em> gaps. AWS makes it impossible for customers to move on from IPv4.</p>

<p>But first, a quick summary, and some costly anti-patterns to watch out for.</p>

<h2 id="whats-going-to-change">What’s Going to Change</h2>

<p>Every <em>dedicated</em> public AWS IPv4 address<sup id="fnref:byoip" role="doc-noteref"><a href="#fn:byoip" rel="footnote">1</a></sup> will be charged with $0.005 per hour (almost $4 per month /
more than $40 per year), starting February 2024.</p>

<p>Some examples of affected services:</p>
<ul>
  <li>Elastic Load Balancers (one address per availability zone, <em>at least</em> two – and IPv4 cannot be turned off)</li>
  <li>EC2 instances &amp; Elastic IPs</li>
  <li>ECS Fargate<sup id="fnref:awsvpc" role="doc-noteref"><a href="#fn:awsvpc" rel="footnote">2</a></sup> tasks configured with public IPs</li>
  <li>Global Accelerator IPs</li>
  <li>Site-to-site VPN IPs</li>
  <li>RDS (nine in ten customers have public IP on RDS by accident)</li>
  <li>Managed NAT Gateways</li>
</ul>

<p>So for a public Elastic Load Balancer configured in three availability zones (as recommended),
this is a base price increase of over 50%.</p>

<p>For Managed NAT Gateway, this is a base price increase of ~10%. I didn’t think those could become more expensive,
but here we are. By the way, this is a good time to remember that a NAT Gateway does <em>not</em> provide zonal redundancy –
a NAT Gateway in every availability zone is required when outbound connections are critical.</p>

<p>All <em>shared</em> IPv4 addresses, like those used for Cloudfront distributions or API Gateway endpoints, are <em>not</em> affected. Public IPv4
addresses used by non-VPC Lambda functions aren’t affected either.</p>

<h2 id="wasteful-patterns">Wasteful Patterns</h2>

<p>I have observed two patterns that waste quite a lot of addresses and are heavily impacted by the upcoming charges.</p>

<p>The first pattern is having multiple Load Balancers (per VPC); this is often the result of using several
readily available Cloudformation templates / Terraform modules, or somehow using Kubernetes ingress controllers that create
a Load Balancer <em>for every service</em>. This is fixed by not doing that! A single Load Balancer can handle many URLs
and services.</p>

<p>The second pattern is intentionally configuring public IPv4 on EC2 instances and Fargate tasks just to avoid Managed NAT Gateway
(commendable until now!). Ironically, this will still be cheaper until hitting around ten public IPv4 addresses per availability zone.</p>

<h2 id="how-to-check">How to Check</h2>

<p>The usage data for public IPv4 addresses is already being accounted, so it’s easy to check.
Usage is currently listed with $0 but will start to cost $0.005 per hour starting in February 2024.</p>

<ul>
  <li>In <em>Billing Dashboard</em> &gt; <em>Bills</em>, usage is displayed as <code>PublicIPv4:InUseAddress</code>
in the <em>Virtual Private Cloud</em> section</li>
  <li>In <em>Cost Explorer</em>, it’s easy to see usage hours by filtering to only include <code>PublicIPv4:InUseAddress</code>;
this is nice because Cost Explorer shows usage per day/hour and it’s easy to group e.g. by member account</li>
  <li><em>VPC</em> &gt; <em>VPC IP Address Manager</em> (at the very bottom) &gt; <em>Public IP insights</em> is a nice way to get an overview of
public IPv4 address usage; this part of IPAM is free, but it shows data only per account and region, and it’s
incomplete data – it doesn’t capture transient address usage.</li>
</ul>

<h2 id="accelerating-your-ipv6-adoption">Accelerating Your IPv6 Adoption</h2>

<p>So it does sound pretty obvious: IPv4 addresses will soon cost a few bucks, while IPv6 addresses are free of charge. Even better:
There is no concept of private addresses in IPv6, which means farewell to the Managed NAT Gateway and its magnificent pricing.
It seems like the perfect time to finally adopt IPv6.</p>

<p>There are two approaches to adopting IPv6. Either by going dual-stack, which means configuring IPv4 <em>and</em> IPv6 addresses on all systems.
Or by going IPv6-only internally, with IPv4 connectivity only where facing the general public – for example at the very edge, on the CDN,
which takes care of serving IPv4 clients.</p>

<p>Running IPv6-only is the optimal solution for a future-proof network. Even the U.S. Government, of all places,
recognizes this. A <a href="https://www.whitehouse.gov/wp-content/uploads/2020/11/M-21-07.pdf">memorandum from the Executive Office of the President</a> states:</p>

<blockquote>
  <p>In recent years it has become clear that [running dual-stack] is overly complex to maintain and unnecessary. As a result, standards bodies and leading technology companies began migrating toward IPv6-only deployments, thereby eliminating complexity, operational cost, and threat vectors associated with operating two network protocols.</p>
</blockquote>

<p>That memorandum was issued three years ago.</p>

<p>The underlying IPv6 support on AWS is excellent. It’s possible to configure IPv6-only EC2 instances in IPv6-only subnets
and everything works fine, including local DNS, time service, host configuration, security
and all the stuff to be expected in an IPv6-enabled network. Components like VPN, Transit Gateway,
Direct Connect and Network Firewall also support IPv6.</p>

<h2 id="cannot-escape-ipv4">Cannot Escape IPv4<sup id="fnref:title" role="doc-noteref"><a href="#fn:title" rel="footnote">3</a></sup></h2>

<p>The problem is actually using all the (other) Amazon Web Services.</p>

<p>It’s practically impossible to run IPv6-only on AWS. Trying to configure core services like API Gateway, Lambda,
ECS or App Runner into IPv6-only subnets makes them either sternly refuse those subnets or throw comically sad error messages like
<code>Not enough IP space available</code> (Elastic Load Balancer).</p>

<p>And it gets worse: Not even running dual-stack internally helps. Most AWS services, when configured into a dual-stack
subnet, happily ignore that the subnet has IPv6 configured; they can connect to IPv4 targets only. This
<a href="https://twitter.com/tim_nolet/status/1696206569090789416">actually hurts customers</a>.</p>

<p>Almost no AWS API can be used from a VPC without public IPv4 addresses<sup id="fnref:privatelink" role="doc-noteref"><a href="#fn:privatelink" rel="footnote">4</a></sup>.
Running an innocent <code>aws s3 ls</code> from an EC2 instance will fail. Using Systems Manager – the recommended way to
connect to and manage an instance – does not work. Accessing SQS from an ECS Task is impossible. And so on.
This is because <a href="https://awsipv6.neveragain.de/">more than 90% of all AWS service API endpoints do not support IPv6</a>.
Other endpoints like SES SMTP or ECR repository endpoints<sup id="fnref:dockerhub" role="doc-noteref"><a href="#fn:dockerhub" rel="footnote">5</a></sup> don’t work either. Cloudfront cannot connect
to IPv6 origins.</p>

<p>And then AWS frames this new charge as an “encouragement” to adopt IPv6. <em>This</em> is what ticked me off:
There is no escape, no way to avoid these charges – because AWS has significantly neglected IPv6 for years.</p>

<h2 id="charges-without-changes">Charges Without Changes</h2>

<p>For most customers, nothing will change. Which is exactly my point.</p>

<p>For AWS customers of any relevant size, this IPv4 charge is a drop in the bucket; they won’t even notice it on their bill.
In my experience so far, this will usually be very well below 1%. Nobody will invest significant engineering effort for that alone.</p>

<p>For many SMBs, hobbyists and startups though, this charge can easily amount to 10-30% of the bill. These customers certainly
would adopt IPv6 to avoid such a price increase.</p>

<p>So that’s where we are: The larger customers do not care about this charge; and those that will feel the impact have
no way to avoid it.</p>

<h2 id="conclusion-so-far">Conclusion So Far</h2>

<p>I think the IPv4 charge by itself is fine – maybe even necessary, given the IPv4 acquisition costs lately.</p>

<p>I would like AWS to be a role model in IPv6 adoption, like they are in many other areas.
If the AWS service landscape had mature IPv6 support, many customers could indeed migrate to IPv6-only environments.
Larger customers still would not care – but it would create a lot of
first movers within the AWS community, setting examples to follow.</p>

<p>But IPv6 is clearly not a first-class citizen on AWS today.</p>

<p>Given those circumstances, this charge will do nothing to drive IPv6 adoption. It will stiffle innovation and make it
<a href="https://www.lastweekinaws.com/blog/aws-has-a-moral-responsibility-to-fix-the-free-tier/">even less</a> attractive to have a
personal AWS account – for example, for learning AWS, to study for certifications (to invest in AWS as a career path) or to
support open-source work. AWS talks big about democratizing cloud; this isn’t it.</p>

<h2 id="upcoming-posts">Upcoming Posts</h2>

<p>This is the first part of a blog series. Upcoming posts will take in-depth looks at options for ingress and intra-VPC traffic
(IPv6 support in Cloudfront, API Gateway, App Runner etc.), egress traffic (avoiding public IPv4 addresses and NAT Gateways),
application programming issues (accessing AWS service endpoints and peculiarities of the AWS CLI and SDKs in dealing with
IPv6), other obscure details and a list of helpful resources.</p>

<hr>

<p><a href="https://twitter.com/apparentorder/status/1704628704876388472">Discuss and/or follow on Twitter!</a></p>

<hr>

<h2 id="links">Links</h2>

<ul>
  <li><a href="https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/IPv6-reference-architectures-for-AWS-and-hybrid-networks-ra.pdf">Dual Stack and IPv6-only Amazon VPC Reference Architectures</a> (PDF)</li>
  <li>AWS Workshop: <a href="https://catalog.workshops.aws/ipv6-on-aws/en-US">Get hands-on with IPv6</a></li>
  <li>AWS blog
    <ul>
      <li><a href="https://aws.amazon.com/blogs/aws/new-aws-public-ipv4-address-charge-public-ip-insights/">AWS Public IPv4 Address Charge + Public IP Insights</a></li>
      <li><a href="https://aws.amazon.com/blogs/networking-and-content-delivery/identify-and-optimize-public-ipv4-address-usage-on-aws/">Identify and optimize public IPv4 address usage on AWS</a></li>
      <li>Dual-stack IPv6 architectures for AWS and hybrid networks
<a href="https://aws.amazon.com/blogs/networking-and-content-delivery/dual-stack-ipv6-architectures-for-aws-and-hybrid-networks/">Part 1</a>
and <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/dual-stack-architectures-for-aws-and-hybrid-networks-part-2/">Part 2</a></li>
    </ul>
  </li>
  <li>AWS whitepaper
    <ul>
      <li><a href="https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/designing-an-ipv6-aws-cloud-network.html">Designing an IPv6 AWS Cloud network</a></li>
      <li><a href="https://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/IPv6-on-AWS.html">IPv6 on AWS</a></li>
    </ul>
  </li>
  <li>AWS VPC documentation
    <ul>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/aws-ipv6-support.html">AWS services that support IPv6</a>
(includes a list of ~30 services that have at least some IPv6 support; not listed at all are the ~200 other services)</li>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-migrate-ipv6.html">Migrate your VPC from IPv4 to IPv6</a></li>
      <li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-nat64-dns64.html">DNS64 and NAT64</a></li>
    </ul>
  </li>
</ul>

<hr>

<p><em>Update 2023-09-21</em> – added “Links” section</p>

<p><em>Update 2023-09-21</em> – affected services: added RDS; made it clear that those are examples
(list is not exhaustive)</p>

<hr>



	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm fed up with it, so I'm writing a browser (338 pts)]]></title>
            <link>https://adayinthelifeof.nl/2023/09/22/browsers.html</link>
            <guid>37608580</guid>
            <pubDate>Fri, 22 Sep 2023 06:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adayinthelifeof.nl/2023/09/22/browsers.html">https://adayinthelifeof.nl/2023/09/22/browsers.html</a>, See on <a href="https://news.ycombinator.com/item?id=37608580">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<article>
			<header>
				
				
			</header>

			

			









<img src="https://www.gravatar.com/avatar/1761ecd7fe763583553dde43e62c47bd?s=50">

Posted on 22 Sep 2023<br>

Tagged with:

    
    [ <a href="https://adayinthelifeof.nl/archive/tags/rustprogrammingbrowsers">rust, programming, browsers</a> ]&nbsp;


 <p>This blogpost starts with me switching of my car radio, and ends with me writing a browser. There is some stuff 
in between as well.</p>

<!--more-->

<p>Some ten years ago, I used to travel a lot by car to customers. I listened to news radio to keep myself occupied during the daily commute. In the Netherlands, I would listen to Business News Radio (BNR)
and/or Radio 1. I liked having people talk and discuss the daily news and not being interrupted by the latest music I didn’t like anyway. I think I still had a car with MP3 CDs.</p>

<p>All was well until one day, I decided to switch off my radio. I didn’t want to hear the news anymore. And that evening on the way back, the radio stayed off. I still didn’t want to listen to the news. In fact, I didn’t want to listen to the news like ever again!</p>

<p>Why? I don’t know exactly, but I don’t think it’s a coincidence that this was also around the time my wife
got pregnant (although I cannot say for sure if this was before or after we knew). But when I thought about it, the
more apparent it got: the ONLY thing you hear on the news.. any news.. are bad things(tm). War, corruption, you name it.
We still should be thankful we still call these things news because often it feels like it would be more newsworthy if
politicians weren’t corrupt, or not kept their promises, or screwed another group of people over. And the problem is
that I - as a single person - have no ways or means to do anything about it. Nothing. Not even if I tried.</p>

<p>Basically, my thinking was: if listening to the news makes me feel bad, I should stop listening to the news. Makes sense
I guess. But I went a bit further: i do not listen to the news, I also do not watch the news or browse
the internet for news. But I also actively try to get the news out of the way. We don’t watch news channels; we switch
channels when the 8 o’clock news starts (that was a thing before streaming), and I leave a room where there is news on
which I cannot control.</p>

<p>Fast forward to almost the present: I still do not watch, listen or browse the news, but I read Reddit for some subreddits that
sometimes posts news facts here and there. Any “important” news that I should need to know (do I really?) comes to me anyway because
of people. So all is well. I’m not getting saddened by the news. Job done.</p>

<p>That was until a year ago, but it’s probably way earlier that the next thing started. So although I don’t follow the classic world and political news, I do like to follow tech news, and for me, this is done 98% through Ycombinator’s hacker news. But I started to get the same feeling I had ten years ago: it’s all negative.
Companies are pushing unwanted updates, breaking all promises, raising subscriptions, and buying up companies while doing
everything in their power to figure out how much more personal data they can take (away) from us. Governments are banning
encryption because of child molesters (they also use vans and candy. Let’s also ban them!). People in power with
absolutely no idea of how the modern world works and people in power who DO know how it works, want to make it even worse for everybody else. Meanwhile, the enshitification keeps speeding up, and I’m afraid
to update my printer drivers in case my printer tells me it doesn’t like my ink and stops printing. And then there is the golden goose: the internet browser. The one place where almost everybody in the world spend their time. How long will it be until browser developers decide that ads could also shown by the browser itself, rather than from the
rendered sites that adblockers can block? We already had those.. they WILL come back. And how about browser-specific extensions that make sites unusable from other browsers (sorry, you cannot view your gmail with Firefox. Please install google chrome)? This has happened, and as soon as lawyers find a way out of monopoly issues, it will happen
again. Things like this make me sad, and again, I’m just one person out of many with little to no influence.</p>

<p>But that’s not quite right this time. This time, I DO have some influence because I am a programmer. I can develop software and I can share this software and code with others so they do not need to use it from companies that only serve their shareholders and pockets.</p>

<p>So we come to the point that I’ve decided that I am going to write a browser. For two reasons: I want this to be a way to push back. Just a tiny amount. And reason two is that I always wanted to write a browser.</p>

<p>I do not expect ANYTHING to come from this project. I do not
expect to finish this project. And I do expect that - if nobody will follow me into helping the project - the project will be dead quite soon. I do not expect this project to become the dominant browser that will topple all the big players in the market. But I do want the project to be open for non-commercial purposes. I want others to be inspired by it and make their project. And those projects will inspire others, and so on, until we DO reach a point where we have a browser that can topple the big players.
I can’t fly. I don’t have x-ray eyes. I don’t have a heavy hammer. But I can develop software. And I can share with as many people that I can. Either as an inspiration for others to write their own code, or as an example on how not to write code.</p>

<p>I’m not an idyllic person. I’m not an activist fighting for a better world or anything. But I get angry about the corporations screwing over people, software, planets. I’m not in a position to solve that. I’m just a
developer that will attempt to write a browser.</p>

<p>Details about the progress of the project can be found here: <a href="https://codemusings.nl/@jaytaph/p/MQpHToAx8c1KXyU98Auip4">https://codemusings.nl/@jaytaph/p/MQpHToAx8c1KXyU98Auip4</a></p>

<p>The project itself can be found on GitHub: <a href="https://github.com/jaytaph/gosub-browser">https://github.com/jaytaph/gosub-browser</a></p>




<hr>



		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Bogus CVE Problem (180 pts)]]></title>
            <link>https://lwn.net/Articles/944209/</link>
            <guid>37608110</guid>
            <pubDate>Fri, 22 Sep 2023 05:08:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/944209/">https://lwn.net/Articles/944209/</a>, See on <a href="https://news.ycombinator.com/item?id=37608110">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
           <div><b>LWN.net needs you!</b><p>Without subscribers, LWN would simply not exist.  Please consider
       <a href="https://lwn.net/subscribe/">signing up for a subscription</a> and helping
       to keep LWN publishing</p></div>
           </center>
           
<p>
The "<a href="https://cve.mitre.org/">Common Vulnerabilities and
Exposures</a>" (CVE) system was launched late 
in the previous century (September&nbsp;1999) to track vulnerabilities in
software.  Over the years since, it has had a <a href="https://lwn.net/Articles/679315/">somewhat checkered
reputation</a>, along with some <a href="https://lwn.net/Articles/851849/">some attempts to
replace it</a>, but CVE numbers are still the only effective way to track
vulnerabilities.  While that can certainly be useful, the
CVE-assignment (and severity scoring) process is not without its problems.
The prominence of CVE numbers, and the consequent increase in 
"reputation" for a reporter, have combined to create a system that can
be—and is—actively gamed.  Meanwhile, the organizations that oversee the
system are ultimately not doing a particularly stellar job.
</p>

<p>
A recent incident highlights some of the problems inherent in the system. <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-19909">CVE-2020-19909</a>,
which is an integer-overflow bug in
the <a href="https://curl.se/">curl tool and library for URL-based data
transfers</a> that was only reported
to the project in&nbsp;2023.  In a <a href="https://daniel.haxx.se/blog/2023/08/26/cve-2020-19909-is-everything-that-is-wrong-with-cves/">blog
post describing the mess</a>, curl maintainer Daniel
Stenberg said that a <a href="https://curl.se/mail/lib-2023-08/0031.html">message to the
curl-library mailing list</a> on August&nbsp;25 alerted the project that the CVE
had become public the week before.
</p>

<p>
The year in the CVE number (2020 in this case) is meant to indicate when
the bug was 
reported to one of the <a href="https://www.cve.org/ProgramOrganization/CNAs">more than&nbsp;300 CVE
numbering authorities</a> (CNAs) that hand out CVE numbers.  Under normal
circumstances, a new bug showing up with a CVE number would have&nbsp;2023 in
it, but sometimes CVEs are given out for older bugs that somehow
slipped through the cracks.  That appears to be what happened in this case,
as Stenberg was able to track the problem back to a <a href="https://hackerone.com/reports/661847">bug report</a> from Jason Lee
in mid-2019.
</p>

<p>
The report was for a legitimate bug, where the
<tt>‑‑retry‑delay</tt> option value was being multiplied
by&nbsp;1000 (to milliseconds) without an overflow check.  But what it was
<i>not</i> was a security 
bug, Stenberg said; giving insanely large values for the
option might result in incorrect 
delays—far shorter than requested—but it is not a security problem to make
multiple requests in a short time span.  If it were, "<q>then a browser
makes a DOS [denial of service] every time you visit a website — and curl
does it when you give it two URLs on the same command line</q>", he said in
a <a href="https://daniel.haxx.se/blog/2023/09/05/bogus-cve-follow-ups/">followup
post</a>. 
</p>

<p>
The problem was <a href="https://github.com/curl/curl/pull/4166">duly
fixed</a>, a test case was added, and Lee was credited with the report in
the commit message.  In September&nbsp;2019, curl&nbsp;7.66.0 was <a href="https://curl.se/mail/archive-2019-09/0002.html">released</a> with
fix, which was mentioned in the announcement; also notable are the two CVEs
mentioned at the top of the bug fixes listed.  As Stenberg noted, the curl
project 
works hard to ensure that it fully documents the (real) CVEs that get filed
for
it; his exasperation with CVE-2020-19909 coming out of the blue is evident:
</p><blockquote>
In the curl project we work hard and fierce on security and we always work
with security researchers who report problems. We file our own CVEs, we
document them and we make sure to tell the world about them. <a href="https://curl.se/docs/security.html">We list over&nbsp;140 of them</a> with every imaginable detail about them provided. We aim at
providing gold-level documentation for <i>everything</i> and that includes our
past security vulnerabilities. 
<p>
That someone else suddenly has submitted a CVE for curl is a surprise. We
have not been told about this and we would <i>really</i> have liked to. [...]
</p></blockquote>


<p>
The <a href="https://nvd.nist.gov/">National Vulnerability Database</a>
(NVD) tracks CVEs and "scores" them using the <a href="https://en.wikipedia.org/wiki/Common_Vulnerability_Scoring_System">Common
Vulnerability Scoring System</a> (CVSS), which is a ten-point scale that is
meant to give an indication of the severity of a vulnerability.  For the
curl bug, which should probably not be scored at all, NVD initially came up
with a "9.8 critical", scoring an integer overflow in a delay parameter
as one of the most severe types of vulnerability possible.  Stenberg, who
has tangled with NVD over scoring before, is even further exasperated:
</p><blockquote>
It was obvious already before that NVD really does not try very hard to
actually understand or figure out the problem they grade. In this case it
is quite impossible for me to understand how they could come up with this
severity level. It's like they saw "integer overflow" and figure that <i>wow,
yeah that is the most horrible flaw we can imagine</i>, but clearly nobody at
NVD engaged their brains nor looked at the "vulnerable" code or the patch
that fixed the bug. Anyone that looks can see that this is not a security
problem. 
</blockquote>


<p>
In fact, the pull request for the fix was attached to the report, but that
apparently made little difference in the assessment from NVD.  Back in
March, 
Stenberg <a href="https://daniel.haxx.se/blog/2023/03/06/nvd-makes-up-vulnerability-severity-levels/">decried</a>
the NVD scoring process and, in particular, the NVD practice of re-scoring
CVEs that have already had severity levels attached to them.  NVD uses
CVSS, but the curl project long ago rejected that scoring system:
</p><blockquote>
In the curl project we decided to abandon CVSS years ago because of its
inherent problems. Instead we use only the four severity names: <b>Low</b>,
<b>Medium</b>, <b>High</b>, and <b>Critical</b> and we work out the
severity together in the 
curl security team as we work on the vulnerability. We make sure we
understand the problem, the risks, its prevalence and more. We take all
factors into account and then we set a severity level we think helps the
world understand it. 
</blockquote>


<p>
His example in that case is <a href="https://curl.se/docs/CVE-2022-42915.html">a double-free in curl</a>
that the project determined had a "medium" severity, while NVD scored it as
"9.8 critical", as can be seen in the <a href="https://github.com/advisories/GHSA-98w6-hw73-ph8m">GitHub advisory
database</a>. Since then, NVD apparently had a change of heart after
Stenberg's complaint as the CVE is <a href="https://nvd.nist.gov/vuln/detail/CVE-2022-42915">now scored "8.1
high"</a>.  In a <a href="https://daniel.haxx.se/blog/2023/06/12/nvd-damage-continued/">followup
on NVD "brokenness"</a>, Stenberg gave another example of a CVE that was
initially scored "9.8 critical", but eventually was reduced to "5.9 medium"
after complaints—though the curl project rates it as "low".  He also noted
that there is a set of projects that never report low or medium CVEs that
they find, in order to avoid these kinds of scoring woes.
</p>

<p>
One could perhaps wonder if this is all just a problem for the curl project
and not more widespread, but there is a fair amount of evidence of a
variety of problems in CVE-land.  For example, the PostgreSQL project had a
similar problem with a <a href="https://www.postgresql.org/about/news/cve-2020-21469-is-not-a-security-vulnerability-2701/">CVE
"from"&nbsp;2020</a> that appeared recently—and is not a security vulnerability
at all, according to the project.  In June, the <a href="https://keepassxc.org/">KeePassXC</a> password manager project
<a href="https://keepassxc.org/blog/2023-06-20-cve-202335866/">had a bogus
CVE filed</a> for the tool; there are other examples as well.
</p>

<p>
Each of these bogus CVE filings, which can apparently be made anonymously and
without much in the way of backing information, require that the project
notice its existence, analyze the problem (or "problem"), and, if
necessary, dispute the existence or score of the CVE.
As noted, several curl CVEs have had their scores reduced rather
substantially due to requests from the project.  The delay parameter
overflow that was initially scored&nbsp;9.8 has been reduced to "3.3
low", marked as "disputed", and had a link to Stenberg's blog post
added to the NVD entry.

</p><p>
Keeping up with all of that is a lot of
work, which Stenberg said he is going to try to avoid in the future by
applying to become the CNA for curl.  Several other open-source projects are
CNAs, which gives them some notification of a reported problem along with
ways to try to ensure that the problem is handled sanely.  He mentioned
Apache, OpenSSL, and Python as some of the projects that are already
CNAs; Python was just <a href="https://pyfound.blogspot.com/2023/08/psf-authorized-as-cna.html">granted
CNA status</a> at the end of August.
</p>

<p>
Meanwhile, though, CVEs are used in ways that elevate their importance well
beyond the level that makes sense given the amount of scrutiny that is
apparently applied to them.
Service-level contracts and governmental requirements mean that a critical
CVE needs to be addressed in short order, so non-critical bugs that get
marked that way can cause real problems.  It does provide incentives for
companies and others to try to downplay the severity of bugs, as well, of
course, which makes for something of a "<a href="https://en.wikipedia.org/wiki/List_of_Doctor_Dolittle_characters#Pushmi-Pullyu">Pushmi-Pullyu</a>"
in CVE-land.
</p>

<p>
As was alluded to in our mid-August <a href="https://lwn.net/Articles/941745/">look at
kernel security reporting for distributions</a>, the CVE system is
generally included in the "security circus" that kernel developers
largely disdain.  A <a href="https://lwn.net/Articles/801157/">2019 talk by Greg
Kroah-Hartman</a> described multiple problems that he sees with the
system as well.
</p>

<p>
All in all, the CVE system seems to be broken in various ways.  It also
seems to be getting more and more entrenched into "cybersecurity" handling
at various levels.  Given that it is effectively run by—and now
for—governmental agencies, the ability to replace it with something more
sensible has likely already passed us by.  CVE, warts and all, will be with
us for a long time to come; FOSS projects and organizations are simply
going to have to figure out how to coexist with it.
</p><br clear="all"><hr><p>
           (<a href="https://lwn.net/Login/?target=/Articles/944209/">Log in</a> to post comments)
           </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Modular forms, the ‘fifth fundamental operation’ of math (105 pts)]]></title>
            <link>https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/</link>
            <guid>37608061</guid>
            <pubDate>Fri, 22 Sep 2023 04:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/">https://www.quantamagazine.org/behold-modular-forms-the-fifth-fundamental-operation-of-math-20230921/</a>, See on <a href="https://news.ycombinator.com/item?id=37608061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><p>Modular forms are one of the most beautiful and mysterious objects in mathematics. What are they?</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/ModularForms-Courtesy-of-SamuelJinglianLi-Lede-1-scaled.webp"></p></div><figcaption></figcaption></figure><div><h2>Introduction</h2><div><p>“There are five fundamental operations in mathematics,” the German mathematician Martin Eichler supposedly said. “Addition, subtraction, multiplication, division and modular forms.”</p>
<p>Part of the joke, of course, is that one of those is not like the others. Modular forms are much more complicated and enigmatic functions, and students don’t typically encounter them until graduate school. But “there are probably fewer areas of math where they don’t have applications than where they do,” said <a href="https://people.mpim-bonn.mpg.de/zagier/">Don Zagier</a>, a mathematician at the Max Planck Institute for Mathematics in Bonn, Germany. Every week, new papers extend their reach into number theory, geometry, combinatorics, topology, cryptography and even string theory.</p>
<p>They are often described as functions that satisfy symmetries so striking and elaborate that they shouldn’t be possible. The properties that come with those symmetries make modular forms immensely powerful. It’s what made them key players in the landmark 1994 proof of Fermat’s Last Theorem. It’s what made them central to <a href="https://www.quantamagazine.org/ukrainian-mathematician-maryna-viazovska-wins-fields-medal-20220705/">more recent work on sphere packing</a>. And it’s what now makes them crucial to the ongoing development of a “mathematical theory of everything” called the <a href="https://www.quantamagazine.org/what-is-the-langlands-program-20220601/">Langlands program</a>.</p>
<p>But what are they?</p>
<h2><strong>Infinite Symmetries</strong></h2>
<p>To understand a modular form, it helps to first think about more familiar symmetries.</p>
<p>In general, a shape is said to have symmetry when there is some transformation that leaves it the same.</p>
</div></div><figure></figure><div><h2>Introduction</h2><div><p>A function can also exhibit symmetries. Consider the parabola defined by the equation $latex f(x) = x^2$. It satisfies one symmetry: It can be reflected over the <em>y</em>-axis. For instance, $latex f(3) = f(−3) = 9$. More generally, if you shift any input $latex x$ to $latex -x$, then $latex x^2$ outputs the same value.</p>
<p>Infinitely many functions satisfy this symmetry. Here are just a few:</p>

<p>The last example is the cosine function from trigonometry. It exhibits reflection symmetry, but it also has other symmetries. If you shift $latex x$<i>&nbsp;</i>by integer multiples of $latex 2\pi$, the function always returns the same value — meaning that there are infinitely many transformations that can leave the function unchanged.</p>

<p>This additional symmetry makes functions like cosine incredibly useful. “Much of basic physics begins with understanding the full implications of the trigonometric functions,” said <a href="https://uva.theopenscholar.com/ken-ono">Ken Ono</a>, a mathematician at the University of Virginia.</p>
<p>“Modular forms are something like trigonometric functions, but on steroids,” he added. They satisfy infinitely many “hidden” symmetries.</p>
<h2><strong>The Complex Universe</strong></h2>
<p>Functions can only do so much when they’re defined in terms of the real numbers — values that can be expressed as a conventional decimal. As a result, mathematicians often turn to the complex numbers, which can be thought of as pairs of real numbers. Any complex number is described in terms of two values — a “real” component and an “imaginary” one, which is a real number multiplied by the square root of −1 (which mathematicians write as $latex i$).</p>
<p>Any complex number can therefore be represented as a point in a two-dimensional plane.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/RepresentingComplexNunmbers-v5_Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><p>It is hard to visualize functions of complex numbers, so mathematicians often turn to color. For example, you can color the complex plane so that it looks like a rainbow wheel. The color of each point corresponds to its angle in polar coordinates. Directly to the right of center, where points have an angle of 0 degrees, you get red. At 90 degrees, or straight up, points are colored bright green. And so on. Finally, contour lines mark changes in size, or magnitude, as on a topographical map.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/graphic5-one-scaled.jpg"></p></div><figcaption><div><p>The complex function&nbsp;<i data-stringify-type="italic">f</i>(<i data-stringify-type="italic">z</i>) =&nbsp;<i data-stringify-type="italic">z</i>&nbsp;depicted as a rainbow wheel of color. It can be used as a reference to illustrate other functions.</p></div></figcaption></figure><div><h2>Introduction</h2><p>You can now use this as a reference graph to illustrate complex functions. A point’s position on the plane represents the input, and you’ll assign that point a color based on the reference graph. For instance, consider the function $latex f(z) = z^2$. When $latex z = 1 + i$, $latex f(z) = 2i$, since&nbsp; $latex (1 + i)^2 = 2i$. Because $latex 2i$ is colored bright green on the reference graph, on your new graph you’ll color the point $latex 1 + i$ bright green.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2059/12/graphic5-two-scaled.jpg"></p></div><figcaption><div dir="auto" data-qa="message-text">
<p>This graph of the complex function <i data-stringify-type="italic">f</i>(z) = $latex z^2$&nbsp;shows outputs via colors chosen using<b data-stringify-type="bold">&nbsp;</b>the reference graph&nbsp;<i data-stringify-type="italic">f</i>(<i data-stringify-type="italic">z</i>) =&nbsp;<i data-stringify-type="italic">z</i>.</p>
</div></figcaption></figure><div><h2>Introduction</h2><div><p>The graph of $latex f(z) = z^2$ runs through the colors twice, because squaring a complex number doubles its angle. It also has more contour lines, because the outputs grow in size more quickly.</p>
<p>More generally, the graph looks the same when you reflect points over a diagonal line drawn through the center (or origin).</p>
<p>This is one symmetry of a complex-valued function. Modular forms exhibit a bewildering variety of such symmetries. But it can be tough to make sense of the actual function those colors and contour lines represent.</p>
<h2><strong>The Fundamental Domain</strong></h2>
<p>To do so, it helps to try to simplify the way we look at these complicated functions.</p>
<p>Because of the modular form’s symmetries, you can compute the entire function based on just a narrow sliver of inputs, located in a region of the plane called the fundamental domain. This region looks like a strip going up from the horizontal axis with a semicircular hole cut out of its bottom.</p>
<p>If you know how the function behaves there, you’ll know what it does everywhere else.</p>
<p>Here’s how:</p>
</div></div><figure></figure><div><h2>Introduction</h2><div><p>Two kinds of transformations copy the fundamental domain to the right and left, as well as to a series of ever-shrinking semicircles along the horizontal axis. These copies fill the entire upper half of the complex plane.</p>
<p>A modular form relates the copies to each other in a very particular way. That’s where its symmetries enter the picture.</p>
<p>If you can move from a point in one copy to a point in another through the first kind of transformation — by shifting one unit to the left or right — then the modular form assigns the same value to those two points. Just as the values of the cosine function repeat in intervals of $latex 2\pi$, a modular form is periodic in one-unit intervals.</p>

<p>Meanwhile, you can get from a point in one copy to a point in another through the second type of transformation — by reflecting over the boundary of the circle with radius 1 centered at the origin. In this case, the modular form doesn’t necessarily assign those points the same value. However, the values at the two points relate to each other in a regular way that also gives rise to symmetry.</p>
<p>You can combine these transformations in infinitely many ways, which gives you the infinitely many symmetry conditions that the modular form must satisfy.</p>
<p>“That doesn’t necessarily sound very exciting,” said <a href="https://math.dartmouth.edu/~jvoight/">John Voight</a>, a mathematician at Dartmouth College. “I mean, carving up the upper half-plane and putting numbers on various places — who cares?”</p>
<p>“But they’re very elemental,” he added. And there’s a reason why that’s the case.</p>
<h2><strong>Controlled Spaces </strong></h2>
<p>In the 1920s and ’30s, the German mathematician Erich Hecke developed a deeper theory around modular forms. Crucially, he realized that they exist in certain spaces — spaces with specific dimensions and other properties. He figured out how to describe these spaces concretely and use them to relate different modular forms to one another.</p>
<p>This realization has driven a lot of 20th- and 21st-century mathematics.</p>
<p>To understand how, first consider an old question: How many ways can you write a given integer as the sum of four squares? There is only one way to write zero, for instance, while there are eight ways to express 1, 24 ways to express 2, and 32 ways to express 3. To study this sequence — 1, 8, 24, 32 and so on — mathematicians encoded it in an infinite sum called a generating function:</p>
<p>$latex 1 + 8q + {{24q}^2} + {{32q}^3} + {{24q}^4} + {{48q}^5} + …$</p>
<p>There wasn’t necessarily a way to know what the coefficient of, say, $latex q^{174}$ should be — that was precisely the question they were trying to answer. But by converting the sequence into a generating function, mathematicians could apply tools from calculus and other fields to infer information about it. They might, for instance, be able to come up with a way to approximate the value of any coefficient.</p>

<p>But it turns out that if the generating function is a modular form, you can do much better: You can get your hands on an exact formula for every coefficient.</p>
<p>“If you know it’s a modular form, then you know everything,” said <a href="https://www.mathematik.tu-darmstadt.de/fb/personal/details/jan_hendrik_bruinier.en.jsp">Jan Bruinier</a> of the Technical University of Darmstadt in Germany.</p>
<p>That’s because the infinitely many symmetries of the modular form aren’t just beautiful to look at — “they’re so constraining,” said <a href="https://math.vanderbilt.edu/rolenl/index.html">Larry Rolen</a> of Vanderbilt University, that they can be made into “a tool for automatically proving congruences and identities between things.”</p>
<p>Mathematicians and physicists often encode questions of interest in generating functions. They might want to count the number of points on special curves, or the number of states in certain physical systems. “If we are lucky, then it is a modular form,” said <a href="https://www.claudia-alfes.de/">Claudia Alfes-Neumann</a>, a mathematician at Bielefeld University in Germany. That can be very difficult to prove, but if you can, then “the theory of modular forms is so rich that it gives you tons of possibilities to investigate these [series] coefficients.”</p>
<h2><strong>Building Blocks</strong></h2>
<p>Any modular form is going to look very complicated. Some of the simplest — which are used as building blocks for other modular forms —&nbsp;are called Eisenstein series.</p>
<p>You can think of an Eisenstein series as an infinite sum of functions. To determine each of those functions, use the points on an infinite 2D grid:</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/MeetaModularFormbyMerrillSherman-v6_Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><p>When you add the functions associated to just four points in the grid near the origin, you can see how distinct symmetries begin to emerge.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/7C-01.webp"></p></div><figcaption><div><p>The sum of the four simple functions above, shown in the upper half of the complex plane.</p></div></figcaption></figure><div><h2>Introduction</h2><p>If you take the full sum of the grid’s infinitely many functions, you get an Eisenstein series that’s arguably the easiest modular form to write down. The patterns reflect the form’s defining symmetries — repeating endlessly to the left and right, and transforming in more complicated ways closer to the horizontal axis.</p></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/7C-02.webp"></p></div><figcaption><div><p>The full Eisenstein series is the sum of an infinite number of functions.</p></div></figcaption></figure><div><h2>Introduction</h2><div><h2><strong>The Game Continues</strong></h2>
<p>The study of modular forms has led to a flood of mathematical triumphs. For instance, recent work on sphere packing, for which the Ukrainian mathematician Maryna Viazovska <a href="https://www.quantamagazine.org/ukrainian-mathematician-maryna-viazovska-wins-fields-medal-20220705/">won the Fields Medal last year</a>, used modular forms. “When I saw that, I was quite surprised,” Bruinier said. “But it somehow works.”</p>
<p>Modular forms have turned out to be connected to an important algebraic object called the <a href="https://www.quantamagazine.org/mathematicians-chase-moonshine-string-theory-connections-20150312/">monster group</a>. They’ve been used to construct special kinds of networks called <a href="https://www.quantamagazine.org/new-proof-shows-that-expander-graphs-synchronize-20230724/">expander graphs</a>, which show up in computer science, communications theory and other applications. They’ve made it possible to study potential models of particle interactions in string theory and quantum physics.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2023/09/ApplicationsofModularFormsbyMerrillSherman-v4_1300-Desktop.svg"></p></div><figcaption><div><p>Merrill Sherman/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Perhaps most famously, the 1994 proof of Fermat’s Last Theorem hinged on modular forms. The theorem, widely considered one of the most important problems in number theory, states that there are no three nonzero integers <em>a</em>, <em>b</em> and <em>c</em> that satisfy the equation $latex {a^n} + {b^n} = {c^n}$ when $latex n$ is an integer greater than 2. The mathematician Andrew Wiles proved it true by assuming the opposite — that a solution to the equation does exist — and then using modular forms to show that such an assumption must lead to a contradiction.</p>
<p>First he used his assumed solution to construct a mathematical object called an elliptic curve. He then showed that you can always associate a unique modular form to such a curve. However, the theory of modular forms dictated that in this case, that modular form couldn’t exist. “It’s too good to be true,” Voight said. Which meant, in turn, that the assumed solution couldn’t exist — thus confirming Fermat’s Last Theorem.</p>
<p>Not only did this resolve a centuries-old problem; it also provided a better understanding of elliptic curves, which can be difficult to study directly (and which play an important role in cryptography and error-correcting codes).</p>

<p>The proof also illuminated a bridge between geometry and number theory. That bridge has since been broadened into the <a href="https://www.quantamagazine.org/what-is-the-langlands-program-20220601/">Langlands program</a>, a greater set of connections between the two fields — and the subject of one of the central research efforts of contemporary mathematics. Modular forms have also been generalized in other areas, where their potential applications are just starting to be recognized.</p>
<p>They continue to turn up all over the place in math and physics, sometimes quite mysteriously. “I look in a paper about black holes,” said <a href="http://www.math.toronto.edu/~skudla/">Steve Kudla</a> of the University of Toronto, “and I find modular forms that are friends of mine. But I don’t know why they’re there.”</p>
<p>“Somehow,” he added, “modular forms capture some of the most fundamental symmetries of the world.”</p>
</div></div></div><div><h2>Next article</h2><p>The Experimental Cosmologist Hunting for the First Sunrise</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suppressing negative thoughts may be good for mental health after all (140 pts)]]></title>
            <link>https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</link>
            <guid>37607203</guid>
            <pubDate>Fri, 22 Sep 2023 02:35:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests">https://www.cam.ac.uk/research/news/suppressing-negative-thoughts-may-be-good-for-mental-health-after-all-study-suggests</a>, See on <a href="https://news.ycombinator.com/item?id=37607203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Researchers at the Medical Research Council (MRC) Cognition and Brain Sciences Unit trained 120 volunteers worldwide to suppress thoughts about negative events that worried them, and found that not only did these become less vivid, but that the participants’ mental health also improved.</p>

<p>“We’re all familiar with the Freudian idea that if we suppress our feelings or thoughts, then these thoughts remain in our unconscious, influencing our behaviour and wellbeing perniciously,” said Professor Michael Anderson.</p>

<p>“The whole point of psychotherapy is to dredge up these thoughts so one can deal with them and rob them of their power. In more recent years, we’ve been told that suppressing thoughts is intrinsically ineffective and that it actually causes people to think the thought more – it’s the classic idea of ‘Don’t think about a pink elephant’."</p>

<p>These ideas have become dogma in the clinical treatment realm, said Anderson, with national guidelines talking about thought avoidance as a major maladaptive coping behaviour to be eliminated and overcome in depression, anxiety, PTSD, for example.</p>

<p>When COVID-19 appeared in 2020, like many researchers, Professor Anderson wanted to see how his own research could be used to help people through the pandemic. His interest lay in a brain mechanism known as inhibitory control – the ability to override our reflexive responses – and how it might be applied to memory retrieval, and in particular to stopping the retrieval of negative thoughts when confronted with potent reminders to them.</p>

<p>Dr Zulkayda Mamat – at the time a PhD student in Professor Anderson’s lab and at Trinity College, Cambridge – believed that inhibitory control was critical in overcoming trauma in experiences occurring to herself and many others she has encountered in life. She had wanted to investigate whether this was an innate ability or something that was learnt – and hence could be taught.</p>

<p>Dr Mamat said: “Because of the pandemic, we were seeing a need in the community to help people cope with surging anxiety. There was already a mental health crisis, a hidden epidemic of mental health problems, and this was getting worse. So with that backdrop, we decided to see if we could help people cope better.”</p>

<p>Professor Anderson and Dr Mamat recruited 120 people across 16 countries to test whether it might in fact be possible – and beneficial – for people to practice suppressing their fearful thoughts. Their findings are published today in <em>Science Advances</em>.</p>

<p>In the study, each participant was asked to think of a number of scenarios that might plausibly occur in their lives over the next two years – 20 negative ‘fears and worries’ that they were afraid might happen, 20 positive ‘hopes and dreams’, and 36 routine and mundane neutral events. The fears had to be worries of current concern to them, that have repeatedly intruded in their thoughts.</p>

<p>Each event had to be specific to them and something they had vividly imagined occurring. For each scenario, they were to provide a cue word (an obvious reminder that could be used to evoke the event during training) and a key detail (a single word expressing a central event detail). For example:</p>

<ul>
	<li>Negative – visiting one’s parents at the hospital as a result of COVID-19, with the cue ‘Hospital’ and the detail ‘Breathing’.</li>
	<li>Neutral – a visit to the opticians, with the cue ‘Optician’ and the detail ‘Cambridge’.</li>
	<li>Positive – seeing one’s sister get married, with the cue ‘Wedding’ and the detail ‘Dress’.</li>
</ul>

<p>Participants were asked to rate each event on a number of points: vividness, likelihood of occurrence, distance in the future, level of anxiety about the event (or level of joy for positive events), frequency of thought, degree of current concern, long-term impact, and emotional intensity.</p>

<p>Participants also completed questionnaires to assess their mental health, though no one was excluded, allowing the researchers to look at a broad range of participants, including many with serious depression, anxiety, and pandemic-related post-traumatic stress.</p>

<p>Then, over Zoom, Dr Mamat took each participant through the 20-minute training, which involved 12 ‘No-imagine’ and 12 ‘Imagine’ repetitions for events, each day for three days.</p>

<p>For No-imagine trials, participants were given one of their cue words, asked to first acknowledge the event in their mind.&nbsp; Then, while continuing to stare directly at the reminder cue, they were asked to stop thinking about the event – they should not try to imagine the event itself or use diversionary thoughts to distract themselves, but rather should try to block any images or thoughts that the reminder might evoke. &nbsp;For this part of the trial, one group of participants was given their negative events to suppress and the other given their neutral ones.</p>

<p>For Imagine trials, participants were given a cue word and asked to imagine the event as vividly as possible, thinking what it would be like and imagining how they would feel at the event. For ethical reasons, no participant was given a negative event to imagine, but only positive or neutral ones.</p>

<p>At the end of the third day and again three months later, participants were once again asked to rate each event on vividness, level of anxiety, emotional intensity, etc., and completed questionnaires to assess changes in depression, anxiety, worry, affect, and wellbeing, key facets of mental health.</p>

<p>Dr Mamat said: “It was very clear that those events that participants practiced suppressing were less vivid, less emotionally anxiety-inducing, than the other events and that overall, participants improved in terms of their mental health. But we saw the biggest effect among those participants who were given practice at suppressing fearful, rather than neutral, thoughts.”&nbsp;</p>

<p>Following training – both immediately and after three months – participants reported that suppressed events were less vivid and less fearful. They also found themselves thinking about these events less.</p>

<p>Suppressing thoughts even improved mental health amongst participants with likely post-traumatic stress disorder. Among participants with post-traumatic stress who suppressed negative thoughts, their negative mental health indices scores fell on average by 16% (compared to a 5% fall for similar participants suppressing neutral events), whereas positive mental health indices scores increased by almost 10% (compared to a 1% fall in the second group).</p>

<p>In general, people with worse mental health symptoms at the outset of the study improved more after suppression training, but only if they suppressed their fears. This finding directly contradicts the notion that suppression is a maladaptive coping process.</p>

<p>Suppressing negative thoughts did not lead to a ‘rebound’, where a participant recalled these events more vividly. Only one person out of 120 showed higher detail recall for suppressed items post-training, and just six of the 61 participants that suppressed fears reported increased vividness for No-Imagine items post-training, but this was in line with the baseline rate of vividness increases that occurred for events that were not suppressed at all. &nbsp;</p>

<p>“What we found runs counter to the accepted narrative,” said Professor Anderson. “Although more work will be needed to confirm the findings, it seems like it is possible and could even be potentially beneficial to actively suppress our fearful thoughts.”</p>

<p>Although participants were not asked to continue practising the technique, many of them chose to do so spontaneously. When Dr Mamat contacted the participants after three months, she found that the benefits in terms of reduced levels of depression and negative emotions, continued for all participants, but were most pronounced among those participants who continued to use the technique in their daily lives.</p>

<p>“The follow up was my favourite time of my entire PhD, because every day was just joyful,” she said. “I didn’t have a single participant who told me ‘Oh, I feel bad’ or ‘This was useless’. I didn't prompt them or ask ‘Did you find this helpful?’ They were just automatically telling me how helpful they found it.”</p>

<p>One participant was so impressed by the technique that she taught her daughter and her own mother how to do it. Another reported how she had moved home just prior to COVID-19 and so felt very isolated during the pandemic.</p>

<p>“She said this study had come exactly at the time she needed it because she was having all these negative thoughts, all these worries and anxiety about the future, and this really, really helped her,” said Dr Mamat. “My heart literally just melted, I could feel goosebumps all over me. I said to her ‘If everyone else hated this experiment, I would not care because of how much this benefited you!’.”</p>

<p>The research was funded by the Medical Research Council and the Mind Science Foundation.</p>

<p><em><strong>Reference</strong><br>
Mamat, Z, and Anderson, MC. <a href="https://doi.org/10.1126/sciadv.adh5292">Improving Mental Health by Training the Suppression of Unwanted Thoughts.</a> Sci Adv; 20 Sept 2023; DOI: 10.1126/sciadv.adh5292</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Luiz André Barroso has died (165 pts)]]></title>
            <link>https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</link>
            <guid>37606775</guid>
            <pubDate>Fri, 22 Sep 2023 01:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/">https://www.wired.com/story/google-mourns-luiz-andre-barroso-veteran-engineer-invented-the-modern-data-center/</a>, See on <a href="https://news.ycombinator.com/item?id=37606775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Luiz André Barroso</span> had never designed a data center before Google asked him to do it in the early 2000s. By the time he finished his first, he had overturned many conventions of the computing industry, laying the foundations for Silicon Valley’s development of cloud computing.</p><p>Barroso, a 22-year veteran of Google who unexpectedly died on September 16 at age 59, <a href="https://www.wired.com/2012/10/ff-inside-google-data-center/">built his data centers</a> with low-cost components instead of expensive specialized hardware. He reimagined how they worked together to develop the concept of “the data center as a computer,” which now underpins the web, mobile apps, and other internet services.</p><p>Jen Fitzpatrick, senior vice president of Google’s infrastructure organization, says Barroso left an indelible imprint at the company whose contributions to the industry are countless. “We lost a beloved friend, colleague and respected leader,” she writes in a statement on behalf of the company.&nbsp;</p><p>Barroso continued to lead major projects at Google, including development of <a href="https://www.wired.com/story/covid-exposure-apps-are-headed-for-a-mass-extinction-event/">its Covid exposure notifications app</a>, for which he served as a mediator across teams within the company and with outside partners. In an email Fitzpatrick sent to Google staff seen by WIRED, she wrote that it's understood Barroso died from natural causes.</p><div data-testid="GenericCallout"><figure><p><span>Luiz André Barroso</span><span>Photograph: Sebastian Kennerknecht</span></p></figure></div><p>Fitzpatrick says Barroso’s family, which includes his wife Catherine Warner, a singer for whom he sometimes <a data-offer-url="https://www.beforebossa.com/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.beforebossa.com/&quot;}" href="https://www.beforebossa.com/" rel="nofollow noopener" target="_blank">played guitar</a>, is seeking privacy. &nbsp;The cause of death could take weeks to determine, according to the medical examiner’s office of Santa Clara County, in Silicon Valley.</p><p>Barroso had wanted to be an electrical engineer since his childhood days in Brazil, where he got into amateur radio with his grandfather and earned bachelor’s and master’s degrees in electrical engineering from the Pontifical Catholic University of Rio de Janeiro. He came to the US for a doctorate in computer architecture from the University of Southern California and worked on chips at Compaq and Digital Equipment Corporation. But he came to Google in 2001 wanting to focus on software engineering.</p><p>Barroso wasn’t a coder for long—the then small startup’s few employees had to pitch in wherever help was needed. Three years after joining Google, <a href="https://www.wired.com/2017/06/google-copes-even-cant-afford-enough-gear/">Urs Hölzle</a>, the company’s first vice president of engineering, tasked Barroso with rebuilding the company's infrastructure. “I was the closest thing we had to a hardware person,” Barroso <a href="https://www.wired.com/2012/01/google-man/">recalled to WIRED in 2012</a>.</p><div><p>When he took on the infrastructure gig, internet businesses such as Google typically hosted their websites on servers in data centers maintained by another company. But these vendors couldn’t handle the surging search startup’s growing needs.</p><p>Barroso’s inexperience in data center design helped lead him to reinventing it, he wrote in <a data-offer-url="https://barroso.org/publications/IEEEMicro2021.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://barroso.org/publications/IEEEMicro2021.pdf&quot;}" href="https://barroso.org/publications/IEEEMicro2021.pdf" rel="nofollow noopener" target="_blank">an essay</a> and <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">recalled during a podcast</a> in 2021. He found himself asking “Wait, wait, wait, but why are we doing it this way?” Barroso said on the podcast. “And it just turns out that the people who had been living in that area hadn't really thought about questioning that. And sometimes it's something that was based on a good reason three years ago, and that reason had a sell-by date, and it's time to do something else.”</p></div></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Google’s first data center consisted of 40-foot, server-filled shipping containers, which enabled advanced cooling and fewer construction headaches. It opened its own data center campus in Oregon in 2006, resembling the conventional bland, boxy, and massive buildings that now dot the world. But Barroso’s ideas made the insides exceptional.</p><p>He and his Google colleagues <a data-offer-url="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf&quot;}" href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/908d5966b1fa946034e382e608999d51e70d5b22.pdf" rel="nofollow noopener" target="_blank">turned away from the then standard approach</a> of centralizing key software in a data center on a few expensive and powerful machines. Instead they began distributing Google’s programs across thousands of cheaper, mid-grade servers. That saved money spent on pricey hardware while also saving energy and allowing software to run more nimbly.</p><p>Barroso laid out his new philosophy in <a data-offer-url="https://research.google/pubs/pub41606/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://research.google/pubs/pub41606/&quot;}" href="https://research.google/pubs/pub41606/" rel="nofollow noopener" target="_blank"><em>The Datacenter as a Computer</em></a>, a book he coauthored with Hölzle that became a seminal text on modern computing infrastructure. “We must treat the data center itself as one massive warehouse-scale computer,” the book says.</p><p>The efforts of Barroso’s “speed-up” team, as he liked to call it, paid off for Google and helped establish its reputation as not just a neat search engine but also a place that broke new ground in computing. By <a data-offer-url="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf&quot;}" href="https://learning.acm.org/binaries/content/assets/leaning-center/bytecast-transcripts/acm_bytecast_luiz_andre_barroso_episode_20_mix_1---transcript.pdf" rel="nofollow noopener" target="_blank">customizing nearly every inch of Google’s data centers and the hardware within them</a>, including power supplies and cooling kits, the search giant could deliver results, emails, and other services faster—even as the “slow-down” teams integrated more algorithms and features.</p><p>“It’s easy to forget just how crazy the amount of computational data is required to be able to give you a new result every 20 milliseconds or something,” he told WIRED’s Steven Levy in 2012. “We’re essentially searching our web corpus, our images corpus, you name it, every time you do a keystroke.”</p><p>Barroso’s ideas spread quickly across Silicon Valley. Meta and other internet giants adopted an approach similar to Google's for their data centers. The architecture Barroso devised became the basis for Google’s cloud computing unit, which now accounts for about 10 percent of the company’s overall revenue.</p><p>Over the past decade, Barroso helped start the team that designed <a href="https://www.wired.com/2017/04/building-ai-chip-saved-google-building-dozen-new-data-centers/">Google’s AI chips known as TPUs</a>; led engineering for Google’s “geo” services, including the infusion of augmented reality and machine learning into Maps; and founded Google’s core unit, which manages software and other tools used across the company. He held the title of Google fellow, the company’s highest rank for technical staff. In 2020, he received the Eckert Mauchly award from the Association for Computing Machinery and the Institute of Electrical and Electronics Engineers for his contributions to computer architecture.</p><p>Barroso recently joined the board of Stone, an ecommerce company in Brazil, where the engineer was born and where he successfully pushed Google to hire more. Stone wrote in a disclosure to investors this week that Barroso “made significant contributions to our technology team and overall strategy” and that “our hearts and thoughts are with [Barroso’s] family, friends, and colleagues.” A spokesperson for the company declined further comment.</p><p>Barroso was also active in environmental projects. He served on the board of Rainforest Trust, a nonprofit for whom he organized and led a weeklong trip to Brazil's Pantanal wetlands last month. He also <a data-offer-url="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/&quot;}" href="https://www.allaboutcircuits.com/podcast/ep-27-two-google-senior-vps-of-engineering-from-shipping-containers-to-todays-data-centers/" rel="nofollow noopener" target="_blank">expressed concern</a> about <a href="https://www.wired.com/story/bitcoin-mining-guzzles-energyand-its-carbon-footprint-just-keeps-growing/">the cryptocurrency industry’s thirst for electricity</a>. Barroso had been executive sponsor for Google’s Hispanic and Latinx employee group and a program awarding fellowships to doctoral students in Latin America.</p><p>Despite all his technical achievements, Barroso told WIRED in 2012 that mentoring interns was “probably the thing I’m best at.” Google chief scientist Jeff Dean, who brought Barroso to Google in 2001 with interviews over crème brûlée, tweeted on Monday without naming his onetime research partner, “Sometimes close friends and colleagues leave us altogether too soon.”</p><p><em>Additional reporting by Steven Levy.</em></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nintendo 3DS Architecture (288 pts)]]></title>
            <link>https://www.copetti.org/writings/consoles/nintendo-3ds/</link>
            <guid>37606380</guid>
            <pubDate>Fri, 22 Sep 2023 00:31:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.copetti.org/writings/consoles/nintendo-3ds/">https://www.copetti.org/writings/consoles/nintendo-3ds/</a>, See on <a href="https://news.ycombinator.com/item?id=37606380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="imagery">Supporting imagery</h2><section><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul></section><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>As smartphones surge in adoption, the videogame market is experiencing an unusual growth led by discount App Stores and affordable development licenses. With this, one can only wonder when kids will prefer an iPhone 4 over a Nintendo DSi.</p><p>In the midst of finding out the answer, Nintendo conceives a thrilling successor to its triumphant portable system. In it, users will find old, present and unfamiliar technology - many of which can’t be replicated by smartphones.</p><p>And so, this new production of the Architecture of Consoles series will give you a profound description of how this new console works, both internally and externally.</p><h3 id="recommended-reading">Recommended reading</h3><p>If you are new to this <a href="https://www.copetti.org/writings/consoles/">article series</a>, I strongly suggest reading the <a href="https://www.copetti.org/writings/consoles/gamecube/">GameCube</a>, <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/">Nintendo DS</a> articles beforehand, as they will explain various terms and concepts referenced in this one.</p><hr><h2 id="models-and-variants">Models and variants</h2><p>Throughout the lifecycle (and struggles) of this console, Nintendo released numerous revisions in an attempt to correct its target audience and recover loyal customers.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_n3ds.305bcbf7b0cd255a64549d2027f9f6478a7b9777009c85e229f6f93b4c025cc5.webp"><picture><img alt="Image" width="1111" height="506" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_huc15dcfe92f3446e20d07697c5f45f979_25336_2089e178d0cad5b37d8d0bc91ff41544.png" loading="lazy"></picture></a><figcaption>An original Nintendo 3DS (the first generation, from 2011) next to a New Nintendo 3DS XL (the last generation, from 2015).</figcaption></figure><p>From the architectural point of view, there were a total of six different models:</p><ul><li><strong>Nintendo 3DS</strong> (2011) and <strong>Nintendo 3DS XL</strong> (2012): The debuting series featuring the original architecture. The only relevant difference between the XL and non-XL models is the screen size.</li><li><strong>Nintendo 2DS</strong> (2013): A cheaper alternative to the original Nintendo 3DS by removing the stereoscopic screen and featuring a <a href="https://www.copetti.org/writings/consoles/game-boy/">Game Boy</a>-inspired shape.</li><li><strong>New Nintendo 3DS</strong> (2014) and <strong>New Nintendo 3DS XL</strong> (2015): A re-engineering of the standard 3DS models. The ‘New’ variants exhibit an incremental hardware upgrade, an NFC reader, a larger button set and an improved stereoscopic system.</li><li><strong>New Nintendo 2DS XL</strong> (2017): The ‘New’ affordable alternative to the New Nintendo 3DS XL by omitting stereoscopic functionality.</li></ul><p>Now, for this article, the focus will be on the original Nintendo 3DS (after all, it’s the lowest common denominator for games). However, since the architectural differences of the ‘New’ series are worth studying, these will receive a dedicated section.</p><hr><h2 id="displays">Displays</h2><p>There’s only one company that keeps altering the standard structure of all my analyses, and that’s Nintendo. This time, I’ll start with the <strong>stereoscopic screens</strong> (a.k.a. ‘3D without glasses’).</p><p>First things first, the Nintendo 3DS, as a successor of the Nintendo DS, includes two LCD screens. The upper screen has a resolution of <strong>800 x 240 pixels</strong> and somehow can display images with a sense of depth. When I first read this, only questions popped into my head:</p><ul><li>What optics principles are they applying?</li><li>How is the screen designed?</li><li>How do games comply with this system?</li></ul><p>Well, here are the answers!</p><h3 id="principles">Principles</h3><p>Liked or not, the fundamentals are not so different from the <a href="https://www.copetti.org/writings/consoles/virtual-boy/">Virtual Boy</a>, which I’ve happened to analyse two years before. To recall, the Virtual Boy displays two images, one to each eye, and shows objects individually shifted from the centre. By looking at the two pictures at the same time, they are perceived as some objects are behind others (sense of depth). This is the basis of <strong>Stereoscopic Parallax</strong>.</p><figure><ul><li id="tab-1-1-left-link"><a href="#tab-1-1-left">Left</a></li><li id="tab-1-2-right-link"><a href="#tab-1-2-right">Right</a></li></ul><figure id="tab-1-1-left"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/left.5650091c8dae2d51fd18a73a5bc37e22b6d001e54d8a18a1ade37e73b8b4d14e.png" loading="lazy"></picture></a><figcaption>Left display.</figcaption></figure><figure id="tab-1-2-right"><a href="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png"><picture><img alt="Image" width="384" height="224" src="https://www.copetti.org/images/consoles/virtualboy/tennis/right.3df8c5ee7cf0e2b8b6acab7349b10cd3b4b33183427fd6a58e813228962928b6.png" loading="lazy"></picture></a><figcaption>Right display.</figcaption></figure><figcaption>Demonstration of how the Virtual Boy displayed stereoscopic imagery.<br>Mario’s Tennis (1995).</figcaption></figure><p>Now, the way the Virtual Boy executed this was a bit cumbersome: it required users to place their heads close to the eyepiece and then adjust the focal length and inter-pupil distance. 15 years later, Nintendo rightly said ‘No’ to all of that nuisance, and designed a new system where users could enjoy 3D-looking scenery without <em>considerable</em> intervention.</p><figure><ul><li id="tab-2-1-left-link"><a href="#tab-2-1-left">Left</a></li><li id="tab-2-2-right-link"><a href="#tab-2-2-right">Right</a></li></ul><figure id="tab-2-1-left"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_left.296143904d4490fa7e10203667d603c4588adf55f7b2dc0b15d6426bbbb31676.png" loading="lazy"></picture></a><figcaption>Top screen, left eye.</figcaption></figure><figure id="tab-2-2-right"><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png"><picture><img alt="Image" width="400" height="240" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/top_right.7d50fe7647f1d01ef0f864286c9e8909fee1821a91fd840fd8eda93ca934b026.png" loading="lazy"></picture></a><figcaption>Top screen, right eye.</figcaption></figure><figcaption>An example of two frames the Nintendo 3DS shows on its top screen at the same time. Looks like the fish is going to hit you. The same principle applies 15 years later.<br>Luigi’s Mansion (2018).</figcaption></figure><p>This brings us to our next question.</p><h3 id="the-special-screen">The special screen</h3><p>Take a look again at the resolution of the upper LCD screen. On paper, it says it’s <strong>800 x 240 pixels</strong> wide, which results in a ludicrous aspect ratio.</p><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_da7038c582d606c66574ccd907cc6aac.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_a8b4b2a43322fb984cdf1b2394d5a99e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud950cf4543f954b474f553674fc302a5_240147_bc329dbcc83d74e2fc1cf5fae9dfa34e.webp 1000w"><img alt="Image" width="1000" height="907" src="https://www.copetti.org/images/consoles/nintendo3ds/international.283e187450b432e5ca6f46f4bb6a495bdfc36f08cb2e1da082b020d6022d1f8e.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS again <sup id="bibref:1"><a href="#bib:photography-amos" role="doc-biblioref">[1]</a></sup>, take a closer look at its screens.</figcaption></figure><p>In reality, the physical screen is made of <strong>half-width pixels</strong> and operates in two modes:</p><ul><li><strong>Traditional/2D mode</strong>: When the stereoscopic function is disabled, groups of two horizontal pixel pairs are treated as a single one.<ul><li>Truth be told, the screen can still display a frame of 800 x 240 px, although no commercial game ever used this.</li></ul></li><li><strong>Stereoscopic/3D mode</strong>: All pixels are treated individually, and with it, the screen displays <strong>two frames</strong> of <strong>400 x 240 pixels</strong> at the same time.</li></ul><p>Moreover, to perform stereoscopic parallax, this particular LCD houses an extra layer called <strong>Parallax Barrier</strong> <sup id="bibref:2"><a href="#bib:graphics-display_teardown" role="doc-biblioref">[2]</a></sup>. These opaque shutters deviate the backlight beamed behind the pixels of the LCD, so each eye will receive the light of a different subset of pixels <sup id="bibref:3"><a href="#bib:graphics-display_howworks" role="doc-biblioref">[3]</a></sup>. The half-width pixels will also appear to be wider, thereby giving the feeling they have the traditional aspect ratio.</p><p>All in all, this recreates the original effect of the Virtual Boy without requiring controls for adjustment.</p></div><p>The technology is not perfect, however, as there are a few caveats:</p><ul><li>The parallax barrier requires extra brightness, thereby impacting the battery life.</li><li>The user must not hold the screen in a tilted position (compared to the user’s eyes). Otherwise, the user will end up seeing a confusing mix of the two parallax frames, which can be a disorienting experience. Not to mention the eyes won’t enjoy the extra fatigue.</li><li>Combining the fact the user must maintain a fixed posture while playing, and that stereoscopic parallax can tire the eyes quicker. The 3D feature, as a whole, can be an unnecessary hassle for most.</li></ul><div><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/tilt.a90f13873f8d4fb7e34e72734a026aa48c63063ed1cfdcb7be0259533b637534.webp"><picture><img alt="Image" width="1000" height="750" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu68de68eb39eeb5477ffb3e92dc8b5860_29218_11a22cc1e2532e946c972f3f59bcc0ba.png" loading="lazy"></picture></a><figcaption>My attempt to capture the tilt effect of the original 3DS. The 3D depth slider (at the right side of the screen) is all the way up, and by looking at the screen from one side, a ghosting effect appears on the top screen. This is quite eye-straining to look at in reality!</figcaption></figure><p>To remediate things, Nintendo added a slider control (called <strong>3D depth slider</strong>) to adjust the level of depth between objects. In doing so, it either increases or decreases the difference between the two frames. This was done to reduce the depth effect for people who didn’t find it enjoyable or too fatiguing.</p><p>Setting the 3D slider to the max can be disorienting at first. In my experience, my eyes eventually got focused, at which point I perceived the top LCD screen as if I were looking through a window. The main problem is that users will need to continuously shift their eyes to see the bottom screen, and the repeated action can be very straining.</p></div><p>As a side note, one can’t help but find it amusing how the graphics pipeline has gone full circle when rendering stereoscopic frames. During rendering, 3D data is projected into a 2D space, and now with the stereoscopic screen, that 2D space is displayed again as 3D. At this point, let’s just use holograms and skip the 3D projection stage altogether.</p><h4 id="a-small-update">A small update</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/new3ds.1ab0c98b61b0946592b63bc6732fcf95f0b6c312e1ce7814df84ec2cb1a8470f.webp"><picture><img alt="Image" width="900" height="158" src="https://www.copetti.org/images/consoles/nintendo3ds/stereoscopy/_hu025c37239d25a6aa4098cc524d1fdd78_10338_753f29eb89554cf724e8ee8dd01456e8.png" loading="lazy"></picture></a><figcaption>Top part of the New 3DS XL. At its centre, there’s a front camera and an infrared LED, both used for head tracking.</figcaption></figure><p>With the advent of the ‘New 3DS’ model, Nintendo revisioned their stereoscopic screen in an effort to reach enjoyability levels. In the new model, the console incorporates a face-tracking mechanism to tackle the tilting effect, so issues don’t need to worry about keeping a good head-console posture anymore.</p><h3 id="the-special-games">The special games</h3><p>Now for this system to work, games must play along (pun intended). Just like they traditionally interact with the GPU to draw frames on the display, they must now broadcast two frames of the scenery but with objects slightly shifted.</p><p>To make life easier for developers, there are official APIs that assist in this, especially for those games with 3D sceneries. These APIs help by providing routines that construct two projection matrices, the graphics pipeline then uses them to render the two slightly-shifted frames.</p><hr><h2 id="cpu">CPU</h2><p>Now that we know how the display works, let’s look at the internals of this console. If you get a hold of the motherboard, you’ll see three big chips, one being the <strong>CPU CTR</strong>. That’s the big System-On-Chip (SoC) that houses the entire system (aside from storage and RAM).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/cpu_photo.f1977dfae04cf532272eac2f256cafc4571f4bd76d90ba92e509f80065d69b41.webp"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7aff8078eb7c7ec1542a270f9f966adc.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_c5342c8a4cc85707ed9853e78e1d98bf.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_7bba4f72e3e2d7084e976dc626153693.webp 1000w"><img alt="Image" width="1000" height="534" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hue122ea6f63ebb1e353dc850f3274175f_71838_eac66c08da1275ec4651225b62bc7d48.png" loading="lazy"></picture></a><figcaption>CPU CTR next to some FCRAM</figcaption></figure><p>CPU CTR follows the design methods of previous portable consoles from Nintendo. That is, squash all your engineering into a single block. In doing so, it will reduce the production of counterfeits, protect sensible components and improve heat dissipation.</p><p>In terms of the actual CPU, Nintendo partnered again with their old friend, <strong>ARM</strong>, to produce their next-generation core. ARM’s licensing model happens to be favourable to Nintendo as they have always offered synthesisable designs, which allows Nintendo to mould to their needs (including, fitting them into a big SoC). In the end, ARM gave them a relatively antiquated with substantial upgrades. Their choice was the <strong>ARM11</strong> core, a successor of the ARM9 (featured with the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a>). More specifically, the <strong>MPCore</strong> variant, ARM’s first <strong>homogenous multi-core</strong> solution.</p><p>Using ARM’s designs, Nintendo crafted an ARM11 MPCore cluster housing <strong>two</strong> ARM11 cores <sup id="bibref:4"><a href="#bib:cpu-lioncash" role="doc-biblioref">[4]</a></sup>. Three years later, with the arrival of the ‘New’ 3DS, the SoC was expanded to contain <strong>four</strong> ARM11 cores. The effects of this will be explained in due time so, before anything else, let’s analyse what the new CPU cores offered to this console.</p><h3 id="an-iconic-industry">An iconic industry</h3><p>The ARM11 series originates from 2002, as a successor of the popular ARM9 and the short-lived ARM10.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/devices.b32e1b8964b560bbeed11ec8ae0e743efb6b9cf4163964bc086ba8aa1b804a5c.webp"><picture><img alt="Image" width="1000" height="384" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/_hucd71f9346efedab899883028b66e7ae5_21546_4af778c28e8044da94eb7237b76f5f62.png" loading="lazy"></picture></a><figcaption>A Nokia 5230 (2009), a red 3DS (2011) and a Raspberry Pi Model B (2012), all carrying an ARM11.</figcaption></figure><p>In case you haven’t heard about them before, ARM11s are best known for powering the 2006-2008 generation of smartphones (back when many of them featured a keypad or a clamshell design). If you owned a Nokia N95, 5230 or the first iPhone, you’ve used an ARM11. This also applied to many high-end cameras, GPS or similar peripherals. If you wonder, other manufacturers like RIM and Samsung held onto Intel XScale (the continuation of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/##tab-2-2-a-question-about-the-hardware-choice">StrongARM</a>, implementing the ARMv5TE instruction set) until 2009, when they made the switch to ARM11 (this is a bit ironic, considering the iPhone’s CPU was supplied by Samsung!). Last but not least, the ARM11 was the choice of the CPU for the first model of the Raspberry Pi.</p><p>Now, by the time Nintendo adopted the ARM11, its creator had already succeeded it with the Cortex-A series. This is nothing but expected, as Nintendo’s model favours cost-effectiveness over avant-garde CPUs. Look at it from another way, saving in CPU costs allows them to focus their budget on other aspects of the console, you’ll soon see.</p><h4 id="new-dialects">New dialects…</h4><p>Along with the new shiny CPUs, a new instruction set arrived, the <strong>ARMv6</strong>.</p><p>From a programmer’s perspective, the ARMv6 ISA innovates with a new set of vector instructions and multi-core support <sup id="bibref:5"><a href="#bib:cpu-thomas" role="doc-biblioref">[5]</a></sup>. The new vector set provides SIMD instructions that operate groups of <strong>four 8-bit values</strong> or <strong>two 16-bit values</strong> at the same time (using the existing 32-bit registers) <sup id="bibref:6"><a href="#bib:cpu-armcc" role="doc-biblioref">[6]</a></sup>. The new multi-core instructions consist of <code>Store</code> and <code>Load</code> opcodes with special care for synchronisation (crucial for an environment of multiple CPUs using the same memory locations) <sup id="bibref:7"><a href="#bib:cpu-sync" role="doc-biblioref">[7]</a></sup>.</p><p>All in all, this may not seem that thriving for a new chip series, but remember that ARM’s CPUs speak many ‘languages’. In the case of an ARM11-based core, you are provided with:</p><ul><li>The main 32-bit ISA, called <strong>ARMv6</strong>.</li><li>A compressed alternative called <strong>Thumb</strong>. Its instructions fit in 16-bit words instead. If you’d like to know more, I go over it in the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#whats-new">Game Boy Advance article</a>, as it weighs significant importance in that console.</li><li><strong>Jazelle</strong>, a Java bytecode interpreter, mostly forgotten and left unused. I’ve mentioned a bit of it in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii article</a>.</li><li>Any extension bundled into the core. For instance, the MPCore includes a <strong>Vector Floating-point Coprocessor</strong> with additional instructions to control said coprocessor <sup id="bibref:8"><a href="#bib:cpu-vfp" role="doc-biblioref">[8]</a></sup>.</li></ul><p>To make matters less confusing, ARM tends to package all of these with a single nomenclature. For instance, in the case of the ARM11 MPCore opcodes, ARM refers to them as the <strong>ARMv6k</strong> instruction set.</p><h4 id="and-a-fragmented-distribution">… and a fragmented distribution</h4><p>The adoption of extensions and alternative instruction sets eventually made things very convoluted for developers targeting generic ARM hardware, you only have to look at the uncountable ARM ports devised for Linux distributions.</p><p>Debian, one of the most popular distributions, tried to tackle the disparities by developing two ports in parallel:</p><ul><li><code>armel</code>: unoptimized, compatible with ARMv4T onwards.</li><li><code>armhf</code>: accelerated with VFP, but only compatible with ARMv7 onwards.</li></ul><p>Yet, with the arrival of the Raspberry Pi (powered by ARMv6 and accelerated with VFP), neither of them was deemed acceptable. Thus, an unofficial port called ‘Raspbian’ was developed to provide a VFP-accelerated version for ARMv6 CPUs <sup id="bibref:9"><a href="#bib:cpu-armhf" role="doc-biblioref">[9]</a></sup>. Even so, the trend continued: years later, with the arrival of ARMv8 and AArch64, Debian spawned yet-another port, <code>arm64</code>, optimised for the new 64-bits ISA.</p><p>I don’t remember seeing this labyrinth with x86, but at least things are now getting more orderly. AArch64 has unified many extensions and dropped alternative modes (<em>farewell, Thumb and Jazelle</em>).</p><h3 id="core-functionality">Core functionality</h3><p>That was a big deviation. Let’s go back to the 3DS CPU, the ARM11, and check what’s inside.</p><p>For this study, we can divide the ARM11 MPCore into two areas:</p><ul><li>The <strong>MP11 cores</strong> that make up the cluster.</li><li>The <strong>Advanced eXtensible Interface (AXI)</strong> bus, a new invention that interconnects the cores and interfaces with the outside world.</li></ul><p>Let’s start with the cores now and then we’ll check the AXI bus.</p><div><ul><li id="tab-3-1-the-original-mpcore-link"><a href="#tab-3-1-the-original-mpcore">The original MPCore</a></li><li id="tab-3-2-the-new-mpcore-link"><a href="#tab-3-2-the-new-mpcore">The ‘New’ MPCore</a></li><li id="tab-3-3-the-axi-bus-link"><a href="#tab-3-3-the-axi-bus">The AXI bus</a></li></ul><div><div id="tab-3-1-the-original-mpcore"><h4 id="tab-3-1-the-original-mpcore">The original MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png"><picture><img alt="Image" width="513" height="542" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_overview.593ad79da3baa7e6b91e954dab46aff1bfe8dd75567506ad3626f9412aa91533.png" loading="lazy"></picture></a><figcaption>Overview of the ARM11 MPCore CPU cluster</figcaption></figure><p>The first ARM11 MPCore variant, which debuted with the original 3DS, includes two cores. Each is called <strong>MP11</strong> and runs at <strong>268 MHz</strong> <sup id="bibref:10"><a href="#bib:cpu-lioncash" role="doc-biblioref">[10]</a></sup>.</p><p>Apart from implementing the ARMv6k instruction set, the CPU features an <strong>8-stage pipeline</strong> <sup id="bibref:11"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[11]</a></sup>. Furthermore, the core provides <strong>two levels of branch prediction</strong>, ‘dynamic’ (based on previous executions) and ‘static’ (based on the current instruction alone). Overall, both enhancements will be quickly noticed, considering the 5-stage ARM9 couldn’t predict a thing!</p><p>Additionally, since the ARM946E-S CPU, ARM has been fitting a <strong>System Control Coprocessor</strong> called <strong>CP15</strong>. This time, it provides <strong>Memory-Management</strong> (MMU functions) and registers that output information about the MPCore cluster.</p><p>Now, there’s no more <strong>Tightly-Coupled Memory</strong> (TCM). There are however <strong>16 KB of instruction cache</strong> and <strong>16 KB of data cache</strong>, this change of model resembles other systems of the same generation. If you are curious, this L1 cache is 4-way set associative.</p><p>Finally, each core houses a co-processor called <strong>Vector Floating-point Coprocessor</strong> (also known as ‘VFP11’). This accelerates arithmetic operations with floating-point numbers, both 32-bit single-precision (a.k.a. <code>float</code>) and 64-bit double-precision (a.k.a. <code>double</code>) ones <sup id="bibref:12"><a href="#bib:cpu-vfp" role="doc-biblioref">[12]</a></sup>. It’s not a big coprocessor though, as its register file is composed of 32 32-bit registers, so doubles will consume two registers. In any case, this processor implements the <strong>VFPv2 instruction set</strong> and follows the <strong>IEEE 754</strong> standard. The latter is a welcomed decision, considering the architecture of <a href="https://www.copetti.org/writings/consoles/playstation-2/#the-leader">previous generations</a>.</p></div><div id="tab-3-2-the-new-mpcore"><h4 id="tab-3-2-the-new-mpcore">The ‘New’ MPCore</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png"><picture><img alt="Image" width="960" height="624" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/mpcore_new_overview.73042809fa9f8f34cbea6e462ef0d89128a814b522968e63bb28ae4e3a00b9e1.png" loading="lazy"></picture></a><figcaption>Overview of the ‘New’ CPU cluster</figcaption></figure><p>With the arrival of the New 3DS in 2014, a new SoC was included (<strong>CPU LGR</strong>) and with it, a luxurious CPU upgrade.</p><p>The most apparent change is that we have now <strong>four MP11 cores</strong> instead of two. The consequences of this, however, are not simple to disseminate, but we’ll see them in due time.</p><p>The second change is that the CPU incorporates <strong>2 MB of L2 cache</strong> shared between the four cores. This type of cache is 16-way associative, which anticipates four cores accessing it at the same time. If you’d like to know more, I went over associative caches with the <a href="https://www.copetti.org/writings/consoles/xbox-360/#shared-cache">Xbox 360 article</a>.</p><p>Moving on, all cores now run at <strong>804 MHz</strong> (three times the original speed, which will certainly raise a few eyebrows).</p></div><div id="tab-3-3-the-axi-bus"><h4 id="tab-3-3-the-axi-bus">The AXI bus</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png"><picture><img alt="Image" width="510" height="600" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/axi.7e49c2473554d83f6944120a017622217417d3b8704a631434e0b4e609048377.png" loading="lazy"></picture></a><figcaption>Example of how the AXI protocol interconnects different types of components</figcaption></figure><p>Whether there are two or four cores, all of these are connected using a specialised bus, proudly authored by ARM, called the <strong>Advanced eXtensible Interface</strong> (AXI). This protocol is part of the AMBA3 model, a successor of the original AMBA revision that we’ve seen in the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#internal-interfaces">Wii U</a> (both housing an ARM9 CPU).</p><p>Generally speaking, the AMBA model provides a set of protocols for connecting components with distinct bandwidth requirements using a <strong>bus topology</strong>. Compare this to the token-ring model of the <a href="https://www.copetti.org/writings/consoles/playstation-3/#inside-cell-the-heart">PlayStation 3</a> or the mesh solution made for the <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-the-messenger">Xbox 360</a>. All of these consoles shared the same problem, but each came up with different solutions, neither better nor worse, just different.</p><p>Following AMBA’s methodologies for interconnecting components, there will be a master-slave hierarchy imposed to maintain order. The master components (typically, the CPU cores) will be the ones sending commands to the slaves (i.e.&nbsp;memory and I/O blocks).</p><p>Now, as part of the AMBA3 specification, ARM offered the AXI model as a critical ingredient for building <strong>System On Chips</strong> (SoC). Instead of using a single bus, AXI uses a dedicated block (called <strong>AXI interconnect</strong>) acting as a <strong>bus matrix</strong> <sup id="bibref:13"><a href="#bib:cpu-axi" role="doc-biblioref">[13]</a></sup>, this is connected to every single component using <strong>64-bit dedicated buses</strong> <sup id="bibref:14"><a href="#bib:cpu-arm11_overview" role="doc-biblioref">[14]</a></sup>. In doing so, AXI overcomes the limitations of high-bandwidth components sharing the same bus (as it happened with the <a href="https://www.copetti.org/writings/consoles/playstation-2/#cpu">PlayStation 2</a>). Moreover, multiple master devices can communicate with slave nodes using separate channels to avoid waiting for other masters to finish. Finally, traditional enhancements like <a href="https://www.copetti.org/writings/consoles/gamecube/#ibms-enhancements">burst transactions</a> are implemented, from which the MP11 cores take advantage.</p><p>In the case of the 3DS, the AXI interconnect is housed in a bigger block called <strong>Snoop Control Unit</strong> (SCU) that also takes care of automatically maintaining L1 cache coherency between the MP11 cores.</p></div></div></div><h3 id="any-other-cpus">Any other CPUs?</h3><p>Up to this moment, I’ve been talking about the MPCore as if it were the only CPU in this system, the reason being mixing up distinct CPUs for this analysis can turn it into an incomprehensible essay. That is, until now.</p><p>The truth is, Nintendo had extra requirements for this console. They wanted a proper security system, but also the possibility to turn the console into a <strong>Nintendo DSi or a GBA</strong> on-the-fly. So, for all of that, they ended up bundling <strong>three distinct CPU packages</strong> - one being the mentioned ARM11. The other two are well hidden, in the sense that games are completely unaware of them. In fact, 3DS emulators like Citra don’t care about them either <sup id="bibref:15"><a href="#bib:cpu-citra_cpu" role="doc-biblioref">[15]</a></sup>.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/side_ds.3eba2a2f3a4631917db480bd5069986d1298ec9182c0806ba209e2b55dd0eb09.webp"><picture><img alt="Image" width="1111" height="530" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hufe4dbc5506e4ce358fb18d5a09cafc51_22792_04c9dc378631b6c66caa6384f2d190a3.png" loading="lazy"></picture></a><figcaption>The Nintendo 3DS next to a predecessor (a Nintendo DS Lite), the latter has become a common denominator.</figcaption></figure><p>But we do! Here’s the complete list of CPUs this system houses:</p><ul><li>The <strong>ARM11 MPCore</strong> we’ve just seen.</li><li>An <strong>ARM946E-S</strong> from the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS days</a>. It’s treated as a secret co-processor and it’s only managed by the operating system. Alternatively, it becomes the main processor whenever a DS or DSi game is executed.<ul><li>Thanks to its bundled CP15 co-processor, there’s a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU) in place. This will protect the CPU from arbitrarily executing code from any location in memory.</li></ul></li><li>An <strong>ARM7TDMI</strong> from the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance days</a>. It’s a relatively ignored CPU, unless a DS or DSi game is being played, in which case it acts as a co-processor. However, on the special occasion when a Game Boy Advanced game is running, the main execution falls into this CPU.</li></ul><p>Unfortunately, or for obvious reasons, the three CPUs are never usable at the same time. Instead, the console has three modes of operation:</p><ul><li><strong>Native 3DS mode</strong>: The ARM11 MPCore executes a 3DS game while the ARM946E-S deals with I/O and security. The ARM7, on the other side, is switched off.</li><li><strong>Nintendo DSi mode</strong>: The ARM946E-S and ARM7TDMI operate in a multi-processor configuration to execute a Nintendo DS or DSi game. Just like with its predecessor, the ARM7TDMI has greater access to I/O. Meanwhile, the ARM11 MPCore will be working in the background to replicate missing and re-located DS hardware (real-time clock, power management, keypad, GBA/DS PPU display and so forth).</li><li><strong>Game Boy Advance mode</strong>: The ARM7TDMI is the only CPU executing instructions (in 99% of cases, that will come from a GBA game). The ARM11 MPCore and ARM9, both still operating within the capacities of ‘Native 3DS mode’, will be working in the background.</li></ul><p>If you stop to think about it, the Nintendo 3DS ends up housing four processors in total (Two MP11 cores + one ARM9 + one ARM7), or the absurd amount of six in the case of the New 3DS. How convoluted is that? Luckily, this system didn’t suffer the complications of the <a href="https://www.copetti.org/writings/consoles/sega-saturn/#cpu">Sega Saturn</a> and you can thank Nintendo and ARM’s engineering for that. After all, 3DS developers only had to deal with the MPCore.</p><p>Since the ARM9 and ARM7 are predominantly for I/O, security and backwards compatibility (neither of which require the developer’s awareness), I discuss them in later sections of this article. But if you’d like to know more about the design of the ARM7 and ARM9, I wrote about them in previous articles (the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#cpu">Game Boy Advance</a> and <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#cpu">Nintendo DS</a> ones, respectively).</p><h4 id="multi-core-communication">Multi-core communication</h4><p>I guess the question now is, how can CPUs and cores talk to each other? Well, the easiest way is to share RAM… but you could also try a more efficient approach, depending on the cores trying to communicate:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png"><picture><img alt="Image" width="1281" height="288" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/inter_core.bb00bae6a6b8bc25be2a95f6c369baac03e16a1fcb64ff5029ed70f1eb5ac0a6.png" loading="lazy"></picture></a><figcaption>Representation of the communication channels each CPU is provided with.</figcaption></figure><ul><li>With <strong>inter-core ARM11 communication</strong>, a core can send interrupts to another core by writing on its <code>Software Interrupt Register</code> <sup id="bibref:16"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[16]</a></sup>.</li><li>In the case of <strong>ARM11↔︎ARM9</strong> or <strong>ARM9↔︎ARM7 communication</strong>, the same <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#interconnection">FIFO model</a> from the Nintendo DS is implemented. Plus, the ARM11↔︎ARM9 FIFO is also called ‘PXI’ <sup id="bibref:17"><a href="#bib:cpu-korth" role="doc-biblioref">[17]</a></sup>.</li></ul><h3 id="memory-available">Memory available</h3><p>Having three different CPUs also means the memory layout will not be simple, especially if you care about security.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png"><picture><img alt="Image" width="621" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/cpu/memory.35bd0fe628c283231e7d16616a17ceaf4fd5d51e9dbca30ab8d8d10a0bfdba65.png" loading="lazy"></picture></a><figcaption>Overview of memory organisation on the Nintendo 3DS.</figcaption></figure><p>To make a long story short, we’ve got the following blocks:</p><ul><li>From the developer’s perspective, the system provides <strong>128 MB of FCRAM</strong>. The New 3DS increased this to <strong>256 MB</strong>. The rest is redundant for games.</li><li>For predominantly security reasons, the ARM11 is also provided with a fast block of <strong>512 KB of SRAM</strong>. The ARM9 is also given a block of <strong>1 MB of SRAM</strong> (<strong>1.5 MB</strong> in the case of the New 3DS).</li><li>By inheriting the model of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-1-2-arm946e-s">Nintendo DS</a>, the ARM9 also houses <strong>Tightly-Coupled Memory</strong> (TCM). Particularly, there’s <strong>32 KB for instructions</strong> and <strong>16 KB for data</strong>.</li></ul><h4 id="a-new-type-of-memory-spotted">A new type of memory spotted</h4><p>It’s all jolly that the Nintendo 3DS includes 32 times the general-purpose memory of its predecessor, but what about that ‘FCRAM’? Is it any different from the other standards?</p><p>Well, <strong>Fast Cycle DRAM</strong> (FCRAM) is yet another RAM invention, this time authored in 2002 by Fujitsu and Toshiba. Presented as an alternative to DRAM-based technology (i.e.&nbsp;<a href="https://www.copetti.org/writings/consoles/xbox/#memory-layout">SDRAM</a>, <a href="https://www.copetti.org/writings/consoles/playstation/#the-offering">EDO DRAM</a>, <a href="https://www.copetti.org/writings/consoles/nintendo-64/#memory-design">RDRAM</a>, etc.), FCRAM excels on non-continuous reads, where it exhibits a lower latency than DRAM <sup id="bibref:18"><a href="#bib:cpu-fcram1" role="doc-biblioref">[18]</a></sup>. This was done to replicate the performance offered by the more expensive SRAM.</p><p>FCRAM competes directly with DDR DRAM by offering a revamped design of the memory arrays. In place of adding more circuitry on top of it, arrays are split into smaller subblocks, which are then accessed using a 3-stage pipeline <sup id="bibref:19"><a href="#bib:cpu-fcram2" role="doc-biblioref">[19]</a></sup>. In doing so, reading and writing on random locations become faster. These changes are still designed with backwards compatibility in mind. Thus, FCRAM is compatible with DDR DRAM controllers (hence, its full name is ‘DDR FCRAM’).</p><h3 id="faster-memory-transfers">Faster memory transfers</h3><p>The inventors of the MPCore and the AMBA bus happen to also offer a brand of <a href="https://www.copetti.org/writings/consoles/playstation/#taking-over-the-cpu">DMA controllers</a> called <strong>CoreLink</strong>, with Nintendo being a loyal client. So, it’s no mystery as to why the 3DS bundles multiple blocks of <strong>CoreLink DMA-330</strong> into their SoC <sup id="bibref:20"><a href="#bib:cpu-korth" role="doc-biblioref">[20]</a></sup>.</p><p>These DMAs in particular are attached to an AXI bus and act as master devices. They can transfer data between two slaves interconnected with the AMBA protocol (either AXI or the slower APB) with the following advantages:</p><ul><li>Faster transfer rates compared to either CPU.</li><li>Support of up to eight channels (eight transfers at the same time) <sup id="bibref:21"><a href="#bib:cpu-corelink" role="doc-biblioref">[21]</a></sup>.</li></ul><p>To be precise, Nintendo fitted one CoreLink DMA next to the ARM9, this is referred to as <strong>XDMA</strong> and provides <strong>up to four channels</strong>. There’s another DMA next to the ARM11 block, this time called <strong>CDMA</strong>, which provides <strong>up to eight channels</strong>. With the arrival of the New 3DS, another CoreLink DMA-330 is fitted next to the ARM11 block (now a quad-core cluster).</p><h3 id="programming">Programming</h3><p>With all being said, how do you program a system featuring this unorthodox CPU arrangement? To be fair, unusual systems are no strangers to videogame developers. But in this case, <strong>3DS programmers only have access to the ARM11 MPCore</strong>. Furthermore, once you reach the ‘Operating System’ section, you’ll learn the abilities with this cluster are further restricted.</p><p>In any case, no matter the console revision, programmers base their algorithms on the <strong>multi-threading model</strong>: the program groups sequences of instructions using <strong>threads</strong>, these are then dispatched by the operating system to the physical cores, as the former deems fit. Once a novelty for <a href="https://www.copetti.org/writings/consoles/xbox-360/#inside-xenon-programming-styles">Xbox 360 software</a>, this standard provides a layer of abstraction that blinds the developers from writing software only compatible with a fixed number and type of CPU cores.</p><h4 id="dealing-with-the-new-hardware">Dealing with the ‘New’ hardware</h4><p>Since the New 3DS diverts considerably from the original specification, Nintendo set up a thin compatibility layer to enable old 3DS games to work with the new hardware without manual intervention.</p><p>In essence, when a game is launched from a New 3DS console, the game’s code specifies if it’s specifically targeting the new models or not <sup id="bibref:22"><a href="#bib:cpu-applet_manager" role="doc-biblioref">[22]</a></sup>. If it is, the operating system will proceed to activate all the novelties (faster clock speed, extra RAM and use of L2 cache) for that game to enjoy. If it’s not, the operating system will keep its exclusive hardware deactivated until the user exits the game, so the game can safely assume it’s running on the old hardware and will do so without issue.</p><p>To keep supporting the old 3DS, games can be packaged with two codebases (one for the ‘New’ model and the other for the ‘Old’ one). It’s up to the game studios to decide whether to support the old and new 3DS, or only the new 3DS.</p><p>You may be wondering what happens with the rest of the exclusive hardware the New 3DS houses (i.e.&nbsp;extra ARM11 cores and DMA). Well, to properly understand the rationale, I explain this once you reach the ‘Operating System’ section, but I’m afraid you won’t like the answer!</p><hr><h2 id="graphics">Graphics</h2><p>Next to a new CPU is always a modern GPU. So, what kind of <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Picture Processing Unit</a> did Nintendo build this time? To tell the truth, none. For the first time in their portable line, <strong>they resorted to a GPU supplier</strong>.</p><p>Nevertheless, the requirements of Nintendo haven’t shifted. The company still wanted a chip with acceptable performance… and the <strong>intellectual property</strong>. This will allow them to embed the GPU into their SoC, in the same way they did with the ARM CPUs.</p><figure><ul><li id="tab-4-1-kart-link"><a href="#tab-4-1-kart">Kart</a></li><li id="tab-4-2-sonic-link"><a href="#tab-4-2-sonic">Sonic</a></li><li id="tab-4-3-mario-link"><a href="#tab-4-3-mario">Mario</a></li><li id="tab-4-4-animal-link"><a href="#tab-4-4-animal">Animal</a></li><li id="tab-4-5-zelda-link"><a href="#tab-4-5-zelda">Zelda</a></li></ul><figure id="tab-4-1-kart"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_hu5324041165f0a097655cc1fd8a33a74b_77731_31339cf3bb9271013ab1a183d3738ff6.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_kart.f3aee78ccef9f74a20c423ce9ea8803dd71a28438cbf4c9f0aed0ed8e3bca596.png" loading="lazy"></picture></a><figcaption>Mario Kart 7 (2011)</figcaption></figure><figure id="tab-4-2-sonic"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/sonic.c943fd06acd604456c0cc247d4364116870b667443bf96ad3a023a50616c0a27.png" loading="lazy"></picture></a><figcaption>Sonic Generations (2011)</figcaption></figure><figure id="tab-4-3-mario"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/mario_bros.5d75761bef28df652709a0ad18ed54be183bd72e52c93558e78c9fddb6a2cbf6.png" loading="lazy"></picture></a><figcaption>New Super Mario Bros.&nbsp;2 (2012)</figcaption></figure><figure id="tab-4-4-animal"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/animal_crossing.7aded94c435aa5cce95ad2fe5395a7c699c3ad91af694f7cae42d43ac377aee5.png" loading="lazy"></picture></a><figcaption>Animal Crossing: New Leaf (2012)</figcaption></figure><figure id="tab-4-5-zelda"><a href="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/games/_huc7d8aa9f2330731c61256ecd3c86f738_72234_d95f355bd8ccd2a1150852bb8d302ac0.webp 400w"><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/games/zelda.d59e2d38f265b4ad9aca33e5213c11cbdf3313b468768abe4bc0914de2be3d68.png" loading="lazy"></picture></a><figcaption>The Legend of Zelda: Majora’s Mask 3D (2015)</figcaption></figure><figcaption>Example of Nintendo 3DS games. All render two frames of 400 x 240 pixels and one frame of 320 x 240 pixels.</figcaption></figure><p>Meanwhile, a potential candidate just finished unveiling their new invention at SIGGRAPH 2006 <sup id="bibref:23"><a href="#bib:graphics-dmp_insight" role="doc-biblioref">[23]</a></sup>. For some time, <strong>Digital Media Professionals Inc.</strong> (also known as ‘DMP’) have been building affordable GPUs for the embedded market and, while their chips are nothing out of the ordinary, they guarantee decent OpenGL ES support. Furthermore, their licensing framework offers <strong>synthesisable GPUs</strong>.</p><p>This seemed enough for Nintendo, who happily negotiated a license for DMP’s latest core, the <strong>PICA200</strong> and subsequently bundled it on CTR CPU (the Nintendo 3DS’ SoC). The GPU runs at <strong>268 MHz</strong>.</p><h3 id="architecture-of-the-pica200">Architecture of the PICA200</h3><p>If I had to summarise it in one sentence, the PICA200 is a budget low-power 3D processor that combines a pre-<a href="https://www.copetti.org/writings/consoles/xbox-360/#a-new-foundation-on-the-way">unified architecture</a> with a modernised API. The underlying architecture of the PICA200 is called <strong>Maestro 2G</strong> <sup id="bibref:24"><a href="#bib:graphics-siggraph" role="doc-biblioref">[24]</a></sup> and its design is compliant with <strong>OpenGL ES 1.1</strong>, but extended with elements from <strong>OpenGL ES 2.0</strong> <sup id="bibref:25"><a href="#bib:graphics-nintendo_gl" role="doc-biblioref">[25]</a></sup>. However, the PICA200’s APIs are not limited to either standard.</p><p>You see, even though the pipeline is segregated and the pixel stage is fixed-function (ala <a href="https://www.copetti.org/writings/consoles/playstation-2/#graphics">PlayStation 2</a>), DMP expanded the limited circuitry with a set of <strong>Maestro functions</strong> that provide capabilities beyond the expectations of the embedded market <sup id="bibref:26"><a href="#bib:graphics-ocp" role="doc-biblioref">[26]</a></sup>. This includes fragment lighting, multiple shadowing algorithms, polygon subdivision, bump mapping, procedural textures and many fog effects.</p><p>Additionally and in contrast to the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-5-3-result">Nintendo DS</a>, the PICA200 <strong>only works with framebuffers</strong>. That’s it. The <a href="https://www.copetti.org/writings/consoles/nes/#graphics">sprite engine</a>, a popular workaround to tackle unaffordable memory requirements, is now a thing of the past. This also includes <a href="https://www.copetti.org/writings/consoles/nes/#secrets-and-limitations">scan-line tricks</a>, as contemporary GPUs work way faster than the refresh rate of a CRT.</p><h4 id="organising-the-content">Organising the content</h4><p>Now that we know that this console can draw 3D shapes, the question now is: where does it store its materials? There are two locations, the large <strong>FCRAM</strong> block and the smaller but faster <strong>VRAM</strong>.</p><p>Nintendo only provided <strong>6 MB of VRAM</strong> exclusively for the GPU. Ideally, programmers would fit as much as they can there, but since it will fill up pretty quickly, it is expected to be used to store data that needs instant access (i.e.&nbsp;commands, buffers and recurrent textures) while placing the rest on FCRAM. The PICA200 comes with a <strong>DMA unit</strong> that can transfer data between FCRAM and VRAM. So, at the end of the day, it’s the responsibility of the programmer to come up with an efficient placement to avoid bottlenecks.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png"><picture><img alt="Image" width="939" height="605" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/content.6bb4820e85f8f07d78fa991cbd9cb8eb5f695a045b6ae921d87f081f5f592e84.png" loading="lazy"></picture></a><figcaption>Example of how data is organised across the memory available.</figcaption></figure><p>During rendering, programmers allocate dedicated render buffers (i.e.&nbsp;frame, stencil, depth, etc.) for many operations. That’s always been the case. With the 3DS, alongside these buffers, programmers are also expected to reserve extra space for <strong>Display buffers</strong>, these are bound to the physical screens. The 3DS requires to allocate <strong>three Display buffers</strong> (two for the stereoscopic upper screen and one for the bottom one). To give you an idea, the display process works as follows:</p><ol><li>The LCD continuously displays the content of the front (active) Display buffer, as instructed by the value of the buffer index.</li><li>Meanwhile, the GPU finishes rendering geometry in a framebuffer.</li><li>The framebuffer is exported to the back (inactive) Display buffer.</li><li>The GPU swaps the index of the front Display buffer.<ul><li>For practical reasons, the index swap should happen at the end of <a href="https://www.copetti.org/writings/consoles/nes/#tab-5-5-result">Vertical Sync</a> to avoid tearing down the picture <sup id="bibref:27"><a href="#bib:graphics-opengl_swap" role="doc-biblioref">[27]</a></sup>. The official APIs provide synchronisation functions to keep all operations at the correct pace.</li></ul></li><li>The LCD will now be scanning the recently updated Display buffer from now on.</li></ol><h4 id="adopting-open-standards">Adopting open standards</h4><p>On an interesting note, just like the ARM11 MPCore adopts ARM’s AXI protocol for interconnecting its cores, DMP adopted a less-proprietary option called <strong>Open Core Protocol</strong> (OCP) <sup id="bibref:28"><a href="#bib:graphics-ocp" role="doc-biblioref">[28]</a></sup>. As its name indicates, the Open Core protocol does not impose any licensing restrictions on its users, something that vendors using the PICA200 may find advantageous. For comparison purposes, AXI was released in 2003 (along with the AMBA 3 specification) while OCP was published in 2001. It does make me wonder what kind of technology Nintendo fitted to adapt the OCP signal coming from the PICA200 into an AXI-compliant signal, so the rest of the SoC understands it. I assume that there’s a bridge between the PICA200 and the AXI bus.</p><p>Interestingly enough, the predecessor of the PICA200, the ULTRAY2000, shares many similarities with its successor. The most notable difference, however, is that the data interfaces use the PCI and DDR-SDRAM protocols instead <sup id="bibref:29"><a href="#bib:graphics-hardware" role="doc-biblioref">[29]</a></sup>.</p><h3 id="constructing-the-frame">Constructing the frame</h3><p>Naturally, the GPU is not aware of the stereoscopic or dual-screen nature of the displays, it will only be tasked with rendering three screens during gameplay:</p><ul><li><strong>Top stereoscopic-left</strong>: 400 x 240 pixels wide.</li><li><strong>Top stereoscopic-right</strong>: 400 x 240 pixels wide.</li><li><strong>Bottom</strong>: 320 x 240 pixels wide.</li></ul><p>All of them can display 8-bit RGB colours, which equates to up to 16.78 million colours.</p><p>Considering players will expect acceptable frame rates on all three screens (especially on the first two), the single PICA200 will be subject to high amounts of workload throughout its operation, an important aspect to remember when judging its performance.</p><p>That being said, here is an overview of how data travels to draw a single frame:</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png"><picture><img alt="Image" width="548" height="479" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline.1242abb39bcb4fa2d51bfbaf5325b31977b76e5f0542e8847a06d84a447de79e.png" loading="lazy"></picture></a><figcaption>Overview of the graphics pipeline in PICA200.</figcaption></figure><p>… and as customary in this series of articles, we’ll now take a look at what happens at each stage.</p><div><ul><li id="tab-5-1-commands-link"><a href="#tab-5-1-commands">Commands</a></li><li id="tab-5-2-vertex-link"><a href="#tab-5-2-vertex">Vertex</a></li><li id="tab-5-3-geometry-link"><a href="#tab-5-3-geometry">Geometry</a></li><li id="tab-5-4-rasteriser-link"><a href="#tab-5-4-rasteriser">Rasteriser</a></li><li id="tab-5-5-fragment-link"><a href="#tab-5-5-fragment">Fragment</a></li><li id="tab-5-6-post-processing-link"><a href="#tab-5-6-post-processing">Post-processing</a></li></ul><div><div id="tab-5-1-commands"><h4 id="tab-5-1-commands">Commands</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png"><picture><img alt="Image" width="548" height="392" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/command.1772409bf9d3938e254256094ca5ce70533f1645a965f019601248ced7fda6a7.png" loading="lazy"></picture></a><figcaption>Overview of the command stage.</figcaption></figure><p>This is Nintendo’s first portable console to finally draw triangles in ‘the usual way’. That is, with the use of commands. But it’s not a surprising factor, as the PICA200 is expected to abide by the teachings of OpenGL ES.</p><p>In essence, the PICA200 draws polygons by reading a <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-1-commands">command buffer</a> <sup id="bibref:30"><a href="#bib:graphics-nintendo_gpu_reg" role="doc-biblioref">[30]</a></sup>. Furthermore, the vertex data can either be embedded within the command or stored in a separate buffer in VRAM, with the latter being the most efficient.</p></div><div id="tab-5-2-vertex"><h4 id="tab-5-2-vertex">Vertex</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/vertex.d863f782cc69f3351251f43b8bbf5f9d05d04d6e1ac080e7cebc7cf336a4424a.png" loading="lazy"></picture></a><figcaption>Overview of the vertex stage.</figcaption></figure><p>The PICA200 provides <strong>four Vertex Processors</strong> (VP) that operate in parallel. However, if the geometry shader (the next pipeline stage) is activated, only <strong>three</strong> processors can be utilised.</p><p>Each core computes 96-bit vectors made of four 24-bit floating-point values <sup id="bibref:31"><a href="#bib:graphics-picasso" role="doc-biblioref">[31]</a></sup>, but unlike the ARM11’s VFP, they don’t comply with IEEE-754 <sup id="bibref:32"><a href="#bib:graphics-shader_isa" role="doc-biblioref">[32]</a></sup>. The vertex processors are programmed using assembly language specific to the PICA200 (reminiscent of the days of the <a href="https://www.copetti.org/writings/consoles/xbox/#graphics">Nvidia NV30</a>) and are operated as follows <sup id="bibref:33"><a href="#bib:graphics-game-vertex" role="doc-biblioref">[33]</a></sup>:</p><ol><li>Developers write the vertex shader using PICA200 assembly. For reference, the instruction set is very similar to Microsoft’s <code>vs_2_0</code> <sup id="bibref:34"><a href="#bib:graphics-vs2" role="doc-biblioref">[34]</a></sup>.</li><li>The shader is compiled using a proprietary assembler.</li><li>The 3DS program must copy the compiled binary to memory (either FCRAM or VRAM).</li><li>Then, the 3DS program issues a GPU command to load the binary and connect it with the program.</li></ol><p>Once the vertex cores finish processing, they output the results to the <strong>Sync Control</strong> block, which acts as a vertex cache and buffer. It has a capacity of <strong>384 Bytes</strong>, enabling it to hold up to 32 96-bit vectors. Finally, the next stage reads from this block.</p></div><div id="tab-5-3-geometry"><h4 id="tab-5-3-geometry">Geometry</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png"><picture><img alt="Image" width="885" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/geometry.0789eaa32552820dd323d1b845f38aa592644f3aa2df21051a90013cad21a0c5.png" loading="lazy"></picture></a><figcaption>Overview of the geometry stage.</figcaption></figure><p>The geometry stage is a signature feature of 8th-generation consoles, allowing developers to spawn complex geometry out of simple vertex data.</p><p>In this case, the PICA200’s geometry stage is implemented by <strong>stealing one of the four Vertex Processors</strong>. Then, the ‘geometry’ vertex core is loaded with a different vertex shader. Finally, it receives the vertex data from the three other processors.</p><p>Even though the geometry shader is programmable, in practice, <strong>Nintendo doesn’t allow this</strong>. Thus, game developers can only choose from a pre-programmed set of geometry shader programs (found in the SDK). Examples of available geometry shaders include square and line generation (using point primitives), geometry subdivision, silhouette edge rendering; and random particle generation.</p></div><div id="tab-5-4-rasteriser"><h4 id="tab-5-4-rasteriser">Rasteriser</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png"><picture><img alt="Image" width="428" height="345" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/rasteriser.f08b4b3d24323cb39f25cf1bc0480ed1229a281438210c64066e59fb6283c992.png" loading="lazy"></picture></a><figcaption>Overview of the rasteriser stage.</figcaption></figure><p>At this stage, all primitives are converted into pixels.</p><p>The rasterizer unit on the PICA200 is very simple, it just generates triangles out of primitives, then applies culling and clipping to remove unseen triangles (hidden behind others and/or outside the view area, respectively). This is all very similar to OpenGL ES’ modus operandi, albeit developers have to watch out for some coordinate systems that are inverted when working with the PICA200.</p></div><div id="tab-5-5-fragment"><h4 id="tab-5-5-fragment">Fragment</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png"><picture><img alt="Image" width="705" height="482" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/fragment.95c8b5fb5695c54a540c32f09ae215f2e5e1274421e46761750fb3e7266fc49c.png" loading="lazy"></picture></a><figcaption>Overview of the fragment stage.</figcaption></figure><p>The fragment stage is made of two areas: the <strong>texture units</strong>, which can fetch textures in memory and process them. And the <strong>shading unit</strong>, which can perform extra operations on the texture data.</p><p>The PICA200 contains <strong>four</strong> texture units <sup id="bibref:35"><a href="#bib:graphics-fragment" role="doc-biblioref">[35]</a></sup>, each houses <strong>256 Bytes of L1</strong> cache and all of them share <strong>8 KB of L2</strong> cache. However, the units are not homogenous. Instead, the range of services varies between each unit <sup id="bibref:36"><a href="#bib:graphics-pica_pipeline_diagram" role="doc-biblioref">[36]</a></sup>:</p><ul><li>Only three units can process 2D textures.</li><li>Only one unit can perform shadow, cube and <a href="https://www.copetti.org/writings/consoles/playstation-portable/#tab-2-4-textures">projective texture</a> mapping.</li><li>The last unit is more of a noise generator, meaning it only outputs <strong>random textures</strong>. It uses a combination of a random number generator and a colour lookup table. This is a slender yet efficient way of implementing <a href="https://www.copetti.org/writings/consoles/playstation-2/#infinite-worlds">procedure generation</a> with textures, saving bandwidth along the way.</li></ul><p>Afterwards, it’s the job of the shading unit to creatively fiddle with the textures coming in. However - and something unexpected considering we’re talking about an 8th-generation console - is that the PICA200’s unit is <strong>not programmable with <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-3-pixel">pixel shaders</a></strong> <sup id="bibref:37"><a href="#bib:graphics-kazakov" role="doc-biblioref">[37]</a></sup>. Instead, we find six <strong>configurable colour combiners</strong>, each combiner receives three RGB or Alpha values and performs a logical operation on them. The result is passed to the next combiner and so forth. Each colour combiner can get its input from the previous combiner (except the first), a texture unit or a constant value.</p><p>All in all, a modern reflection of the <a href="https://www.copetti.org/writings/consoles/gamecube/#tab-1-3-texture">Flipper era</a> (while abiding by the OpenGL specification <sup id="bibref:38"><a href="#bib:graphics-glTexEnv" role="doc-biblioref">[38]</a></sup>), but don’t forget developers may also combine this with the aforementioned Maestro functions.</p></div><div id="tab-5-6-post-processing"><h4 id="tab-5-6-post-processing">Post-processing</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png"><picture><img alt="Image" width="956" height="527" src="https://www.copetti.org/images/consoles/nintendo3ds/gpu/pipeline/post.23c51da1a21d56daf18466fbb6c4373d4b9b47d82f41956f636b096d7e6b8eca.png" loading="lazy"></picture></a><figcaption>Overview of the post-processing stage.</figcaption></figure><p>After the frame is processed and ready to be written into the framebuffer (or <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-6-4-pixel-shader">render targets</a>), it goes through a sequence of final ‘corrections’. This is similar to the OpenGL ES 2.0’s pipeline.</p><p>That being said, the frame goes through <strong>alpha</strong>, <strong>stencil</strong> and <strong>depth</strong> testing. Afterwards, the result can be mixed with an existing frame (in the framebuffer) using the colour blender or logical operators (AND, XOR, etc.). Finally, the frame is written into the assigned buffer in memory either as a whole or through a stencil filter (for masking).</p><p>For additional smoothing of the edges, the PICA200 can render the framebuffer at twice the selected dimensions, and then average it with antialiasing 2x2. This is an <a href="https://www.copetti.org/writings/consoles/xbox/#tab-2-4-post-processing">old technique</a> known as <strong>supersampling</strong>.</p><p>Once the framebuffer is ready to be displayed, it must be copied into another block in memory called <strong>Display Buffer</strong> (whose format is better aligned to the scan-line procedure of the LCD screen) and then transferred to the LCD in the form of scan-lines.</p></div></div></div><h3 id="interactive-comparison">Interactive comparison</h3><p>Now that you’ve seen how the PICA200 draws its triangles on the screen, it’s time for some practical examples. Here I’ve gathered two Marios from Smash Bros games, the Wii and 3DS one. Notice how the level of detail of ‘angry Mario’ hasn’t changed that much, considering we’re comparing a 2006 home console with its 2011 portable.</p><p>It’s worth reminding again that, in practice, the PICA200 will be rendering three screens at the same time, something that the <a href="https://www.copetti.org/writings/consoles/wii/#graphics">Wii’s GPU</a> wasn’t subjected to.</p><h3 id="nostalgic-rendering">Nostalgic rendering</h3><p>After all that’s been explained, there’s one question left unanswered: How does the PICA200 render Nintendo DS and Game Boy Advanced games? You may remember that the DS and GBA’s GPU exhibit completely different modus operandi for <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#the-3d-accelerator">rendering and broadcasting</a> frames.</p><figure><a href="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png"><picture><img alt="Image" width="256" height="192" src="https://www.copetti.org/images/consoles/nintendods/mario/complete.ad64c1f4bb4e348934057c8f4809801019adc8e0ad46312f00c17fd40c24b475.png" loading="lazy"></picture></a><figcaption>A frame rendered by the Nintendo DS’ <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#graphics">Graphics Engine</a>, whose pipeline segregates between 2D and 3D data. That’s something the OpenGL-compliant PICA200 doesn’t understand.</figcaption></figure><p>The explanation is that the <strong>DS and GBA PPUs are housed in the SoC</strong> and DSi/DS/GBA games will operate them as they originally did on previous consoles. The PPUs output (scanlines) is delivered to a block called <strong>LgyFB</strong>, which may optionally upscale the frame, and then forwarded to the framebuffer, where the PICA200 will take care of displaying it. It’s the job of the ARM11 and its DMA to take care of all memory transfers during this process.</p><p>Naturally, this arrangement will add some delay (a.k.a. lag), albeit negligible to the user.</p><hr><h2 id="audio">Audio</h2><p>Overall, the SoC houses <strong>two audio blocks</strong>:</p><ul><li>A proprietary <strong>DSP</strong> exclusively programmed for sound operations. This is used by 3DS games.</li><li>A variant of the <a href="">Nintendo DS audio block</a> named <strong>CSND</strong>. 3DS, DS and GBA games use it.</li></ul><h3 id="the-3ds-only-hardware">The 3DS-only hardware</h3><p>You may know that this same DSP was previously bundled with the Nintendo DSi, but treated as an optional accelerator instead. With the 3DS, it’s become the designated audio processor, so it’s no longer a voluntary component.</p><p>The DSP is called <strong>CEVA TeakLite II</strong> <sup id="bibref:39"><a href="#bib:audio-teakra" role="doc-biblioref">[39]</a></sup> and operates at <strong>~134 MHz</strong> <sup id="bibref:40"><a href="#bib:audio-teakra_arch" role="doc-biblioref">[40]</a></sup>. It’s manufactured by ParthusCeva, a company that provides synthesisable cores for audio processing <sup id="bibref:41"><a href="#bib:audio-dsp_press" role="doc-biblioref">[41]</a></sup>, and I guess ‘synthesisable’ was the keyword Nintendo was looking for when they partnered.</p><p>Moving on, the DSP outputs stereo samples (<strong>2 channels</strong>) of up to <strong>32 kHz</strong> of sampling rate and <strong>16-bit</strong> resolution.</p><p>Next to this component, we can find <strong>512 KB of RAM</strong> that is used by the DSP as a working area. It’s double-buffered (256 KB per buffer), so both the CPU and DSP can read and write without interruption <sup id="bibref:42"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[42]</a></sup>. Apart from that, the DSP comes with a dedicated DMA that can transfer data in and out of those 512 KB.</p><h4 id="operation">Operation</h4><p>For all intents and purposes, games treat this as an opaque DSP. Thus, only Nintendo knows how to program it.</p><p>3DS programs, as a consequence of being developed using the official SDK, bundle a DSP firmware (solely authored by Nintendo) which is then uploaded to the DSP chip at runtime <sup id="bibref:43"><a href="#bib:audio-dsp_binary" role="doc-biblioref">[43]</a></sup>. Afterwards, programs rely on that firmware to execute audio-related routines. Furthermore, the audio services provided by the operating system further abstract the communication between the program and the DSP’s firmware <sup id="bibref:44"><a href="#bib:audio-dsp_services" role="doc-biblioref">[44]</a></sup>.</p><p>In any case, while the DSP firmware may change over the years, some capabilities have remained the same. For instance, the DSP can mix <strong>ADPCM</strong> and <strong>PCM</strong> samples. with support of up to <strong>24 channels</strong> <sup id="bibref:45"><a href="#bib:audio-dsp_memory" role="doc-biblioref">[45]</a></sup>. There’s also functionality for filtering and sequencing, including the generation of <a href="https://www.copetti.org/writings/consoles/nes/#audio">PSG</a>-like sounds.</p><p>Interestingly enough, the steps followed for hacking the 3DS (so it can execute homebrew application) optionally involve extracting the HOME Menu’s DSP firmware, so homebrew may use it to provide audio output <sup id="bibref:46"><a href="#bib:audio-dsp_dump" role="doc-biblioref">[46]</a></sup>.</p><h3 id="the-backwards-compatible-block">The backwards-compatible block</h3><p>At the other end of the spectrum, we find the CSND block. 3DS may use it as an extension of the DSP and DS/DSi/GBA games rely on it to replicate their hardware.</p><p>In terms of functionality, the CSND features <strong>32 channels</strong> <sup id="bibref:47"><a href="#bib:audio-3ds_sound" role="doc-biblioref">[47]</a></sup>, which is twice the amount of the Nintendo DS counterpart.</p><p>Curiously enough, early homebrew defaulted to this block for providing sounds, while waiting for the DSP to be reverse-engineered.</p><h3 id="pipeline">Pipeline</h3><p>Both DSP and CSND work independently and separately output their audio to the speaker.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png"><picture><img alt="Image" width="665" height="303" src="https://www.copetti.org/images/consoles/nintendo3ds/audio.57ad2d480615199c857bfd09b469bde2b54de077df9f05eefab4c38ceed8b390.png" loading="lazy"></picture></a><figcaption>Overview of the audio pipeline.</figcaption></figure><p>As a curious note, the original Nintendo 3DS didn’t play well with the speaker’s capabilities, as Nintendo ended up providing troubleshooting guides for cases of buzzing noises and fluctuations with 3D slider <sup id="bibref:48"><a href="#bib:audio-buzzing" role="doc-biblioref">[48]</a></sup>, all caused by the design of the case.</p><hr><h2 id="io">I/O</h2><p>This section tends to be very rich in technologies considering Nintendo’s consoles favour generous I/O before state-of-the-art CPUs and GPUs. Let’s see what the Nintendo 3DS offers.</p><h3 id="external-interfaces-and-peripherals">External interfaces and peripherals</h3><p>The Nintendo DS had tons of modules built-in and the Nintendo DSi added more on top of it (after removing the GBA Slot). Now we found ourselves with a new console combining interfaces from two decades (the 2000s and 2010s).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_01c078a07c7bbb5949a953c8edcc6b9f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_2c4c532e83c7d9e10248df14d45e089e.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/_hud261010f8fda45b6fb91d14bdc6124f7_88175_678c74a178b4bb3822d17fa443a083bc.webp 2252w"><img alt="Image" width="2252" height="971" src="https://www.copetti.org/images/consoles/nintendo3ds/diagram.4c27d5fbaf9d12fd48f1f6fd2a03bbd5b6652a15e22d418a486fc874896b126c.png" loading="lazy"></picture></a><figcaption>Main diagram of the console’s architecture. You can sense that the I/O area on the left side was a strong selling point of this console.</figcaption></figure><p>To be fair, we still don’t have a standard like USB, but that may be expendable considering the Nintendo 3DS bundles the following:</p><ul><li>A keypad composed of <strong>digital buttons</strong>, an <strong>analogue circle pad</strong>.</li><li>3D and volume <strong>sliders</strong>.</li><li>A Wi-Fi <strong>switch</strong>.</li><li>A <strong>resistive touch sensor</strong> on the bottom screen.</li><li>A <strong>gyroscope</strong> measuring the console’s rotation changes.</li><li>An <strong>accelerometer</strong> to measure the console’s motion.</li><li><strong>One front camera</strong> and <strong>two back cameras</strong>, the latter allowing to take stereoscopic pictures.</li><li>An <strong>infrared transceiver</strong>, used to transfer data between external accessories.</li><li>An <strong>SD card</strong> slot, serving as external storage.</li><li>A standard <strong>3.5 mm jack socket</strong> for headphones.</li><li>A <strong>game card reader</strong>, where 3DS, DSi and DS retail games are read from.</li></ul><h4 id="the-new-enhancements">The ‘New’ enhancements</h4><p>If that wasn’t enough, the New 3DS came with more modules on top. This includes:</p><ul><li>Two <strong>extra digital buttons</strong> and an extra analogue circle pad (called <strong>‘C-Stick’</strong>).</li><li>An <strong>NFC Reader</strong> on the bottom screen.</li><li>An <strong>infrared LED</strong>, reserved for head tracking.</li><li>The SD slot is replaced with a <strong>microSD slot</strong>.</li><li>The Wi-Fi switch has been removed, now it’s only controlled through software.</li></ul><p>Now, to prevent leaving ‘old’ users behind, Nintendo provided external accessories to enhance the old models, although most of them relied on the single infrared transceiver to connect. Thus, only one accessory could be connected at the same time.</p><p>Not all the exclusive features of the New 3DS can be replicated, however. For instance, the New 3DS’ head tracking mechanism depends on the extra ARM11 core.</p><h3 id="internal-interfaces">Internal interfaces</h3><p>Now it’s time to check how are these interfaces - and others - internally wired up.</p><p>Firstly, a large subset is interconnected with the standard <strong>Serial Peripheral Interface</strong> (SPI) protocol. There are four SPI buses and all of them are accessed by the ARM9 (which I assume also includes the ARM7). The ARM11 only has access to <em>most</em> of them <sup id="bibref:49"><a href="#bib:io-spi_registers" role="doc-biblioref">[49]</a></sup>. In any case, the SPI buses connect the following modules <sup id="bibref:50"><a href="#bib:io-spi_devices" role="doc-biblioref">[50]</a></sup>:</p><ul><li>The flash memory found inside 3DS game cards, for storing save data.</li><li>DS’ Power Management.</li><li>Parts of the Wi-Fi chip.</li><li>Touch screen.</li><li>Sound.</li><li>Microphone.</li><li>Circle Pad.</li></ul><p>Curiously enough, some peripherals are interfaced twice to replicate the old DS/DSi’s I/O layout and also provide extended capabilities for 3DS software.</p><p>Secondly, there’s a <strong>Human-interface device</strong> (HID) module connected to both ARM11 and ARM9 data buses. This is how the digital keypad is accessed. The data is read through a 16-bit register.</p><p>Moving on, we got an <strong>I²C</strong> block which uses a more sophisticated serial protocol. This is connected to the following <sup id="bibref:51"><a href="#bib:io-i2c_devices" role="doc-biblioref">[51]</a></sup>:</p><ul><li>Front camera, also works in DSi mode.</li><li>Two back cameras, the right camera is accessible in DSi mode as well.</li><li>Infrared transceiver.</li><li>The NFC interface, in the case of the New 3DS.</li><li>The ‘QTM’ module, used for head-tracking (New 3DS only).</li><li>Gyroscope.</li><li>MCU chip, a separate controller that interfaces more components (explained in the next section).</li></ul><p>Finally, there are various <strong>registers</strong> interfacing FIFO blocks which, in turn, connect to two relatively high-speed (16 MB/s) peripherals <sup id="bibref:52"><a href="#bib:io-misc" role="doc-biblioref">[52]</a></sup>:</p><ul><li>Internal eMMC memory.</li><li>SD card slot.</li></ul><p>As confusing as it may sound, there’s more hardware left to discuss. The rest is handled by a middle-man chip called <strong>Auxiliary Microcontroller</strong> (MCU) <sup id="bibref:53"><a href="#bib:graphics-hardware" role="doc-biblioref">[53]</a></sup>. This is just a microcontroller designed by NEC and manufactured by Renesas. Particularly, the <strong>model 78K0R</strong>, which bundles a proprietary (yet low-power and relatively modern) processor and a ROM <sup id="bibref:54"><a href="#bib:io-renesas" role="doc-biblioref">[54]</a></sup>. The 78K0R stores a firmware handled by the console’s operating system, both ARM9 and ARM11 can interact with it but so do other peripherals.</p><p>The MCU chip exclusively controls the following <sup id="bibref:55"><a href="#bib:graphics-hardware" role="doc-biblioref">[55]</a></sup> <sup id="bibref:56"><a href="#bib:io-i2c_mcu" role="doc-biblioref">[56]</a></sup>:</p><ul><li>Accelerometer.</li><li>LCD screens.</li><li>LED indicators.</li><li>Power Management.</li><li>Battery fuel gauge and rejection (whether to enable the charging circuitry or not).</li><li>Wi-Fi’s EEPROM.</li><li>Real-Time Clock (RTC).</li><li>3D slider and Wi-Fi switch, the latter is only found on old 3DS models.</li><li>HOME and power buttons.</li></ul><p>A subset of this group is already accessible by the main CPUs. This is because the MCU also perform monitoring tasks, thereby saving resources from the ARM11 or ARM9.</p><h3 id="ready-for-trends">Ready for trends</h3><p>With such a heavy list of I/O hardware, you can now see how Nintendo tried to compete against the smartphone market. This led to interesting services deployed throughout the console’s lifecycle:</p><ul><li>A <strong>QR Reader</strong> bundled with the camera app.</li><li><strong>AR Games</strong>: Nintendo shipped ‘AR Cards’ that could be scanned with the 3DS camera using an app called ‘AR Games’. This would make static Nintendo characters pop up in your room, like any traditional augmented-reality-based application.</li><li><strong>Face Riders</strong>: Another camera-based app, but in this case, takes a photo of the player to compose the game’s characters. The player must then use the gyroscope and microphone to battle his/her evil clones.</li><li><strong>Amiibos</strong>: Uses the NFC reader to scan figurines and unlock game bonuses, the same service was also <a href="https://www.copetti.org/writings/consoles/wiiu/#the-supplemental-interface">implemented in the Wii U</a>.</li><li><strong>SpotPass</strong>: The continuation of <a href="https://www.copetti.org/writings/consoles/wii/#games">WiiConnect24</a>, now automatically connects to unsecured Wi-Fi access points.</li><li><strong>StreetPass</strong>: Automatically exchanges data between nearby 3DS systems. Nintendo marketed it as a way of connecting random 3DS players on the street.</li><li><strong>Play Coins</strong>: Unlocks game content by doing some exercise (walking).</li></ul><hr><h2 id="operating-system">Operating System</h2><p>Having a large number of CPUs eventually impacts the overall complexity of the operating system. Not only that, but this console also stores more than one OS. This originates as a mechanism for providing large services (i.e.&nbsp;DSi/DS/GBA backwards compatible, rescue mode, etc.).</p><p>So, to avoid making this section any more confusing, let’s go by steps.</p><h3 id="architecture">Architecture</h3><p>The Nintendo 3DS, as a whole, comes with four firmware <sup id="bibref:57"><a href="#bib:operating_system-firm" role="doc-biblioref">[57]</a></sup>:</p><ul><li><strong>NATIVE_FIRM</strong>: Operates the console in ‘native’ mode (with the functionality exclusive to the Nintendo 3DS). Here, the ARM11 executes the main program.<ul><li>Curiously enough, there are two instances of NATIVE_FIRM installed (named <strong>FIRM0</strong> and <strong>FIRM1</strong>, respectively) in case the first one gets corrupted, for some reason.</li><li>This firmware is often referred to as ‘Horizon’ as well.</li></ul></li><li><strong>TWL_FIRM</strong>: It commands the Nintendo 3DS to behave like a Nintendo DSi. It does come at the expense of disabling all the exclusive features, but considering how the CPU, GPU, sound and I/O are intertwined; TWL_FIRM is truly a work of art. Consequently, the ARM9 and ARM7 are placed in the foreground (they execute the main program) <sup id="bibref:58"><a href="#bib:operating_system-gbatek_firm" role="doc-biblioref">[58]</a></sup>.<ul><li>The name ‘TWL’ comes from the codename of the Nintendo DSi.</li></ul></li><li><strong>AGB_FIRM</strong>: Similarly to TWL_FIRM but the 3DS now becomes a Game Boy Advance. Here, the ARM7 executes the main program.</li><li><strong>SAFE_FIRM</strong>: Used solely for maintenance-related tasks, such as system updates. This firmware is basically an early revision of NATIVE_FIRM (doesn’t go beyond version <code>3.0</code> on the old 3DS and version <code>8.1</code> on the New 3DS <sup id="bibref:59"><a href="#bib:operating_system-safehax" role="doc-biblioref">[59]</a></sup>).</li></ul><p>All of these firmware come with separate binaries for the ARM11, ARM9 and ARM7 CPUs. The only exception is that the ARM7 won’t be active under NATIVE_FIRM and SAFE_FIRM.</p><p>Generally speaking, the Nintendo 3DS will first launch a Boot ROM and then bootstrap NATIVE_FIRM. Afterwards, the running operating system may choose to reboot to another firmware based on the user’s actions (i.e.&nbsp;load a Nintendo DS game or run the firmware update assistant).</p><p>Let’s take a look now at how each CPU behaves in NATIVE_FIRM mode.</p><h4 id="the-security-processor">The security processor</h4><p>Once NATIVE_FIRM is bootstrapped, the ARM9 runs its own operating system made of a kernel called <strong>Kernel9</strong> and a single program called <strong>Process9</strong> <sup id="bibref:60"><a href="#bib:operating_system-overview" role="doc-biblioref">[60]</a></sup>.</p><p>Kernel9’s design follows the <strong>microkernel</strong> model, meaning it only provides essential abstraction with the hardware, including:</p><ul><li>Memory management.</li><li>Process scheduling.</li><li>Inter-Process Communication.</li></ul><p>On the other side, Process9 is a userland application that implements these services:</p><ul><li>Communication with the ARM11, called ‘PXI’.</li><li>Cryptography-related functions. This involves AES, RSA, SHA and ECDSA.</li><li>I/O management.</li><li>File System.</li><li>Title (3DS software) verification and installation.</li></ul><p>Both Kernel and Process9 reside on an ARM9-only block of 1 MB of SRAM (1.5 MB in the case of the New 3DS).</p><p>In terms of security, there’s no privilege distinction between Kernel9 and Process9, since the latter has unconditional access to a system call that runs arbitrary code in kernel mode.</p><p>In summary, combined with the exclusive I/O hardwired into the ARM9, this CPU has the role of a <strong>security processor</strong>, much like what the <a href="https://www.copetti.org/writings/consoles/wii/#the-hidden-co-processor">Wii and Wii U’s ARM9</a> also did, and unlike the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#design">co-processor architecture</a> of the Nintendo DS, where its second processor just offloaded I/O and audio tasks.</p><h4 id="the-user-processor">The user processor</h4><p>Likewise, the ARM11 runs a kernel of similar architecture to the ARM9 one (now called <strong>Kernel11</strong>). The big difference is that the ARM11 will be running multiple userland processes, and in doing so they provide services like:</p><ul><li>Communication with the ARM9, called ‘PXI’.</li><li>Multi-core processing.</li><li>Networking, HTTP and SSL.</li><li>Connection with Nintendo online infrastructure.</li><li>The graphical shell (called ‘HOME Menu’).</li><li>The ability to launch applications.</li><li>A layer of abstraction for apps called <strong>Services</strong>, which games must call to access hardware resources. Some components like the GPU are interfaced by a very thin API, nonetheless.<ul><li>Furthermore, services are implemented in a layered manner. Games only access a subset of these, and the latter in turn invokes greater privileged and specialised services.</li></ul></li></ul><p>The ARM11’s kernel resides on a dedicated block 512 KB of SRAM <sup id="bibref:61"><a href="#bib:operating_system-glossary" role="doc-biblioref">[61]</a></sup>, also called ‘AXI Work RAM’ or ‘AXI WRAM’, because it’s connected to the ARM11 using the AXI protocol.</p><h4 id="imposed-behaviour">Imposed behaviour</h4><p>Now for the bitter news, NATIVE_FIRM also enforces unusual restrictions on user programs.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png"><picture><img alt="Image" width="761" height="519" src="https://www.copetti.org/images/consoles/nintendo3ds/os_levels.a8d96b55f3e7dd4e555220d3bbcf9a0dff72eb90418a51175a6abac405813dd3.png" loading="lazy"></picture></a><figcaption>Overview of the privilege levels in the Nintendo 3DS running NATIVE_FIRM, after combining both operating systems.</figcaption></figure><p>To start with, the ARM11’s scheduler is hard-coded with specific behaviour for each core (as opposed to treating every core as general-purpose units):</p><ul><li>The first core, called <strong>appcore</strong>, is for the game to use. Yet, its thread scheduling policy is FIFO, meaning that the game can deadlock itself if multi-threading is improperly used <sup id="bibref:62"><a href="#bib:operating_system-multithreading" role="doc-biblioref">[62]</a></sup>.</li><li>The second core, called <strong>syscore</strong>, gets assigned system-related tasks.</li></ul><p>Conversely, syscore can lend 30% of its execution time to user applications, which may be helpful to offload some operations, although not every routine may work under syscore (especially time-sensitive ones) <sup id="bibref:63"><a href="#bib:operating_system-dsx86" role="doc-biblioref">[63]</a></sup>.</p><p>The curtailment is further extended with the New 3DS, whose ARM11 MPCore now comes with four cores, namely:</p><ul><li><strong>The fourth MP11 is solely used for face-tracking</strong>. Instead of adding circuitry, Nintendo engineers implemented face-tracking through pure software. Thus, it reserves one MP11 for this. I’m guessing this was a cost-effective solution for Nintendo.</li><li><strong>The third MP11 core is permanently idle</strong>, as the scheduler is never instructed to dispatch threads there. Game meta-data does include a flag to enable thread scheduling on this core, albeit its usage on any commercial game is yet to be confirmed. It’s highly possible that at one point Nintendo considered it, but ultimately rendered it unfeasible for battery consumption reasons, or maybe because single-core games would suffer some compatibility issue.<ul><li>Considering the New Nintendo 2DS XL is a New Nintendo 3DS without the stereoscopic screen, that means half of its quad-core CPU is wasted!</li></ul></li><li>CDMA, the ‘New’ DMA unit, is only accessible during the console’s boot <sup id="bibref:64"><a href="#bib:operating_system-dma" role="doc-biblioref">[64]</a></sup>. After the boot process finishes, <strong>CDMA is never used again</strong>.</li></ul><p>Moving on, in terms of usable RAM for games, we know that the Nintendo 3DS and New Nintendo 3DS come with 128 MB and 256 MB of FCRAM, respectively. What you need to know now is that the available RAM for apps is only <strong>64 MB</strong> and <strong>124 MB</strong> <sup id="bibref:65"><a href="#bib:operating_system-memory" role="doc-biblioref">[65]</a></sup>, respectively. This means that the OS consumes ~50% of the console’s main memory, not a particularly pleasant quality! To alleviate this, games also have the option to set a flag in their metadata (called <code>APPMEMTYPE</code>) to claim more FCRAM from the system, up to <strong>96 MB</strong> and <strong>176 MB</strong>, respectively. Behind the scenes, that flag instructs the system to reboot the console and boot the game without launching the HOME Menu beforehand, saving memory in the way.</p><p>All things considered, you can now sense how not all extra hardware in the New 3DS will automatically imply faster software. It’s a shame, and it gives me the feeling that the New 3DS was a rushed product, from the software perspective. But to be fair, Nintendo never planned the ‘New’ 3DS to be a full successor of the original 3DS. The ‘New’ brand was a clear move to refresh the 3DS line, considering the sales number wasn’t satisfying, to say the least.</p><h3 id="storage-medium">Storage Medium</h3><p>Now that we know how the operating system is designed, let’s look at where and how data is stored in this console.</p><div><ul><li id="tab-6-1-boot-roms-link"><a href="#tab-6-1-boot-roms">Boot ROMs</a></li><li id="tab-6-2-otp-memory-link"><a href="#tab-6-2-otp-memory">OTP memory</a></li><li id="tab-6-3-emmc-nand-link"><a href="#tab-6-3-emmc-nand">eMMC NAND</a></li><li id="tab-6-4-sdmicrosd-link"><a href="#tab-6-4-sdmicrosd">SD/microSD</a></li></ul><div><div id="tab-6-1-boot-roms"><h4 id="tab-6-1-boot-roms">Boot ROMs</h4><p>Following its long ancestor, the <a href="https://www.copetti.org/writings/consoles/game-boy/#cpu">Game Boy</a>, the SoC stores a series of unencrypted ROMs containing the programs used for booting up NATIVE_FIRM <sup id="bibref:66"><a href="#bib:operating_system-bootloader" role="doc-biblioref">[66]</a></sup>. These bootstrappers are called <strong>Boot9</strong> and <strong>Boot11</strong>; and are executed by the ARM9 and ARM11, respectively. Likewise, they are physically and virtually kept hidden for security reasons. To give you an example, Boot9 stores AES decryption keys, which are not something to carelessly leave anywhere.</p><p>Interestingly enough, Boot9’s code has revealed that it’s more capable than just bootstrapping NATIVE_FIRM from eMMC NAND. However, due to certain routines having hardcoded directories and security layers added on top, the only firmware the Boot ROMs can ultimately load is NATIVE_FIRM from eMMC NAND.</p><p>Moreover, while multiple components have changed with the arrival of the New 3DS, the BootROMs have not changed a bit <sup id="bibref:67"><a href="#bib:operating_system-boot" role="doc-biblioref">[67]</a></sup>.</p></div><div id="tab-6-2-otp-memory"><h4 id="tab-6-2-otp-memory">OTP memory</h4><p>To further increase the level of security, the console stores a series of console-unique information in <strong>One-Time-Programmable</strong> (OTP) memory <sup id="bibref:68"><a href="#bib:operating_system-otp" role="doc-biblioref">[68]</a></sup>. Similarly to the <a href="https://www.copetti.org/writings/consoles/wii/#tab-7-1-shared-encryption">Wii</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#tab-7-1-dedicated-hardware">Wii U</a>, this information also includes encryption keys.</p><p>OTP is written once during manufacturing, so the keys differ between each console. Hence, one hacked console won’t necessarily be able to compromise the rest. This is a significant milestone for a portable console, considering a certain <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-10-1-encryption-system">previous implementation</a> included global keys.</p></div><div id="tab-6-3-emmc-nand"><h4 id="tab-6-3-emmc-nand">eMMC NAND</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/emmc.8c2a1c478ffded9127a1389a411ec8ff8eb6b16c39a7ebc62f693480261828e4.webp"><picture><img alt="Image" width="900" height="699" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hua5f2cd62267f437d12e49c5b8a374e56_58072_86c90e2811a715b2aa2a3980f340716e.png" loading="lazy"></picture></a><figcaption>Samsung-made eMMC chip on the original 3DS.</figcaption></figure><p>Next to the big SoC, there’s an eMMC NAND Flash chip. However, its size is slightly different depending on the manufacturer <sup id="bibref:69"><a href="#bib:operating_system-fs" role="doc-biblioref">[69]</a></sup>. For instance, Toshiba supplied <strong>943 MB</strong> and <strong>1,888 MB</strong> chips, while Samsung provided ones with <strong>954 MB</strong> and <strong>1,240 MB</strong>.</p><p>To tackle this disparity, Nintendo defined the 3DS partition table using a common size: <strong>943 MB</strong> for Old 3DS (Toshiba-size) and <strong>1,240 MB</strong> (Samsung-size) for New 3DS. So, if your console came with a larger eMMC chip, the extra space is unfortunately left unemployed.</p><p>In any case, the console relies on eMMC for storing its system data, including the multiple firmware, and user data (saves and configs within 3DS and DSi mode).</p></div><div id="tab-6-4-sdmicrosd"><h4 id="tab-6-4-sdmicrosd">SD/microSD</h4><p>Once an optional (and sometimes, symbolic) medium, SD cards now enjoy similar responsibilities to internal storage, as the 3DS is dependent on it to download software from the eShop and store user data (game saves, camera pictures and microphone recordings) <sup id="bibref:70"><a href="#bib:operating_system-sd" role="doc-biblioref">[70]</a></sup>.</p><p>Software and user data stored here are protected with AES-128-CTR encryption.</p></div></div></div><h3 id="boot-process">Boot process</h3><p>Now that we know how the operating system is structured and where data is stored, let’s see how the Nintendo 3DS goes from a glossy powered-off brick to becoming an operating console offering multiple services.</p><h4 id="multi-core-chaos">Multi-core chaos</h4><p>Considering the 3DS must be able to manage four processors (2-core ARM11 + ARM9 + ARM7) in its SoC - or six, if you look at the New 3DS - one can only wonder how these ‘central’ processors suddenly become exceptionally coordinated during the console’s startup. Well, it’s all about implementing a master-slave hierarchy.</p><p>With the ARM9 and ARM7, there’s not a lot of room for doubt, both can be powered on separately and load different binaries. So the challenge is mainly focused on disseminating the homogenous multi-core ARM11.</p><p>In the ARM11 MPCore cluster, all cores start execution at vector <code>0x00000000</code> <sup id="bibref:71"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[71]</a></sup>. However, CP15 (the System Control co-processor) provides a register called <code>CPU ID</code> which, among other things, serves to identify the core currently executing instructions. Thus, programmers can query this register to decide whether the current CPU core should give orders (master) or wait for commands (slave). ARM later improved this technique by supplying a dedicated register called <code>mpidr</code>, found in ARMv7 CPUs.</p><p>Thanks to this, Nintendo engineers were able to identify any CPU core within the 3DS cluster and implement a bootloader where all cores become coordinated, and then carry out the necessary functions to bring the console to life.</p><h4 id="boot-procedure">Boot procedure</h4><p>Time to dive into the boot process. As with any other console of its generation, security is of great importance, which will have an impact on the boot stage. To avoid making this section too dense, I’ve simplified the stages where the security system is set up, but you’ll find more information in the ‘Anti-piracy’ section.</p><p>Having said that, once the console is powered on, the following sequence of events takes place <sup id="bibref:72"><a href="#bib:operating_system-boot" role="doc-biblioref">[72]</a></sup> <sup id="bibref:73"><a href="#bib:operating_system-bootrom" role="doc-biblioref">[73]</a></sup>:</p><ol><li>The ARM9 and ARM11 power up.</li><li>The ARM9’s reset vector is at address <code>0xFFFF0000</code>, which points to Boot9 <sup id="bibref:74"><a href="#bib:operating_system-memory" role="doc-biblioref">[74]</a></sup>. The ARM11 is induced in an infinite reset until its reset pin is lifted.</li><li>Boot9 clears ARM11’s reset pin and then initialises the ARM9’s MPU.</li></ol><p>The ARM11 MPCore will now start execution of Boot11 in parallel:</p><ol><li>ARM11’s reset vector is at address <code>0x00000000</code> <sup id="bibref:75"><a href="#bib:cpu-arm_reference" role="doc-biblioref">[75]</a></sup>, which happens to be in the same place as Boot11.</li><li>Boot11 will branch depending on which core is it being executed on. If it’s greater than core 2, it hangs indefinitely.</li><li>Wait until ARM9 is finished bootstrapping a firmware</li></ol><p>Meanwhile, the ARM9 will be busy continuig with Boot9 execution:</p><ol start="4"><li>The AES and RSA public keys are exported to the AES and RSA engines (these will be explained in the ‘Anti-piracy’ section).</li><li>Boot9 will try to boot from NAND.<ol><li>In NAND, there’s a partition at location <code>0x0</code> called ‘NCSD header’, this states that there are eight partitions, each with a firmware to boot from.</li><li>For each firmware partition listed, Boot9 will fetch its header, validate the SHA-256 hash and RSA-2048 signature (using a set of keys previously loaded from BootROM) and repeat this process until one validation succeeds. Then, it will boot from there.</li></ol></li><li>If all validations in NAND fail, Boot9 will try to boot from a Flash memory in the Wi-Fi module. If that also fails, the console will display an error screen.</li><li>The first partition validated happens to contain NATIVE_FIRM. Boot9 will proceed to copy the firmware to different memory areas based on the header’s parameters.</li><li>Disable half of Boot9 and Boot11. In doing so, FCRAM will be accessible.</li><li>Redirect ARM9 and ARM11’s execution to the firmware’s entry points.</li></ol><p>Now that NATIVE_FIRM is bootstrapped:</p><ol><li>The ARM9 will:<ol><li>Load Kernel9.</li><li>Kernel9 hides OTP memory and loads Process9.</li><li>The ARM9 CPU is now up and running Process9.</li></ol></li><li>Whilst the ARM11 does the following:<ol><li>Load Kernel11.</li><li>In the case of the New 3DS, Kernel11 will write to a new register called <code>CFG11_BOOTROM_OVERLAY_CNT</code> to overlay Boot11 code <sup id="bibref:76"><a href="#bib:operating_system-pdn" role="doc-biblioref">[76]</a></sup>. This will allow to redirect execution of the new ARM11 cores (core 3 and core 4) away from Boot11 to arbitrary functions in Kernel11, thereby taking control of them.</li><li>Kernel11 will start various system processes, including PM (Process Manager).</li><li>PM will start the ‘NS’ (Nintendo User Interface Shell) system module.</li><li>NS will either launch a game or the HOME Menu application.</li><li>The user is now in control.</li></ol></li></ol><h4 id="alternative-boot-processes">Alternative boot processes</h4><p>All of the previous explanations have been focused on booting up NATIVE_FIRM, which results in the traditional native 3DS mode. For other firmware such as TWL_FIRM, AGB_FIRM and SAFE_FIRM, it’s a bit more complicated. Turns out the previous boot process is still needed because only NATIVE_FIRM can be booted from a power cycle. But once this is running, it can bootstrap any of those firmware, and each will program the ARM9 differently. In either case, the security set-up during Boot9 will still be enforced.</p><p>TWL_FIRM and AGB_FIRM, in particular, operate a special set of registers that mould the 3DS hardware and memory layout in accordance with what DS, DSi or GBA games expect to find. FCRAM can still be accessed, allowing to boot a game ROM from those places as well (apart from the NTR card reader). However, FCRAM will be reconfigured to follow the DS and GBA bus specification (16-bit wide, instead of 32-bit).</p><p>A big difference about the backwards compatible firmware is that, at last, the ARM7 will be active (as Nintendo DSi/DS and GBA software require it).</p><p>To exit either mode, non-NATIVE firmware contain a routine that reboots the system and consequently returns it to NATIVE_FIRM. Thus, 3DS mode.</p><h3 id="interactive-shell">Interactive shell</h3><p>The 7th generation of console interfaces has landed on the Nintendo 3DS. A clear indication is that users don’t need a retail game to make the most out of their console, just navigate through the shell and you’ll find numerous apps and services bundled. This includes the special offering of this console (3D camera, stereoscopic view and augmented reality). The pressure to compete against smartphones couldn’t be clearer.</p><p>In terms of user interface design, I’m inclined to say there are many patterns borrowed from the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System Menu</a>, yet ported to a dual-screen portable system. The <strong>HOME Menu</strong> (name of the interactive shell) uses a 1-page navigation system where every installed application is shown on a scrollable grid. Except for a few shortcuts here and there, every service is an application to be launched.</p><p>Now, being a Nintendo product, you can expect a special focus on creativity and attention to detail. Families are the target audience, nevertheless, adults are the ones paying, and Nintendo knows that.</p><h4 id="maintaining-consistency">Maintaining consistency</h4><p>The NS module is not only responsible for launching the interactive shell, it also offers 3DS software with the ability to invoke routines to handle certain interactions. One example is the ‘Back to HOME Menu’ overlay, which must be shown whenever the user presses the ‘HOME’ button.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/back_home.2ce3303f63dc4e934fb2426840b63819f305e9727fd44f5ff4de45a7c927d4c4.png" loading="lazy"></picture></a><figcaption>Users can press the ‘HOME’ button midgame, this will reinstate the HOME Menu without closing the current application. This event is handled by the running application but the routines are provided by the NS service.</figcaption></figure><p>Furthermore, 3DS software may also invoke ‘mini applications’ for attending other events (i.e.&nbsp;show the virtual keyboard), these are known as <strong>Applets</strong> <sup id="bibref:77"><a href="#bib:operating_system-ns" role="doc-biblioref">[77]</a></sup>.</p><p>Both sets are a crucial dependency for all applications, as they are responsible for properly reacting to external events consistently. Interestingly enough, since Applets and NS routines are not part of the game itself, in the case of New 3DS systems, even if a game is running in compatibility mode (that is, with all the ‘New’ hardware disabled), they will still be executed using the full extend of hardware, giving a small performance boost to unoptimised 3DS games.</p><h4 id="the-legacy-shell">The legacy shell</h4><p>Whilst the special firmware includes the <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#operating-system">old BIOS routines</a> DS/GBA games will expect, there’s no DS or DSi shell in sight.</p><p>The old <strong>Wi-Fi setup screens</strong> (invoked by DSi and DS games) are the only exceptions. Interestingly enough, while the original DS Wi-Fi settings are useless (as they can only connect to WEP-protected access points), the DSi counterpart (accessed from DSi and ‘DSi enhanced’ DS games) can alter the 3DS’ Wi-Fi settings. Yet, these games bundle an old Wi-Fi driver that only worked with the real DSi (the 3DS contains an Atheros AR6014 while the DSi came with an Atheros AR6002 or AR6013). So, to tackle this, both Wi-Fi settings are automatically synced when the firmware boots up <sup id="bibref:78"><a href="#bib:operating_system-firm" role="doc-biblioref">[78]</a></sup>.</p><h4 id="updatability">Updatability</h4><p>Well, of course, an updatable system is pretty much a requirement, not only for providing new functionality but also from a security perspective.</p><p>You can update the system software either online or through a game cartridge. Confusingly enough, both contain different update packages. Game cartridges only bundle system updates without the updated user apps, whereas network updates include everything <sup id="bibref:79"><a href="#bib:operating_system-home" role="doc-biblioref">[79]</a></sup>. Consequently, version names encode a mix of the two, in case the user used both channels.</p><p>To install updates, the NS service reboots into <code>SAFE_FIRM</code>, where the <strong>System Updater</strong> takes care of this process <sup id="bibref:80"><a href="#bib:operating_system-settings" role="doc-biblioref">[80]</a></sup>.</p><hr><h2 id="games">Games</h2><p>It’s time to check how game development and distribution were carried out. Additionally, we’ll see some exclusive services Nintendo prepared for this console.</p><h3 id="development-ecosystem">Development ecosystem</h3><p>Before the Nintendo 3DS arrived, developing for embedded system involved monumental efforts and high levels of patience. Compared to desktop applications, the tooling wasn’t standardised and sometimes it didn’t converge well with each other (ActiveSync is the clearest example I remember). The range of documentation didn’t usually go beyond what the manufacturer provided, the same applied for technical support.</p><p>Enter the 2010s decade, coinciding with the influx of an ARM-based smartphone industry and more efficient compilers, development for those platforms was no longer a complicated endeavour. Consequently, game studios developing for the Nintendo 3DS were able to enjoy this evolution. Now, Nintendo was not providing a standard toolchain yet, but they were on the right track (finally reached with the Nintendo Switch).</p><p>Curiously enough (and this is an interesting contrast), back in 2011, Apple offered Clang/LLVM 1.3 and OpenGL ES 3.0 for developing iOS apps, this was considered state-of-the-art for mobile projects. Well, you’ll see throughout this section that this wasn’t the case for Nintendo. Yet, at present, if you grab an old iPhone 4s and try to install any app on the App Store (its only official medium), it will tell you your system is too old. Whereas you can still play any retail game on your 3DS. Food for thought.</p><div><ul><li id="tab-8-1-hardware-kits-link"><a href="#tab-8-1-hardware-kits">Hardware Kits</a></li><li id="tab-8-2-software-kits-link"><a href="#tab-8-2-software-kits">Software Kits</a></li></ul><div><div id="tab-8-1-hardware-kits"><h4 id="tab-8-1-hardware-kits">Hardware Kits</h4><p>Nintendo partnered with two suppliers to produce development kits <sup id="bibref:81"><a href="#bib:games-hardware" role="doc-biblioref">[81]</a></sup>. The first supplier was the well-known <strong>Intelligent Systems</strong> and the other was <strong>Kyoto MicroComputers</strong>.</p><p>Among the many options, studios could rent a general-purpose ‘CTR-BOX’. This is a metallic box housing the 3DS hardware, and connected to it is a ‘dummy’ 3DS case that serves as a controller and display. With it, developers could deploy, test and debug their code.</p><p>For more single-purpose tools, studios could get official <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#tab-9-1-the-hardware">flashcards</a> to distribute game prototypes to external testers. These flashcards still only run on non-retail equipment, though this included cheaper options (with reduced functionality) than the fully-fledged CTR-BOX.</p><p>With the arrival of the New 3DS, IS and Partner offered the ‘SNAKE’ kits with updated hardware.</p></div><div id="tab-8-2-software-kits"><h4 id="tab-8-2-software-kits">Software Kits</h4><p>As always, licensed studios would also get an SDK package from Nintendo which included <sup id="bibref:82"><a href="#bib:games-software" role="doc-biblioref">[82]</a></sup>:</p><ul><li>A variant of <code>armcc</code> (ARM’s <strong>C</strong> and <strong>C++ compiler</strong>) modelled for the Nintendo 3DS and the ARM11 MPCore.<ul><li>This will just produce code for the ARM11 MPCore. Both ARM9 and ARM7 are out of the equation, as 3DS games only run within the ARM11 cluster.</li></ul></li><li><strong>Debuggers</strong> made for IS and Partner’s development kits.</li><li><strong>APIs</strong> to communicate with the hardware and operating system’s services.</li><li>Four <strong>graphics libraries</strong>:<ul><li><strong>GL</strong>: a simpler but slower OpenGL ES API.</li><li><strong>GD</strong>: a faster alternative to GL that generates PICA200 commands.</li><li><strong>GR</strong>: the closest-to-metal PICA200 command API, albeit with the steepest learning curve.</li><li><strong>GX</strong>: the general-purpose library used for PICA200 management.</li></ul></li><li>A 3DS <strong>app packager</strong>, so an executable can be created.</li><li>Further libraries to ease common development tasks, such as implementing network protocols, online gaming and audio/video decoding and processing.</li><li>A plugin for <strong>Visual Studio 2010</strong>, so it can be adopted as the main IDE.</li><li><strong>Assistant tools</strong> for the PICA200.</li><li><strong>Profilers</strong>, for measuring and optimising performance.</li></ul><p>If that wasn’t enough, developers also had access to NintendoWare to download code samples, libraries and further tools designed for Nintendo 3DS development. Furthermore, with the arrival of the New 3DS, game engines like Unity lent their support to this (once ignored) platform <sup id="bibref:83"><a href="#bib:games-unity" role="doc-biblioref">[83]</a></sup>.</p></div></div></div><h3 id="medium">Medium</h3><p>The Nintendo 3DS can run software from three different mediums.</p><div><ul><li id="tab-9-1-gamecards-link"><a href="#tab-9-1-gamecards">Gamecards</a></li><li id="tab-9-2-eshopsd-card-link"><a href="#tab-9-2-eshopsd-card">eShop/SD Card</a></li><li id="tab-9-3-local-wireless-link"><a href="#tab-9-3-local-wireless">Local wireless</a></li></ul><div><div id="tab-9-1-gamecards"><h4 id="tab-9-1-gamecards">Gamecards</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/gamecard.7848bd8ebfd8dcfe983d8f2883ce66afafff32cbd45fd9742c0a8161b400692c.webp"><picture><img alt="Image" width="815" height="571" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu73e8e42e37f2e81f9775ed553ee0b728_42910_ac83099bc32b8ab4125c6c805b799620.png" loading="lazy"></picture></a><figcaption>Example of a retail game. Notice the creative touch with Luigi holding onto the 3DS banner.</figcaption></figure><p>This is the distribution channel for retail software. Internally called ‘CTR cards’, they’re just another proprietary card/cartridge designed by Nintendo. To be fair, they’re not very different from <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#games">NTR Cards</a> (used by the Nintendo DS), aside from a cosmetic notch at the top right. Inside them, there is read-only and/or writable storage.</p><p>The variants of CTR cards range drastically. Their PCB can bundle a ROM chip ranging from <strong>128 MB</strong> to <strong>4 GB</strong> in size, while also including some ‘backup memory’ to store saves, this can be either <strong>128 KB or 512 KB</strong>. In other variants, the whole CTR storage is instead filled with <strong>Flash</strong> (up to 2 GB), and it’s partitioned to store both the game and saves in the same physical chip.</p><p>Moreover, since games will be bundling the official SDK as well, the usable capacity of ROM/RAM allowed to the game depends on the revision of the SDK linked to.</p><p>Internally, the ROM chip is connected to an 8-bit data bus <sup id="bibref:84"><a href="#bib:games-gamecards" role="doc-biblioref">[84]</a></sup>, while the backup memory relies on a serial bus. Both are connected to a 16.6 MHz clock. When the card is inserted into the console, the 3DS first queries it using NTR (Nintendo DS) commands <sup id="bibref:85"><a href="#bib:games-card_registers" role="doc-biblioref">[85]</a></sup>, and then switches to ‘CTR mode’ once it detects it’s a 3DS card.</p></div><div id="tab-9-2-eshopsd-card"><h4 id="tab-9-2-eshopsd-card">eShop/SD Card</h4><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/store/store_mario.68333a025886ba2f935c53f3da7502b46a56678e36170d0b904ac02ec8320e2c.png" loading="lazy"></picture></a><figcaption>The Nintendo eShop store for the Nintendo 3DS.</figcaption></figure><p>Expandable storage, an icon of the 8th generation of consoles. The Nintendo 3DS now enjoys installing and launching software from the SD (or microSD) card; and with it, retail cards are no longer the only medium for games. In fact, SD storage is the result of an emerging distribution channel: The Nintendo online store, bundled into every Nintendo 3DS.</p><p>Thanks to the eShop, there were new distribution techniques: users could <strong>pre-order</strong> games and DLCs before the shipping date <sup id="bibref:86"><a href="#bib:games-preorder" role="doc-biblioref">[86]</a></sup>. These would get downloaded ahead of time, but can only be played once the release date arrived.</p><p>When it comes to storing the respective saves, Nintendo allows its downloaded software to request up to <strong>1 MB</strong> of SD card storage. However, this rule is waived if the retail counterpart already requires more space, in which case the system will allocate as much ‘backup memory’ as the respective CTR card already provides.</p></div><div id="tab-9-3-local-wireless"><h4 id="tab-9-3-local-wireless">Local wireless</h4><p><strong>Download Play</strong>, a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#medium">debuting feature</a> of the Nintendo DS that enabled players to transfer small games between their consoles, has been pushed into the Nintendo 3DS. It now comes with a revamped protocol that relies on a thick security layer, in line with the rest of the software executed in the new console.</p><p>I’d say the big difference now is that transferred games (or ‘demos’, for it’s worth), are now installed into NAND (as any other installable package) <sup id="bibref:87"><a href="#bib:games-download_play" role="doc-biblioref">[87]</a></sup>. There’s only one slot reserved for them, so the installed program is replaced whenever a new game is transferred in.</p></div></div></div><h3 id="virtual-consoles">Virtual console(s)</h3><p>If, after all explained, users still got bored, Nintendo prepared another offering for them: <strong>Virtual Console</strong>.</p><p>Once again, thanks to the bundled eShop app, Nintendo also expanded its 3DS-only catalogue by incorporating games originally published for the following consoles:</p><ul><li>Nintendo DSi (DSiWare only).</li><li>NES/Famicom.</li><li><em>Sega</em> Game Gear.</li><li>Game Boy.</li><li>Game Boy Color.</li></ul><p>Virtual Console games behave as any other application installed into the console. Except for DSiWare software, the application package includes a ROM and emulator. The latter implements interesting capabilities, such as Download Play (for some games) and save states.</p><p>Once again, things were different for New 3DS users, as they could also access the <a href="https://www.copetti.org/writings/consoles/super-nintendo/">Super Nintendo</a> catalogue. This particularly strikes me as odd, as I remember a time when (homebrew) SNES emulators were developed for the original Nintendo DS (with its mere ARM9-ARM7 and a couple of megs of RAM).</p><p>Now, here’s another peculiarity of Virtual console games: this console can also play <a href="https://www.copetti.org/writings/consoles/game-boy-advance/">Game Boy Advance</a> games, officially. Yet, they’re not available for everyone. Only those who purchased a Nintendo 3DS before August 2011 (right before the console received an $80 price cut), became members of the ‘Ambassador Program’ <sup id="bibref:88"><a href="#bib:games-ambassador" role="doc-biblioref">[88]</a></sup>. One of the perks included access to a selection of GBA games which, for one reason or another, were kept exclusive until date.</p><p>Even more puzzling, GBA titles don’t run on the ARM11 using an emulator (albeit there’s one installed, but never been used!). Instead, they kickstart the third firmware, AGB_FIRM, to run natively on top of the ARM7. What makes it puzzling, is that these GBA games, only offered through the Ambassador Program, remained the only purpose of AGB_FIRM, as if Nintendo planned for something bigger in the future, but never materialised. This is another example of how the Nintendo 3DS possessed more hardware than the software ever took advantage of.</p><p>If you’re curious, GBA titles make use of the bundled ARM7 core instead. Thus, they don’t allow for the extra features that emulators (running on ARM11 cores) provide. Although, this happens at the exchange of running at full speed and precision. Be as it may, since the 3DS doesn’t contain a GBA cartridge slot, the GBA game is instead copied into FCRAM before the system reboots into AGB_FIRM, and then lets the ARM7 take control (while the ARM11 and ARM9 provide basic support tasks) <sup id="bibref:89"><a href="#bib:io-misc" role="doc-biblioref">[89]</a></sup>.</p><p>With all these capabilities, and on top of being a portable console, one can’t help but wonder why Nintendo didn’t distribute GBA and Nintendo DS/DSi games on the eShop as well. Most probably a marketing and licensing issue, I sense.</p><h3 id="game-updates">Game updates</h3><p>Another requirement of the 8th generation of consoles, games can now receive patches after the shipping date. That’s right, no more need to quality control the game before selling it!</p><p>Leaving irony aside, game updates are distributed through the eShop as well <sup id="bibref:90"><a href="#bib:games-updates" role="doc-biblioref">[90]</a></sup>, which applies to all types of games (except Download Play). All updates are downloaded onto the SD card and eShop games get their updates applied altogether (along with the game itself).</p><h3 id="network-service">Network service</h3><p>Out with the old (<a href="https://www.copetti.org/writings/consoles/nintendo-ds/#network-service">Nintendo Wi-Fi Connection</a>), in with the new (<a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Nintendo Network</a>)! The same service offered with the Wii U is also implemented on the Nintendo 3DS, and they’re admirably unified, so I recommend checking the <a href="https://www.copetti.org/writings/consoles/wiiu/#network-service">Wii U article</a> where it’s been explained in detail.</p><hr><h2 id="anti-piracy-and-homebrew">Anti-Piracy and Homebrew</h2><p>The history of hacking this console is a long and interesting sequence of events. At first, interests focused on cracking game card readers (in an attempt to replicate the success of the <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#defeat">Nintendo DS</a>) and then shifted towards more sophisticated approaches, only involving the operating system.</p><h3 id="main-targets">Main targets</h3><p>First things first, let’s start by describing the two main targets of this system:</p><ul><li>The <strong>Game/CTR card reader</strong>: This is where physical games are loaded. Historically, the Nintendo DS implemented a <a href="https://www.copetti.org/writings/consoles/nintendo-ds/#security-mechanisms">weak security mechanism</a> that was eventually cracked and subsequently led to an influx of <a href="https://www.copetti.org/writings/consoles/game-boy-advance/#flashcarts">Flashcards</a>.</li><li>The <strong>Operating System</strong>: This area is responsible for verifying the authenticity and integrity of every single program before execution (aside from the Boot ROM). Disabling said mechanisms would grant the execution of Homebrew (unauthorised applications) without any restrictions, in theory.</li></ul><p>They may look like two independent fronts (similar to the <a href="https://www.copetti.org/writings/consoles/xbox-360/#main-targets">Xbox 360</a> and <a href="https://www.copetti.org/writings/consoles/wiiu/#main-targets">Wii U</a>), but in the case of this console, both are intertwined. You’ll see it in a bit.</p><h3 id="the-card-reader-front">The card reader front</h3><p>The card reader is the interface between the CPUs and the Gamecards’ memory chip. Its only job is to simplify the communication with the use of commands.</p><p>Inside the ROM/Flash of CTR Carts, the system will find a block of data in a secured format called <strong>NCSD</strong>, this will be handled by the operating system, who will be in charge of authenticating, validating and decrypting it.</p><p>In conclusion, it seems the OS is solely responsible for the communication with the card reader, so let’s move on to the next front.</p><h3 id="the-operating-system-front">The Operating System front</h3><p>Before we continue, if you’re not familiar with symmetric and asymmetric encryption systems, I recommend reading previous articles of this series. They will also explain why systems like this rely so much on asymmetric encryption systems (such as RSA and ECDSA).</p><div><ul><li id="tab-10-1-dedicated-hardware-link"><a href="#tab-10-1-dedicated-hardware">Dedicated hardware</a></li><li id="tab-10-2-chain-of-trust-link"><a href="#tab-10-2-chain-of-trust">Chain of trust</a></li><li id="tab-10-3-operating-system-functions-link"><a href="#tab-10-3-operating-system-functions">Operating system functions</a></li></ul><div><div id="tab-10-1-dedicated-hardware"><h4 id="tab-10-1-dedicated-hardware">Dedicated hardware</h4><p>You’d be right if you suspected that the ARM11 lacks the powerhouse to protect the whole system. Nintendo knew that too, so they took extra care and bundled extra components to compensate:</p><ul><li>The ARM11 cores implement the <strong>XN flag</strong> and the ARM9 bundles a <a href="https://www.copetti.org/writings/consoles/playstation-portable/#focused-memory-management">Memory Protection Unit</a> (MPU), meaning the CPUs won’t execute code from any location in memory just because the current program tells it to.</li><li>As said before, the ARM9 acts as a <strong>dedicated processor</strong> to handle all security-related tasks while the ARM11 MPCore executes the game. Additionally, the ARM9 is exclusively wired to a few hidden <strong>cryptographic accelerators</strong>:<ul><li>An <strong>AES engine</strong> that performs AES-128 encryption/decryption without consuming (and exposing) CPU resources. This was inherited from the Nintendo DSi, but can now store up to 64 keys and can operate in numerous block cipher modes, including CTR, CCM, CBC and ECB <sup id="bibref:91"><a href="#bib:anti_piracy-aes" role="doc-biblioref">[91]</a></sup>. Each key slot also features its own <strong>key-scrambler</strong>, meaning that two arbitrary keys can be used to generate the final AES key. Moreover, the key-scrambler won’t allow anyone to read the generated key, only to treat it as a blackbox to encrypt/decrypt data.</li><li>An <strong>RSA engine</strong>. By contrast, this performs RSA encryption/decryption using a given RSA public key. This time, it only contains four key slots and there’s no key-scrambler <sup id="bibref:92"><a href="#bib:anti_piracy-crypto" role="doc-biblioref">[92]</a></sup>. However, it’s still a write-only space, meaning no one will be able to read the keys stored there. You’ll soon see that this system is filled with RSA-2048 and RSA-4096 signatures, which explains why this component is as crucial as the AES engine.</li><li>A <strong>Pseudo Random Number Generator</strong> (PRNG): These are registers that return a different value every time they’re read from.</li></ul></li><li><strong>OTP</strong> (one-time programmable) memory that stores console-unique keys, information about the console and <strong>CTCert</strong> (an ECDSA private key to authenticate with Nintendo’s servers). To complicate things further, these keys will be encrypted with an AES-CBC key found in Boot9. Finally, this region includes a flag to disable its access once it’s not needed anymore.</li><li>Last but not least, the eMMC memory contains a register called <strong>CID</strong> (Card Identification) and stores unique information about the eMMC’s manufacturing, which will be fed to the AES’ key-scrambler for further obfuscation.</li></ul><p>To top it off, everything is sealed in an SoC, including the <strong>two boot ROMs</strong> (Boot9 and Boot11). These are unencrypted, but since they’re inaccessible, they don’t represent a concern.</p></div><div id="tab-10-2-chain-of-trust"><h4 id="tab-10-2-chain-of-trust">Chain of trust</h4><p>This should come as no surprise considering we’ve already introduced RSA, AES and the boot ROM as part of the security system. To give you an overview of the Nintendo 3DS’ change of trust:</p><ol><li>ARM9’s boot ROM (Boot9) bundles the public key for decrypting and validating the contents of the NAND. The AES engine will be initialised with the keys stored in Boot9. With this, the contents of OTP memory will be accessed.</li><li>The contents of eMMC are decrypted using Boot9’s AES keys combined with the eMMC CID.</li><li>NAND and CTR cards are formatted using the <strong>NCSD</strong> format <sup id="bibref:93"><a href="#bib:anti_piracy-ncsd" role="doc-biblioref">[93]</a></sup>. NCSD stores a header and a collection of up to eight partitions. The NCSD header contains a signature using RSA-2048 and SHA-258, which is quite strong. To decrypt this signature, the system finds its public RSA key in the boot ROM or ITCM memory (the latter was previously decrypted and copied from OTP). The choice depends on where the NCSD block came from (NAND or CTR card).</li><li>Once the NCSD block is validated, the system accesses each partition. These are structured using the <strong>NCCH</strong> (Nintendo Content Container Header) format. Independently whether the data was pulled from NAND, the CTR card or the SD card, the NCCH block also contains an RSA-2048 + SHA-258 signature <sup id="bibref:94"><a href="#bib:anti_piracy-ncch" role="doc-biblioref">[94]</a></sup>, and its payload is encrypted with AES-128 CTR.</li><li>Furthermore, installed software is catalogued in the form of <strong>Titles</strong> (similar to the <a href="https://www.copetti.org/writings/consoles/wii/#broadways-os">Wii System</a>). In this case, all titles are signed with either RSA-2048, RSA-4096 or ECDSA; plus SHA256 <sup id="bibref:95"><a href="#bib:anti_piracy-titles" role="doc-biblioref">[95]</a></sup>. The public keys are stored in <code>NATIVE_FIRM</code>.<ul><li>It does surprise me that some signatures are in the form of ECDSA, considering there’s no hardware accelerator installed for it.</li></ul></li><li>Once the payload is verified and decrypted, the system will find either an executable, library or asset (i.e.&nbsp;manual, icon or banner) that the ARM11 can read.</li></ol><p>Please note, this explanation focuses on the main 3DS firmware (<code>NATIVE_FIRM</code>). Yet, <code>TWL_FIRM</code> and <code>AGB_FIRM</code> will also have their share of cryptography implemented.</p><p>As time passed by and hackers got the handle on how this console was protected, Nintendo shuffled the chain of trust further to deter the decryption of NCCH data. In some ways, it achieved its purpose, but in others, Nintendo ended up revealing too much. You’ll see it in the following sections.</p></div><div id="tab-10-3-operating-system-functions"><h4 id="tab-10-3-operating-system-functions">Operating system functions</h4><p>Once <code>NATIVE_FIRM</code> is up and running, in addition to the aforementioned chain of trust, the following security mechanisms are present:</p><ul><li>User programs only access hardware functions through system calls, authorised at Kernel11’s discretion. Depending on the hardware, it will also involve Kernel9.</li><li>From an architecture perspective, ARM11 user programs are completely unaware of the ARM9 and its neighbouring components.</li><li>User applications are <strong>sandboxed</strong>, meaning they can’t access each other’s space.</li><li>Last but not least, with the increase in online services, users will require a legitimate game card and an updated firmware to access the new functions. This will deter users who may consider keeping their console on a vulnerable firmware.</li><li>Software downloaded from the eShop also comes with its quirks. In this scenario, the license of a Title is encoded in the form of a <strong>Ticket</strong> which, again, is signed with RSA-2048 and SHA-256 <sup id="bibref:96"><a href="#bib:anti_piracy-cdn" role="doc-biblioref">[96]</a></sup>. Tickets are either linked to a single console ID and the eShop’s user account; or made global for any console. Furthermore, Nintendo uses additional RSA certificates in the downloaded Title’s metadata to further enlarge the chain of trust <sup id="bibref:97"><a href="#bib:anti_piracy-tmd" role="doc-biblioref">[97]</a></sup>.</li></ul></div></div></div><h4 id="flaws">Flaws</h4><p>Even though the Nintendo 3DS enjoyed modern protection techniques, such as asymmetric cryptography and lots of hardware at its disposal, there were some fundamental flaws in its implementation. Take a look at the following findings discovered by the hacking community:</p><ul><li>While the XN flag in the ARM11 works without problems, Kernel11 sets up the page table in AXI WRAM (where Kernel11 resides) in a way that it grants Read, Write and Execute permissions to the whole memory block <sup id="bibref:98"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[98]</a></sup>, rendering the capabilities of XN a bit useless (at least for protecting Kernel11).</li><li>Before system version <code>3.0.0</code>, OTP memory was never hidden <sup id="bibref:99"><a href="#bib:operating_system-otp" role="doc-biblioref">[99]</a></sup>, meaning that with the help of any exploit, the OTP keys could be extracted without problem.</li><li>There’s no separation between Process9 and Kernel9, as Kernel9 provides a system call that allows Process9 to perform any function with Kernel9 privileges <sup id="bibref:100"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[100]</a></sup>.</li><li>There’s no <strong>ASLR</strong> (Address space layout randomization) implemented <sup id="bibref:101"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[101]</a></sup>, enabling Return-oriented programming (ROP) for exploitation purposes.</li><li>Similarly, there’s no protection against system <strong>downgrading</strong>.</li><li>Once again, this system also comes with a <strong>Web browser based on Webkit</strong>, which is under constant attack (especially if the fork is old).</li></ul><p>This will not only pave the way to the first exploitation attempts, but will also act as a constraint for Nintendo when they try to patch their system.</p><h3 id="defeat">Defeat</h3><p>The history of the Nintendo 3DS and Homebrew is a successful one. Tons of video tutorials can attest to that. Yet, the passage exposes very clever discoveries, which evolved from initially requiring proprietary and expensive equipment to just a couple of clicks on your computer.</p><h4 id="the-ds-flashcard-era-2011-2013">The DS flashcard era (2011-2013)</h4><p>Where to begin? Well, from where the Nintendo DSi left it off: <strong>Flashcards</strong>.</p><p>After the release of the Nintendo DSi in 2008, Nintendo incorporated a new element to fight against Flashcards: A <strong>whitelist file</strong> listing every single licensed card and thereby blocking the ‘unauthorised ones’ <sup id="bibref:102"><a href="#bib:anti_piracy-card_whitelist" role="doc-biblioref">[102]</a></sup>. By no means Flashcard manufacturers ceased their production, they just shipped new variants of their old Flashcards that allowed the user to re-program the cartridge header, enabling the card to identify as a different authorised game whilst Nintendo kept amending the list (through software updates).</p><p>This method encompassed the Nintendo 3DS as well, following the same process as the Nintendo DSi. On no account they would get access to the exclusive 3DS hardware, yet this is how Homebrew started in this console.</p><h4 id="the-3ds-flashcard-era-2013-2016">The 3DS flashcard era (2013-2016)</h4><p>There was much progress during the first two years of this console (a big achievement for Nintendo!). Yet, things took a turn in August 2013…</p><div><ul><li id="tab-11-1-the-first-real-3ds-flashcard-link"><a href="#tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</a></li><li id="tab-11-2-inside-the-gateway3ds-link"><a href="#tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</a></li><li id="tab-11-3-subsequent-anecdotes-link"><a href="#tab-11-3-subsequent-anecdotes">Subsequent anecdotes</a></li></ul><div><div id="tab-11-1-the-first-real-3ds-flashcard"><h5 id="tab-11-1-the-first-real-3ds-flashcard">The first real 3DS Flashcard</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg"><picture><img alt="Image" width="650" height="650" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/gateway3ds.628421a0ac282a6b29e0f974bc8c45a02539e71d5fa3e6bfe2ca0b3a54c003de.jpg" loading="lazy"></picture></a><figcaption>The Gateway3DS package <sup id="bibref:103"><a href="#bib:anti_piracy-gateway_review" role="doc-biblioref">[103]</a></sup>.</figcaption></figure><p>Ignoring teasers of ‘3DS Flascards’ that never appeared <sup id="bibref:104"><a href="#bib:anti_piracy-crown3ds" role="doc-biblioref">[104]</a></sup>. <strong>Gateway3DS</strong> can be considered the first 3DS Flashcard to reach the stores. The instructions were not as simple as DS Flashcards, however. You can sense this by looking at the contents of the box:</p><ul><li>A whitelisted <strong>DS Flashcard</strong> (known as <em>Blue Gateway</em>) whose only purpose is to run a Nintendo DS ROM crafted by Gateway. As part of the ‘installation’ process, users were first required to run this ‘game’ and follow the instructions.</li><li>A <strong>Launcher.dat</strong> to be placed in the 3DS’ SD card.</li><li>A <strong>3DS Flashcard</strong> (known as <em>Red Gateway</em>) where the 3DS game is loaded from. Like any other Flashcard, it also features a microSD slot where the 3DS game is stored. The big difference, however, is that the 3DS game image is flashed into the microSD card, meaning that only one 3DS game can be stored at a time.<ul><li>This makes sense, as RSA signatures can’t be faked (at least, that’s computationally unfeasible). Yet, replicating an exact clone of the game (NCSD block) worked. Forget about Homebrew, for now.</li></ul></li></ul><p>After completing the installation process, users would have to follow these instructions to run any game:</p><ol><li>Insert the <em>Red</em> Gateway card. Nothing will appear, yet.</li><li>Open the 3DS settings app and navigate to the DS profile editor screen.</li><li>For some reason, the 3DS will restart and the flashed 3DS game will show up.</li><li>After finishing playing a game, returning to the HOME Menu will create a savefile in the 3DS’ SD card.</li></ol><p>And just like that, users were now able to download 3DS ROMs from the net and run them on their consoles… but how was all of this possible? How did anyone manage to extract decrypted games? What exploits did Gateway3DS employ (or even discover)?</p><p>Truth is, there’s a lot of hidden functionality within this product. Let’s analyse it step by step.</p></div><div id="tab-11-2-inside-the-gateway3ds"><h5 id="tab-11-2-inside-the-gateway3ds">Inside the Gateway3DS</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/shell/settings_ds_profile.fd6a65c58c189a525f1d8e65f40cbbc48687a3d05127ff4bf17975477cf7bae2.png" loading="lazy"></picture></a><figcaption>The DS Message editor found on the 3DS settings app. The character limit rule depends solely on the graphical interface.</figcaption></figure><p>Sometime in 2012, hacker ‘ichfly’ discovered interesting behaviour in the Nintendo DS’ old profile editor, found on both <code>NATIVE_FIRM</code> and <code>TWL_FIRM</code>. In one of its text fields, you can enter a ‘Message’ value, which will then be displayed as a greeting on PictoChat rooms <sup id="bibref:105"><a href="#bib:anti_piracy-profile" role="doc-biblioref">[105]</a></sup>. The 3DS’ settings app won’t allow you to enter more characters than allowed. Yet, nothing prevents a Nintendo DS game from doing so. When that happens, opening the 3DS’ System Settings app (called <strong>MSET</strong>) will crash, and what makes it interesting is that this is caused by <strong>stack overflow</strong> <sup id="bibref:106"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[106]</a></sup>. Does this remind you of <a href="https://www.copetti.org/writings/consoles/wii/#the-dawn-of-homebrew">a certain horse name</a>?</p><p>Now, the mysterious Launcher.dat by Gateway is a configuration file that the Settings app normally reads. What happened is that Gateway crafted their own Launcher.dat to embed data used for the next stages of their exploit. Curiously enough, Launcher.dat is stored in NAND (not in the SD), so the initial exploit chain also alters where the Settings app loads this from.</p><p>If you combine this with a Process9/Kernel9 exploit, you get full execution privileges on this console and can start fiddling with system services. Some hardware like OTP and the boot ROMs will still be out of reach. Yet, this is a significant milestone.</p><p>So far so good? Let’s now connect this information with Gateway’s package:</p><ul><li>The DS/Blue flashcard is just an entry point to install the corrupted DS profile (which will trigger the MSET exploit).</li><li>The 3DS/Red flashcard houses a ProASIC3 FPGA programmed with a firmware (distributed by Gateway). The FPGA and the microSD card are combined to replicate a retail game.</li><li>Launcher.dat is the payload of the MSET exploit. It bundles a Kernel exploit and a collection of system patches. In other words, a <strong>Custom Firmware</strong> (CFW). A console running Gateway’s CFW can extract games or load a 3DS game using the red flashcard. Surprisingly, Gateway also crafted their CFW so it requires the red flashcard inserted to work (<em>a DRM mechanism in a Flashcard, have the tables turned?</em>).</li></ul></div><div id="tab-11-3-subsequent-anecdotes"><h5 id="tab-11-3-subsequent-anecdotes">Subsequent anecdotes</h5><p>All seemed jolly for Gateway until November 2013, when a stream of clones of their card landed. ‘R4i Gold 3DS Deluxe’ came for some healthy competition, albeit by using some of Gateway’s firmware code. In retaliation, Gateway3DS took drastic measures: Subsequent firmware updates of Gateway3DS corrupted the 3DS NAND if a clone was detected. <em>The irony!</em></p><p>In October 2013, hacker ‘Smealum’ published a video showing his own MSET-based implementation that instead booted a copy of <code>NATIVE_FIRM</code> stored in the 3DS’ SD <sup id="bibref:107"><a href="#bib:anti_piracy-rednand" role="doc-biblioref">[107]</a></sup>. This meant that consoles stuck on system <code>4.5.0</code> could boot newer system versions without losing the ability to run exploits. Smealum called this function <strong>redNAND</strong> (from ‘redirected NAND’) and, while it wasn’t publicly released, Gateway later incorporated this functionality (now referred to as <strong>emuNAND</strong>) with their CFW released in December 2013 <sup id="bibref:108"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[108]</a></sup>. This became a strong selling point for Gateway3DS.</p><p>It’s not known what Process9/Kernel9 exploit Gateway employed. Yet, in December 2013, Fierce_Waffle, Xerpi and Megazig reversed engineered and open-sourced Gateway’s payload in the form of a tool called ‘3DS Toolkit’ <sup id="bibref:109"><a href="#bib:anti_piracy-waffle" role="doc-biblioref">[109]</a></sup> <sup id="bibref:110"><a href="#bib:anti_piracy-ramdump" role="doc-biblioref">[110]</a></sup>.</p><p>In the following years, a second generation of 3DS flashcards will appear in the market. Examples include <strong>Stargate</strong>, <strong>Sky3DS</strong> and dozens of clones. This time, they didn’t rely on an operating system exploit to work and could load multiple games from their microSD. However, their utility will be entirely based on replicating retail 3DS games (including their signatures), in other words, for solely piracy purposes.</p></div></div></div><h4 id="nintendo-acts-fast">Nintendo acts fast</h4><p>Having an updatable system software meant Nintendo didn’t have to stand there and watch how its system got cracked:</p><ul><li>In March 2013, system update <code>5.0.0-11</code> updated the settings app, provisionally fixing the MSET exploit <sup id="bibref:111"><a href="#bib:anti_piracy-neko" role="doc-biblioref">[111]</a></sup>. If you check the timeline, this was before Gateway3DS shipped their card! Hence, it was a prerequisite for users to stay on older versions.<ul><li>It won’t be until 2015 when the Gateway team released a notable firmware update. From then on, the flashcard relied on a new Web Browser exploit (called <strong>spider exploit</strong>, discovered by MathewE) as the entry point. This method lasted until the end of Gateway3DS’ lifespan.</li></ul></li><li>In December 2013, system update <code>7.0.0-13</code> fixed the kernel exploits used in combination with MSET and, most importantly, added the RSA module into the chain of trust to decrypt NCCH blocks (where the game data is found) <sup id="bibref:112"><a href="#bib:anti_piracy-70013" role="doc-biblioref">[112]</a></sup>. RSA keys are cleared once Kernel9 finishes loading, meaning existing exploits won’t be able to decrypt games that adopted the new <code>7.0.0</code> encryption system (unless a vulnerability is used before Kernel9 boots).</li><li>As Gateway3DS’ Launcher.dat file contained copyrighted code by Nintendo, the latter company sent Cease &amp; Desist letters to many forums, including GBATemp, which in turn blocked the distribution of those files.</li></ul><p>As always, this marked the start of another cat-and-mouse game. Though, to make a long story short, system update <code>9.3.0</code> (released in December 2014) finally put an end to Gateway3DS by patching their private Kernel exploit <sup id="bibref:113"><a href="#bib:anti_piracy-gateway3ds" role="doc-biblioref">[113]</a></sup>. Since then, Gateway3DS’ firmware updates only improved emuNAND support with the latest system versions (for those who didn’t update past the breaking update). In 2016, Gateway’s last update was released. Meanwhile, Sky3DS enjoyed support until system software <code>11.0</code> (released in May 2016) <sup id="bibref:114"><a href="#bib:anti_piracy-sky3ds" role="doc-biblioref">[114]</a></sup>, when Nintendo blacklisted it for good.</p><p>I think now it’s fair to say that the 3DS flashcard market ended up being too turbulent and unreliable for the average user, compare this to the ‘plug &amp; play’ experience Nintendo DS flashcard offered. Finally some good news for Nintendo, so far.</p><h4 id="the-dawn-of-homebrew-2014">The dawn of homebrew (2014)</h4><p>2014 saw an emergence of homebrew-focused solutions in a circle populated by piracy-oriented developments <sup id="bibref:115"><a href="#bib:anti_piracy-proto_homebrew" role="doc-biblioref">[115]</a></sup>. Hacking a 3DS still required an old system version, a Gateway3DS card and emuNAND - but that would slowly shift once alternative tools gained traction.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg"><picture><img alt="Image" width="854" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/32c3.603e3f980c96df85d410f37756afa2355a73eecf8413059c6e11241ac60e4777.jpeg" loading="lazy"></picture></a><figcaption>Plutoo, Derrek and Smealum presenting their findings at the 32nd Chaos Communication Congress (2015) <sup id="bibref:116"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[116]</a></sup>, the following paragraphs will explain most of them.</figcaption></figure><div><ul><li id="tab-12-1-open-source-sdks-link"><a href="#tab-12-1-open-source-sdks">Open-source SDKs</a></li><li id="tab-12-2-ninjhax-chain-link"><a href="#tab-12-2-ninjhax-chain">Ninjhax chain</a></li><li id="tab-12-3-gaining-kernel11-access-link"><a href="#tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</a></li></ul><div><div id="tab-12-1-open-source-sdks"><h5 id="tab-12-1-open-source-sdks">Open-source SDKs</h5><p>Initial Homebrew appeared in the form of Laucher.dat files, these were produced with the help of devkitARM (a general-purpose toolchain for ARM-based CPUs) and a set of scripts. Fierce Waffle provided ‘ROP Loader’, a toolkit that included a DS program to install the MSET exploit; and a Launcher.dat that triggered a Kernel11 exploit. It’s worth pointing out that there wasn’t any tool available, yet, that helped access the 3DS’ exclusive hardware.</p><p>At the start of 2014, Smealum, with the collaboration of yellows8, ichfly, WinterMute, fincs, mtheall and plutoo, released <strong>ctrulib</strong>, an open-source C library to facilitate Homebrew development <sup id="bibref:117"><a href="#bib:anti_piracy-libctru" role="doc-biblioref">[117]</a></sup>. This is now known as <strong>libctru</strong> and maintained by the devkitPro group, who have incorporated it into their toolchain.</p><p>A year later, neobrain released <strong>nihstro</strong> <sup id="bibref:118"><a href="#bib:anti_piracy-nihstro" role="doc-biblioref">[118]</a></sup>, a PICA200 shader assembler a disassembler, making the job of programming the PICA200 a bit more enjoyable.</p><p>To run Homebrew, users had the option to flash a homebrew binary into a microSD, and then use the Gateway3DS to boot it (as their CFW already disabled signature checks) <sup id="bibref:119"><a href="#bib:anti_piracy-gateway_homebrew" role="doc-biblioref">[119]</a></sup>.</p></div><div id="tab-12-2-ninjhax-chain"><h5 id="tab-12-2-ninjhax-chain">Ninjhax chain</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png"><picture><img alt="Image" width="400" height="480" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/launcher.a5ef019f4fd8b8aa7dc8af5beea6e53aba3af518a507b6a1f7f174ce4d627235.png" loading="lazy"></picture></a><figcaption>The Homebrew Launcher, inspired by the <a href="https://www.copetti.org/writings/consoles/wii/#a-permanent-state">iconic Wii counterpart</a>. Its arrival marked the sophistication of 3DS Homebrew.</figcaption></figure><p>The poisoned updates of Gateway left a bitter mark on their users. The time had come to look for nonproprietary alternatives.</p><p>Thankfully, people were working on this. During the second half of 2014, a new milestone awaited for the Homebrew community: Smealum published <strong>Ninjhax</strong>, a package composed of the following components <sup id="bibref:120"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[120]</a></sup>:</p><ol><li>A crafted <strong>QR code</strong> to be scanned by ‘Cubic Ninja’, a game that allows to share user-designed levels using QR codes. This served as a new entry point exploit.</li><li><strong>GSPWN</strong>: A userland vulnerability where the GPU’s DMA is used to write over the HOME Menu’s heap. Furthermore, the combination with ROP leads to privilege escalation. This resulted in the ability to create &amp; kill processes, SD card access, decrypt &amp; dump titles and override executable data.<ul><li><a href="https://www.copetti.org/writings/consoles/xbox-360/#graphics">Other GPUs</a> were also known for <a href="https://www.copetti.org/writings/consoles/xbox-360/#tab-20-3-king-kong-exploit">intruding</a> into the system’s RAM.</li></ul></li><li><strong>Homebrew launcher</strong>: A new service running under the HOME Menu process thanks to GSPWN. It provides a graphical user interface to load unsigned Homebrew apps (using a new portable .3dsx format) and take over processes. The launcher loads homebrew by opening an official application with enough privileges and then hijacks it with GSPWN, replaces the code with Homebrew code and finally executes it.<ul><li>With its ability to alter user data, the Homebrew launcher can also be used to install alternative entry points as they’re discovered (i.e.&nbsp;OotHax, Ironhax and so forth). Thus, reducing its dependency on Cubic Ninja. A notable aftermarket exploit was <strong>MenuHax</strong>, which exploited a vulnerability in the HOME Menu theme engine and was triggered at boot, <strong>making it a permanent solution to launch a payload</strong>.</li><li>If you are curious, the Wii U also experienced <a href="https://www.copetti.org/writings/consoles/wiiu/#fooling-iosu">similar methodologies</a> as early attempts to run Homebrew.</li></ul></li></ol><p>Notice how Gateway3DS is, for once, out of the equation. Be as it may, the Homebrew Launcher was still under the scope of userland (meaning homebrew apps could only access 64 MB of RAM and had no access to the audio DSP <sup id="bibref:121"><a href="#bib:anti_piracy-hbl_limitations" role="doc-biblioref">[121]</a></sup>).</p></div><div id="tab-12-3-gaining-kernel11-access"><h5 id="tab-12-3-gaining-kernel11-access">Gaining Kernel11 access</h5><p>Turns out that before the publication of Ninjhax, in February 2014, yellows8 made a very important discovery: An exploit leading to Kernel11 privileges.</p><p>Kernel11 keeps track of the unused memory pages in FCRAM using a structure called <code>memchunk header</code>. This data is stored as a linked list, where each header contains the address of the previous and next header. Well, it so happens <code>memchunk headers</code> <strong>are stored in FCRAM</strong>, which may be overwritten thanks to other exploits like GSPWN. Consequently, memchunk headers can be modified to grant userland access to AXI WRAM. In doing so, the attacker can eventually modify the Kernel11’s page table to grant all FCRAM access to user-space, leading to arbitrary control of Kernel11. This discovery was called <strong>memchunkhax</strong>.</p><p>Nevertheless, Nintendo patched it in December 2014 <sup id="bibref:122"><a href="#bib:anti_piracy-32c3" role="doc-biblioref">[122]</a></sup>. However, another hacker by the name of derrek found a race condition where the ‘next’ pointer of a <code>memchunk header</code> may be replaced with the location of a crafted one. So, when Kernel11 tries to access the crafted <code>memchunk header</code>, it will end up executing arbitrary code with Kernel11 privileges. Ipso facto, <strong>memchunkhax2</strong> came into existence.</p><p>Thanks to the new privilege escalation, Homebrew software gained complete control of the system up to the ARM9 area… but why stop there?</p></div></div></div><h4 id="most-wanted-tools">Most-wanted tools</h4><p>Considering the availability of Gateway3DS’ emuNAND, Ninjhax, CTRLib and the new Kernel exploits, the flood of new software was too great to ignore. To mention a few:</p><ul><li><strong>CtrBootManager</strong> by cpasjuste: An extra stage in HomeMenuHax’s chain that acts as a boot manager, enabling the selection of various payloads <sup id="bibref:123"><a href="#bib:anti_piracy-ctrbootmanager" role="doc-biblioref">[123]</a></sup>.<ul><li>Shortly after, a new implementation with extended functionality emerged: <strong>BootCtr</strong> by m45t3r <sup id="bibref:124"><a href="#bib:anti_piracy-bootctr" role="doc-biblioref">[124]</a></sup>.</li></ul></li><li><strong>RxTools</strong> by Roxas75: A Swiss knife for Gateway3DS users <sup id="bibref:125"><a href="#bib:anti_piracy-rxtools" role="doc-biblioref">[125]</a></sup>. This was offered as a replacement for Gateway3DS’ binaries. Among many things, it includes a CFW called <strong>RXMode</strong>. This alternative and open-source solution disables signature checks on 3DS binaries, provides emuNAND and removes <code>TWL_FIRM</code>’s whitelist checks, to mention a few.<ul><li>Other CFWs will soon make their appearance, like CakesFW, ReiNand and Pasta CFW <sup id="bibref:126"><a href="#bib:anti_piracy-cfw_old_list" role="doc-biblioref">[126]</a></sup>. These serve different purposes and include their own set of modifications.</li></ul></li><li><strong>Custom HomeMenu Manager</strong> (CHMM) by Rinnegatamante: Allows to install HOME Menu themes from the SD card <sup id="bibref:127"><a href="#bib:anti_piracy-chmm" role="doc-biblioref">[127]</a></sup>.</li><li><strong>AGB_FIRM Signature patcher</strong> by Riku. Loads arbitrary Game Boy Advance ROMs into AGB_FIRM <sup id="bibref:128"><a href="#bib:anti_piracy-agb_converter" role="doc-biblioref">[128]</a></sup>, finally expanding the abandoned catalogue of Nintendo Ambassador games.</li><li><strong>Ftpbrony</strong> by mtheall (later known as <strong>ftpd</strong>): A simple FTP server <sup id="bibref:129"><a href="#bib:anti_piracy-ftpbrony" role="doc-biblioref">[129]</a></sup>.</li><li><strong>DevMenu</strong>: Not exactly a homebrew app, but a <em>stolen</em> Nintendo-authored app from development units, enabling users to install app packages (in the form of ‘CIA’ files) into the system, just like the eShop did behind the scenes.<ul><li>Months later, <strong>BigBlueMenu</strong> was used instead, which also came from Nintendo’s development kit.</li><li>It wasn’t until a real open-source solution was brought forward some months after. <strong>FBI</strong> by Steveice10 became the standard dilemma-free tool for installing CIA files (notice the pun in the names) <sup id="bibref:130"><a href="#bib:anti_piracy-fbi" role="doc-biblioref">[130]</a></sup>.</li></ul></li></ul><h4 id="new-console-permanent-mods-2015">New console, permanent mods (2015)</h4><p>While homebrew developers were busy fiddling with their system, Nintendo released a <em>new</em> product to the surprise of everyone: The <strong>New 3DS</strong>.</p><p>Apart from the extra hardware (already mentioned throughout this article), a new stage was added to the boot process: <strong>arm9loader</strong>. With this, Nintendo enhanced their chain of trust by adding new keys, which must be decrypted with the help of a hash of OTP memory (therefore, using console-unique values) <sup id="bibref:131"><a href="#bib:anti_piracy-arm9loader" role="doc-biblioref">[131]</a></sup>. However, arm9loader and the new keys are still stored in NAND, meaning that the contents may be overwritten. This led to one of the most disrupting vulnerabilities of 2015, involving Plutoo, Yellows8 and Delebile.</p><h5 id="arm9loaderhax">arm9loaderhax</h5><p>The first implementation of arm9loader was flawed: the decryption key for the ARM9 system was never removed from the AES engine. So, with the help of additional exploitation, one could reconstruct part of the encryption keys <sup id="bibref:132"><a href="#bib:anti_piracy-arm9loaderhax" role="doc-biblioref">[132]</a></sup>. Consequently, Nintendo quickly tried again with <code>arm9loader v1.1</code> (found on system update <code>9.6.0</code>). As luck would have it, this led to a more powerful exploit: Plutoo discovered that the key used to decrypt the ARM9 system was never verified. Hence, arm9Loader will boot <code>NATIVE_FIRM</code> even if the decrypted data is wrong (a.k.a. garbage). Plus, if Firm0 (the first copy of <code>NATIVE_FIRM</code>) fails to boot, Boot9 will try to load Firm1 while the remains of Firm0 stay in the ARM9’s RAM.</p><p>All in all, if:</p><ul><li>NAND is modified (somehow) so the encrypted Firm0 contains extra crafted code at the end.</li><li>The ARM9 OS key is mangled in a way that the decrypted Firm1 will contain a jump instruction to Firm0’s crafted code.</li></ul><p>… you got yourself <strong>arm9loaderhax</strong>, a permanent exploit that provides <strong>arbitrary code execution</strong> with <strong>Kernel9 privileges</strong> at <strong>boot time</strong>!</p><p>Since Kernel9 access was now possible, albeit through difficult means, work was put into simplifying the process (i.e.&nbsp;developing an automated installer).</p><h5 id="the-effects-of-arm9loaderhax">The effects of arm9loaderhax</h5><p>New discoveries meant new developments. Over the following months, more advanced tools will become part of the ‘must have’ list of every homebrew user.</p><p>To start with, a new CFW to-rule-them-all shipped: <strong>Luma3DS</strong> <sup id="bibref:133"><a href="#bib:anti_piracy-luma3ds" role="doc-biblioref">[133]</a></sup>. Among many features, Luma3DS provides:</p><ul><li>The removal of signature and region checks.</li><li>A layered filesystem to redirect file operations to the SD card (enabling game modifications).</li><li>Rosalina Menu, an in-game menu overlay where many utilities can be accessed without closing any application.</li></ul><p>Initially, Luma3DS was bootstrapped with BootCtr, but that changed once arm9loaderhax became the de-facto hack for any 3DS. Thus, the arm9loaderhax + Luma3DS combination became part of any hacking tutorial.</p><p>Along it, other software appeared:</p><ul><li><strong>Godmode9</strong> by d0k3: A next-generation Swiss knife that takes advantage of the permissions granted by ARM9 exploits <sup id="bibref:134"><a href="#bib:anti_piracy-godmode" role="doc-biblioref">[134]</a></sup>, enabling the user to read and modify every corner of the console. It can be loaded by arm9loaderhax, Luma3DS or any other compatible hack. Now, the more powerful the exploit, the more functionality is provided. Examples of functionality include a file browser and NAND backup. Plus it’s further extended with scripts.</li><li><strong>Anemone3DS</strong> by astronautlevel: With a multitude of features, it soon became <em>the app</em> for managing HOME Menu themes <sup id="bibref:135"><a href="#bib:anti_piracy-anemone3ds" role="doc-biblioref">[135]</a></sup>.</li><li><strong>nds-bootstrap</strong> by Rocket Robz: As the name indicates, it loads Nintendo DS software (ROMs and homebrew) from the SD card <sup id="bibref:136"><a href="#bib:anti_piracy-nds_bootstrap" role="doc-biblioref">[136]</a></sup>. While it’s designed to support the three portable consoles (the Nintendo 3DS, DSi and DS, the latter requiring a flashcard), loading it from the 3DS will kickstart <code>TWL_FIRM</code>, meaning there’s no emulation at all. It’s most commonly used through ‘TWLMenu’ (now ‘TWiLight Menu++’), the front-end of nds-bootstrap.</li></ul><p>It’s worth mentioning that, at the time of this writing, these are the most popular utilities to install on a hacked 3DS.</p><h4 id="the-golden-age-2016-2017">The Golden Age (2016-2017)</h4><p>While a universal and powerful solution, installing arm9loaderhax was still considered a complicated and dangerous activity. Not only does this require dumping the console’s OTP memory beforehand (using other exploits), but neglecting any step could potentially turn a working Nintendo 3DS into a rock.</p><p>But fear not as new developments were in the works (a mighty effort considering Nintendo was still battling to protect their console).</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg"><picture><img alt="Image" width="1280" height="720" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/33c3.437cce691de5706d0c70eab1e6a3bb0427a28c33f4285a3c0bbd5c8083b49c5b.jpeg" loading="lazy"></picture></a><figcaption>naehrwert, nedwill and derrek presenting a new set of findings at the 33nd Chaos Communication Congress (December 2016) <sup id="bibref:137"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[137]</a></sup>.</figcaption></figure><p>At the 33C3 conference, derrek unveiled two major discoveries <sup id="bibref:138"><a href="#bib:anti_piracy-33c3" role="doc-biblioref">[138]</a></sup>, which led to subsequent milestones.</p><div><ul><li id="tab-13-2-sighax-link"><a href="#tab-13-2-sighax">Sighax</a></li><li id="tab-13-3-boot9strap-link"><a href="#tab-13-3-boot9strap">Boot9strap</a></li><li id="tab-13-4-ntrboot-link"><a href="#tab-13-4-ntrboot">Ntrboot</a></li></ul><div><div id="tab-13-1-extracting-boot9"><p>For some reason, the contents of ARM9’s RAM are not cleared upon reset. Thus, Derrek discovered that, with the use of external hardware, he could override the exception vectors from ARM9’s RAM (previously copied from Boot9) with arbitrary code. Then, reset the system, glitch it at very precise timing (also using external hardware) to trigger an exception and hope for the ARM9 to have executed the new code. This will have included something like ‘Copy all contents of Boot9 to X location in RAM’.</p><p>Lo and behold, this did work. With this, Derrek and others managed to analyse the contents of the Boot9 ROM, allowing new vulnerabilities to be found.</p></div><div id="tab-13-2-sighax"><h5 id="tab-13-2-sighax">Sighax</h5><p>One vulnerability from Boot9 was <strong>Sighax</strong> <sup id="bibref:139"><a href="#bib:anti_piracy-sighax" role="doc-biblioref">[139]</a></sup>, a flaw in Boot9’s RSA-2048 signature verification. RSA signatures of type ‘PKCS #1 v1.5’ (adopted by this system) contain an area called <strong>padding</strong> to prevent being reversed. Additionally, they store an SHA-256 hash encoded with a model called ‘ASN.1’, this <strong>guarantees the authenticity of the data</strong> being decrypted.</p><p>Now, the respective parser found in Boot9 <strong>lacks several protections</strong>, including bounds checking. In the end, this allowed Derrek to produce a crafted RSA signature (through brute-forcing) that will always succeed on any data. In doing so, the <strong>entirety of the chain of trust was nullified</strong>.</p><p>For the curious, I recommend reading a comprehensive post in GBATemp describing the theory more calmly <sup id="bibref:140"><a href="#bib:anti_piracy-mrjason" role="doc-biblioref">[140]</a></sup>.</p><p>With this, one would now be allowed to craft a firmware for the ARM9 core, sign it with a crafted RSA signature, install it on NAND and Boot9 will ‘just run it’. The question now is, how can the average user do this using an unmodified console?</p></div><div id="tab-13-3-boot9strap"><h5 id="tab-13-3-boot9strap">Boot9strap</h5><p>The year is 2017. Most know about the existence of Sighax but only a handful can apply it, all because the new method requires a crafted RSA signature and writing access to the NAND, none of which is easy to come by (and let’s not forget Nintendo was still clamping down hard on user-land exploits through system updates). Luckily, Sighax was in the process of being democratised.</p><p>Even though Derrek’s announcement didn’t include a suitable RSA signature or a copy of Boot9 (due to copyright reasons, I’m guessing), that didn’t stop hackers SciresM and Myria from finding alternative resources that would enable them to craft an RSA signature.</p><p>In summary, they discovered that system versions before <code>1.0.0</code> shared similar flaws to those previously exposed with Sighax <sup id="bibref:141"><a href="#bib:anti_piracy-sighax_pres" role="doc-biblioref">[141]</a></sup> and, thanks to this, they were able to begin brute-forcing RSA signatures. The result was a success, a match was eventually found with the help of plenty of Nvidia GPUs <sup id="bibref:142"><a href="#bib:anti_piracy-sighax_math" role="doc-biblioref">[142]</a></sup>.</p><p>Now that they could craft an alternative firmware that Boot9 would accept, they needed to find a way to redirect Boot9 to their payload. The challenge was to redirect execution before Boot9 hides its Boot ROM. To tackle this, the duo found a route through the ARM9’s exception handlers. The ARM9 can’t override these, but the NDMA can - and the CPU can command the NDMA to do so.</p><p>All in all, the team were able to use the NDMA to fill the exception handlers with a jump to arbitrary code, and then instruct the ARM9 to copy to <code>NULL</code>, resulting in an exception that would execute the payload with unrestricted access. In the end, this was packaged in a solution called <strong>boot9strap</strong> and served as an alternative bootloader that could either load a payload from the SD card or continue to boot normally. Consequently, Godmode9 was extended to backup OTP and the Boot ROMs, if needed.</p><p>And so, boot9strap quickly displaced arm9loaderhax as the de facto solution for loading arbitrary code with maximum privileges.</p></div><div id="tab-13-4-ntrboot"><h5 id="tab-13-4-ntrboot">Ntrboot</h5><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg"><picture><img alt="Image" width="947" height="900" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/ntrboot_flashcard.70f62f8dd94963d654415db4d0f2a190b5a1c15eb60acc6ba713c26bb295d849.jpg" loading="lazy"></picture></a><figcaption>Some DS Flashcards sold after the discovery of ntrboot came with a switch to enable a ‘3DS mode’ (see the top corner of the photo), this enables trigger ntrboot.</figcaption></figure><p>At this point, there was only one question left: How could users install boot9strap?</p><p>Well, the team didn’t stop there. By taking a look at their recent Boot ROM dumps, they found an interesting routine: During boot, Boot9 will query if a specific <strong>key combination is pressed</strong> and the <strong>lid is closed</strong>. If so, Boot9 will redirect execution to the inserted Nintendo DS card (with full privileges).</p><p>Thus, <strong>ntrboot</strong> came to fruition: Flash a sighax-signed payload into a Nintendo DS flashcard, use a magnet to simulate a closed shell and press the required key combination. Instant Boot9 privileges.</p><p>If this wasn’t enough, Nintendo couldn’t fix any of these vulnerabilities through software updates, as they’re hardwired into the Boot ROM. A possible solution would’ve been to ship new hardware revisions, yet, none ever appeared.</p></div></div></div><h4 id="the-remaining-years-2018-present">The remaining years (2018-present)</h4><p>Now that the homebrew community has achieved its magnum opus, the remaining years of the Nintendo 3DS will only see the streamlining of hacking methods, all of which share the same objective: Install boot9strap.</p><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_67144305ea863bc41e118057e27ecc2f.webp 500w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_15f7a7c7977b5b7d951c8f22169b0417.webp 800w,
https://www.copetti.org/images/consoles/nintendo3ds/homebrew/_hu1ed3670aa3a156a8d4deb11d9d975c6b_89755_60452d1ade267dfc9302be1e561295f6.webp 985w"><img alt="Image" width="985" height="640" src="https://www.copetti.org/images/consoles/nintendo3ds/homebrew/3dshacks.93122646fef0484a0abbe6ae842c573dc81e29a734f760d158547a50a2964172.png" loading="lazy"></picture></a><figcaption>As the methodologies used to hack a 3DS drastically evolve, sometimes too quickly for new users, community-maintained websites like 3ds.hacks.guide currently holds a reputation as the most reliable and updated set of tutorials.</figcaption></figure><p>By this point in time, there were many exploits in the wild: ‘SoundHax’, ‘Safehax’, ‘Browserhax’… too many to mention here. For the curious, 3DBrew provides a comprehensive list <sup id="bibref:143"><a href="#bib:anti_piracy-user_flaws" role="doc-biblioref">[143]</a></sup>.</p><p>To give you an idea of how elegant exploitation became by 2023, let me show you a common method users relied on and didn’t require extra hardware. This process was called ‘seedminer + BannerBomb3’ and combined the following vulnerabilities, the majority of them authored by zoogie:</p><ol><li><strong>seedminer</strong>: User data installed in the 3DS’ SD card is encrypted using AES-128-CTR. Its key is constructed from other keys found in a file called <code>movable.sed</code> (console-unique, stored in NAND). Well, it was discovered that this file can be re-constructed by using the console’s Friend Code, subdirectory names in the SD card (generated by the console) and short-term brute-forcing. Once extracted, the keys allowed to tamper with DSiWare data in the SD card.</li><li><strong>BannerBomb3</strong>: An exploit that overflows the stack of the Settings app while it tries to parse the banner of an installed DSiWare title <sup id="bibref:144"><a href="#bib:anti_piracy-bannerbomb" role="doc-biblioref">[144]</a></sup>. Combined with seedminer, this serves as an entry-level exploit with Kernel11 privileges.</li><li>Now, how to take advantage of BannerBomb3 (i.e.&nbsp;which payload to use) depended on the tutorial the user was following at the time. For simplicity purposes, there were two routes:</li></ol><div><ul><li id="tab-14-1-the-safe-mode-route-link"><a href="#tab-14-1-the-safe-mode-route">The Safe Mode route</a></li></ul><div><div id="tab-14-1-the-safe-mode-route"><h5 id="tab-14-1-the-safe-mode-route">The Safe Mode route</h5><p>This route consisted of exploiting <code>SAFE_FIRM</code> and was described in earlier tutorials:</p><ol><li><strong>unSAFE_MODE</strong>: Users can boot into Safe Mode by pressing a combination of buttons during the console’s boot, the alternative firmware then enables the user to perform a system update, which is useful for repairing the console. Well, zoogie discovered that the proxy settings can be overflowed <sup id="bibref:145"><a href="#bib:anti_piracy-unsafe_mode" role="doc-biblioref">[145]</a></sup>. Hence, providing user-land execution within Safe Mode.</li><li><strong>safehax</strong>: a port of ‘firmlaunch-hax’ to work under SAFE_FIRM. Nintendo originally patched it with system update <code>9.5.0</code> released in February 2015 <sup id="bibref:146"><a href="#bib:anti_piracy-9_5" role="doc-biblioref">[146]</a></sup>. Yet, SAFE_FIRM is an immutable replica of the factory firmware, and thus it features old exploits NATIVE_FIRM once <em>enjoyed</em>.<ol><li><strong>firmlaunch-hax</strong>: When the firmware is booting, the ARM9 stores the firmware’s header in FCRAM for verifying and then parsing. With the help of a race condition, execution can take control of the ARM9, so the boot9strap installer can be launched. Nintendo fixed this by keeping the header in ARM9 RAM instead, although this stayed unpatched on SAFE_FIRM.</li></ol></li></ol></div><div id="tab-14-2-the-home-menu-route"><p>Sometime later, a new route was proposed by new tutorials. This exploited the HOME Menu with a new Menuhax-style hack:</p><ol><li><strong>menuhax67</strong>: The screen brightness configuration value can be overflown <sup id="bibref:147"><a href="#bib:anti_piracy-menuhax67" role="doc-biblioref">[147]</a></sup>, leading to user-land control from the HOME Menu.</li><li><strong>nimdsphax</strong>: An modern exploit chain combining ‘ctr-httpwn’, ‘nimhax’ and ‘dsp pwn’ <sup id="bibref:148"><a href="#bib:anti_piracy-nimdsphax" role="doc-biblioref">[148]</a></sup>.<ol><li><strong>ctr-httpwn</strong> by yellows8: The HTTP service used for network connections can be controlled by overriding its heap memory (using the old GPU DMA exploit) <sup id="bibref:149"><a href="#bib:anti_piracy-httpwn" role="doc-biblioref">[149]</a></sup>.</li><li><strong>nimhax</strong> by luigoalma: Uses ctr-httpwn to escalate and take over the services that control the user file system, console configuration and application management <sup id="bibref:150"><a href="#bib:anti_piracy-nimhax" role="doc-biblioref">[150]</a></sup>.</li><li><strong>dsp pwn</strong> by luigoalma: Uses nimhax to take control of the DSP, which in turn uses the GPU’s DMA to override the Kernel9 memory space. Thus, obtaining ARM9 privileges.</li></ol></li></ol></div></div></div><h5 id="post-2023-and-conclusions">Post-2023 and conclusions</h5><p>Be as it may, at the time of this writing, Nintendo hasn’t quite surrendered to the cat-and-mouse game. In May 2023, system update <code>11.17.0</code> patched BannerBomb3, nullifying one of the last entry points that didn’t require additional materials <sup id="bibref:151"><a href="#bib:anti_piracy-11_17" role="doc-biblioref">[151]</a></sup>. This means users will now need to either obtain a legitimate 3DS game (which can then be exploited), an ntrboot-compatible DS flashcard; or wait for a WebKit exploit (there’s one only left for the New 3DS browser <sup id="bibref:152"><a href="#bib:anti_piracy-skaterhax" role="doc-biblioref">[152]</a></sup>).</p><hr><h2 id="thats-all-folks">That’s all folks</h2><figure><a href="https://www.copetti.org/images/consoles/nintendo3ds/photos/my3dss.291f0c00616803ac9f9b570af267a17901a729074ded39c03576c16d4cd6917f.webp"><picture><img alt="Image" width="1037" height="500" src="https://www.copetti.org/images/consoles/nintendo3ds/photos/_hu8ac0a8a08ff3edfea9a6be84b3a66ace_53002_245d657b9cd3a8518579bd5017c67086.png" loading="lazy"></picture></a><figcaption>My Nintendo 3DS(s). Apart from the XL one on the left, I bought two extra for this article: The red one you see on the right (originally listed as ‘broken’, turned out the power socket is just flaky) and another one (<em>correctly</em> listed ‘for parts’) to take the motherboard photos.</figcaption></figure><p>Phew, that was another one of those long articles. I’m glad you managed to keep up and reach the end!</p><p>If you are curious, this article took me almost a year to finish, mainly due to a combination of multiple factors, but the important thing is that I ultimately managed to complete it.</p><p>Looking back, it’s hard to admit this console didn’t enjoy the same degree of success as the Nintendo DS. Considering all of its offerings analysed here, I think many external factors hindered its marketing. For starters, the timing was unfortunate and the price tag wasn’t exactly tempting. From my perspective, back when it launched in 2011, the ‘08 financial crisis was hitting hard (I was living in Spain back then), so the Nintendo 3DS wasn’t exactly on adults’ (and kids’) priorities. I eventually got mine in 2018, by then living in the UK.</p><p>In any case, I want to thank the #ReSwitched and #Godmode9 for spotting lots of mistakes in my initial drafts. This has been the most intricate console I’ve written about (to this date!), nevertheless, I’m very grateful to find communities willing to help out.</p><p>Until next time!<br>Rodrigo</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Outperforming larger language models with less training data and smaller models (238 pts)]]></title>
            <link>https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</link>
            <guid>37606352</guid>
            <pubDate>Fri, 22 Sep 2023 00:29:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html">https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</a>, See on <a href="https://news.ycombinator.com/item?id=37606352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2762083676649407668">
<p><span>Posted by Cheng-Yu Hsieh, Student Researcher, and Chen-Yu Lee, Research Scientist, Cloud AI Team
</span>

</p><p>
Large language models (LLMs) have enabled a new data-efficient learning paradigm wherein they can be used to solve unseen new tasks via <a href="https://arxiv.org/abs/2005.14165">zero-shot or few-shot prompting</a>. However, LLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using <a href="https://arxiv.org/abs/2201.12023">specialized infrastructure</a>, not to mention that today's state-of-the-art LLMs are composed of over <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">500 billion parameters</a>. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance.
</p>

<p>
To circumvent these deployment challenges, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: <a href="https://arxiv.org/abs/1801.06146">fine-tuning</a> or <a href="https://arxiv.org/abs/1503.02531">distillation</a>. Fine-tuning updates a pre-trained smaller model (e.g., <a href="https://arxiv.org/abs/1810.04805">BERT</a> or <a href="https://arxiv.org/abs/1910.10683">T5</a>) using downstream manually-annotated data. Distillation trains the same smaller models with labels generated by a larger LLM. Unfortunately, to achieve comparable performance to LLMs, fine-tuning methods require human-generated labels, which are expensive and tedious to obtain, while distillation requires large amounts of unlabeled data, which can also be hard to collect.
</p>

<p>
In “<a href="https://arxiv.org/abs/2305.02301">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</a>”, presented at <a href="https://2023.aclweb.org/">ACL2023</a>, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs’ performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s1570/image3.png"><img data-original-height="788" data-original-width="1570" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjeIs4yaBA3Ir55j869FMzdmRdf7OxiIjsWl05GU48ikYOHZGLk1H8tIHeKKBaY_xER0QITv5DUhADZvqS1os6mNA_nLQKqwW7DOXnwcnPl6BhsMJ_LKTvglGUrHR5_QC8MIe3K7i9zyfcWkwzvjPhXLifYijgkeeG_1yn9EMm-ol9eI9Cv_rz71wMyGfk2/s16000/image3.png"></a></td></tr><tr><td>While LLMs offer strong zero and few-shot performance, they are challenging to serve in practice. On the other hand, traditional ways of training small task-specific models require a large amount of training data. Distilling step-by-step provides a new paradigm that reduces both the deployed model size as well as the number of data required for training.</td></tr></tbody></table>


<br>


<h2>Distilling step-by-step</h2>


<p>
The key idea of distilling step-by-step is to extract informative <em>natural language</em> <em>rationales (i.e., </em>intermediate reasoning steps)<em> </em>from LLMs, which can in turn be used to train small models in a more data-efficient way. Specifically, natural language rationales explain the connections between the input questions and their corresponding outputs. For example, when asked, “<em>Jesse's room is 11 feet long and 15 feet wide. If she already has 16 square feet of carpet, how much more carpet does she need to cover the whole floor?</em>”, an LLM can be prompted by the few-shot <a href="https://blog.research.google/2022/05/language-models-perform-reasoning-via.html">chain-of-thought</a> (CoT) prompting technique to provide intermediate rationales, such as, “<em>Area = length * width. Jesse’s room has 11 * 15 square feet.</em>” That better explains the connection from the input to the final answer, “<em>(11 * 15 ) - 16</em>”. These rationales can contain relevant task knowledge, such as “<em>Area = length * width”</em>, that may originally require many data for small models to learn. We utilize these extracted rationales as additional, richer supervision to train small models, in addition to the standard task labels.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s1999/image4.png"><img data-original-height="932" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN3UISRCKswIxZuTsi08LUV15urAL9GuG65SHPLQcyxa6JKL_aKMtYCiaFmaQ-TC59otrYI7g-DXLTa8v-h4WgOT_B1CqKtMZG7gyRiw4YoQcUn1EUj386PgYZ1PP-Wq9vDSer0D2kdYsT0n8XgAq9AdokWEtfgUBs-1KUZc2H8lMHuyjQ-nA6YFDuewrI/s16000/image4.png"></a></td></tr><tr><td>Overview on distilling step-by-step: First, we utilize CoT prompting to extract rationales from an LLM. We then use the generated rationales to train small task-specific models within a multi-task learning framework, where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix.</td></tr></tbody></table>


<p>
Distilling step-by-step consists of two main stages. In the first stage, we leverage few-shot CoT prompting to extract rationales from LLMs. Specifically, given a task, we prepare few-shot exemplars in the LLM input prompt where each example is composed of a triplet containing: (1) input, (2) rationale, and (3) output. Given the prompt, an LLM is able to mimic the triplet demonstration to generate the rationale for any new input. For instance, in a <a href="https://arxiv.org/abs/1811.00937">commonsense question answering task</a>, given the input question “Sammy wanted to go to where the people are. Where might he go? Answer Choices: (a) populated areas, (b) race track, (c) desert, (d) apartment, (e) roadblock”, distilling step-by-step provides the correct answer to the question, “(a) populated areas”, paired with the rationale that provides better connection from the question to the answer, “The answer must be a place with a lot of people. Of the above choices, only populated areas have a lot of people.” By providing CoT examples paired with rationales in the prompt, the <a href="https://arxiv.org/abs/2005.14165">in-context learning ability</a> allows LLMs to output corresponding rationales for future unseen inputs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s1999/image2.png"><img data-original-height="1175" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqsexcOGkZbTGQlOWdNiio-F46cqdntwxpwL0lQL-qi1aszPBpwRkWVL3IpCpINbWI0lQ3ZT2MWH_E27vMzrHbjdJc4rFgbzkHMK1u2EcS3nwKx2-UG1S9sVnVH9OUPqn1IVAYu2kVxX9PHpgklxQ_VEWBFQ2nwd-cZ77EaPnLjClRSyedSrpG6uc-HQkg/s16000/image2.png"></a></td></tr><tr><td>We use the few-shot CoT prompting, which contains both an example rationale (<strong>highlighted in green</strong>) and a label (<strong>highlighted in blue</strong>), to elicit rationales from an LLM on new input examples. The example is from a commonsense question answering task.</td></tr></tbody></table>


<p>
After the rationales are extracted, in the second stage, we incorporate the rationales in training small models by framing the training process as a multi-task problem. Specifically, we train the small model with a novel <em>rationale generation task</em> in addition to the standard <em><a href="https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html?m=1">label prediction task</a></em>. The rationale generation task enables the model to learn to generate the intermediate reasoning steps for the prediction, and guides the model to better predict the resultant label. We prepend <a href="https://arxiv.org/abs/1910.10683">task prefixes</a> (i.e., [label] and [rationale] for label prediction and rationale generation, respectively) to the input examples for the model to differentiate the two tasks.
</p>




<h2>Experimental setup</h2>


<p>
In the experiments, we consider a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model as the LLM. For task-specific downstream models, we use <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5 models</a>. For CoT prompting, we use the <a href="https://arxiv.org/abs/2201.11903">original CoT prompts</a> when available and curate our own examples for new datasets. We conduct the experiments on four benchmark datasets across three different NLP tasks: <a href="https://arxiv.org/abs/1812.01193">e-SNLI</a> and <a href="https://arxiv.org/abs/1910.14599">ANLI</a> for<a href="https://arxiv.org/abs/1508.05326"> natural language inference</a>; <a href="https://arxiv.org/abs/1811.00937">CQA</a> for commonsense question answering; and <a href="https://arxiv.org/abs/2103.07191">SVAMP</a> for <a href="https://aclanthology.org/N16-1136/">arithmetic math word problems</a>. We include two sets of baseline methods. For comparison to <a href="https://arxiv.org/abs/2005.14165">few-shot prompted LLMs</a>, we compare to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompting</a> with a <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">540B PaLM</a> model. In the <a href="https://arxiv.org/abs/2305.02301">paper</a>, we also compare standard task-specific model training to both <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a> and <a href="https://arxiv.org/abs/1503.02531">standard distillation</a>. In this blogpost, we will focus on the comparisons to standard fine-tuning for illustration purposes.
</p>



<h3>Less training data</h3>


<p>
Compared to <a href="https://arxiv.org/abs/1801.06146">standard fine-tuning</a>, the distilling step-by-step method achieves better performance using much less training data. For instance, on the e-SNLI dataset, we achieve better performance than standard fine-tuning when using only 12.5% of the full dataset (shown in the upper left quadrant below). Similarly, we achieve a dataset size reduction of 75%, 25% and 20% on ANLI, CQA, and SVAMP.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s1999/image6.png"><img data-original-height="1477" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgKP_rzQubcfVH0qhwwBuxfPMMNMsQz0q1a7CriO3VzoNfmbeEH_9LfFs2dioPdw3jGNAkrje2kuzcRswHhugAIFrIe-1qU5b7tU_dTGzjLgYf9uQp_Ag64sDlPR3xaQtXnSYEbYRW9eY37si8LcVtLMVh5d2MMlAEp1ZdVC8K--ajgaUmVYfD5POBJINtj/s16000/image6.png"></a></td></tr><tr><td>Distilling step-by-step compared to standard fine-tuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, distilling step-by-step is able to outperform standard fine-tuning, trained on the full dataset, by using much less training examples.</td></tr></tbody></table>

<br>


<h3>Smaller deployed model size</h3>


<p>
Compared to <a href="https://arxiv.org/abs/2201.11903">few-shot CoT prompted LLMs</a>, distilling step-by-step achieves better performance using much smaller model sizes. For instance, on the e-SNLI dataset, we achieve better performance than 540B PaLM by using a 220M T5 model. On ANLI, we achieve better performance than 540B PaLM by using a 770M T5 model, which is over 700X smaller. Note that on ANLI, the same 770M T5 model struggles to match PaLM’s performance using standard fine-tuning.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s1999/image5.png"><img data-original-height="1384" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjsdcmUOguiWiZ4Uy_PJht9ygmWRnS0KZyKpFZDOOGqTn5MhkVMpKJWxq44-6lIg6oEU4Gf26JQ56Onaf-i218CIVPZUyv5XexmcL3UwB6QcsiRGL0VR4Ye_ZVXJqYPqoN_3P3AEXswNqUIjryoj2Mzlik4mhjQAE4NUnnhIuQrmqSRO26cD13ZZqYOXokD/s16000/image5.png"></a></td></tr><tr><td>We perform distilling step-by-step and standard fine-tuning on varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700× smaller models on ANLI. Standard fine-tuning fails to match LLM’s performance using the same model size.</td></tr></tbody></table>


<br>


<h3>Distilling step-by-step outperforms few-shot LLMs with smaller models using less data</h3>


<p>
Finally, we explore the smallest model sizes and the least amount of data for distilling step-by-step to outperform PaLM’s few-shot performance. For instance, on ANLI, we surpass the performance of the 540B PaLM using a 770M T5 model. This smaller model only uses 80% of the full dataset. Meanwhile, we observe that standard fine-tuning cannot catch up with PaLM’s performance even using 100% of the full dataset. This suggests that distilling step-by-step simultaneously reduces the model size as well as the amount of data required to outperform LLMs.
</p>


<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s1999/image1.png"><img data-original-height="1400" data-original-width="1999" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi5-l100eeQMDnxDnZYquKK0wF1DsFQF597trg--HbmCJI3F6DJhohdzdIEDIcvZoSDAUoKWmmT75ZQV1eSl56r_GifKPumMuxEUlLbA2kUQTm9KNQLI3PzfjbdeOCVvXAeNTbMFh8VmYYHpes6PhCXlgJo3O5m8SqoRyEcwYtIE2puC6v13HL6e-76OErd/s16000/image1.png"></a></td></tr><tr><td>We show the minimum size of T5 models and the least amount of human-labeled examples required for distilling step-by-step to outperform LLM’s few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to standard fine-tuning.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
We propose distilling step-by-step, a novel mechanism that extracts rationales from LLMs as informative supervision in training small, task-specific models. We show that distilling step-by-step reduces both the training dataset required to curate task-specific smaller models and the model size required to achieve, and even surpass, a few-shot prompted LLM’s performance. Overall, distilling step-by-step presents a resource-efficient paradigm that tackles the trade-off between model size and training data required.
</p>




<h2>Availability on Google Cloud Platform</h2>


<p>
Distilling step-by-step is available for private preview on <a href="https://cloud.google.com/vertex-ai">Vertex AI</a>. If you are interested in trying it out, please contact <a href="mailto:vertex-llm-tuning-preview@google.com">vertex-llm-tuning-preview@google.com</a> with your Google Cloud Project number and a summary of your use case.
</p>




<h2>Acknowledgements</h2>


<p>
<em>This research was conducted by Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Thanks to Xiang Zhang and Sergey Ioffe for their valuable feedback.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unlimited Kagi searches for $10 per month (1480 pts)]]></title>
            <link>https://blog.kagi.com/unlimited-searches-for-10</link>
            <guid>37603905</guid>
            <pubDate>Thu, 21 Sep 2023 20:32:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/unlimited-searches-for-10">https://blog.kagi.com/unlimited-searches-for-10</a>, See on <a href="https://news.ycombinator.com/item?id=37603905">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p><img src="https://assets.kagi.com/v1/kagi_assets/doggo/doggo_2.png" alt=""></p>

<p>This year has been extraordinary for <a href="https://kagi.com/">Kagi</a>. We had tremendous support from our customers, and we want to start this by taking a moment to say <strong>thank you</strong>. We really appreciate it.</p>

<p>Since we launched Kagi, our users have raved about the search quality, but many have stressed about the cost of search and having to change their search habits when switching from an ad-supported service, so we’re fixing that.&nbsp;</p>

<p>We’re thrilled to announce that <strong>unlimited search is now included in our $10/month Professional plan and our Ultimate, Family, and Duo plans</strong>.</p>

<blockquote>
<p>“When I first heard of Kagi (and saw the prices) I thought: this is the stupidest thing ever, jesus it’s priced for Silicon Valley bros.</p>

<p>Well, I just turned on yearly payments seconds ago… I guess it’s just that good?”</p>
</blockquote>

<p><em>Kagi Discord user, a couple of weeks ago - good news for this member :)</em></p>

<p>When we first <a href="https://blog.kagi.com/update-kagi-search-pricing">adjusted our pricing</a>, the search landscape was undergoing seismic shifts and brimming with uncertainty. The world watched as the emergence of powerful AI began rewriting the rules of search, and giants in the tech industry started a battle for dominance. It was a time of challenges and tough decisions for a small bootstrapped company like ours. We were determined to ensure Kagi was still around next year and remains a constant, reliable companion for your search needs.</p>

<p>Today, the tides are changing. With new search sources proving more cost-efficient, the improved efficiency of our infrastructure, and the broader market embracing Kagi, we can again offer an unlimited experience to a broader group of users. We’re excited that this change will let many more people enjoy a fun, ad-free, and user-centric web search.</p>

<p>Here is how the changes look through our membership plans.</p>

<h3>Professional Plan</h3>

<p>Kagi has one goal -  creating the most delightful search experience for our members. Starting today, enjoy unlimited searches with Kagi. For a mere $10/mo, step into the expansive world of Kagi’s renowned search quality and rich features, all crafted with you in mind.</p>

<p>Oh, and by the way, this also includes unlimited use of our <a href="https://kagi.com/summarizer/index.html">Universal Summarizer</a>, which can summarize unlimited-length documents, audio, and video!</p>

<p>As a part of this update, we are also introducing the annual payment option with 10% off!</p>

<blockquote>
<p>“For me Kagi represents an incredible accomplishment: the first search engine that gives better results than Google, respects privacy, offers <a href="https://help.kagi.com/kagi/features/website-info-personalized-results.html">customization</a> and so much more.</p>

<p>Thank you.”</p>
</blockquote>

<p><em>No, thank you, for supporting us!</em></p>

<h3>Starter Plan (formerly called Standard)</h3>

<p>This plan continues as our special introductory offer, designed for those ready to step beyond the free trial but still exploring whether the Professional plan aligns with their needs or fall into the category of most internet users who search just a few times a day on average.</p>

<p>We are removing the pay-per-use component (to simplify our billing infrastructure) and making it simple: 300 searches for $5/month. If a user hits this cap consistently, we recommend upgrading to unlimited searches at $10/month. This change also allows us to add the annual payment option at 10% off, which many of you requested.</p>

<blockquote>
<p>“3 months in - I find myself annoyed when I’m on a device that isn’t mine and I have to use google.”</p>
</blockquote>

<p><em>Same here, same here</em></p>

<h3 id="ultimate-plan">Ultimate Plan</h3>

<p>We’re aware that many of you used the Ultimate Plan for its access to unlimited searches. Starting today, we will be giving Ultimate Plan users access to the closed beta of the next generation of tools that Kagi is bringing to the web, and we’re targeting access for all our current Ultimate users in the next few days.</p>

<p>While we can not reveal yet what that is in detail as we’re still iterating quickly, no competitor comes close to the value proposition of this new feature suite. We would like you to help us better shape their future and join the fun!</p>

<p>When we release these new tools publicly, Ultimate users will have exclusive use of their most advanced features - ensuring your Kagi experience remains top of the line.</p>

<p>The Ultimate plan is also still the best way to support Kagi on its mission to humanize the web, and we are incredibly grateful to all of you who have chosen to do so.</p>

<blockquote>
<p>“The belief the search was better because they have so much history of me and such a strong ML team is why I kept using Google Search despite trying to get away from other Google products.
Kagi <a href="https://help.kagi.com/kagi/features/lenses.html">lenses</a> really do show how much it’s not true, and how much Google’s collection of my data isn’t actually returning on my data and privacy investment.”</p>
</blockquote>

<p><em>Turns out people know what they want better than algorithms do</em></p>

<p><a href="https://kagifeedback.org/assets/files/2023-09-21/1695322703-234286-screenshot-2023-09-21-at-115717.png"><img src="https://kagifeedback.org/assets/files/2023-09-21/1695322703-234286-screenshot-2023-09-21-at-115717.png" \=""></a>
</p><center><em>New Kagi Search Individual plans</em></center>

<h3>Family &amp; Duo Plans</h3>

<p>Family moments are precious, and we’re here to enrich them. Now, the whole family can explore a world of information together, with unlimited searches for $20/month (Family, up to six members) and $14/mo (Duo, for a couple).</p>

<p>We are inching closer to our vision where families can trust their search engine to prioritize their well-being over third parties and advertisers and where young minds grow uninfluenced by the data-hungry algorithms profiling them and changing their behavior.</p>

<p>Discover the joy of safe and ad-free search, nurturing curiosity without compromise. Learn more about the values that shape the <a href="https://blog.kagi.com/family-plan">Kagi Family plan</a>.</p>

<blockquote>
<p>“This sort of stuff makes me really happy to be a Kagi subscriber. Not only do I get value out of Kagi, but it shows me that the money is being used to develop Kagi in a way I agree with. By comparison, REDACTED (just picking one of my subs) feels hostile to me. I pay them, but I would cancel in a heartbeat if I felt I had options.
I really appreciate Kagi’s development matching what i feel like i’m buying. Thanks Kagi Team ”</p>
</blockquote>

<p><em>Awww, thank you! We'll keep on putting your money to good use</em></p>

<p><a href="https://kagifeedback.org/assets/files/2023-09-21/1695322702-804109-screenshot-2023-09-21-at-115735.png"><img src="https://kagifeedback.org/assets/files/2023-09-21/1695322702-804109-screenshot-2023-09-21-at-115735.png" \=""></a>
</p><center><em>New Kagi Search Family plans</em></center>

<h3>Frequently Asked Questions</h3>

<p>Q. <strong>How do I sign up?</strong><br>
A. Click <a href="https://kagi.com/onboarding?p=choose_plan">here</a> to become a member.</p>

<p>Q. <strong>Do I need to do anything to get the benefits of the new plans?</strong><br>
A. All changes will be automatic and applied to your account today.</p>

<p>Q. <strong>I do not have a credit card. Can I also pay for Kagi with PayPal or crypto?</strong><br>
A. Yes. We have recently added <a href="https://blog.kagi.com/accepting-paypal-bitcoin">new payment options</a> to allow payment through credit card, PayPal, and Bitcoin/Lightning through OpenNode.</p>

<p>Q. <strong>Now that I don’t have to count my searches, how can I switch all my browsers and devices to Kagi?</strong><br>
A. We have several options available. See <a href="https://help.kagi.com/kagi/getting-started/setting-default.html">here</a> for more details.</p>

<p>Q. <strong>I have an outstanding pay-per-use balance on my current Standard or Professional plan. What happens now?</strong><br>
A. Effective today, we are waiving all pending metered usage charges for you. Yahoo!</p>

<p>Q. <strong>I noticed that the Ultimate plan changed to 10% off for annual payments. I previously had 15% off. Do I remain locked in?</strong><br>
A. Yes, as long as you do not cancel, you will have the previous discount applied to your renewals.</p>

<p>Q. <strong>When do I get the new advanced features of the Ultimate plan?</strong><br>
A. We are starting to roll out access today in batches, and we expect to onboard all Ultimate plan users to new features by the end of the next week.</p>

<p>Q. <strong>How can I get the new Ultimate plan level features in the Family plan?</strong><br>
A. We haven’t established a mechanism to incorporate Ultimate plan features into the Family plan yet. However, we are open to suggestions and would appreciate any ideas you may have. Please share your thoughts at <a href="https://kagifeedback.org/">KagiFeedback.org</a>.</p>

<p>Q. <strong>Do I need to do anything with my grandfathered Early-adopter/Legacy Professional plan?</strong><br>
A. No. Additionally, we will be simplifying this over the next few days, as this is all the same Professional plan now. If you have the Early-Adopter flag, you will still keep it - we just didn’t have anything to tie it to right now, which does not mean we won’t in the future.</p>

<p>Q. <strong>I’m keen to support Kagi but concerned about its longevity, given the history of search startups. How can I be assured Kagi is here to stay?</strong><br>
A. What sets Kagi apart from other search engines is our fundamentally different, user-centric approach that prioritizes the interests of our members and the alignment of incentives. It’s worth noting that Kagi’s growth has been purely organic, without any expenditure on marketing or customer acquisition, attesting to the inherent value and trust our user base places in us and the growing need for better search.</p>

<p>We have priced our offering so that we do not rely financially on VC funding, data sales, or anything but our users’ contributions. We have had requests from our users to invest in Kagi, and we’re happy to offer that way to support us (see below).</p>

<p>We are striving to build a sustainable business, and we invite you to review our <a href="https://kagi.com/stats">live stats</a> for a transparent view of our steady progress. Your support matters, and we appreciate it a lot.</p>

<p>Q. <strong>How can I deepen my involvement with Kagi’s journey?</strong><br>
A. We are thrilled to hear of your interest! </p>

<p>Kagi is very open to collaboration, and we like to say that 50% of our product has been built by its members. We have a variety of ways you can <a href="https://help.kagi.com/kagi/support-and-community/contribute.html">get involved and contribute</a>. </p>

<p>We are also open to <a href="https://help.kagi.com/kagi/company/hiring-kagi.html">applications for positions</a> at Kagi. </p>

<p>If you’re already a Kagi user and are interested in becoming a shareholder, we’re considering <a href="https://blog.kagi.com/safe-round">another fundraiser</a> early next year. Please <a href="https://forms.gle/1Try2v6JtXbKSjKx9">get in touch</a>, and we will let you know once we are ready. </p>

<p>For other inquiries or simply to connect, join our <a href="https://kagi.com/discord">discord</a> or email Vladimir Prelovac (Kagi Founder) at <a href="mailto:vlad@kagi.com">vlad@kagi.com</a>.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Volvo to cease production of diesel cars in a few months (101 pts)]]></title>
            <link>https://www.reuters.com/business/autos-transportation/volvo-end-diesel-car-production-by-early-2024-2023-09-19/</link>
            <guid>37603769</guid>
            <pubDate>Thu, 21 Sep 2023 20:24:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/business/autos-transportation/volvo-end-diesel-car-production-by-early-2024-2023-09-19/">https://www.reuters.com/business/autos-transportation/volvo-end-diesel-car-production-by-early-2024-2023-09-19/</a>, See on <a href="https://news.ycombinator.com/item?id=37603769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/HR37LMMA75MNFK2EURH5SJ65FE.jpg" srcset="https://www.reuters.com/resizer/eKUimhD8oc7JKiXUm9Z8LC94IAk=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/HR37LMMA75MNFK2EURH5SJ65FE.jpg 480w,https://www.reuters.com/resizer/Xa1rfUNB1KkVAWjXq24DyfWAjv8=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/HR37LMMA75MNFK2EURH5SJ65FE.jpg 960w,https://www.reuters.com/resizer/3xCmZ1rKTF_YqdklDavjW3Q-wz0=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/HR37LMMA75MNFK2EURH5SJ65FE.jpg 1080w,https://www.reuters.com/resizer/iFGEctqmjnlvgP2QtB3M1Z832BQ=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/HR37LMMA75MNFK2EURH5SJ65FE.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="4019" height="2679" alt="Volvo Cars launches EX30 electric SUV in Milan"></p></div><p data-testid="Body"><span>Volvo reveals their new Volvo EX30 fully-electric small SUV vehicle during an event in Milan, Italy June 7, 2023. REUTERS/Claudia Greco/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">LONDON, Sept 19 (Reuters) - Volvo Cars <a data-testid="Link" href="https://www.reuters.com/markets/companies/VOLCARb.ST" target="_blank">(VOLCARb.ST)</a> said on Tuesday that it will end production of any remaining diesel models by early 2024 as it heads towards becoming an all-electric carmaker.</p><p data-testid="paragraph-1">"In a few months from now, the last diesel-powered Volvo car will have been built, making Volvo Cars one of the first legacy car makers to take this step," the Swedish company said in a statement.</p><p data-testid="paragraph-2">Majority owned by China's Geely <a data-testid="Link" href="https://www.reuters.com/markets/companies/0175.HK" target="_blank">(0175.HK)</a>, Volvo has committed to going fully electric by 2030.</p><p data-testid="paragraph-3">While a majority of the cars Volvo sold in Europe were diesel as recently as 2019, in 2022 they made up just 8.9% of the Swedish carmaker's sales.</p><p data-testid="paragraph-4">In August 33% of Volvo's sales were fully-electric or hybrid models. The company did not break out how many of the remaining 67% combustion-engine models were diesel and how many ran on petrol.</p><p data-testid="paragraph-5">Sales of diesel models have declined rapidly in Europe since Volkswagen's <a data-testid="Link" href="https://www.reuters.com/markets/companies/VOWG_p.DE" target="_blank">(VOWG_p.DE)</a> emission-cheating scandal and carmakers have been gradually reducing the number of diesel models available in their model lineups.</p><p data-testid="paragraph-6">Diesel vehicles comprised more than 50% of Europe's new car sales in 2015, but accounted for just over 14% of sales in July.</p><p data-testid="Body">Reporting by Nick Carey, Editing by Louise Heavens</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A customer stuck due to a hurricane who needed SSH (196 pts)]]></title>
            <link>https://rachelbythebay.com/w/2023/09/21/hurricane/</link>
            <guid>37603554</guid>
            <pubDate>Thu, 21 Sep 2023 20:09:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rachelbythebay.com/w/2023/09/21/hurricane/">https://rachelbythebay.com/w/2023/09/21/hurricane/</a>, See on <a href="https://news.ycombinator.com/item?id=37603554">Hacker News</a></p>
Couldn't get https://rachelbythebay.com/w/2023/09/21/hurricane/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Android 14 adds support for using your smartphone as a webcam (378 pts)]]></title>
            <link>https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam</link>
            <guid>37603467</guid>
            <pubDate>Thu, 21 Sep 2023 20:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam">https://www.esper.io/blog/android-14-adds-support-for-using-your-smartphone-as-a-webcam</a>, See on <a href="https://news.ycombinator.com/item?id=37603467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Video conferencing platforms like Zoom and Google Meet exploded in popularity during the COVID era, but the webcam market struggled to keep up. The best webcams were hard to get or were too expensive, causing most people to turn to cheaper, more readily available webcams or their laptop’s integrated webcam. The camera in most smartphones offers significantly better image quality than the one found in the vast majority of webcams, though, so many people installed third-party software on their phones to turn them into webcams. Starting in <a href="https://blog.esper.io/android-14-deep-dive/">Android 14</a>, it may not be necessary to use a third-party app to turn your smartphone into a webcam for your PC, as that functionality is getting baked into the Android OS itself — though there’s a catch.</p><p>When you plug an Android phone into a PC, you have the option to change the USB mode between file transfer/Android Auto (MTP), USB tethering (NCM), MIDI, or PTP. In Android 14, however, a new option can appear in USB Preferences: USB webcam. Selecting this option switches the USB mode to UVC (USB Video Class), provided the device supports it, turning your Android device into a standard USB webcam that other devices will recognize, including Windows, macOS, and Linux PCs, and <a href="https://source.android.com/docs/core/camera/external-usb-cameras">possibly even other Android devices</a>.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f7545560167bedc1f93ad3_Android_14_webcam_in_usb_preferences.png" loading="lazy" alt=""></p></figure><p>Webcam support in Android 14 is not enabled out of the box, however. In order to enable it, four things are required: a Linux kernel config needs to be enabled, the UVC device needs to be configured, the USB HAL needs to be updated, and a new system app needs to be preloaded.&nbsp;</p><h3><strong>Kernel config</strong></h3><p>The Linux kernel config (<a href="https://cateee.net/lkddb/web-lkddb/USB_CONFIGFS_F_UVC.html">CONFIG_USB_CONFIGFS_F_UVC</a>) is necessary so that the Android device can be mounted as a UVC gadget. Fortunately, many devices upgrading to and nearly all devices launching with Android 14 will have a kernel with this config enabled. This is because it is enabled by default on Generic Kernel Image (GKI) versions <a href="https://cs.android.com/android/_/android/kernel/common/+/8d5dd0a5a458f951f0fdc25aba0cb8329b121d51">starting from android12-5.10 and later</a>, and devices launching with Android 12 or later on top of Linux kernel version 5.10 or higher <a href="https://source.android.com/docs/core/architecture/kernel/generic-kernel-image#gki2">are required</a> to ship the GKI kernel.</p><p>Because major kernel version upgrades are rare in the Android space and since the <a href="https://blog.esper.io/android-dessert-bites-11-grf-323579/">Google Requirements Freeze (GRF) program</a> allows for older vendor implementations to still be certifiable, some devices upgrading to Android 14 won’t have kernels that support the USB webcam function. It’s hard to put together a list of such devices, so the best way to tell if your device is capable is to check its kernel version and whether the config is enabled.</p><p>To check your device’s kernel version, run:</p><p>‍</p><p>	adb shell “cat /proc/version”</p><p>‍</p><p>However, to actually verify that the kernel config is enabled, it’s necessary to run another command to see if “CONFIG_USB_CONFIGFS_F_UVC” appears in config.gz, a compressed copy of the configuration file used to build the kernel on the device.</p><p>‍</p><p>	adb shell "zcat /proc/config.gz | grep 'CONFIG_USB_CONFIGFS_F_UVC'"</p><p>‍</p><p>For example, here is the output from a Galaxy Z Fold 5 running Android 13. Since it is using the android13-5.15 GKI and since “CONFIG_USB_CONFIGFS_F_UVC=y”, it should be capable of supporting the USB webcam feature once it’s upgraded to Android 14. That’s assuming, though, that the device meets the other prerequisites I mentioned before.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f75477809e9cc6e3d8c7f2_Galaxy_Z_Fold_5_kernel_and_UVC_config.png" loading="lazy" alt=""></p></figure><h3><strong>Webcam Service app</strong></h3><p>Within the Android 14 QPR1 beta for select Pixel devices is a new system app called “Webcam Service” (com.android.deviceaswebcam). This app relies on a shared library named libjni_deviceAsWebcam.so. This app and library are set to be included as part of Android 14’s upcoming source code release.</p><p>The Webcam Service app implements the “DeviceAsWebcam” service that <a href="https://twitter.com/MishaalRahman/status/1621194700790054914">I previously reported</a> would “[turn] an Android device into a webcam.” The service forwards camera frames to a /dev/video node that host devices can read from. <a href="https://android-review.googlesource.com/c/platform/system/sepolicy/+/2410788">SELinux policy</a> dictates that only processes in the device_as_webcam domain, ie. only the Webcam Service system app, can access the /dev/video nodes. Thus, only the device maker and not any third-party can actually take advantage of Android 14’s native USB webcam support.</p><p>How does the Webcam Service app actually know when to start forwarding camera frames? When the new “USB webcam” option in “USB Preferences” is toggled, the system broadcasts the android.hardware.usb.action.USB_STATE intent with the “connected” and “uvc” intent extras. Webcam Service has a receiver for this intent, which starts the system service if the “uvc” intent extra is set to “true” and the framework method android.hardware.usb#isUvcSupportEnabled() returns true.</p><p>The method isUvcSupportEnabled() returns true when the system property “ro.usb.uvc.enabled” is set to true. This property must be set by the OEM at build time, and if it is not set, then “USB Preferences” won’t show the “USB webcam” option and Webcam Service won’t start. This property is <a href="https://android-review.googlesource.com/c/platform/system/sepolicy/+/2415830">only readable by</a> system apps like Settings and the Webcam Service app.</p><p>When the Webcam Service starts, a new notification is posted that lets the user configure the webcam. Tapping the notification opens a camera preview where the user can zoom in or out or change lenses. Under the hood, the Webcam Service app starts a foreground service to ensure it is kept alive by the system. It uses the Camera2 API and supports streaming at either 720p (1280x720) or 1080p (1920x1080p) resolutions. The webcam device appears on the connected host as “Android Webcam”.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/650b5a0741e33b9aefbf3f8b_Android_14_webcam_service_3.png" loading="lazy" alt=""></p></figure><h3><strong>ConfigFS and USB HAL</strong></h3><p>The exact encoding method, video parameters, and name displayed to the host depends on how the device maker sets up the UVC gadget using configfs. For example, on the Tensor-powered Pixel devices, Google’s UVC gadget function configuration can be found in the init.gs[101|201].usb.rc file located in /vendor/etc/init/hw.</p><figure><p><img src="https://assets-global.website-files.com/602d2f7be70ffc3452f5a079/64f7549e858c09450c069ea4_init.gs201.usb.rc.jpeg" loading="lazy" alt=""></p></figure><p>In addition, the device needs an updated USB HAL so that Android is able to switch USB modes to UVC when the option is selected in Settings. Due to the aforementioned GRF program, however, it’s likely that many devices upgrading to Android 14 won’t receive an updated USB HAL, meaning this feature won’t work.</p><h2>Conclusion</h2><p>It’s good to see Google implement native USB webcam functionality into Android. Assuming that the Webcam Service app will be available in AOSP as we expect, that means this feature can be picked up by any device maker that wishes to implement webcam functionality. Many will call this feature a clone of Apple’s “<a href="https://support.apple.com/en-us/HT213244">Continuity Camera</a>”, but it’s worth noting that Android’s version works with multiple platforms. Any phone running Android 14 that meets the requirements mentioned in this article can be turned into a standard USB webcam that works with any PC, and that’s a big deal.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LabelContactRelationYoungerCousinMothersSiblingsDaughterOrFathersSistersDaughter (295 pts)]]></title>
            <link>https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter</link>
            <guid>37603331</guid>
            <pubDate>Thu, 21 Sep 2023 19:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter">https://developer.apple.com/documentation/contacts/cnlabelcontactrelationyoungercousinmotherssiblingsdaughterorfatherssistersdaughter</a>, See on <a href="https://news.ycombinator.com/item?id=37603331">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Speeding up the JavaScript ecosystem – Polyfills gone rogue (211 pts)]]></title>
            <link>https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/</link>
            <guid>37602923</guid>
            <pubDate>Thu, 21 Sep 2023 19:31:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/">https://marvinh.dev/blog/speeding-up-javascript-ecosystem-part-6/</a>, See on <a href="https://news.ycombinator.com/item?id=37602923">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>📖 tl;dr: Many popular npm packages depend on 6-8x more packages than they need to. Most of these are unnecessary polyfills and it's one of the key reasons node_modules folders are so large. The eslint ecosystem seems to be most affected by this.</p><div>
						<p>In the previous posts we looked at runtime performance and I thought it would be fun to look at node modules install time instead. A lot has been already written about various algorithmic optimizations or using more performant syscalls, but why do we even have this problem in the first place? Why is every <code>node_modules</code> folders so big? Where are all these dependencies coming from?</p>
						<p>It all started when a buddy of mine noticed something odd with the dependency tree of his project. Whenever he updated a dependency, it would pull in several new ones and with each subsequent update the total number of dependencies grew over time.</p>
						<pre><code> ├─┬ arraybuffer.prototype.slice <span>1.0</span>.2<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ function.prototype.name <span>1.1</span>.6<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ globalthis <span>1.0</span>.3<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ object.assign <span>4.1</span>.4<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ regexp.prototype.flags <span>1.5</span>.1<br> │ ├─┬ define-properties <span>1.2</span>.1<br> │ │ └── define-data-property <span>1.1</span>.0<br> │ └─┬ set-function-name <span>2.0</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ string.prototype.trim <span>1.2</span>.8<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> ├─┬ string.prototype.trimend <span>1.0</span>.7<br> │ └─┬ define-properties <span>1.2</span>.1<br> │   └── define-data-property <span>1.1</span>.0<br> └─┬ string.prototype.trimstart <span>1.0</span>.7<br>   └─┬ define-properties <span>1.2</span>.1<br> 	└── define-data-property <span>1.1</span>.0</code></pre>
						<p>Now to be fair, there are valid reasons why a package might depend on an additional dependencies. Here, we began to notice a pattern though: The new dependencies were all polyfills for JavaScript functions that have long been supported everywhere. The <code>Object.defineProperties</code> method for example was shipped as part of the very first public Node <code>0.10.0</code> release dating back to 2013. Heck, even Internet Explorer 9 supported that. And yet there were numerous packages in that dependend on a polyfill for it.</p>
						<p>
							Among the various packages that pulled in <code>define-properties</code> was <a href="https://github.com/jsx-eslint/eslint-plugin-react/"><code>eslint-plugin-react</code></a>. It caught my eye, because it's very popular in the React ecosystem. Why does it pull in a polyfill for <code>Object.defineProperties</code>? There is no JavaScript engine that doesn't come with it already built in.
						</p>
						<h2>Polyfills that don’t polyfill</h2>
						<p>Reading the source of the packages revealed something even more bizarre: The polyfill functions were imported and called directly, rather than patching missing functionality in the runtime environment. The whole point of a polyfill is to be transparent to the user’s code. It should check if the function or method to patch is available and only add it if it’s missing. When there is no need for the polyfill it should do nothing. The odd thing to me is that the functions were used directly like a function from a library.</p>
						<pre><code><span>// Why is the `define` function imported directly?</span><br><span>var</span> define <span>=</span> <span>require</span><span>(</span><span>"define-properties"</span><span>)</span><span>;</span><br><span>// ...</span><p><span>// and even worse, why is called directly?</span><br><span>define</span><span>(</span>polyfill<span>,</span> <span>{</span><br>	<span>getPolyfill</span><span>:</span> getPolyfill<span>,</span><br>	<span>implementation</span><span>:</span> implementation<span>,</span><br>	<span>shim</span><span>:</span> shim<span>,</span><br><span>}</span><span>)</span><span>;</span></p></code></pre>
						<p>Instead they should call <code>Object.defineProperties</code> directly. The whole point of polyfills is to patch <em>the environment</em> not be called directly. Compare this to what a polyfill for <code>Object.defineProperties</code> is supposed to look like:</p>
						<pre><code><span>// Check if the current environment already supports</span><br><span>// `Object.defineProperties`. If it does, then we do nothing.</span><br><span>if</span> <span>(</span><span>!</span>Object<span>.</span>defineProperties<span>)</span> <span>{</span><br>	<span>// Patch in Object.defineProperties here</span><br><span>}</span></code></pre>
						<p>The most common place where <code>define-properties</code> was used was ironically inside other polyfills, which loaded even more polyfills. And before you ask, the <code>define-properties</code> package relies on even more dependencies than just itself.</p>
						<pre><code><span>var</span> keys <span>=</span> <span>require</span><span>(</span><span>"object-keys"</span><span>)</span><span>;</span><br><span>// ...</span><br><span>var</span> defineDataProperty <span>=</span> <span>require</span><span>(</span><span>"define-data-property"</span><span>)</span><span>;</span><br><span>var</span> supportsDescriptors <span>=</span> <span>require</span><span>(</span><span>"has-property-descriptors"</span><span>)</span><span>(</span><span>)</span><span>;</span><p><span>var</span> <span>defineProperties</span> <span>=</span> <span>function</span> <span>(</span><span>object<span>,</span> map</span><span>)</span> <span>{</span><br>	<span>// ...</span><br><span>}</span><span>;</span></p><p>module<span>.</span>exports <span>=</span> defineProperties<span>;</span></p></code></pre>
						<p>Inside <code>eslint-plugin-react</code>, that polyfill is loaded via a polyfill for <code>Object.entries()</code> to process <a href="https://github.com/jsx-eslint/eslint-plugin-react/blob/ecadb92609998520be80d64c0bd6bc5e05934aa9/configs/all.js#L4">their configuration</a>:</p>
						<pre><code><span>const</span> fromEntries <span>=</span> <span>require</span><span>(</span><span>"object.fromentries"</span><span>)</span><span>;</span> <span>// &lt;- Why is this used directly?</span><br><span>const</span> entries <span>=</span> <span>require</span><span>(</span><span>"object.entries"</span><span>)</span><span>;</span> <span>// &lt;- Why is this used directly?</span><br><span>const</span> allRules <span>=</span> <span>require</span><span>(</span><span>"../lib/rules"</span><span>)</span><span>;</span><p><span>function</span> <span>filterRules</span><span>(</span><span>rules<span>,</span> predicate</span><span>)</span> <span>{</span><br>	<span>return</span> <span>fromEntries</span><span>(</span><span>entries</span><span>(</span>rules<span>)</span><span>.</span><span>filter</span><span>(</span><span>entry</span> <span>=&gt;</span> <span>predicate</span><span>(</span>entry<span>[</span><span>1</span><span>]</span><span>)</span><span>)</span><span>)</span><span>;</span><br><span>}</span></p><p><span>const</span> activeRules <span>=</span> <span>filterRules</span><span>(</span>allRules<span>,</span> <span>rule</span> <span>=&gt;</span> <span>!</span>rule<span>.</span>meta<span>.</span>deprecated<span>)</span><span>;</span></p></code></pre>
						<h2>Doing a little bit of housekeeping</h2>
						<p>At the time of this writing installing <code>eslint-plugin-react</code> pulls in a whopping number of 97 dependencies in total. I was curious about how much of these were polyfills and began patching them out one by one locally. After all was done, this brought down the total number of dependencies down to 15. Out of the original 97 dependencies 82 of them are not needed.</p>
						<p>Coincidentally, <code>eslint-plugin-import</code> which is equally popular in various eslint presets, shows a similar problems. Installing that fills up your <code>node_modules</code> folder with 87 packages. After another local cleanup pass I was able to cut down that number to just 17.</p>
						<h2>Filling up everyone's disk space</h2>
						<p>Now you might be wondering if you’re affected or not. I did a quick search and basically every widely popular eslint plugin or preset that you can think of is affected. For some reason this whole ordeal reminds me of the <code>is-even</code>/<code>is-odd</code> incident the industry had a while back.</p>
						<p>Having so many dependencies makes it much harder to audit the dependencies of a project. It's a waste of space too. Case in point: Deleting all eslint plugins and presets in a project alone got rid of <code>220</code> packages.</p>
						<pre><code><span>pnpm</span> <span>-r</span> <span>rm</span> eslint-plugin-react eslint-plugin-import eslint-import-resolver-typescript eslint-config-next eslint @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint-plugin-prettier prettier eslint-config-prettier eslint-plugin-react-hooks<br>Scope: all <span>8</span> workspace projects<br><span>.</span>                                        <span>|</span> <span>-220</span> ----------------------</code></pre>
						<p>Maybe we don't need that many dependencies in the first place. My mind went to this fantastic quote by the creator of the Erlang programming language:</p>
						<blockquote>
							<p>You wanted a banana but what you got was a gorilla holding the banana and the entire jungle - Joe Armstrong</p>
						</blockquote>
						<p>All I wanted was some linting rules. I didn’t want a bunch of polyfills that I don't need.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open source AI will win (163 pts)]]></title>
            <link>https://varunshenoy.substack.com/p/why-open-source-ai-will-win</link>
            <guid>37602674</guid>
            <pubDate>Thu, 21 Sep 2023 19:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://varunshenoy.substack.com/p/why-open-source-ai-will-win">https://varunshenoy.substack.com/p/why-open-source-ai-will-win</a>, See on <a href="https://news.ycombinator.com/item?id=37602674">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4>Discover more from Public Experiments</h4><p>Marginalia on science, engineering, and culture.</p> </div><div dir="auto"><blockquote><p>Linux is subversive. Who would have thought even five years ago (1991) that a world-class operating system could coalesce as if by magic out of part-time hacking by several thousand developers scattered all over the planet, connected only by the tenuous strands of the Internet? </p><p>Certainly not I.</p><p><em>opening remarks in The Cathedral and the Bazaar by Eric Raymond.</em></p></blockquote><p>There’s a popular floating theory on the Internet that a combination of the existing foundation model companies will be the end game for AI. </p><p>In the near future, every company will rent a “brain” from a model provider, such as OpenAI/Anthropic, and build applications that build on top of its cognitive capabilities.</p><p>In other words, AI is shaping up to be an oligopoly of sorts, with only a small set of serious large language model (LLM) providers.</p><p>I don’t think this could be farther from the truth. I truly believe that open source will have more of an impact on the future of LLMs and image models than the broad public believes.</p><p>There are a few arguments against open source that I see time and time again.</p><ol><li><p><strong>Open source AI cannot compete with the resources at industry labs.</strong><span> Building foundation models is expensive, and non-AI companies looking to build AI features will outsource their intelligence layer to a company that specializes in it. Your average company cannot scale LLMs or produce novel results the same way a well capitalized team of talented researchers can. On the image generation side, Midjourney is miles ahead of anything else.</span></p></li><li><p><strong>Open source AI is not safe.</strong><span> Mad scientists cooking up intelligence on their </span><a href="https://nonint.com/2022/05/30/my-deep-learning-rig/" rel="">cinderblock-encased GPUs</a><span> will not align their models with general human interests</span></p><span>. </span></li><li><p><strong>Open source AI is incapable of reasoning.</strong><span> Not only do open source models perform more poorly than closed models on benchmarks, but they also lack </span><a href="https://arxiv.org/abs/2206.07682" rel="">emergent capabilities</a><span>, those that would enable agentic workflows, for example.</span></p></li></ol><p>While they seem reasonable, I think these arguments hold very little water.</p><p>Outsourcing a task is fine — when the task is not business critical. </p><p><a href="https://www.baseten.co/" rel="">Infrastructure products</a><span> save users from wasting money and energy on learning Kubernetes or hiring a team of DevOps engineers. No company should have to hand-roll their own HR/bill payments software. There are categories of products that enable companies to “focus on what makes their beer taste better”</span></p><p><span>. </span></p><p>LLMs, for the most part, do not belong in this category. There are some incumbents building AI features on existing products, where querying OpenAI saves them on hiring ML engineers. For them, leveraging closed AI makes sense. </p><p>However, there’s a whole new category of AI native businesses for whom this risk is too great. Do you really want to outsource your core business, one that relies on confidential data, to OpenAI or Anthropic? Do you want to spend the next few years of your life working on a “GPT wrapper”? </p><p><strong>Obviously not.</strong></p><p>If you’re building an AI native product, your primary goal is getting off of OpenAI as soon as you possibly can. Ideally, you can bootstrap your intelligence layer using a closed source provider, build a data flywheel from engaged users, and then fine-tune your own models to perform your tasks with higher accuracy, less latency, and more control.</p><p><span>Every business needs to own their core product, and for AI native startups, their core product is a model trained on proprietary data</span></p><p><span>. Using closed source model providers for the long haul exposes an AI native company to undue risk.</span></p><p><span>There is too much pressure pent up for open source LLMs to flop. The lives of many companies are at stake. Even Google has acknowledged that </span><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither" rel="">they have no moat</a><span> in this new world of open source AI.</span></p><p>The general capabilities of LLMs open them up to an exponential distribution of use cases. The most important tasks are fairly straightforward: summarization, explain like I’m 5, create a list (or some other structure) from a blob of text, etc. </p><p>Reasoning, the type you get from scaling these models to get larger, doesn’t matter for 85% of use cases. Researchers love sharing that their 200B param model can solve challenging math problems or build a website from a napkin sketch, but I don’t think most users (or developers) have a burning need for these capabilities.</p><p><span>The truth is that open source models are </span><em>incredibly</em><span> good at the most valuable tasks, and can be fine-tuned to cover likely up to 99% of use-cases when a product has collected enough labeled data.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png" width="578" height="328.74010079193664" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:1389,&quot;resizeWidth&quot;:578,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Llama 2 performance&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Llama 2 performance" title="Llama 2 performance" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30a262cc-296a-4854-b80e-07acf37342fb_1389x790.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Fine-tuned Llama 2 models vs. GPT-4 (from </span><a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="">Anyscale</a><span>)</span></figcaption></figure></div><p>Reasoning, the holy grail that researchers are chasing, probably doesn’t matter nearly as much as people think.</p><p>More important than reasoning is context length and truthfulness. </p><p>Let’s start with context length. The longer the context length for a language model, the longer the prompts and chat logs you can pass in. </p><p>The original Llama has a context length of 2k tokens. Llama 2 has a context length of 4k. </p><p><span>Earlier this year, </span><a href="https://kaiokendev.github.io/til" rel="">an indie AI hacker</a><span> discovered that a single line code change to the RoPE embeddings for Llama 2 would give you up to 8K of context length </span><em>for free with no additional training.</em><span> </span></p><p><span>Just last week another indie research project was released, </span><a href="https://github.com/jquesnelle/yarn" rel="">YaRN</a><span>, that extends Llama 2’s context length to 128k tokens. </span></p><p>I still don’t have access to GPT-4 32k. This is the speed of open source.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png" width="466" height="410.95054945054943" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1284,&quot;width&quot;:1456,&quot;resizeWidth&quot;:466,&quot;bytes&quot;:574648,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe239225e-0f1b-4f22-8ad9-6febe723b76c_1476x1302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>While contexts have scaled up, the hardware requirements to run massive models have also scaled down. You can now run state-of-the-art </span><a href="https://twitter.com/ggerganov/status/1699791226780975439?s=20" rel="">massive</a><span> language models from your Macbook thanks to projects like </span><a href="https://github.com/ggerganov/llama.cpp" rel="">Llama.cpp</a><span>. Being able to use these models locally is a huge plus for security and costs as well. In the limit, you can run your models on your users’ hardware. Models are continuing to scale down while retaining quality. Microsoft’s Phi-1.5 is only 1.3 billion parameters but meets Llama 2 7B </span><a href="https://x.com/Teknium1/status/1701422303643615571?s=20" rel="">on several benchmarks</a><span>. Open source LLM experimentation will continue to explode as consumer hardware and </span><a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini" rel="">the GPU poor</a><span> rise to the challenge.</span></p><p><span>On truthfulness: out-of-the-box open source models are less truthful than closed source models, and I think this is actually fine. In many cases, </span><a href="https://towardsdatascience.com/llm-hallucinations-ec831dcd7786" rel="">hallucination</a><span> can be a feature, not a bug, particularly when it comes to creative tasks like storytelling. </span></p><p><span>Closed AI models have a certain filter that make them sound </span><em>artificial</em><span> and less interesting. </span><a href="https://huggingface.co/Gryphe/MythoMax-L2-13b" rel="">MythoMax-L2</a><span> tells significantly better stories than Claude 2 or ChatGPT, at only 13B parameters. When it comes to honestly, the latest open source LLMs work well with </span><a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" rel="">retrieval augmented generation</a><span>, and they will only get better.</span></p><p>Let’s take a brief look at the image generation side. </p><p>I would argue that Stable Diffusion XL (SDXL), the best open source model, is nearly on-par with Midjourney. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png" width="1456" height="507" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:507,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2871704,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F048c306f-67b4-4926-ae74-a2c5d92f392d_2472x860.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Stable Diffusion XL generations for the prompt “an astronaut playing a guitar on Mars with a llama”. These images were generated on the first try, no cherry-picking needed.</figcaption></figure></div><p><span>In exchange for the slightly worse ergonomics, Stable Diffusion users have access to hundreds of community crafted LoRAs</span></p><p><span>, fine-tunes, and textual embeddings. Users quickly discovered hands were a sore for SDXL, and within weeks a LoRA </span><a href="https://minimaxir.com/2023/08/stable-diffusion-xl-wrong/" rel="">that fixes hands appeared online</a><span>. </span></p><p><span>Other open source projects like </span><a href="https://huggingface.co/docs/diffusers/training/controlnet" rel="">ControlNet</a><span> give Stable Diffusion users significantly more power when it comes to structuring their outputs, where Midjourney falls flat.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png" width="512" height="208.21333333333334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:305,&quot;width&quot;:750,&quot;resizeWidth&quot;:512,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c5bfd5d-23de-48ba-af79-abccb0d62824_750x305.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A flowchart of how Stable Diffusion + ControlNet works. Clipped from </span><a href="https://stable-diffusion-art.com/controlnet/" rel="">here</a><span>.</span></figcaption></figure></div><p>Moreover, Midjourney doesn’t have an API, so if you want to build a product with an image diffusion feature, you would have to use Stable Diffusion in some form. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg" width="432" height="432" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:960,&quot;resizeWidth&quot;:432,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;r/StableDiffusion - Spiral Town - different approach to qr monster&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="r/StableDiffusion - Spiral Town - different approach to qr monster" title="r/StableDiffusion - Spiral Town - different approach to qr monster" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F797232a8-346a-4da3-a01c-ea4096aa5dbd_960x960.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>This image went viral on </span><a href="https://x.com/MrUgleh/status/1702041188482658758?s=20" rel="">Twitter</a><span> and </span><a href="https://www.reddit.com/r/StableDiffusion/comments/16ew9fz/spiral_town_different_approach_to_qr_monster/" rel="">Reddit</a><span> this week. It uses Stable Diffusion with ControlNet. Currently, you can’t create images like this on Midjourney.</span></figcaption></figure></div><p>There are similar controllable features and optimizations that open source LLMs enable.</p><p>An LLM’s logits, the token-wise probability mass function at each iteration, can be used to generate structured output. In other words, you can guarantee the generation of JSON without entering a potentially expensive “validate-retry” loop, which is what you would need to do if you were using OpenAI. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png" width="818" height="218" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:218,&quot;width&quot;:818,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" title="How to Get Better Outputs from Your Large Language Model | NVIDIA Technical  Blog" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5062f2f3-4d9a-4d4c-8389-94c3d86454b5_818x218.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>An example of logits from </span><a href="https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/" rel="">NVIDIA</a><span>.</span></figcaption></figure></div><p><span>Open source models are smaller and run on your own dedicated instance, leading to lower end-to-end </span><a href="https://twitter.com/abacaj/status/1699602420882378932?s=20" rel="">latencies</a><span>. You can improve throughput by batching queries and using inference servers like </span><a href="https://vllm.ai/" rel="">vLLM</a><span>. </span></p><p><span>There are many more tricks (see: </span><a href="https://arxiv.org/abs/2302.01318" rel="">speculative</a><span> </span><a href="https://twitter.com/ggerganov/status/1697262700165013689?s=20" rel="">sampling</a><span>, </span><a href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html#concurrent-model-execution" rel="">concurrent model execution</a><span>, </span><a href="https://github.com/ggerganov/llama.cpp/issues/64" rel="">KV caching</a><span>) that you can apply to improve on the axes of latency and throughput. The latency you see on the OpenAI endpoint is the best you can do with closed models, rendering it useless for many latency-sensitive products and too costly for large consumer products.</span></p><p><span>On top of all this, you can also fine-tune or train your own LoRAs on top of open source models with maximal control. Frameworks like </span><a href="https://github.com/OpenAccess-AI-Collective/axolotl#axolotl" rel="">Axolotl</a><span> and </span><a href="https://huggingface.co/docs/trl/sft_trainer" rel="">TRL</a><span> have made this process simple</span></p><p><span>. While closed source model providers also have their own fine-tuning endpoints, you wouldn’t get the same level of control or visibility than if you did it yourself. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png" width="468" height="382.7168674698795" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1086,&quot;width&quot;:1328,&quot;resizeWidth&quot;:468,&quot;bytes&quot;:384308,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf5b21ec-0292-4e2f-b6f7-99f7063f9aa6_1328x1086.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Falcon 180B, the largest open source model to date, was </span><a href="https://huggingface.co/blog/falcon-180b" rel="">released last week</a><span>. Within hours, Discords filled with mostly anonymous developers began exploring how they could recreate GPT-4 using this new model as a base layer.</span></figcaption></figure></div><p>Open source also provides guarantees on privacy and security.</p><p>You control the inflow and outflow of data in open models. The option to self-host is a necessity for many users, especially those working in regulated fields like healthcare. Many applications will also need to run on proprietary data, on both the training and inference side.</p><p><span>Security is best explained by </span><a href="https://en.wikipedia.org/wiki/Linus%27s_law" rel="">Linus’s Law</a><span>:</span></p><blockquote><p>Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone. </p><p>Or, less formally, ‘‘Given enough eyeballs, all bugs are shallow.’’</p></blockquote><p><span>Linux succeeded because it was built in the open. Users knew </span><em>exactly</em><span> what they were getting and had the opportunity to file bugs or even attempt to fix them on their own with community support. </span></p><p><span>The same is true for open source models. Even </span><a href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="">software 2.0</a><span> needs to be audited. Otherwise, things can change under the hood, leading to regressions in your application. This is unacceptable for most business use cases.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png" width="526" height="377.15934065934067" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1044,&quot;width&quot;:1456,&quot;resizeWidth&quot;:526,&quot;bytes&quot;:275898,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa92ec351-79a0-4848-899e-1c27d8182818_1754x1258.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><a href="https://arxiv.org/pdf/2307.09009.pdf" rel="">This paper</a><span> recently showed that OpenAI’s endpoints drift over time. You cannot be confident that a prompt that works flawlessly will perform the same a month from now.</span></figcaption></figure></div><p><span>Adopting an open source approach for AI technology can create a wide-reaching network of checks and balances. Scientists and developers globally can peer-review, critique, study, and understand the underlying mechanisms, leading to improved safety, reliability, interpretability, and trust. Furthermore, widespread knowledge helps advance the technology responsibly while mitigating the risk of its misuse. </span><strong>Hugging Face is the new RedHat.</strong><span> </span></p><p>You can only trust models that you own and control. The same can’t be said for black box APIs. This is also why the AI safety argument against open source makes zero sense. History suggests, open source AI is, in fact, safer. </p><p><span>Why do people currently prefer closed source? Two reasons: </span><em>ease-of-use</em><span> and </span><em>mindshare</em><span>.</span></p><p>Open source is much harder to use than closed source models. It seems like you need to hire a team of machine learning engineers to build on top of open source as opposed to using the OpenAI API. This is ok, and will be true in the short-term. This is the cost of control and the rapid pace of innovation. People who are willing to spend time at the frontier will be treated by being able to build much better products. The ergonomics will get better.</p><p><span>The more unfortunate issue is </span><strong>mindshare</strong><span>. </span></p><p>Closed source model providers have captured the collective mindshare of this AI hype cycle. People don’t have time to mess around with open source nor do they have the awareness of what open source is capable of. But they do know about OpenAI, Pinecone, and LangChain. </p><p>Using the right tool is often conflated with using the best known tool. The current hype cycle has put closed source AI in the spotlight. As open source offerings mature and become more user-friendly and customizable, they will emerge as the superior choice for many applications. </p><p>Rather than getting swept up in the hype, forward-thinking organizations will use this period to deeply understand their needs and lay the groundwork to take full advantage of open source AI. They will build defensible and differentiated AI experiences on open technology. This measured approach enables a sustainable competitive advantage in the long run. </p><p>The future remains bright for pragmatic adopters who see past the hype and keep their eyes on the true prize: truly open AI.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA’s Webb finds carbon source on surface of Jupiter’s moon Europa (389 pts)]]></title>
            <link>https://webbtelescope.org/contents/news-releases/2023/news-2023-113</link>
            <guid>37602239</guid>
            <pubDate>Thu, 21 Sep 2023 18:52:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webbtelescope.org/contents/news-releases/2023/news-2023-113">https://webbtelescope.org/contents/news-releases/2023/news-2023-113</a>, See on <a href="https://news.ycombinator.com/item?id=37602239">Hacker News</a></p>
Couldn't get https://webbtelescope.org/contents/news-releases/2023/news-2023-113: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Most UI applications are broken real-time applications (166 pts)]]></title>
            <link>https://thelig.ht/ui-apps-are-broken/</link>
            <guid>37601064</guid>
            <pubDate>Thu, 21 Sep 2023 17:34:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thelig.ht/ui-apps-are-broken/">https://thelig.ht/ui-apps-are-broken/</a>, See on <a href="https://news.ycombinator.com/item?id=37601064">Hacker News</a></p>
<div id="readability-page-1" class="page">

<p>I’ve been programming for a long time. When I say long time, I mean
decades, with an S. Hopefully that’s long enough. In that time my
experience has primarily been programming for contemporary platforms,
e.g.&nbsp;Linux, Windows, macOS on desktop-class or server-class CPU
architectures. Recently, I embarked on building a <a href="https://www.supermidipak.com/">MIDI engine for a system with
significantly less processing power</a>.</p>
<p>Soon after I started, I ran into the issue of guaranteeing that it
was impossible for the queue of input events to build up indefinitely.
This essentially boils down to making sure that each event handler
doesn’t run longer than some maximum amount. Then it hit me, I’ve heard
this before, <em>maximum amount of time</em>: I’m building a real-time
system.</p>
<p>Once I realized that I had to additionally take real-time constraints
into account while building, it drove a lot of the engineering decisions
I made in a specific direction. In particular, the worst case time of
every sequence of code must be accounted for, the average-case time was
irrelevant for correctness. Under this discipline, algorithms which had
better worst-case time but worse average-case time are preferred,
branching usually must be to the faster path, and adding fast paths to
slow algorithms was not helpful. It was interesting work and it changed
how I thought about building systems in a profound way.</p>
<p>Armed with this new awareness, I began to notice the lack of
real-time discipline in other applications, including my own. This was a
jarring experience, how could I have never noticed this before? The
juggernaut during this period was when I realized that most mainstream
desktop UI applications were <em>fundamentally broken</em>.</p>
<p>When I click a mouse button, when I press a key on the keyboard, I
expect a response in a bounded amount of time. <em>Bounded amount of
time?</em> We’ve heard this before! UI applications are also real-time
systems. How much time is this bounded amount of time? 100ms or maybe
250ms. Well, take your pick, the key point is that the response time
should not be indefinite. I should never see a <strong>beach ball of
death</strong>. <em>Never</em>.</p>
<h2 id="library-functions-are-not-real-time">Library Functions are not
Real-time</h2>
<p>One of the fundamental problems is that many UI applications on
Windows, Linux, and macOS call functions that are not specified to run
in a bounded amount of time. Here’s a basic example: many applications
don’t think twice about doing file IO in a UI event handler. That
results in a tolerable amount of latency most of the time on standard
disk drives but what if the file is stored on a network drive? It could
take much longer than a second to service the file request. This will
result in a temporarily hung application with the user not knowing what
is happening. The network drive is operating correctly, the UI
application isn’t.</p>
<p>So all we have to do is avoid file system IO functions from the main
thread? Not a big deal. That doesn’t mean UI applications are
fundamentally broken. That’s just one broken application and it’s still
relatively easily fixable.</p>
<p>It’s not just file system IO functions. File system IO functions
belong to a class of functions called blocking functions. These are
functions that are specified not to return until some external event
happens. So correct UI applications cannot call any blocking function
from their main threads.</p>
<p>It gets worse. Literally none of the standard library functions on
contemporary systems are guaranteed to return in any amount of time. If
you want to write a correct UI application, you technically cannot call
any of them. I’m talking <code>malloc()</code>. Each call risks taking
an amount of time longer than the maximum time allotted to respond to
the event.</p>
<p>You may think I am being excessively pedantic with the previous
point. Maybe you think, “No sane implementation of any standard library
function will take more than a 500us on good data. It’s good enough to
avoid blocking functions on the main thread.” I have two words for you:
virtual memory.</p>
<h2 id="virtual-memory">Virtual Memory</h2>
<p>When it comes to Windows, Linux, and macOS, these operating systems
are virtual memory systems. When applications allocate memory, they are
not actually allocating physical memory. They are telling the operating
system that they will be using a certain memory region for a certain
purpose. This enables lots of functionality but in particular this
allows operating systems to save physical memory by transparently
storing memory pages onto a hard disk and restoring the page when the
application accesses the page again. This means that <em>a memory access
can block on a hard disk access</em>.</p>
<p>This is a transparent process that is not under control of the
application. Thus, if any given memory access can block on IO from a
disk drive, that means the system is fundamentally not real-time,
therefore UI applications on such a system are fundamentally broken.</p>
<p>This doesn’t seem like a common problem but whole system “out of
memory” conditions are not that uncommon. When the system is in this
state, it starts rapidly paging memory onto the hard disk. UI
applications will be affected and this will cause your system to hang
without warning and with no way to intervene since keypresses cannot be
processed. From a user standpoint, this is worse than a kernel panic.
This type of failure has happened to me multiple times on Linux so I
know it’s a problem there. Perhaps Windows and macOS engineers have
already considered this issue but I doubt it.</p>
<p>Is there a way to fix this? At least on Linux there is the
<code>mlock()</code> family of functions that tell the operating system
to put and keep the process’s memory pages into RAM. There are likely
similar functions available on Windows and macOS. Of course there are
still complications, e.g.&nbsp;is the application or the operating system
responsible for locking memory pages? how does the application know
which pages to lock? how does the operating system know which pages to
lock?</p>
<h2 id="real-time-scheduling">Real-time Scheduling</h2>
<p>The final fundamental issue with implementing real-time UI on top of
contemporary mainstream platforms is the lack of real-time scheduling
for the active UI application. These systems are time-sharing systems,
meaning that a process’s execution can be indefinitely paused if there
are many other processes competing for use of the CPU.</p>
<p>Imagine you have multiple background process running at 100% CPU,
then a UI event comes in to the active UI application. The operating
system may block for 100ms * N before allowing the UI application to
process the event, where N is the number of competing background
processes, potentially causing a delayed response to the user that
violates the real-time constraint (NB: 10Hz is a common timeslice for
time-sharing systems).</p>
<p>There is a solution for this as well, the window manager or
equivalent can tell the OS to give scheduling priority to whatever UI
application has active focus. This means that while the UI application
is active and needs CPU, background processes are starved. There are
complications with adapting a solution like this to existing systems as
well, e.g.&nbsp;What to do when the active UI application runs into an
infinite loop? What about multi-process UI applications?</p>
<h2 id="conclusion">Conclusion</h2>
<p>Hopefully I’ve at least convinced you that mainstream UI applications
are built on poor foundations. Do these UI applications work? Sure, most
of the time but when they fail due to bad real-time assumptions, they
fail in an annoying way. <strong>Beach ball of death</strong>. It’s
unacceptible for workstation-class interactive systems to ever fail this
way. I want to use responsive, correct applications. In the future, the
UIs on which people will depend will take real-time constraints into
account across the entire stack.</p>
<p>As far as I can tell, fixing this in a meaningful way will require
large ecosystem-level changes and broad awareness. Lots of wide-reaching
architectural decisions that ignore these issues have accumulated over
decades. I’m tempted to abandon using Windows, macOS and Linux as the
main platforms with which I interact.</p>
<p>Send any comments to <a href="https://twitter.com/cejetvole"><span data-cites="cejetvole">@cejetvole</span></a></p>
<p>Rian Hunter<br> 2023-09-21</p>
<p><em>Edit: The “Real-time Scheduling” section was added shortly after
initial publication.</em></p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The urgent need for memory safety in software products (144 pts)]]></title>
            <link>https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products</link>
            <guid>37600937</guid>
            <pubDate>Thu, 21 Sep 2023 17:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products">https://www.cisa.gov/news-events/news/urgent-need-memory-safety-software-products</a>, See on <a href="https://news.ycombinator.com/item?id=37600937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For over <a href="https://apps.dtic.mil/sti/citations/AD0758206" title="DTIC Bibliography">half a century</a>, software engineers have known malicious actors could exploit a class of software defect called “memory safety vulnerabilities” to compromise applications and systems. During that time, experts have repeatedly warned of the problems associated with memory safety vulnerabilities. Memory unsafe code even led to a major internet outage in 1988. Just how big a problem is memory unsafety? In a blog post, <a href="https://msrc-blog.microsoft.com/2019/07/16/a-proactive-approach-to-more-secure-code/" title="Microsoft Blog: A Proactive Approach to More Secure Code">Microsoft reported</a> that “~70% of the vulnerabilities Microsoft assigns a CVE [Common Vulnerability and Exposure] each year continue to be memory safety issues.” <a href="https://www.chromium.org/Home/chromium-security/memory-safety/" title="Chromium Security: Memory Safety">Google likewise reported</a> that “the Chromium project finds that around 70% of our serious security bugs are memory safety problems.” <a href="https://hacks.mozilla.org/2019/02/rewriting-a-browser-component-in-rust/" title="Mozilla: Rewriting a Browser Component in Rust">Mozilla reports</a> that in an analysis of security vulnerabilities, that “of the 34 critical/high bugs, 32 were memory-related.”</p><p>These vulnerabilities are not theoretical. Attackers use them in the commission of attacks against real people. For example, Google’s Project Zero team analyzed vulnerabilities that were used in the wild by attackers before they were reported to software providers (also called “zero-day vulnerabilities”). <a href="https://googleprojectzero.blogspot.com/2022/04/the-more-you-know-more-you-know-you.html" title="Google: The More You Know, The More You Know">They report</a> that “out of the 58 [such vulnerabilities] for the year, 39, or 67% were memory corruption vulnerabilities.” <a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/" title="Citizen Lab Blog">Citizen Lab uncovered</a> spyware used against civil society organizations that exploited memory safety vulnerabilities.</p><p>In what other industry would the market tolerate such well-understood and severe dangers for users of products for decades?</p><p>Over the years, software engineers have invented numerous clever, but ultimately insufficient mitigations for this class of vulnerability, including tools like memory randomization and sandboxing techniques that reduce impact, and tools for static and dynamic code analysis that reduce occurrence. In addition to those tools, organizations have spent significant time and money training their developers to avoid unsafe memory operations. There are also several parallel efforts to improve the memory safety of existing C/C++ code. Despite these efforts (and associated costs in complexity, time, and money), memory unsafety has been the most common type of software security defect for decades.</p><p>There are, however, a few areas that every software company should investigate. First, there are some promising memory safety mitigations in hardware. The Capability Hardware Enhanced RISC Instructions (<a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/" title="CHERI Research Paper">CHERI</a>) research project uses modified processors to give memory unsafe languages like C and C++ protection against many widely exploited vulnerabilities. Another hardware assisted technology comes in the form of memory tagging extensions (MTE) that are available in some systems. While some of these hardware-based mitigations are still making the journey from research to shipping products, many observers believe they will become important parts of an overall strategy to eliminate memory safety vulnerabilities.</p><p>Second, companies should investigate memory safe programming languages. Most modern programming languages other than C/C++ are already memory safe. Memory safe programming languages manage the computer’s memory so the programmer cannot introduce memory safety vulnerabilities. Compared to other available mitigations that require constant upkeep – either in the form of developing new defenses, sifting through vulnerability scans, or human labor – no work has to be done once code is written in a memory safe programming language to keep it memory safe.</p><p>What has been lacking until a few years ago is a language with the speed of C/C++ with built-in memory safety assurances. In 2006, a software engineer at Mozilla began working on a new programming language called Rust. Rust version 1.0 was officially announced in 2015. Since then, several prominent software organizations have started to use it in their systems, including Amazon, Facebook, Google, Microsoft, Mozilla, and many others. It is also supported in the development of the Linux kernel.</p><p>Different products will require different investment strategies to mitigate memory unsafe code. The balance between C/C++ mitigations, hardware mitigations, and memory safe programming languages may even differ between products from the same company. No one approach will solve all problems for all products. The one thing software manufacturers cannot do, however, is ignore the problem. The software industry must not kick the can down the road another decade through inaction.</p><p>CISA’s <a href="https://www.cisa.gov/securebydesign" title="CISA | Secure By Design">secure by design white paper</a> outlines three core principles for software manufacturers: take ownership of customer security outcomes, embrace radical transparency, and lead security transformations from the top of the organization. Solutions to the memory unsafety problem will incorporate all three principles.</p><p>CISA urges software manufacturers to make it a top-level company goal to reduce and eventually eliminate memory safety vulnerabilities from their product lines. To demonstrate such a commitment, companies can publish a “memory safety roadmap” that includes information about how they are modifying their software development lifecycle (SDLC) to accomplish this goal. A roadmap might include details like the date after which it will build new products or components in a memory safe programming language and plans to support the memory safety initiatives of open source libraries that are part of their supply chain.</p><p>Memory unsafety has plagued the software industry for decades and will continue to be a major source of vulnerabilities and real-world harm until top business leaders from the software manufacturers make appropriate investments and take ownership of the security outcomes of their customers. As we recognize National Coding Week, we look forward to participants across the software industry working together to make software that is safer by design, and memory safety is the key to achieving that goal.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The WebP 0day (317 pts)]]></title>
            <link>https://blog.isosceles.com/the-webp-0day/</link>
            <guid>37600852</guid>
            <pubDate>Thu, 21 Sep 2023 17:21:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.isosceles.com/the-webp-0day/">https://blog.isosceles.com/the-webp-0day/</a>, See on <a href="https://news.ycombinator.com/item?id=37600852">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <p>Early last week, Google released a new <a href="https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html?ref=blog.isosceles.com">stable update</a> for Chrome. The update included a single security fix that was reported by Apple's Security Engineering and Architecture (SEAR) team. The issue, CVE-2023-4863, was a heap buffer overflow in the WebP image library, and it had a familiar warning attached: </p><p>"Google is aware that an exploit for CVE-2023-4863 exists in the wild."</p><p>This means that someone, somewhere, had been caught using an exploit for this vulnerability. But who discovered the vulnerability and how was it being used? How does the vulnerability work? Why wasn't it discovered earlier? And what sort of impact does an exploit like this have?</p><p>There are still a lot of details that are missing, but this post attempts to explain what we know about the unusual circumstances of this bug, and provides a new technical analysis and proof-of-concept trigger for CVE-2023-4863 ("the WebP 0day").</p><p>This work was made possible by major technical contributions from <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a> -- thank you!</p><h2 id="unraveling-the-timeline">Unraveling the Timeline</h2><p>Immediately after the Chrome security update was released, experts began to <a href="https://twitter.com/msuiche/status/1701714151091949812?ref=blog.isosceles.com">speculate</a> that there was a link between CVE-2023-4863 and an earlier CVE from Apple, CVE-2023-41064. The theory goes something like this.</p><p>Early in September (exact date unknown), Citizen Lab detected suspicious behavior on the iPhone of "an individual employed by a Washington DC-based civil society organization":</p><p><a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/?ref=blog.isosceles.com">BLASTPASS: NSO Group iPhone Zero-Click, Zero-Day Exploit Captured in the Wild</a></p><p>They attributed the behavior to a "zero-click" exploit for iMessage being used to deploy NSO group's Pegasus spyware, and sent their technical findings to Apple. Apple responded swiftly, and on September 7 they released a <a href="https://support.apple.com/en-us/HT213905?ref=blog.isosceles.com">security bulletin</a> that featured two new CVEs from the attack Citizen Lab identified. On each CVE they note: "<em>Apple is aware of a report that this issue may have been actively exploited.</em>"</p><p>Citizen Lab called this attack "BLASTPASS", since the attackers found a clever way to bypass the "<a href="https://googleprojectzero.blogspot.com/2021/01/a-look-at-imessage-in-ios-14.html?ref=blog.isosceles.com">BlastDoor</a>" iMessage sandbox. We don't have the full technical details, but it looks like by bundling an image exploit in a <a href="https://developer.apple.com/documentation/passkit/?ref=blog.isosceles.com">PassKit</a> attachment, the malicious image would be processed in a different, unsandboxed process. This corresponds to the first CVE that Apple released, CVE-2023-41061.</p><p>But you'd still need an image exploit to take advantage of this situation, and indeed, the second CVE that Apple released is CVE-2023-41064, a buffer overflow vulnerability in ImageIO. ImageIO is Apple's image parsing framework. It will take a sequence of bytes and attempt to match the bytes to a suitable image decoder. Several different formats are supported, and ImageIO has been an <a href="https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html?ref=blog.isosceles.com">active</a> <a href="https://support.apple.com/en-ng/HT213670?ref=blog.isosceles.com">area</a> of security research. We don't have any technical details about CVE-2023-41064 yet, so we don't know which image format it affects. </p><p>But we do know that ImageIO recently began to support WebP files, and we know that on September 6 (one day before the iOS/macOS security bulletin), Apple's security team reported a WebP vulnerability to Chrome that was urgently patched (just 5 days after the initial report) and marked by Google as "exploited in the wild". Based on this, it seems likely that the BLASTPASS vulnerability and CVE-2023-4863 ("the WebP 0day") are the same bug.</p><h2 id="the-webp-0daytechnical-analysis">The WebP 0day -- Technical Analysis</h2><p>By cross-referencing the bug ID from Chrome's security bulletin with recent open source commits to the libwebp library code, it's possible to find the following patch:</p><p><a href="https://chromium.googlesource.com/webm/libwebp/+/902bc9190331343b2017211debcec8d2ab87e17a?ref=blog.isosceles.com">Fix OOB write in BuildHuffmanTable</a></p><p>This patch was created on September 7 (one day after Apple's report), and corresponds to CVE-2023-4863. Based on an initial review of the patch, we learn the following:</p><ul><li>The vulnerability is in the "lossless compression" support for WebP, sometimes known as VP8L. A lossless image format can store and restore pixels with 100% accuracy, meaning that the image will be displayed with perfect accuracy. To achieve this, WebP uses an algorithm called <a href="https://en.wikipedia.org/wiki/Huffman_coding?ref=blog.isosceles.com">Huffman coding</a>.<br></li><li>Although Huffman coding is conceptually based on a tree data structure, modern implementations have been optimized to use tables instead. The patch suggests that it was possible to overflow the Huffman table when decoding an untrusted image.<br></li><li>Specifically, the vulnerable versions use memory allocations based on pre-calculated buffer sizes from a fixed table, and will then construct the Huffman tables directly into that allocation. The new version does a "first pass" construction that calculates the total size that the output table will require, but doesn't actually write the table to the buffer. If the total size is bigger than the pre-calculated buffer size, then a larger allocation is made.</li></ul><p>This is a great start, but it's non-constructive. We want to be able to construct an example file that can actually trigger the overflow, and to do that we have to understand how this code is actually working and why the pre-calculated buffer sizes weren't sufficient.</p><p>Stepping back, what is the vulnerable code actually doing? When a WebP image is compressed in a lossless way, a frequency analysis of the input pixels is performed. The basic idea is that input values that occur more frequently can be assigned to a shorter sequence of output bits, and values that occur less frequently can be assigned to longer sequences of output bits. The real trick is that the output bits are cleverly chosen so that the decoder can always work out the length of that particular sequence -- i.e. it's always possible to disambiguate between a 2-bit code and a 3-bit code, and so on, and so the decoder always knows how many bits to consume.</p><p>To achieve this, the compressed image has to include all of the statistical information about frequencies and code assignments, so that the decoder can reproduce the same mapping between codes and values. As mentioned, internally webp uses a table for this (they call it the "huffman_table")... but the tables themselves can be quite large, and including them alongside the compressed image would make the file size increase. The solution is to use Huffman coding to compress the tables as well. It's turtles all the way down.</p><p>This means that there's a non-trivial amount of mental gymnastics involved in analyzing/triggering the bug. Based on a review of the patch, we can isolate the memory allocation that is the most likely candidate for being overflowed and come up with a plan. </p><p>We're trying to overflow the huffman_tables allocation in ReadHuffmanCodes (src/dec/vp8l_dec.c), and the idea is to use the VP8LBuildHuffmanTable/BuildHuffmanTable call in ReadHuffmanCode (not the one in ReadHuffmanCodeLengths) to shift the huffman_table pointer past the pre-calculated buffer size. To add to the complexity, there's actually 5 different segments of the Huffman table, each with a different alphabet size (e.g. number of possible output symbols for that particular segment of the table) -- and we'll probably have to craft all 5 of those to get close enough to the end of the buffer to cause an overflow.</p><p>At this point I had come up with a basic theory of how to proceed and started manually crafting a file that could reach this deep into the code, and around this time I started chatting with <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>. It turns out that they had also been attempting to reproduce this issue, and they had built harness code to create a well-formed WebP with arbitrary Huffman coding data ("code lengths"). I tried it out and it worked perfectly, we could pass arbitrary code_lengths array into the BuildHuffmanTable call that we were targeting. Brilliant.</p><p>Now the challenge was to find a group of code_lengths that would make BuildHuffmanTable exceed the pre-calculated buffer size. I started with some manual experimentation -- changing the code_lengths array to affect the internal histogram (essentially the count array in BuildHuffmanTable), and then watching what affect each of the 16 histogram entries had on total_size, the key variable that we needed to increase to a larger than expected value. </p><p>It quickly became clear that there was a complex interaction between the histogram's starting state, the tree statistics (num_open and num_nodes), and the "key" variable that tracks the starting location of the "ReplicateValue" operation that wrote entries into the output table that we're trying to overflow. It reminded me of watching the internal state of a cryptographic hash function, and without knowing a lot more about Huffman trees and WebP's specific implementation choices, I didn't feel confident that I'd be able to manually craft an input that would even be considered correct by BuildHuffmanTable, let alone one that makes BuildHuffmanTable return an unexpectedly large value.</p><p>My next idea was to brute-force a solution. I had noticed that the first 9 entries in the histogram (e.g. count[0] .. count[8], which are called the "root table") wouldn't have much influence on the total_size, but could influence the internal state for subsequent computations (such as by pushing the number of nodes too high). The final entries in the histogram (e.g. count[9] .. count[15], which are called the "second level tables") had a direct effect on the final total_size value. With this in mind I created a few different statistical distributions that generally kept the values of the root table low (typically summing to less than 8) and the second level table higher. This approach managed to find correct inputs, and some of them resulted in output tables that were quite large, but still less than the pre-calculated buffer sizes.</p><p>I decided I needed to understand how the pre-calculated sizes were derived. There are actually several different pre-calculated size buckets depending on the number of color cache bits that are specified. The buckets are defined in kTableSize, which includes a helpful description of the values and an invaluable tip: <em>"All values computed for 8-bit first level lookup with Mark Adler's tool: &nbsp; </em><a href="https://github.com/madler/zlib/blob/v1.2.5/examples/enough.c?ref=blog.isosceles.com"><em>https://github.com/madler/zlib/blob/v1.2.5/examples/enough.c</em></a><em>"</em></p><p>The "enough" tool emits the histogram for the largest possible Huffman tree lookup table for any given alphabet size, root table size, and maximum code length. Using Mark Adler's tool, I could replicate the pre-calculated buffer sizes, and using <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>'s tool I could verify that the specific code_lengths emitted by "enough" would 100% fill up the huffman_tables allocation. That's great, but the whole idea of a heap overflow would be to fill up the allocation to 101%...</p><p>I followed a dead-end here, which is that the "enough" tool only works for color_cache sizes up to 8-bits. How did they derive the values for 9-bit, 10-bit, or 11-bit caches, all of which are considered valid? Maybe they just guessed and these values are wrong? I think Google must have modified "enough" to work on larger alphabet sizes, because I managed to replicate their numbers by making some minor changes to "enough" (things like using the 128-bit integer scalar type compiler extension to be able to count the number of trees without overflow).</p><p>At this point there was a long process of angst. The "enough" tool is clear in its documentation that it calculates the maximum value for <em>valid and complete</em> codes. There must be some configuration of this input histogram that produces a tree that WebP considers to be valid and complete, but is actually incomplete/invalid in a way that produces a larger expansion than anticipated. The patch even hints in this direction, saying: <em>"make sure that valid (but unoptimized because of unbalanced codes) streams are still decodable"</em></p><p>In the end I managed to convince myself that this wasn't possible by enumerating all of the possible valid trees in the smallest of the tables (a symbol size of 40), which also happened to be the last of the 5 tables we needed to fill. The purported maximum size for a symbol size of 40 with a root table of 8-bits and a maximum code length of 15 is 410. If you can generate anything bigger than 410, then you win. But none of the codes that BuildHuffmanTable would consider valid had a size bigger than 410 (and most of them were much smaller). It seems like the consistency check at the end of BuildHuffmanTable, e.g. checking that the number of output nodes is an expected value, was ensuring that the codes it accepted were in line with "enough" and the pre-calculated buffer sizes it gave.</p><p>But the BuildHuffmanTable function is writing values to the output table using the "ReplicateValue" operation mentioned earlier. What if we built 4 valid Huffman trees that resulted in 4 maximally sized output tables, and then supplied an invalid Huffman tree for the last table? Could we get ReplicateValue to write out-of-bounds from an invalid starting key prior to the final consistency check on the node count? The answer is: yes, we can.</p><p>Here's how to replicate the bug:<br></p><pre><code>  # checkout webp
$ git clone https://chromium.googlesource.com/webm/libwebp/ webp_test
$ cd webp_test/
  # checkout vulnerable version
$ git checkout 7ba44f80f3b94fc0138db159afea770ef06532a0
  # enable AddressSanitizer
$ sed -i 's/^EXTRA_FLAGS=.*/&amp; -fsanitize=address/' makefile.unix
  # build webp
$ make -f makefile.unix
$ cd examples/
  # fetch mistymntncop's proof-of-concept code
$ wget https://raw.githubusercontent.com/mistymntncop/CVE-2023-4863/main/craft.c
  # build and run proof-of-concept
$ gcc -o craft craft.c
$ ./craft bad.webp
  # test trigger file
$ ./dwebp bad.webp -o test.png
=================================================================
==207551==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x626000002f28 at pc 0x56196a11635a bp 0x7ffd3e5cce90 sp 0x7ffd3e5cce80
WRITE of size 1 at 0x626000002f28 thread T0
#0 0x56196a116359 in BuildHuffmanTable (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb6359)
#1 0x56196a1166e7 in VP8LBuildHuffmanTable (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb66e7)
#2 0x56196a0956ff in ReadHuffmanCode (/home/isosceles/source/webp/webp_test/examples/dwebp+0x356ff)
#3 0x56196a09a2b5 in DecodeImageStream (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3a2b5)
#4 0x56196a09e216 in VP8LDecodeHeader (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3e216)
#5 0x56196a0a011b in DecodeInto (/home/isosceles/source/webp/webp_test/examples/dwebp+0x4011b)
#6 0x56196a0a2f06 in WebPDecode (/home/isosceles/source/webp/webp_test/examples/dwebp+0x42f06)
#7 0x56196a06c026 in main (/home/isosceles/source/webp/webp_test/examples/dwebp+0xc026)
#8 0x7f7ea8a8c082 in __libc_start_main ../csu/libc-start.c:308
#9 0x56196a06e09d in _start (/home/isosceles/source/webp/webp_test/examples/dwebp+0xe09d)
0x626000002f28 is located 0 bytes to the right of 11816-byte region [0x626000000100,0x626000002f28)
allocated by thread T0 here:
#0 0x7f7ea8f2d808 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cc:144
#1 0x56196a09a0eb in DecodeImageStream (/home/isosceles/source/webp/webp_test/examples/dwebp+0x3a0eb)
SUMMARY: AddressSanitizer: heap-buffer-overflow (/home/isosceles/source/webp/webp_test/examples/dwebp+0xb6359) in BuildHuffmanTable
...</code></pre><p>In practice there are many such inputs that will overflow huffman_tables. I've found code lengths that result in writes as far as 400 bytes past the end of the huffman_tables allocation. Even with only partial control of the value being written, it definitely looks exploitable. To exploit this issue you would likely need to use the color cache bits (or num_htree_groups) to get a huffman_tables allocation that is roughly page aligned, but that shouldn't be a problem. It may be that there are other ways of causing an OOB write on the huffman_tables allocation, but this method looks like an acceptable approach. </p><p>The invalid input itself is quite unusual -- mistymntncop provided the following visualization of the Huffman tree it creates using a <a href="https://github.com/mistymntncop/CVE-2023-4863/blob/main/print_tree.c?ref=blog.isosceles.com">tool</a> they wrote to assist in this analysis:</p><figure><img src="https://blog.isosceles.com/content/images/2023/09/webp_a.jpg" alt="" loading="lazy" width="2000" height="625" srcset="https://blog.isosceles.com/content/images/size/w600/2023/09/webp_a.jpg 600w, https://blog.isosceles.com/content/images/size/w1000/2023/09/webp_a.jpg 1000w, https://blog.isosceles.com/content/images/size/w1600/2023/09/webp_a.jpg 1600w, https://blog.isosceles.com/content/images/2023/09/webp_a.jpg 2048w" sizes="(min-width: 720px) 720px"></figure><p>If you zoom in, you can see that the tree is partially unbalanced, and that a section of the unbalanced branch has a large number of internal nodes with no children in them at all. This structure results in a "key" index that a valid tree would never be able to reach. Here's what a valid tree looks like:</p><figure><img src="https://blog.isosceles.com/content/images/2023/09/webp_b.jpg" alt="" loading="lazy" width="2000" height="1518" srcset="https://blog.isosceles.com/content/images/size/w600/2023/09/webp_b.jpg 600w, https://blog.isosceles.com/content/images/size/w1000/2023/09/webp_b.jpg 1000w, https://blog.isosceles.com/content/images/size/w1600/2023/09/webp_b.jpg 1600w, https://blog.isosceles.com/content/images/2023/09/webp_b.jpg 2048w" sizes="(min-width: 720px) 720px"></figure><p>As for the patch, it seems to work almost by accident. As mentioned earlier, the patched version does a first pass with BuildHuffmanTable to calculate the total size required. In practice, this issue is patched because BuildHuffmanTable will fail (return 0) for all of the invalid inputs that would otherwise have resulted in an out-of-bounds write, and since the first pass is explicitly not writing to the table, it doesn't matter that the invalid tree is partially processed. In other words, I thought the patch was dynamically increasing the size of the buffer as needed to prevent heap overflow, but it's actually just denying the inputs that would cause a heap overflow instead. It's definitely hard to reason about, but I searched for "valid and complete" codes that would still trigger this overflow, and I couldn't find any. So it looks like the patch should be sufficient.</p><h2 id="early-discovery">Early Discovery?</h2><p>Immediately after Chrome's security update, there was some discussion about fuzzing. A binary file format implemented by a C code library is an ideal target for fuzzing -- so why hadn't this bug been found earlier? Had the library not been fuzzed enough? Or had it not been fuzzed right?</p><p>Google's OSS-Fuzz project has fuzzed hundreds of open source libraries for many years now, including libwebp and many other image decoding libraries. It's possible to look in <a href="https://storage.googleapis.com/oss-fuzz-coverage/libwebp/reports/20230901/linux/src/libwebp/src/utils/report.html?ref=blog.isosceles.com">full detail</a> at the code coverage for OSS-Fuzz projects, and it's clear that lossless support for WebP was being fuzzed extensively: </p><figure><img src="https://lh4.googleusercontent.com/QSgfxn8AQf2OzBIZuM3D6WYjT3lMFV9hFfP5zzfCTFZb6ekL1yabmrEo9QYn2qFhgnd7fBDneL6Jfei58v06VjUCSfsCL8AQW8Lj_QLVr5dz0dk2kyOU4OQQX7KqMuMRih0IhhLI_6YzrDzzY1A6dt4" alt="" loading="lazy" width="406" height="341"></figure><p>The problem, we now know, is that this format is incredibly complex and fragile, and the preconditions to trigger this issue are immense. Out of billions of possibilities, we have to construct a sequence of 4 valid Huffman tables that are maximally sized for two different alphabet sizes (280 and 256) before constructing a very specific type of invalid Huffman table for a third alphabet size (40). If a single bit is wrong at any stage, the image decoder throws an error and nothing bad happens.</p><p>In fact one of the first things that Google did after the WebP 0day was fixed was to release a new fuzzer specifically for the Huffman routines in WebP. I tried running this fuzzer for a bit (with a bit of backporting required due to API changes) and it predictably did not find CVE-2023-4863.</p><p>Perhaps I'm wrong and some of the newer techniques involving symbolic execution (like Quarkslab's <a href="https://blog.quarkslab.com/introducing-tritondse-a-framework-for-dynamic-symbolic-execution-in-python.html?ref=blog.isosceles.com">TritonDSE</a>) would be able to solve this -- but standard approaches based on bitflip mutations with a code-coverage feedback loop, and even slightly more sophisticated approaches like <a href="https://github.com/AFLplusplus/AFLplusplus/blob/stable/instrumentation/README.cmplog.md?ref=blog.isosceles.com">CmpLog</a> (input-to-state), would not be able to navigate through all of these intermediary steps to reach this extremely pessimal state. </p><p>It's interesting to contrast this bug with an earlier vulnerability, the <a href="https://googleprojectzero.github.io/0days-in-the-wild/0day-RCAs/2020/CVE-2020-15999.html?ref=blog.isosceles.com">Load_SBit_Png</a> bug in FreeType, which was also discovered "in the wild" in an advanced 0day exploit. It's similar in the sense of being a heap overflow in a common library for a binary file format (for fonts in this instance) written in C, it's similar that it affected Chrome, and it's similar in the sense that FreeType had been heavily fuzzed in the months and years leading up to this attack. The difference was that the Load_SBit_Png bug wasn't found during fuzzing due to a lack of adequate harnessing, rather than some specific constraint of the vulnerability that made it difficult to fuzz. If the fuzzing harnesses had been updated earlier to better reflect the APIs usage, the Load_SBit_Png bug would have been discovered with fuzzing.</p><p>That's not the case for the WebP 0day (CVE-2023-4863) -- unless, perhaps, you got incredibly lucky by having a file in your fuzzing corpus that was already extremely close to the bug and your fuzzer was very well calibrated in terms of its mutation rates. </p><p>In practice, I suspect this bug was discovered through manual code review. In reviewing the code, you &nbsp;would see the huffman_tables allocation being made during header parsing of a VP8L file, so naturally you would look to see how it's used. You would then try to rationalize the lack of bounds checks on the huffman_tables allocation, and if you're persistent enough, you would progressively go deeper and deeper into the problem before realizing that the code was subtly broken. I suspect that most code auditors aren't that persistent though -- this Huffman code stuff is mind bending -- so I'm impressed.</p><h2 id="whats-the-big-deal">What's The Big Deal</h2><p>There's some good news, and some bad news. </p><p>✓ The good news is that the team at Citizen Lab has, once again, done an amazing job of catching a top tier exploit being used in the wild. They have cultivated a lot of trust with the organizations and individuals that are most likely to be harmed by exploits. It's very impressive.</p><p>✗ The bad news is that exploits like this continue to have societal ramifications, and we can only guess how bad the situation really is. The truth is that nobody knows for sure, even the people with exploits.</p><p>✓ The good news is that Apple and Chrome did an amazing job at responding to this issue with the urgency that it deserves. It looks like both groups pushed out an update to their <em>billions</em> of users in just a number of days. That's an impressive feat, it takes an incredible effort and coordination across threat analysis, security engineering, software engineering, product management, and testing teams to make this even remotely possible.</p><p>✗ The bad news is that Android is still likely affected. Similar to Apple's ImageIO, Android has a facility called the <a href="https://developer.android.com/reference/android/graphics/BitmapFactory?ref=blog.isosceles.com">BitmapFactory</a> that handles image decoding, and of course libwebp is supported. As of today, Android hasn't released a security bulletin that includes a fix for CVE-2023-4863 -- although the fix has been merged into AOSP. To put this in context: if this bug does affect Android, then it could potentially be turned into a remote exploit for apps like Signal and WhatsApp. I'd expect it to be fixed in the October bulletin.</p><p>✓ The good news is that the bug seems to be patched correctly in the upstream libwebp, and that patch is making its way to everywhere it should go. </p><p>✗ The bad news is that libwebp is used in a lot of places, and it could be a while until the patch reaches saturation. Also, the code is still very difficult to reason about, and we can't rely on fuzzers to find any other bugs that are lurking here.</p><h2 id="final-thoughts">Final Thoughts</h2><p>The WebP 0day (CVE-2023-4863) is a subtle but powerful vulnerability in a widely used open source library that is highly exposed to attacker inputs. It's both very difficult to fuzz, and very difficult to manually trigger -- but the prize is an exploitable heap overflow that works on multiple browsers, operating systems, and applications. It's likely that CVE-2023-4863 is the same vulnerability used in the <a href="https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/?ref=blog.isosceles.com">BLASTPASS</a> attacks.</p><p>I started this technical analysis shortly after releasing last week's <a href="https://blog.isosceles.com/phineas-fisher-hacktivism-and-magic-tricks/">blog post on Phineas Fisher</a>, which means I was several days late to the party. In practice it took about 3 full work days worth of work (with a lot of additional help from <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a>) to figure out the bug and build a reproducing testcase. </p><p>The lack of available technical information from the vendors here made verification challenging, and it's questionable who this really benefits. Attackers are <a href="https://googleprojectzero.blogspot.com/2023/09/analyzing-modern-in-wild-android-exploit.html?ref=blog.isosceles.com">clearly highly motivated</a> to track and exploit N-day vulnerabilities, and the lack of technical details being released won't significantly slow them down. On the other hand, very few defenders are resourced to be able to perform the type of technical analysis I've shared today. It's counter-intuitive, but withholding basic technical details about how these attacks are working in an asymmetry that mostly benefits attackers -- you quickly end up in a situation where attackers have access to insights about the vulnerability/exploit that defenders don't have.</p><p>This bug also shows that we have an over-reliance on fuzzing for security assurance of complex parser code. Fuzzing is great, but we know that there are many serious security issues that aren't easy to fuzz. For sensitive attack surfaces like image decoding (zero-click remote exploit attack surface), there needs to 1) be a bigger investment in proactive source code reviews, and 2) a renewed focus on ensuring these parsers are adequately sandboxed.</p><p>Finally, thanks again to <a href="https://twitter.com/mistymntncop?ref=blog.isosceles.com">@mistymntncop</a> for both their encouragement and huge technical contributions to this post.</p>
                    
                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USPS In-Person Identity Proofing (167 pts)]]></title>
            <link>https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing</link>
            <guid>37600618</guid>
            <pubDate>Thu, 21 Sep 2023 17:06:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing">https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing</a>, See on <a href="https://news.ycombinator.com/item?id=37600618">Hacker News</a></p>
Couldn't get https://faq.usps.com/s/article/USPS-In-Person-Identity-Proofing: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Bloomberg Is Throwing $500M at Efforts to Shut Down All U.S. Coal Plants (147 pts)]]></title>
            <link>https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082</link>
            <guid>37600484</guid>
            <pubDate>Thu, 21 Sep 2023 16:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082">https://gizmodo.com/michael-bloomberg-500-million-shut-down-coal-plants-1850861082</a>, See on <a href="https://news.ycombinator.com/item?id=37600484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Billionaire, philanthropist, and former NYC Mayor Michael Bloomberg <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.bloomberg.org/press/michael-r-bloomberg-doubles-down-with-additional-500m-to-help-end-fossil-fuels-and-usher-in-a-new-era-of-clean-energy-in-the-united-states/#:~:text=Starting%20with%20the%20Beyond%20Coal,plants%2C%20stopping%20the%20expansion%20of&quot;,{&quot;metric25&quot;:1}]]" href="https://www.bloomberg.org/press/michael-r-bloomberg-doubles-down-with-additional-500m-to-help-end-fossil-fuels-and-usher-in-a-new-era-of-clean-energy-in-the-united-states/#:~:text=Starting%20with%20the%20Beyond%20Coal,plants%2C%20stopping%20the%20expansion%20of" target="_blank" rel="noopener noreferrer">announced this week</a></span> that he will invest $500 million into his campaign to shut down coal plants and <!-- -->halve<!-- --> gas use<!-- --> by 2030.</p><div data-video-id="193562" data-monetizable="true" data-position="sidebar" data-video-title="What Is Carbon Capture? With Gizmodo’s Molly Taft | Techmodo" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="424" data-playlist="193562,195689,195681" data-current="193562"><div><p>What Is Carbon Capture? With Gizmodo’s Molly Taft | Techmodo</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/193562/193562_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/193562/193562_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/19012.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>Through the Beyond Carbon campaign, Bloomberg has successfully helped shut down about 70% of all coal plants in the U.S. This new push is intended to shut down the remaining 150 coal plants. The Beyond Carbon initiative also aims to work with a range of local and state organizations to block the construction of new gas plants. </p><p>The financing is intended to support research, including studies and analysis to deliver accurate data to partnering organizations and officials for better decision-making<!-- -->. It will also fund local policy and advocacy, along with litigation brought against power companies. </p><p>According to a press release from Bloomberg Philanthropies, this push to shut down coal-fired<!-- --> power plants will push officials to invest in renewable energy and clean jobs. “Our climate is warming at a breakneck pace, and there’s more urgency than ever to cut emissions from fossil fuels &amp; move the U.S. faster toward a clean energy future,” Bloomberg <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://twitter.com/MikeBloomberg/status/1704629062763692184&quot;,{&quot;metric25&quot;:1}]]" href="https://twitter.com/MikeBloomberg/status/1704629062763692184" target="_blank" rel="noopener noreferrer">tweeted yesterday</a></span>, referring to the recent announcement.</p><p>Several organizations that have worked closely with the Beyond Carbon campaign have lauded Bloomberg’s efforts and financing. “Combatting the climate crisis is the most critical fight of our time,” Ben Jealous, the executive director of the Sierra Club, said in a press release. “We must transition from fossil fuels to clean energy if we want to protect our health, our environment, and our children, and we must do so in a way that empowers local communities and prioritizes environmental justice.”</p><p>When the Beyond Carbon campaign was first launched, the goal was to retire about 30% of coal plants by 2020. But with the support of environmental groups nationwide, and an early $500 million from Bloomberg, the campaign managed to shut down more than half of the nation’s coal plants by 2022, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.reuters.com/business/energy/michael-bloomberg-pumps-500-million-into-bid-close-all-us-coal-plants-2023-09-20/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.reuters.com/business/energy/michael-bloomberg-pumps-500-million-into-bid-close-all-us-coal-plants-2023-09-20/" target="_blank" rel="noopener noreferrer">Reuters reported</a></span>. </p><p>Bloomberg had long championed environmental and climate causes. When he was the mayor of New York, Bloomberg took the subway regularly—by way of SUV, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://gothamist.com/news/mayor-bloombergs-subway-commute-not-like-yours&quot;,{&quot;metric25&quot;:1}]]" href="https://gothamist.com/news/mayor-bloombergs-subway-commute-not-like-yours" target="_blank" rel="noopener noreferrer">Gothamist reported back in 2007</a></span>. He also pushed for more biking infrastructure in the city, and the Citi Bike system was launched in 2013.</p><p>Bloomberg has also financed other environmental campaigns, including one to stop new petrochemical plants that produce packaging, plastics, and fertilizers, The New York Times<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/2023/09/20/climate/michael-blooomberg-climate-petrochemicals.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/2023/09/20/climate/michael-blooomberg-climate-petrochemicals.html" target="_blank" rel="noopener noreferrer"> reported</a></span>. He launched a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.bloomberg.org/press/michael-r-bloomberg-launches-new-85-million-campaign-to-stop-rapid-rise-of-pollution-from-the-petrochemical-industry-in-the-united-states/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.bloomberg.org/press/michael-r-bloomberg-launches-new-85-million-campaign-to-stop-rapid-rise-of-pollution-from-the-petrochemical-industry-in-the-united-states/" target="_blank" rel="noopener noreferrer">$85 million campaign</a></span> last year to support the new Beyond Petrochemicals campaign with the goal to “block the expansion of more than 120 proposed petrochemical projects” in Louisiana, the Ohio River Valley, and Texas. </p><p><em>Want more climate and environment stories? Check out Earther’s guides to </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/what-s-the-best-way-to-decarbonize-your-home-1847518817&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/what-s-the-best-way-to-decarbonize-your-home-1847518817" target="_blank"><em>decarbonizing your home</em></a></span><em>, </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-can-i-divest-from-fossil-fuels-1847774633&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-can-i-divest-from-fossil-fuels-1847774633" target="_blank"><em>divesting from fossil fuels</em></a></span><em>, </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-to-pack-a-go-bag-climate-disasters-wildfires-floods-1849457027&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-to-pack-a-go-bag-climate-disasters-wildfires-floods-1849457027" target="_blank"><em>packing a disaster go bag</em></a></span><em>, and </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-can-you-overcome-climate-dread-1847606185&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-can-you-overcome-climate-dread-1847606185" target="_blank"><em>overcoming climate dread</em></a></span><em>. And don’t miss our coverage of the </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/ipcc-report-2023-climate-change-paris-agreement-un-1850242687&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/ipcc-report-2023-climate-change-paris-agreement-un-1850242687" target="_blank"><em>latest IPCC climate report</em></a></span><em>, the future of </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/future-of-carbon-dioxide-removal-frontier-project-1848782278&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/future-of-carbon-dioxide-removal-frontier-project-1848782278" target="_blank"><em>carbon dioxide removal</em></a></span><em>, and the un-greenwashed facts on </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/what-is-bioplastic-biodegradable-plant-based-plastic-1848999921&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/what-is-bioplastic-biodegradable-plant-based-plastic-1848999921" target="_blank"><em>bioplastics</em></a></span><em> and </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/least-recyclable-plastics-1848853267&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/least-recyclable-plastics-1848853267" target="_blank"><em>plastic recycling</em></a></span><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My Single-File Python Script I Used to Replace Splunk in My Startup (276 pts)]]></title>
            <link>https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer</link>
            <guid>37600019</guid>
            <pubDate>Thu, 21 Sep 2023 16:26:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer">https://github.com/Dicklesworthstone/automatic_log_collector_and_analyzer</a>, See on <a href="https://news.ycombinator.com/item?id=37600019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-automatically-download-and-analyze-log-files-from-remote-machines" dir="auto"><a href="#automatically-download-and-analyze-log-files-from-remote-machines">Automatically Download and Analyze Log Files from Remote Machines</a></h2>
<p dir="auto">This application is designed to collect and analyze logs from remote machines hosted on Amazon Web Services (AWS) and other cloud hosting services.</p>
<p dir="auto"><strong>Note</strong>: This application was specifically designed for use with Pastel Network's log files. However, it can be easily adapted to work with any log files by modifying the parsing functions, data models, and specifying the location and names of the log files to be downloaded. It is compatible with log files stored in a standard format, where each entry is on a separate line and contains a timestamp, a log level, and a message. The application has been tested with log files several gigabytes in size from dozens of machines and can process all of it in minutes. It is designed for Ubuntu 22.04+, but can be adapted for other Linux distributions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/Dicklesworthstone/automatic_log_collector_and_analyzer/main/demo_screenshot.png"><img src="https://raw.githubusercontent.com/Dicklesworthstone/automatic_log_collector_and_analyzer/main/demo_screenshot.png" alt="Demo Screenshot:"></a></p>
<h2 tabindex="-1" id="user-content-customization" dir="auto"><a href="#customization">Customization</a></h2>
<p dir="auto">To adapt this application for your own use case, refer to the included sample log files and compare them to the parsing functions in the code. You can also modify the data models to store log entries as desired.</p>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<p dir="auto">The application consists of various Python scripts that perform the following functions:</p>
<ul dir="auto">
<li><strong>Connect to Remote Machines</strong>: Using the boto3 library for AWS instances and an Ansible inventory file for non-AWS instances, the application establishes SSH connections to each remote machine.</li>
<li><strong>Download and Parse Log Files</strong>: Downloads specified log files from each remote machine and parses them. The parsed log entries are then queued for database insertion.</li>
<li><strong>Insert Log Entries into Database</strong>: Uses SQLAlchemy to insert the parsed log entries from the queue into an SQLite database.</li>
<li><strong>Process and Analyze Log Entries</strong>: Processes and analyzes log entries stored in the database, offering functions to find error entries and create views of aggregated data based on specified criteria.</li>
<li><strong>Generate Network Activity Data</strong>: Fetches and processes network activity data from each remote machine.</li>
<li><strong>Expose Database via Web App using Datasette</strong>: Once the database is generated, it can be shared over the web using Datasette.</li>
</ul>
<h2 tabindex="-1" id="user-content-compatibility" dir="auto"><a href="#compatibility">Compatibility</a></h2>
<p dir="auto">The tool is compatible with both AWS-hosted instances and any list of Linux instances stored in a standard Ansible inventory file with the following structure:</p>
<div dir="auto" data-snippet-clipboard-copy-content="all:
  vars:
    ansible_connection: ssh
    ansible_user: ubuntu
    ansible_ssh_private_key_file: /path/to/ssh/key/file.pem
  hosts:
    MyCoolMachine01:
      ansible_host: 1.2.3.41
    MyCoolMachine02:
      ansible_host: 1.2.3.41.19"><pre><span>all</span>:
  <span>vars</span>:
    <span>ansible_connection</span>: <span>ssh</span>
    <span>ansible_user</span>: <span>ubuntu</span>
    <span>ansible_ssh_private_key_file</span>: <span>/path/to/ssh/key/file.pem</span>
  <span>hosts</span>:
    <span>MyCoolMachine01</span>:
      <span>ansible_host</span>: <span>1.2.3.41</span>
    <span>MyCoolMachine02</span>:
      <span>ansible_host</span>: <span>1.2.3.41.19</span></pre></div>
<p dir="auto">(Both can be used seamlessly.)</p>
<h2 tabindex="-1" id="user-content-warning" dir="auto"><a href="#warning">Warning</a></h2>
<p dir="auto">To simplify the code, the tool is designed to delete all downloaded log files and generated databases each time it runs. Consequently, this can consume significant bandwidth depending on your log files' size. However, the design's high level of parallel processing and concurrency allows it to run quickly, even when connecting to dozens of remote machines and downloading hundreds of log files.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>
<p dir="auto">Designed for Ubuntu 22.04+, first install the requirements:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 -m venv venv
source venv/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install wheel
pip install -r requirements.txt"><pre>python3 -m venv venv
<span>source</span> venv/bin/activate
python3 -m pip install --upgrade pip
python3 -m pip install wheel
pip install -r requirements.txt</pre></div>
<p dir="auto">You will also need to install Redis:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install redis -y"><pre>sudo apt install redis -y</pre></div>
<p dir="auto">And install Datasette to expose the results as a website:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install pipx -y &amp;&amp; pipx ensurepath &amp;&amp; pipx install datasette"><pre>sudo apt install pipx -y <span>&amp;&amp;</span> pipx ensurepath <span>&amp;&amp;</span> pipx install datasette</pre></div>
<p dir="auto">To run the application every 30 minutes as a cron job, execute:</p>

<p dir="auto">And add the following line:</p>
<div dir="auto" data-snippet-clipboard-copy-content="*/15 * * * * . $HOME/.profile; /home/ubuntu/automatic_log_collector_and_analyzer/venv/bin/python /home/ubuntu/automatic_log_collector_and_analyzer/automatic_log_collector_and_analyzer.py >> /home/ubuntu/automatic_log_collector_and_analyzer/log_$(date +\%Y-\%m-\%dT\%H_\%M_\%S).log 2>&amp;1"><pre><span>*</span>/15 <span>*</span> <span>*</span> <span>*</span> <span>*</span> <span>.</span> <span>$HOME</span>/.profile<span>;</span> /home/ubuntu/automatic_log_collector_and_analyzer/venv/bin/python /home/ubuntu/automatic_log_collector_and_analyzer/automatic_log_collector_and_analyzer.py <span>&gt;&gt;</span> /home/ubuntu/automatic_log_collector_and_analyzer/log_<span><span>$(</span>date +<span>\%</span>Y-<span>\%</span>m-<span>\%</span>dT<span>\%</span>H_<span>\%</span>M_<span>\%</span>S<span>)</span></span>.log <span>2&gt;&amp;1</span></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RAG is more than just embedding search (143 pts)]]></title>
            <link>https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</link>
            <guid>37599873</guid>
            <pubDate>Thu, 21 Sep 2023 16:18:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/">https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</a>, See on <a href="https://news.ycombinator.com/item?id=37599873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
      
      <main data-md-component="main">
        <div data-md-component="content">
    
    <article>
      
        

  
  



<p>With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of <a href="https://jxnl.notion.site/Working-with-me-ec2bb36a5ac048c2a8f6bd888faea6c2?pvs=4">helping startups</a> integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p>
<div>
<p>What is RAG?</p>
<p>Retrival augmented generation (RAG) is a technique that uses a LLM to generate responses, but uses a search backend to augment the generation, in the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p>
</div>
<figure>
<p><img alt="RAG" src="https://jxnl.github.io/instructor/blog/img/dumb_rag.png">
  </p>
<figcaption>Simple RAG that embedded the user query and makes a search.</figcaption>
</figure>
<p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think.</p>
<h2 id="the-dumb-rag-model">The 'Dumb' RAG Model</h2>
<p>When you ask a question like, "what is the capital of France?" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>
<h3 id="why-is-this-a-problem">Why is this a problem?</h3>
<ul>
<li>
<p><strong>Query-Document Mismatch</strong>: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p>
</li>
<li>
<p><strong>Monolithic Search Backend</strong>: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p>
</li>
<li>
<p><strong>Limitation of text search</strong>: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, <code>what problems did we fix last week</code> that cannot be answered by a simple text search, since documents that contain <code>problem, last week</code> are going to be present at every week.</p>
</li>
<li>
<p><strong>Limited ability to plan</strong>: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results.</p>
</li>
</ul>
<p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>
<h2 id="improving-the-rag-model-with-query-understanding">Improving the RAG Model with Query Understanding</h2>

<p>Ultimately what you want to deploy is a <a href="https://en.wikipedia.org/wiki/Query_understanding">system that understands</a> how to take the query and rewrite it to improve precision and recall. </p>
<figure>
<p><img alt="RAG" src="https://jxnl.github.io/instructor/blog/img/query_understanding.png">
  </p>
<figcaption>Query Understanding system routes to multiple search backends.</figcaption>
</figure>
<p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>
<h2 id="whats-instructor">Whats instructor?</h2>
<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling api..</p>
<ul>
<li><strong>Widespread Adoption</strong>: Pydantic is a popular tool among Python developers.</li>
<li><strong>Simplicity</strong>: Pydantic allows model definition in Python.</li>
<li><strong>Framework Compatibility</strong>: Many Python frameworks already use Pydantic.</li>
</ul>

<p>Take <a href="https://metaphor.systems/">Metaphor Systems</a>, which turns natural language queries into their custom search-optimized query. If you take a look web ui you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using an language model, and turn it into a fully specified metaphor systems query.</p>
<figure>
<p><img alt="Metaphor Systems" src="https://jxnl.github.io/instructor/blog/img/meta.png"></p>
<figcaption>Metaphor Systems UI</figcaption>
</figure>
<p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. Its actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p>
<div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span>class</span> <span>DateRange</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span>start</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span>end</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span>class</span> <span>MetaphorQuery</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span>rewritten_query</span><span>:</span> <span>str</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span>published_daterange</span><span>:</span> <span>DateRange</span>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span>domains_allow_list</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>]</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span>async</span> <span>def</span> <span>execute</span><span>():</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span>return</span> <span>await</span> <span>metaphor</span><span>.</span><span>search</span><span>(</span><span>...</span><span>)</span>
</span></code></pre></div>
<p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This is a powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. </p>
<div><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span>import</span> <span>instructor</span> 
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span>import</span> <span>openai</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span># Enables response_model in the openai client</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span>instructor</span><span>.</span><span>patch</span><span>()</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span>query</span> <span>=</span> <span>openai</span><span>.</span><span>ChatCompletion</span><span>.</span><span>create</span><span>(</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    <span>model</span><span>=</span><span>"gpt-4"</span><span>,</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span>response_model</span><span>=</span><span>MetaphorQuery</span><span>,</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span>messages</span><span>=</span><span>[</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>        <span>{</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>            <span>"role"</span><span>:</span> <span>"system"</span><span>,</span> 
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>            <span>"content"</span><span>:</span> <span>"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ..."</span>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span>},</span>
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>        <span>{</span>
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>            <span>"role"</span><span>:</span> <span>"user"</span><span>,</span> 
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>            <span>"content"</span><span>:</span> <span>"What are some recent developments in AI?"</span>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>        <span>}</span>
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    <span>],</span>
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span>)</span>
</span></code></pre></div>
<p><strong>Example Output</strong></p>
<div><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span>{</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span>    </span><span>"rewritten_query"</span><span>:</span><span> </span><span>"novel developments advancements ai artificial intelligence machine learning"</span><span>,</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span>    </span><span>"published_daterange"</span><span>:</span><span> </span><span>{</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span>        </span><span>"start"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span>        </span><span>"end"</span><span>:</span><span> </span><span>"2021-06-17"</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a><span>    </span><span>},</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span>    </span><span>"domains_allow_list"</span><span>:</span><span> </span><span>[</span><span>"arxiv.org"</span><span>]</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span>}</span>
</span></code></pre></div>
<p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that is deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p>
<div><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span>class</span> <span>DateRange</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>    <span>start</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>    <span>end</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    <span>chain_of_thought</span><span>:</span> <span>str</span> <span>=</span> <span>Field</span><span>(</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>        <span>None</span><span>,</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>        <span>description</span><span>=</span><span>"Think step by step to plan what is the best time range to search in"</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    <span>)</span>
</span></code></pre></div>
<p>Now, let's see how this approach can help model an agent like personal assistant.</p>
<h2 id="case-study-2-personal-assistant">Case Study 2: Personal Assistant</h2>
<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, "What do I have today?", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p>
<div><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span>class</span> <span>ClientSource</span><span>(</span><span>enum</span><span>.</span><span>Enum</span><span>):</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span>GMAIL</span> <span>=</span> <span>"gmail"</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>    <span>CALENDAR</span> <span>=</span> <span>"calendar"</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>
</span><span id="__span-4-5"><a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a><span>class</span> <span>SearchClient</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-4-6"><a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>    <span>query</span><span>:</span> <span>str</span>
</span><span id="__span-4-7"><a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>    <span>keywords</span><span>:</span> <span>List</span><span>[</span><span>str</span><span>]</span>
</span><span id="__span-4-8"><a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>    <span>email</span><span>:</span> <span>str</span>
</span><span id="__span-4-9"><a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>    <span>source</span><span>:</span> <span>ClientSource</span>
</span><span id="__span-4-10"><a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>    <span>start_date</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-4-11"><a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>    <span>end_date</span><span>:</span> <span>datetime</span><span>.</span><span>date</span>
</span><span id="__span-4-12"><a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a>
</span><span id="__span-4-13"><a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>    <span>async</span> <span>def</span> <span>execute</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
</span><span id="__span-4-14"><a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>        <span>if</span> <span>self</span><span>.</span><span>source</span> <span>==</span> <span>ClientSource</span><span>.</span><span>GMAIL</span><span>:</span>
</span><span id="__span-4-15"><a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>            <span>...</span>
</span><span id="__span-4-16"><a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>        <span>elif</span> <span>self</span><span>.</span><span>source</span> <span>==</span> <span>ClientSource</span><span>.</span><span>CALENDAR</span><span>:</span>
</span><span id="__span-4-17"><a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a>            <span>...</span>
</span><span id="__span-4-18"><a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a>
</span><span id="__span-4-19"><a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a><span>class</span> <span>Retrival</span><span>(</span><span>BaseModel</span><span>):</span>
</span><span id="__span-4-20"><a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a>    <span>queries</span><span>:</span> <span>List</span><span>[</span><span>SearchClient</span><span>]</span>
</span><span id="__span-4-21"><a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a>
</span><span id="__span-4-22"><a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a>    <span>async</span> <span>def</span> <span>execute</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>str</span><span>:</span>
</span><span id="__span-4-23"><a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a>        <span>return</span> <span>await</span> <span>asyncio</span><span>.</span><span>gather</span><span>(</span><span>*</span><span>[</span><span>query</span><span>.</span><span>execute</span><span>()</span> <span>for</span> <span>query</span> <span>in</span> <span>self</span><span>.</span><span>queries</span><span>])</span>
</span></code></pre></div>
<p>Now we can call this with a simple query like "What do I have today?" and it will try to async dispatch to the correct backend. Its will important to prompt the language model well, but we'll leave that for another day.</p>
<div><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span>import</span> <span>instructor</span> 
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span>import</span> <span>openai</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span># Enables response_model in the openai client</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span>instructor</span><span>.</span><span>patch</span><span>()</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>
</span><span id="__span-5-7"><a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a><span>retrival</span> <span>=</span> <span>openai</span><span>.</span><span>ChatCompletion</span><span>.</span><span>create</span><span>(</span>
</span><span id="__span-5-8"><a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>    <span>model</span><span>=</span><span>"gpt-4"</span><span>,</span>
</span><span id="__span-5-9"><a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>    <span>response_model</span><span>=</span><span>Retrival</span><span>,</span>
</span><span id="__span-5-10"><a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>    <span>messages</span><span>=</span><span>[</span>
</span><span id="__span-5-11"><a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>        <span>{</span><span>"role"</span><span>:</span> <span>"system"</span><span>,</span> <span>"content"</span><span>:</span> <span>"You are Jason's personal assistant."</span><span>},</span>
</span><span id="__span-5-12"><a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>        <span>{</span><span>"role"</span><span>:</span> <span>"user"</span><span>,</span> <span>"content"</span><span>:</span> <span>"What do I have today?"</span><span>}</span>
</span><span id="__span-5-13"><a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>    <span>],</span>
</span><span id="__span-5-14"><a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a><span>)</span>
</span></code></pre></div>
<p><strong>Example Output</strong></p>
<div><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span>{</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span>    </span><span>"queries"</span><span>:</span><span> </span><span>[</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span>        </span><span>{</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span>            </span><span>"query"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span>            </span><span>"keywords"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span>            </span><span>"email"</span><span>:</span><span> </span><span>"jason@example.com"</span><span>,</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span>            </span><span>"source"</span><span>:</span><span> </span><span>"gmail"</span><span>,</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span>            </span><span>"start_date"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a><span>            </span><span>"end_date"</span><span>:</span><span> </span><span>No</span><span>ne</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a><span>        </span><span>},</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a><span>        </span><span>{</span>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a><span>            </span><span>"query"</span><span>:</span><span> </span><span>No</span><span>ne</span><span>,</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a><span>            </span><span>"keywords"</span><span>:</span><span> </span><span>[</span><span>"meeting"</span><span>,</span><span> </span><span>"call"</span><span>,</span><span> </span><span>"zoom"</span><span>]]],</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a><span>            </span><span>"email"</span><span>:</span><span> </span><span>"jason@example.com"</span><span>,</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a><span>            </span><span>"source"</span><span>:</span><span> </span><span>"calendar"</span><span>,</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a><span>            </span><span>"start_date"</span><span>:</span><span> </span><span>"2023-09-17"</span><span>,</span>
</span><span id="__span-6-17"><a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a><span>            </span><span>"end_date"</span><span>:</span><span> </span><span>No</span><span>ne</span>
</span><span id="__span-6-18"><a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a>
</span><span id="__span-6-19"><a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a><span>        </span><span>}</span>
</span><span id="__span-6-20"><a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a><span>    </span><span>]</span>
</span><span id="__span-6-21"><a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a><span>}</span>
</span></code></pre></div>
<p>Notice that we have a list of queries, that route to different search backends, email and calendar. We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well, perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p>
<div>
<p>Can I used framework X?</p>
<p>I get this question many times, but its just code, within these dispatchs you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information, the sky is the limit.</p>
</div>
<p>Both of these examples show case how both search providors and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build a LLM layer, from scratch, in front of any arbitrary backend.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This isnt about fancy embedding tricks, its just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>
<h2 id="whats-next">What's Next?</h2>
<p>Here I want to show that `instructor`` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs.</p>
<p>I believe collaboration between domain experts and AI engineers the key to enable advanced tool use. I’ve been building a new tool on top of instructor that enables seamless collaboration and experimentation on LLMs with structured outputs. If you’re interested, visit <a href="https://useinstructor.com/">useinstructor.com</a> and take our survey to join the waitlist.
Together, let’s create tools that are as brilliant as the minds that use them.</p>


  


  



      
    </article>
  </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insider trade on Splunk acquisition for 45,650% return (400 pts)]]></title>
            <link>https://twitter.com/unusual_whales/status/1704870849831125446</link>
            <guid>37599587</guid>
            <pubDate>Thu, 21 Sep 2023 15:58:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/unusual_whales/status/1704870849831125446">https://twitter.com/unusual_whales/status/1704870849831125446</a>, See on <a href="https://news.ycombinator.com/item?id=37599587">Hacker News</a></p>
Couldn't get https://twitter.com/unusual_whales/status/1704870849831125446: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lead poisoning causes more death, IQ loss than thought: study (424 pts)]]></title>
            <link>https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html</link>
            <guid>37599542</guid>
            <pubDate>Thu, 21 Sep 2023 15:55:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html">https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html</a>, See on <a href="https://news.ycombinator.com/item?id=37599542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2023/lead-poisoning-causes.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2023/lead-poisoning-causes.jpg" data-sub-html="Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: <i>The Lancet Planetary Health</i> (2023). DOI: 10.1016/S2542-5196(23)00166-3">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2023/lead-poisoning-causes.jpg" alt="Lead poisoning causes far more death, IQ loss than thought: study" title="Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: The Lancet Planetary Health (2023). DOI: 10.1016/S2542-5196(23)00166-3" width="800" height="355">
             <figcaption>
                Blood lead level IQ loss function from Crump and colleagues. The blood lead level is mean lifetime blood lead level in children younger than 5 years. The solid line is the central estimate and the shaded area is the 95% CI as per the study by Crump and colleagues. IQ=intelligence quotient. Credit: <i>The Lancet Planetary Health</i> (2023). DOI: 10.1016/S2542-5196(23)00166-3
            </figcaption>        </figure>
    </div>
<p>Lead poisoning has a far greater impact on global health than previously thought, potentially contributing to over five million deaths a year and posing a similar threat to air pollution, modeling research suggested Tuesday.
                                                </p>                                                                                
<p>The study, described as "a wake-up call", also estimated that exposure to the toxic metal causes <a href="https://medicalxpress.com/tags/young+children/" rel="tag">young children</a> in developing countries to lose an average of nearly six IQ points each.
</p><p>Lead pollution has been shown to cause a range of serious health problems, particularly relating to <a href="https://medicalxpress.com/tags/heart+disease/" rel="tag">heart disease</a> and the brain development of small children, resulting in <a href="https://medicalxpress.com/tags/leaded+gasoline/" rel="tag">leaded gasoline</a> being banned worldwide.
</p><p>But people can still be exposed to the <a href="https://medicalxpress.com/tags/potent+neurotoxin/" rel="tag">potent neurotoxin</a> via food, soil, cookware, fertilizers, cosmetics, lead-acid car batteries and other sources.
</p><p>The two World Bank economists who authored the study, published in the <i>Lancet Planetary Health</i> journal, said it was the first to assess the impact of lead exposure on heart disease deaths and child IQ loss in wealthy and developing nations.
</p><p>Lead author Bjorn Larsen told AFP that when the pair first saw the figure their model calculated, "we didn't even dare to whisper the number" because it was so "enormous".
</p><p>Their model estimates that 5.5 million adults died from heart disease in 2019 because of lead exposure, 90 percent of them in low- and <a href="https://medicalxpress.com/tags/middle-income+countries/" rel="tag">middle-income countries</a>.
</p><p>That is six times higher than the previous estimate, and represents around 30 percent of all deaths from <a href="https://medicalxpress.com/tags/cardiovascular+disease/" rel="tag">cardiovascular disease</a>—the leading cause of death worldwide.
</p><p>It would mean that lead exposure is a bigger cause of heart disease than smoking or cholesterol, Larsen said.
</p><h2>$6 trillion cost</h2>
<p>The research also estimated that children under five lost a cumulative 765 million IQ points due to lead poisoning globally in 2019, with 95 percent of those losses coming in developing countries.
</p><p>That number is nearly 80 percent higher than previously estimated.
</p><p>The World Bank researchers put the economic cost of lead exposure at $6 trillion in 2019, equivalent to seven percent of global gross domestic product.
</p><p>For the analysis, the researchers used estimates of blood lead levels in 183 countries taken from the landmark 2019 Global Burden of Disease study.
</p><p>Previous research had measured only lead's effect on heart disease when it came to raising <a href="https://medicalxpress.com/tags/blood+pressure/" rel="tag">blood pressure</a>. But the new study looked at numerous other ways lead affects hearts, such as the hardening of arteries that can lead to stroke, resulting in the higher numbers, Larsen said.
</p><p>Roy Harrison, an expert in <a href="https://medicalxpress.com/tags/air+pollution/" rel="tag">air pollution</a> and health at Birmingham University in the UK, who was not involved in the study, told AFP it was "interesting, but subject to many uncertainties".
</p><p>For example, the relationship between lead in blood and <a href="https://medicalxpress.com/tags/heart/" rel="tag">heart</a> disease is based on a survey in the United States, and whether those findings could be applied worldwide "is a huge jump of faith", he said.
</p><p>Harrison also pointed out that the model used estimations—not tests—of lead in blood in many developing countries.
</p><p>If the results were confirmed, "they would be of major public health significance, but at present, this is simply an interesting hypothesis", he said.
</p><h2>'Piece of the puzzle'</h2>
<p>Richard Fuller, president of the NGO Pure Earth, said that when surveys in developing countries did test for lead in blood, they mostly found higher levels than estimated in the new study.
</p><p>This means "the impact of lead might be worse than the report describes", he told AFP, calling it a "wake-up call".
</p><p>Larsen said "we're still a little in the dark" when it came to understanding how much different sources of lead contribute to blood contamination.
</p><p>Fuller said part of this "missing piece of the puzzle" was revealed in a Pure Earth report released on Tuesday, which analyzed 5,000 samples of consumer goods and food in 25 developing countries.
</p><p>It found high rates of lead contamination in metal pots and pans, ceramic cookware, paint, cosmetics and toys.
</p><p>"This is why poorer countries have so much <a href="https://medicalxpress.com/tags/lead+poisoning/" rel="tag">lead poisoning</a>," Fuller said. "It's items in the kitchen that are poisoning them."
                                                                                
                                        											</p><div>
												                                                    <p><strong>More information:</strong>
                                                    Bjorn Larsen et al, Global health burden and cost of lead exposure in children and adults: a health impact and economic modelling analysis, <i>The Lancet Planetary Health</i> (2023).  <a data-doi="1" href="https://dx.doi.org/10.1016/S2542-5196(23)00166-3" target="_blank">DOI: 10.1016/S2542-5196(23)00166-3</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                © 2023 AFP
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Lead poisoning causes far more death, IQ loss than thought: study (2023, September 12)
                                                 retrieved 21 September 2023
                                                 from https://medicalxpress.com/news/2023-09-poisoning-death-iq-loss-thought.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Matrix 2.0: The Future of Matrix (491 pts)]]></title>
            <link>https://matrix.org/blog/2023/09/matrix-2-0/</link>
            <guid>37599510</guid>
            <pubDate>Thu, 21 Sep 2023 15:53:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matrix.org/blog/2023/09/matrix-2-0/">https://matrix.org/blog/2023/09/matrix-2-0/</a>, See on <a href="https://news.ycombinator.com/item?id=37599510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><strong><em>TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to <a href="https://element.io/blog/element-x-ignition/">Element X</a>.</em></strong></p>
<p>Matrix has been going for over 9 years now, providing an open standard for secure, decentralised communication for the open Web - and it’s been quite the journey to get to where we are today.  Right now, according to Synapse’s opt-in usage reporting, in total there are 111,873,374 matrix IDs on the public network, spanning 17,289,201 rooms, spread over 64,256 servers.  This is just scratching the surface, given we estimate that 66% of servers in the public network don’t report stats, and there are many enormous private networks of servers too.  We’ve come a long way from creating Matrix HQ as the first ever room on today’s public network, back on Aug 13th 2014 :)</p>
<p>Meanwhile, the Matrix ecosystem has continued to grow unbelievably - with huge numbers of independent clients, bots and bridges maturing into ecosystems of their own, whole new companies forming around the protocol, and organisations ranging from open source projects to governments, NGOs and Fortune 100 companies adopting Matrix as a way to run their own secure, decentralised, standards-based self-sovereign communication.</p>
<p>The world needs Matrix more than ever.  Every day the importance of decentralisation is more painfully obvious, as we concretely see the terrifying risks of centralised Internet services - whether that’s through corporate takeover, state censorship, blanket surveillance, Internet shutdowns, surveillance capitalism, or the spectre of gigantic centralised data breaches.  It’s been amazing to see the world pivot in favour of decentralisation over the time we’ve been building Matrix, and our mission has never been more important.</p>
<p>On one hand it feels we’re creeping ever closer to that goal of providing the missing communication layer for the open Web.  The European Union’s Digital Markets Act (DMA) is a huge step in that direction - regulation that mandates that if the large centralised messaging providers are to operate in the EU, they <strong>must</strong> interoperate.  We’ve been busy working away to make this a reality, including participating in the IETF for the first time as part of the MIMI working group - demonstrating <a href="https://datatracker.ietf.org/meeting/117/materials/slides-117-mimi-linearized-matrix-for-mimi-01">concretely</a> how (for instance) Android Messages could natively speak Matrix in order to interoperate with other services, while preserving end-to-end encryption.</p>
<p>On the other hand, Matrix has often got stuck in focusing on solving the Hard Problems of decentralisation, decentralised end-to-end encryption, and the logistical complexities of supporting a massive heterogeneous public communication network and its surrounding heterogeneous ecosystem.  It’s fair to say that in the early days our focus was on making something that worked at all - and then later, we shifted to focusing on something that worked and scaled correctly… but we hadn’t managed to focus on ensuring that Matrix provides the building blocks necessary to create blazingly fast, hyper-efficient communication apps which has potential to outperform the centralised mainstream messaging services…</p>
<p><strong>…until now!</strong></p>
<h2 id="matrix-2-0">Matrix 2.0</h2>
<p>Back at FOSDEM <a href="https://archive.fosdem.org/2023/schedule/event/matrix20/">we announced the idea of Matrix 2.0</a> - a series of huge step changes in terms of Matrix’s usability and performance, made up of <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3575">Sliding Sync</a> (instant login/launch/sync), <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3861">Native OIDC</a> (industry-standard authentication), <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3401">Native Group VoIP</a> (end-to-end encrypted large-scale voice &amp; video conferencing) and <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3902">Faster Joins</a> (lazy-loading room state when your server joins a room).</p>
<p>Now, we’re excited to announce that as of today everyone can start playing with these Matrix 2.0 features. There’s still some work to bring them formally into the specification, but we’re putting it out there for folks to experience right now. Developers: watch this space for updates on the spec front.</p>
<p>Practically speaking, this means there are now implementations of the four pillars of Matrix 2.0 available today which you can use to power a daily-driver Matrix 2.0 client.  The work here has been driven primarily by <a href="https://element.io/">Element</a>, using their new <a href="https://element.io/labs/element-x">Element X</a> client as the test-bed for the new Matrix 2.0 functionality and to prove that the new APIs are informed by real-world usage and can concretely demonstrably create an app which begins to outperform iMessage, WhatsApp and Telegram in terms of usability and performance… all while benefiting from being 100% built on Matrix.</p>
<h3 id="matrix-rust-sdk-and-element-x">matrix-rust-sdk and Element X</h3>
<p><a href="https://element.io/blog/element-x-ignition/"><img src="https://matrix.org/blog/img/20230921-element-x.png"></a></p>
<p>The mission of Matrix 2.0 has been to provide a huge step forwards in real-world performance, usability and stability - and that means using a real client codebase as a guinea pig to ensure the new protocol is fit for purpose. <a href="https://github.com/matrix-org/matrix-rust-sdk">matrix-rust-sdk</a> has been the main vehicle for this, with <a href="https://element.io/labs/element-x">Element X</a> as the app primarily driving the new features (although other clients built on matrix-rust-sdk such as <a href="https://gitlab.gnome.org/GNOME/fractal#beta-version">Fractal 5</a> can then automatically benefit from the work should they wish).</p>
<p>To see what all the fuss is about, your best bet is probably to head over to the <a href="https://element.io/blog/element-x-ignition/">Element X launch blog post</a> and read all about it!  But from the Matrix perspective, this is a flag day in terms of the existence of a Matrix client which empirically outperforms the mainstream clients both in terms of usability and performance: it shows that Matrix is indeed viable to power communication for billions of users, should we get the chance.</p>
<p>From a client perspective: this has meant implementing Sliding Sync (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/kegan/sync-v3/proposals/3575-sync.md">MSC3575</a>) in matrix-rust-sdk - and then creating the entirely new <a href="https://matrix-org.github.io/matrix-rust-sdk/matrix_sdk_ui/index.html">matrix-sdk-ui</a> crate in order to expose higher level APIs to help apps efficiently drive their UI, without each app having to keep reinventing the wheel and risking getting it wrong.  The new UI crate gives APIs for efficiently managing a lazy-loaded room list, lazy-loaded room timelines (including edits, reactions, aggregations, redactions etc), and even when the app should show a sync spinner or not.  As a result, the vast majority of the heavy lifting can be handled in matrix-rust-sdk, ensuring that the app layer can focus on UI rather than Matrix guts - and performance improvements (e.g. roomlist caching and timeline caching) can all be handled in one place to the benefit of all clients using the SDK.</p>
<p>This is a huge breakthrough relative to the old days of Matrix where each client would have no choice but burn significant amounts of time hand-carving its own timeline and encryption glue logic (although of course clients are still very welcome to do so if they wish!) - but for those wanting higher-level building building blocks, matrix-rust-sdk now provides an excellent basis for experimenting with Matrix 2.0 clients.  It’s worth noting that the library is still evolving <strong>fast</strong>, though, and many APIs are not long-term stable.  Both the Sliding Sync API and the UI crates are still subject to significant change, and while the crypto crate and its underlying <a href="https://github.com/matrix-org/vodozemac">vodozemac</a> E2EE implementation is pretty stable, features such as E2EE Backup are still being added to the top-level matrix-rust-sdk (and thence Element X).</p>
<p>In order to hook matrix-rust-sdk up to Element X, the Element team <a href="https://github.com/mozilla/uniffi-rs/pull/1346">ended</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1292">up</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1259">contributing</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1684">cancellable</a> <a href="https://github.com/mozilla/uniffi-rs/pull/1409">async bindings</a> to <a href="https://mozilla.github.io/uniffi-rs/">uniffi</a>, Mozilla’s language binding generator, so you can now call matrix-rust-sdk directly from Swift, Kotlin and (in theory) other languages, complete with beautifully simple async/await non-blocking semantics.  This looks to be a pretty awesome stack for doing modern cross-platform development - so even if you have a project which isn’t natively in Rust, you should be able to lean on matrix-rust-sdk if you so desire!  We hope that other projects will follow the Rust + Swift/Kotlin pattern for their extreme performance needs :)</p>
<h3 id="sliding-sync">Sliding Sync</h3>
<p>The single biggest change in Matrix 2.0 is the proposal of an entirely new sync API called Sliding Sync (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/kegan/sync-v3/proposals/3575-sync.md">MSC3575</a>).  The goal of Sliding Sync is to ensure that the application has the option of loading the absolutely bare essential data required to render its visible user interface - ensuring that operations which have historically been horribly slow in Matrix (login and initial sync, launch and incremental sync) are instant, no matter how many rooms the user is in or how large those rooms are.</p>
<p>While matrix-rust-sdk implements both Sync v2 (the current API in Matrix 1.8) as well as Sliding Sync, Element X deliberately only implements Sliding Sync, in order to focus exclusively on getting the fastest UI possible (and generally to exercise the API).  Therefore to use Element X, you need to be running a homeserver with Sliding Sync support, which (for now) means running a <a href="https://github.com/matrix-org/sliding-sync">sliding-sync proxy</a> which bolts Sliding Sync support on to existing homeservers.  You can check out Thib’s <a href="https://www.youtube.com/watch?v=25wkV2ZCSsM">excellent tutorial</a> for how to get up and running (or <a href="https://element.io/server-registration">Element Server Suite</a> provides packages from the Element team)</p>
<p>Now, implementing Sliding Sync in matrix-rust-sdk has been a bit of a journey.  Since we <a href="https://archive.fosdem.org/2023/schedule/event/matrix_clients_as_good_as_youd_expect/">showed off</a> the very first implementation at FOSDEM, two big problems came to light.  For a bit of context: the original design of Sliding Sync was heavily inspired by Discord’s architecture - where the server calculates an ordered list of large numbers of items (your room list, in Matrix’s case); the client says which window into the list it’s currently displaying; and the server sends updates to the client as the view changes.  The user then scrolls around that list, sliding the window up and down, and the server sends the appropriate updates - hence the name Sliding Sync.</p>
<p>Sliding Sync was originally driven by our work on <a href="https://github.com/matrix-org/lb">Low Bandwidth Matrix</a> - as it makes no sense to have a fancy line protocol which can run over a 2400 baud modem… if the first thing the app tries to do is download a 100MB Sync v2 initial-sync response, or for that matter a 10MB incremental-sync response after having been offline for a few days (10MB takes 9 hours to shift over a 2400 baud modem, for those who missed out on the 80s).  Instead, you clearly only want to send the absolute essentials to the client, no matter how big their account is, and that’s what Sliding Sync does.</p>
<p>The first minor flaw in the plan, however, is that the server doesn’t necessarily have all the data it needs to order the room list.  Room ordering depends on what the most recent visible events are in a room, and if the room’s end-to-end encrypted, the server has no way of knowing which events are going to be visible for a given client or not.  It also doesn’t know which rooms have encrypted mentions inside them, and we <a href="https://github.com/matrix-org/matrix-spec-proposals/pull/3952#discussion_r1112203279">don’t want to leak mention metadata</a> to the server, or design out keyword mentions.  So, MSC3575 proposed some complicated contortions to let the client tweak the order client-side based on its superior knowledge of the ordering (given most clients would need to sync all the encrypted rooms anyway, in order to index them and search for keyword notifications etc).  Meanwhile, the order might be ‘good enough’ even without those tweaks.</p>
<p>The second minor flaw in the plan was that having implemented Sliding Sync in Element X, it turns out that the user experience on mobile of incrementally loading in room list entries from the server as the user scrolls around the list is simply not good enough, especially on bad connectivity - and the last thing we want to do is to design out support for bad connectivity in Matrix.  Users have been trained on mobile to expect to be able to swipe rapidly through infinite-scrolling lists of tens of thousands of photos in their photo gallery, or tens of thousands of emails in their mail client, without ever seeing a single placeholder, even for a frame.  So if the network roundtrip time to your server is even 100ms, and Sliding Sync is operating infinitely quickly, you’re still going to end up showing a placeholders for a few frames (6 frames, at 60fps, to be precise) if the user starts scrolling rapidly through their room list.  And empirically that doesn’t look great - the 2007-vintage <a href="https://www.amazon.co.uk/Creative-Selection-Ken-Kocienda/dp/1250194466">iOS team</a> have a lot to answer for in terms of setting user expectations!</p>
<p>So, the obvious way to solve both of these problems is simply to pull in more data in the background, to anticipate the user scrolling around.  In fact, it turns out we need to do that anyway, and indeed pull in <em>all</em> the room data so that room-search is instantly responsive; waiting 100ms or more to talk to the server whenever the user tries to search their roomlist is no fun at all, and it transpires that many users navigate their roomlist entirely by search rather than scrolling.  As a result, the sliding sync implementation in matrix-rust-sdk has ended up maintaining an ‘all rooms’ list, which starts off syncing the roomlist details for the most recent N rooms, and then in the background expands to sync all the rest.  At which point we’re not really sliding a window around any more: instead it’s more of a QoSed incremental sync.</p>
<p>So, to cut a long story short: while the current Sliding Sync implementation in matrix-rust-sdk and Element X empirically works very well, it’s ended up being a bit too complicated and we expect some pretty significant simplifications in the near future based on the best practices figured out with clients using it.  Watch this space for updates, although it’s likely that the current form of MSC3575 will prevail in some respect in order to support low-bandwidth environments where roomlist ordering and roomsearch latency is less important than preserving bandwidth.  Critically, we want to figure this out before we encourage folks to implement native server implementations - so for now, we’ll be keeping using the sliding-sync proxy as a way to rapidly experiment with the API as it evolves.</p>
<h3 id="native-matrix-group-voip">Native Matrix Group VoIP</h3>
<p>Another pillar of Matrix 2.0 is that we finally have native Matrix Group VoIP calling (<a href="https://github.com/matrix-org/matrix-spec-proposals/blob/matthew/group-voip/proposals/3401-group-voip.md">MSC3401</a>)!  Much like Sliding Sync has been developed using Element X as a testbed, <a href="https://call.element.io/">Element Call</a> has been the guinea pig for getting fully end-to-end-encrypted, scalable group voice/video calling implemented on top of Matrix, building on top of matrix-js-sdk.  And as of today, Element Call finally has it working, complete with end-to-end encryption (and integrated in Element X, for that matter)!</p>
<p><img src="https://matrix.org/blog/img/20230921-element-call.png" alt=""></p>
<p>Much like Sliding Sync, this has also been a bit of a journey.  The <a href="https://element.io/blog/introducing-native-matrix-voip-with-element-call/">original</a> implementations of Element Call strictly followed MSC3401, using full mesh conferencing to effectively have every participant place a call to every other participant - thus decentralising the conference and avoiding the need for a conferencing ‘focus’ server… but limiting the conference to 7 or 8 participants given all the duplication of the sent video required.  In Element Call <a href="https://element.io/blog/element-call-beta-2-encryption-spatial-audio-walkie-talkie-mode-and-more/">Beta 2</a>, end-to-end encryption was enabled; easy, given it’s just a set of 1:1 calls.</p>
<p>Then the real adventure began: to implement a Selective Forwarding Unit (SFU) which can be used to scale up to hundreds of users - or beyond. The unexpected first move came from Sean DuBois, project lead of the awesome <a href="https://pion.ly/">Pion</a> WebRTC stack for Golang - who wrote a proof-of-concept called sfu-to-sfu to demonstrate the viability of decentralised heterogenous cascading SFUs, as detailed in <a href="https://github.com/matrix-org/matrix-spec-proposals/blob/SimonBrandner/msc/sfu/proposals/3898-sfu.md">MSC3898</a>. This would not only let calls on a single focus scale beyond hundreds of users, but also share the conferencing out across all the participating foci, providing the world’s first heterogeneous decentralised video conferencing.  Element took the sfu-to-sfu implementation, hooked it up to Element Call on a branch, and renamed it as <a href="https://github.com/matrix-org/waterfall">waterfall</a>.</p>
<p>However, when Sean first contributed sfu-to-sfu, he mentioned to us that if Matrix is serious about SFUs, we should take a look at <a href="https://livekit.io/">LiveKit</a> - an open source startup not dissimilar to Element who were busy building best-in-class SFUs on top of Pion. And while waterfall worked well as a proof of concept, it became increasingly obvious that there’s a lot of work to be done around tuning congestion control, error correction, implementing end-to-end encryption etc which the LiveKit team had already spent years doing.  So, Element reached out to the LiveKit team, and started experimenting with what it might take to implement a Matrix-capable SFU on top of the LiveKit engine.</p>
<p>The end result was Element Call <a href="https://element.io/blog/element-call-beta-3/">Beta 3</a>, which is an interesting hybrid between MSC3401 and LiveKit’s existing signalling: the high-level signalling of the call (its existence, membership, duration etc) is advertised by Matrix - but the actual WebRTC signalling is handled by LiveKit, providing support for hundreds of users per call.</p>
<p>Finally, today marks the release of Element Call <a href="https://element.io/blog/element-x-ignition/#native-matrix-video-conferencing-with-element-call">Beta 4</a>, which adds back end-to-end encryption via the LiveKit SFU (currently by using a shared static secret, but in the near future will support full Matrix-negotiated end-to-end encryption with sender keys) - and also includes a complete visual refresh.  The next steps here include bringing back support for full mesh as well as SFU, for environments without an SFU, and updating all the MSCs to recognise the hybrid signalling model that reality has converged on when using LiveKit.  Meanwhile, head over to <a href="https://call.element.io/">https://call.element.io</a> to give it a go, or read more about it in the <a href="https://element.io/blog/element-x-ignition/">Element X Ignition blog post</a>!</p>
<h3 id="native-open-id-connect">Native Open ID Connect</h3>
<p>Finally, last but not least, we’re proud to announce that the project to replace Matrix’s venerable existing authentication APIs with industry-standard Open ID Connect in Matrix 2.0 has taken a huge leap forwards today, with <a href="https://matrix-org.github.io/matrix-authentication-service">matrix-authentication-service</a> now being available to add Native OIDC support to Synapse, as well as Element X now implementing account registration, login and management via Native OIDC (with legacy support only for login/logout).</p>
<p>This is a critical step forwards in improving the security and maintainability for Matrix’s authentication, and you can read all about it in this <a href="https://matrix.org/blog/2023/09/better-auth/">dedicated post</a>, explaining the rationale for adopting OpenID Connect for all forms of authentication throughout Matrix, and what you need to know about the transition.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There has been an <strong>enormous</strong> amount of work that has gone into Matrix 2.0 so far - whether that’s implementing sliding sync in matrix-rust-sdk and sliding-sync proxy, matrix-authentication-service and all the native OIDC infrastructure on servers and clients, the entirety of Element Call and its underpinning matrix-js-sdk and SFU work, or indeed Faster Joins in Synapse, which shipped back in <a href="https://matrix.org/blog/2023/01/31/synapse-1-76-released/">Jan</a>.</p>
<p>It’s been a pretty stressful sprint to pull it all together, and huge thanks go to everyone who’s contributed - both from the team at Element, but also contributors to other projects like matrix-rust-sdk who have got caught in the crossfire :)  It’s also been amazing seeing the level of support, high quality testing and excellent feedback from the wider community as folks have got excited about the promise of Matrix 2.0.</p>
<p>On the Foundation side, we’d like to thank the <a href="https://matrix.org/blog/2023/06/membership-program/">Members</a> whose financial support has been critical in providing bandwidth to enable the progress on Matrix 2.0 - and for those who want to help accelerate Matrix, especially those commercially building on top of Matrix, please consider <a href="https://matrix.org/membership/">joining the Foundation</a> as a member!  Also, in case you missed it, we’re super excited to <a href="https://matrix.org/blog/2023/09/introducing-josh-simmons-managing-director/">welcome Josh Simmons as Managing Director</a> for the Foundation - focusing on running the Foundation membership programme and generally ensuring the growth of the Foundation funding for the benefit of the whole Matrix community. Matthew and Amandine continue to lead the overall project (alongside their day jobs at Element), with the support of the other three independent Guardians - but Josh is working full time exclusively on running the non-profit foundation and gathering funds to support Matrix.</p>
<p>Talking of funding, we should mention that we’ve had to pause work in other places due to lack of Matrix funding - especially while focusing on successfully shipping Matrix 2.0. Major next-generation projects including <a href="https://thirdroom.io/">Third Room</a>, <a href="https://arewep2pyet.com/">P2P Matrix</a>, and <a href="https://matrix.org/blog/2021/06/10/low-bandwidth-matrix-an-implementation-guide/">Low Bandwidth Matrix</a> have all been paused unless there’s a major shift in circumstances - so, if you have money and you’re interested in a world where the more experimental next-generation Matrix projects progress with folks working on them as their day job, please <a href="https://matrix.org/membership/">get in touch</a> with the Foundation.</p>
<h2 id="what-s-next">What’s next?</h2>
<p>While this is the first usable release of Matrix 2.0 implementations, there’s loads of work still to be done - obvious work on Matrix 2.0 includes:</p>
<ul>
<li>Getting Native OIDC enabled on matrix.org, and providing migration tools to Native OIDC for existing homeservers in general</li>
<li>Reworking Sliding Sync based on the lessons learned implementing it in matrix-rust-sdk</li>
<li>Actually getting the Matrix 2.0 MSCs stabilised and matured to the point they can be approved and merged into the spec</li>
<li>Adding encrypted backups to matrix-rust-sdk</li>
<li>Reintroducing full-mesh support for Native Matrix Group VoIP calling</li>
<li>Having a big Matrix 2.0 launch party once the spec lands!</li>
</ul>
<p>Outside of Matrix 2.0 work, other big items on the horizon include:</p>
<ul>
<li>Adding Rust matrix-sdk-crypto to matrix-js-sdk, at which point all the official Matrix.org client SDKs will (at last!) be using the same stable performant E2EE implementation</li>
<li>Continuing to contribute Matrix input to the MIMI working group in IETF for Digital Markets Act interoperability</li>
<li>Working on <a href="https://arewemlsyet.com/">MLS</a> for next-generation E2EE</li>
<li>Next generation moderation tooling and capabilities</li>
<li>Account Portability and Multihomed accounts</li>
<li>…and much much more.</li>
</ul>
<p>So: welcome to our brave new Matrix 2.0 world. We hope you’re excited about it as we are - and thanks to everyone for continuing to use Matrix and build on it.  Here’s to the beginning of a whole new era!</p>
<p>Matthew, Amandine and the whole Matrix team.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sunken temple and sanctuary from ancient Egypt (130 pts)]]></title>
            <link>https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/</link>
            <guid>37599312</guid>
            <pubDate>Thu, 21 Sep 2023 15:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/">https://www.franckgoddio.org/projects/sunken-civilizations/heracleion/</a>, See on <a href="https://news.ycombinator.com/item?id=37599312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									<p>With a unique survey-based approach that utilises the most sophisticated <a href="https://www.franckgoddio.org/franck-goddio/system-approach-technology/">technical equipment</a>, Franck Goddio and his team, in cooperation with the Egyptian Supreme Council of Antiquities, were able to locate, map and excavate parts of the city of Thonis-Heracleion, which lies 6.5 kilometres off today’s coastline. The city is located within an overall research area of 11 by 15 kilometres in the western part of Aboukir Bay at a depth of approx. 10 metres. Research started in 1996. It took years to map the entire area. First discoveries could be made in 2000. Franck Goddio has found important information on the ancient landmarks of Thonis-Heracleion, such as the <b>grand temple of Amun</b> and his son Khonsou (Herakles for the Greeks), the harbours that once controlled all trade into Egypt, and the daily life of its inhabitants. He has also solved a <a href="https://www.franckgoddio.org/fileadmin/pics/3_5_finds/documents/Franck_Goddio_Stele_Heracleion.pdf" title="Opens internal link in current window" target="_blank">historic enigma</a> that has puzzled Egyptologists over the years: the archaeological material has revealed that Heracleion and Thonis were in fact one and the same city with two names; Heracleion being the name of the city for the Greeks and Thonis for the Egyptians.</p>
									<h5>What the City looked like</h5>
									<p>The <a href="https://www.franckgoddio.org/finds/" title="Opens internal link in current window" target="_blank">objects</a> recovered from the excavations illustrate the cities’ beauty and glory, the magnificence of their grand temples and the abundance of historic evidence: <a href="https://www.franckgoddio.org/fileadmin/pics/3_5_finds/documents/Franck_Goddio_ColossalStatues.pdf" title="Opens internal link in current window" target="_blank">colossal statues</a>, inscriptions and architectural elements, jewellery and coins, ritual objects and ceramics - a civilization frozen in time.</p>
									<p>The quantity and quality of the archaeological material excavated from the site of Thonis-Heracleion show that this city had known a time of opulence and a peak in its occupation from the 6th to the 4th century BC. This is readily seen in the large quantity of coins and ceramics dated to this period. The port of Thonis-Heracleion had numerous large basins and functioned as a <b>hub of international trade</b>. The intense activity in the port fostered the city’s prosperity. More than <b>seven hundred discovered ancient anchors</b> of various forms and <b>79 wrecks</b> dating from the 6th to the 2nd century BC (further 40 wrecks could be detected but still require verification) are also an eloquent testimony to the intensity of maritime activity here.</p>
									<p>The city extended all around the temple and a network of canals in and around the city must have given it a lake dwelling appearance. On the islands and islets dwellings and secondary sanctuaries were located. Excavations here have revealed beautiful archaeological material such as <a href="https://www.franckgoddio.org/finds/#c2007">bronze statuettes</a>. On the north side of the temple to Herakles, a grand canal flowed through the city from east to west and connected the port basins with a lake to the west.</p>
									<div><p>Thonis-Heracleion was also the site of the <b>celebration of the Mysteries of Osiris</b>. This important ceremony was performed each year in honour of the rebirth of the god Osiris. Texts and figures in the Osirian chapels in the temple of Dendera and on the stele of the royal Decree of Canopus describe the details of the celebration of this vigil and re-awakening of the god. In his ceremonial boat Osiris was brought in procession from the city's great temple of Amun-Gereb to his shrine in <a href="https://www.franckgoddio.org/projects/sunken-civilizations/canopus/">Canopus</a>.</p><p> The underwater archaeological research in Thonis-Heracleion is ongoing until today. Franck Goddio estimates that only 5 percent of the city have yet been discovered.</p></div>
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Article reply “Godot is not the new Unity” from Juan Linietsky (BDFL of Godot) (253 pts)]]></title>
            <link>https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21</link>
            <guid>37598985</guid>
            <pubDate>Thu, 21 Sep 2023 15:21:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21">https://gist.github.com/reduz/cb05fe96079e46785f08a79ec3b0ef21</a>, See on <a href="https://news.ycombinator.com/item?id=37598985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-godot_binding_system_explained-md">
    <article itemprop="text"><p dir="auto">During the past days, this <a href="https://sampruden.github.io/posts/godot-is-not-the-new-unity/" rel="nofollow">great article</a> by Sam Pruden has been
making the rounds around the gamedev community. While the article provides an in-depth analysis, its a bit easy to miss the
point and exert the wrong conclusions from it. As such, and in many cases, users unfamiliar with Godot internals have used
it points such as following:</p>
<ul dir="auto">
<li>Godot C# support is inefficient</li>
<li>Godot API and binding system is designed around GDScript</li>
<li>Godot is not production ready</li>
</ul>
<p dir="auto">In this brief article, I will shed a bit more light about how the Godot binding system works and some detail on the Godot
architecture. This should hopefully help understand many of the technical decisions behind it.</p>
<h3 id="user-content-built-in-types" dir="auto"><a href="#built-in-types">Built-in Types</a></h3>
<p dir="auto">Compared to other game engines, Godot is designed with a relatively high level data model in mind. At the heart, it uses several
datatypes across the whole engine. These datatypes are:</p>
<ul dir="auto">
<li><strong>Nil</strong>: To indicate an empty value.</li>
<li><strong>Bool, Int64 and Float64</strong>: For scalar math.</li>
<li><strong>String</strong>: For String and Unicode handling.</li>
<li><strong>Vector2, Vector2i, Rect2, Rect2i, Transform2D</strong>: For 2D Vector math.</li>
<li><strong>Vector3, Vector4, Quaternion, AABB, Plane, Projection, Basis, Transform3D</strong>: For 3D Vector math.</li>
<li><strong>Color</strong>: For color space math.</li>
<li><strong>StringName</strong>: For fast processing of Unique IDs (internally a unique pointer).</li>
<li><strong>NodePath</strong>: For referencing paths between nodes in the Scene Tree.</li>
<li><strong>RID</strong>: Resource ID for referencing a resource inside a server.</li>
<li><strong>Object</strong>: An instance of a class.</li>
<li><strong>Callable</strong>: A generic function pointer.</li>
<li><strong>Signal</strong>: A signal (see Godot docs).</li>
<li><strong>Dictionary</strong>: A generic dictionary (can contain any of these datatypes as either key or value).</li>
<li><strong>Array</strong>: A generic array (can contain any of these datatypes).</li>
<li><strong>PackedByteArray, PackedInt32Array, PackedInt64Array, PackedFloatArray, PackedDoubleArray</strong>: Scalar packed arrays.</li>
<li><strong>PackedVector2Array, PackedVector3Array, PackedColorarray</strong>: Vector packed arrays.</li>
<li><strong>PackedStringArray</strong>: Packed string array.</li>
</ul>
<p dir="auto">Does this mean that anything you do in Godot has to use these datatypes? Absolutely not.
These datatypes have several roles in Godot:</p>
<ul dir="auto">
<li><strong>Storage</strong>: Any of these datatypes can be saved to disk and loaded back very efficiently.</li>
<li><strong>Transfer</strong>: These datatypes can be very efficiently marshalled and compressed for transfer over a network.</li>
<li><strong>Introspection</strong>: Objects in Godot can only expose their properties as any of those datatypes.</li>
<li><strong>Editing</strong>: When editing any object in Godot, it is done via any of these datatypes (of course, different editors can exist for the same datatype, depending on the context).</li>
<li><strong>Languge API</strong>: Godot exposes its API to all languages it binds via those datatypes.</li>
</ul>
<p dir="auto">Of course, if you are absolutely unfamliar to Godot, the first questions that come to mind are:</p>
<ul dir="auto">
<li>How do you expose more complex datatypes?</li>
<li>What about other datatypes such as int16?</li>
</ul>
<p dir="auto">In general, you can expose more complex datatypes via Object API, so this is not much of an issue. Additionally, modern processors all have at minimum 64 bit buses, so exposing anything other than 64 bit scalar types makes no sense.</p>
<p dir="auto">If you are unfamliar to Godot, I can totally understand the disbelief. But in truth, it works fine and it makes everything
far simpler at the time of developing the engine. This data model is one of the main reasons why Godot is such a tiny,
efficient and yet feature packed engine compared to the large mainstream mamooths. As you get more familiar with the source
code, you will start to see why.</p>
<h3 id="user-content-language-binding-system" dir="auto"><a href="#language-binding-system">Language Binding System</a></h3>
<p dir="auto">Now that we have our data model, Godot imposes a strict requirement that almost any function exposed to the engine API
must be done via those datatypes. Any function parameters, return types or properties exposed must be via them too.</p>
<p dir="auto">This makes the job of the binder much simpler. As such, Godot has what we call an universal binder. How does this binder work, then?</p>
<p dir="auto">Godot registers any C++ function to the binder like this:</p>
<div dir="auto"><pre>Vector3 <span>MyClass::my_function</span>(<span>const</span> Vector3&amp; p_argname) {
   <span><span>//</span>..//</span>
}

<span><span>//</span> Then, on a special function, Godot does:</span>

<span><span>//</span> Describe the method as having a name and the name of the argument, the pass the method pointer</span>
<span>ClassDB::bind_method</span>(D_METHOD(<span><span>"</span>my_function<span>"</span></span>,<span><span>"</span>my_argname<span>"</span></span>), &amp;MyClass::my_function);</pre></div>
<p dir="auto">Internally, <em>my_function</em> and <em>my_argument</em> are converted to a StringName (described above), so from now onwards they are
treated just as a unique pointer by the binding API. In fact, when compiling on release, the argument name is ignored by
the template and no code is generated, since it serves no purpose.</p>
<p dir="auto">So, what does <code>ClassDB::bind_method</code> do? If you want to dive into the depths of insanity and try to understand the
incredibly complex and optimized C++17 variadic templates black magic, feel free <a href="https://github.com/godotengine/godot/blob/master/core/object/method_bind.h">to go ahead</a>.</p>
<p dir="auto">But In short, it creates a static function like this, which Godot calls "ptrcall" form.:</p>
<div dir="auto"><pre><span><span>//</span> Not really done like this, but simplifying as much as possible so you get an idea:</span>

<span>static</span> <span>void</span> <span>my_function_ptrcall</span>(<span>void</span> *instance, <span>void</span> **arguments, <span>void</span> *ret_value) {
    MyClass *c = (MyClass*)instance;
    Vector3 *ret = (Vector3*)ret_value;
    *ret = c-&gt;<span>my_method</span>( *(Vector3*)arguments[<span>0</span>] );
}</pre></div>
<p dir="auto">This wrapper is basically as efficient as it can be. In fact, for critical functions, inline is forced into the class method, resulting in a C function pointer to the actual function code.</p>
<p dir="auto">Then Language API works by allowing the request of any engine function in "ptrcall" format. To call this format,
the language must:</p>
<ul dir="auto">
<li>Allocate a bit of stack (basically just adjusting the stack pointer of the CPU)</li>
<li>set a pointer to the arguments (which already exist in native form in this language 1:1, be it GodotCPP, C#, Rust, etc).</li>
<li>call.</li>
</ul>
<p dir="auto">And that's it. This is an incredibly efficient generic glue API that you can use to expose any language to Godot efficiently.</p>
<p dir="auto">So, as you can imagine, the C# API in Godot basically uses a C function pointer via unsafe API to call after assigning pointers
to native C# types. It is very, very efficient.</p>
<h3 id="user-content-godot-is-not-the-new-unity---the-anatomy-of-a-godot-api-call" dir="auto"><a href="#godot-is-not-the-new-unity---the-anatomy-of-a-godot-api-call">Godot is not the new Unity - The anatomy of a Godot API call</a></h3>
<p dir="auto">I want to insist that the article written by Sam Pruden is fantastic, but if you are not familiar with how Godot is intended to work under the hood it can be very misleading. I will proceed to explain a bit more in detail what is easy to misunderstand.</p>
<h4 id="user-content-only-a-pathological-use-case-is-shown-the-rest-of-the-api-is-fine" dir="auto"><a href="#only-a-pathological-use-case-is-shown-the-rest-of-the-api-is-fine">Only a pathological use case is shown, the rest of the API is fine.</a></h4>
<p dir="auto">The use case shown in the article, the ray_cast function, is a pathological one in the Godot API.
Cases like this are most likely less 0.01% of the API exposed by Godot. Why the author found this specific one,
I have no idea nor I will speculate, but I think it was just an unfortunate coincidence.</p>
<p dir="auto">The problem is that, at the C++ level, this function takes a struct pointer for performance. But at the language
binding API this is difficult to expose properly. This is very old code (dating to the opensourcing of Godot) and
a Dictionary was hacked-in to use temporarily until something better is found. Of course, other stuff was more prioritary and very few games need thousands of raycasts, so pretty much nobody complained. Still, there is a <a href="https://github.com/godotengine/godot-proposals/issues/7329" data-hovercard-type="issue" data-hovercard-url="/godotengine/godot-proposals/issues/7329/hovercard">recently open proposal</a> to discuss more efficient binding of these types of functions.</p>
<p dir="auto">Additionally, to add to how unfortunate this choice of function is, the Godot language binding system <em>does</em> support
struct pointers like this. GodotCPP and Rust bindings can use pointers to structs without any issue. The problem
is that C# support in Godot predates the extension system and it was not converted to it yet. Eventually, C# will be
moved to the universal extension system and this will allow the unifying of the default and .net editors, it is just
not the case yet, but its top in the list of priorities.</p>
<h4 id="user-content-the-workaround-is-even-more-pathological" dir="auto"><a href="#the-workaround-is-even-more-pathological">The workaround is even more pathological</a></h4>
<p dir="auto">Although this time, due to a limitation of C#. If you bind C++ to C#, you need to create a C# version of a C++ instance
as an adapter. This is not an unique problem to Godot, any other engine or application doing this will require the same.</p>
<p dir="auto">Why is it troublesome? because C# has a garbage collector and C++ does not. This forces the C++ instance to keep a link
to the C# instance to avoid it from being collected.</p>
<p dir="auto">Due to this, the C# binder must do extra work when calling Godot functions that take class instances. You can see
this code in Sam's article:</p>
<div dir="auto"><pre><span>public</span> <span><span>static</span></span> GodotObject <span>UnmanagedGetManaged</span><span>(</span><span>IntPtr</span> <span>unmanaged</span><span>)</span>
<span>{</span>
    <span>if</span> <span>(</span><span>unmanaged</span> <span>==</span> IntPtr<span>.</span>Zero<span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>IntPtr</span> <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_get_script_instance_managed</span><span>(</span>unmanaged<span>,</span> <span>out</span> <span>var</span> r_has_cs_script_instance<span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span> <span>return</span> <span>(</span>GodotObject<span>)</span>GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target<span>;</span>
    <span>if</span> <span>(</span>r_has_cs_script_instance<span>.</span><span>ToBool</span><span>(</span><span>)</span><span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_get_instance_binding_managed</span><span>(</span>unmanaged<span>)</span><span>;</span>
    <span>object</span> <span>obj</span> <span>=</span> <span>(</span><span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span> <span>?</span> GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target <span>:</span> <span>null</span><span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>obj</span> <span>!=</span> <span>null</span><span>)</span> <span>return</span> <span>(</span><span>GodotObject</span><span>)</span><span>obj</span><span>;</span>

    <span>intPtr</span> <span>=</span> NativeFuncs<span>.</span><span>godotsharp_internal_unmanaged_instance_binding_create_managed</span><span>(</span>unmanaged<span>,</span> intPtr<span>)</span><span>;</span>
    <span>if</span> <span>(</span><span>!</span><span>(</span><span>intPtr</span> <span>!=</span> IntPtr<span>.</span>Zero<span>)</span><span>)</span> <span>return</span> <span>null</span><span>;</span>

    <span>return</span> <span>(</span>GodotObject<span>)</span>GCHandle<span>.</span><span>FromIntPtr</span><span>(</span>intPtr<span>)</span><span>.</span>Target<span>;</span>
<span>}</span></pre></div>
<p dir="auto">While very efficient, it's still not ideal for hot paths so the Godot API exposed is considerate and does not expose anything critical this way. The workaround used, however, is quite complex and hits this path due to not using the actual function
intended for it.</p>
<h4 id="user-content-the-question-of-cherry-picking" dir="auto"><a href="#the-question-of-cherry-picking">The question of cherry picking</a></h4>
<p dir="auto">I firmly believe the author did not cherry pick this API on purpose. In fact, he himself writes that he checked other places of
API usages and did not find anything with this level of pathology either.</p>
<p dir="auto">However, he mentions:</p>
<pre><code>Let’s also remember that Dictionary is only part of the problem. If we look a little wider for things returning 
Godot.Collections.Array&lt;T&gt; (remember: heap allocated, contents as Variant) we find lots from physics, 
mesh &amp; geometry manipulation, navigation, tilemaps, rendering, and more.
</code></pre>
<p dir="auto">From my side and contributors side, none of those usages are hot paths or pathological. Remember that, as I mentioned above,
Godot uses the Godot types mainly for serialization and API communication. While it is true that they do heap allocation,
this only happens once when the data is created.</p>
<p dir="auto">I think what may have confused Sam and a few others in this area (which is normal if you are not familiar with the Godot codebase) is that Godot containers don't work like STL containers. Because they are used mainly to pass data around, they are
allocated once and then kept via reference counting.</p>
<p dir="auto">This means, the function that reads your mesh data from disk is the only one doing the allocation, then this pointer gets
passed through many layers via reference counting until arrives Vulkan and is uploaded to the GPU. Zero copies happen along
the way.</p>
<p dir="auto">Likewise, when these containers are exposed to C# via the Godot collections, they are also reference counted internally.
If you create one of those arrays to pass the the Godot API, the allocation only happens <em>once</em>. Then no further copies happen
and the data arrives intact to the consumer.</p>
<p dir="auto">Of course, intenally, Godot uses far more optimized containers that are not directly exposed to the binder API.</p>
<h4 id="user-content-misleading-conclusion" dir="auto"><a href="#misleading-conclusion">Misleading conclusion</a></h4>
<p dir="auto">The article concludes like this:</p>
<pre><code>Godot has made a philosophical decision to be slow. The only practical way to interact with the engine is via this binding layer, and its core design prevents it from ever being fast. No amount of optimising the implementation of Dictionary or speeding up the physics engine is going to get around the fact we’re passing large heap allocated values around when we should be dealing with tiny structs. While C# and GDScript APIs remain synchronised, this will always hold the engine back.
</code></pre>
<p dir="auto">As you have read in the above points, the binding layer is absolutely not slow. What can be slow is an extremely limited amount of use cases that can be pathological. For those cases, a dedicated solution is found. This is a general <a href="https://docs.godotengine.org/en/stable/contributing/development/best_practices_for_engine_contributors.html#to-each-problem-its-own-solution" rel="nofollow">philosophy</a> behind Godot development that helps keep the codebase small, tidy, maintainable and easy to understand.</p>
<p dir="auto">In other words, this principle:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/6265307/269689859-d3bb5bec-1473-4803-a7d4-9ebc33736d48.png"><img src="https://user-images.githubusercontent.com/6265307/269689859-d3bb5bec-1473-4803-a7d4-9ebc33736d48.png" alt="image"></a></p>
<p dir="auto">The current binder serves its purpose and works well and efficiently for over 99.99% of use cases. For the exceptional ones, as mentioned before, the extension API supports structs already (which you can see here in this excerpt of the extension api dump):</p>
<pre><code>		{
			"name": "PhysicsServer2DExtensionRayResult",
			"format": "Vector2 position;Vector2 normal;RID rid;ObjectID collider_id;Object *collider;int shape"
		},
		{
			"name": "PhysicsServer2DExtensionShapeRestInfo",
			"format": "Vector2 point;Vector2 normal;RID rid;ObjectID collider_id;int shape;Vector2 linear_velocity"
		},
		{
			"name": "PhysicsServer2DExtensionShapeResult",
			"format": "RID rid;ObjectID collider_id;Object *collider;int shape"
		},
		{
			"name": "PhysicsServer3DExtensionMotionCollision",
			"format": "Vector3 position;Vector3 normal;Vector3 collider_velocity;Vector3 collider_angular_velocity;real_t depth;int local_shape;ObjectID collider_id;RID collider;int collider_shape"
		},
		{
			"name": "PhysicsServer3DExtensionMotionResult",
			"format": "Vector3 travel;Vector3 remainder;real_t collision_depth;real_t collision_safe_fraction;real_t collision_unsafe_fraction;PhysicsServer3DExtensionMotionCollision collisions[32];int collision_count"
		},
</code></pre>
<p dir="auto">So, ultimately, I believe that the conclusion that "Godot is slow by design" is a bit rushed. What is currently missing is the move of the C# language to the GDExtension system in order to be able to take advantage of these. This is currently a work in progress.</p>
<h3 id="user-content-to-sum-up" dir="auto"><a href="#to-sum-up">To sum up</a></h3>
<p dir="auto">I hope that this short article is used to dispell a few misconceptions that unintentionally arised from Sam's excellent article:</p>
<ul dir="auto">
<li><strong>Godot C# API is inefficient:</strong> This is absolutely not the case, but very few pathological cases remain to be solved and were already being in discussion before last week. In practice, very very few games may run into them and, by next year, hopefully none.</li>
<li><strong>Godot API is designed around GDScript:</strong> This is also not true. In fact, until Godot 4.1, typed GDScript did calls via "ptrcall" syntax, and the argument encoding was a bottleneck. As a result, we created a <a href="https://github.com/godotengine/godot/pull/79893" data-hovercard-type="pull_request" data-hovercard-url="/godotengine/godot/pull/79893/hovercard">special path</a> for GDScript to call more efficiently.</li>
</ul>
<p dir="auto">Thanks for reading and remember that Godot is not commercial software developed behind closed doors. All of us who make it are available online in the same communities as you. If you have any doubt, feel free to ask us directly.</p>
<p dir="auto"><strong>Bonus:</strong> As a side note, and contrary to popular belief, the Godot data model was not created for GDScript. Originaly, the engine used other languages such as Lua or Squirrel, with several published games while an in-house engine. GDScript was developed afterwards.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We're the Researchers who looked into the privacy of 25 of the top car brands (104 pts)]]></title>
            <link>https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/</link>
            <guid>37598845</guid>
            <pubDate>Thu, 21 Sep 2023 15:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/">https://old.reddit.com/r/IAmA/comments/16oi17v/were_the_researchers_who_looked_into_the_privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=37598845">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>UPDATE: Thank you for joining us and for your thoughtful questions! To learn more, you can visit <a href="https://www.privacynotincluded.org/">www.privacynotincluded.org</a> and read our full reviews. You can also get smarter about your online life with regular <a href="https://foundation.mozilla.org/en/newsletter/">newsletters</a> from Mozilla and remember to sign our <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/car-companies-stop-your-huge-data-collection-programs-en/">petition</a> to help us demand change!</p>

<p>To learn more about the data your car might be collecting, access your free Vehicle Privacy Report from Privacy4Cars here: <a href="https://vehicleprivacyreport.com/">https://vehicleprivacyreport.com</a>.   </p>

<p>Hi, we’re Jen Caltrider, Misha Rykov and Zoe MacDonald- lead Researchers of the <a href="https://foundation.mozilla.org/en/privacynotincluded/">*Privacy Not Included Guide</a> from Mozilla! We're also joined by Andrea from Privacy4Cars,a privacy-tech company focused on solving privacy challenges posed by vehicle data, and we’re all here to answer your burning questions about our recent Cars + Privacy report.</p>

<p><a href="https://twitter.com/mozilla/status/1704602084291613032?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet">Here's our proof.</a></p>

<p>We’ve reviewed a lot of product privacy policies over the years, but the car category is the worst for privacy that we have ever reviewed. All 25 of of the brands we researched failed our review and earned our *Privacy Not Included label; a sad first.Here's a summary of what we found:</p>

<ul>
<li>They collect too much personal data (all of them) - On top of collecting information regarding your in-car app usage and connected services, they can also collect super intimate information about you -- from your medical information, your genetic information, to your “sex life”</li>
<li>Most (84%) share or sell your data, and some (56%) also say they can share your information with the government or law enforcement in response to a “request.”</li>
<li>Most (92%) give drivers little to no control over their personal data - All but two of the 25 car brands we reviewed earned our “ding” for data control</li>
<li>We couldn’t confirm whether any of them meet our Minimum Security Standards</li>
</ul>

<p>Learn more about our findings and read the full report <a href="https://foundation.mozilla.org/en/privacynotincluded/articles/its-official-cars-are-the-worst-product-category-we-have-ever-reviewed-for-privacy/">here</a>.</p>

<p>Also! Check out Privacy4Cars' Vehicle <a href="https://vehicleprivacyreport.com/">Privacy Report</a> to know about and take actions for your vehicle.</p>

<p>Ask us anything about our guide, research or anything else!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The SEC cracks down on greenwashing (160 pts)]]></title>
            <link>https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing</link>
            <guid>37598329</guid>
            <pubDate>Thu, 21 Sep 2023 14:40:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing">https://www.semafor.com/article/09/21/2023/the-sec-cracks-down-on-greenwashing</a>, See on <a href="https://news.ycombinator.com/item?id=37598329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img width="30" height="30" src="https://img.semafor.com/eb687b94b93248efbff9e65b02568febf0405a45-2000x2127.png" alt="Jeronimo Gonzalez"><span>/</span></p><p><span>The U.S. Securities and Exchange Commission, the country’s top financial regulator, adopted a new rule to crack down on investment fund “greenwashing.”</span> Funds will be required to ensure that 80% of their portfolio match the asset advertised by the fund’s name. Since 2021, the SEC has prosecuted funds that bill themselves as investing exclusively in securities that rank highly in environmental, social, and governance measurements but which rather cast a much wider net with their investments. “<a href="https://www.ft.com/content/c626c311-7699-43b1-a98d-9740e06efc85" rel="no-referrer">It is truth in advertising,</a>” Gary Gensler, the SEC chair, said.</p></div><div><div><p><strong>The move was largely welcomed by activists and environmentalists, including some who had long considered the lax restrictions deceptive or even predatory of retail investors. </strong>“These rules will help cut down on greenwashing and misleading marketing so that millions of U.S. investors ensure … their money is being invested in line with their interests and their values,” a strategist at the Sierra Club, an environmental advocacy organization, said. “The SEC’s action today is a vital step,” the Environmental Defense Fund wrote.</p></div><div><figure><img width="800" height="607" src="https://img.semafor.com/aefe84b7e5a2af5f8ac552065f61a3c91ac02a03-1106x840.png?w=1600&amp;q=75&amp;auto=format" alt="" loading="lazy"></figure><p><strong>Despite a market downturn in 2022</strong> — during which traditional funds suffered billions of dollars in outflows — investors continued to flock to ESG funds, pushing their assets under management to a record $2.8 trillion last year. Demand has been driven largely by Europe<span>• <!-- -->1<!-- --> </span>, which accounted for 89% of sustainable funds’ assets. After years of out-performing traditional funds, however, sustainable funds’ returns fell below those of traditional ones last year, according to Morgan Stanley research.</p></div><div><p><strong>The EU is also cracking down on greenwashing.</strong> From 2026, products that can’t back up the accuracy of products marketed as being “climate neutral,” “eco,” or other sweeping environmental claims will be banned. The new rule, which climate NGOs have long agitated for, will make the EU the toughest region in the world in terms of green claims made to the public, the Financial Times reported. “Carbon neutral claims are greenwashing <span>• <!-- -->2<!-- --> </span>,” the head of a European consumer association said. “The truth is that these claims are scientifically incorrect and should never be used.”</p></div></div></div></div>]]></description>
        </item>
    </channel>
</rss>