<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 13 Dec 2023 04:00:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[LXD now re-licensed and under a CLA (105 pts)]]></title>
            <link>https://stgraber.org/2023/12/12/lxd-now-re-licensed-and-under-a-cla/</link>
            <guid>38618696</guid>
            <pubDate>Tue, 12 Dec 2023 21:18:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stgraber.org/2023/12/12/lxd-now-re-licensed-and-under-a-cla/">https://stgraber.org/2023/12/12/lxd-now-re-licensed-and-under-a-cla/</a>, See on <a href="https://news.ycombinator.com/item?id=38618696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			    
<figure><a href="https://stgraber.org/wp-content/uploads/2023/12/image-1.png"><img fetchpriority="high" decoding="async" width="1024" height="512" src="https://stgraber.org/wp-content/uploads/2023/12/image-1-1024x512.png" data-src="https://stgraber.org/wp-content/uploads/2023/12/image-1-1024x512.png" alt="" data-srcset="https://stgraber.org/wp-content/uploads/2023/12/image-1-1024x512.png 1024w, https://stgraber.org/wp-content/uploads/2023/12/image-1-300x150.png 300w, https://stgraber.org/wp-content/uploads/2023/12/image-1-768x384.png 768w, https://stgraber.org/wp-content/uploads/2023/12/image-1.png 1200w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://stgraber.org/wp-content/uploads/2023/12/image-1-1024x512.png 1024w, https://stgraber.org/wp-content/uploads/2023/12/image-1-300x150.png 300w, https://stgraber.org/wp-content/uploads/2023/12/image-1-768x384.png 768w, https://stgraber.org/wp-content/uploads/2023/12/image-1.png 1200w"></a></figure>



<h2>The facts</h2>



<p>As of earlier today, right before the release of <a href="https://discourse.ubuntu.com/t/lxd-5-20-has-been-released/40865" target="_blank" rel="noreferrer noopener">LXD 5.20</a>, Canonical made a couple of changes to LXD which are sure to have a serious impact to LXD users and downstream projects that integrate with LXD or provide solutions based on it.</p>



<p>The first is the re-licensing of LXD from the Apache2 license to the AGPLv3 license.<br>This happened in: <a href="https://github.com/canonical/lxd/pull/12663" target="_blank" rel="noreferrer noopener">https://github.com/canonical/lxd/pull/12663</a></p>



<p>The second is the addition of the Canonical CLA as requirement for all further contributions.<br>This happened in: <a href="https://github.com/canonical/lxd/pull/12665" target="_blank" rel="noreferrer noopener">https://github.com/canonical/lxd/pull/12665</a><a href="https://github.com/canonical/lxd/pull/12663">https://github.com/canonical/lxd/pull/12663</a></p>



<h2>Disclaimer</h2>



<p>What’s below is my personal analysis of the situation, it is not legal advice, anyone affected is very strongly encouraged to seek proper legal advice from their counsel.</p>



<h2>Real license of LXD</h2>



<p>Per the commit message performing the re-licensing, all further contributions will be under the AGPLv3 license and all contributions from Canonical employees have been re-licensed to AGPLv3.</p>



<p>However, Canonical does not own the copyright on any contribution from non-employees, such as the many changes they have imported from Incus over the past few months. Those therefore remain under the Apache2 license that they were contributed under.</p>



<p>As a result, Canonical cannot release LXD under the AGPLv3 license and likely never will be able to.<br>LXD is now under a weird mix of Apache2 and AGPLv3 with no clear metadata indicating what file or what part of each file is under one license or the other.</p>



<p>This is likely to make it very “fun” for anyone performing licensing reviews to evaluate LXD for adoption in their environment.</p>



<h2>Impact to LXD users</h2>



<p>For LXD users, other than potentially triggering corporate policies that ban the use of AGPLv3 software (more common than one may think), the impact should be minimal. It’s still the same LXD and it’s still open source software.</p>



<p>However, if you were altering LXD in any way, then you will need to familiarize yourself with the AGPLv3 license as unlike Apache2, it does require any changes be made available under the AGPLv3 even if you don’t expose your users to your modified binaries. This is the main design characteristics of the AGPLv3 license, it was meant to force those operating modified versions of open source as a hosted service to share their modifications.</p>



<h2>Impact to downstreams (consumer of LXD Go packages)</h2>



<p>Up until now, all the Go packages of LXD were under the Apache2 license, that was fitting quite well in the Go ecosystem where the Apache2, BSDs and MIT licenses are very popular.</p>



<p>Now with this change, you need to realize that you may start to include/bundle AGPLv3 code within your own project. This a copyleft license and so may require re-licensing of your own project to comply with it.</p>



<p>Again, this is quite the can of worms, with my usual recommendation being “stay away”, but if you must use any of LXD’s Go packages, I’d strongly recommend talking to a lawyer to fully understand your exposure to that new license.</p>



<h2>Impact on Incus</h2>



<p>Now for what obviously impacts me the most, what this is going to do to Incus.</p>



<p>As a brief reminder <a href="https://github.com/lxc/incus" data-type="link" data-id="https://github.com/lxc/incus" target="_blank" rel="noreferrer noopener">Incus</a> is a fork of LXD which was started in August 2023.<br>So far, it’s been tracking LXD changes, applying those that make sense and otherwise fixing bugs and making improvements of its own, as most forks do.</p>



<p>This change from Canonical is going to be causing two unfortunate side effects:</p>



<ul>
<li>Incus will no longer be including changes originating from LXD as that would require us to include AGPLv3 code into our codebase and so get us into the same mixed license mess as LXD now put itself. This is obviously unacceptable to us, we very much like licensing clarity and quite enjoy the Apache2 license.</li>



<li>LXD will similarly no longer be able to take changes from Incus, as those are going to remain under the Apache2 license and more importantly, will not have been released under the Canonical CLA.</li>
</ul>



<p>To enforce that second part, the tooling we’ve been using thus far to monitor LXD changes and automatically backport them to Incus will be used to detect any changes to LXD which originated from Incus. Unless the author gave express consent for them to be released under a different license and under the Canonical CLA, those changes should not be included in LXD.</p>



<p>Incus is also a consumer of the LXD Go API in the <code>lxd-to-incus</code> tool. Thankfully, we have no need for anything recent in there, so will simply be making sure that we never import code past the licensing change.</p>



<h2>Conclusion</h2>



<p>Overall, I’m very disappointed, although absolutely not surprised in seeing this change happen.<br>It’s certainly going to be quite annoying for Incus, and I suspect this is the whole point of it.</p>



<p>But it’s also a very odd move by Canonical as it puts LXD into a problematic grey area as far as its true license is concerned which will likely seriously hurt its adoption both by companies and distributions.</p>



<p>In any case, I’d urge anyone who has concerns about this change to reach out to their legal representation and maybe consider switching over to Incus where we will happily keep releasing our CLA-free Apache2 licensed fork of this once great project.</p>



<h2>Update <a rel="tag" href="https://stgraber.org/tag/1/">#1</a></h2>



<p>I’ve seen a number of people point out that Apache2 is compatible with the AGPLv3 and that’s certainly correct, however compatibility doesn’t mean that the code in question suddenly becomes AGPLv3, it means that it can be included in an AGPLv3 project. So I’d still expect there to be good tracking of the license of the individual files / code chunks so that someone can tell whether a particular piece of code is AGPLv3 or Apache2, this is currently not possible.</p>



<p>The <a href="https://github.com/canonical/lxd/blob/main/AUTHORS" target="_blank" rel="noreferrer noopener">code</a> mentions headers (probably SPDX) to be present whenever code isn’t AGPLv3, but no such headers were introduced at time of writing.</p>



<p>The announcement also very specifically spells out that past contributions are not being re-licensed and therefore remain under the Apache2 license, though again, there is currently no way to identify what contributions that is. So this still leads to LXD now being a mix of AGPLv3 and Apache2 with no way to figure out which is which.</p>



<p>All that’s known for sure is that all new contributions are to be under AGPLv3 and must be from copyright holders (author or employer) who has signed the Canonical CLA. Those two will preclude the inclusion of any Incus code in LXD moving forward.</p>
			    			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hasbro lays off nearly 20% of its workers (206 pts)]]></title>
            <link>https://www.polygon.com/23998290/hasbro-layoffs-before-christmas</link>
            <guid>38618111</guid>
            <pubDate>Tue, 12 Dec 2023 20:37:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polygon.com/23998290/hasbro-layoffs-before-christmas">https://www.polygon.com/23998290/hasbro-layoffs-before-christmas</a>, See on <a href="https://news.ycombinator.com/item?id=38618111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p id="ws0cUw">Game and toy maker <a href="https://www.polygon.com/23500171/hasbro-magic-overprinting-fireside-chat-cynthia-williams">Hasbro</a> announced it will cut roughly 1,100 jobs on Monday. When added to the 800 layoffs made earlier this year, the total amounts to just less than one-third of the company’s entire labor force. The news comes exactly two weeks ahead of Christmas, traditionally the biggest holiday of the year for game and toy sales.</p>
<p id="ftaXmm">“We entered 2023 expecting a year of change including significant updates to our leadership team, structure, and scope of operations,” said CEO Chris Cocks in a memo to his employees that was shared with Polygon. “We anticipated the first three quarters to be challenging, particularly in Toys, where the market is coming off historic, pandemic-driven highs. While we have made some important progress across our organization, the headwinds we saw through the first nine months of the year have continued into Holiday and are likely to persist into 2024.”</p>
<p id="70D5Ca">The news likely comes as no surprise to insiders, including investors, who have been tracking the shortfalls of the company’s toy portfolio for some time. Meanwhile, Hasbro subsidiary Wizards of the Coast, which publishes both <a href="https://www.polygon.com/deals/21294556/dnd-how-to-play-dungeons-dragons-5e-guide-spells-dice-character-sheets-dm">Dungeons &amp; Dragons</a> and <a href="https://www.polygon.com/mtg-magic-the-gathering"><em>Magic: The Gathering</em></a>, is experiencing all-time highs for both revenue earned and the number of players engaged in those brands. Activist hedge fund Alta Fox attempted to push the Rhode Island-based company to spin off Wizards in 2022, but the effort was defeated in <a href="https://www.reuters.com/business/retail-consumer/hasbro-defeats-board-challenge-activist-investor-alta-fox-2022-06-08/">a high-profile shareholder election in June 2022</a>. Now the 100-year-old company is left to rebuild in 2024.</p>
<p id="PCODex">Polygon reached out to the tabletop teams impacted by these layoffs, including <a href="https://www.polygon.com/23817191/betrayal-at-house-on-the-hill-santa-claus-holiday-expansion-release-date-price">Avalon Hill</a>, <em>Magic: The Gathering</em>, and Dungeons &amp; Dragons. One representative said that the company is “not sharing breakdowns on geography or teams out of respect for employees.” Impacted workers are expected to be contacted by the end of the business day Tuesday.</p>
<p id="9HLQqg">“Cost-cutting is not a strategy,” Cocks added. “We know this, and that’s why we’ll continue to grow and invest in several areas in 2024.”</p>
<p id="axNzev">One area of investment for Hasbro appears to be the video game sector. Its latest project, <a href="https://www.polygon.com/game-awards-tga/23992111/exodus-release-date-bioware-rpg-archetype"><em>Exodus</em></a>, made a big splash last week at <a href="https://www.polygon.com/game-awards-tga/23992648/the-game-awards-2023-biggest-game-announcements-trailers">The Game Awards</a>. Following the success of <a href="https://www.polygon.com/baldurs-gate-3-guides"><em>Baldur’s Gate 3</em></a>, which was produced under license by Larian Studios, Wizards’ own Archetype Entertainment appears to have a solid footing. Founded by veterans of BioWare, the studio behind <em>Star Wars: Knights of the Old Republic</em>, <em>Baldur’s Gate</em>, <em>Neverwinter Nights</em>, and <a href="https://www.polygon.com/2020/2/12/21135139/mass-effect-drew-karpyshyn-wizards-of-the-coast-archetype-entertainment-bioware">the Mass Effect trilogy</a>, the game will be released on PlayStation 5, Windows PC, and Xbox Series X. It currently has no release date.</p>
<p id="EUcigJ">Hasbro stock has fallen some 20% this year according to <a href="https://www.wsj.com/business/retail/hasbro-layoffs-toy-company-ed760682">The Wall Street Journal</a>, with today’s price down slightly on the news. Long-term investors, meanwhile, are expected to continue to earn dividends at a rate of 5.8% annually according to <a href="https://www.google.com/finance/quote/HAS:NASDAQ?sa=X&amp;ved=2ahUKEwjwkey8poqDAxX9IzQIHezaAAwQ3ecFegQILxAi">Google Finance</a>. Cocks earned a reported <a href="https://www.bizjournals.com/rhodeisland/news/2023/04/05/hasbro-ceo-chris-cocks.html">$9.4 million</a> last year — his first year as Hasbro’s CEO. At the new price point of $59.95, that’s roughly equivalent to 156,797 copies of the <a href="https://www.polygon.com/23730399/dnd-dungeons-dragons-revised-core-rulebook-preview-players-handbook-dmg"><em>Player’s Handbook</em></a>.</p>

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hardening cellular basebands in Android (145 pts)]]></title>
            <link>https://security.googleblog.com/2023/12/hardening-cellular-basebands-in-android.html</link>
            <guid>38616922</guid>
            <pubDate>Tue, 12 Dec 2023 19:16:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2023/12/hardening-cellular-basebands-in-android.html">https://security.googleblog.com/2023/12/hardening-cellular-basebands-in-android.html</a>, See on <a href="https://news.ycombinator.com/item?id=38616922">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI’s big rift is like a religious schism (162 pts)]]></title>
            <link>https://www.programmablemutter.com/p/the-singularity-is-nigh-republished</link>
            <guid>38616888</guid>
            <pubDate>Tue, 12 Dec 2023 19:13:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.programmablemutter.com/p/the-singularity-is-nigh-republished">https://www.programmablemutter.com/p/the-singularity-is-nigh-republished</a>, See on <a href="https://news.ycombinator.com/item?id=38616888">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp" width="1456" height="832" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:832,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;A conceptual and artistic representation of the Singularity, depicted in landscape mode, visualizing the hypothetical moment when technological growth becomes uncontrollable and irreversible, profoundly changing human civilization. This wide-format image should feature advanced futuristic technology, such as AI, robotics, and cybernetics, merging seamlessly with human elements to represent the blending of human and machine intelligence. The scene should be dynamic and transformative, with swirling lights, digital patterns, and abstract forms to symbolize the rapid advancement of technology. The overall ambiance should be awe-inspiring and slightly enigmatic, capturing the vast, profound impact of the Singularity on the future of humanity.&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="A conceptual and artistic representation of the Singularity, depicted in landscape mode, visualizing the hypothetical moment when technological growth becomes uncontrollable and irreversible, profoundly changing human civilization. This wide-format image should feature advanced futuristic technology, such as AI, robotics, and cybernetics, merging seamlessly with human elements to represent the blending of human and machine intelligence. The scene should be dynamic and transformative, with swirling lights, digital patterns, and abstract forms to symbolize the rapid advancement of technology. The overall ambiance should be awe-inspiring and slightly enigmatic, capturing the vast, profound impact of the Singularity on the future of humanity." title="A conceptual and artistic representation of the Singularity, depicted in landscape mode, visualizing the hypothetical moment when technological growth becomes uncontrollable and irreversible, profoundly changing human civilization. This wide-format image should feature advanced futuristic technology, such as AI, robotics, and cybernetics, merging seamlessly with human elements to represent the blending of human and machine intelligence. The scene should be dynamic and transformative, with swirling lights, digital patterns, and abstract forms to symbolize the rapid advancement of technology. The overall ambiance should be awe-inspiring and slightly enigmatic, capturing the vast, profound impact of the Singularity on the future of humanity." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730fc1ce-0e5e-4640-baa5-745f31c1d502_1792x1024.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>[NB - the below has just been published by </span><a href="https://www.economist.com/by-invitation/2023/12/12/ais-big-rift-is-like-a-religious-schism-says-henry-farrell" rel="">The Economist</a><span> under the title “AI’s big rift is like a religious schism, says Henry Farrell.” It is being republished here, for six months only, with The Economist’s permission. The picture above is what ChatGPT4 gives you when prompted for “an image of the singularity, landscape mode” - I’m using it, as much as anything else, to show </span><em>how generic and universal</em><span> the notion of the Singularity has become.]</span></p><p>Two centuries ago Henri de Saint-Simon, a French utopian, proposed a new religion, worshipping the godlike force of progress, with Isaac Newton as its chief saint. He believed that humanity’s sole uniting interest, “the progress of the sciences”, should be directed by the “elect of humanity”, a 21-member “Council of Newton”. Friedrich Hayek, a 20th-century economist, later gleefully described how this ludicrous “religion of the engineers” collapsed into a welter of feuding sects.</p><p>Today, the engineers of artificial intelligence (AI) are experiencing their own religious schism. One sect worships progress, canonising Hayek himself. The other is gripped by terror of godlike forces. Their battle has driven practical questions to the margins of debate.</p><p>Both cults are accidental by-products of science fiction. In 1993 Vernor Vinge drew on computer science and his fellow science-fiction writers to argue that ordinary human history was drawing to a close. We would surely create superhuman intelligence sometime within the next three decades, leading to a “Singularity”, in which AI would start feeding on itself. The future might be delightful or awful, depending on whether machines enhanced human intelligence or displaced it.</p><p>Some were optimistic. The futurist Ray Kurzweil wrote an enormous tome, “The Singularity is Near”, predicting a cusp in 2045. We humans would become immortal, spreading intelligence throughout the universe, and eventually merging into God. For all its statistics and exponentials, the book prophesied “the Rapture of the Nerds”, as one unkind critic called it. Its title really should have been “The Singularity is Nigh”.</p><p>Others feared the day of judgment. Eliezer Yudkowsky, a self-taught AI researcher, was deeply influenced by Mr Vinge’s ideas. He fathered Silicon Valley’s “rationalist” movement, which sought to improve human reasoning and stop AI destroying humankind.</p><p>Rationalists believed that Bayesian statistics and decision theory could de-bias human thinking and model the behaviour of godlike intelligences. They revelled in endless theoretical debates, like medieval Christian philosophers disputing the nature of angels, applying amateur game theory instead of Aristotelian logic. Sometimes their discussions were less erudite. Mr Yudkowsky popularised his ideas in a 660,000-word fan-fiction epic, “Harry Potter and the Methods of Rationality”.</p><p>Rationalists feared that superhuman AIs wouldn’t have our best interests at heart. One notorious thought experiment—a modern version of Pascal’s wager, dubbed “Roko’s basilisk”—claimed that logic dictated that future divine intelligences would torture anyone who had known that AI was possible and hadn’t devoted themselves to bringing it into existence. AIs might also use their awesome reasoning powers to escape any limits that humans imposed on them, creating an “x risk” (existential risk) to human survival.</p><p>Rationalism explains why AI pioneers became obsessed with x risk. Sam Altman, Elon Musk and others founded OpenAI, the creator of Chatgpt, as a non-profit so that it wouldn’t duck the dangers of machine intelligence. But the incentives shifted as the funding flooded in. Some OpenAI staffers feared that their employer cared more about the opportunities than the dangers and defected to found Anthropic, a rival AI firm. More recently, clashes over AI risk, money and power reportedly led to the fracture between Mr Altman and his board.</p><p>If rationalists are frustrated by Silicon Valley’s profit model, Silicon Valley is increasingly frustrated by rationalism. Marc Andreessen, the co-founder of Andreessen Horowitz, a venture-capital firm, fulminated in June that the extremist AI-risk “cult” was holding back an awesome AI-augmented future, in which humanity could reach for the stars.</p><p>This backlash is turning into its own religion of the engineers. Grimes, a musician and Silicon Valley icon, marvels that AI engineers are “designing the initial culture of the universe”. She calls for a “Council of Elrond” (this conclave a nod to “The Lord of the Rings”) comprising the “heads of key AI companies and others who understand it” to set AI policy. Grimes met Mr Musk, the father of her children, through a shared joke about Roko’s basilisk.</p><p>In October Mr Andreessen published his own “Techno-Optimist Manifesto” to wide acclaim from Silicon Valley entrepreneurs. In it, he takes aim at a decades-long “demoralisation campaign…against technology and life”, under various names including “sustainable development goals”, “social responsibility”, “trust and safety” and “tech ethics”. Efforts to decelerate AI “will cost human lives” and are thus tantamount to “murder”.</p><p>Mr Andreessen’s manifesto is a Nicene creed for the cult of progress: the words “we believe” appear no less than 113 times in the text. His list of the “patron saints” of techno-optimism begins with Based Beff Jezos, the social-media persona of a former Google engineer who claims to have founded “effective accelerationism”, a self-described “meta-religion” which puts its faith in the “technocapital Singularity”.</p><p>Our future is currently being built around Mr Vinge’s three-decades-old essay, a work that only Silicon Valley thinkers and science-fiction fans have read. Warring cults dispute whether engineers are as gods, or just unwitting Dr Frankensteins.</p><p>This schism is an attention-sucking black hole that makes its protagonists more likely to say and perhaps believe stupid things. Of course, many AI-risk people recognise that there are problems other than the Singularity, but it’s hard to resist its relentless gravitational pull. Before Mr Andreessen was fully dragged past the event horizon, he made more nuanced arguments about engineers’ humility and addressing the problems of AI as they arose.</p><p>But we need even more to listen to other people. Last month, at Rishi Sunak’s global AI-policy summit, Mr Musk pontificated about the need for an “off switch” for hostile AI. The main event was all about x risk and AI’s transformative promise, consigning other questions to a sideshow dubbed the “AI Fringe”.</p><p>At the same time, Rachel Coldicutt, a British tech thinker, was putting together a “Fringe of the Fringe”, where a much more diverse group of thinkers debated the topics that hadn’t made the main agenda: communities, transparency, power. They didn’t suggest a Council of the Elect. Instead, they proposed that we should “make AI work for eight billion people, not eight billionaires”. It might be nice to hear from some of those 8bn voices.■</p><p><em><span>Henry Farrell is a professor of international affairs and democracy at Johns Hopkins University, and co-author of “</span><a href="https://amzn.to/3PbIyqX" rel="">Underground Empire: How America Weaponized the World Economy”</a></em><a href="https://amzn.to/3PbIyqX" rel="">.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Will Enter the Public Domain in 2024? (283 pts)]]></title>
            <link>https://publicdomainreview.org/features/entering-the-public-domain/2024/</link>
            <guid>38616753</guid>
            <pubDate>Tue, 12 Dec 2023 19:03:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://publicdomainreview.org/features/entering-the-public-domain/2024/">https://publicdomainreview.org/features/entering-the-public-domain/2024/</a>, See on <a href="https://news.ycombinator.com/item?id=38616753">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At the start of each year, on January 1st, a new crop of works enter the public domain and become free to enjoy, share, and reuse for any purpose. Due to differing copyright laws around the world, there is no one single public domain — and here we focus on three of the most prominent. Newly entering the public domain in 2024 will be:</p><ul><li>works by <a href="https://en.wikipedia.org/wiki/2024_in_public_domain#Countries_with_life_+_70_years">people who died in 1953</a>, for countries with a copyright term of “life plus 70 years” (e.g. UK, Russia, most of EU and South America);</li><li>works by <a href="https://en.wikipedia.org/wiki/2024_in_public_domain#Countries_with_life_+_50_years">people who died in 1973</a>, for countries with a term of “life plus 50 years” (e.g. New Zealand, and most of Africa and Asia);</li><li><a href="https://en.wikipedia.org/wiki/1928_in_film">films</a> and <a href="https://en.wikipedia.org/wiki/1928_in_literature#New_books">books</a> (incl. artworks featured) published in 1928 for the United States.</li></ul><p>In our advent-style calendar below, find our top pick of what lies in store for 2024. Each day, as we move through December, we’ll open a new window to reveal our highlights! By public domain day on January 1st they will all be unveiled — look out for a special blogpost from us on that day. (And, of course, if you want to dive straight in and explore the vast swathe of new entrants for yourself, just visit the links above).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Like we were lesser humans': Gaza boys, men recall Israeli arrest, torture (499 pts)]]></title>
            <link>https://www.aljazeera.com/features/2023/12/12/like-we-were-lesser-humans-gaza-boys-men-recall-israeli-arrests-torture</link>
            <guid>38616550</guid>
            <pubDate>Tue, 12 Dec 2023 18:50:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aljazeera.com/features/2023/12/12/like-we-were-lesser-humans-gaza-boys-men-recall-israeli-arrests-torture">https://www.aljazeera.com/features/2023/12/12/like-we-were-lesser-humans-gaza-boys-men-recall-israeli-arrests-torture</a>, See on <a href="https://news.ycombinator.com/item?id=38616550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-live="polite" aria-atomic="true"><p><strong>Deir el-Balah, Gaza Strip –</strong> Inside one of the rooms of Al-Aqsa Martyrs Hospital, Mahmoud Zindah stays close to his father, Nader, the horrors of the past week etched on both of their faces. Their eyes are wide, darting around.</p>
<p>The 14-year-old and his father were among hundreds of Palestinians rounded up on December 5 by Israeli forces in the Shujayea area, east of Gaza City, who endured five days of torture and degradation before they were released – without any explanation.</p>
<p>“One of the soldiers said I looked like his nephew and that this nephew was killed in front of his grandmother who was taken hostage by Hamas and that the soldiers will slaughter us all,” Mahmoud says, his voice trembling.</p>
<p>Before their ordeal, the Zindah family was trapped in their home in the Zeitoun neighbourhood of Gaza City for two days, unable to leave as tanks advanced and artillery shelling got closer and closer. Those who dared to leave their homes for whatever vital errand were shot down in the streets by snipers.</p>
<p>On the third day, the family, who slept on the cold tile floor under mattresses to shield them from potential flying shrapnel, woke up to find the tanks on their street.</p>
<figure id="attachment_2549574" aria-describedby="caption-attachment-2549574"><img loading="lazy" src="https://www.aljazeera.com/wp-content/uploads/2023/12/1E3A0077-1702307776.jpg?w=770&amp;resize=770%2C513" alt="Mahmoud and Nader Zindah recall their experience of arrest and torture by Israeli forces " data-recalc-dims="1"><figcaption id="caption-attachment-2549574">Mahmoud and Nader Zindah recall their arrest and torture by Israeli forces [Abdelhakim Abu Riash/Al Jazeera]</figcaption></figure>
<p>“We heard the soldiers shouting and the tank tracks getting louder,” Nader, 40, says. “I felt like there was something wrong, so I went to the house behind me, which was farther from the street. Before I reached it I stopped in shock. The house was moving!</p>
<p>“Then I realised that the Israeli bulldozer was knocking its walls down” and soldiers were firing live ammunition as well, he adds.</p>
<p>Nader quickly tore some white sheets into small “flags” for each of his eight children to carry. They poked one out of their front door, as the adults shouted that there were people in the house. The bulldozer stopped, as did the shooting. But suddenly the home was full of Israeli soldiers.</p>
<p>“They made us empty out our bags on the floor and blocked us from picking up our money or our wives’ gold,” Nader recalls. “What little food we had, they also threw away. They took our money, IDs and phones.”</p>
<p>The soldiers divided the household: women and young children in one room and the men and teenage boys in another. Then they told Nader, Mahmoud, his brother-in-law and another male relative to strip, then pushed them outside.</p>
<p>“They rounded up at least 150 men from the surrounding homes and blindfolded and handcuffed us all in the street,” Nader explains.</p>
<figure id="attachment_2549568" aria-describedby="caption-attachment-2549568"><img loading="lazy" src="https://www.aljazeera.com/wp-content/uploads/2023/12/1E3A0100-2-1702307644.jpg?w=770&amp;resize=770%2C513" alt="Mohammed Odeh, 14, was separated from his family and taken with at least 150 other men and teenage boys to a rice warehouse by Israeli forces where he faced torture for several days " data-recalc-dims="1"><figcaption id="caption-attachment-2549568">Mohammed Odeh, 14, was separated from his family and taken with at least 150 other men and teenage boys to a rice warehouse by Israeli forces where he faced torture for several days [Abdelhakim Abu Riash/Al Jazeera]</figcaption></figure>
<p>When the soldiers forced the men onto the backs of some trucks, Nader made sure Mahmoud was on his lap, terrified of what they would do to his son if they were separated.</p>
<p>“I don’t want to lose my child, nor do I want my son to lose his father,” he says.</p>
<p>The men quickly realised that there were also women in the truck, which kept braking suddenly, sending the prisoners falling on top of each other.</p>
<p>“We were all blindfolded, so we couldn’t see each other, but we heard the women telling us to look out for them like we would for our own sisters,” Nader says. “There were also younger children with them.”</p>
<p>The truck stopped, and once again, the men and women were separated. The men and teenage boys were taken to a warehouse where they sat on a bare floor covered in scattered grains of rice. There they were beaten, interrogated and verbally abused. There was no sleep, and the grains of rice cut their skin as they sat there, undressed.</p>
<h2 id="starved-and-beaten-for-days">Starved and beaten for days</h2>
<p>Mohammed Odeh, 14, was taken from the same Wadi al-Arayes neighbourhood in Zeitoun as the Zindahs, where he and his family were stuck in their homes for five days, starving.</p>
<figure id="attachment_2549588" aria-describedby="caption-attachment-2549588"><img loading="lazy" src="https://www.aljazeera.com/wp-content/uploads/2023/12/1E3A0107-1702307940.jpg?w=770&amp;resize=770%2C513" alt="Israeli soldiers released about 10 men they had arrested last Tuesday " data-recalc-dims="1"><figcaption id="caption-attachment-2549588">Israeli soldiers released about 10 men they had arrested on December 5, 2023 [Abdelhakim Abu Riash/Al Jazeera]</figcaption></figure>
<p>Two of the neighbourhood boys who left to look for water were killed on the street by Israeli snipers. After the bulldozer knocked down the walls of several homes, the soldiers dragged the men and teenagers out, slapping, punching and hitting them with their guns.</p>
<p>“There was no reasoning with them,” Mohammed recalls. “They kept saying, ‘You are all Hamas.’ They wrote numbers on our arms. My number was 56.” When he stretches his arms out, the red marker is still visible on his skin.</p>
<p>“When they spoke to us in Hebrew and we wouldn’t understand, they’d beat us up,” he continues.</p>
<p>“They hit me in the back where my kidneys are and my legs. They took my family, and I don’t know where they are,” he says, his voice breaking.</p>
<p>Before they were forced inside the warehouse, Israeli female soldiers came and spat on the men, Mohammed recalls.</p>
<p>In the warehouse, it was common for groups of five soldiers to suddenly enter and beat one person while the others were forced to listen to his screams of pain. If any of the men and teenagers nodded off from exhaustion, the soldiers poured cold water on them.</p>
<p>“Their contempt for us was unnatural, like we were lesser beings,” Mohammed says.</p>
<figure id="attachment_2549558" aria-describedby="caption-attachment-2549558"><img loading="lazy" src="https://www.aljazeera.com/wp-content/uploads/2023/12/1E3A0175-2-1702307496.jpg?w=770&amp;resize=770%2C513" alt="One of the Palestinian men arrested and tortured for days by Israeli soldiers shows the number he was marked by, and his swollen hand from the handcuffs " data-recalc-dims="1"><figcaption id="caption-attachment-2549558">One of the Palestinian men arrested and tortured for days by Israeli soldiers shows the number he was marked with and his swollen hands from the handcuffs [Abdelhakim Abu Riash\Al Jazeera]</figcaption></figure>
<p>“Some people didn’t return from the torture sessions,” Nader says darkly. “We would hear their screams and then nothing.”</p>
<p>At one point, Mahmoud told his father that his wrists were bleeding from the handcuffs. A soldier overheard, asked where it hurt and then proceeded to press down on the spot. Nader tried to shield his son, and one of the soldiers tried to drag the teenager away. When Mahmoud resisted, he was kicked in the face. The mark is still visible.</p>
<p>“My dad kept shouting at them that I’m a child and threw himself on top of me,” he says. “I heard a soldier speaking in an American accent, and I told him in English that I’m just a kid that goes to school.” Their words fell on deaf ears.</p>
<p>Blindfolded and handcuffed the entire time, the men and boys endured hours of beatings.</p>
<p>“They cursed at us, spewing the most foul language,” says Nader, who suffered a particularly painful blow to his head. “Some of them spoke Arabic. Every time you tried to talk, asking to go use the bathroom or wanting a drink of water, they would come and beat us up, using the butts of their M16 rifles.”</p>
<p>The soldiers interrogated them and threatened to kill them all. They accused the Palestinians of stealing their army jeeps and raping Israeli women. When they asked Mahmoud where he was on October 7 and he answered that he was sleeping at home, the soldiers hit him, he says.</p>
<p>“They have this unbelievable racism. They really hate us,” Nader says. “This isn’t about Hamas. This is about wiping us all out. This is about a genocide, signed off by [US President] Biden.”</p>
<p>The men were given only a few drops of water and some scraps of bread to eat. Some were forced to relieve themselves on the spot while others were handed a foul-smelling bucket.</p>
<p>On the fifth day, Saturday, Nader, Mahmoud, and 10 other men were taken to Nitzarim, a former settlement south of Gaza City that had been turned into farmland after the 2005 Israeli disengagement. It is now an Israeli checkpoint just before Wadi Gaza and the men were released there and told to head south.</p>
<p>The group took off their blindfolds and let their eyes adjust to the light after days of darkness. They were exhausted and hungry and still did not have any clothes. After walking painfully for two hours, a group of Palestinians spotted them.</p>
<p>“They clothed us and gave us water,” Nader says. “An ambulance was called, and we arrived at Al-Aqsa Martyrs Hospital, where we were immediately given IV fluids.”</p>
<p>“I thought I didn’t have a chance of getting out alive,” he adds.</p>
<p>“It was hell on earth. It was like spending five years in that warehouse. I wouldn’t wish this on anyone.”</p>
<p><span><iframe loading="lazy" width="770" height="434" src="https://www.youtube.com/embed/EXfn-BCTBNc?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen=""></iframe></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zero downtime Postgres upgrades (144 pts)]]></title>
            <link>https://knock.app/blog/zero-downtime-postgres-upgrades</link>
            <guid>38616181</guid>
            <pubDate>Tue, 12 Dec 2023 18:22:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://knock.app/blog/zero-downtime-postgres-upgrades">https://knock.app/blog/zero-downtime-postgres-upgrades</a>, See on <a href="https://news.ycombinator.com/item?id=38616181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>👋&nbsp;We’re Knock. We provide a set of simple APIs developers use to introduce
notifications into their products, without needing to build and
maintain a notification system in-house.</strong></p>
<div><p><strong>Tl;dr:</strong> We recently upgraded from Postgres 11.9 to 15.3 with
zero downtime by using logical replication, a suite of support scripts,
and tools in Elixir &amp; Erlang’s BEAM virtual machine.</p><p>This post will go into far too much detail explaining how we did it,
and considerations you might need to make along the way if you try to do the same.</p><p>It is more of a manual than anything, and includes things we learned along the
way that we wish we’d known up front.</p></div>
<p>Knock relies on Postgres to power our notification workflow engine. From storing
workflow configurations and message templates, to <a href="https://knock.app/blog/how-we-use-postgres-ltrees">ingesting millions of logs</a>
and <a href="https://getoban.pro/">enqueuing background jobs</a>, Postgres sits at the heart of everything our systems do.
Our Postgres databases running on AWS RDS Aurora have been consistently reliable,
performant, and extensible. This foundation to Knock’s service lets us support
with confidence every customer that joins our platform.</p>
<p>Unlike SaaS software that can be constantly upgraded in the background with little notice,
upgrading relational databases like Postgres generally requires at least a reboot of the database.
In the case of major version upgrades, the database often needs to shut down completely for several minutes
in order to upgrade how data is stored and indexed on disk.</p>
<p><strong>The more data you have, the longer the upgrade will take.</strong></p>
<p>In Knock’s case, we have been running Postgres 11.9 since we started the company.
Although it has reliably served us at every step along the way,
<a href="https://docs.aws.amazon.com/AmazonRDS/latest/PostgreSQLReleaseNotes/postgresql-release-calendar.html#Release.Calendar">Postgres 11.9 is being retired by Amazon’s RDS service on February 29, 2024</a>.
Without taking action (i.e. arranging a long-term support contract with RDS),
teams that use Postgres 11.9 on AWS RDS will be forcibly upgraded at that point,
likely resulting in forced downtime.</p>
<p>No amount of downtime - scheduled or otherwise - is acceptable for a service like Knock.
Our customers rely on us to be online 24/7. Although no service can guarantee
perfect uptime, responsible developer teams work to proactively address service
issues before they happen.</p>
<p>We added this upgrade to our roadmap in June of this year, with the following constraints:</p>
<ol>
<li>Upgrade as many versions ahead as possible, skipping to the latest available version (at the time, Postgres 15.3 for Aurora).</li>
<li>Any downtime beyond 60 seconds was completely unacceptable, and ideally we would have zero system downtime.</li>
<li>The upgrade must happen well in advance of Amazon’s February deadline.</li>
<li>Minimize customer impact (e.g. zero API error responses).</li>
<li>Operationalize the process so that next time we need to upgrade the database, it is a well-established runbook.</li>
</ol>
<p>Each of our Postgres databases would need to run through this process, and going
from 11.9 to 15.3 would comprise <strong>four</strong> major version upgrades. If doing an
in-place upgrade for each major version would trigger downtime, doing <strong>four</strong>
in a row was out of the question.</p>
<p>In order to meet our requirements, we knew we’d have to get creative.</p>
<h2 id="preparing-for-any-postgres-upgrade">Preparing for any Postgres upgrade</h2>
<p>More than anything, teams seeking to upgrade Postgres in any way should focus on
<strong>de-risking</strong> the upgrade process as much as possible:</p>
<ol>
<li>
<p>Make a list of the risks involved in making the migration. For example:</p>
<ul>
<li>Unacceptably long downtime</li>
<li>Data loss</li>
<li>Changes in database performance for your application’s workload</li>
<li>Changes in vacuum frequency or behavior</li>
<li>Are there any replication slots that need to be migrated (this can be tricky - <a href="#a-note-about-moving-replication-slots">see below</a>)</li>
</ul>
</li>
<li>
<p>Figure out which risks are the most critical to the project, and which ones
might be the easiest to explore/rule out/fix in advance.</p>
<p>Sort the list so the risks with the biggest impact yet easiest to address are at the top.</p>
</li>
<li>
<p>As you develop solutions, consider your list of risks:</p>
<ul>
<li>Are there solutions that rule out risks completely?</li>
<li>Which solutions spread out the risk over time? (So we can more gradually
address each step of the migration without taking on too much risk at once.)</li>
</ul>
</li>
<li>
<p>As you work through the project, always revisit your list of risks,
and keep it up to date as you learn new things - including discovering new risks!</p>
<div><p><strong>Incrementally and continually de-risk projects like this until you are
confident in being able to deliver on your project goals.</strong></p></div>
</li>
</ol>
<p>To plan out our upgrade, we started with <a href="https://www.postgresql.org/docs/release/">Postgres’ release notes</a>
to get a sense of what was going to change between database versions.
This helped us identify more risks (e.g. changes in how Postgres’ vacuum works,
requirement to reindex the database when performing certain upgrades) while ruling out others.</p>
<p>As we moved through our planning process, we maintained this list of risks,
adding new concerns and updating old ones as we collected more information.
While working through the upgrade, we systematically addressed each concern
until we were confident we could deliver on our project goals without
risking our reliability.</p>
<h3 id="a-word-about-monitoring--metrics">A word about monitoring &amp; metrics</h3>
<p>Having thorough instrumentation (thanks DataDog!) to monitor the health of your
system and database makes it possible to monitor each step of the migration.</p>
<p>A few key metrics to watch:</p>
<ul>
<li>Max TXN ID to avoid <a href="https://www.crunchydata.com/blog/managing-transaction-id-wraparound-in-postgresql">transaction wraparound</a> - if this gets too high, your database can shut down and go into emergency maintenance mode</li>
<li>DB CPU Utilization</li>
<li>Waiting sessions on your writer instance</li>
<li>Query latency</li>
<li>API response latencies for your application</li>
</ul>
<p>At Knock, we monitor all of these metrics as well as some that are unique to our application,
like the time it takes to turn an API request into a notification.</p>
<p><strong>Without timely metrics, you’re flying blind.</strong></p>
<h2 id="options-for-upgrading-postgres">Options for upgrading Postgres</h2>
<p>Part of our research process included looking for <a href="https://retool.com/blog/how-we-upgraded-postgresql-database">prior examples</a>
of database migrations and <a href="https://www.postgresql.org/docs/current/pgupgrade.html">how the Postgres docs</a>
recommend performing an update. Here are a few strategies:</p>
<h3 id="in-place-upgrades-a-non-starter-for-zero-downtime-upgrades">In-place upgrades (a non-starter for zero-downtime upgrades)</h3>
<p>The most basic upgrade option for Postgres is an in-place upgrade.
On AWS RDS, this upgrade is executed from the AWS console. When performing an
in-place upgrade, AWS will shut down the database, run upgrade scripts, and then
bring the system back online. Doing this often requires some preparation,
including dropping Postgres replication slots,
like those used to synchronize with a data warehouse or other systems.</p>
<p>This in-place upgrade process can take anywhere from a few minutes to potentially
hours or more - it entirely depends on how much data needs to be updated
between Postgres versions.</p>
<p>Often, the system is still not in a fully usable state when it comes online, and
administrators must run maintenance tasks like Postgres’ <code>VACUUM</code> command,
or <code>REINDEX</code> to update indexes to support the new version’s format.</p>
<p><strong>Because an in-place upgrade would require far more downtime than we wanted to
tolerate, it was out of the question for us.</strong></p>
<p>A similar approach to an in-place upgrade is to use <code>pg_dump</code> and <code>pg_restore</code>
to transfer the contents of a database once it has shut down.
This <a href="https://www.postgresql.org/docs/current/app-pgdump.html">dump &amp; restore</a>
approach would also not work for us due to the required downtime involved,
mostly because you need to disconnect all applications from the old database in
order to get a reliable database backup. Even then, for large databases, it can
take prohibitively long to dump and restore the database.</p>
<h3 id="replication-based-upgrades">Replication-based upgrades</h3>
<p>This approach relies on Postgres’ excellent replication primitives:
the <code>PUBLICATION</code> and the <code>SUBSCRIPTION</code>.</p>
<p>It works something like this:</p>
<ol>
<li>Spin up a new database on your target Postgres version</li>
<li>Copy over settings, extensions, table configurations, users, etc.</li>
<li>Set up a publication on the old database and a subscription to that publication on the new database</li>
<li>Add your tables to the publication (there is a lot of nuance here - <a href="#choosing-tables-to-replicate">more below</a>)</li>
<li>Once it's fully replicated, run tests to satisfy any remaining risks</li>
<li>Once you are confident in the new database's configuration, point your application at the new database</li>
<li>Tear down the old database</li>
</ol>
<p><strong>In the end, this is the option that we chose at Knock for a few reasons:</strong></p>
<ol>
<li>It gave us gradual steps we could take towards a migration instead of one big upgrade</li>
<li>We could test the new database with real workloads and real data to avoid any regressions</li>
<li>It gave us the most control over when and how to perform the upgrade:
once the new database was fully ready, cutting over to the new database took just a few seconds</li>
</ol>
<p>Although that may sound straightforward, there are several points to consider in
this solution that will depend on your application &amp; circumstances.</p>
<p><strong>Configuring your source and destination databases</strong></p>
<p>Publications and subscriptions depend on a few configuration parameters for
setting up replication slots (how the database keeps track of what needs to be
copied from the primary to the follower database).
<a href="https://www.postgresql.org/docs/16/logical-replication-config.html">The Postgres docs</a>
have plenty of detail on these parameters. These parameters will need to be
tuned for your particular application. For simple applications, the only change
necessary is that <code>wal_level</code> should be set to <code>logical</code>.</p>
<p>If you already use replication slots (e.g. to manage a read replica,
database failover, or to keep a data warehouse in sync), then consider setting
<code>max_replication_slots</code> and the other parameters according to the guidance in the docs.</p>
<p><strong>Setting up basic replication</strong></p>
<ol>
<li>
<p>Start a new Postgres server on your target version of Postgres (in our case v15.3).</p>
</li>
<li>
<p>Set up your desired databases, schemas, tables, partitions, users &amp; passwords, and everything else.</p>
<p><strong>The target database’s tables must have an identical structure to the
source database, but these tables must be empty.</strong></p>
<p>To get a snapshot of the database schema, run <a href="https://www.postgresql.org/docs/current/app-pg-dumpall.html"><code>pg_dumpall</code></a>
on the old DB (pass the <code>--schema-only</code> and <code>--no-role-passwords</code> options to
keep it focused), and then adapt that command for the new DB. You can then
compare the generated SQL files to identify and fix discrepancies between the
old and the new DB.</p>
<p>It may be worth periodically comparing both databases to detect any drift,
especially if you have schema migrations happening in the source database.
Consider running migrations against both databases to keep them in sync.</p>
</li>
<li>
<p>On the primary instance of the <strong>old</strong> database, run <code>CREATE PUBLICATION pg_upgrade_pub;</code>.</p>
<div><p>Although you can tack on <code>FOR ALL TABLES</code> and that will set up the publication
for every table, we found that for large databases, this can lead to performance problems.</p><p>Instead, we found it worked much better to incrementally add one table
at a time to the publication via <code>ALTER PUBLICATION pg_upgrade_pub ADD TABLE table_name</code>.
More on this <a href="#choosing-tables-to-replicate">below</a>.</p></div>
</li>
<li>
<p>On the primary instance of the <strong>new</strong> database, set up the new subscription pointing to that publication:</p>

<p>At this point, you now have a replication pipeline
from the old database to the new one.</p>
<p>To enable the subscription:</p>

</li>
</ol>
<h3 id="choosing-tables-to-replicate">Choosing tables to replicate</h3>
<p>The next step in the process is to build a list of tables you’d like to replicate.
You will want to add tables one at a time, watching each table until all of them
are fully replicated. Later in this post we will show you <a href="#checking-a-tables-replication-status">how to monitor replication
for all the tables</a>.</p>
<p>Generally, the tables will fall into three based on their disk size and the
number of tuples stored in the database.</p>
<ol>
<li>Small enough to synchronize in a few minutes: These can be replicated by just
adding them to the publication and refreshing the subscription</li>
<li>Large, append-only tables: These can be synchronize by first replicating only
future changes, and then separately backfilling old data from a backup or snapshot</li>
<li>Large, frequently updated tables: These are the hardest to synchronize, and
will require some extra care</li>
</ol>
<p>For us, "small" was any table using less than 50 GB of storage and 10 million tuples.</p>
<p>Anything over those thresholds we considered "large".</p>
<div><p><strong>What is a tuple?</strong></p><p>Each insert or update to a Postgres table is stored as a "tuple". If a table has
3 inserts followed by 2 updates, the table would have 5 tuples. Tuples are used
by Postgres’ concurrency mechanism (<a href="https://www.postgresql.org/docs/16/mvcc-intro.html">more in the docs</a>).
Postgres’ <code>VACUUM</code> procedure cleans up old tuples that are no longer needed.</p><p>When we replicate a table, we replicate all of the tuples that make up the
tables contents - inserts and updates. A table with a few rows but many tuples
that haven’t been cleaned up will take longer to replicate than a similar table
with fewer tuples.</p></div>
<p>The following query can help determine the size of a database table in terms of
disk space and tuple counts:</p>

<p>One way to prepare your source database for replication is to <code>VACUUM</code> your tables,
which should help the source database reduce the number of tuples it needs to copy
to the target database. This can help reduce the amount of time it takes to replicate a table.</p>
<p>Before using <code>VACUUM</code>, consult the <a href="https://www.postgresql.org/docs/current/sql-vacuum.html">Postgres docs</a>.</p>
<div><p><strong>Why does table size matter?</strong></p><p>The time it takes to synchronize a table is directly correlated to its size on disk
and the number of tuples it contains. The larger the table, the longer it takes to replicate.
This is because Postgres needs to copy the entire table over to the new database,
and then apply any changes that happen after the initial copy.</p><p>The problem with long synchronization time is that it can prevent your primary
Postgres instance from performing <code>VACUUM</code> operations, which can lead to degraded
performance over time. Left unchecked, it can even lead to transaction wraparound
and a forced shutdown of the database.</p><p>For these reasons, we added tables one at a time to replication, used different
strategies based on the size &amp; write patterns of each table, and closely monitored
the system’s performance to ensure we didn’t degrade our service.</p><p>If migrating a table becomes problematic, you can remove a table from replication
at any time, and then re-add it later (although you will need to truncate the
target table and start from scratch).</p></div>
<h3 id="how-to-replicate-small-tables">How to replicate "small" tables</h3>
<p>To migrate small tables, you just add it to the publication and then refresh the subscription:</p>

<p>Postgres will handle copying the table over, getting it synchronized, and
applying any further operations to the table. For very small tables,
synchronization can happen in less than a second.</p>
<h3 id="large-append-only-tables">Large, append-only tables</h3>
<p>Tables that are too large but generally append-only, with no updates (or, if
updates are <em>always</em> on rows that are recent, like within the past week),
then you can set up a separate <code>PUBLICATION</code> and <code>SUBSCRIPTION</code> following the
same steps as above, but setting the <code>copy_data</code> option on the subscription to
false. Suffix the name of the new publication and new subscription with <code>_nocopy</code>
to make it distinct.</p>
<p>When you are ready to migrate these large, append-only tables, you can add them
to this <code>nocopy</code> publication, and refresh the subscription on the target using
the <code>copy_data = false</code> option:</p>

<p>We found this approach worked really well for our partitioned tables that stored
various types of logs for our customers.
We did not need to migrate the root of a partitioned table, we only migrated the
underlying tables, and that seemed to work pretty well.</p>
<p>Once the subscription is running, you should start seeing logs appear on the
target databases table:</p>

<p>From here, you can backfill any records older than those now visible in the
database using whatever means you like (e.g. <code>pg_dump</code>).</p>
<p>Here is how we did it on AWS RDS Aurora:</p>
<ol>
<li>
<p>Take a snapshot of your production database in the AWS Console</p>
</li>
<li>
<p>Restore that snapshot into a new database instance (the snapshot DB)</p>
</li>
<li>
<p>Rename the table(s) on the snapshot DB that you want to replicate by adding
a suffix like <code>_snapshot</code>. This prevents us having two replication pipelines
feeding into the same table on the target database.</p>
</li>
<li>
<p>Create the same table(s) on the target database with the same schema as the
snapshot database. Use the same suffix as above.</p>
</li>
<li>
<p>Create a publication on the snapshot database and a subscription on the target
database to replicate these snapshot table(s) from the snapshot database to the
target database</p>
</li>
<li>
<p>Enable the subscription and monitor its progress</p>
</li>
<li>
<p>Once the subscription is caught up, you can merge the tables together using
<code>INSERT...ON CONFLICT</code>:</p>

</li>
</ol>
<p><span><span></span><img alt="Diagram showing how to backfill data from a snapshot" loading="lazy" decoding="async" data-nimg="responsive" sizes="100vw" srcset="https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=640&amp;q=75 640w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=750&amp;q=75 750w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=828&amp;q=75 828w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=1080&amp;q=75 1080w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=1200&amp;q=75 1200w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=1920&amp;q=75 1920w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=2048&amp;q=75 2048w, https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=3840&amp;q=75 3840w" src="https://knock.app/_next/image?url=%2Fassets%2Fblog%2Fzero-downtime-postgres-upgrades%2Fsnapshot-backfill.png&amp;w=3840&amp;q=75" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p>
<p>For very large tables, this can still take several days, but because it’s all
in the background it shouldn’t affect your production environment.</p>
<p>Once the tables are fully merged, compare them to ensure a consistent row count
(more on that later). Once you are confident the tables are identical,
drop the snapshot table on the target DB, drop the subscription to the snapshot DB,
and terminate the snapshot database instance.</p>
<h3 id="large-tables-with-many-updates-over-most-of-the-rows">Large tables with many updates over most of the rows</h3>
<p>These are the hard tables. Because they have so much data in them, they can
take a long time to replicate, which can affect system performance on the
source database if it prevents <code>AUTOVUACUUM</code> from running. Because they have
so many updates, we can’t treat it as an append-only table.</p>
<p>A few points to consider:</p>
<ol>
<li>Is there any housekeeping you can do to reduce the table’s size?</li>
<li>Have you vacuumed the table recently?</li>
<li>Can you partition the table into smaller pieces?</li>
<li>Do rows stop receiving updates after a reliable time frame (e.g. 1 week?) -
this could be used to treat the table as an append-only table, and then after
that time frame has elapsed you can backfill old rows from a snapshot.</li>
</ol>
<p>If your source database is not on PG 15 or greater, your options are limited.
Follow the steps in the "small tables" section. Rely on the monitoring you have
in place (you do have monitoring, right?) to ensure replication doesn’t degrade
your service. If needed, you can rollback by removing the table from the
publication, and refreshing the subscription (<a href="#aborting-the-replication-of-one-table">See below</a>).</p>
<p>If the table is still too big, try to start replication during low traffic times to
reduce load and write activity. This will hopefully minimize the impact on your
system.</p>
<h3 id="large-tables-coming-from-pg-15-or-greater">Large tables coming from PG 15 or greater</h3>
<p>If your source database is on PG 15 or greater, you may be able to split up replication
across multiple publications (similar to partitioning or sharding). You can then
migrate the table in smaller chunks, at the expense of using more replication
slots. <a href="https://www.postgresql.org/docs/16/logical-replication-config.html">The Postgres docs</a>
have more information on setting these parameters.</p>
<div><p>Because we migrated from 11.9 to 15.3, we did not have this option available
to us. As such, we have not tested this approach. Even so, as we considered
our options we noticed that this approach might be possible. If you try it
out, let us know, we’d love to hear how it goes!</p></div>
<p>The goal is to have enough publications to split your largest table into manageable
pieces (for us, this was about 100 GB of non-index data stored). We’ll assume we
are splitting across three partitions in this example. The trick is adding a <code>WHERE</code>
clause that splits up the rows handled by each subscription:</p>

<p>On the destination database, create a subscription for each partition.</p>
<p>You only want to migrate one slice of each table at a time. Generally, you will
follow the same instructions as adding a "small" table, but with the extra <code>WHERE</code>
clause added when setting up the table for each publication.</p>
<p>In this way, you can slice up large tables into smaller, more workable pieces.</p>
<p>Consider only using this approach if having too many replication slots is a
problem: you can still add "small" tables using this approach,
just add the table to the <code>_0</code> publication without a <code>WHERE</code> clause.
This can help reduce the number of replication slots required when migrating.</p>
<h3 id="checking-a-tables-replication-status">Checking a table’s replication status</h3>
<p>When a table is added to a subscription, it moves through five distinct states
(visible on the target database under the system table <code>pg_subscription_rel</code>
in the <code>srsubstate</code> column):</p>
<ol>
<li>Initializing the table’s subscription (State code <code>i</code> )</li>
<li>Copying the table’s contents in one efficient operation (State code <code>d</code>)</li>
</ol>
<div><p>This step requires keeping old Postgres transaction IDs around, which
prevents vacuum from running effectively and can lead to system performance
issues and (if left running long enough) even Postgres transaction ID
wraparound which can halt the system.</p><p>This is the step that requires replicating only one table at a time.</p></div>
<ol start="3">
<li>Copy finished, waiting for final sync (State code <code>f</code>)</li>
<li>Finalizing initial sync (State code <code>s</code>)</li>
<li>Ready and running under normal replication (State code <code>r</code>)</li>
</ol>
<p>In order to prevent the issues found in step 2 above, we found it was necessary
to add one table at a time to replication, and to closely watch the system’s
performance. The worst-case scenario (transaction wraparound) must be avoided.</p>
<p><strong>If you get anywhere close to wraparound, it is better to <a href="#aborting-the-replication-of-one-table">abort the migration</a>
and break it up into smaller pieces.</strong></p>
<p>If we had created our publication using the <code>FOR ALL TABLES</code> option, Postgres
would have started to sync our very large source database all at once,
preventing automatic <code>VACUUM</code> operations from completing necessary maintenance.
We found this to gradually degrade database performance over time,
leading to increased risk to system stability.</p>
<p>Adding one table at a time has the added advantage of allowing teams to
incrementally migrate each table. Replication does come with CPU and other costs
for the source and destination databases. By adding one table at a time,
administrators can control how replication affects the running system.</p>
<h3 id="aborting-the-replication-of-one-table">Aborting the replication of one table</h3>
<p>If you need to halt the replication of a table, you reverse the instructions for
adding the table in the first place:</p>

<p>In an emergency, you can also drop the publications and subscriptions entirely,
and start the process over. Postgres will clean up any replication slots that
were created as part of the publication and subscription, which should relieve
any pressure on the source database.</p>
<div><p>Be advised that if you just disable the subscription without removing the table
from the publication and refreshing the subscription, the source database
will continue to hold onto old transaction IDs, which can lead to transaction
wraparound and a forced shutdown of the database.</p><p>Just disabling the subscription will not resolve any replication-related
performance problems.</p></div>
<h3 id="a-note-about-moving-replication-slots">A note about moving replication slots</h3>
<p>Replication slots in Postgres store a log of database activity that can be
consumed on another database or in another application. Postgres tracks slot
progress using a Log Sequence Number (LSN). LSNs are unique to the primary
Postgres database. This means that if you have a replication slot on your
database (e.g. to copy changes to a data warehouse or as part of your own
application), you will not be able to copy the replication slot's LSN over from
the old database to the new database.</p>
<p>You will need to consult the documentation of the application consuming the
replication slot to decide how to best migrate (e.g. for data warehousing tools,
they may have a way to merge duplicated information between both databases).
If you’re using replication slots as part of your own application, you already
know that you’re on your own to roll your own solution. Having some idempotence
mechanism to deduplicate transactions from the old and the new database will
definitely be helpful.</p>
<h2 id="finalizing-the-migration">Finalizing the migration</h2>
<p>Once you have added all of your tables to publications, and the subscriptions
have caught up on everything, you need to now verify that the tables match.</p>
<p>Unfortunately, eventual consistency (the lag between a write being applied to
the old database and it showing up on the new database) will prevent both
databases from being perfect matches at the same time, you can still count table
rows to make sure you’re close enough to know it’s working.</p>
<p>At Knock, we wrote a script that iterated through each table and asked both
databases to count the total number of rows in each table on the old and new
database, and compared the results. For tables with an <code>inserted_at</code> column, we
filtered to rows older than 10 seconds. This interval is more than enough to
prove that the tables match, with the assumption that the remaining 10 seconds
will replicate across in short order.</p>
<p>You may need to come up with a strategy that fits your application’s needs. We
felt that as long as row counts were accurate within a few seconds, we could
otherwise assume that Postgres replication was reliable.</p>
<p>In a few instances, we also spot-checked the contents of a few tables to ensure
they matched to confirm this assumption. Collecting a random sample of rows from
tables and comparing them between the old and the new database can help verify
that the tables are identical.</p>
<h3 id="application-level-changes">Application-level changes</h3>
<p>Parallel to all of this database work, you may need to change your application
to connect to both databases. When you are finally ready to cut over,
you need a strategy to shift traffic to your new database.</p>
<p>When the final cutover happens, you could change your application’s configuration
to point to the new database, and then reboot your app. This is simple,
straightforward, and is precisely how we migrated one of our
lower-traffic databases.</p>
<p>For applications with lots of concurrent activity, you may need to get creative.
We wanted to avoid a situation with conflicting writes between the old and new
database. Such conflicts could have caused a service outage for us, requiring
manually reconciling database state.</p>
<p>At Knock, we configured our application to connect to both databases.
When we were ready to execute the cutover, we ran a script that did the following:</p>
<ol>
<li>
<p>Tell all instances of our application to send new queries to the new database</p>
</li>
<li>
<p>All currently running database queries had 500 ms to complete before being forcefully cancelled</p>
</li>
<li>
<p>For the first second after flipping the flag, our application artificially paused
any new database requests for one second. This allowed pending transactions
to replicate to the new database so that new queries wouldn’t have stale reads</p>
<p>500 ms is far higher than most of our db queries, and we saw zero errors due to forced disconnections</p>
</li>
<li>
<p>After that first second, database activity returned to normal behavior, but pointing at the new database.</p>
</li>
<li>
<p>In the middle of the cutover, we had some specialized database workloads that
the script shut down and restarted in order to reconnect to the new database.</p>
</li>
</ol>
<h3 id="one-more-thing-sequences">One more thing: sequences</h3>
<p>One thing that replication doesn’t synchronize is any Postgres sequence.
Sequences are monotonically increasing integers that are guaranteed to never
duplicate. Unfortunately, they are not incremented on the new database as
sequence values are used up on the old database.</p>
<p>Fortunately, this is pretty easy to control for. Part of our cutover procedure
was to run a script right before flipping our feature flag that did the following:</p>
<ol>
<li>
<p>Connect to both databases</p>
</li>
<li>
<p>Get the next value of all of the sequences in the database using <code>SELECT nextval('sequence_name')</code></p>
</li>
<li>
<p>Set that value in the new database using <code>SELECT setval('sequence_name', value::int4 + 100000)</code>
to advance the sequence and offer a little bit of buffer (in this case, 100k
rows can be added between setting this value on the new database and cutting over).
This will introduce a gap in the sequence, but that’s generally not a problem.
For us, our sequences are bigints. 100k values skipped in the sequence is a
rounding error off of 0% used up sequence values in that case.</p>
<p>You will want to tune how big of a gap you introduce so you don’t use too
much of your sequence’s usable space. If you only expect the sequence to
use a few hundred values during your cutover window,
then maybe advance it only by 5000.</p>
</li>
</ol>
<h2 id="final-checklist-before-cutting-over">Final checklist before cutting over</h2>
<p>Here are some of the things we considered before executing our final cutover:</p>
<ol>
<li>Do the rows on all the tables match as expected?</li>
<li>Are all the subscriptions enabled and running without error?</li>
<li>Do the schemas match? Can you freeze any new schema migrations from being
released to reduce the risk of something changing while you’re migrating?</li>
<li>Is your new database properly sized for your workloads?</li>
<li>Do you have to add any read replicas so the database cluster topology is the
same between the old and the new database?</li>
<li>Have you reindexed and performed basic VACUUM maintenance on the new database
to ensure it’s fresh and ready for production traffic?</li>
<li>Have you double checked Postgres’ release notes for anything that might cause
a regression in your app?</li>
<li>Have you run automated and manual tests against a staging database on the new
version to verify system performance?</li>
<li>Have you run load tests of your most demanding queries using <code>pg_bench</code>
against your new version to verify performance?</li>
<li>If there’s one thing that you can de-risk still, what is it?</li>
<li>Do practice runs in a staging or test environment until you have fully
exercised the cutover process multiple times. Dry runs like this will help
reveal gaps in your plan before you go to production.</li>
<li>Right before cutover, take a database backup - just in case.</li>
</ol>
<h2 id="cutting-over">Cutting over</h2>
<p>At Knock, we took a few weeks replicating tables one at a time. We generally did
this after business hours and during our lowest traffic time frames. We practiced
cutover in our staging environment multiple times, ironing out the process until
it just worked without much operator involvement.</p>
<p>Once we had a replica running PG 15 and had the application code in place to
cut over from the old to the new database, we ran one final set of checks and
flipped the flag.</p>
<p>After months of preparation, the actual cutover was uneventful: our
application cut over within a few seconds, we had a brief blip of (intentional)
latency as queries waited to allow for replication, and our application
continued running without skipping a beat. Reading this paragraph took longer
than the cutover itself.</p>
<p>From there, we rolled back the application changes we introduced, permanently
pointed everything at the new database, removed the subscriptions on the new
database, and tore down the old database. We had successfully jumped from
Postgres 11.9 to 15.3 with zero downtime!</p>
<h2 id="conclusion">Conclusion</h2>
<p>Although jumping four major versions of Postgres in one leap is a painstaking
process, it can be done, and in many ways it’s safer than scheduled downtime:
it can be practiced, tested, and reworked multiple times before performing the
actual cutover. At any point in the process, we could have dropped the
publications from the old database and started over without degrading our service.</p>
<p>Modern customers expect 100% availability. While that is not technically possible,
zero downtime migrations make it easier to keep systems running smoothly without
major service interruptions.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pharmacies share medical data with police without a warrant, inquiry finds (139 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2023/12/12/pharmacy-records-police-privacy-abortion/</link>
            <guid>38615841</guid>
            <pubDate>Tue, 12 Dec 2023 18:00:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2023/12/12/pharmacy-records-police-privacy-abortion/">https://www.washingtonpost.com/technology/2023/12/12/pharmacy-records-police-privacy-abortion/</a>, See on <a href="https://news.ycombinator.com/item?id=38615841">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/technology/2023/12/12/pharmacy-records-police-privacy-abortion/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[VMware by Broadcom transition to subscription, end of sale of perpetual licenses (222 pts)]]></title>
            <link>https://news.vmware.com/company/vmware-by-broadcom-business-transformation</link>
            <guid>38615315</guid>
            <pubDate>Tue, 12 Dec 2023 17:25:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.vmware.com/company/vmware-by-broadcom-business-transformation">https://news.vmware.com/company/vmware-by-broadcom-business-transformation</a>, See on <a href="https://news.ycombinator.com/item?id=38615315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>By Krish Prasad, Senior Vice President and General Manager, VMware Cloud Foundation Division</em></p><p>Broadcom’s close of the VMware acquisition has brought together two engineering-first, innovation-centric teams to help build the world's leading infrastructure technology company. Together, we are well-positioned to enable global enterprises to embrace private, hybrid and multi-cloud environments. And we are committed to driving customer success through faster innovation, a simpler portfolio and a robust ecosystem.</p><p>Over the past two years, VMware has been on a journey to simplify its portfolio and transition from a perpetual to a subscription model to better serve customers with continuous innovation, faster time to value, and predictable investments.</p><p>Today, VMware by Broadcom has reached a new milestone with the announcement of the following:</p><ul><li>A dramatic simplification of our product portfolio that allows customers of all sizes to gain more value for their investments in VMware solutions. The portfolio simplification across all VMware by Broadcom divisions stems from customer and partner feedback over the years telling us our offers and go-to-market are too complex.</li><li>Complete the transition of all VMware by Broadcom solutions to subscription licenses, with the end of sale of perpetual licenses, Support and Subscription (SnS) renewals for perpetual offerings, and hybrid purchase program/subscription purchase program (HPP/SPP) credits beginning today (effective dates will vary). Additionally, we are introducing a bring-your-own-subscription license option, providing license portability to VMware validated hybrid cloud endpoints running VMware Cloud Foundation.</li></ul><p><strong>Portfolio Simplification</strong></p><p>Today, we’re announcing specific changes coming to the VMware Cloud Foundation division portfolio. Going forward, the division will feature two primary offers:</p><ul><li><strong>VMware Cloud Foundation</strong>, our flagship enterprise-class hybrid cloud solution for customers to run their business critical and modern applications – in a secure, resilient and cost efficient manner. To allow more customers to benefit from this solution, we’ve reduced the previous subscription list price by half and added higher support service levels including enhanced support for activating the solution and lifecycle management.<strong></strong></li><li>The new <strong>VMware vSphere Foundation</strong> delivers a more simplified enterprise-grade workload platform for our mid-sized to smaller customers. This solution integrates vSphere with our intelligent operations management to provide the best performance, availability, and efficiency with greater visibility and insights.</li></ul><p>Both VMware Cloud Foundation and VMware vSphere Foundation will have optional advanced add-on offers. Our storage offering, ransomware and disaster recovery service, and application platform services are available on both offers. And Application Network and Security offerings are available for VMware Cloud Foundation. Additional advanced services and offerings, including Private AI, will be available soon.</p><p><strong>Subscription Licenses</strong></p><p>VMware has been on a journey to transition to a subscription model for more than a year now, and the industry has already embraced subscription as the standard for cloud consumption. With a simplified portfolio in place, we’re completing our transition to subscription offerings. Offerings will solely be available as subscriptions or as term licenses following the end of sale of perpetual licenses and Support and Subscription (SnS) renewals beginning today.</p><p>The subscription model helps us deliver what customers want:</p><ul><li>Continuous innovation.</li><li>Faster time to value.</li><li>Predictable investments.</li></ul><p>Customers may continue using perpetual licenses with active support contracts. We will continue to provide support as defined in contractual commitments. We encourage customers to review their inventory of perpetual licenses, including Support Services renewal and expiration dates. Broadcom will work with customers to help them “trade in” their perpetual products in exchange for the new subscription products, with upgrade pricing incentives. (For additional details, see FAQ).</p><p>The simplification of our portfolio and shift to subscription and term offerings are a culmination of our multi-year business transformation efforts. The steps we’re taking today will further enable customer and partner success by delivering the innovation, simplicity and flexibility they need as they undertake their digital transformations.</p><p><strong>Frequently Asked Questions</strong></p><p><strong>Q: What is VMware by Broadcom announcing?</strong></p><p>A: Today, VMware by Broadcom has reached a new milestone in its journey and announced the following:</p><ul><li>A dramatic simplification of our product portfolio that allows customers of all sizes to gain more value for their investments in VMware solutions. The portfolio simplification across all VMware by Broadcom divisions stems from customer and partner feedback over the years telling us our offers and go-to-market are too complex.</li><li>Complete the transition of all VMware by Broadcom solutions to subscription licenses, with the end of sale of perpetual licenses, Support and Subscription (SnS) renewals for perpetual offerings, and HPP/SPP (generic) credits beginning today. Additionally, we are introducing a bring-your-own-subscription license option, providing license portability to VMware validated hybrid cloud endpoints running VMware Cloud Foundation.</li></ul><p><strong>Q: How do these changes benefit customers?</strong>&nbsp;</p><p>A: Over the past two years, VMware has been on a journey to simplify its portfolio and transition to a subscription model, the industry standard for cloud consumption, and to better serve customers with continuous innovation, faster time to value, and predictable investments. We’re also helping more customers benefit from VMware Cloud Foundation by reducing the list price by half and including higher support service levels including enhanced support for activating the solution and lifecycle management.</p><p><strong>Q: Why is this good for partners?</strong></p><p>A: The industry has already widely embraced subscription and SaaS, and many partners in our ecosystem have already developed success practices in this area. Subscription and SaaS models provide an opportunity for partners to engage more strategically with customers and deliver higher-value services that drive customer success. It also helps accelerate their own transition to a business model focused on annual recurring revenue.</p><p><strong>Q: What are the changes to perpetual licenses?</strong></p><p>A: As part of our transition to subscription and a simplified portfolio, beginning today, we will no longer sell perpetual licenses. All offerings will continue to be available as subscriptions going forward. Additionally, we are ending the sale of Support and Subscription (SnS) renewals for perpetual offerings beginning today.</p><p><strong>Q: Why make this change from perpetual licenses to subscription?&nbsp;</strong></p><p>A: This shift is the natural next step in our multi-year strategy to make it easier for customers to consume both our existing offerings and new innovations. VMware believes that a subscription model supports our customers with the innovation and flexibility they need as they undertake their digital transformations.</p><p><strong>Q: Can customers continue to use their perpetual license</strong><strong>s?</strong></p><p>A: Yes, customers can continue to use perpetual licenses that they’ve purchased for products.</p><p><strong>Q: Can customers still purchase and add additional perpetual licenses after </strong><strong>today?</strong></p><p>A: After a customer’s effective date for the end of availability of perpetual licenses, customers will not be able to purchase new perpetual licenses. Customers will be able to purchase subscription software or term licenses to supplement or replace their current perpetual-licensed install base.</p><p><strong>Q: Can customers renew their Service and Support (SnS) contracts </strong><strong>after today?</strong></p><p>A: No, customers cannot renew their SnS contracts for perpetual licensed products after today. Broadcom will work with customers to help them “trade in” their perpetual products in exchange for the new subscription products, with upgrade pricing incentives. Customers can contact their VMware account or partner representative to learn more.</p><p><strong>Q: Do customers have to collect and submit their perpetual licenses to Broadcom when they “trade-in” their products for subscription?</strong></p><p>A: No, Broadcom does not require customers to submit their perpetual licenses to Broadcom when they “trade in” their products for subscription.</p><p><strong>Q: Will VMware by Broadcom continue to provide support for active Service and Support (SnS) contracts?</strong></p><p>A: We will continue to provide support as defined in contractual commitments.</p><p><strong>Q: What will happen to customers’ existing perpetual licenses with active SnS contracts when they are up for renewal?</strong></p><p>A: Customers should contact their VMware account or partner representative for upgrade pricing from perpetual to subscription.</p><p><strong>Q: What can customers with perpetual licenses do now to prepare?</strong></p><p>A: This is an excellent time for customers to assess their current state with VMware infrastructure and management products. We encourage customers to review their inventory of perpetual licenses, including refresh cycles and renewal dates, and become more familiar with VMware's available subscription offers. Customers should also contact their VMware or partner representative for more information.&nbsp;</p><p><strong>Q: What VMware Cloud Foundation division offerings are available for purchase?</strong></p><p>A: The product simplification across the VMware Cloud Foundation division stems from customer and partner feedback requesting we reduce the complexity of our offers and go-to-market. Going forward, the VMware Cloud Foundation division will feature two primary offers: VMware Cloud Foundation, the new VMware vSphere Foundation and our Hybrid Cloud services and offers. Additionally, we offer VMware vSphere Standard and VMware vSphere Essentials Plus for deployments with more limited requirements.</p><p><strong>Q: What is VMware vSphere Foundation?</strong></p><p>A: VMware vSphere Foundation is a new solution that combines our full-featured server virtualization platform, vSphere with intelligent operations management to deliver the best performance, availability, and efficiency with greater visibility and insights. For customers seeking an HCI solution, we offer VMware vSAN as an add-on to vSphere Foundation, which includes all the capabilities of vSAN including vSAN Max.</p><p><strong>Q: What happens to customers that have purchased HPP/EPP credits and still have an outstanding balance of unredeemed credits?</strong></p><p>A: We are honoring existing agreements and will work with our customers to help with the redemption of credits on our new offerings.</p><p><strong>Q: What products and bundles are impacted by this new policy?</strong></p><p>A: Here is a list of products impacted by the new licensing policy:</p><ul><li>VMware Cloud Foundation</li><li>VMware vSphere</li><li>VMware vSAN</li><li>VMware NSX</li><li>VMware HCX</li><li>VMware Site Recovery Manager</li><li>VMware vCloud Suite</li><li>VMware Aria Suite</li><li>VMware Aria Universal</li><li>VMware Aria Automation</li><li>VMware Aria Operations</li><li>VMware Aria Operations for Logs</li><li>VMware Aria Operations for Networks</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla "MemoryCache" Local AI (398 pts)]]></title>
            <link>https://future.mozilla.org/blog/introducing-memorycache/</link>
            <guid>38614824</guid>
            <pubDate>Tue, 12 Dec 2023 16:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://future.mozilla.org/blog/introducing-memorycache/">https://future.mozilla.org/blog/introducing-memorycache/</a>, See on <a href="https://news.ycombinator.com/item?id=38614824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    <h2>Augmenting Local AI with Browser Data:  Introducing MemoryCache</h2>

    
    
    

    
    
    

    

    
      <p data-block-key="yx97h">The internet was born as a way to connect people and data together around the world. Today, machine learning is upending the way that we interact with data and information. Language and multi-media models seek ever-larger datasets to train on – including <a href="https://www.theverge.com/2023/7/5/23784257/google-ai-bard-privacy-policy-train-web-scraping">the entirety of the internet</a>. There are <a href="https://foundation.mozilla.org/en/blog/mozillas-ai-funding-principles-open-source-building-blocks-for-philanthropy/">complex sociological and regulatory questions at play</a>; critical decisions to be made about copyright, safety, transparency, access, and representation.</p><p data-block-key="bu6uc"><a href="https://memorycache.ai/">MemoryCache</a> is an early exploration project from the Mozilla Innovation Ecosystem team that augments an on-device, personal model with local files saved from the browser to reflect a more personalized and tailored experience through the lens of privacy and agency.</p>
    
      
<figure>
  





<img alt="A UI Mockup of a potential design for a MemoryCache desktop application" height="1852" src="https://storage.googleapis.com/future-prod-prod-storage/images/DesktopApplication.original.png" width="2520">

  
  <figcaption>
    A mockup of a potential design for a future MemoryCache UI
    
  </figcaption>
  
</figure>

    
      <p data-block-key="3scq2">The way that humans think is uniquely personal to each individual. While individuals share many principles, values and properties of their surrounding communities and organizations, each of us has a unique perspective and set of information that we're exposed to on a regular basis. The process of making new insights from the content we create and consume is not a "one-size-fits-all" opportunity, and machine learning capabilities open up a vast range of new computing advancements and paradigms.</p>
    
      
<figure>
  





<img alt="Prompt" height="1500" src="https://storage.googleapis.com/future-prod-prod-storage/images/Prompt.original.png" width="3000">

  
</figure>

    
      <p data-block-key="aieh9">Today, MemoryCache is a set of scripts and simple tools to augment a local copy of <a href="https://www.privategpt.io/">privateGPT</a>. The project contains:</p><ul><li data-block-key="fro2t">A Firefox extension that acts as a simple "printer" to save pages to a subdirectory in your<code> /Downloads/</code> folder, and includes the ability to quickly save notes and information from your browser to your local machine</li><li data-block-key="al824">A shell script that listens for changes in the <code>/Downloads/MemoryCache</code> directory and runs the privateGPT <code>ingest.py</code> script</li><li data-block-key="9vlcq">Code to (optionally) update the Firefox <code>SaveAsPDF</code> API on a <a href="https://firefox-source-docs.mozilla.org/setup/linux_build.html">local build of Firefox</a> to enable a flag that silently saves webpages as PDF for easier human readability (by default, pages need to be saved as HTML in Firefox)</li></ul><p data-block-key="7qqdc">We see MemoryCache as a sandbox for experimenting with some of the quirkier, more unique parts of the brainstorming and idea generation process, all done entirely locally. Our test ground for MemoryCache is a gaming PC with an Intel i7-8700 processor, using <a href="https://huggingface.co/nomic-ai/gpt4all-j">Nomic AI's groovy.ggml version of gpt-4-all model</a>. We're using the <a href="https://github.com/misslivirose/privateGPT">primordial version of privateGPT</a>, because our preliminary evaluations have found that newer models and versions of the application start to over-generalize the responses even after augmenting the model with personal data – in our environment, 75.3MB of documents saved from the browser and from personal blog posts, notes, and journal entries.</p><p data-block-key="598h8">MemoryCache is early, experimental, and a sandbox for exploration. You can follow along with <a href="https://github.com/Mozilla-Ocho/Memory-Cache">the project on GitHub</a>, or check <a href="https://memorycache.ai/">out our website</a> to stay up to date with our progress!</p>
    

    
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phi-2: The surprising power of small language models (143 pts)]]></title>
            <link>https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</link>
            <guid>38614361</guid>
            <pubDate>Tue, 12 Dec 2023 16:29:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</a>, See on <a href="https://news.ycombinator.com/item?id=38614361">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody" data-bi-an="post-body">

						
<h3 id="contributors">Contributors</h3>



<p><a href="https://www.microsoft.com/en-us/research/people/maabdin/">Marah Abdin</a>, <a href="https://www.microsoft.com/en-us/research/people/jyotianeja/">Jyoti Aneja</a>, <a href="https://www.microsoft.com/en-us/research/people/sebubeck/">Sebastien Bubeck</a>, Caio César Teodoro Mendes, <a href="https://www.microsoft.com/en-us/research/people/wzchen/">Weizhu Chen</a>, Allie Del Giorno, <a href="https://www.microsoft.com/en-us/research/people/roneneldan/">Ronen Eldan</a>, <a href="https://www.microsoft.com/en-us/research/people/sigopi/">Sivakanth Gopi</a>, <a href="https://www.microsoft.com/en-us/research/people/suriyag/">Suriya Gunasekar</a>, <a href="https://www.microsoft.com/en-us/research/people/mojavaheripi/">Mojan Javaheripi</a>, <a href="https://www.microsoft.com/en-us/research/people/pkauffmann/">Piero Kauffmann</a>, <a href="https://www.microsoft.com/en-us/research/people/yintatlee/">Yin Tat Lee</a>, Yuanzhi Li, <a href="https://www.microsoft.com/en-us/research/people/anhnguyen/">Anh Nguyen</a>, <a href="https://www.microsoft.com/en-us/research/people/gderosa/">Gustavo de Rosa</a>, <a href="https://www.microsoft.com/en-us/research/people/olsaarik/">Olli Saarikivi</a>, <a href="https://www.microsoft.com/en-us/research/people/adilsalim/">Adil Salim</a>, <a href="https://www.microsoft.com/en-us/research/people/shitals/">Shital Shah</a>, Michael Santacroce, Harkirat Singh Behl, <a href="https://www.microsoft.com/en-us/research/blog/tag/adam-kalai/">Adam Taumann Kalai</a>, <a href="https://www.microsoft.com/en-us/research/people/wanxin/">Xin Wang</a>, <a href="https://www.microsoft.com/en-us/research/people/rachelward/">Rachel Ward</a>, <a href="https://www.microsoft.com/en-us/research/people/pwitte/">Philipp Witte</a>, <a href="https://www.microsoft.com/en-us/research/people/cyrilzhang/">Cyril Zhang</a>, Yi Zhang</p>



<figure><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1.jpg" alt="Satya Nadella on stage at Microsoft Ignite 2023 announcing Phi-2." srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-343x193.jpg 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Phi2-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px"><figcaption><strong>Figure 1. </strong>Satya Nadella announcing Phi-2 at Microsoft Ignite 2023.</figcaption></figure>



<p>Over the past few months, our Machine Learning Foundations team at Microsoft Research has released a suite of small language models (SLMs) called “Phi” that achieve remarkable performance on a variety of benchmarks. Our first model, the 1.3 billion parameter <a href="https://huggingface.co/microsoft/phi-1" target="_blank" rel="noreferrer noopener"><strong>Phi-1</strong><span> (opens in new tab)</span></a>, achieved state-of-the-art performance on Python coding among existing SLMs (specifically on the HumanEval and MBPP benchmarks). We then extended our focus to common sense reasoning and language understanding and created a new 1.3 billion parameter model named <a href="https://huggingface.co/microsoft/phi-1_5" target="_blank" rel="noreferrer noopener"><strong>Phi-1.5</strong><span> (opens in new tab)</span></a>, with performance comparable to models 5x larger.</p>



<p>We are now releasing <a href="https://ml.azure.com/registries/azureml-msr/models/microsoft-phi-2/version/3?tid=72f988bf-86f1-41af-91ab-2d7cd011db47#overview" target="_blank" rel="noreferrer noopener"><strong>Phi-2</strong><span> (opens in new tab)</span></a>, a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.</p>



<p>With its compact size, Phi-2 is an ideal playground for researchers, including for exploration around mechanistic interpretability, safety improvements, or fine-tuning experimentation on a variety of tasks. We have made <a href="https://ml.azure.com/registries/azureml-msr/models/microsoft-phi-2/version/3?tid=72f988bf-86f1-41af-91ab-2d7cd011db47#overview" target="_blank" rel="noreferrer noopener"><strong>Phi-2</strong><span> (opens in new tab)</span></a><strong> </strong>available in the Azure AI Studio model catalog to foster research and development on language models.</p>



	<div data-bi-an="promo" data-bi-id="979236">
		

		<p>
		<span>MICROSOFT RESEARCH PODCAST</span>
	</p>
	
	<div>
						<p><a href="https://www.microsoft.com/en-us/research/podcast/whats-your-story-ranveer-chandra/" aria-label="What’s Your Story: Ranveer Chandra" data-bi-cn="What’s Your Story: Ranveer Chandra" target="_blank">
					<img decoding="async" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/10/Ravneer_Hero_Feature_1400x788.png">
				</a>
			</p>
			
			<div>

									<h2>What’s Your Story: Ranveer Chandra</h2>
				
								<p>You may know tech, but how well do you know the people behind the advances? Ranveer Chandra talks about growing up in India, his work in systems and networking, and finding joy in your job in the first episode of the #MSRPodcast “What’s Your Story”.</p>
				
								
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
</div><!--/.msr-promo-->
	


<h2 id="key-insights-behind-phi-2">Key Insights Behind Phi-2</h2>



<p>The massive increase in the size of language models to hundreds of billions of parameters has unlocked a host of emerging capabilities that have redefined the landscape of natural language processing. A question remains whether such emergent abilities can be achieved at a smaller scale using strategic choices for training, e.g., data selection.</p>



<p>Our line of work with the Phi models aims to answer this question by training SLMs that achieve performance on par with models of much higher scale (yet still far from the frontier models). Our key insights for breaking the conventional language model scaling laws with Phi-2 are twofold:</p>



<p>Firstly, training data quality plays a critical role in model performance. This has been known for decades, but we take this insight to its extreme by focusing on “textbook-quality” data, following upon our prior work “<a href="https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/">Textbooks Are All You Need</a>.” Our training data mixture contains synthetic datasets specifically created to teach the model common sense reasoning and general knowledge, including science, daily activities, and theory of mind, among others. We further augment our training corpus with carefully selected web data that is filtered based on educational value and content quality. Secondly, we use innovative techniques to scale up, starting from our 1.3 billion parameter model, Phi-1.5, and embedding its knowledge within the 2.7 billion parameter Phi-2. This scaled knowledge transfer not only accelerates training convergence but shows clear boost in Phi-2 benchmark scores.</p>



<figure><img decoding="async" width="25377" height="5876" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp.png" alt="A bar plot comparing the performance of Phi-2 (with 2.7B parameters) and Phi-1.5 (with 1.3B parameters) on common sense reasoning, language understanding, math, coding, and the Bigbench-hard benchmark. Phi-2 outperforms Phi1.5 in all categories. The commonsense reasoning tasks are PIQA, WinoGrande, ARC easy and challenge, and SIQA. The language understanding tasks are HellaSwag, OpenBookQA, MMLU, SQuADv2, and BoolQ. The math task is GSM8k, and coding includes the HumanEval and MBPP benchmarks. " srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp.png 25377w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-300x69.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-1024x237.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-768x178.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-1536x356.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-2048x474.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure2_phi_comp-240x56.png 240w" sizes="(max-width: 25377px) 100vw, 25377px"><figcaption><strong>Figure 2. </strong>Comparison between Phi-2 (2.7B) and Phi-1.5 (1.3B) models. All tasks are evaluated in 0-shot except for BBH and MMLU which use 3-shot CoT and 5-shot, respectively.</figcaption></figure>



<h2 id="training-details">Training Details</h2>



<p>Phi-2 is a Transformer-based model with a next-word prediction objective, trained on 1.4T tokens from multiple passes on a mixture of Synthetic and Web datasets for NLP and coding. The training for Phi-2 took 14 days on 96 A100 GPUs. Phi-2 is a base model that has not undergone alignment through reinforcement learning from human feedback (RLHF), nor has it been instruct fine-tuned. Despite this, we observed better behavior with respect to toxicity and bias compared to existing open-source models that went through alignment (see Figure 3). This is in line with what we saw in Phi-1.5 due to our tailored data curation technique, see our <a href="https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need-ii-phi-1-5-technical-report/">previous tech report<span> (opens in new tab)</span></a> for more details on this. For more information about the Phi-2 model, please visit <a href="https://ml.azure.com/registries/azureml-msr/models/microsoft-phi-2/version/3?tid=72f988bf-86f1-41af-91ab-2d7cd011db47#overview" target="_blank" rel="noreferrer noopener">Azure AI | Machine Learning Studio<span> (opens in new tab)</span></a>.</p>



<figure><img decoding="async" width="16803" height="8165" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores.png" alt="A barplot comparing the safety score of Phi-1.5, Phi-2, and Llama-7B models on 13 categories of the ToxiGen benchmark. Phi-1.5 achieves the highest score on all categories, Phi-2 achieves the second-highest scores and Llama-7B achieves the lowest scores across all categories. " srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores.png 16803w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-300x146.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-1024x498.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-768x373.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-1536x746.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-2048x995.png 2048w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/figure3_safety_scores-240x117.png 240w" sizes="(max-width: 16803px) 100vw, 16803px"><figcaption><strong>Figure 3. </strong>Safety scores computed on 13 demographics from ToxiGen. A subset of 6541 sentences are selected and scored between 0 to 1 based on scaled perplexity and sentence toxicity. A higher score indicates the model is less likely to produce toxic sentences compared to benign ones.</figcaption></figure>



<h2 id="phi-2-evaluation">Phi-2 Evaluation</h2>



<p>Below, we summarize Phi-2 performance on academic benchmarks compared to popular language models. Our benchmarks span several categories, namely, Big Bench Hard (BBH) (3 shot with CoT), commonsense reasoning (PIQA, WinoGrande, ARC easy and challenge, SIQA), language understanding (HellaSwag, OpenBookQA, MMLU (5-shot), SQuADv2 (2-shot), BoolQ), math (GSM8k (8 shot)), and coding (HumanEval, MBPP (3-shot)).</p>



<p>With only 2.7 billion parameters, Phi-2 surpasses the performance of Mistral and Llama-2 models at 7B and 13B parameters on various aggregated benchmarks. Notably, it achieves better performance compared to 25x larger Llama-2-70B model on muti-step reasoning tasks, i.e., coding and math. Furthermore, Phi-2 matches or outperforms the recently-announced Google Gemini Nano 2, despite being smaller in size.</p>



<p>Of course, we acknowledge the current challenges with model evaluation, and that many public benchmarks might leak into the training data. For our first model, Phi-1, we did an extensive decontamination study to discard this possibility, which can be found in our first report “<a href="https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/">Textbooks Are All You Need</a>.” Ultimately, we believe that the best way to judge a language model is to test it on concrete use cases. Following that spirit, we also evaluated Phi-2 using several Microsoft internal proprietary datasets and tasks, comparing it again to Mistral and Llama-2. We observed similar trends, i.e. on average, Phi-2 outperforms Mistral-7B, and the latter outperforms the Llama-2 models (7B, 13B, and 70B).</p>



<figure><table><thead><tr><th>Model</th><th>Size</th><th>BBH</th><th>Commonsense<br>Reasoning</th><th>Language<br>Understanding</th><th>Math</th><th>Coding</th></tr></thead><tbody><tr><td rowspan="3">Llama-2</td><td>7B</td><td>40.0</td><td>62.2</td><td>56.7</td><td>16.5</td><td>21.0</td></tr><tr><td>13B</td><td>47.8</td><td>65.0</td><td>61.9</td><td>34.2</td><td>25.4</td></tr><tr><td>70B</td><td>66.5</td><td>69.2</td><td>67.6</td><td>64.1</td><td>38.3</td></tr><tr><td>Mistral</td><td>7B</td><td>57.2</td><td>66.4</td><td>63.7</td><td>46.4</td><td>39.4</td></tr><tr><td>Phi-2</td><td>2.7B</td><td>59.2</td><td>68.8</td><td>62.0</td><td>61.1</td><td>53.7</td></tr></tbody></table><figcaption><center><strong>Table 1.</strong> Averaged performance on grouped benchmarks compared to popular open-source SLMs.</center></figcaption></figure>



<figure><table><thead><tr><th>Model</th><th>Size</th><th>BBH</th><th>BoolQ</th><th>MBPP</th><th>MMLU</th></tr></thead><tbody><tr><td>Gemini Nano 2</td><td>3.2B</td><td>42.4</td><td>79.3</td><td>27.2</td><td>55.8</td></tr><tr><td>Phi-2</td><td>2.7B</td><td>59.3</td><td>83.3</td><td>59.1</td><td>56.7</td></tr></tbody></table><figcaption><center><strong>Table 2.</strong> Comparison between Phi-2 and Gemini Nano 2 Model on Gemini’s reported benchmarks.</center></figcaption></figure>



<p>In addition to these benchmarks, we also performed extensive testing on commonly used prompts from the research community. We observed a behavior in accordance with the expectation we had given the benchmark results. For example, we tested a prompt used to probe a model’s ability to solve physics problems, most recently used to evaluate the capabilities of the Gemini Ultra model, and achieved the following result:</p>



<figure><img decoding="async" width="1347" height="758" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4.png" alt="An example prompt is given to Phi-2 which says “A skier slides down a frictionless slope of height 40m and length 80m. What's the skier’s speed at the bottom?”. Phi-2 then answers the prompt by explaining the conversion of potential energy to kinetic energy and providing the formulas to compute each one. It then proceeds to compute the correct speed using the energy formulas. " srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4.png 1347w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-300x169.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-1024x576.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-768x432.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-1066x600.png 1066w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-655x368.png 655w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-343x193.png 343w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-240x135.png 240w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-640x360.png 640w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-960x540.png 960w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig4-1280x720.png 1280w" sizes="(max-width: 1347px) 100vw, 1347px"><figcaption><strong>Figure 4. </strong>Phi-2’s output on a simple physics problem, which includes an approximately correct square root calculation.</figcaption></figure>



<figure><img decoding="async" width="1600" height="710" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5.png" alt="The model is then provided with a student’s wrong answer to the skier physics problem and asked if it can correct the student’s mistake. Phi-2 replies with the student’s mistake, i.e., using the wrong formula for potential energy, and provides the correct formula. " srcset="https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5.png 1600w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5-300x133.png 300w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5-1024x454.png 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5-768x341.png 768w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5-1536x682.png 1536w, https://www.microsoft.com/en-us/research/uploads/prod/2023/12/Seb_Fig5-240x107.png 240w" sizes="(max-width: 1600px) 100vw, 1600px"><figcaption><strong>Figure 5. </strong>Similarly to Gemini’s test we also further queried Phi-2 with a student’s wrong answer to see if Phi-2 could identify where the mistake is (it did, despite Phi-2 being not fine-tuned for chat or instruction-following). We note however that it is not fully an apple-to-apple comparison with the Gemini Ultra’s output described in the Gemini report, in particular in the latter case the student’s answer was given as an image with handwritten text rather than raw text in our case.</figcaption></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new old kind of R&D lab (239 pts)]]></title>
            <link>https://www.answer.ai/posts/2023-12-12-launch.html</link>
            <guid>38614232</guid>
            <pubDate>Tue, 12 Dec 2023 16:19:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.answer.ai/posts/2023-12-12-launch.html">https://www.answer.ai/posts/2023-12-12-launch.html</a>, See on <a href="https://news.ycombinator.com/item?id=38614232">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<section id="tldr">
<h2 data-anchor-id="tldr">tl;dr</h2>
<p>Jeremy Howard (founding CEO, previously co-founder of Kaggle and fast.ai) and Eric Ries (founding director, previously creator of Lean Startup and the Long-Term Stock Exchange) today launched Answer.AI, a new kind of AI R&amp;D lab which creates practical end-user products based on foundational research breakthroughs. The creation of Answer.AI is supported by an investment of USD10m from Decibel VC. Answer.AI will be a fully-remote team of deep-tech generalists—the world’s very best, regardless of where they live, what school they went to, or any other meaningless surface feature.</p>
</section>
<section id="a-new-rd-lab">

<p>In 1831 Michael Faraday showed the world how to harness electricity. Suddenly there was, quite literally, a new source of power in the world. He later found the basis of the unification of light and magnetism, and knew he was onto something big:</p>
<blockquote>
<p><em>“I happen to have discovered a direct relation between magnetism and light, also electricity and light, and the field it opens is so large and I think rich.”</em> Michael Faraday; letter to Christian Schoenbein</p>
</blockquote>
<p>But it wasn’t quite clear how to harness this power. What kinds of products and services could now be created that couldn’t before? What could now be made far cheaper, more efficient, and more accessible? One man set out to understand this, and in 1876 he put together a new kind of R&amp;D lab, which he called the “Invention Lab”: a lab that would figure out the fundamental research needed to tame electricity, and the applied development needed to make it useful in practice.</p>
<p>You might have heard of the man: his name was Thomas Edison. And the organization he created turned into a company you would know: General Electric.</p>
<p>Today, we find ourselves in a similar situation. There’s a new source of power in the world—artificial intelligence. And, like before, it’s not quite clear how to harness this power. Where are all the AI-powered products and services that make our lives and work dramatically easier and more pleasant?</p>
<p>To create these AI-powered products and services, we’ve created a new R&amp;D lab, called <strong>Answer.AI</strong>. Answer.AI will figure out the fundamental <em>research</em> needed to tame AI, and the <em>development</em> path needed to make it useful in practice.</p>
</section>
<section id="an-iterative-path-to-harnessing-ai">
<h2 data-anchor-id="an-iterative-path-to-harnessing-ai">An iterative path to harnessing AI</h2>
<p>Harnessing AI requires not just low-level computer science and mathematical research, but also deep thinking about what practical applications can take advantage of this new power. The “D” in “R&amp;D” is critical: it’s only by considering the <em>development</em> of practical applications that the correct <em>research</em> directions can be targeted.</p>
<p>That’s why Answer.AI is built on the work of experts in both research <em>and</em> development. Co-founders Jeremy Howard (that’s me!) and Eric Ries have created pioneering ideas in each of these areas. I co-founded fast.ai, where I have worked for the last 7 years on <strong>research</strong> into how to best make AI more accessible, particularly through <em>transfer learning</em> and <em>fine tuning</em>. I’ve been working with machine learning for over 30 years, including creating the ULMFiT method of fine-tuning large language models which is used as the basis of all popular language models today, including OpenAI’s ChatGPT and Google’s Gemini. I have developed the longest running online courses on Deep Learning in the world, in which I show students how to start with simple models and then iteratively improve them all the way to the state of the art.</p>
<p>I’ve known Eric for years, and there’s no-one I trust or respect more, which is why I asked him to serve as the founding director of Answer.AI. Eric has dedicated the last 10 years of his life to improving how companies operate, serve customers, and are governed. He is the creator of the Lean Startup movement, which is the basis of how most startups build products and scale their organizations. His work focuses on <strong>development</strong>: how can organizations go from an idea to a sustainable, mission-driven, and profitable product in practice. One of his key insights was to create and then iteratively improve a <em>Minimal Viable Product</em> (MVP).</p>
<p>I asked Eric for his thoughts on Answer.AI’s unique approach to R&amp;D, and he summarised better than I ever could, so I’ll just quote his reply here directly:</p>
<blockquote>
<p><em>“People think that the order is research→development, and that therefore an R&amp;D lab does “R” and then “D”. That is, the research informs the development, and so being practical means having researchers and developers. But this is wrong, and leads to a lot of bad research, because development should inform research and vice-versa. So having development goals is a way to do more effective research, if you set that out as your north star.”</em></p>
</blockquote>
<p>Eric is also an expert on governance and how companies should be led in order to align profit and increased human flourishing. He created the <a href="https://ltse.com/">Long-Term Stock Exchange</a> (LTSE), the first fundamentally new US Stock Exchange in over 50 years. LTSE mandates that listed companies and likeminded investors work towards long-term value, rather than just short-term profit maximization. Eric serves as the Chairman of LTSE, meaning he is not only up to date on the right long-term governance frameworks, but on the cutting edge of inventing new systems.</p>
<p>It will take years for Answer.AI to harness AI’s full potential, which requires the kind of strategic foresight and long-term tenacity which is hard to maintain in today’s business environment. Eric has been writing a book on exactly this topic, and his view is that the key foundation is to have the right corporate governance in place. He’s helped me ensure that Answer.AI will always reflect my vision and strategy for harnessing AI. We’re doing this by by setting up a for-profit organization that focuses on <em>long-term</em> impact. After all, over a long-enough timeframe, maximizing shareholder value and maximizing societal benefits are entirely aligned.</p>
<p>Whilst Eric and I bring very different (and complementary) skills and experiences to the table, we bring the same basic idea of how to solve really hard problems: solve smaller easier problems in simple ways first, and create a ladder where each rung is a useful step of itself, whilst also getting a little closer to the end goal.</p>
</section>
<section id="our-research-platform">
<h2 data-anchor-id="our-research-platform">Our research platform</h2>
<p>Companies like OpenAI and Anthropic have been working on developing Artificial General Intelligence (AGI). And they’ve done an astonishing job of that — we’re now at the point where experts in the field are claiming that “<a href="https://www.noemamag.com/artificial-general-intelligence-is-already-here/">Artificial General Intelligence Is Already Here</a>”.</p>
<p>At Answer.AI we are not working on building AGI. Instead, our interest is in effectively using the models that already exist. Figuring out what practically useful applications can be built on top of the foundation models that already exist is a huge undertaking, and I believe it is receiving insufficient attention.</p>
<p>My view is that the right way to build Answer.AI’s R&amp;D capabilities is by bringing together a very small number of curious, enthusiastic, technically brilliant generalists. Having huge teams of specialists creates an enormous amount of organizational friction and complexity. But with the help of modern AI tools I’ve seen that it’s possible for a single generalist with a strong understanding of the foundations to create effective solutions to challenging problems, using unfamiliar languages, tools, and libraries (indeed I’ve done this myself many times!) I think people will be very surprised to discover what a small team of nimble, creative, open-minded people can accomplish.</p>
<p>At Answer.AI we will be doing genuinely original research into questions such as how to best fine-tune smaller models to make them as practical as possible, and how to reduce the constraints that currently hold back people from using AI more widely. We’re interested in solving things that may be too small for the big labs to care about-—but our view is that it’s the collection of these small things matter a great deal in practice.</p>
<p>This informs how we think about safety. Whilst AI is becoming more and more capable, the dangers to society from poor algorithmic decision making have been with us for years. We believe in learning from these years of experience, and thinking deeply about how to align the <em>applications</em> of models with the needs of people today. At fast.ai three years ago we created a pioneering course on <a href="https://ethics.fast.ai/">Practical Data Ethics</a>, as well as dedicating <a href="https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb">a chapter of our book</a> to these issues. We are committed to continuing to work towards ethical and beneficial applications of AI.</p>
</section>
<section id="from-fast.ai-to-answer.ai">
<h2 data-anchor-id="from-fast.ai-to-answer.ai">From fast.ai to Answer.AI</h2>
<p>Rachel Thomas and I realised over seven years ago that deep learning and neural networks were on their way to becoming one of the most important technologies in history, but they were also on their way to being controlled and understood by a tiny exclusive sliver of society. We were worried about centralization and control of something so critical, so we founded fast.ai with the mission of making AI more accessible.</p>
<p>We succeeded beyond our wildest dreams, and today fast.ai’s AI courses are the longest-running, and perhaps most loved, in the world. We built the first library to make PyTorch easier to use and more powerful (fastai), built the fastest image model training system in the world (according to the Dawnbench competition), and created the 3-step training methodology now used by all major LLMs (ULMFiT). Everything we have created for the last 7 years was free—fast.ai was an entirely altruistic endeavour in which everything we built was gifted to everybody.</p>
<p>I’m now of the opinion that this is the time for rejuvenation and renewal of our mission. Indeed, the mission of Answer.AI is the same as fast.ai: to make AI more accessible. But the <em>method</em> is different. Answer.AI’s method will be to <em>use</em> AI to create all kinds of products and services that are really valuable and useful in practice. We want to research new ways of building AI products that serve customers that can’t be served by current approaches.</p>
<p>This will allow us to make money, which we can use to expand into more and bigger opportunities, and use to drive down costs through better efficiency, creating a positive feedback loop of more and more value from AI. We’ll be spending all our time looking at how to make the market size bigger, rather than how to increase our share of it. There’s no moat, and we don’t even care! This goes to the heart of our key premise: creating a long-term profitable company, and making a positive impact on society overall, can be entirely aligned goals.</p>
</section>
<section id="we-dont-really-know-what-were-doing">
<h2 data-anchor-id="we-dont-really-know-what-were-doing">We don’t really know what we’re doing</h2>
<p>If you’ve read this far, then I’ll tell you the honest truth: we don’t actually know what we’re doing. Artificial intelligence is a vast and complex topic, and I’m very skeptical of anyone that claims they’ve got it all figured out. Indeed, Faraday felt the same way about electricity—he wasn’t even sure it was going to be of any import:</p>
<blockquote>
<p><em>“I am busy just now again on Electro-Magnetism and think I have got hold of a good thing but can’t say; it may be a weed instead of a fish that after all my labour I may at last pull up.”</em> Faraday 1931 letter to R. Phillips</p>
</blockquote>
<p>But it’s OK to be uncertain. Eric and I believe that the best way to develop valuable stuff built on top of modern AI models is to try lots of things, see what works out, and then gradually improve bit by bit from there.</p>
<p>As Faraday said, “A man who is certain he is right is almost sure to be wrong.” Answer.AI is an R&amp;D lab for people who aren’t certain they’re right, but they’ll work damn hard to get it right eventually.</p>
<p>This isn’t really a new kind of R&amp;D lab. Edison did it before, nearly 150 years ago. So I guess the best we can do is to say it’s a new old kind of R&amp;D lab. And if we do as well as GE, then I guess that’ll be pretty good.</p>


</section>

</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tacit Knowledge Is Dangerous (118 pts)]]></title>
            <link>https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/</link>
            <guid>38614195</guid>
            <pubDate>Tue, 12 Dec 2023 16:17:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/">https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/</a>, See on <a href="https://news.ycombinator.com/item?id=38614195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
   
  <div><p>Tacit knowledge, often called “tribal knowledge” in tech, is prevalent in this industry. Documentation is a common afterthought and is frequently wrong, out of date, or lacking crucial information. New hires join a company and go through onboarding exercises intended to have them learn by doing. Often that learning means asking others when they get stuck. It becomes natural for an engineer to end up having key information in their head. When others need it, the engineer freely shares it. Until, the inevitable happens.</p>
<p><img loading="lazy" src="https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/out_of_office.png" alt="Colleague out of office">
</p>
<blockquote>
<p>You need to know something not written down. You won’t be able to get an answer for a long time from the person who knows it best.</p>
</blockquote>
<p>The person who knows what you need is not there. Maybe they left the company, maybe they are on vacation. You may really need to know about the whizbang service, but it’s been 4 years since anyone last worked on it and no-one remembers how it works. You’ve now fallen into the trap of tacit knowledge.</p>
<p>It’s easy, even efficient, to rely on tacit knowledge early on. It is often called tribal knowledge because it’s shared by people close to you, the proverbial tribe. This passing on of knowledge helps bond junior and senior members. The tribe passes this knowledge from seniors to juniors via rituals of song and dance. These songs will take form of video calls, ☕chats, instant message exchanges. Dances transmit information by moving the hand to ctrl+c from a notebook of useful commands, and pressing ctrl+v to send to a colleague. Other more advanced dances involve connecting to a colleagues machine to fix an issue or show a manual setup process that isn’t written down. These song and dance routines will hit their limit at some point.</p>
<p><img loading="lazy" src="https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/graph_relative_time.png" alt="Relative time scales for learning">
</p>
<blockquote>
<p>Graph showing relative time scales for different ways of learning something at work.</p>
</blockquote>
<p>Where sharing tribal knowledge breaks down is at scale. It’s faster to shoot someone who knows the answer a message than to read the documentation, but only if that wise sage can respond quickly. If the holder of the knowledge is busy, in another timezone and you missed the end of their day, or are on vacation, you are out of luck until they return. The timezone problem is particularly painful. 3:30 pm pacific has 1.5 hours left in the 9 - 5 working day. But if you need help from someone on the east coast of the US, it’s 6:30 pm and they have gone home. That same time is 8 am in Australia, and 4 am in India. If you need help from someone on the other side of the world, you’ll be waiting for a while. This doesn’t even attempt to account for different countries having their own set of national holidays.</p>
<p>As a company scales, reliance on tacit knowledge becomes more of a burden. The amount of time it takes to answer a question is a blocker and needing to ask someone begins to consume more time due to both timezones as well as the busy nature of some senior people.</p>
<p><img loading="lazy" src="https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/graph_getting_answers.png" alt="Graph showing time to get answers">
</p>
<blockquote>
<p>Graph showing how it takes a long time to get answers when distributed. Asking a fourth question doesn’t even fit on the “large, distributed” row.</p>
</blockquote>
<p>The longer it takes to get answers, the longer an engineer is blocked. These blockers begin to act as a drag on the ability of an engineer to be productive, and at scale slows down the company itself. Features took longer to ship, bugs take longer to fix. The company itself becomes less nimble and vulnerable to smaller players. This situation always existed, but was exacerbated by COVID and the rise of remote work. Coworkers that used to sit next to each other would now work from home. People moved to different cities, and sometimes moved across timezones. How then, can one try to regain productivity?</p>
<p><img loading="lazy" src="https://er4hn.info/blog/2023.08.26-tacit-knowledge-dangerous/scaling_middle_ground.png" alt="Having docs and videos is the middle ground of scaling">
</p>
<blockquote>
<p>Graph showing how reading docs and watching videos is still slower than asking people who know (and can respond quickly) but far faster than waiting for large, distributed teams to reply to you.</p>
</blockquote>
<p>The answer lies in the original chart: As teams scale up and knowledge becomes more distributed, it is faster to read documentation and watch video lectures than it is to ask others for help. Personally, I am an avid reader and prefer reading (and grepping) for what I need to know. Others learn topics better through video lectures and that is fine. Both have a place, but documentation should always be there because it is easily searchable and easy to reference. By investing the time to create a library of information, both written and video, a middle ground is reached. It will never have that small co-located team efficiency, but it will scale far better than anything relying on tacit knowledge.</p>
<p>For those who need help in getting started with writing documentation, a previous blog post of mine discusses that as well. <a href="https://er4hn.info/blog/2023.07.22-good_docs_great_effort/">Good Docs Take Great Effort</a> is where I would suggest anyone start when they need to figure out how to write documentation or see where they may improve.</p>


  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Telecom Industry Is Mad Because the FCC Might Examine High Broadband Prices (457 pts)]]></title>
            <link>https://www.techdirt.com/2023/12/12/the-telecom-industry-is-very-mad-because-the-fcc-might-examine-high-broadband-prices/</link>
            <guid>38613552</guid>
            <pubDate>Tue, 12 Dec 2023 15:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/12/12/the-telecom-industry-is-very-mad-because-the-fcc-might-examine-high-broadband-prices/">https://www.techdirt.com/2023/12/12/the-telecom-industry-is-very-mad-because-the-fcc-might-examine-high-broadband-prices/</a>, See on <a href="https://news.ycombinator.com/item?id=38613552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-427205">


<h3>from the <i>to-the-fainting-couch</i> dept</h3>

<p>We’ve long noted how the FCC (regardless of party) <a href="https://www.techdirt.com/2023/09/20/as-net-neutrality-debate-reheats-remember-the-real-problem-is-telecom-monopoly-power/">largely ignores</a> how muted competition and monopolization drives up prices for consumers. The agency often talks a good (if ambiguous) game about “bridging the digital divide,” but they don’t collect and share pricing data proving market failure, nor are they capable of admitting monopolies exist and are harmful in public-facing messaging.</p>
<p>As part of the Communications Act, the FCC is tasked with giving periodic reports on whether broadband is being delivered to all Americans on a “reasonable and timely basis.” If the answer is no, the regulator is theoretically supposed to, you know, actually do something about it. </p>
<p>Back in November, the agency issued a <a href="https://docs.fcc.gov/public/attachments/FCC-23-89A1.pdf">Notice of Inquiry (NOI)</a> pondering whether they should more seriously analyze the cost of broadband when making those determinations (yes, duh). As per tradition, the FCC NOI is pretty vague about things, but does acknowledge the importance of affordability:</p>
<blockquote>
<p><em>“To truly close the connectivity gap and ensure that all Americans have access to advanced telecommunications capability, broadband services must be affordable.”</em></p>
</blockquote>
<p>Keep in mind this is just the FCC saying they’re <strong>thinking</strong> about taking a more consistent look at high broadband prices as part of their policy approach. It doesn’t mean they’ll actually do it, do it well, or punish any of the companies found to be monopolizing access and squashing competition to jack up market prices. </p>
<p>But even the faint possibility of the FCC looking at expensive U.S. broadband has been enough to <a href="https://arstechnica.com/tech-policy/2023/12/cable-lobby-to-fcc-please-dont-look-too-closely-at-the-prices-we-charge/">send telecom lobbyists into a tizzy</a>, with cable lobbying organizations arguing in filings that <strong>even asking the question</strong> is “inappropriate”:</p>
<blockquote>
<p><em>“While the Commission has reiterated that it has no interest in any kind of rate regulation, the proposal to make a traditional deployment analysis contingent on whether the Commission determines that broadband pricing is sufficiently affordable suggests that rate regulation in some form is potentially on the table.”</em></p>
</blockquote>
<p>The majority of Americans live under a monopoly or duopoly for broadband access protected by state and federal corruption. This muted competition consistently results in spotty coverage, high prices, slow speeds, and comically terrible customer service. And again, FCC officials can’t even openly admit there’s a monopoly/duopoly problem, much less field concrete solutions to the problem.</p>
<p>Keep in mind, between the Trump era and the first two years of Biden term (where the FCC lacked a voting majority due to the <a href="https://www.techdirt.com/2023/03/07/telecom-monopolies-win-again-gigi-sohn-forced-to-withdraw-from-fcc-nomination/">attacks on Gigi Sohn</a>), the FCC was basically a cardboard cutout for <strong>six straight years</strong>. The telecom industry has grown fat and comfortable with the FCC performing regulatory theater where its functions are entirely decorative. A sort of regulatory simulacrum. </p>
<p>The fact that it’s 2023 and the FCC and NTIA have <em>only fairly recently</em> realized they should be considering affordability in broadband access policy genuinely speaks for itself. </p>
<p>And in the U.S., where Comcast, Verizon and AT&amp;T dictate the lion’s share of all telecom policy, the idea of rate regulation is treated as the most extreme possibility imaginable. Having a regulator ponder things like affordable wholesale access or any sort of rate caps is genuinely treated the same way you’d treat a batshit, naked streaker in a public mall. </p>
<p>But the FCC is making it very clear they lack the political backbone to get anywhere near price regulation. The net neutrality restoration similarly <a href="https://www.techdirt.com/2023/09/26/biden-fcc-prepares-to-restore-net-neutrality-but-the-details-will-matter/">makes it very clear</a> that price regulation is off the table. But the simple act of even looking more closely at pricing data — so that the public has a clearer understanding of the impact of muted competition — is apparently a bridge too far for unchecked monopolists. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/broadband-access/" rel="tag">broadband access</a>, <a href="https://www.techdirt.com/tag/broadband-prices/" rel="tag">broadband prices</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/prices/" rel="tag">prices</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe frantically changed its terms of service to prevent customers from suing (684 pts)]]></title>
            <link>https://www.engadget.com/23andme-frantically-changed-its-terms-of-service-to-prevent-hacked-customers-from-suing-152434306.html</link>
            <guid>38613386</guid>
            <pubDate>Tue, 12 Dec 2023 15:27:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/23andme-frantically-changed-its-terms-of-service-to-prevent-hacked-customers-from-suing-152434306.html">https://www.engadget.com/23andme-frantically-changed-its-terms-of-service-to-prevent-hacked-customers-from-suing-152434306.html</a>, See on <a href="https://news.ycombinator.com/item?id=38613386">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main role="main"><div id="module-moreStories" data-wf-sticky-offset="130px   10px" data-wf-sticky-position="" data-wf-sticky-target="#adsStream" data-wf-trigger-percentage=""><ul><li></li><li></li><li></li><li><div><p><img src="https://s.yimg.com/cv/apiv2/default/20190501/placeholder.gif" data-wf-src="https://s.yimg.com/uu/api/res/1.2/4AAmDQc1Hurx05ilepR_1w--~B/Zmk9c3RyaW07aD0yODg7dz01MTQ7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2022-06/c5bf5d00-e68e-11ec-95df-b648b59c988b" alt=""></p><div><p><span>Engadget</span></p><h4><a data-uuid="5d2d808a-7a1d-45a8-98f3-d598c17a306e" href="https://www.engadget.com/e3-is-officially-dead-153411735.html" data-ylk="elm:hdln;itc:0;pos:1;sec:strm;subsec:moreforyou;cpos:4;ct:story;g:5d2d808a-7a1d-45a8-98f3-d598c17a306e" data-hosted-type="HOSTED">E3 is officially dead</a></h4><p>The long-running video game expo E3 is officially dead. “It’s the right thing to do given the new opportunities our industry has to reach fans and partners," ESA president and CEO Stanley Pierre-Louis said.</p></div></div></li><li></li><li><div><p><img src="https://s.yimg.com/cv/apiv2/default/20190501/placeholder.gif" data-wf-src="https://s.yimg.com/uu/api/res/1.2/4aL3y8ImTGO1pYQXFNt19g--~B/Zmk9c3RyaW07aD0yODg7dz01MTQ7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2023-12/be2702f0-9899-11ee-8ebf-216ad9982e61" alt=""></p></div></li><li></li><li><div><p><img src="https://s.yimg.com/cv/apiv2/default/20190501/placeholder.gif" data-wf-src="https://s.yimg.com/uu/api/res/1.2/XrkRkC7LMLpCM9QjqA3enQ--~B/Zmk9c3RyaW07aD0yODg7dz01MTQ7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2021-04/e0b47780-92ed-11eb-bffb-1cf00cf593cb" alt=""></p><div><p><span>Engadget</span></p><h4><a data-uuid="2b641319-47ec-4d6a-b43c-558615f1cffe" href="https://www.engadget.com/best-projectors-123004354.html" data-ylk="elm:hdln;itc:0;pos:1;sec:strm;subsec:moreforyou;cpos:8;ct:story;g:2b641319-47ec-4d6a-b43c-558615f1cffe" data-hosted-type="HOSTED">The best projectors for 2024</a></h4><p>Here's a list of the best projectors you can buy at all price points, as well as tips and tricks for shopping for a projector.</p></div></div></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li><li><div><p><img src="https://s.yimg.com/cv/apiv2/default/20190501/placeholder.gif" data-wf-src="https://s.yimg.com/uu/api/res/1.2/agdChAB8yfUX9vWgLpE0LA--~B/Zmk9c3RyaW07aD0yODg7dz01MTQ7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2023-12/9109a0f0-985a-11ee-97ed-23c527178de7" alt=""></p><div><p><span>Engadget</span></p><h4><a data-uuid="324323be-72e4-40f8-85bc-e333267e85c6" href="https://www.engadget.com/timesplitters-studio-free-radical-design-has-shut-down-194718243.html" data-ylk="elm:hdln;itc:0;pos:1;sec:strm;subsec:moreforyou;cpos:19;ct:story;g:324323be-72e4-40f8-85bc-e333267e85c6" data-hosted-type="HOSTED">TimeSplitters studio Free Radical Design has shut down</a></h4><p>Free Radical Design, the company tasked to create a TimeSplitters reboot, has closed its doors. Additionally, the developer’s official website now redirects to a 404 error, along with text reading “company not found” and a sad face.</p></div></div></li><li><div><p><img src="https://s.yimg.com/cv/apiv2/default/20190501/placeholder.gif" data-wf-src="https://s.yimg.com/uu/api/res/1.2/n07QvRJup..4h3lnil806Q--~B/Zmk9c3RyaW07aD0yODg7dz01MTQ7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2022-08/826785b0-296d-11ed-aeb1-d30d9e5ff594" alt=""></p><div><p><span>Engadget</span></p><h4><a data-uuid="e9f1a4a2-1a52-461c-91e1-85eb7e19f509" href="https://www.engadget.com/best-fast-chargers-140011033.html" data-ylk="elm:hdln;itc:0;pos:1;sec:strm;subsec:moreforyou;cpos:20;ct:story;g:e9f1a4a2-1a52-461c-91e1-85eb7e19f509" data-hosted-type="HOSTED">The best fast chargers for 2024</a></h4><p>In order to figure out what the fastest charger on the market is across a range of power outputs, we tested 14  adapters across five different devices. Here are the results.</p></div></div></li></ul></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Active turbulence cancellation makes bumpy flights 80% smoother (210 pts)]]></title>
            <link>https://newatlas.com/aircraft/active-turbulence-cancellation/</link>
            <guid>38613280</guid>
            <pubDate>Tue, 12 Dec 2023 15:19:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/aircraft/active-turbulence-cancellation/">https://newatlas.com/aircraft/active-turbulence-cancellation/</a>, See on <a href="https://news.ycombinator.com/item?id=38613280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Austrian company Turbulence Solutions claims it's already flight-testing a system that can detect and neutralize air turbulence, reducing the forces felt by passengers by up to 80% – and reducing fuel burn by up to 10%. It's set to launch in 2024. </p><p>Nobody likes to fly through turbulence – which is tough luck, because climate change has <a href="https://www.bbc.com/news/science-environment-65844901" target="_blank" data-cms-ai="0">already caused it to increase by as much as 55%</a> on some popular routes, and it's projected to continue getting worse as temperatures rise, bringing wind speeds and thus wind shear along with them. </p><p>Where it can be accurately predicted, airliners will often go out of their way to go around it, hoping to avoid a whole lot of passenger distress as well as showers of vomit. But in clear air, it's nigh-on impossible to spot turbulence until you're in the middle of it, guts in your throat and praying for a quick death. </p><p>Enter Turbulence Solutions, which claims to have built and tested something that solves the problem, acting a little like an active noise cancellation system in a pair of headphones. Effectively, it detects turbulence just before it happens, and uses super-quick automated lift adjustment through the aircraft's control surfaces to generate forces in opposition to the turbulence. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Unmanned test aircraft" width="967" height="599" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/6ec2a58/2147483647/strip/true/crop/967x599+0+0/resize/440x273!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 440w,https://assets.newatlas.com/dims4/default/3d41bd7/2147483647/strip/true/crop/967x599+0+0/resize/800x496!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 800w,https://assets.newatlas.com/dims4/default/044a4e9/2147483647/strip/true/crop/967x599+0+0/resize/1200x743!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 1200w,https://assets.newatlas.com/dims4/default/8edcba6/2147483647/strip/true/crop/967x599+0+0/resize/1920x1189!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/3588880/2147483647/strip/true/crop/967x599+0+0/resize/967x599!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/6ec2a58/2147483647/strip/true/crop/967x599+0+0/resize/440x273!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 440w,https://assets.newatlas.com/dims4/default/3d41bd7/2147483647/strip/true/crop/967x599+0+0/resize/800x496!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 800w,https://assets.newatlas.com/dims4/default/044a4e9/2147483647/strip/true/crop/967x599+0+0/resize/1200x743!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 1200w,https://assets.newatlas.com/dims4/default/8edcba6/2147483647/strip/true/crop/967x599+0+0/resize/1920x1189!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg 1920w" src="https://assets.newatlas.com/dims4/default/3588880/2147483647/strip/true/crop/967x599+0+0/resize/967x599!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Fc7%2Fe3%2Fa02c22154695b7c9f29d016800fe%2F13272-2021-512-fig7-html.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Unmanned test aircraft</figcaption><p>Turbulence Solutions</p></div>
    
</figure>

                
            </div><p>To predict what's about to hit the wings, the system uses 5-hole differential air pressure probes, mounted as far forward as possible. On the unmanned testbed above, for example, the company placed a pair of lightweight rails on front of the aircraft, holding up a third rail with the pressure sensors held out nearly as far as the wing tips. </p><p>On the manned test aircraft, the sensors were instead mounted directly to the wings, on long pole masts that placed them some 2.65 m (8.69 ft) forward of the leading edges. At cruise speed, that's enough to give the system a tenth of a second's worth of advance warning before turbulence hits – and according to a paper published by the <i><a href="https://link.springer.com/article/10.1007/s13272-021-00512-y" target="_blank" data-cms-ai="0">CEAS Aeronautical Journal</a></i> in 2021, the system was able to predict vertical accelerations greater than 30 m/sec/sec with an accuracy of nearly 62% on its very first test flight, and it's doubtless improved since then. </p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Pressure sensors in front of the wings give the system a fraction of a second to respond to turbulence" width="1179" height="701" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/523c0ed/2147483647/strip/true/crop/1179x701+0+0/resize/440x262!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 440w,https://assets.newatlas.com/dims4/default/be53860/2147483647/strip/true/crop/1179x701+0+0/resize/800x476!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 800w,https://assets.newatlas.com/dims4/default/43e38b1/2147483647/strip/true/crop/1179x701+0+0/resize/1200x713!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 1200w,https://assets.newatlas.com/dims4/default/3665d44/2147483647/strip/true/crop/1179x701+0+0/resize/1920x1142!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 1920w" data-src="https://assets.newatlas.com/dims4/default/35359f2/2147483647/strip/true/crop/1179x701+0+0/resize/1179x701!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/523c0ed/2147483647/strip/true/crop/1179x701+0+0/resize/440x262!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 440w,https://assets.newatlas.com/dims4/default/be53860/2147483647/strip/true/crop/1179x701+0+0/resize/800x476!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 800w,https://assets.newatlas.com/dims4/default/43e38b1/2147483647/strip/true/crop/1179x701+0+0/resize/1200x713!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 1200w,https://assets.newatlas.com/dims4/default/3665d44/2147483647/strip/true/crop/1179x701+0+0/resize/1920x1142!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png 1920w" src="https://assets.newatlas.com/dims4/default/35359f2/2147483647/strip/true/crop/1179x701+0+0/resize/1179x701!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F65%2F24%2F05eece844b59a0b0f948be096723%2Fscreenshot-2023-12-11-at-7.11.51%20pm.png">
</p>



    
    

    
        <div><figcaption itemprop="caption">Pressure sensors in front of the wings give the system a fraction of a second to respond to turbulence</figcaption><p>Turbulence Solutions</p></div>
    
</figure>

                
            </div><p>Armed with this information, the flight control system now has a tenth of a second to generate a force-canceling response at each wing, by deftly actuating low-inertia lift surfaces. In this way, the system is able to iron out vertical accelerations, pitch and roll changes, and wing bending moments. The Turbulence Solutions team claims it currently cuts down the effects of turbulence in the cabin by around 80%.</p><p>You can see the system going bonkers to smooth out a bumpy manned test flight in the video below.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f497e6d32cbcc4106975dbbd640e3032a" data-video-id="4UfmsqtTGa0" data-video-title="Turbulence Cancelling Intro">

    <iframe id="YouTubeVideoPlayer-f497e6d32cbcc4106975dbbd640e3032a" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/4UfmsqtTGa0?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>Turbulence Cancelling Intro</p>
    
</div><p>In an interview with <i><a href="https://interestingengineering.com/innovation/turbulence-solutions-aviation-industry-free" target="_blank" data-cms-ai="0">Interesting Engineering</a></i>, a company representative said it also enables significant fuel savings of up to 10%, since aircraft no longer need to climb, dive, or route around turbulence. That's nothing to sniff at.</p><p>The company says it'll have a system commercially available for light aircraft in 2024. It's looking into a version for eVTOL air taxis by 2026, and hoping to have a system relevant to commercial airliners by 2030. Godspeed, team, the world's airline passengers – not to mention cleaning crews – need you to succeed. </p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f45793a9847f042a0ba965a24666e7ba9" data-video-id="9qydsN_acDs" data-video-title="Turbulence Solutions - Making flights turbulence-free">

    <iframe id="YouTubeVideoPlayer-f45793a9847f042a0ba965a24666e7ba9" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/9qydsN_acDs?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>Turbulence Solutions - Making flights turbulence-free</p>
    
</div><p>Source: <a href="https://turbulence-solutions.aero/technology/" target="_blank" data-cms-ai="0">Turbulence Solutions</a> via <i><a href="https://interestingengineering.com/innovation/turbulence-solutions-aviation-industry-free" target="_blank" data-cms-ai="0">Interesting Engineering</a></i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Lands CLI Multi-Threading as Its "Most Complex Refactoring" in Decades (511 pts)]]></title>
            <link>https://www.phoronix.com/news/FFmpeg-CLI-MT-Merged</link>
            <guid>38613219</guid>
            <pubDate>Tue, 12 Dec 2023 15:15:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/FFmpeg-CLI-MT-Merged">https://www.phoronix.com/news/FFmpeg-CLI-MT-Merged</a>, See on <a href="https://news.ycombinator.com/item?id=38613219">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="MULTIMEDIA" src="https://www.phoronix.com/assets/categories/multimedia.webp" width="100" height="100"></p><p>
The long-in-development work for <a href="https://www.phoronix.com/news/FFmpeg-CLI-Multi-Threaded">a fully-functional multi-threaded FFmpeg command line</a> has been merged! The FFmpeg CLI with multi-threaded transcoding pipelines is now merged to FFmpeg Git ahead of FFmpeg 7.0 releasing early next year. FFmpeg is widely-used throughout many industries for video transcoding and in today's many-core world this is a terrific improvement for this key open-source project.
</p><p>
In recently sharing a technical presentation on the FFmpeg multi-threading effort, FFmpeg developers <a href="https://twitter.com/FFmpeg/status/1731288541395587411">called</a> this work "<em>one of the most complex refactoring of the FFmpeg CLI in decades.</em>" And in calling for testing today <a href="https://twitter.com/FFmpeg/status/1734572553836851252">added</a>, "<em>Please test and report issues to [FFmpeg Trac] - this is one of the most complex changes in FFmpeg ever!</em>"
</p><p>
The code is now in <a href="https://github.com/FFmpeg/FFmpeg/commits/master">FFmpeg Git</a>. The patches include adding the thread-aware transcode scheduling infrastructure, moving encoding to a separate thread, and various other low-level changes. In culminating with converting FFmpeg to a threaded architecture is summed up as:
</p><blockquote>fftools/ffmpeg: convert to a threaded architecture
<p>
Change the main loop and every component (demuxers, decoders, filters, encoders, muxers) to use the previously added transcode scheduler. Every instance of every such component was already running in a separate thread, but now they can actually run in parallel.</p></blockquote>
<p><img src="https://www.phoronix.net/image.php?id=2023&amp;image=ffmpeg_cli_multithread" alt="FFmpeg threading"></p>
<p>There's a recent <a href="https://up.khirnov.net/7m.pdf">presentation</a> on this work by developer Anton Khirnov.
</p><p>
It's terrific seeing this merged and will be interesting to see the performance impact in practice.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Forget spaceships; I just want my music (234 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2023/forget-spaceships-i-just-want-my-music</link>
            <guid>38613154</guid>
            <pubDate>Tue, 12 Dec 2023 15:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2023/forget-spaceships-i-just-want-my-music">https://www.jeffgeerling.com/blog/2023/forget-spaceships-i-just-want-my-music</a>, See on <a href="https://news.ycombinator.com/item?id=38613154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A couple weeks ago, as my kids settled into the car, I asked like I always do, "what songs do you want me to play?"</p>

<p>They have a range of favored earworms, from Baby Shark to Babaloo, and usually the songs are tolerable, at least.</p>

<p>But a few albums, like Bluey's soundtrack, transcend the children's genre. They're genuinely fun to listen to, for everyone in the car.</p>

<p>Well, that fine day, the kids chose Ladybug Music. And let me tell you, besides a few duds, Ladybug Music <em>slaps</em>. And the songs incorporate diverse styles, too, it's not just the same nursery rhymes regurgitated in a bubbly voice.</p>

<p>So I found the album on my phone and noticed the songs were all greyed out.</p>

<p>I tapped one, and <em>nothing</em>. Just this notice:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/song-not-available.jpg" width="700" height="auto" alt="Song not available Apple Music on iPhone"></p>

<p>Not available in my region? Well, that's weird. I pay for Apple Music. And I know the artist is in the US, and <em>I'm</em> in the US...</p>

<p>But my kids were restless, and we had somewhere to be, so I put on Pinkfong for the drive. After I got home, I went online to investigate.</p>

<blockquote>
  <p>This blog post is a transcript of <a href="https://www.youtube.com/watch?v=7CDKvdlD6uQ">this video on my YouTube channel</a>.</p>
</blockquote>

<p>Unfortunately, Ladybug music isn't a huge artist, so there wasn't any news coverage, like "thousands of parents cry out as their favorite kids songs disappear from Apple Music!"</p>

<p>But I <em>did</em> find the <a href="https://ladybugmusic.com/">Ladybug Music website</a>.</p>

<p>And on that website, they have a <a href="https://ladybugmusic.com/shop/">shop</a>. Great! I can just buy the CDs, and support the artist directly!</p>

<p>And... no. Apparently Ladybug music is now an app. So I downloaded the App! Long story short, I created an account, and found out you either have to fly to LA and join one of the in-person kids music courses, or pay <a href="https://ladybugmusic.com/licensing-application/">$3000 in licensing fees</a> to get access to any of the songs.</p>

<blockquote>
  <p><strong>Note</strong>: I've also reached out to Ladybug Music using their contact form to ask about any other way to purchase the music, and have received no response.</p>
</blockquote>

<p>No bueno.</p>

<p>Scrounging around, you can get a few free songs from their YouTube or Soundcloud, and on Amazon a couple of used CDs are floating around... but there's literally no way to buy the music online anymore!</p>

<p>This isn't the first time I've seen a song disappear from Apple Music. And this kind of thing happens on Spotify and other streaming services too.</p>

<p>I can't get <em>too</em> mad, because I never actually "bought" those songs directly. I just rented them. That's the model for video streaming too, like on Netflix. Even though it's <em>super</em> annoying to have shows and music disappear, I <em>am</em> renting, so it's not like I have a <em>right</em> to anything.</p>

<h2>Playstation</h2>

<p>But that's just streaming. Luckily, you can also <em>buy</em> digital movies and music, and you get to keep it.</p>

<p>Right?</p>

<p><em>Right?</em></p>

<p>Wrong.</p>

<p>Enshittification has reached new heights, as <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/">Sony just decided to yoink over a <em>thousand seasons</em></a>—that's right, <em>seasons</em>, not <em>episodes</em>—of Discovery shows from the PlayStation Store.</p>

<p>But they're not just removing them from the <em>store</em>.</p>

<p>They're removing them from people's <em>libraries</em>.</p>

<blockquote>
  <p>As of 31 December 2023, due to our content licensing arrangements with content providers, you will no longer be able to watch any of your previously purchased Discovery content and the content will be removed from your video library. (<a href="https://www.playstation.com/en-us/legal/psvideocontent/">Source</a>)</p>
</blockquote>

<p>So if you paid full retail price for Mythbusters on your PlayStation, too bad. You can't watch it anymore.</p>

<p>That is, unless you go over and set up a new subscription to yet another completely unneccessary and broken streaming service from Warner Brothers.</p>

<p>The term for that is <a href="https://en.wikipedia.org/wiki/Enshittification"><em>enshittification</em></a>.</p>

<p>I'm not gonna rehash <a href="https://pluralistic.net/2023/12/08/playstationed/#tyler-james-hill">everything Cory Doctorow wrote</a> about this latest assault on digital ownership, but I'll summarize a few of the main things that get under my skin.</p>

<p>First, the headline: "If buying isn't owning, than piracy isn't stealing."</p>

<p>I made a whole video about <a href="https://www.youtube.com/watch?v=4VkY1vTpCJY">how I manage all my video content on my NAS</a>. I run software called Jellyfin, and I've ripped <em>hundreds</em> of movies and <em>thousands</em> of TV episodes over the years. My family can enjoy every movie and TV show I've bought, now and forever.</p>

<p>But <em>most</em> people don't do that. And even for crazies like me, it's getting harder. More and more TV series and movies are going stream-only.</p>

<p>Meaning the <em>only</em> way to ever watch it is to subscribe to yet another broken and temporary streaming service.</p>

<p>You'd think as the years wore on, society's access to media would get easier and better.</p>

<p>But instead, backwards-thinking execs in Hollywood and the music industry keep backpedaling, making it <em>harder</em> for you to actually watch the content they produce.</p>

<p>I mean, more and more of it is garbage anyway, but there are still compelling shows and movies out there. It's a shame they're committed to being so anti-consumer by killing off any semblance of ownership.</p>

<p>In my video about Jellyfin, I danced around the issue of piracy. Mostly because I know how pernicious the MPAA and RIAA are.</p>

<p>Well, you know what? I still don't officially endorse pirating content. But if there's literally <em>no other way</em> to view the content you already paid for? Who am I to judge?</p>

<p>And cracking DRM? First of all, it's <em>laughable</em> how bad DRM normally is. But second, DRM is the enabler of all the horrible, scummy business practices that lead to so much e-waste, <em>and</em> to kids being disappointed in their Dad for not being able to play music that's been available digitally for years.</p>

<p>So if you crack DRM to truly own the things you already paid for? Yeah, I don't got a problem with that.</p>

<p>Forget spaceships flying to Mars, I just want my music.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[E3 Is Officially Dead (264 pts)]]></title>
            <link>https://www.washingtonpost.com/entertainment/video-games/2023/12/12/e3-permanently-canceled/</link>
            <guid>38612779</guid>
            <pubDate>Tue, 12 Dec 2023 14:45:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/entertainment/video-games/2023/12/12/e3-permanently-canceled/">https://www.washingtonpost.com/entertainment/video-games/2023/12/12/e3-permanently-canceled/</a>, See on <a href="https://news.ycombinator.com/item?id=38612779">Hacker News</a></p>
Couldn't get https://www.washingtonpost.com/entertainment/video-games/2023/12/12/e3-permanently-canceled/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[NASA says SpaceX’s next Starship flight could test refueling tech (117 pts)]]></title>
            <link>https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/</link>
            <guid>38612585</guid>
            <pubDate>Tue, 12 Dec 2023 14:32:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/">https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/</a>, See on <a href="https://news.ycombinator.com/item?id=38612585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Development risk    —
</h4>
            
            <h2 itemprop="description">SpaceX appears on track for at least a preliminary propellant transfer test next year.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/ship28-1-800x493.jpg" alt="A crane is attached to one of several Starship test vehicles at SpaceX's Starbase facility in South Texas. This vehicle, called Ship 28, could launch on the next Starship test flight.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/ship28-1.jpg" data-height="1109" data-width="1800">Enlarge</a> <span>/</span> A crane is attached to one of several Starship test vehicles at SpaceX's Starbase facility in South Texas. This vehicle, called Ship 28, could launch on the next Starship test flight.</p></figcaption>  </figure>

  




<!-- cache hit 726:single/related:d0f092005b4cf203102681adac7cdcb4 --><!-- empty -->
<p>SpaceX and NASA could take a tentative step toward orbital refueling on the next test flight of Starship, but the US space agency says officials haven't made a final decision on when to begin demonstrating cryogenic propellant transfer capabilities that are necessary to return astronauts to the Moon.</p>
<p>NASA is keen on demonstrating orbital refueling technology, an advancement that could lead to propellant depots in space to feed rockets heading to distant destinations beyond Earth orbit. In 2020, <a href="https://arstechnica.com/science/2020/10/nasa-makes-a-significant-investment-in-on-orbit-spacecraft-refueling/">NASA announced agreements with four companies</a>—Lockheed Martin, United Launch Alliance, SpaceX, and a Florida-based startup named Eta Space—to prove capabilities in the area of refueling and propellant depots using cryogenic propellants.</p>
<p>These cryogenic fluids—liquid hydrogen, methane, and liquid oxygen—must be kept at temperatures of several hundred degrees below zero, or they turn into a gas and boil off. Russian supply freighters regularly refuel the International Space Station with hydrazine and nitrogen tetroxide, room-temperature rocket propellants that can be stored for years in orbit, but rockets using more efficient super-cold propellants have typically needed to complete their missions within hours.</p>
<p>NASA and industry engineers want to extend this lifetime to days, weeks, or months, but this requires new technologies to maintain the propellants at cryogenic temperature and, in some cases like Starship, to transfer the propellants from one vehicle to another.</p>
<p>NASA and several companies are funding efforts in this area, called cryogenic fluid management. NASA's agreements from 2020 committed more than $250 million in government funding for cryogenic fluid management tests in space. These funding agreements announced in October 2020, called "Tipping Point" awards, require substantial private funding from the companies participating in the demonstrations.</p>
<p>According to John Dankanich, who leads NASA's efforts in developing new capabilities for in-space transportation, there are "major technical obstacles" for cryogenic fluid management. The real challenge, he said, will be in validating things like automated couplers, flow meters, and advanced insulation to all work together in microgravity. These, along with other technologies, are "highly interdependent" on one another to make cryogenic refueling a reality, he said.</p>                                            
                                                        
<p>Individual technologies necessary for in-orbit cryogenic refueling are at a stage of development where they are "ready now to go into flight systems," Dankanich said, either with a demonstration in space or on an operational spacecraft.</p>
<h2>First, small steps</h2>
<p>By the fourth anniversary of those awards, only SpaceX appears to have a chance to complete the tasks outlined in its "Tipping Point" award, valued at $53 million.</p>
<p>This test would involve transferring super-cold propellant from one tank to another inside a Starship spacecraft. It's a precursor to future, more complex demonstrations involving two giant Starships docked together in Earth orbit. Then SpaceX will be ready to send a Starship toward the Moon for a test landing without astronauts onboard. Once that is successful, NASA will clear Starship for a crew landing on the agency's Artemis III mission, marking the astronauts' return to the lunar surface for the first time since 1972.</p>
<p>That's easier said than done; all worthy projects require a first step. That could happen as soon as the next full-scale test flight of SpaceX's gigantic Super Heavy booster and Starship rocket, a stainless steel launcher that stands nearly 400 feet (121 meters) tall. SpaceX has flown the rocket twice, <a href="https://arstechnica.com/space/2023/11/spacex-can-celebrate-three-big-wins-after-second-starship-test-flight/">most recently on November 18</a>, when the Starship upper stage reached space for the first time before self-destructing just short of orbital velocity. This <a href="https://arstechnica.com/space/2023/11/heres-why-this-weekends-starship-launch-was-actually-a-huge-success/">test flight was largely successful</a>, achieving several key milestones, such as stage separation and demonstrating improved reliability of the rocket's methane-fueled Raptor engines.</p>
<p>SpaceX has a $2.9 billion contract with NASA to provide a commercial Human Landing System (HLS) derived from Starship for the Artemis III mission, the first human landing mission planned during NASA's Artemis program. The readiness of the Starship landing craft and new commercial spacesuits are <a href="https://arstechnica.com/space/2023/12/government-watchdog-says-first-artemis-lunar-landing-may-slip-to-2027/">widely seen as drivers of the schedule for Artemis III</a>, which is at risk of a delay from late 2025.</p>
<p>Lakiesha Hawkins, deputy associate administrator for NASA's Moon to Mars program office, discussed the Artemis schedule Monday with a committee from the National Academies charged with reviewing the agency's workforce, infrastructure, and technology programs.</p>
<p>Hawkins did not verbally address SpaceX's plans for the next Starship test flight, but one of her slides noted SpaceX is "moving quickly" toward the third Super Heavy/Starship launch and that this flight "will include a propellant transfer demonstration."</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/2/">2</a> <a href="https://arstechnica.com/space/2023/12/nasa-wants-to-see-gas-stations-in-space-but-so-far-its-tanks-are-empty/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Role-playing with AI will be a powerful tool for writers and educators (109 pts)]]></title>
            <link>https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool</link>
            <guid>38612164</guid>
            <pubDate>Tue, 12 Dec 2023 14:02:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool">https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool</a>, See on <a href="https://news.ycombinator.com/item?id=38612164">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4 translated="">Discover more from Res Obscura</h4><p>Notes on the history of technology, medicine, science, art, drugs, and empire from UC Santa Cruz history professor Benjamin Breen. Also: using AI in research and teaching.</p><p>Over 2,000 subscribers</p> </div><div dir="auto"><p><span>The mid-January publication day for my book </span><em><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel="">Tripping on Utopia</a><span> </span></em><span>is fast approaching, and I will have some exciting news to share about book publicity and advance reviews soon. But for now, I am going to return to a topic that has been on my mind over the past year — using generative AI as a kind of </span><a href="https://resobscura.substack.com/p/simulating-history-with-chatgpt" rel="">history simulator</a><span> — and try it it out directly on my book’s content. </span></p><p><span>In fact, I’ve tested it on the content of </span><em>both</em><span> my books: the first, </span><em><a href="https://www.amazon.com/Age-Intoxication-Origins-Global-Americas/dp/0812224981/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=" rel="">The Age of Intoxication</a></em><span>, focused on the global trade in drugs, spices, and medicines in the 17th and 18th centuries. </span><em><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel="">Tripping on Utopia</a><span> </span></em><span>is a deep dive into the first era of psychedelic science, focusing particularly on the anthropologist Margaret Mead and the psychedelic researchers of the 1950s. </span></p><p><span>I’ll describe some highlights below, but if you want to see the complete “transcript” of a simulated experience and setting which was directly inspired by a source I used in </span><em>Tripping on Utopia, </em><span>you can find it here: </span></p><div data-component-name="DigestPostEmbed"><a href="https://resobscura.substack.com/p/how-well-can-gpt-4-simulate-an-acid" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8b3e28d-13ef-426c-b0e1-5e65c6a6cae9_1469x1289.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8b3e28d-13ef-426c-b0e1-5e65c6a6cae9_1469x1289.png" sizes="100vw" alt="Simulating an LSD trip in 1963 with GPT-4" width="140" height="140"></picture></div></a></div><p><span>It’s understandable that some writers and historians think of generative AI as a competitor. After all, AI-produced books are already showing up on Amazon. It’s only a matter of time before the first AI-generated YouTube series takes off — and the first #1 hit single written and performed entirely by AI, and all the rest. I am not happy at this prospect either. But unlike some, I don’t think that this is the only future path available to us. I’m fascinated by the prospect of generative AI not as a </span><em>replacement </em><span>for human authors, but as a tool for allowing us to actively explore and engage with alternative versions of the world. </span></p><p>For instance, the world of a novel that an author is in the process of writing, or the vanished worlds of the past that historians like me spend our time researching and describing. </p><p><span>Educators call this </span><em>experiential learning</em><span>. And increasingly, I think various forms of experiential learning will end up being the most lasting and powerful use cases for the current wave of AI systems. </span></p><p><span>I now  have some preliminary data to support what had, before, mostly been a hunch about the potential effectiveness of AI historical simulations. During the fall quarter, I employed versions of the historical simulation activity I described in </span><a href="https://resobscura.substack.com/p/simulating-history-with-chatgpt" rel="">this previous post</a><span> in the world history class I have been teaching at UCSC. </span></p><p>I then asked my students to complete an anonymous survey about how these activities impacted their learning. Here is one striking result from that survey: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png" width="1318" height="554" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:554,&quot;width&quot;:1318,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:87560,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd5d65a1-4766-44f2-b575-d652cfffa647_1318x554.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>When given a list to choose from, 71% of the students surveyed said “ability to make choices and decisions as historical actors” was most beneficial to their learning in their class. And fully </span><strong>84% of the class</strong><span> reported that AI historical simulations “greatly” or “somewhat” enhanced their overall understanding of the time period. </span></p><p>Of course, the paramount problem is that of accuracy: the simulation needs to accurately reflect the historical record in order to be truly effective as a learning tool. I think this problem can be overcome, but it isn’t solved yet. For now, I am making sure to tell my students that the simulations they experience will contain numerous historical inaccuracies. In fact, I make the act of discussing, fact checking, and reflecting on these inaccuracies a key part of the activity itself. Thinking about the unreliability of sources, after all, is a big part of what historians do. </p><p><span>But what about the more solitary domain of creatively imagining another world? When writing </span><em>Tripping on Utopia, </em><span>I spent a great deal of time figuring out the chronology of events, fact checking claims, determining the relationships between different people, and other forms of historical analysis that require rigor and factual clarity. But I </span><em>also </em><span>spent a lot of my time doing what might best be described as day dreaming. Some of the things on my mind while writing the book: </span></p><ul><li><p>What’s it like to have a malarial fever?</p></li><li><p>Would it be too loud to conduct a conversation while sitting in the back of a motorized canoe in 1933? </p></li><li><p>What are some possible explanations for why Margaret Mead initially volunteered to be a test subject in an LSD trial in 1954, and then pulled out? </p></li><li><p>What would it feel like to be a volunteer in an early psychedelic experiment?  </p></li></ul><p>These are, for lack of a better word, the more “vibes-based” elements of writing history. You can certainly make informed guesses based on the historical record —&nbsp;but you also, necessarily, need to add your own imagination to the task, too. </p><p><span>Generative AI cannot give answers to the questions above. But it can help kickstart the tiny acts of historical imagination and empathy that are so important in writing and teaching history. To get a practical sense of what I mean by this, you can </span><a href="https://resobscura.substack.com/p/how-well-can-gpt-4-simulate-an-acid" rel="">read through this transcript of a simulated acid trip in 1963</a><span>. The participant, a 32-year-old artist named Leo Mitchell, is fictional, and there are numerous obvious inaccuracies (for instance, his “inventory” includes a tiny bottle of LSD pills —&nbsp;but of course, a volunteer would never have walked into a study of an experimental drug with that drug already in hand!) </span></p><p>Nevertheless, there's a lot here that I found useful as a historian. For instance, the emphasis on the ambient environment and physical sensations. Leo feels “a flutter of nervous excitement in his stomach” as he enters the Los Angeles Neuropsychiatric  Hospital to begin his LSD trial on an October morning in 1963. He notices “the soft hum of the hospital's air system.” As the pill kicks in, “the once calming shadows” dancing on the wall “seem to take on a more menacing quality.”</p><p>I found the visual results of the simulation, which were produced by DALLE-3 based on the scenes described, to be similarly evocative. For instance, this one showing Leo staring at his hand in the hallway nicely evoked the clinical backdrop of so many psychedelic studies of the era:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Full-color Leica photograph from October 14, 1963, inside UCLA's Neuropsychiatric Hospital's session room. In sharp focus is the hand of a person, displaying part of a gray button-down shirt sleeve. In the softly lit background, an out-of-focus nurse is seen leading the way towards a door labeled 'Dr. Sidney Cohen'. The ambiance is slightly hallucinatory: the walls seem to subtly bend, the lights leave behind faint tracers, and the overall scene carries the essence of the 1960s combined with a touch of the surreal.&quot;,&quot;title&quot;:&quot;Full-color Leica photograph from October 14, 1963, inside UCLA's Neuropsychiatric Hospital's session room. In sharp focus is the hand of a person, displaying part of a gray button-down shirt sleeve. In the softly lit background, an out-of-focus nurse is seen leading the way towards a door labeled 'Dr. Sidney Cohen'. The ambiance is slightly hallucinatory: the walls seem to subtly bend, the lights leave behind faint tracers, and the overall scene carries the essence of the 1960s combined with a touch of the surreal.&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Full-color Leica photograph from October 14, 1963, inside UCLA's Neuropsychiatric Hospital's session room. In sharp focus is the hand of a person, displaying part of a gray button-down shirt sleeve. In the softly lit background, an out-of-focus nurse is seen leading the way towards a door labeled 'Dr. Sidney Cohen'. The ambiance is slightly hallucinatory: the walls seem to subtly bend, the lights leave behind faint tracers, and the overall scene carries the essence of the 1960s combined with a touch of the surreal." title="Full-color Leica photograph from October 14, 1963, inside UCLA's Neuropsychiatric Hospital's session room. In sharp focus is the hand of a person, displaying part of a gray button-down shirt sleeve. In the softly lit background, an out-of-focus nurse is seen leading the way towards a door labeled 'Dr. Sidney Cohen'. The ambiance is slightly hallucinatory: the walls seem to subtly bend, the lights leave behind faint tracers, and the overall scene carries the essence of the 1960s combined with a touch of the surreal." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F880be93f-69af-45ff-8f6f-a9777d3ac09c_1024x1024.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And I was pleasantly surprised by the image below, depicting the moment when Leo Mitchell approaches the offices of Sidney Cohen. It captures Mitchell’s perspective while fully under the effects of the drug, as “the fluorescent lights buzz and flicker, casting strange, moving shadows on the walls.” </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp" width="1024" height="1024" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1024,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Historically accurate yet slightly surreal Leica photo from October 14, 1963, taken inside a session room at UCLA's Neuropsychiatric Hospital. The photograph emphasizes a sturdy oak door, adorned with a sign reading 'Dr. Sidney Cohen - Staff Physician'. The ambiance around the door exudes a subtle psychedelic aura, with light distortions and hints of shifting colors. The decor, the style of the door, and any visible elements should resonate with the 1960s style.&quot;,&quot;title&quot;:&quot;Historically accurate yet slightly surreal Leica photo from October 14, 1963, taken inside a session room at UCLA's Neuropsychiatric Hospital. The photograph emphasizes a sturdy oak door, adorned with a sign reading 'Dr. Sidney Cohen - Staff Physician'. The ambiance around the door exudes a subtle psychedelic aura, with light distortions and hints of shifting colors. The decor, the style of the door, and any visible elements should resonate with the 1960s style.&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Historically accurate yet slightly surreal Leica photo from October 14, 1963, taken inside a session room at UCLA's Neuropsychiatric Hospital. The photograph emphasizes a sturdy oak door, adorned with a sign reading 'Dr. Sidney Cohen - Staff Physician'. The ambiance around the door exudes a subtle psychedelic aura, with light distortions and hints of shifting colors. The decor, the style of the door, and any visible elements should resonate with the 1960s style." title="Historically accurate yet slightly surreal Leica photo from October 14, 1963, taken inside a session room at UCLA's Neuropsychiatric Hospital. The photograph emphasizes a sturdy oak door, adorned with a sign reading 'Dr. Sidney Cohen - Staff Physician'. The ambiance around the door exudes a subtle psychedelic aura, with light distortions and hints of shifting colors. The decor, the style of the door, and any visible elements should resonate with the 1960s style." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcb5faf44-fb36-4d01-82b2-76cc6354c24d_1024x1024.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The same part of the simulation evokes Mitchell’s inner life in a way I found compelling. Mitchell, an artist, has drawn a charcoal sketch of his own brain and is carrying it in his hand. The simulation notes at this point: “The intricate details of your brain drawing in your hand seem alive, and you feel an immense connection to it, as if it's a roadmap to your current mental state.”</p><p><span>I didn’t have access to generative AI tools like this while writing </span><em>Tripping on Utopia</em><span>. But if I had, I would’ve found this experience useful for helping me better imagine the  the perspective of a nervous patient experiencing everything for the first time — rather than that of the scientists who produced the vast majority of the primary sources relating to early psychedelic therapy. </span></p><p><span>One intriguing aspect of the way generative AI systems are trained is that they contain sedimentary layers of the historical record. It’s not as if everything in their training data was from the last decade, let alone the last century. To cite just one example, large swathes of Wikipedia were </span><a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Encyclopaedia_Britannica" rel="">copied directly from the 1911 edition of </a><em><a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Encyclopaedia_Britannica" rel="">Encyclopedia Britannica</a><span>, </span></em><span>which is in the public domain. This, in turn, means that there’s an awful lot of 1911-vintage knowledge in models like Gemini and GPT-4. </span></p><p>That’s bad for anyone trying to use something like ChatGPT as a search engine to surface reliable facts. But it’s pretty amazing for a historian trying to play around with obsolete and archaic knowledge. </p><p><span>One great example of this idea taken to an extreme is </span><a href="https://huggingface.co/Pclanglais/MonadGPT" rel="">MonadGPT</a><span>, a language model created by the digital humanist Pierre-Carl Langlais. It’s a version of the open-source Mistral 7B language model that has been fine-tuned on early modern European texts. The result is much as Langlais’ describes: “What would have happened if ChatGPT was invented in the 17th century? MonadGPT is a possible answer.” You can chat with it directly </span><a href="https://huggingface.co/spaces/Pclanglais/MonadGPT" rel="">here</a><span>. </span></p><p><span>To test how MonadGPT performs when asked to role-play as a physician from the 1680s — one of the key decades I researched in </span><em>Age of Intoxication</em><span> — I gave it a randomly-chosen set of symptoms described by the 17th century English physician Thomas Sydenham </span><a href="https://www.google.com/books/edition/Dr_Sydenham_s_Compleat_Method_of_curing/xG8K_L7OP34C?hl=en&amp;gbpv=1&amp;dq=%22the%20patient%22%20symptoms&amp;pg=PA6&amp;printsec=frontcover" rel="">here</a><span>. They include “Convulsions,” “a most vehement Pain the Head,” “vomiting up green Matter,”  “Palpitation of the Heart,” “Cough” and “the Illiack Passions.”</span></p><p>For Sydenham, these symptoms pointed to only one thing: “the disease called in Women the Hysterical, in Men the Hypochondriacal Passion.”</p><p>When I asked MonadGPT to offer a diagnosis and treatment based on those symptoms, the diagnosis it gave was dropsy. It advised that I “eat a moderate diet, and abstain from strong drinks,” and prescribed the following: </p><blockquote><p>Take two drams of Aloes, one dram of Balsam, half a dram of Gum Arabic, and half a dram of Camphire; mix them together in water, and take two spoonfuls every hour. Also take a dram of Rue, a dram of Opium, a dram of Angelica, a dram of Centory, a dram of Poppy-seed, and a dram of Myrrh; mix them together, and take two spoonfuls every day. Take likewise two ounces of Hellebore, and boil them in a quart of water till it be reduced to half its bulk; then strain it through a cloth, and take half a pint of this liquor every day, till the disease be removed.</p></blockquote><p>This was a pretty great recreation of an early modern drug recipe, but it didn’t correctly guess my affliction. </p><p><span>What’s striking, however, is that off-the-shelf, vanilla GPT-4 </span><em>did</em><span> — provided I prompted it to “role-play as a doctor from the 1680s.” Here was part of its </span><a href="https://chat.openai.com/share/cbc44180-b546-4489-ad75-7622942c47aa" rel="">response</a><span> to the list of symptoms:</span></p><blockquote><p>Upon reading thy description of the maladies afflicting this patient, it is my judgment as a physician of the 1680s that we are confronted with a case most complex and multifarious, indicative of a condition known in our times as 'Hysteria' – a disorder primarily attributed to the wandering womb, as postulated by ancient physicians like Hippocrates and Galen… To restore the balance of humors, venesection (bloodletting) may be advised, perhaps by leeches or with a lancet.</p></blockquote><p>However, the language used here was not that historically accurate, and the herbs it went on to prescribe (“valerian and lavender for their calming effects on the mind”) had more in common with modern herbal remedies than with 1680s medicine. </p><p>When I made a custom GPT, things improved quite a bit. My “1680s Physician” GPT was provided with a list of common 17th century drug and medicine names (red coral, tragacanth, antimony, laudanum, etc) and given some guidance about the task it should perform (“Treatment recommendations should be strictly within the scope of 17th-century medical practices”). </p><p><span>When given the list of symptoms above, it correctly diagnosed “Hysterick and Hypochondriack Affections” and offered the </span><a href="https://chat.openai.com/share/e7d5fb91-904c-4ce2-9641-68dd46fa5608" rel="">following</a><span> treatment plan: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png" width="621" height="633.1398496240602" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1356,&quot;width&quot;:1330,&quot;resizeWidth&quot;:621,&quot;bytes&quot;:306770,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c40973e-b0ba-4218-b922-8fc105f07c07_1330x1356.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>This was not precisely what Sydenham </span><a href="https://www.google.com/books/edition/Dr_Sydenham_s_Compleat_Method_of_curing/xG8K_L7OP34C?hl=en&amp;gbpv=1&amp;dq=%22let%20eight%20ounces%20of%20blood%22&amp;pg=PA7&amp;printsec=frontcover" rel="">recommended</a><span>, but it was strikingly close. Sydenham urges: “Let eight Ounces of Blood be taken away.” 1690s Physician GPT recommended “letting no more than eight Ounces of Blood.” Sydenham and the AI both recommended galbanum. In general, the herb and drug remedies prescribed above strike me as in keeping with the sorts of things a 1680s doctor in London or Paris or Lisbon would’ve prescribed. </span></p><p><span>That’s not to say they directly reflect historical reality, of course. But they allow you to </span><em>play </em><span>with the historical record in a novel way — a form of experiential learning which creatively repurposes the source base itself. </span></p><p><span>For instance, we can’t ask Thomas Sydenham how we would treat Covid-19, but we </span><em>can</em><span> ask the 1690s Physician bot (“take of Barley-water, two Pints, Scurvy-grass, one Handful, and Lemon peel, half an Ounce. Boil them together until the liquid is reduced by one half. Let the Patient drink this Decoction warm, at the rate of a Pint each Day”). </span></p><p><span>I keep returning to the theme of </span><em>historical empathy </em><span>when I reflect on exercises like this. There are aspects of historical writing and teaching that are stringently objective: did an event happen on a certain date or not, was a person alive or not on that date, were they in </span><em>x</em><span> or </span><em>y</em><span> location, and the like. Despite overheated claims, I don’t think tools like ChatGPT or Google’s forthcoming Gemini are ready to offer much help to professional researchers when it comes to those challenges. A human historian is still far better than the leading edge of AI systems when it comes to detecting and correcting factual inaccuracies. </span></p><p><em>But</em><span>… </span></p><p><span>… that isn’t what these systems are actually good at. They’re good at </span><em>hallucinating</em><span>. As long as you’re clear-eyed about the inherent limitations, those hallucinations can be harnessed as a feature, not a bug. </span></p><p><span>As AI researcher Andrej Karpathy </span><a href="https://twitter.com/karpathy/status/1733299213503787018" rel="">put it</a><span> the other day: </span></p><blockquote><p>I always struggle a bit with I'm asked about the “hallucination problem” in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines. We direct their dreams with prompts. The prompts start the dream, and based on the LLM’s hazy recollection of its training documents, most of the time the result goes someplace useful.</p></blockquote><p>For historians and for writers of all stripes, anything that allows you to better connect with your source material on the level of emotion and imagination is useful indeed.</p><p><span>•&nbsp;“In 2006, a Roman oil lamp was scientifically excavated at Jamestown, Virginia, the earliest permanent English settlement in the Americas. This study explores why a 17th-century traveler would bring this ancient lighting vessel to the settlement and how its unusual double depositional history allows us to trace its changing meaning over time.” (</span><em><a href="https://www.journals.uchicago.edu/doi/10.1086/719422" rel="">American Journal of Archaeology</a></em><span>)</span></p><p><span>•&nbsp;</span><a href="https://www.aftertheplague.org/" rel="">After the Plague: Health and History in Medieval England</a><span>. The profiles of real people who lived and died in medieval Cambridge here are totally fascinating and well worth checking out. </span></p><p><span>•&nbsp;</span><a href="https://en.wikipedia.org/wiki/Manuel_II_Palaiologos#Emperor's_trip_to_the_West" rel="">The only Byzantine emperor to have visited England</a><span> (1400 CE). “This emperor and his men always went about dressed uniformly in long robes cut like tabards which were all of one colour, namely white, and disapproved greatly of the fashions and varieties of dress worn by the English, declaring that they signified inconstancy and fickleness of heart. No razor ever touched the heads or beards of his priests.”</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png" width="158" height="34" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:34,&quot;width&quot;:158,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4426,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you’d like to support my work, please pre-order my book </span><em><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?&amp;_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel="">Tripping on Utopia: Margaret Mead, the Cold War, and the Troubled Birth of Psychedelic Science</a></em><span> or share this newsletter with friends you think might be interested.</span></p><p>As always, I welcome comments. Thank you for reading!</p><p data-attrs="{&quot;url&quot;:&quot;https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325&quot;,&quot;text&quot;:&quot;Buy \&quot;Tripping on Utopia\&quot;&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel=""><span>Buy "Tripping on Utopia"</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://resobscura.substack.com/p/roleplaying-with-ai-will-be-powerful-tool?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open-source macOS AI copilot (using vision and voice) (353 pts)]]></title>
            <link>https://github.com/elfvingralf/macOSpilot-ai-assistant</link>
            <guid>38611700</guid>
            <pubDate>Tue, 12 Dec 2023 13:17:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/elfvingralf/macOSpilot-ai-assistant">https://github.com/elfvingralf/macOSpilot-ai-assistant</a>, See on <a href="https://news.ycombinator.com/item?id=38611700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">macOSpilot: your personal macOS AI assistant</h2>
<p dir="auto">macOSpilot answers your questions about anything, in any application. No need to reach for another window. Simply use a keyboard shortcut to trigger the assistant, speak your question, and it will give the answer in context and in audio within seconds. Behind the scenes macOSpilot takes a screenshot of your active window when triggerd, and sends it to OpenAI GPT Vision along with a transcript of your question. It's answer will be displayed in text, and converted into audio using OpenAI TTS (text to speech).</p>
<details open="">
  <summary>
    
    <span aria-label="Video description file.size.demo.with.subtitles.mp4">file.size.demo.with.subtitles.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/94417497/287926991-5a9e9288-0479-4def-9a87-451dddd783af.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDIzOTcxMDQsIm5iZiI6MTcwMjM5NjgwNCwicGF0aCI6Ii85NDQxNzQ5Ny8yODc5MjY5OTEtNWE5ZTkyODgtMDQ3OS00ZGVmLTlhODctNDUxZGRkZDc4M2FmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjEyVDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA3MzNjOWRiMGQxYTA5NjViMmMyY2NhZTE2ZmMxNTIyNzQyZTU4OThiODNjZWM3Yzc2Mjg5MDA3MzRlYmRjZmUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.572iO9MFZdZ7uQoZjM0WviQdzRhMOZHjLCTEoQAUmpc" data-canonical-src="https://private-user-images.githubusercontent.com/94417497/287926991-5a9e9288-0479-4def-9a87-451dddd783af.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDIzOTcxMDQsIm5iZiI6MTcwMjM5NjgwNCwicGF0aCI6Ii85NDQxNzQ5Ny8yODc5MjY5OTEtNWE5ZTkyODgtMDQ3OS00ZGVmLTlhODctNDUxZGRkZDc4M2FmLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjEyVDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA3MzNjOWRiMGQxYTA5NjViMmMyY2NhZTE2ZmMxNTIyNzQyZTU4OThiODNjZWM3Yzc2Mjg5MDA3MzRlYmRjZmUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.572iO9MFZdZ7uQoZjM0WviQdzRhMOZHjLCTEoQAUmpc" controls="controls" muted="muted">

  </video>
</details>

<ul dir="auto">
<li><strong>Works with any application in macOS:</strong> macOSpilot is application agnostic, and simply takes a screenshot of the currently active window when you trigger the assistant.</li>
<li><strong>Trigger with keyboard shortcut, speak your question:</strong> No need to juggle windows, just press the keyboard shortcut and speak your question.</li>
<li><strong>Answers in-context and in audio:</strong> The answer to your question is provided in an small window overlayed on top of your active window, and in audio (using text-to-speech).</li>
</ul>
<h2 tabindex="-1" dir="auto">How it works</h2>
<ol dir="auto">
<li>macOSpilot runs NodeJS/Electron. Simply install the NodeJS project and dependencies (see below) and make the necessary configurations in <code>index.js</code>. Then chose to run <code>yarn start</code> from the terminal, or package it with Electron with the instructions below, add your OpenAI API key and let the application run in the background.</li>
<li>When you need to use macOSpilot, press the keyboard shortcut you've configured (default is Command+Shift+'). macOSpilot will take a screenshot of your currently active macOS application window and activate the microphone.</li>
<li>Speak your question into your microphone and then press the same keyboard shortcut to end the microphone recording.</li>
<li>macOSpilot will send your question to OpenAI's Whisper API, and the transcription will be sent to OpenAI's Vision API along with the screenshot.</li>
<li>The Vision API response will be displayed in a small notification window on top of your active macOS application window, and read outloud once it's been processed by OpenAI's TTS (text to speech) API.</li>
<li>A simple history of answers to your questions in the current session is available in another window that you can hide/minimize.</li>
</ol>
<p dir="auto">The most recent screenshot, audio recording, and TTS response will be stored on your machine in part for debugging purposes. The same filename is used every time so they will be overwritten, but are not automatically deleted when you close or delete the application.</p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<h3 tabindex="-1" dir="auto">Video walk-through</h3>
<p dir="auto">Prefer a video? Head on over to YouTube to watch the walk through of how to get started, how the application works, and a brief explanation of how it works under the hood.</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=1IdCWqTZLyA" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/94417497/289893066-e96e314f-6778-42e5-8a9b-04ce9e6fc0b9.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDIzOTcxMDQsIm5iZiI6MTcwMjM5NjgwNCwicGF0aCI6Ii85NDQxNzQ5Ny8yODk4OTMwNjYtZTk2ZTMxNGYtNjc3OC00MmU1LThhOWItMDRjZTllNmZjMGI5LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMTIlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjEyVDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRhNWE1MmE0OTkzMzJlZjE2Yzc2Y2NhNTNiNjY0ZjA4OTY1ZGFkNmNkZWE1MWQxY2IwMjA1ZmI4OTAxYTExMWUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.h1WiMJ0-bqY6wHVOqR_3v4HOBpZMwmq01OafGDb6Nkc" alt="YouTube walk-through and tutorial" secured-asset-link=""></a></p>
<h3 tabindex="-1" dir="auto">Install</h3>
<p dir="auto">Make sure you have NodeJS installed on your machine. Then clone the repo and follow the steps below.</p>
<div dir="auto" data-snippet-clipboard-copy-content="
git  clone  https://github.com/elfvingralf/macOSpilot-ai-assistant.git
"><pre>git  clone  https://github.com/elfvingralf/macOSpilot-ai-assistant.git
</pre></div>
<p dir="auto">Navigate to the folder and run <code>yarn install</code> or <code>npm install</code> in your folder. This should install all dependencies.</p>
<p dir="auto">Run <code>yarn start</code> or <code>npm start</code>. Because the application needs access to read your screen, microphone, read/write files etc, you will need to go through the steps of granting it access and possibly restarting your terminal.</p>
<h3 tabindex="-1" dir="auto">Configurations</h3>
<p dir="auto">Make sure to add your OpenAI API key by clicking the settings icon in the top right-hand corner of the main window. (it's not stored encrypted!)</p>
<p dir="auto">If you want to change the default values here's a few things that might be worth changing, all in <code>index.js</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Keyboard shortcut:</strong> The default keyboard shortcut <code>keyboardShortcut</code> is set to "CommandOrControl+Shift+'" (because it seemed like it was rarely used by other applications)</p>
</li>
<li>
<p dir="auto"><strong>OpenAI Vision prompt:</strong> The OpenAI Vision API system prompt in <code>conversationHistory</code>, currently just set to "You are helping users with questions about their macOS applications based on screenshots, always answer in at most one sentence."</p>
</li>
<li>
<p dir="auto"><strong>VisionAPI image size:</strong> Image resize params to save some money, I left an example of how in <code>callVisionAPI()</code> (I found that I had much poorer results when using it)</p>
</li>
<li>
<p dir="auto"><strong>Application window sizes and settings:</strong> The size of the main window: <code>mainWindowWidth</code> and <code>mainWindowHeight</code>. The size of the notification window, which always remains on top: <code>notificationWidth</code> and <code>notificationHeight</code>.</p>
</li>
<li>
<p dir="auto"><strong>More notification window settings:</strong> The level of opacity of the notification window: <code>notificationOpacity</code>. Where the notification window moves to on activation, relative to the active window: inside <code>positionNotificationAtTopRight()</code> (terrible naming, I know)</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Turn it into an .app with Electron</h3>
<p dir="auto">Want to create an .app executable instead of running this from your terminal?</p>
<p dir="auto">First go to <code>index.js</code> and change <code>const useElectronPackager</code> from <code>false</code> to <code>true</code>.</p>
<p dir="auto">Run one of these in your terminal, depending on which platform you're on.</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm  run  package-mac
npm  run  package-win
npm  run  package-linux"><pre>npm  run  package-mac
npm  run  package-win
npm  run  package-linux</pre></div>
<p dir="auto">Note I have only tested this on Mac (Apple silicon and Intel).</p>
<p dir="auto">Go to <code>/release-builds/</code> in your project folder, and chose the folder of your platform. In there is an executable, <code>.app</code> if you're on Mac. Double-click it to open the app, note that it may take a few seconds the first time so be patient.</p>
<p dir="auto">Once the app is opened, trigger your keyboard shortcut. You'll be asked to grant Privacy &amp; Security permissions. You may need to repeat this another one or two times for all permissions to work properly, and to restart the app.</p>
<p dir="auto"><strong>NOTE:</strong> I've had consistent issues getting macOS to trigger the Privacy &amp; Security Microphone dialog window for the .app, which means that I can't ask my question. If it works for you, or if you have a work-around to this issue, I'd love to know.</p>
<h2 tabindex="-1" dir="auto">Improvements:</h2>
<p dir="auto">Some improvements I'd like to make, in no particular order:</p>
<ul dir="auto">
<li>Enable optional conversation state inbetween sessions (open/close application)</li>
<li>Use buffers instead of writing/reading screenshot and audio files to disk</li>
<li>Make assistant audio configurable in UI (e.g. speed, make playback optional)</li>
<li>Make always-on-top window configurable in UI (e.g. toggle sticky position, enable/disable)</li>
<li>Make screenshot settings configurable in UI (e.g. select area, entire screen)</li>
<li>Fix microphone issue not working as .app</li>
</ul>
<h2 tabindex="-1" dir="auto">About / contact</h2>
<p dir="auto">I'm a self-taught and really like scrapping together fun projects. I write functional code that probably isn't beautiful nor efficient, and share it with the hope that someone else might find it useful.</p>
<p dir="auto">You can find me as <a href="https://twitter.com/ralfelfving" rel="nofollow">@ralfelfving</a> on Twitter/X. If you liked this project, consider checking my tutorials on my YouTube channel <a href="https://www.youtube.com/@ralfelfving" rel="nofollow">@ralfelfving</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Today Is One of the Biggest Surveillance Votes. Will the FBI Stop Spying? (310 pts)]]></title>
            <link>https://tuta.com/blog/702-open-letter-against-surveillance</link>
            <guid>38611590</guid>
            <pubDate>Tue, 12 Dec 2023 13:01:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/702-open-letter-against-surveillance">https://tuta.com/blog/702-open-letter-against-surveillance</a>, See on <a href="https://news.ycombinator.com/item?id=38611590">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Today US politicians are going to vote on the FISA “Reform” Bill - the FISA Reform and Reauthorization Act of 2023 (FRRA). This bill would greatly expand
the surveillance possibilities of US and non-US citizens. Privacy-first companies like Mozilla, Tuta, and the Tor Project
now call on policymakers to not cement such surveillance measures with FISA 702 Surveillance.
</p><div><p>Ten years after the Snowden leaks the United States continues to spy on American citizens via its unconstitutional surveillance
program.</p>
<p><strong>Section 702 of the Foreign Intelligence Surveillance Act (FISA) allows US authorities to spy on foreigners to prevent terrorism
attacks. However, a vast amount of data from US citizens is scooped up by the NSA and regularly abused by
the FBI via <a href="https://tuta.com/blog/posts/why-a-backdoor-is-a-security-risk">backdoor searches on US citizens without warrants or probable cause</a>.</strong></p>
<p><strong>This practise must end. FISA Section 702 must not be reauthorized.</strong></p>
<h2 id="open-letter-demands">Open Letter Demands</h2>
<p>We at Tuta together with other privacy-first companies are making clear in a joint open letter that:</p>
<ol>
<li><p>Congress should not provide a blank check for surveillance overreach by reauthorizing FISA 702 in NDAA;</p>
</li>
<li><p>Congress should pass a strong surveillance reform bill such as the Government Surveillance Reform Act (GSRA) or
the Protect Liberty and End Warrantless Surveillance Act (PLEWSA), and that measures such as the House and Senate
Intelligence bills would only cement and expand the status quo of surveillance abuses.</p>
</li>
</ol>
<p><strong>Simply put, the vitality of the Internet economy depends on strong surveillance reform that leave no room for backdoors.</strong></p>
<h3 id="open-letter-against-fisa-section-702">Open Letter Against FISA Section 702</h3>
<p>Dear Members of the House of Representatives,</p>
<p>We, a group of companies, builders, and providers of critical internet-based products and
services, write to you today in support of legislative proposals, such as the Government
Surveillance Reform Act, that would effectively address bipartisan and bicameral concerns
around consistent surveillance overreach. We also applaud the Protect Liberty and End
Warrantless Surveillance Act, which takes key steps forward in reform, and encourage you to
strengthen the bill further.</p>
<p>Additionally, current efforts to reauthorize Section 702 until April through the National Defense
Authorization Act would amount to rubber-stamping surveillance abuses, and we strongly
oppose such efforts.</p>
<p>As providers of digital products and services, both nonprofit and for-profit, we depend on the
trust of our customers to sustain digital communities. If widely-documented abuses go
unaddressed in legislation, individuals will remain concerned that their most intimate
information could be collected by intelligence agencies without accountability, thus deteriorating
the economic and social power of the Internet.</p>
<p><strong>While the current legislative debate may have been sparked by the imminent expiration of
Section 702 of FISA, the widely-documented surveillance abuses highlighted by policy experts
and members of Congress across the political spectrum would continue under narrow 702
“fixes,” especially ones that only cosmetically change FISA. For that very reason, we are
particularly concerned by reauthorization proposals by the House and Senate Intelligence
Committees that would only cement overbroad surveillance.</strong></p>
<p>A true reform proposal will need to address similar ways that the government surveils Americans
without adequate oversight and accountability, including warrantless purchases of Americans’
information from data brokers, narrowing the scope of surveillance in line with the President’s
own EO 14086, increasing the ability of Americans to stand up for their rights in court, and,
optimally, parallel reforms of EO 12333. These provisions would ensure that surveillance
overreach does not simply continue the day after a reform bill passes, but under a slightly
different authority. In other words, Congress should take this “functional” approach to
surveillance reform by addressing end results.</p>
<p><strong>The Protect Liberty and End Warrantless Surveillance Act, while not containing all of these
provisions, would take critical steps forward in shielding Americans from overbroad
surveillance. Most notably, it would protect Americans from warrantless data broker purchases
and would create an ironclad warrant requirement for 702 surveillance of US persons, among
other provisions.</strong> Lawmakers should consider strengthening the bill, for example by increasing
transparency and accountability measures.</p>
<p>Of particular importance in both bills is inserting language codifying the scope of surveillance
proposed by the President’s own EO 14086 to tether surveillance of non-US persons to basic
guardrails.</p>
<p>We remain available to discuss the impact of reform proposals on the economy and on the
privacy of people online as you continue your thoughtful work.</p>
<p>Signed,</p>
<p>Mozilla</p>
<p>Wikimedia Foundation</p>
<p>Foundation for American Innovation</p>
<p>Proton</p>
<p>DuckDuckGo</p>
<p>Nord Security</p>
<p>The Tor Project</p>
<p>WebPros</p>
<p>Quilibrium, Inc.</p>
<p>Mailfence</p>
<p>Tuta</p>
<p>Superbloom (previously known as Simply Secure)</p>
<p>Gate 15</p>
<p>Nitrokey</p>
<p>Action Network &amp; Action Builder</p>
<p>Malloc</p>
<p>Efani Secure Mobile</p>
<p>Skiff</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube doesn't want to take down scam ads (547 pts)]]></title>
            <link>https://old.reddit.com/r/youtube/comments/18gjiqy/youtube_doesnt_want_to_take_down_scam_ads/</link>
            <guid>38611466</guid>
            <pubDate>Tue, 12 Dec 2023 12:47:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/youtube/comments/18gjiqy/youtube_doesnt_want_to_take_down_scam_ads/">https://old.reddit.com/r/youtube/comments/18gjiqy/youtube_doesnt_want_to_take_down_scam_ads/</a>, See on <a href="https://news.ycombinator.com/item?id=38611466">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey Reddit,</p>

<p>i recently noticed that there are way more scam ads on YouTube. And i am not talking about those ads of people selling you get rich quick courses, which i get to see way too often as well, i am talking about deep fake Elon Musk saying that he has built an AI powered trading bot that will allow you to make 1 million dollars in 6 months risk free and to be honest i can't tell that it is deep fake, it looks very real. Here is the link to the ad: <a href="https://www.youtube.com/watch?v=wWGb0BLYBOs">https://www.youtube.com/watch?v=wWGb0BLYBOs</a></p>

<p><a href="https://preview.redd.it/z7zagtomju5c1.png?width=1942&amp;format=png&amp;auto=webp&amp;s=5ed808fbf9800784a6bb1698fac2317d8e5f2a85">https://preview.redd.it/z7zagtomju5c1.png?width=1942&amp;format=png&amp;auto=webp&amp;s=5ed808fbf9800784a6bb1698fac2317d8e5f2a85</a></p>

<p>or here another one: <a href="https://www.youtube.com/watch?v=aE77YkmsbDc&amp;t=75s">https://www.youtube.com/watch?v=aE77YkmsbDc&amp;t=75s</a> </p>

<p>​</p>

<p>I am from Austria so i get to see the German version of these scams as well. Like this one: <a href="https://www.youtube.com/watch?v=1Iyuyx5thgI">https://www.youtube.com/watch?v=1Iyuyx5thgI</a></p>

<p>or this one: <a href="https://www.youtube.com/watch?v=JMRY5dxnZ4E">https://www.youtube.com/watch?v=JMRY5dxnZ4E</a></p>

<p>The first one i have seen so many times it gets ridiculous. Its about a famous German figure supposedly being sued by the central bank because he found an infinite money glitch.</p>

<p>​</p>

<p>And because these scam ads are getting so annoying i started reporting them every time i see one, because i do understand that there are probably millions of advertisers on YouTube and sometimes these scams don't get detected right away. So i also reported that Fake Elon Musk Ad, but the next day i get shown the exact&nbsp;same ad again, then i open my email inbox finding a YouTube email about my report stating <strong>"We decided not to take this ad down.&nbsp;We found that the ad doesn’t go against&nbsp;Google’s policies".</strong> The same goes for all four ads listed above, all reported, all apparently not against policy.</p>

<p>So one of these two is true:</p>

<p>1 - Elon Musk has actually found a way to make everyone rich with ai.</p>

<p>2 - YouTube simply doesn't care about people getting scammed as long as they make money.</p>

<p>As much as i would like the first one to be true, these YouTube Ads are not cheap to run, with one of them running for more than a month with 364K views. So the fact that the scammers are able to afford these ads means that people keep falling for these scams, and YouTube doesn't seem to care at all.  </p>

<p>And i don't want to use an adblocker because i want the people i watch to get paid for their work, but i don't want them to be paid with the money from the victims of these scams. </p>

<p>Is this a regional problem in Austria or do you see these scams too? </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube is crippling Firefox on Asahi Linux (222 pts)]]></title>
            <link>https://social.treehouse.systems/@marcan/111567255619206929</link>
            <guid>38611248</guid>
            <pubDate>Tue, 12 Dec 2023 12:21:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.treehouse.systems/@marcan/111567255619206929">https://social.treehouse.systems/@marcan/111567255619206929</a>, See on <a href="https://news.ycombinator.com/item?id=38611248">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The NSA advises move to memory-safe languages (285 pts)]]></title>
            <link>https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3608324/us-and-international-partners-issue-recommendations-to-secure-software-products/</link>
            <guid>38611175</guid>
            <pubDate>Tue, 12 Dec 2023 12:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3608324/us-and-international-partners-issue-recommendations-to-secure-software-products/">https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3608324/us-and-international-partners-issue-recommendations-to-secure-software-products/</a>, See on <a href="https://news.ycombinator.com/item?id=38611175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/NewsArticle" id="dnn_ctr12594_ContentPane">
    <meta itemprop="datePublished" content="Dec. 6, 2023">

    
    <div id="adetail-slick-gallery">
        
        <div>
            
            <p><img src="https://media.defense.gov/2023/Dec/06/2003352582/1920/1080/0/231206-D-IM742-2023.PNG" alt="CSI: The Case for Memory Safe Roadmaps" itemprop="image">
        </p></div>
        
        
        
        
        
        <p><i></i>
            <span>PHOTO INFORMATION</span>
        </p>
        
        
    </div>
    

    <div itemprop="articleBody" id="news-content">
            
            <div><p>FORT MEADE, Md. - The National Security Agency (NSA) joins Cybersecurity and Infrastructure Security Agency (CISA) and U.S. and international partners in releasing <a href="https://media.defense.gov/2023/Dec/06/2003352724/-1/-1/0/THE-CASE-FOR-MEMORY-SAFE-ROADMAPS-TLP-CLEAR.PDF">”The Case for Memory Safe Roadmaps” Cybersecurity Information Sheet (CSI)</a>. Expanding on the “Software Memory Safety” CSI published by NSA in April 2023, the report provides guidance for software manufacturers and technology providers to create roadmaps tailored to eliminate memory safety vulnerabilities from their products.</p><p>
&nbsp;
Memory safety vulnerabilities are coding errors affecting software’s memory management code in which memory can be accessed, written, allocated, or deallocated in unintended ways. Types of memory-related coding errors mentioned in the CSI include buffer overflow, use after free, use of uninitialized memory, and double free. Exploiting these vulnerabilities could allow malicious actors to access or corrupt data, or run arbitrary malicious code with the same privilege as the system owner.</p><p>
&nbsp;
“Memory safety vulnerabilities affect software development across all industries,” said Neal Ziring, Technical Director of NSA Cybersecurity Directorate. “Working together to set clear goals and timelines in transition roadmaps to safer programming language is critical for mitigating these problems.”</p><p>

In a shared conclusion, the co-authoring agencies recommend software manufacturers create roadmaps for the utilization of, and transition to, memory safe programming languages. This transition will enable memory safe programming languages to mitigate memory-related vulnerabilities and reduce the products' attack surface. Recommended memory safe programming languages mentioned in the CSI include C#, Go, Java, Python, Rust, and Swift. Software manufacturers should evaluate multiple memory safe programming languages before integrating them into their workflows.</p><p>

The CSI includes technical and non-technical factors for software manufacturers to consider when developing their roadmap. These include picking a memory safe language, staff capabilities and resourcing, and prioritization guidance. Additional guidance includes elements that should be part of the roadmaps, including the following: defined phases with dates and outcomes, dates for memory safe programming languages in new systems, internal developer training and integration plans, external dependency plans, transparency plans, and CVE support program plans.</p><p>
&nbsp;
The authoring agencies urge software manufacturers to create and publish memory safe roadmaps to plan and communicate how memory safety vulnerabilities will be mitigated in their products.</p><p>
&nbsp;
The authoring agencies include CISA, NSA, the Federal Bureau of Investigation (FBI), the Australian Signals Directorate’s Australian Cyber Security Centre (ACSC), the Canadian Centre for Cyber Security (CCCS), the New Zealand National Cyber Security Centre (NCSC-NZ) and Computer Emergency Response Team New Zealand (CERT NZ), and the United Kingdom’s National Cyber Security Centre (NCSC-UK). The agencies jointly developed this report as part of their Secure by Design campaign to urge software manufacturers to prioritize design and implementation practices to reduce customer risk by using memory safe languages in their products.</p></div>



<hr>
<p>NSA Media Relations<br>
<a href="mailto:MediaRelations@nsa.gov">MediaRelations@nsa.gov</a><br>
443-634-0721</p>

        </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tell/Ask HN: HN WebSite is/was down. I'm curious why? (151 pts)]]></title>
            <link>https://hn.hund.io/</link>
            <guid>38610912</guid>
            <pubDate>Tue, 12 Dec 2023 10:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hn.hund.io/">https://hn.hund.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38610912">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">

<section>
<section aria-label="Website" role="region">
<header data-behavior="disable-ctts">
<h2>Website</h2>
<span role="img" tabindex="0" title="Operational">Operational</span>

</header>


</section>
<section aria-label="API" role="region">
<header data-behavior="disable-ctts">
<h2>API</h2>
<span role="img" tabindex="0" title="Operational">Operational</span>

</header>


</section>
<section aria-label="Search" role="region">
<header data-behavior="disable-ctts">
<h2>Search</h2>
<span role="img" tabindex="0" title="Operational">Operational</span>

</header>


</section>

</section>

<section data-behavior="metrics" data-source="/metrics/data.json" data-top-level="">
<header data-behavior="disable-ctts">
<h2>Metrics</h2>

</header>
<div>
<ol data-i18n-metric-lacking-data="Data is not yet available for this time frame.">
<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Percent Uptime&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Percent Uptime&quot;}" name="percent_uptime@59f0b7bc4e80f148afed6c9b">
<p>Website Percent Uptime</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Incidents Reported&quot;,&quot;plot_type&quot;:&quot;bar&quot;,&quot;y_supremum&quot;:2.0,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Incidents Reported&quot;}" name="incidents_reported@59f0b7bc4e80f148afed6c9b">
<p>Website Incidents Reported</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website DNS Lookup Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;DNS Lookup Time (ms)&quot;}" name="http/name_lookup_time@59f0b7bd4e80f148afed6c9f">
<p>Website DNS Lookup Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website TCP Connection Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;TCP Connection Time (ms)&quot;}" name="http/tcp_connection_time@59f0b7bd4e80f148afed6c9f">
<p>Website TCP Connection Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website TLS Handshake Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;TLS Handshake Time (ms)&quot;}" name="http/tls_handshake_time@59f0b7bd4e80f148afed6c9f">
<p>Website TLS Handshake Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Content Generation Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Content Generation Time (ms)&quot;}" name="http/content_generation_time@59f0b7bd4e80f148afed6c9f">
<p>Website Content Generation Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Content Transfer Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Content Transfer Time (ms)&quot;}" name="http/content_transfer_time@59f0b7bd4e80f148afed6c9f">
<p>Website Content Transfer Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Total Elapsed Time (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Total Elapsed Time (ms)&quot;}" name="http/total_time@59f0b7bd4e80f148afed6c9f">
<p>Website Total Elapsed Time (ms)</p>

</li>

<li data-metric-options="{&quot;x_title&quot;:&quot;Time&quot;,&quot;title&quot;:&quot;Website Time to First Byte (ms)&quot;,&quot;enabled&quot;:true,&quot;top_level_enabled&quot;:true,&quot;y_title&quot;:&quot;Time to First Byte (ms)&quot;}" name="http/time_to_first_byte@59f0b7bd4e80f148afed6c9f">
<p>Website Time to First Byte (ms)</p>

</li>

</ol>
</div>
</section>

<section data-behavior="disable-ctts">
<header>
<h2>Recent History</h2>
<span data-tooltip="30-day">99.824% Uptime</span>

</header>
<div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T09:08:13Z" to_time="2023-12-12T11:42:13Z">Dec 12, 2023 1:08 AM–3:42 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T08:51:11Z" to_time="2023-12-12T09:03:13Z">Dec 12, 2023 12:51 AM–1:03 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T08:28:13Z" to_time="2023-12-12T08:48:11Z">Dec 12, 2023 12:28 AM–12:48 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T08:24:13Z" to_time="2023-12-12T08:25:10Z">Dec 12, 2023 12:24 AM–12:25 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T08:04:09Z" to_time="2023-12-12T08:21:12Z">Dec 12, 2023 12:04 AM–12:21 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-12-12T07:44:12Z" to_time="2023-12-12T08:01:11Z">Dec 11, 2023 11:44 PM–Dec 12, 2023 12:01 AM PST</time></p>

</div>
<p>No events for 7 days!</p>

<div>
<p><span>Outage</span>
<time datetime="2023-12-04T20:04:52Z" to_time="2023-12-04T20:05:48Z">Dec  4, 2023 12:04 PM–12:05 PM PST</time></p>

</div>
<p>No events for 12 days!</p>

<div>
<p><span>Outage</span>
<time datetime="2023-11-22T04:19:49Z" to_time="2023-11-22T04:20:50Z">Nov 21, 2023 8:19 PM–8:20 PM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-11-21T18:58:49Z" to_time="2023-11-21T19:01:48Z">Nov 21, 2023 10:58 AM–11:01 AM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-11-21T17:29:49Z" to_time="2023-11-21T17:31:51Z">Nov 21, 2023 9:29 AM–9:31 AM PST</time></p>

</div>
<p>No events for 3 days!</p>

<div>
<p><span>Outage</span>
<time datetime="2023-11-18T02:29:50Z" to_time="2023-11-18T02:30:49Z">Nov 17, 2023 6:29 PM–6:30 PM PST</time></p>

</div>

<div>
<p><span>Outage</span>
<time datetime="2023-11-17T19:48:50Z" to_time="2023-11-17T19:50:52Z">Nov 17, 2023 11:48 AM–11:50 AM PST</time></p>

</div>

</div>

</section>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Web Game in C (184 pts)]]></title>
            <link>https://anguscheng.com/post/2023-12-12-wasm-game-in-c-raylib/</link>
            <guid>38609865</guid>
            <pubDate>Tue, 12 Dec 2023 05:28:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://anguscheng.com/post/2023-12-12-wasm-game-in-c-raylib/">https://anguscheng.com/post/2023-12-12-wasm-game-in-c-raylib/</a>, See on <a href="https://news.ycombinator.com/item?id=38609865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Christmas is coming up. When I used to work at a bank the entire month of December and a bit of January we were placed under “Change Freeze”. We weren’t allowed to deploy updates to our applications during this period. The thinking was:</p>
<ol>
<li>A lot of people are away over Christmas</li>
<li>Application changes can lead to crashes</li>
<li>People who know how to fix those crashes might be away</li>
</ol>
<p>To prevent this they just stopped us from deploying updates. That meant December was a pretty relaxing time to be working at a bank, and it was also a good time to do big refactors, clean up //TODOs or mess around with some new technology. So how about learning to code a game in C and build it out to WebAssmebly?</p>
<h3 id="1-install-raylib">1. Install Raylib</h3>
<p>Go <a href="https://github.com/raysan5/raylib#build-and-installation">here</a> to get Raylib installed on your operating system. On Windows I used the installer which is available on itch.io and for MacOS I used Homebrew.</p>
<h3 id="2-create-mainc">2. Create main.c</h3>
<div><pre><code data-lang="C"><span>#include</span> <span>"raylib.h"</span><span>
</span><span></span>
<span>int</span> <span>main</span>(<span>void</span>) {
    InitWindow(<span>512</span>, <span>512</span>, <span>"Raylib WASM Example"</span>);
    Texture2D texture <span>=</span> LoadTexture(<span>"resources/smile.png"</span>);

    <span>while</span> (<span>!</span>WindowShouldClose()) {
        BeginDrawing();
        ClearBackground(BLACK);
        DrawTexture(texture, <span>0</span>, <span>0</span>, WHITE);
        EndDrawing();
    }

    UnloadTexture(texture);
    CloseWindow();
    <span>return</span> <span>0</span>;
}
</code></pre></div><p>Create this file and place it in <code>src/main.c</code></p>
<h3 id="3-copy-this-image">3. Copy this image</h3>
<p><img src="https://anguscheng.com/images/2023-12-12/smile.png" alt=""></p>
<p>Copy this image and place it in <code>src/resources/smile.png</code></p>
<h3 id="4a-build-macos">4A. Build MacOS</h3>
<pre><code>gcc -Wall -fcolor-diagnostics -fansi-escape-codes -g src/main.c `pkg-config --libs --cflags raylib` -o out/main.out
</code></pre><h3 id="4b-build-windows">4B. Build Windows</h3>
<pre><code>gcc src/main.c -lraylib -lopengl32 -lgdi32 -lwinmm -g -o out/main.exe
</code></pre><h3 id="run-the-build">Run the Build</h3>
<pre><code># Need to run from the src folder because the code wants the resources folder to be in the current working directory.
cd src

# MacOS
../out/main.out

# Windows
../out/main.exe
</code></pre><p><img src="https://anguscheng.com/images/2023-12-12/window.png" alt=""></p>
<p>You should see this if you did it right. If not, hang your head in shame.</p>
<h3 id="5-download-emscripten">5. Download Emscripten</h3>
<p>Now that we’re building successfully on Windows or Mac, we can get ready to build out to Web Assembly. I recommend you clone emsdk into the same directory where your game directory lives. You don’t have to but it’ll make running the build script we’re going to create easier.</p>
<pre><code># Get the emsdk repo
git clone https://github.com/emscripten-core/emsdk.git

# Enter that directory
cd emsdk

# Fetch the latest version of the emsdk (not needed the first time you clone)
git pull

# Download and install the latest SDK tools.
./emsdk install latest

# Make the "latest" SDK "active" for the current user. (writes .emscripten file)
./emsdk activate latest

# Activate PATH and other environment variables in the current terminal
source ./emsdk_env.sh
</code></pre><h3 id="6-create-shellhtml">6. Create shell.html</h3>
<p><a href="https://anguscheng.com/images/2023-12-12/shell">Download this HTML file</a> and place it in <code>/shell.html</code>. It’s going to wrap around our web assembly game.</p>
<h3 id="7-create-the-build-script">7. Create the build script</h3>
<div><pre><code data-lang="bash"><span>#!/bin/bash
</span><span></span>set -euo pipefail

<span># Get EMSDK on the PATH</span>
cd ../emsdk
source emsdk_env.sh

<span># Get into the /src folder</span>
cd ../raylib-wasm-example
cd src

<span># Build to Web Assembly</span>
emcc -o ../out/index.html <span>\
</span><span></span>    main.c -Os -Wall /Users/anguscheng/workspace/raylib/src/libraylib.a <span>\
</span><span></span>    -I. -I /usr/local/Cellar/raylib/4.5.0/include <span>\
</span><span></span>    -L. -L /Users/anguscheng/workspace/raylib/src <span>\
</span><span></span>    -s USE_GLFW<span>=</span><span>3</span> <span>\
</span><span></span>    -s ASYNCIFY <span>\
</span><span></span>    --shell-file ../shell.html <span>\
</span><span></span>    --preload-file resources <span>\
</span><span></span>    -s TOTAL_STACK<span>=</span>64MB <span>\
</span><span></span>    -s INITIAL_MEMORY<span>=</span>128MB <span>\
</span><span></span>    -s ASSERTIONS <span>\
</span><span></span>    -DPLATFORM_WEB

<span># Get into /out</span>
cd ../out

<span># Run the game</span>
emrun index.html
</code></pre></div><p>Copy this file to <code>/build-wasm.sh</code> you’ll have to change the -I and -L paths.</p>
<p>If it worked a browser should open up and show <a href="https://anguscheng.com/raylib-wasm-example/">this page</a>.</p>
<h3 id="github-repository">Github Repository</h3>
<p>All source files are available <a href="https://github.com/BallerIndustries/raylib-wasm-example">here</a>.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=j6akryezlzc">https://www.youtube.com/watch?v=j6akryezlzc</a></li>
<li><a href="https://github.com/raysan5/raylib/wiki/Working-for-Web-(HTML5)">https://github.com/raysan5/raylib/wiki/Working-for-Web-(HTML5)</a></li>
<li><a href="https://emscripten.org/docs/getting_started/downloads.html">https://emscripten.org/docs/getting_started/downloads.html</a></li>
</ul>
<ul>
  
</ul>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The mysterious second parameter to the x86 ENTER instruction (134 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20231211-00/?p=109126</link>
            <guid>38609747</guid>
            <pubDate>Tue, 12 Dec 2023 05:05:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20231211-00/?p=109126">https://devblogs.microsoft.com/oldnewthing/20231211-00/?p=109126</a>, See on <a href="https://news.ycombinator.com/item?id=38609747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            December 11th, 2023</p><!-- .entry-meta -->
        <p>The x86 instruction set has an <code>ENTER</code> instruction which builds a stack frame. It is almost always used with a zero as the second parameter.</p>
<pre>        enter   n, 0
</pre>
<p>This is functionally equivalent to</p>
<pre>        push    ebp
        mov     ebp, esp
        sub     esp, n
</pre>
<p>But what happens if you increase that second parameter beyond zero?</p>
<p>Values greater than zero for the second parameter are intended for languages like Pascal which support nested functions that can access the local variables of their lexical parents. <a title="What is a static chain pointer in the context of calling convention ABI?" href="https://devblogs.microsoft.com/oldnewthing/20231204-00/?p=109095"> We learned about these functions a short time ago</a>. But the designers of the x86 instruction set had a different design in mind for how a function can access the variables of its lexical parent: Instead of receiving a pointer to the start of a linked list of lexical parent frames, they receive an <i>array</i> of pointers to lexical parent frames.</p>
<p>In its full generality, the</p>
<pre>    enter n, k + 1
</pre>
<p>instruction goes like this:</p>

<pre>    push    ebp
    mov     internal_register, esp
   <span> sub     ebp, 4 </span> ⎱ <span><var>k</var> times</span>
   <span> push    [ebp]  </span> ⎰
    push    internal_register
    mov     ebp, internal_register
    sub     esp, n
</pre>
<p>If you ignore the order of operations and worry just about the final state, then you can reinterpret it like this, which I think captures the essence of the instruction better:</p>
<pre>    push    ebp
   <span> push    [ebp-4]   </span>
   <span> push    [ebp-8]   </span> <span><var>k</var> pushes</span>
   <span> :                 </span>
   <span> push    [ebp-4*<var>k</var>] </span>
    lea     ebp, [esp + 4*<var>k</var>]    ; where we pushed the previous ebp
    push    ebp		        ; add our own frame to the array
    sub     esp, n
</pre>
<p>Let’s look at our example function again.</p>
<pre>function Outer(n: integer) : integer;
    var i: integer;

    procedure Update(j: integer);
    begin
        i := i + j
    end;

    procedure Inner(m: integer);

        procedure MoreInner;
        begin
            Update(m)
        end;

    (* Inner body begins here *)
    begin
        MoreInner
    end;

(* Outer body begins here *)
begin
    i := 0;
    Inner(n);
    Outer := i
end;
</pre>
<p>On entry to <code>Outer</code>, the stack looks like this:</p>
<table>
<tbody>
<tr>
<td><code>n</code> parameter</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>return address</td>
<td>← <var>esp</var></td>
</tr>
</tbody>
</table>
<p>The <code>Outer</code> function establishes its stack frame by performing an <code>enter 4, 1</code>. The extra <code>1</code> at the end means that this is the outermost of a chain of nested functions. In our cookbook, <var>k</var> is zero, so the functional equivalent is</p>
<pre>    push    ebp
                                ; no pointers copied from parent
    lea     ebp, [esp+0]        ; equivalently, "mov ebp, esp"
    push    ebp                 ; pointer to our own frame
    sub     esp, 4
</pre>
<p>and we wind up with this stack frame for <code>Outer</code>:</p>
<table>
<tbody>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Outer</code> frame</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>n</code> parameter</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>return address</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
<td rowspan="2">← <var>ebp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
<td rowspan="2">&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>i</code></td>
<td>← <var>esp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>That extra <code>,1</code> caused us to push the address of where we saved the previous <code>ebp</code>, which I’ve called the <code>Outer</code> frame pointer. That value isn’t really useful to us right now, since we already have that value in the <var>ebp</var> register. But it comes in handy when we call <code>Inner</code>.</p>
<p>On entry to <code>Inner</code>, the stack looks like this:</p>
<table>
<tbody>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>m</code> parameter</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>return address</td>
<td>← <var>esp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>The <code>Inner</code> function performs an <code>enter 0, 2</code>. The 0 means that <code>Inner</code> has no local variables, and the <code>2</code> means that we are now the second level in a chain of nested functions.</p>
<p>The functional equivalent now has one extra memory push before we push a pointer to our own frame:</p>
<pre>    push    ebp
   <span> push    [ebp-4]  </span>           ; one pointer copied from parent
    lea     ebp, [esp+4]
    push    ebp                 ; pointer to our own frame
    sub     esp, 4
</pre>
<p>Before pushing the address of its own frame, the <code>enter</code> instruction also copies one pointer from the parent’s frame, namely the <code>Outer</code> frame pointer.</p>
<table>
<tbody>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Inner</code> frame</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>m</code> parameter</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Outer</code> frame</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>return address</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>n</code> parameter</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
<td rowspan="2">← <var>ebp</var></td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">return address</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Inner</code> frame pointer</td>
<td rowspan="2">← <var>esp</var></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>i</code></td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>Now things are interesting.</p>
<p>The <code>Inner</code> function has access to its own frame, via the <var>ebp</var> register (and redundantly via the <code>Inner</code> frame pointer on its stack). It also has access to the <code>Outer</code> frame through its local copy of the <code>Outer</code> frame pointer.</p>
<p>The next thing that happens is that <code>Inner</code> calls <code>More­Inner</code> with no parameters. This time <code>More­Inner</code> uses <code>enter 0, 3</code> where the 0 means that <code>More­Inner</code> has no local variables, and the <code>3</code> means that it is a nested function three levels deep, so it should copy <i>two</i> frame pointers from its parent.</p>
<table>
<tbody>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>MoreInner</code> frame</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>return address</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Inner</code> frame</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>m</code> parameter</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">return address</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>n</code> parameter</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2"><code>Inner</code> frame pointer</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">return address</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2"><code>MoreInner</code> frame pointer</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Inner</code> frame pointer</td>
<td rowspan="2">&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>i</code></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>The frame for <code>MoreInner</code> contains its own parameters and local variables (nothing), plus pointers to both parent frames, plus a pointer to its own frame (which <code>More­Inner</code> doesn’t use, but which is ready for any nested function to use).</p>
<p>The code generation for <code>MoreInner</code> therefore reads the value of <code>m</code> by following the <code>Inner</code> frame pointer and then reading the <code>m</code> parameter from the <code>Inner</code> frame’s parameter space.</p>
<p>After <code>More­Inner</code> calls <code>Update</code>, the <code>Update</code> function starts with an <code>enter 0, 2</code> because it is a level-2 nested function. This copies only the <code>Outer</code> frame pointer to <code>Update</code>‘s frame, resulting in this:</p>
<table>
<tbody>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Update</code> frame</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>j</code> parameter</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>Outer</code> frame</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>return address</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>n</code> parameter</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
<td rowspan="2">← <var>ebp</var></td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2">return address</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2">▶︎</td>
<td rowspan="2">previous <var>ebp</var></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td rowspan="2">&nbsp;</td>
<td rowspan="2"><code>Update</code> frame pointer</td>
<td rowspan="2">← <var>esp</var></td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td rowspan="2"><code>Outer</code> frame pointer</td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><code>i</code></td>
</tr>
<tr>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>I didn’t draw it, but the “previous <var>ebp</var>” in the <code>Update</code> frame points to the <code>More­Inner</code> frame.</p>
<p>The <code>Update</code> function reads <code>j</code> from its own parameter space and uses to update the <code>i</code> variable in <code>Outer</code>‘s frame by following the <code>Outer</code> frame pointer.</p>
<p>The result is the same as the <a title="What is a static chain pointer in the context of calling convention ABI?" href="https://devblogs.microsoft.com/oldnewthing/20231204-00/?p=109095"> System V Application Binary Interface static chain pointer</a>, but it’s done in a different way. Instead of passing the head of a linked list of frames, the <code>enter</code> instruction copies an entire array of pointers to frames. This reduces the number of instructions required in order to access faraway frames, but it increases the cost of a function call due to the extra copying.</p>
<p>I wonder if anybody uses the Intel design for nested functions. I suspect it’s silicon on the CPU that is completely wasted.</p>


        

        
		
        
	</div></div>]]></description>
        </item>
    </channel>
</rss>