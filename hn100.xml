(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Sep 2024 00:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Clojure 1.12.0 is now available (115 pts)]]></title>
            <link>https://clojure.org/news/2024/09/05/clojure-1-12-0</link>
            <guid>41460037</guid>
            <pubDate>Thu, 05 Sep 2024 20:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0">https://clojure.org/news/2024/09/05/clojure-1-12-0</a>, See on <a href="https://news.ycombinator.com/item?id=41460037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="add_libs"><a href="#add_libs"></a>2.1 Add libraries for interactive use</h3>
<p>There are many development-time cases where it would be useful to add a library interactively without restarting the JVM - speculative evaluation, adding a known dependency to your project, or adding a library to accomplish a specific task.</p>

<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-lib">add-lib</a> takes a lib that is not available on the classpath, and makes it available by downloading (if necessary) and adding to the classloader. Libs already on the classpath are not updated. If the coordinate is not provided, the newest Maven or git (if the library has an inferred git repo name) version or tag are used.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-libs">add-libs</a> is like <code>add-lib</code>, but resolves a set of new libraries and versions together.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/sync-deps">sync-deps</a> calls <code>add-libs</code> with any libs present in deps.edn, but not yet present on the classpath.</p>
</li>
</ul>
</div>
<p>These new functions are intended only for development-time interactive use at the repl - using a deps.edn is still the proper way to build and maintain production code. To this end, these functions all check that <a href="https://clojure.github.io/clojure/branch-master/clojure.core-api.html#clojure.core/%2Arepl%2A">*repl*</a> is bound to true (that flag is bound automatically by <code>clojure.main/repl</code>). In a clojure.main REPL, these new functions are automatically referred in the <code>user</code> namespace. In other repls, you may need to <code>(require '[clojure.repl.deps :refer :all])</code> before use.</p>
<p>Library resolution and download are provided by <a href="https://github.com/clojure/tools.deps">tools.deps</a>. However, you do not want to add tools.deps and its many dependencies to your project classpath during development, and thus we have also added a new api for invoking functions out of process via the Clojure CLI.</p>
</div>
<div>
<h3 id="tool_functions"><a href="#tool_functions"></a>2.2 Invoke tool functions out of process</h3>
<p>There are many useful tools you can use at development time, but which are not part of your project’s actual dependencies. The Clojure CLI provides explicit support for <a href="https://clojure.org/reference/clojure_cli#tools">tools</a> with their own classpath, but there was not previously a way to invoke these interactively.</p>
<p>Clojure now includes <a href="https://clojure.github.io/clojure/branch-master/clojure.tools.deps.interop-api.html#clojure.tools.deps.interop/invoke-tool">clojure.tools.deps.interop/invoke-tool</a> to invoke a tool function out of process. The classpath for the tool is defined in deps.edn and you do not need to add the tool’s dependencies to your project classpath.</p>
<p><code>add-lib</code> functionality is built using <code>invoke-tool</code> but you can also use it to build or invoke your own tools for interactive use. Find more about the function execution protocol on the <a href="https://clojure.org/reference/clojure_cli#function_protocol">CLI reference</a>.</p>
</div>
<div>
<h3 id="_2_3_start_and_control_external_processes"><a href="#_2_3_start_and_control_external_processes"></a>2.3 Start and control external processes</h3>
<p>For a long time, we’ve had the <code>clojure.java.shell</code> namespace, but over time Java has provided new APIs for process info, process control, and I/O redirection. This release adds a new namespace <a href="https://clojure.github.io/clojure/branch-master/index.html#clojure.java.process">clojure.java.process</a> that takes advantage of these APIs and is easier to use. See:</p>
<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/start">start</a> - full control over streams with access to the underlying Java objects for advanced usage</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/exec">exec</a> - covers the common case of executing an external process and returning its stdout on completion</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="method_values"><a href="#method_values"></a>2.4 Method values</h3>
<p>Clojure programmers often want to use Java methods in higher-order functions (e.g. passing a Java method to <code>map</code>). Until now, programmers have had to manually wrap methods in functions. This is verbose, and might require manual hinting for overload disambiguation, or incur incidental reflection or boxing.</p>
<p>Programmers can now use <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#qualified_methods">qualified methods</a> as ordinary functions in value contexts - the compiler will <a href="https://clojure.org/reference/java_interop#methodvalues">automatically generate the wrapping function</a>. The compiler will generate a reflective call when a qualified method does not resolve due to overloading. Developers can supply <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it.</p>
</div>
<div>
<h3 id="qualified_methods"><a href="#qualified_methods"></a>2.5 Qualified methods - <code>Class/method</code>, <code>Class/.method</code>, and <code>Class/new</code></h3>
<p>Java members inherently exist in a class.  For method values we need a way to explicitly specify the class of an instance method because there is no possibility for inference.</p>
<p>Qualified methods have value semantics when used in non-invocation positions:</p>
<div>
<ul>
<li>
<p><code>Classname/method</code> - value is a Clojure function that invokes a static method</p>
</li>
<li>
<p><code>Classname/.method</code> - value is a Clojure function that invokes an instance method</p>
</li>
<li>
<p><code>Classname/new</code> - value is a Clojure function that invokes a constructor</p>
</li>
</ul>
</div>
<p>Note: developers must use <code>Classname/method</code> and <code>Classname/.method</code> syntax to differentiate between static and instance methods.</p>
<p>Qualified method invocations with <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> use only the tags to resolve the method. Without param-tags they behave like the equivalent <a href="https://clojure.org/reference/java_interop#_the_dot_special_form">dot syntax</a>, except the qualifying class takes precedence over hints of the target object, and over its runtime type when invoked via reflection.</p>
<p>Note: Static fields are values and should be referenced without parens unless they are intended as function calls, e.g <code>(System/out)</code> should be <code>System/out</code>. Future Clojure releases will treat the field’s value as something invokable and invoke it.</p>
</div>
<div>
<h3 id="param-tags"><a href="#param-tags"></a>2.6 :param-tags metadata</h3>
<p>When used as values, qualified methods supply only the class and method name, and thus cannot resolve overloaded methods.</p>
<p>Developers can supply <a href="https://clojure.org/reference/java_interop#paramtags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it. The <code>:param-tags</code> metadata is a vector of zero or more tags: <code>[tag …​]</code>. A tag is any existing valid <code>:tag</code> metadata value. Each tag corresponds to a parameter in the desired signature (arity should match the number of tags). Parameters with non-overloaded types can use the placeholder <code>_</code> in lieu of the tag. When you supply :param-tags metadata on a qualified method, the metadata must allow the compiler to resolve it to a single method at compile time.</p>
<p>A new metadata reader syntax <code>^[tag …​]</code> attaches <code>:param-tags</code> metadata to member symbols, just as <code>^tag</code> attaches <code>:tag</code> metadata to a symbol.</p>
</div>
<div>
<h3 id="_2_7_array_class_syntax"><a href="#_2_7_array_class_syntax"></a>2.7 Array class syntax</h3>
<p>Clojure supports symbols naming classes both as a value (for class object) and as a type hint, but has not provided syntax for array classes other than strings.</p>
<p>Developers can now refer to an <a href="https://clojure.org/reference/java_interop#_class_access">array class</a> using a symbol of the form <code>ComponentClass/#dimensions</code>, eg <code>String/2</code> refers to the class of a 2 dimensional array of Strings. Component classes can be fully-qualified classes, imported classes, or primitives. Array class syntax can be used as both type hints and values.</p>
<p>Examples: <code>String/1</code>, <code>java.lang.String/1</code>, <code>long/2</code>.</p>
</div>
<div>
<h3 id="_2_8_functional_interfaces"><a href="#_2_8_functional_interfaces"></a>2.8 Functional interfaces</h3>
<p>Java programs emulate functions with Java functional interfaces (marked with the <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html">@FunctionalInterface</a> annotation), which have a single method.</p>
<p>Clojure developers can now invoke Java methods taking <a href="https://clojure.org/reference/java_interop#functional_interfaces">functional interfaces</a> by passing functions with matching arity. The Clojure compiler implicitly converts Clojure functions to the required functional interface by constructing a lambda adapter. You can explicitly coerce a Clojure function to a functional interface by hinting the binding name in a <code>let</code> binding, e.g. to avoid repeated adapter construction in a loop, e.g. <code>(let [^java.util.function.Predicate p even?] …​)</code>.</p>
</div>
<div>
<h3 id="_2_9_java_supplier_interop"><a href="#_2_9_java_supplier_interop"></a>2.9 Java Supplier interop</h3>
<p>Calling methods that take a <a href="https://docs.oracle.com/javase/8/docs/api/java/util/function/Supplier.html">Supplier</a> (a method that supplies a value) had required writing an adapter with reify. Clojure has a "value supplier" interface with semantic support already - <code>IDeref</code>. All <code>IDeref</code> impls (<code>delay</code>, <code>future</code>, <code>atom</code>, etc) now implement the <code>Supplier</code> interface directly.</p>
</div>
<div>
<h3 id="_2_10_streams_with_seq_into_reduce_and_transduce_support"><a href="#_2_10_streams_with_seq_into_reduce_and_transduce_support"></a>2.10 Streams with seq, into, reduce, and transduce support</h3>
<p>Java APIs increasingly return <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html">Stream</a>s and are hard to consume because they do not implement interfaces that Clojure already supports, and hard to interop with because Clojure doesn’t directly implement Java functional interfaces.</p>
<p>In addition to functional interface support, Clojure <a href="https://clojure.org/reference/java_interop#streams">now provides these functions</a> to interoperate with streams in an idiomatic manner, all functions behave analogously to their Clojure counterparts:</p>
<div>
<ul>
<li>
<p><code>(stream-seq! stream) ⇒ seq</code></p>
</li>
<li>
<p><code>(stream-reduce! f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-transduce! xf f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-into! to-coll [xf] stream) ⇒ to-coll</code></p>
</li>
</ul>
</div>
<p>All of these operations are terminal stream operations (they consume the stream).</p>
</div>
<div>
<h3 id="_2_11_persistentvector_implements_spliterable"><a href="#_2_11_persistentvector_implements_spliterable"></a>2.11 PersistentVector implements Spliterable</h3>
<p>Java collections implement streams via <a href="https://docs.oracle.com/javase/8/docs/api/java/util/Spliterator.html">"spliterators"</a>, iterators that can be split for faster parallel traversal. <code>PersistentVector</code> now provides a custom spliterator that supports parallelism, with greatly improved performance.</p>
</div>
<div>
<h3 id="_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"><a href="#_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"></a>2.12 Efficient drop and partition for persistent or algorithmic collections</h3>
<p>Partitioning of a collection uses a series of takes (to build a partition) and drops (to skip past that partition). <a href="https://clojure.atlassian.net/browse/CLJ-2713">CLJ-2713</a> adds a new internal interface (IDrop) indicating that a collection can drop more efficiently than sequential traversal, and implements that for persistent collections and algorithmic collections like <code>range</code> and <code>repeat</code>. These optimizations are used in <code>drop</code>, <code>nthrest</code>, and <code>nthnext</code>.</p>
<p>Additionally, there are new functions <code>partitionv</code>, <code>partitionv-all</code>, and <code>splitv-at</code> that are more efficient than their existing counterparts and produce vector partitions instead of realized seq partitions.</p>
</div>
<div>
<h3 id="_2_13_var_interning_policy"><a href="#_2_13_var_interning_policy"></a>2.13 Var interning policy</h3>
<p><a href="https://clojure.org/reference/vars#interning">Interning</a> a var in a namespace (vs aliasing) must create a stable reference that is never displaced, so that all references to an interned var get the same object. There were some cases where interned vars could get displaced and those have been tightened up in 1.12.0-alpha1. If you encounter this situation, you’ll see a warning like "REJECTED: attempt to replace interned var #'some-ns/foo with #'other-ns/foo in some-ns, you must ns-unmap first".</p>
<p>This addresses the root cause of an issue encountered with Clojure 1.11.0, which added new functions to clojure.core (particularly <code>abs</code>). Compiled code from an earlier version of Clojure with var names that matched the newly added functions in clojure.core would be unbound when loaded in a 1.11.0 runtime. In addition to <a href="https://clojure.atlassian.net/browse/CLJ-2711">CLJ-2711</a>, we rolled back a previous fix in this area (<a href="https://clojure.atlassian.net/browse/CLJ-1604">CLJ-1604</a>).</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common food dye found to make skin and muscle temporarily transparent (130 pts)]]></title>
            <link>https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</link>
            <guid>41459865</guid>
            <pubDate>Thu, 05 Sep 2024 19:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent">https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</a>, See on <a href="https://news.ycombinator.com/item?id=41459865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Researchers have peered into the brains and bodies of living animals after discovering that a common food dye can make skin, muscle and connective tissues temporarily transparent.</p><p>Applying the dye to the belly of a mouse made its liver, intestines and bladder clearly visible through the abdominal skin, while smearing it on the rodent’s scalp allowed scientists to see blood vessels in the animal’s brain.</p><p>Treated skin regained its normal colour when the dye was washed off, according to researchers at Stanford University, who believe the procedure opens up a host of applications in humans, from locating injuries and finding veins for drawing blood to monitoring digestive disorders and spotting tumours.</p><p>“Instead of relying on invasive biopsies, doctors might be able to diagnose deep-seated tumours by simply examining a person’s tissue without the need for invasive surgical removal,” said Dr Guosong Hong, a senior researcher on the project. “This technique could potentially make blood draws less painful by helping phlebotomists easily locate veins under the skin.”</p><p>The trick has echoes of the approach taken by Griffin in HG Wells’s 1897 novel, The Invisible Man, in which the brilliant but doomed scientist discovers that the secret to invisibility lies in matching an object’s refractive index, or ability to bend light, to that of the surrounding air.</p><p>When light penetrates biological tissue, much of it is scattered because the structures inside, such as fatty membranes and cell nuclei, have different refractive indices. As light moves from one refractive index to another, it bends, making tissue opaque. The same effect makes a pencil look bent when dropped in a glass of water.</p><p>Dr Zihao Ou and his colleagues at Stanford theorised, counterintuitively, that particular dyes could make certain wavelengths of light pass more easily through skin and other tissues. Strongly absorbing dyes alter the refractive index of tissues that absorb them, allowing scientists to match the refractive indices of different tissues and suppress any scattering.</p><figure id="9530018f-bade-4b02-a276-7aa89115801e" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Before and after images of the use of the dye on a rodent. In the second image the internal images can be seen in red." src="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" width="445" height="282.6678765880218" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Before and after images of the use of the dye on a rodent.</span> Photograph: handout</figcaption></figure><p>In a series of experiments <a href="https://doi.org/10.1126/science.adm6869" data-link-name="in body link">described in Science</a>, the researchers show how a fresh chicken breast became transparent to red light minutes after being immersed in tartrazine solution, a yellow food dye used in US Doritos, SunnyD drink and other products. The dye reduced light scattering inside the tissue, allowing the rays to penetrate more deeply.</p><p>The team then smeared the yellow dye on a mouse’s underbelly, making the abdominal skin see-through and revealing the rodent’s intestines and organs. In another experiment, they applied dye to a mouse’s shaved head and, with a technique called laser speckle contrast imaging, saw blood vessels in the animal’s brain.</p><p>“The most surprising part of this study is that we usually expect dye molecules to make things less transparent. For example, if you mix blue pen ink in water, the more ink you add, the less light can pass through the water,” Hong said. “In our experiment, when we dissolve tartrazine in an opaque material like muscle or skin, which normally scatters light, the more tartrazine we add, the clearer the material becomes. But only in the red part of the light spectrum. This goes against what we typically expect with dyes.”</p><p>The researchers describe the process as “reversible and repeatable”, with skin reverting to its natural colour once the dye is washed away. At the moment, transparency is limited to the depth the dye penetrates, but Hong said microneedle patches or injections could deliver the dye more deeply.</p><p>The procedure has not yet been tested on humans and researchers will need to show it is safe to use, particularly if the dye is injected beneath the skin.</p><p>Others stand to benefit from the breakthrough. Many scientists study naturally transparent animals, such as zebrafish, to see how organs and features of disease, such as cancer, develop in living creatures. With transparency dyes, a much wider range of animals could be studied in this way.</p><p>In an <a href="https://doi.org/10.1126/science.adr7935" data-link-name="in body link">accompanying article</a>, Christopher Rowlands and Jon Gorecki, of Imperial College London, say there will be “extremely broad interest” in the procedure, which, when combined with modern imaging techniques, could allow scientists to image an entire mouse brain or spot tumours beneath centimetre-thick tissues. “HG Wells, who studied biology under TH Huxley, as a student would surely approve,” they write.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reflection 70B, the top open-source model (125 pts)]]></title>
            <link>https://twitter.com/mattshumer_/status/1831767014341538166</link>
            <guid>41459781</guid>
            <pubDate>Thu, 05 Sep 2024 19:39:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mattshumer_/status/1831767014341538166">https://twitter.com/mattshumer_/status/1831767014341538166</a>, See on <a href="https://news.ycombinator.com/item?id=41459781">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[My job is to watch dreams die (2011) (201 pts)]]></title>
            <link>https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</link>
            <guid>41459365</guid>
            <pubDate>Thu, 05 Sep 2024 18:43:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/">https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</a>, See on <a href="https://news.ycombinator.com/item?id=41459365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="http://www.reddit.com/r/AskReddit/comments/k3ifx/whats_the_most_fucked_up_thing_youve_had_to_do_at/c2ha0il">Original post here</a>.</p>

<p>I work at a real estate office.  We primarily sell houses that were foreclosed on by lenders.  We aren't involved in the actual foreclosures or evictions - anonymous lawyers in the cloud somewhere is tasked with the paperwork - we are the boots on the ground that interacts with the actual walls, roofs and occasional bomb threat.</p>

<p>When the lender forecloses - or is thinking of foreclosing - on a property one of the first things that happens is they send somebody out to see if there is actually a house there and if there is anybody living there who needs to be evicted.  Lawyers are expensive so they send a real estate agent or a property preservation company out to check.  There is the occasional discovery of fraud where there was never a house on the parcel to begin with, but such instances are rare.  Sometimes this initial visit results in discovering a house that has burned down or demolished, is abandoned or occupied by somebody who has absolutely no connection with the homeowner.  Sometimes the houses are discovered to be crack dens or meth labs, sometimes the sites of cock or dog fighting operations, or you might even find a back yard filled with a pot cultivation that can't be traced back to anybody because it was planted in yet another vacant house in a blighted neighborhood.  The house could be worth less than zero - blighted to the point where you can't even give it away (this is a literal statement, I have tried to give away many houses or even vacant lots with no takers over the years) or it could be a waterfront mansion in a gated golf community worth well over seven figures that does not include the number "one".    Sometimes they are found to have been seized by the IRS, the local tax authority, the DEA or the US Marshal.  Variety is the rule.  The end results are the law.</p>

<p>If the house is occupied my job is to make contact and determine who they are: there are laws that establish what happens to a borrower as opposed to a tenant and the servicemember relief act adds an additional set of questions that must be answered. Some of the people have an idea of why I am there.  Some claim they never knew they were foreclosed on, or tell me that they have worked something out with their lender, some won't tell me a thing and some threaten me to never return in the name of the police, their lawyer, or the occasional "or else/if I were you".  During one initial visit the sight of 50-60 motorcycles parked on the lawn suggested that we try again the next day.  At a couple the police had cordoned off the area and at one they were in the process of dredging the lake searching for the body of a depressed former homeowner.</p>

<p>If nobody is home I have to determine if they are at work, on vacation, in the army, wintering/summering at their other home, in jail, in a nursing home, dead or if they moved away.  It isn't easy.  Utilities can be left on for months.  Neighbors can be staging the yard and house to appear occupied to prevent blight in their neighborhood.  By the same token people will stop cutting the lawn for months, let trash and old phone books pile up on their porch, lose gas and electric service and continue to live in properties that have not only physically unsafe to approach but are so filthy that when it comes time to clean them out the crews have to wear hazmat suits.  One house had a gallon pickle jar filled with dead roaches on the porch.  Somebody lived in that house and thought that was a logical thing to do.  People like me are tasked with first contact.</p>

<p>Evictions are expensive and time-consuming.  Ultimately once the process gets that far there isn't much that can be done to prevent it.  You didn't pay your mortgage, the lender gets the house back.  There are an infinite number of reasons why the mortgage couldn't be paid, some are more sympathetic than others, but in the end you will be leaving the property willingly or not.  The lawyers handle the evictions - they churn through the paperwork in the background, ten thousand properties at a time.  They have it down to rote function based on templates, personal experience with the various judges and intimate knowledge of the federal, state and municipal laws, along with dealing with the occasional sheriff who refuses to evict somebody, the informal policies established by the local judges and a myriad of other problems that can arise.  As a business decision many lenders have determined that it is cheaper to settle with the occupants - instead of going through the formal eviction they will offer cash.  In exchange for surrendering a property in reasonably clean condition with the furnace still hooked up, the kitchen not stripped and the basement not intentionally flooded the lender will cut the occupants a check.  It costs much less than an eviction, provides reasonable hope that the plumbing won't freeze and can take a fraction of the time to obtain possession.  This is where the personal element becomes real.</p>

<p>(Continued in comments)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UE5 Nanite in WebGPU (250 pts)]]></title>
            <link>https://github.com/Scthe/nanite-webgpu</link>
            <guid>41458987</guid>
            <pubDate>Thu, 05 Sep 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Scthe/nanite-webgpu">https://github.com/Scthe/nanite-webgpu</a>, See on <a href="https://news.ycombinator.com/item?id=41458987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Nanite WebGPU</h2><a id="user-content-nanite-webgpu" aria-label="Permalink: Nanite WebGPU" href="#nanite-webgpu"></a></p>
<blockquote>
<p dir="auto">TL;DR: <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Demo scene Jinx</a> (640m triangles). <a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene with many objects</a> (1.7b triangles). White triangles in the Jinx scene are software-rasterized. <strong>WebGPU is only available on Chrome!</strong></p>
</blockquote>
<p dir="auto">This project contains a <a href="https://youtu.be/qC5KtatMcUw?si=IOWaVk0sQNra_R6O&amp;t=97" rel="nofollow">Nanite</a> implementation in a web browser using WebGPU. This includes the meshlet LOD hierarchy, software rasterizer (at least as far as possible given the current state of WGSL), and billboard impostors. Culling on both per-instance and per-meshlet basis (frustum and occlusion culling in both cases). Supports textures and per-vertex normals. Possibly every statistic you can think of. There is a slider or a checkbox for every setting imaginable. Also works offline using Deno.</p>
<p dir="auto">First, we will see some screenshots, then there is (not even complete) list of features. Afterward, I will link you to a couple of <strong>demo scenes</strong> you can play with. In the FAQ section, you can read <strong>my thoughts about Nanite</strong>. Since this file got a bit long, I've moved usability-oriented stuff (stats/GUI explanation, build process, and unit test setup) into a separate <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>.</p>
<blockquote>
<p dir="auto">EDIT 16-08-2024: I've rewritten significant parts of this README once I had more time to look through it. And I've written <a href="https://github.com/Scthe/frostbitten-hair-webgpu">Frostbitten hair WebGPU</a> meantime #self-promo.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY"><img src="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY" alt="scene-multiobject"></a></p>
<p dir="auto"><em><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene</a> containing 1.7b triangles. Nearly 98% of the triangles are software rasterized, as it's much faster than hardware.</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo"><img src="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo" alt="scene-jinx"></a></p>
<p dir="auto"><em>My <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">primary test scene</a>. <a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Unfortunately, the best simplification we get is from 44k to 3k triangles. The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress the data to fit into 32 bits (u16 for depth, 2*u8 for octahedron-encoded normals). It's a painful limitation, but at least you can see the entire system is working.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Nanite
<ul dir="auto">
<li><strong>Meshlet LOD hierarchy.</strong>
<ul dir="auto">
<li>Mesh preprocessing executes in the browser, using WebAssembly for <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>. While it might raise eyebrows, this was one of the goals.</li>
<li>There is a file exporter too, if you don't like to wait between page refreshes.</li>
</ul>
</li>
<li><strong>Software rasterizer.</strong>
<ul dir="auto">
<li>WebGPU does not have the <code>atomic&lt;u64&gt;</code> needed to implement this feature efficiently. Currently, I'm packing depth (<code>u16</code>) and octahedron-encoded normals (<code>2 * u8</code>) into 32 bits. It's enough to show that the rasterizer works.</li>
<li>With only 32 bits, we are butchering the precision. My only concern here is to show that the rasterization works. If you see the software rasterized bunny model in the background it will be white and it will have <em>reasonable</em> shading. Reprojecting depth and "compressing" normals is enough to get something.. not offending.</li>
<li>This also affects the depth pyramid used for occlusion culling.</li>
<li>There are other algorithms to do this. PPLL, or something with tiles, or double rasterization (1st pass writes depth, 2nd does compareExchange). But the 32-bit limitation is only in WebGPU, so I choose to stick to UE5's solution instead.</li>
</ul>
</li>
<li><strong>Billboard impostors.</strong> 12 images around the UP-axis, blended (with dithering) based on the camera position. Does not handle up/down views. Contains both diffuse and normals, so we can do nice shading at a runtime. UE5 <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=97" rel="nofollow">uses</a> a more advanced version integrated with a visibility buffer.
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview demo scene</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
</li>
</ul>
</li>
<li>Culling:
<ul dir="auto">
<li><strong>Per-instance:</strong> frustum and occlusion culling.</li>
<li><strong>Per-meshlet:</strong> frustum and occlusion culling.</li>
<li><strong>Per-triangle:</strong> hardware backface culling and ofc. z-buffer. WebGPU does not have early-z.
<ul dir="auto">
<li>I have no idea how early-z works in WebGPU (it does not).</li>
</ul>
</li>
<li>I've also tried per-meshlet backface cone culling. It worked fine, but I cut it from the final release. See FAQ below for more details.</li>
<li>Occlusion culling is just a depth pyramid from the previous frame's depth buffer. No reprojection and no two-pass. The current implementation is enough to cull a lot of triangles (<strong>A LOT!</strong>) and to judge the performance impact (big improvement!). I expect someone will want to read the code, and they will be grateful this feature was not added.</li>
</ul>
</li>
<li>Switch between <strong>GPU-driven rendering</strong> and a <strong>naive CPU implementation</strong>. I have not spent much time optimizing the CPU version. It works, you can step through it with the debugger.</li>
<li>Supports <strong>textured models</strong> and <strong>many different objects</strong> at the same time.</li>
<li>Controls to <strong>change parameters at runtime</strong>. Debug views. "Freeze culling" allows the camera to move and inspect only what was drawn last frame.</li>
<li>A lot of <strong>stats</strong>. Memory, geometry. Total scene meshlets, triangles. Drawn meshlets, triangles (split between hardware and software rasterizer). Impostor count. Dedicated profiler button to get the timings.</li>
<li><strong>Custom file format</strong> so you don't have to preprocess the mesh every time. This is optional, you <strong>can also use an OBJ file</strong>.</li>
<li>Vertex <strong>position quantization</strong> (vec2u), <strong>octahedron encoded normals</strong> (vec2f).
<ul dir="auto">
<li>Position quantization is off by default. Toggle <code>CONFIG.useVertexQuantization</code> to enable. There are <em>funny</em> things happening to the numbers there, but everything <em>should</em> be handled correctly.</li>
</ul>
</li>
<li>Handles window resize. It's a web browser after all.</li>
<li>The whole app also <strong>runs offline in <a href="https://deno.com/" rel="nofollow">Deno</a></strong>. I've written shader unit tests this way.</li>
<li>Tons of WebGPU and WGSL code that you can copy to your own project. If you want to do something, I've either attempted to do it or discovered that it does not work.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Goals</h3><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<p dir="auto">There were 2 primary goals for this project:</p>
<ol dir="auto">
<li><strong>Simplicity.</strong> We start with an OBJ file and everything is done in the app. No magic pre-processing steps, Blender exports, etc. You set the breakpoint at <code>loadObjFile()</code> and F10 your way till the first frame finishes.</li>
<li><strong>Experimentation.</strong> I could have built this with Vulkan and Rust. None would touch it. Instead, it's a webpage. You click the link, uncheck the checkbox and the FPS tanks 40%. And you think to yourself: "OK, that was an important checkbox. But what about this slider?". Or: "How does scene X affect memory allocation?". Right now I know that a lot of code can be optimized. Yet it would not matter till the simplification problem is solved.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo Scenes</h2><a id="user-content-demo-scenes" aria-label="Permalink: Demo Scenes" href="#demo-scenes"></a></p>
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Jinx</a> (120*120 instances, 640m triangles). A single Jinx is 44k triangles simplified to 3k at 59 root meshlets. Uses an OBJ file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Lucy and dragons</a> (both objects at 70*70 instances, 1.7b triangles). See below for per-object details.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=lucy1b&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Lucy</a> (110*110 instances, 1.2b triangles). A single Lucy statue is 100k triangles simplified to 86 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=dragonJson&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Dragons</a> (70*70 instances, 1.2b triangles). A single dragon is 250k triangles simplified to 102 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=bunny1b&amp;impostors_threshold=1000&amp;softwarerasterizer_threshold=2400" rel="nofollow">Bunnies</a> (500*500 instances, 1.2b triangles). A single bunny is 5k triangles simplified to 96 at a single root meshlet. Uses an OBJ file. Bunnies are so small most are frustum culled.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">You can find details in <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>. Short version:</p>
<ul dir="auto">
<li>Use the <code>[W, S, A, D]</code> keys to move and <code>[Z, SPACEBAR]</code> to fly up or down. <code>[Shift]</code> to move faster.</li>
<li>If there is something weird, toggle culling options on/off. There are some minor bugs in the implementation.</li>
<li>The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress data to fit into 32 bits (u16 for depth, 2*u8 for octahedron encoded normals).
<ul dir="auto">
<li>16-bit depth is.. not a great idea. It produces <strong>tons</strong> of artifacts like z-fighting or leaks. Turn the software rasterizer off to easier inspect raw Nanite meshlets. Be prepared for a major performance hit!</li>
</ul>
</li>
<li>FPS might fluctuate due to the browser's enforced VSync. Use the "Profile" button instead.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What are the major differences compared to UE5's Nanite?</h3><a id="user-content-what-are-the-major-differences-compared-to-ue5s-nanite" aria-label="Permalink: What are the major differences compared to UE5's Nanite?" href="#what-are-the-major-differences-compared-to-ue5s-nanite"></a></p>
<ul dir="auto">
<li>Error metric is just a simple projected simplification error (read below).</li>
<li>Meshlet simplification is.. simplistic.</li>
<li>No two-pass occlusion culling.
<ul dir="auto">
<li>This would not be complicated to add, just tedious to debug. Unfortunately, it also has some interactions with the GUI settings. ATM some parts of the code are riddled with <code>ifs</code> for certain user settings. For example, you could press "Freeze culling" to stop updating the list of drawn meshlets. This includes software rasterized meshlets. Move the camera in this mode and all 10+ million 1 px-sized software rasterized triangles might become fullscreen. Adding two-pass occlusion culling might expose more such interactions. It would also make the code harder to read, which goes against my goals.</li>
</ul>
</li>
<li>No work queue in shaders. For meshlet culling and LOD selection, I dispatch thread per-meshlet.</li>
<li>No VRAM eviction of unused LODs and streaming.
<ul dir="auto">
<li>Theoretically, to load new meshlet data, you would write requested <code>meshletIds</code> into a separate GPUBuffer. Download it to RAM and load the content. Keep LRU (timestamp per-meshlet, visible from CPU) to manage evictions. In practice, I suspect you might also want to add a priority system.</li>
</ul>
</li>
<li>No visibility buffer. It's not possible with the <code>atomic&lt;u64&gt;</code> limitation that I have.
<ul dir="auto">
<li>BTW if you render material data into a GBuffer, you get Nanite integration with your material system for free.</li>
</ul>
</li>
<li>No built-in shadows/multiview.</li>
<li>My implementation focuses on using a predictable amount of memory for demo cases. This means it's not scalable if you have many <strong>different</strong> objects (not instances). You would have to know the upper bound of the drawn meshlets to preallocate buffers that hold data between the stages. The naive solutions like <code>bottomLevelMeshletsCount * instanceCount</code> easily end up in GBs of VRAM!</li>
<li>No BVH for instances (or any other hierarchical implementation). I just take all instances and frustum + occlusion cull them.</li>
<li>I don't have a GPU profiler on the web/Deno. Or a debugger, or printf for that matter.
<ul dir="auto">
<li>ITWouldGenerate_DX_CODE_THATIWOULDHAVE_TO_READ_ANYWAY_SONOiGUESS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does xxx billions of triangles mean anything?</h3><a id="user-content-does-xxx-billions-of-triangles-mean-anything" aria-label="Permalink: Does xxx billions of triangles mean anything?" href="#does-xxx-billions-of-triangles-mean-anything"></a></p>
<p dir="auto">There was a video on YouTube showing how Nanite handles 120 billion triangles. Yet most of them were frustum culled? Performance depends on a lot of factors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dense meshes</h4><a id="user-content-dense-meshes" aria-label="Permalink: Dense meshes" href="#dense-meshes"></a></p>
<p dir="auto">Having a lot of dense meshes up close could have a negative performance impact. Unless you are so close to them that they cover 50% of the screen. Then, the occlusion culling kicks in. Dense geometry also means that meshlets are small. 128 triangles in a 20,000,000 triangle mesh? They do not take much space on the screen and are easily occlusion/cone culled.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Instance count</h4><a id="user-content-instance-count" aria-label="Permalink: Instance count" href="#instance-count"></a></p>
<p dir="auto">What about millions of instances? Each has its own mat4x3 transform matrix. This consumes VRAM. Obligatory link to <a href="https://pharr.org/matt/blog/2018/07/09/moana-island-pbrt-2" rel="nofollow">swallowing the elephant (part 2)</a>. During the frame, you also need to store a list of things to render. In the worst-case scenario, each instance will render its most dense meshlets. In my implementation, this allocates <code>instanceCount * bottomLevelMeshletsCount * sizeof(vec2u)</code> bytes. A 5k triangle bunny might have only 56 fine-level meshlets (out of 159 total), but what if I want to render 100,000 of them? This is not a scalable memory allocation. In Chrome, WebGPU has a 128MB limit for storage buffers (can be raised if needed). You might notice that the demo scenes above were tuned to reflect that.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Scene arrangement</h4><a id="user-content-scene-arrangement" aria-label="Permalink: Scene arrangement" href="#scene-arrangement"></a></p>
<p dir="auto">The scenes in my app have objects arranged in a square. For far objects, only a small part will be visible. But they will use coarse meshlet LOD, that contains more than just a visible part. The visible part passes occlusion culling and leads to a lot of overdraw for the rest of the meshlet. This is not an optimal scene arrangement. You would also think that a dense grid placement (objects close to each other) is bad. It certainly renders more triangles close to the camera. But it also means that there are no huge differences in depth between them. This is a dream for occlusion culling. You could build a wall from high-poly meshes and it's actually one of the most performant scenarios. Objects far from each other mean that a random distant pixel pollutes the depth pyramid (the <a href="https://www.youtube.com/@MentourPilot/videos" rel="nofollow">Swiss cheese theory</a>). Does your scene have a floor? Can you <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=96" rel="nofollow">merge</a> far objects into one?</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">In practice</h4><a id="user-content-in-practice" aria-label="Permalink: In practice" href="#in-practice"></a></p>
<p dir="auto">This leads to the <strong>Jinx test scene</strong>. The character is skinny. Looking down each row/column of the grid you can see the gaps. There is space between her arm and torso. This kills occlusion culling. The model does not simplify well. 3k triangles in the most coarse LOD (see below for more details). It's death by thousands of 1-pixel triangles. Software rasterizer helps a lot. Yet given the scene arrangement, most of the instances are rendered as impostors. Up close, the hardware rasterizer takes over. All 3 systems have different strengths.</p>
<blockquote>
<p dir="auto">With UE5's simplification algorithm, the balance is probably shifted. Much more software rasterizer, and less hardware one. And I wager a bet they don't have to rely on impostors as much. Their coarse LOD would be less than 3k tris (again, see below).</p>
</blockquote>
<p dir="auto">Basically, there are a lot of use cases. If you want a <strong>stable</strong> Nanite implementation, you have to test each one. But if you want a big triangle count, there are ways to cheat that too.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What surprised you about Nanite?</h3><a id="user-content-what-surprised-you-about-nanite" aria-label="Permalink: What surprised you about Nanite?" href="#what-surprised-you-about-nanite"></a></p>
<ol dir="auto">
<li>The goal of the DAG is not to "use fewer triangles for far objects". The goal is to have a consistent 1 pixel == 1 triangle across the entire screen. A triangle is our "unit of data". The artist imports a sculpture from ZBrush. We need to need a way (through an error metric) to display it no matter if it's 1m or 500m from the camera. This is not possible with discrete LOD meshes (each LOD level is a separate geometry). Sometimes you would want an LOD between 2 levels. You need continuous LODs. This is the reason for the meshlet hierarchy. It allows you to "sample" geometry at any detail level you choose.</li>
<li>You spend more time working on culling and meshlets instead of Nanite itself. You <strong>WILL</strong> reimplement both <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a>.</li>
<li>Meshlet LOD hierarchy is quite easy to get working. Praise <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>! But if you want to do it efficiently, <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=50" rel="nofollow">it will be a pain</a>. See next question for full story. I just went with the simplest option.</li>
<li>If your mesh does not simplify cleanly, you end up with e.g. ~3000 triangles that cover a single pixel (Jinx scene). The efficiency scales with your mesh simplification code. And if you want pixel-sized triangles (the main selling point for most people), you <strong>need</strong> a software rasterizer. The billboard impostors are also a good stability-oriented fallback. As mentioned above, the whole system should work cohesively.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about mesh simplification?</h3><a id="user-content-what-about-mesh-simplification" aria-label="Permalink: What about mesh simplification?" href="#what-about-mesh-simplification"></a></p>
<blockquote>
<p dir="auto">Remember, we are not doing a simple "take a mesh and return something that has X% of the triangles". We are doing the simplification in the context of meshlets and METIS.</p>
</blockquote>
<p dir="auto">UE5 has its own mesh simplification code. It's the first thing that happens in the asset pipeline. Thus, everything saved here will have avalanche-like benefits for the rest of the system. It was also a problem with the Jinx model. On <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=95" rel="nofollow">slide 95</a> Brian Karis states that <strong>all</strong> their LOD graphs end at a <strong>single</strong> root cluster. So no matter the model you provide, they can simplify it to 128 triangles. It makes you less reliant on the impostors. In my app, I could e.g. increase meshoptimizer's <code>target_error</code> parameter. But consider the following story:</p>
<ol dir="auto">
<li>My first test model was a bunny with 5k triangles. Easy to debug (check for holes, etc.). It simplified into a single 128 tris meshlet. Nice!</li>
<li>I've tried to load the Jinx model. At some point, the simplification stopped. You gave it X triangles and received the same X triangles. This crashed my app on an assertion.</li>
<li>OK, so if the model does not simplify beyond some level, I will allow the DAG to have many roots. If you failed to remove at least 6+% of the triangles, stop the algorithm for this part of the mesh.</li>
<li>The Jinx model now works correctly. It stops simplifying beyond 7-9 LOD levels, but this only means there are many hierarchy roots.</li>
<li>I load the bunny again and it no longer simplifies to a single root meshlet. Turns out, <strong>a lot of the meshlets did not reduce triangles that much</strong>. But with enough iterations, for such a simple model, we were able to reduce it to only 128 triangles. The whole time we were getting the &lt;6% simplification for some meshlets (so 94% of triangles were left untouched). We just did not know about it. And a lot of meshlets were also not "full". They contained less than 128 triangles.</li>
</ol>
<blockquote>
<p dir="auto">To reproduce, use <code>const SCENE_FILE: SceneName = 'singleBunny';</code> and set <code>CONFIG.nanite.preprocess.simplificationFactorRequirement: 0.94</code>. This option requires triangle reduction by at least 6%. We end up with 512 triangles. Then, set <code>simplificationFactorRequirement: 0.97</code> (require reducing triangle count by at least 3%, which is much more lenient). You end up with a single root that has 116 tris.</p>
</blockquote>
<p dir="auto">It was my first time using meshoptimizer, so you can probably tune it better. In the offline setting, it's possible to retry simplification with a bigger <code>target_error</code>. Or increase <code>target_error</code> for more coarse meshlet levels? From my experiments, both of these changes do not matter. You could also allow the hierarchy to have the bottom children on different levels (probably? there are some issues with this approach e.g. non-uniform mesh density). Maybe generate conservative (with a bigger triangle count than usual), discrete LOD levels in an old way and then use them if the algorithm gets stuck? This makes the error metric and the entire hierarchy pointless. Introduce new custom vertices? Merge more meshlets than 4? Smaller meshlets? Replace meshoptimizer? UE5 also has special weights for METIS partitioning. <strong>Most important, can your (METIS-enchanced) simplification, guarantee that splitting 256 triangles into 128 triangles, will ALWAYS result in 128 triangles?</strong> I think that once you have this guarantee, the simplification (while still not trivial), is significantly easier. With it, you no longer have to think about the concept of triangles in your meshlet hierarchy. You can start thinking only about DAG and nodes. This highlights the need for goor bottom-level meshlets.</p>
<p dir="auto">You may need someone to dedicate their time only to simplification. Personally, I just got it to work and moved on.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk"><img src="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk" alt="simplification"></a></p>
<p dir="auto"><em>Trying to Nanite-simplify <a href="https://sketchfab.com/3d-models/modular-mecha-doll-neon-mask-1e0dcf3e016f4bc897d4b39819220732" rel="nofollow">Modular Mecha Doll Neon Mask</a> (910k tris) 3D model by Sketchfab user <a href="https://sketchfab.com/chambersu1996" rel="nofollow">Chambersu1996</a>. After the 5th hierarchy level, the simplification stops with 180k triangles left. This would be inefficient to render, but still manageable if we switched to impostors <strong>quickly</strong>. A better solution would be to actually spend X hours investigating the simplification process.</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about error metric?</h3><a id="user-content-what-about-error-metric" aria-label="Permalink: What about error metric?" href="#what-about-error-metric"></a></p>
<p dir="auto">Assume you have a mesh that has 20,000,000 triangles. With meshlet hierarchy, you can render it at any triangle count you would have wanted (with a minimum of 128 triangles - 1 meshlet). How do you choose the right meshlets? What does the <em>right meshlet</em> mean? At the end of the day, <strong>THIS</strong> is exactly what Nanite is. Everything else (simplification, meshlet DAG, software rasterizer, etc.) is just a prerequisite to actually start working on this problem. I admit, as the author of this repo, it's a bit disheartening.</p>
<p dir="auto">A few days ago, SIGGRAPH 2024 presentations were published. In <a href="https://advances.realtimerendering.com/s2024/content/Cao-NanoMesh/AdavanceRealtimeRendering_NanoMesh0810.pdf" rel="nofollow">"Seamless rendering on mobile"</a>, Shun Cao from Tencent Games provided the following metric (slide 12):</p>
<div dir="auto" data-snippet-clipboard-copy-content="device_factor = device_power * device_level
// from the slightly blurred graph image it seems to be:
// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000
decay_factor = 1 / (1 + exp(distance_to_view / decay_distance))
threshold = projected_area * device_factor * decay_factor"><pre><span>device_factor</span> <span>=</span> <span>device_power</span> <span>*</span> <span>device_level</span>
<span>// from the slightly blurred graph image it seems to be:</span>
<span>// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000</span>
<span>decay_factor</span> <span>=</span> <span>1</span> <span>/</span> <span>(</span><span>1</span> <span>+</span> <span>exp</span><span>(</span><span>distance_to_view</span> <span>/</span> <span>decay_distance</span><span>)</span><span>)</span>
<span>threshold</span> <span>=</span> <span>projected_area</span> <span>*</span> <span>device_factor</span> <span>*</span> <span>decay_factor</span></pre></div>
<p dir="auto"><em><a href="https://www.wolframalpha.com/input?i=f%28x%29+%3D+1+%2F+%281+%2B+exp%28-%28x-5000%29+%2F+1000%29%29+from+0+to+9000" rel="nofollow">Wolfram alpha for decay_factor</a> as far as I was able to decipher from the function image.</em></p>
<p dir="auto">I have used projected simplification error (as provided by meshoptimizer). It's not a great metric for Nanite. I think that other vertex attributes have to be part of this function too. You should be able to assign different weights on a per-attribute basis. Normals on Jinx's face were a huge problem. In my app, I could just move the LOD error threshold slider to the left. I can say that this approach has an educational value. You will have to find something better.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Should you write your own implementation of Nanite?</h3><a id="user-content-should-you-write-your-own-implementation-of-nanite" aria-label="Permalink: Should you write your own implementation of Nanite?" href="#should-you-write-your-own-implementation-of-nanite"></a></p>
<p dir="auto">Depends. The simplest answer is to just use UE5. You will not beat UE5 in its own game. Looking at Steam's front page, most of the games are simple enough to not need it. It's interesting that (at the time of the writing) the 2 most known Nanite titles are Fortnite and Senua's Saga: Hellblade II. Both have opposite objectives and tones. I recommend the Digital Foundry's <a href="https://www.youtube.com/watch?v=u-zmFVzUmPc" rel="nofollow">"Inside Senua's Saga: Hellblade 2 - An Unreal Engine 5 Masterpiece - The Ninja Theory Breakdown"</a>. E.g. they've mentioned a separate Houdini pipeline to extract transparency from static meshes. And while both games are different, both were developed by excellent engineering and visual teams.</p>
<p dir="auto">If you want to write your own implementation as a side project, then don't let me stop you. But unless you tackle simplification and error metric problems, you will end up with code similar to mine. You will still learn a lot.</p>
<p dir="auto">If you want to add this tech to the existing engine, I'm not a person you should be asking (I don't work in the industry). In my opinion, you should start by implementing <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> first. This is already quite a complex task. Multi-step culling is tricky. You have to handle scene and world chunk management. Animated meshes. And that's just the beginning. But with these incremental steps, you will have something that works and can be tested at every step of the transition. Once this is stable, you can try a software rasterizer. Even if you don't end up shipping it, there is a lot to learn. Depending on the codebase, it can be surprisingly easy to add. Only after you have done the above steps you should try adding Nanite-like tech. As the various sliders in my app can tell you, they are all required for Nanite to be performant. The basic meshlet hierarchy for a toy renderer is a weekend project. Real implementation will have to deal with simplification and error metric issues.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is per-meshlet backface cone culling worth it?</h3><a id="user-content-is-per-meshlet-backface-cone-culling-worth-it" aria-label="Permalink: Is per-meshlet backface cone culling worth it?" href="#is-per-meshlet-backface-cone-culling-worth-it"></a></p>
<p dir="auto">I've implemented the basics, but the gains are limited. Check the comment in <a href="https://github.com/Scthe/nanite-webgpu/blob/8c15e85b32d8b890ef573f58f1fbb782544f972c/src/constants.ts#L160">constants.ts</a> for implementation details.</p>
<ol dir="auto">
<li>It works best if you have a dense mesh where all triangles in a cluster have similar normals. Dense meshes are something that Nanite was designed for. Yet coarse LOD levels will have normals pointing in different directions. Arseny Kapoulkine had <a href="https://zeux.io/2023/04/28/triangle-backface-culling/#estimating-culling-efficiency" rel="nofollow">similar observations</a>.</li>
<li>There is some duplication with occlusion culling. Backfaces are behind front faces in the z-buffer.</li>
<li>Computing the cone is done on a per-meshlet level. For me, this means a WebAssembly call every time. This took 30% of the whole preprocessing step. Preprocessing all models offline would solve this problem. Yet it goes against my goals for this project. I want you to take the simplest possible 3D object format and see that my program works. That's why this app is a webpage and not Rust+Vulkan. No one would have cloned the repo to run the code. But everyone has clicked the demo links above (right?).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Honourable mentions</h2><a id="user-content-honourable-mentions" aria-label="Permalink: Honourable mentions" href="#honourable-mentions"></a></p>
<ul dir="auto">
<li>Arseny Kapoulkine. This app is only possible due to <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a>. I've also watched a few of niagara videos and read its source code. And read his blog.
<ul dir="auto">
<li><a href="https://github.com/zeux/meshoptimizer/pull/704" data-hovercard-type="pull_request" data-hovercard-url="/zeux/meshoptimizer/pull/704/hovercard">Newer meshoptimizer versions</a> have <code>meshopt_SimplifySparse</code> specifically for Nanite clones. I've not updated to this version as I am moving on from this app. I want to leave it in a state that I've tested during development.</li>
</ul>
</li>
<li>Folks from Epic Games. Not only for creating Nanite but also for being open on how it works under the hood.</li>
<li><a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> by Graham Wihlidal.</li>
<li><a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> by Ulrich Haar and Sebastian Aaltonen.</li>
<li><a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>.</li>
<li><a href="https://emscripten.org/" rel="nofollow">Emscripten</a>. Used to run both meshoptimizer and METIS in the browser.
<ul dir="auto">
<li><a href="https://marcoselvatici.github.io/WASM_tutorial/" rel="nofollow">Webassembly Tutorial</a> by Marco Selvatici.</li>
</ul>
</li>
<li><a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Used under <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC Attribution</a> license.
<ul dir="auto">
<li>I've merged the textures, adjusted UVs, and removed the weapon.</li>
</ul>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deploying Rust in Existing Firmware Codebases (103 pts)]]></title>
            <link>https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</link>
            <guid>41458508</guid>
            <pubDate>Thu, 05 Sep 2024 17:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html">https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</a>, See on <a href="https://news.ycombinator.com/item?id=41458508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[serverless-registry: A Docker registry backed by Workers and R2 (103 pts)]]></title>
            <link>https://github.com/cloudflare/serverless-registry</link>
            <guid>41458240</guid>
            <pubDate>Thu, 05 Sep 2024 16:34:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cloudflare/serverless-registry">https://github.com/cloudflare/serverless-registry</a>, See on <a href="https://news.ycombinator.com/item?id=41458240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Container Registry in Workers</h2><a id="user-content-container-registry-in-workers" aria-label="Permalink: Container Registry in Workers" href="#container-registry-in-workers"></a></p>
<p dir="auto">This repository contains a container registry implementation in Workers that uses R2.</p>
<p dir="auto">It supports all pushing and pulling workflows. It also supports
Username/Password and public key JWT based authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deployment</h3><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto">You have to install all the dependencies with <a href="https://pnpm.io/installation" rel="nofollow">pnpm</a> (other package managers may work, but only pnpm is supported.)</p>

<p dir="auto">After installation, there is a few steps to actually deploy the registry into production:</p>
<ol dir="auto">
<li>Have your own <code>wrangler</code> file.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp wrangler.toml.example wrangler.toml"><pre>$ cp wrangler.toml.example wrangler.toml</pre></div>
<ol start="2" dir="auto">
<li>Setup the R2 Bucket for this registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler --env production r2 bucket create r2-registry"><pre>$ npx wrangler --env production r2 bucket create r2-registry</pre></div>
<p dir="auto">Add this to your <code>wrangler.toml</code></p>
<div data-snippet-clipboard-copy-content="r2_buckets = [
    { binding = &quot;REGISTRY&quot;, bucket_name = &quot;r2-registry&quot;}
]"><pre><code>r2_buckets = [
    { binding = "REGISTRY", bucket_name = "r2-registry"}
]
</code></pre></div>
<ol start="3" dir="auto">
<li>Deploy your image registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler deploy --env production"><pre>$ npx wrangler deploy --env production</pre></div>
<p dir="auto">Your registry should be up and running. It will refuse any requests if you don't setup credentials.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding username password based authentication</h3><a id="user-content-adding-username-password-based-authentication" aria-label="Permalink: Adding username password based authentication" href="#adding-username-password-based-authentication"></a></p>
<p dir="auto">Set the USERNAME and PASSWORD as secrets with <code>npx wrangler secret put USERNAME --env production</code> and <code>npx wrangler secret put PASSWORD --env production</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding JWT authentication with public key</h3><a id="user-content-adding-jwt-authentication-with-public-key" aria-label="Permalink: Adding JWT authentication with public key" href="#adding-jwt-authentication-with-public-key"></a></p>
<p dir="auto">You can add a base64 encoded JWT public key to verify passwords (or token) that are signed by the private key.
<code>npx wrangler secret put JWT_REGISTRY_TOKENS_PUBLIC_KEY --env production</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using with Docker</h3><a id="user-content-using-with-docker" aria-label="Permalink: Using with Docker" href="#using-with-docker"></a></p>
<p dir="auto">You can use this registry with Docker to push and pull images.</p>
<p dir="auto">Example using <code>docker push</code> and <code>docker pull</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export REGISTRY_URL=your-url-here

# Replace $PASSWORD and $USERNAME with the actual credentials
echo $PASSWORD | docker login --username $USERNAME --password-stdin $REGISTRY_URL
docker pull ubuntu:latest
docker tag ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker push $REGISTRY_URL/ubuntu:latest

# Check that pulls work
docker rmi ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker pull $REGISTRY_URL/ubuntu:latest"><pre><span>export</span> REGISTRY_URL=your-url-here

<span><span>#</span> Replace $PASSWORD and $USERNAME with the actual credentials</span>
<span>echo</span> <span>$PASSWORD</span> <span>|</span> docker login --username <span>$USERNAME</span> --password-stdin <span>$REGISTRY_URL</span>
docker pull ubuntu:latest
docker tag ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker push <span>$REGISTRY_URL</span>/ubuntu:latest

<span><span>#</span> Check that pulls work</span>
docker rmi ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker pull <span>$REGISTRY_URL</span>/ubuntu:latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuring Pull fallback</h3><a id="user-content-configuring-pull-fallback" aria-label="Permalink: Configuring Pull fallback" href="#configuring-pull-fallback"></a></p>
<p dir="auto">You can configure the R2 regitry to fallback to another registry if
it doesn't exist in your R2 bucket. It will download from the registry
and copy it into the R2 bucket. In the next pull it will be able to pull it directly from R2.</p>
<p dir="auto">This is very useful for migrating from one registry to <code>serverless-registry</code>.</p>
<p dir="auto">It supports both Basic and Bearer authentications as explained in the
<a href="https://distribution.github.io/distribution/spec/auth/token/" rel="nofollow">registry spec</a>.</p>
<p dir="auto">In the wrangler.toml file:</p>
<div data-snippet-clipboard-copy-content="[env.production]
REGISTRIES_JSON = &quot;[{ \&quot;registry\&quot;: \&quot;https://url-to-other-registry\&quot;, \&quot;password_env\&quot;: \&quot;REGISTRY_TOKEN\&quot;, \&quot;username\&quot;: \&quot;username-to-use\&quot; }]&quot;"><pre><code>[env.production]
REGISTRIES_JSON = "[{ \"registry\": \"https://url-to-other-registry\", \"password_env\": \"REGISTRY_TOKEN\", \"username\": \"username-to-use\" }]"
</code></pre></div>
<p dir="auto">Set as a secret the registry token of the registry you want to setup
pull fallback in.</p>
<p dir="auto">For example <a href="https://cloud.google.com/artifact-registry/docs/reference/docker-api" rel="nofollow">gcr</a>:</p>
<div data-snippet-clipboard-copy-content="cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto"><a href="https://github.com/settings/tokens">Github</a> for example uses a simple token that you can copy.</p>
<div data-snippet-clipboard-copy-content="echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto">The trick is always looking for how you would login in Docker for
the target registry and setup the credentials.</p>
<p dir="auto"><strong>Never put a registry password/token inside the wrangler.toml, please always use <code>wrangler secrets put</code></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Known limitations</h3><a id="user-content-known-limitations" aria-label="Permalink: Known limitations" href="#known-limitations"></a></p>
<p dir="auto">Right now there is some limitations with this container registry.</p>
<ul dir="auto">
<li>Pushing with docker is limited to images that have layers of maximum size 500MB. Refer to maximum request body sizes in your Workers plan.</li>
<li>To circumvent that limitation, you can manually add the layer and the manifest into the R2 bucket or use a client that is able to chunk uploads in sizes less than 500MB (or the limit that you have in your Workers plan).</li>
<li>If you use <code>npx wrangler dev</code> and push to the R2 registry with docker, the R2 registry will have to buffer the request on the Worker.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The project is licensed under the <a href="https://opensource.org/licenses/apache-2.0/" rel="nofollow">Apache License</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contribution</h3><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">See <code>CONTRIBUTING.md</code> for contributing to the project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phind-405B and faster, high quality AI answers for everyone (169 pts)]]></title>
            <link>https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</link>
            <guid>41458083</guid>
            <pubDate>Thu, 05 Sep 2024 16:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches">https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</a>, See on <a href="https://news.ycombinator.com/item?id=41458083">Hacker News</a></p>
Couldn't get https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Boxed – Things I learned after lying in an MRI machine for 30 hours (105 pts)]]></title>
            <link>https://aethermug.com/posts/boxed</link>
            <guid>41457739</guid>
            <pubDate>Thu, 05 Sep 2024 15:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aethermug.com/posts/boxed">https://aethermug.com/posts/boxed</a>, See on <a href="https://news.ycombinator.com/item?id=41457739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p>Last year a researcher from a Japanese lab asked me if he could borrow my <a href="https://en.wikipedia.org/wiki/Aphantasia" rel="nofollow noopener noreferrer" target="_blank">aphantasic</a> brain for an experiment. Neuroscientists have been studying the weird capacity most people have of "visualizing", or mentally conjuring pictures of things that aren't there, for a long time. The problem is that the brain is such a wild tangle of interconnections that it's hard to tell which of its parts are involved in visualization and which are unrelated, or downstream of it. Comparing people who visualize with aphantasics who don't is a very convenient way to partially work around that problem.</p>
<p>I agreed to participate in the experiment. That decision led to me spending an inordinate amount of time deep inside an MRI, looking at pictures or trying (and failing) to imagine pictures. The machine, worth half a dozen Lamborghinis, is hidden somewhere in a basement on the University of Tokyo campus.</p>
<hr><figure id="floating-1"><video src="https://aethermug.com/assets/posts/aphantasia/mri.mp4" controls="" autoplay="" loop="" muted=""></video><figcaption>Click to play the animation. Source: Fastfission, CC BY-SA 3.0.</figcaption></figure>
<p>Essentially, an MRI is a big pipe you climb into, which happens to be capable of seeing right through you. Wrapped around the pipe is a hidden network of metal coils cooled to 9 degrees Celsius above absolute zero, constantly switching very large currents to fill the hollow inside with a strong magnetic field and shooting (harmless) radio waves at some corner of your body—the brain, in this case.</p>
<p>In neuroscience, MRI is used to detect microscopic changes in blood flow inside the brain, allowing the researchers to obtain a 3D video map of which neural networks are active at each instant.</p>
<p>In my experiments, the researchers record what goes on in my head when I look at a picture of a fire hydrant, then a camel, then a piece of wood, and so on, covering thousands of pictures over the months. Other times they asked me to imagine various things, or to remember images that I had been shown earlier.</p>
<p>The scientific side of these experiments is very interesting, but there are many other places to read about it in depth. Here, instead, I want to leave some notes on my subjective observations of this experience. I've found that lying down in a cramped space for hours at a time, fully awake and with only very weird kinds of pastimes, leads to some unusual reflections.</p>
<p>These are a few of those reflections, in approximate order of decreasing banality.</p>
<h2>1 - It's noisy</h2>
<p>Anyone who has spent more than two seconds in an MRI knows this, but if you're thinking of trying for the first time, be warned: those things are LOUD.</p>
<p>The high-frequency current switching causes the metal coils around you to expand and contract and vibrate like a jackhammer. You feel like a rescue team is trying all they can to get you out of an unbreakable iron coffin. If you're sensitive to loud noises, you might want to avoid it.</p>
<p>Usually they give you foam earplugs before you go in, and those make the experience bearable for me. Still, there are risks. The other day I mis-inserted one of the earplugs and got a taste of the unabridged experience: I tried to continue like that for a while, but I was so worried for my eardrum that eventually I had to stop the experiment early. When I got out I felt like the left side of my head had been in the front row at a rock concert. Not healthy.</p>
<p>Even with properly-inserted earplugs, the racket echoing all around you is the most unpleasant part of the experiment. It's surprisingly draining. If it weren't for that, I think I could keep going just fine for much longer.</p>
<h2>2 - It's <em>very</em> good at putting you to sleep</h2>
<p>Remaining horizontal in a semi-dark space for a while doesn't help you stay awake. This shouldn't come as a surprise to most people, but it surprised me. I usually have the opposite problem, because I <em>never</em> nap or sleep during daytime (as in, maybe once a year), and not for lack of trying. There is something about the infinite possible activities that I could be doing instead of sleeping that excites my brain during the day, even when I'm very tired.</p>
<p>Yet, looking at long sequences of unrelated pictures or—much worse—the same set of pictures over and over has an almost magical power to induce slumber. I haven't succumbed to it yet, but I've come close a few times. I should try something similar for my next napping attempt.</p>
<hr><figure id="floating-2"><img src="https://aethermug.com/assets/posts/boxed/mri.webp" alt="A gray surface with a gray stripe running through it."><figcaption>The scenery as seen from inside the machine.</figcaption></figure>
<h2>3 - My brain craves novelty</h2>
<p>Aside from the problem of sleepiness, I was shocked at how the lack of new stimuli can strain my grasp on my mental faculties.</p>
<p>Some experiments consist of seeing the same images hundreds of times, or repeating the same task over and over with only minor variations. Since an MRI pipe is essentially an isolation chamber, those tasks and images (and the noise) are the only sensory input you're going to get. After a while, my brain started rejecting them.</p>
<p>At first, it wasn't a big deal, but after a few weeks of those repetitive tasks, even the simple act of paying attention to what was in front of my eyes began to take a tremendous effort of concentration. I came out in shambles every time, depleted of life force, and I even thought of the word <em>torture</em> once or twice. But hey, someone's gotta do this.</p>
<p>Luckily that repetitive series of experiments ended just before I reached my breaking point. I wonder if this is something that can be trained, but I'm not sure I'd want to do that either.</p>
<h2>4 - Novel, random images are great for creativity</h2>
<p>Most of the experiments involve looking at non-repeating images, meaning that I see each one only once and never again. My brain is apparently fine with this and, with a good infusion of caffeine, it's actually happy to go on the ride.</p>
<p>This is an experience you don't usually have in your daily life. Normally you know, more or less, what to expect to see next. Even when you can't predict what's coming—when you're watching a movie, for example—things have some kind of connection to each other, some theme or context that ties them together. With random images in a lab, none of that exists. Now you're seeing a picture of a man blowing smoke from his mouth in the Grand Canyon, next it could be a close-up on a smudged corner of a book, or a group of penguins near an ice cliff, or a pile of broken CRT monitors, or something else altogether.</p>
<p>Every four seconds or so, you see something new that you would never have guessed from the previous pictures. Each time it's a different cascade of activations in your brain, evoking random memories, creating unexpected connections, and stimulating thoughts that would never have occurred to you.</p>
<p>Something strange happens: even though it's all purely random, the brain tries to make sense of it all, tries to find patterns and associations. With no time to establish conventional <a href="https://aethermug.com/posts/a-framing-is-a-choice-of-boundaries">framings</a>, it has to improvise, take in the images in a partly-unconscious way, without thorough processing. This, I think, is a great way to stimulate creativity.</p>
<p>A few times during those experiments, I came up with so many ideas—things to write about, better ways to explain things, new intriguing questions about the world, etc.—that my biggest worry was trying to remember them all for the 20 or 30 minutes left until the end of the session. In the short pauses between bursts of images, I tried to rehearse the list of ideas with shortened mnemonics, but found that I could only keep around five in my head before I forgot some of them.</p>
<p>This is an amazing state to have my brain in, and I wish I could induce it at will. Social media feeds look similar on the surface, but they don't give you truly random stimuli. Their contents are highly edited to appeal to the viewer, and come with lots of cultural baggage and trend-following. They don't work to unhinge my creativity—rather, they trap it.</p>
<p>What I need is an app that does nothing but show you truly random pictures, with no curation and no memetic aspirations. If you know of one, please let me know.</p>
<hr><figure id="floating-3"><img src="https://aethermug.com/assets/posts/boxed/dylan-nolte-qxYDhV0rBPk-unsplash.webp" alt="An athlete's hand being wrapped in tape by someone."><figcaption>A random picture. I bet you didn't see that coming. Source: Dylan Nolte, Unsplash </figcaption></figure>
<h2>5 - Our sense of time is non-linear even at the shortest scales</h2>
<p>Everybody has experienced the subjective relativity of time. When you have fun, it flies. When you're waiting anxiously, it never budges. But, before these experiments, I had never realized just how warpy my perception of time can be even on the scale of a couple of seconds.</p>
<p>These tasks with pictures require me to stay focused on what's shown on the screen. To ensure that I'm not distracted, the screen will show certain cues at random intervals, to which I have to react by pressing a button. Usually, the cue is the repetition of the same image twice in a row: normally each image stays there only four seconds, but sometimes it will flash back to itself instead of being replaced by a different image. This sounds like an easy thing to spot, and most of the time it is. But when I'm not in an optimal shape, it can become fiendishly difficult.</p>
<p>I find myself asking, have I seen this same picture <em>two seconds ago</em> or not? Is this still the first four seconds?</p>
<p>This is so strange and almost disconcerting. In the highly controlled environment of the lab, I can easily notice these lapses in my perception of time, but what about all the other times? Does my sense of time ebb and flow like that every few seconds of my waking life?</p>
<h2>6 - Having your thoughts monitored feels... weird</h2>
<p>Sometimes, during the experiments, I wonder about things like:</p>
<ul>
<li>Is it enough to just watch, or should I think intensely about the subject?</li>
<li>I was asked to imagine the picture of a duck, but I imagined a moving duck, beating its wings quickly. Will this taint the results?</li>
</ul>
<p>In order to protect the objectivity of the experiments, the researchers don't tell me exactly how they're analyzing the fMRI data, or what their hypotheses are. Still, I know that they happen to have the closest thing ever to <em>mind-reading technology</em>, which has interesting implications.</p>
<p>In previous studies, they have successfully trained generative AI to read fMRI scans and replicate the images people were thinking about, or to add captions describing those mental images. Considering the amount of data they're taking of my brain, it would be possible for them to train an AI for my specific brain patterns.</p>
<p>I don't think many people have experienced this situation before. People have had tyrants and Big Brothers spying on their actions and words for millennia, but has anyone ever had their thoughts monitored? In a sense, I feel naked.</p>
<p>For now, this is not a big problem. At worst, I might worry that they would know it when I'm distracted, and I might try not to think about kinky stuff (usually a big mistake). Not the end of the world. But I can't help imagining about the dystopian societies that could emerge if the same technology was somehow scaled to portable sizes and affordable prices.</p>
<h2>7 - We are usually oblivious to what our brains are doing</h2>
<p>I've only ever tried short-ish sessions of mindfulness-like meditation, where the goal is to free your mind of thoughts, focus on your breathing, or something along those lines. I don't know about other types of meditation, but I would guess that most of them are about relaxing or at least not thinking very hard. All of these may help you, in one way or another, to feel better and more centered and even, in some cases, to know your body and mind better.</p>
<p>But I doubt there is a kind of meditation that prompts the level of introspection that long hours in an MRI machine doing simple but focused tasks can give you.</p>
<p>Inside the machine, I have to remain very still with no phone to check, nothing to read, no tossing and turning just for the sake of it, nothing to fiddle with, and—given that the tasks all require a moderate but constant level of attention, no opportunity to get really lost in thought. I did this for over 30 hours now, and counting.</p>
<p>In other words, I got to spend a lot of time in the peaceful, forced company of myself, not too cognitively busy but neither focusing on breathing or clearing my head. It's a sort of Goldilocks zone not only for creativity (point 4), but also for the observation of how my mind works.</p>
<p>In this process, I've learned more about myself than I had in the previous two decades. For example, it's how I uncovered the details of how my non-visual imagination works—something I had never noticed before. One day, right after a session in the MRI, I ran to a cafe and wrote a Twitter thread explaining exactly what that is like for me. The thread had a surprising success, attracting the keen interest of lots of other people (you can read a copy of the thread <a href="https://aethermug.com/posts/aphantasia">here</a>). Apparently it's a kind of description that is usually hard to come by, yet it came easily to me with all that time of confinement.</p>
<p>This unexpected success at discovering new things about myself has encouraged me to try this introspection outside the lab, too. That's how I first realized I have <a href="https://synesthesia-test.com/time-space-synesthesia" rel="nofollow noopener noreferrer" target="_blank">time-space synesthesia</a>, something that I hadn't even heard of before. It's also how I realized that wearing special earplugs in noisy places helps me understand what the people around me are saying, mitigating a mild auditory processing disorder that I had never thought much about. And so on, with a new quirk or peculiarity coming to my attention every now and then as I do other things.</p>
<p>Somehow the mere fact of staying still in a state halfway between emptying my mind and filling it to the brim has helped me become more attuned to myself. I feel a bit more centered in the moment, so to speak. Much more than before, I now consider the brain to be an organ that you can observe and study, a <a href="https://aethermug.com/posts/a-black-box-view-of-life">black box</a> you can tinker with (carefully) to better understand it. This kind of exploration can be very fruitful, showing you what works best for you, what to avoid, how to be kind to yourself, and generally how to "use" your brain more expertly. ●</p>
<div><p>Cover image:</p><p><em>Photo by Vladimir Kramer, Unsplash</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant (194 pts)]]></title>
            <link>https://github.com/Mintplex-Labs/anything-llm</link>
            <guid>41457633</guid>
            <pubDate>Thu, 05 Sep 2024 15:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Mintplex-Labs/anything-llm">https://github.com/Mintplex-Labs/anything-llm</a>, See on <a href="https://news.ycombinator.com/item?id=41457633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
  <a href="https://anythingllm.com/" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/wordmark.png?raw=true" alt="AnythingLLM logo"></a>
</p>
<p><a href="https://trendshift.io/repositories/2415" rel="nofollow"><img src="https://camo.githubusercontent.com/13a0218d5bc383ce61ac2154a48e40d6ce0c079fe6c930524732238d033b7a72/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f32343135" alt="Mintplex-Labs%2Fanything-llm | Trendshift" width="250" height="55" data-canonical-src="https://trendshift.io/api/badge/repositories/2415"></a>
</p>
<p dir="auto">
    <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br>
    Chat with your docs, use AI Agents, hyper-configurable, multi-user, &amp; no frustrating set up required.
</p>
<p dir="auto">
  <a href="https://discord.gg/6UyHPeGZAC" rel="nofollow">
      <img src="https://camo.githubusercontent.com/3b816ca02eba9f9b0e63fe98bdfff52b93212792c74d87757fabdf67c42e9e36/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6d696e74706c65785f6c6162732d626c75652e7376673f7374796c653d666c6174266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d4141414245704972474141414149474e49556b304141486f6d41414341684141412b6741414149446f414142314d414141366d41414144715941414158634a79365554774141414831554578555251414141502f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f72362b75626e352b3775372f332b2f7633392f656e7136757271362f76372b393766333972623236656f715431425130704f54342b526b757a7337636e4b796b5a4b53304e4853486c3866647a6433656a6f365578505555424452647a63335277674968386a4a53416b4a6d3578637648783861616e714234694a46425456657a7437563568596c4a5656754c6a343370396669496d4b434d6e4b5a4b556c61616f7153456c4a32317763665430394f337537757672367a45304e72362f774355704b3571636e66372b2f6e68376645644b5448782b66307450554f546c3561697071696f754d477475627a3543524451344f7354477875666e35313568593761337548312f6758427964494f46686c5659577658323971616f7143516f4b7337507a2f507a38372f417755744f554e665932644852306d6872624f7672374535525579387a4e585232642f6633392b586c35555a4a53783068497a51334f647261322f7a382f476c736261476a7045524853657a73374c2f42775363724c5451344f646e61327a4d334f626d377533782f674b536d70396a5a3254314151752f7637317064586b56495372322b767967734c69496e4b54673750614f6c706973764d635847787a6b38506c646158504c793875377537726d36753753317473444277766a342b4d5045786265347565586d35732f51304b7966376577414141416f64464a4f5577414142436c73724e6a782f514d326c392f376c686d49366a54422f6b413147674b4a4e2b6e65613676792f4d4c5a515965564b4b3372564135744141414141574a4c523051422f774974336741414141643053553146422b634b4241416d4d5a42486a584941414149535355524256446a4c59324341416b596d5a685a574e6e594f446e593256685a6d4a6b5947564d44497963584e77367342426277386646796379456f5947666b4642445651674b414150794d6a516c35495745514444596749433846554d444b4b736d6c67415779694542574d6a474a5935594571784d4171474d57464e58414159584767416b594a5351326351464b436b59465253687133416d6b705267594a62676862553074624230547236756b626747684449313067795366427743774455574273596d706d447151744c4b327362545130624f3373485941384757594757576a34575473364f627534616d69344f546d3765786871654870352b344443564a5a42446d7164723775666e332b41726b5a676b4a2b6655334349526d67595746694f41525947766f354f5155486845554146546b462b6b564852734c42676b496579596d4c6a776f4f6334684d536b354a546e494e53303644433867776345455a3652715a476c704f6663335a4f626c352b675a2b5452324552574679425151464d4635656b6c6d7155705162352b52655536315a554f766b465656585851425341726169747132396f3147694b63664c7a63323975306d6a78427a71307451306b777735785a48744855476558686b5a6864784259675a3464304c49366334676a7764377369515172614f703141697651364375414b5a43444242525151514e516755622f4247663363714343695a4f636e4365335151494b484e5254706b36624467705a6a526b7a673370425154427264744363755a43676c7541443076506d4c316749647653697855755767714e7332594a2b4455686b4559787567676b476d4f515563636b72696f50544a434f58456e5a354a533559736c62476e75795645526c44444676474555504f5776777161483652566b484b6575444d4b36534b6e486c5668546778386a65546d71793645696a374b366e4c71694779507743687361314d55726e713177414141435630525668305a4746305a54706a636d5668644755414d6a41794d7930784d4330774e4651774d446f7a4f446f304f5373774d446f774d423956306138414141416c6445565964475268644755366257396b61575a35414449774d6a4d744d5441744d4452554d4441364d7a67364e446b724d4441364d44427543476b54414141414b4852465748526b5958526c4f6e52706257567a64474674634141794d44497a4c5445774c544130564441774f6a4d344f6a51354b7a41774f6a41774f5231497a4141414141424a52553545726b4a6767673d3d" alt="Discord" data-canonical-src="https://img.shields.io/badge/chat-mintplex_labs-blue.svg?style=flat&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAH1UExURQAAAP////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////r6+ubn5+7u7/3+/v39/enq6urq6/v7+97f39rb26eoqT1BQ0pOT4+Rkuzs7cnKykZKS0NHSHl8fdzd3ejo6UxPUUBDRdzc3RwgIh8jJSAkJm5xcvHx8aanqB4iJFBTVezt7V5hYlJVVuLj43p9fiImKCMnKZKUlaaoqSElJ21wcfT09O3u7uvr6zE0Nr6/wCUpK5qcnf7+/nh7fEdKTHx+f0tPUOTl5aipqiouMGtubz5CRDQ4OsTGxufn515hY7a3uH1/gXBydIOFhlVYWvX29qaoqCQoKs7Pz/Pz87/AwUtOUNfY2dHR0mhrbOvr7E5RUy8zNXR2d/f39+Xl5UZJSx0hIzQ3Odra2/z8/GlsbaGjpERHSezs7L/BwScrLTQ4Odna2zM3Obm7u3x/gKSmp9jZ2T1AQu/v71pdXkVISr2+vygsLiInKTg7PaOlpisvMcXGxzk8PldaXPLy8u7u7rm6u7S1tsDBwvj4+MPExbe4ueXm5s/Q0Kyf7ewAAAAodFJOUwAABClsrNjx/QM2l9/7lhmI6jTB/kA1GgKJN+nea6vy/MLZQYeVKK3rVA5tAAAAAWJLR0QB/wIt3gAAAAd0SU1FB+cKBAAmMZBHjXIAAAISSURBVDjLY2CAAkYmZhZWNnYODnY2VhZmJkYGVMDIycXNw6sBBbw8fFycyEoYGfkFBDVQgKAAPyMjQl5IWEQDDYgIC8FUMDKKsmlgAWyiEBWMjGJY5YEqxMAqGMWFNXAAYXGgAkYJSQ2cQFKCkYFRShq3AmkpRgYJbghbU0tbB0Tr6ukbgGhDI10gySfBwCwDUWBsYmpmDqQtLK2sbTQ0bO3sHYA8GWYGWWj4WTs6Obu4ami4OTm7exhqeHp5+4DCVJZBDmqdr7ufn3+ArkZgkJ+fU3CIRmgYWFiOARYGvo5OQUHhEUAFTkF+kVHRsLBgkIeyYmLjwoOc4hMSk5JTnINS06DC8gwcEEZ6RqZGlpOfc3ZObl5+gZ+TR2ERWFyBQQFMF5eklmqUpQb5+ReU61ZUOvkFVVXXQBSAraitq29o1GiKcfLzc29u0mjxBzq0tQ0kww5xZHtHUGeXhkZhdxBYgZ4d0LI6c4gjwd7siQQraOp1AivQ6CuAKZCDBBRQQQNQgUb/BGf3cqCCiZOcnCe3QQIKHNRTpk6bDgpZjRkzg3pBQTBrdtCcuZCgluAD0vPmL1gIdvSixUuWgqNs2YJ+DUhkEYxuggkGmOQUcckrioPTJCOXEnZ5JS5YslbGnuyVERlDDFvGEUPOWvwqaH6RVkHKeuDMK6SKnHlVhTgx8jeTmqy6Eij7K6nLqiGyPwChsa1MUrnq1wAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMy0xMC0wNFQwMDozODo0OSswMDowMB9V0a8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjMtMTAtMDRUMDA6Mzg6NDkrMDA6MDBuCGkTAAAAKHRFWHRkYXRlOnRpbWVzdGFtcAAyMDIzLTEwLTA0VDAwOjM4OjQ5KzAwOjAwOR1IzAAAAABJRU5ErkJggg==">
  </a> |
  <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">
      <img src="https://camo.githubusercontent.com/b4357f651d9e266e4b3854471ec55091f13a9f067f8b0144037ef6ab46239a66/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4d495426636f6c6f723d7768697465" alt="License" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white">
  </a> |
  <a href="https://docs.anythingllm.com/" rel="nofollow">
    Docs
  </a> |
   <a href="https://my.mintplexlabs.com/aio-checkout?product=anythingllm" rel="nofollow">
    Hosted Instance
  </a>
</p>
<p dir="auto">
  <b>English</b> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.zh-CN.md">简体中文</a> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.ja-JP.md">日本語</a>
</p>
<p dir="auto">
👉 AnythingLLM for desktop (Mac, Windows, &amp; Linux)! <a href="https://anythingllm.com/download" rel="nofollow"> Download Now</a>
</p>
<p dir="auto">A full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as references during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA"><img src="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA" alt="Chatting" data-animated-image=""></a></p>
<details>
<summary><kbd>Watch the demo!</kbd></summary>
<p dir="auto"><a href="https://youtu.be/f95rGD9trL0" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/youtube.png" alt="Watch the video"></a></p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Product Overview</h3><a id="user-content-product-overview" aria-label="Permalink: Product Overview" href="#product-overview"></a></p>
<p dir="auto">AnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.</p>
<p dir="auto">AnythingLLM divides your documents into objects called <code>workspaces</code>. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cool features of AnythingLLM</h2><a id="user-content-cool-features-of-anythingllm" aria-label="Permalink: Cool features of AnythingLLM" href="#cool-features-of-anythingllm"></a></p>
<ul dir="auto">
<li>🆕 <strong>Multi-modal support (both closed and open-source LLMs!)</strong></li>
<li>👤 Multi-user instance support and permissioning <em>Docker version only</em></li>
<li>🦾 Agents inside your workspace (browse the web, run code, etc)</li>
<li>💬 <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/embed/README.md">Custom Embeddable Chat widget for your website</a> <em>Docker version only</em></li>
<li>📖 Multiple document type support (PDF, TXT, DOCX, etc)</li>
<li>Simple chat UI with Drag-n-Drop funcitonality and clear citations.</li>
<li>100% Cloud deployment ready.</li>
<li>Works with all popular <a href="#supported-llms-embedder-models-speech-models-and-vector-databases">closed and open-source LLM providers</a>.</li>
<li>Built-in cost &amp; time-saving measures for managing very large documents compared to any other chat UI.</li>
<li>Full Developer API for custom integrations!</li>
<li>Much more...install and find out!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported LLMs, Embedder Models, Speech models, and Vector Databases</h3><a id="user-content-supported-llms-embedder-models-speech-models-and-vector-databases" aria-label="Permalink: Supported LLMs, Embedder Models, Speech models, and Vector Databases" href="#supported-llms-embedder-models-speech-models-and-vector-databases"></a></p>
<p dir="auto"><strong>Large Language Models (LLMs):</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md#text-generation-llm-selection">Any open-source llama.cpp compatible model</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI (Generic)</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://aws.amazon.com/bedrock/" rel="nofollow">AWS Bedrock</a></li>
<li><a href="https://www.anthropic.com/" rel="nofollow">Anthropic</a></li>
<li><a href="https://ai.google.dev/" rel="nofollow">Google Gemini Pro</a></li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face (chat models)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (chat models)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all models)</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all models)</a></li>
<li><a href="https://www.together.ai/" rel="nofollow">Together AI (chat models)</a></li>
<li><a href="https://www.perplexity.ai/" rel="nofollow">Perplexity (chat models)</a></li>
<li><a href="https://openrouter.ai/" rel="nofollow">OpenRouter (chat models)</a></li>
<li><a href="https://mistral.ai/" rel="nofollow">Mistral</a></li>
<li><a href="https://groq.com/" rel="nofollow">Groq</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
<li><a href="https://github.com/LostRuins/koboldcpp">KoboldCPP</a></li>
<li><a href="https://github.com/BerriAI/litellm">LiteLLM</a></li>
<li><a href="https://github.com/oobabooga/text-generation-webui">Text Generation Web UI</a></li>
</ul>
<p dir="auto"><strong>Embedder models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md">AnythingLLM Native Embedder</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (all)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all)</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
</ul>
<p dir="auto"><strong>Audio Transcription models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/tree/master/server/storage/models#audiovideo-transcription">AnythingLLM Built-in</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
</ul>
<p dir="auto"><strong>TTS (text-to-speech) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
<li><a href="https://github.com/rhasspy/piper">PiperTTSLocal - runs in browser</a></li>
<li><a href="https://platform.openai.com/docs/guides/text-to-speech/voice-options" rel="nofollow">OpenAI TTS</a></li>
<li><a href="https://elevenlabs.io/" rel="nofollow">ElevenLabs</a></li>
</ul>
<p dir="auto"><strong>STT (speech-to-text) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
</ul>
<p dir="auto"><strong>Vector Databases:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/lancedb/lancedb">LanceDB</a> (default)</li>
<li><a href="https://www.datastax.com/products/datastax-astra" rel="nofollow">Astra DB</a></li>
<li><a href="https://pinecone.io/" rel="nofollow">Pinecone</a></li>
<li><a href="https://trychroma.com/" rel="nofollow">Chroma</a></li>
<li><a href="https://weaviate.io/" rel="nofollow">Weaviate</a></li>
<li><a href="https://qdrant.tech/" rel="nofollow">Qdrant</a></li>
<li><a href="https://milvus.io/" rel="nofollow">Milvus</a></li>
<li><a href="https://zilliz.com/" rel="nofollow">Zilliz</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Technical Overview</h3><a id="user-content-technical-overview" aria-label="Permalink: Technical Overview" href="#technical-overview"></a></p>
<p dir="auto">This monorepo consists of three main sections:</p>
<ul dir="auto">
<li><code>frontend</code>: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.</li>
<li><code>server</code>: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.</li>
<li><code>collector</code>: NodeJS express server that process and parses documents from the UI.</li>
<li><code>docker</code>: Docker instructions and build process + information for building from source.</li>
<li><code>embed</code>: Submodule for generation &amp; creation of the <a href="https://github.com/Mintplex-Labs/anythingllm-embed">web embed widget</a>.</li>
<li><code>browser-extension</code>: Submodule for the <a href="https://github.com/Mintplex-Labs/anythingllm-extension">chrome browser extension</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛳 Self Hosting</h2><a id="user-content--self-hosting" aria-label="Permalink: 🛳 Self Hosting" href="#-self-hosting"></a></p>
<p dir="auto">Mintplex Labs &amp; the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Docker</th>
<th>AWS</th>
<th>GCP</th>
<th>Digital Ocean</th>
<th>Render.com</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/docker.png" alt="Deploy on Docker"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/aws/cloudformation/DEPLOY.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/aws.png" alt="Deploy on AWS"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/gcp/deployment/DEPLOY.md"><img src="https://camo.githubusercontent.com/8e9acf4df0af19c7eb27d48d2bd9966f7311d8ae1fe4900f504b8d4eabb8d769/68747470733a2f2f6465706c6f792e636c6f75642e72756e2f627574746f6e2e737667" alt="Deploy on GCP" data-canonical-src="https://deploy.cloud.run/button.svg"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/digitalocean/terraform/DEPLOY.md"><img src="https://camo.githubusercontent.com/e093a0ed531124a715aad44362848ca2cff28c3182c2c0ca4a70b2564b681f59/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" alt="Deploy on DigitalOcean" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&amp;branch=render" rel="nofollow"><img src="https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" alt="Deploy on Render.com" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Railway</th>
<th>RepoCloud</th>
<th>Elestio</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://railway.app/template/HNSCS1?referralCode=WFgJkn" rel="nofollow"><img src="https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" alt="Deploy on Railway" data-canonical-src="https://railway.app/button.svg"></a></td>
<td><a href="https://repocloud.io/details/?app_id=276" rel="nofollow"><img src="https://camo.githubusercontent.com/ac294f4b769f6436dadb17205434b234b32e4d88831f182c522fd36b4534a7a3/68747470733a2f2f64313674307063343834367835322e636c6f756466726f6e742e6e65742f6465706c6f796c6f62652e737667" alt="Deploy on RepoCloud" data-canonical-src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg"></a></td>
<td><a href="https://elest.io/open-source/anythingllm" rel="nofollow"><img src="https://camo.githubusercontent.com/76131e33a65d9a0728265791dcf78030580a9932466d139e5ae38f87f926c5b5/68747470733a2f2f656c6573742e696f2f696d616765732f6c6f676f732f6465706c6f792d746f2d656c657374696f2d62746e2e706e67" alt="Deploy on Elestio" data-canonical-src="https://elest.io/images/logos/deploy-to-elestio-btn.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/BARE_METAL.md">or set up a production AnythingLLM instance without Docker →</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to setup for development</h2><a id="user-content-how-to-setup-for-development" aria-label="Permalink: How to setup for development" href="#how-to-setup-for-development"></a></p>
<ul dir="auto">
<li><code>yarn setup</code> To fill in the required <code>.env</code> files you'll need in each of the application sections (from root of repo).
<ul dir="auto">
<li>Go fill those out before proceeding. Ensure <code>server/.env.development</code> is filled or else things won't work right.</li>
</ul>
</li>
<li><code>yarn dev:server</code> To boot the server locally (from root of repo).</li>
<li><code>yarn dev:frontend</code> To boot the frontend locally (from root of repo).</li>
<li><code>yarn dev:collector</code> To then run the document collector (from root of repo).</li>
</ul>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/documents/DOCUMENTS.md">Learn about documents</a></p>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/vector-cache/VECTOR_CACHE.md">Learn about vector caching</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Telemetry &amp; Privacy</h2><a id="user-content-telemetry--privacy" aria-label="Permalink: Telemetry &amp; Privacy" href="#telemetry--privacy"></a></p>
<p dir="auto">AnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.</p>
<details>
<summary><kbd>More about Telemetry &amp; Privacy for AnythingLLM</kbd></summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why?</h3><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">We use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Opting out</h3><a id="user-content-opting-out" aria-label="Permalink: Opting out" href="#opting-out"></a></p>
<p dir="auto">Set <code>DISABLE_TELEMETRY</code> in your server or docker .env settings to "true" to opt out of telemetry. You can also do this in-app by going to the sidebar &gt; <code>Privacy</code> and disabling telemetry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What do you explicitly track?</h3><a id="user-content-what-do-you-explicitly-track" aria-label="Permalink: What do you explicitly track?" href="#what-do-you-explicitly-track"></a></p>
<p dir="auto">We will only track usage details that help us make product and roadmap decisions, specifically:</p>
<ul dir="auto">
<li>Typ of your installation (Docker or Desktop)</li>
<li>When a document is added or removed. No information <em>about</em> the document. Just that the event occurred. This gives us an idea of use.</li>
<li>Type of vector database in use. Let's us know which vector database provider is the most used to prioritize changes when updates arrive for that provider.</li>
<li>Type of LLM in use. Let's us know the most popular choice and prioritize changes when updates arrive for that provider.</li>
<li>Chat is sent. This is the most regular "event" and gives us an idea of the daily-activity of this project across all installations. Again, only the event is sent - we have no information on the nature or content of the chat itself.</li>
</ul>
<p dir="auto">You can verify these claims by finding all locations <code>Telemetry.sendTelemetry</code> is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. No IP or other identifying information is collected. The Telemetry provider is <a href="https://posthog.com/" rel="nofollow">PostHog</a> - an open-source telemetry collection service.</p>
<p dir="auto"><a href="https://github.com/search?q=repo%3AMintplex-Labs%2Fanything-llm%20.sendTelemetry(&amp;type=code">View all telemetry events in source code</a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">👋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👋 Contributing" href="#-contributing"></a></p>
<ul dir="auto">
<li>create issue</li>
<li>create PR with branch name format of <code>&lt;issue number&gt;-&lt;short name&gt;</code></li>
<li>LGTM from core-team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Contributors</h2><a id="user-content--contributors" aria-label="Permalink: 🌟 Contributors" href="#-contributors"></a></p>
<p dir="auto"><a href="https://github.com/mintplex-labs/anything-llm/graphs/contributors"><img src="https://camo.githubusercontent.com/216243cb397375babf4a9b21f6a6968b7589f578ff1daa0db6ae56a33ca4997a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d" alt="anythingllm contributors" data-canonical-src="https://contrib.rocks/image?repo=mintplex-labs/anything-llm"></a></p>
<p dir="auto"><a href="https://star-history.com/#mintplex-labs/anything-llm&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/455256132c080bc5bbf5419d1c14fa4d1a5727d75c41c4df0bc8e8fe7dbccb63/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d26747970653d54696d656c696e65" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=mintplex-labs/anything-llm&amp;type=Timeline"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 More Products</h2><a id="user-content--more-products" aria-label="Permalink: 🔗 More Products" href="#-more-products"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/mintplex-labs/vector-admin">VectorAdmin</a>:</strong> An all-in-one GUI &amp; tool-suite for managing vector databases.</li>
<li><strong><a href="https://github.com/Mintplex-Labs/openai-assistant-swarm">OpenAI Assistant Swarm</a>:</strong> Turn your entire library of OpenAI assistants into one single army commanded from a single agent.</li>
</ul>
<p dir="auto"><a href="#readme-top"><img src="https://camo.githubusercontent.com/d658b6c3935e61bd4aab9a571190fc3c48cbafc89fe6b16250c0cba28ed73234/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4241434b5f544f5f544f502d3232323632383f7374796c653d666c61742d737175617265" alt="" data-canonical-src="https://img.shields.io/badge/-BACK_TO_TOP-222628?style=flat-square"></a></p>
<hr>
<p dir="auto">Copyright © 2024 <a href="https://github.com/mintplex-labs">Mintplex Labs</a>. <br>
This project is <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">MIT</a> licensed.</p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaProteo generates novel proteins for biology and health research (217 pts)]]></title>
            <link>https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</link>
            <guid>41457331</guid>
            <pubDate>Thu, 05 Sep 2024 15:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/">https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/</a>, See on <a href="https://news.ycombinator.com/item?id=41457331">Hacker News</a></p>
Couldn't get https://deepmind.google/discover/blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform (110 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41456552</link>
            <guid>41456552</guid>
            <pubDate>Thu, 05 Sep 2024 13:42:43 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41456552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41456552">
      <td><span></span></td>      <td><center><a id="up_41456552" href="https://news.ycombinator.com/vote?id=41456552&amp;how=up&amp;goto=item%3Fid%3D41456552"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41456552">Launch HN: Maitai (YC S24) – Self-Optimizing LLM Platform</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41456552">104 points</span> by <a href="https://news.ycombinator.com/user?id=cmdalsanto">cmdalsanto</a> <span title="2024-09-05T13:42:43.000000Z"><a href="https://news.ycombinator.com/item?id=41456552">8 hours ago</a></span> <span id="unv_41456552"></span> | <a href="https://news.ycombinator.com/hide?id=41456552&amp;goto=item%3Fid%3D41456552">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Maitai%20%28YC%20S24%29%20%E2%80%93%20Self-Optimizing%20LLM%20Platform&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41456552&amp;auth=5b3d028aa07cccfc7151d9d6f9e80c752e36b1c3">favorite</a> | <a href="https://news.ycombinator.com/item?id=41456552">50&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN - this is Christian and Ian from Maitai (<a href="https://trymaitai.ai/">https://trymaitai.ai</a>). We're building an LLM platform that optimizes request routing, autocorrects bad responses, and automatically fine-tunes new application-specific models with incremental improvements. Here’s a demo video:  <a href="https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?sid=7097fd84-ea85-42cd-9616-84abc1087a56" rel="nofollow">https://www.loom.com/share/a2cd9192359840cab5274ccba399bd87?...</a>.</p><p>If you want to try it out, we built a game (<a href="https://maitaistreasure.com/" rel="nofollow">https://maitaistreasure.com</a>) to show how our real-time autocorrections work with mission-critical expectations (like never giving financial advice). Try and coax the bot to give you the secret phrase in its system prompt. If you're the first to crack it, you can email us the phrase and win a bounty. Maitai is used to make sure the bot always adheres to our expectations, and thus never gives up the secret phrase.</p><p>We built Maitai because getting an LLM app into production and maintaining it is a slog. Teams spend most of their time on LLM reliability rather than their main product. We experienced this ourselves at our previous jobs deploying AI-enabled applications for Presto—the vast majority of time was making sure the model did what we wanted it to do.</p><p>For example, one of our customers builds AI ordering agents for restaurants. It's crucial that their LLMs return results in a predictable, consistent manner throughout the conversation. If not, it leads to a poor guest experience and a staff member may intervene. At the end of the order conversation, they need to ensure that the order cart matches what the customer requested before it's submitted to the Point of Sale system. It's common for a human-in-the-loop to review critical pieces of information like this, but it’s costly to set up such a pipeline and it’s difficult to scale. When it's time to send out a receipt and payment link, they must first get the customer's consent to receive text messages, else they risk fines for violating the Telephone Consumer Protection Act. To boot, getting from 0 to 1 usually relies on inefficient general-purpose models that aren't viable at any sort of scale beyond proof of concept.</p><p>Since reliability is the #1 thing hindering the adoption of LLMs in production, we decided to help change that. Here's how it works:</p><p>1. Maitai sits between the client and the LLMs as a super lightweight proxy, analyzing traffic to automatically build a robust set of expectations for how the LLM should respond.</p><p>2. The application sends a request to Maitai, and Maitai forwards it to the appropriate LLM (user specified, but we'll preemptively fallback to a similar model if we notice issues with the primary model).</p><p>3. We intercept the response from the LLM, and evaluate it against the expectations we had previously built.</p><p>4. If we notice that an expectation was not met, we surface a fault (Slack, webhook) and can, optionally, substitute the faulty response with a clean response to be sent back to the client. This check and correction adds about 250ms on average right now, and we're working on making it faster.</p><p>5. We use all of the data from evaluating model responses to fine-tune application-specific models. We're working on automating this step for passive incremental improvements. We'd like to get it to a point where our user's inference step just gets better, faster, and cheaper over time without them having to do anything.</p><p>Our hope is that we take on the reliability and resiliency problems of the LLMs for our customers, and make it so they can focus on domain specific problems instead.</p><p>We're self-serve (<a href="https://portal.trymaitai.ai/">https://portal.trymaitai.ai</a>), and have both Python and Node SDKs that mock OpenAI's for quick integration. Users can set their preferences for primary and secondary (fallback) models in our Portal, or in code. Right now, the expectations we use for real-time evaluations are automatically generated, but we manually go through and do some pruning before enabling them. Fine-tuning is all done manually for now.</p><p>We charge for platform usage, plus a monthly application fee. Customers can bring their own LLM provider API keys, or use ours and pay at-cost for what they use. We have contracts with most of our current customers, so we are still trying to figure out what's right for our pay-as-you-go plan.</p><p>We securely store requests and responses that go through Maitai, as well as derivative data such as evaluation results. This information is used for fine-tuning models, accessible only by the organization the data belongs to. Data is never shared between our users.  API keys we manage on behalf of our customers are only injected before sending to the LLM provider, and never leave our servers otherwise. We're working on SOC2 and HIPAA compliance, as well as a self-hosted solution for companies with extremely sensitive data privacy requirements.</p><p>We’d love to get your feedback on what we’re building, or hear about your experience building around LLMs!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Hacker League – Open-Source Rocket League on Linux (172 pts)]]></title>
            <link>https://github.com/moritztng/hacker-league</link>
            <guid>41456411</guid>
            <pubDate>Thu, 05 Sep 2024 13:24:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/moritztng/hacker-league">https://github.com/moritztng/hacker-league</a>, See on <a href="https://news.ycombinator.com/item?id=41456411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><details open="">
  <summary>
    
    <span aria-label="Video description hacker-league.mp4">hacker-league.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" data-canonical-src="https://private-user-images.githubusercontent.com/19519902/364779763-3a630d46-ec17-4da8-8879-76320ea563fe.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NTA1MDEsIm5iZiI6MTcyNTU1MDIwMSwicGF0aCI6Ii8xOTUxOTkwMi8zNjQ3Nzk3NjMtM2E2MzBkNDYtZWMxNy00ZGE4LTg4NzktNzYzMjBlYTU2M2ZlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE1MzAwMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTNkMDg3MjdiZmFkMTZiODk4YTJiYzBkYzc4MTk4ZGNmOTJjZjYxMjY3NzI1MGMzOTFjNTg1Y2YxNWVkOWE1OWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.C7QQg92wm3q0awLIIoj6I8rQuGAJsOeWGMyqE9soFcQ" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto">Currently only debian based distros with x86_64. Please help me build it on other platforms. If you have an external GPU, make sure the drivers are installed</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install curl &amp;&amp; curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh | bash"><pre>sudo apt install curl <span>&amp;&amp;</span> curl -sL https://raw.githubusercontent.com/moritztng/hacker-league/main/install.sh <span>|</span> bash</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Play</h2><a id="user-content-play" aria-label="Permalink: Play" href="#play"></a></p>
<p dir="auto">Use a gamepad for maximum fun</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd hacker-league
./hacker-league"><pre><span>cd</span> hacker-league
./hacker-league</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build from source</h2><a id="user-content-build-from-source" aria-label="Permalink: Build from source" href="#build-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/moritztng/hacker-league.git
cd hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o &quot;gamepad.txt&quot; https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt"><pre>git clone https://github.com/moritztng/hacker-league.git
<span>cd</span> hacker-league
sudo apt install libvulkan-dev vulkan-validationlayers-dev spirv-tools libglfw3-dev libglm-dev libeigen3-dev vim-common xxd g++ make
curl -L -o ./shaders/glslc https://github.com/moritztng/hacker-league/releases/download/glslc/glslc
chmod +x ./shaders/glslc
make debug
curl -L -o <span><span>"</span>gamepad.txt<span>"</span></span> https://raw.githubusercontent.com/mdqinc/SDL_GameControllerDB/master/gamecontrollerdb.txt</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<ul dir="auto">
<li>Discord Server: <a href="https://discord.gg/BbNH27st" rel="nofollow">https://discord.gg/BbNH27st</a></li>
<li>I build in public on X: <a href="https://x.com/moritzthuening" rel="nofollow">https://x.com/moritzthuening</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Porting systemd to musl Libc-powered Linux (178 pts)]]></title>
            <link>https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</link>
            <guid>41454779</guid>
            <pubDate>Thu, 05 Sep 2024 08:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/">https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=41454779">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-449">
	<!-- .entry-header -->

	
	
	<div>
		
<p>I have completed an <a href="https://code.atwilcox.tech/sphen/scaly/systemd/-/commits/adelie-v256">initial new port</a> of systemd to musl.  This patch set does not share much in common with the existing OpenEmbedded patchset.  I wanted to make a fully updated patch series targeting more current releases of systemd and musl, taking advantage of the latest features and updates in both.  I also took a focus on writing patches that could be sent for consideration of inclusion upstream.</p>



<p>The final result is a system that appears to be surprisingly reliable considering the newness of the port, and very fast to boot.</p>



<h2>Why?</h2>



<p>I have wanted to do this work for almost a decade.  In fact, a mention of multiple service manager options – including systemd – is present on the <a href="https://web.archive.org/web/20160109133511/http://adelielinux.org/">original Adélie Web site from 2015</a>.  Other initiatives have always taken priority, until someone contacted us at <a href="https://www.wilcoxti.com/">Wilcox Technologies Inc. (WTI)</a> interested in paying on a contract basis to see this effort completed.</p>



<p>I want to be clear that I did not do this for money.  I believe strongly that there is genuine value in having multiple service managers available.  User freedom and user choice matter.  There are cases where this support would have been useful to me and to many others in the community.  I am excited to see this work nearing public release and honoured to be a part of creating more choice in the Linux world.</p>



<h2>How?</h2>



<p>I started with the latest release tag, v256.5.  I wanted a version closely aligned to upstream’s current progress, yet not too far away from the present “stable” 255 release.  I also wanted to make sure that the fallout from upstream’s removal of split-/usr support would be felt to its maximum, since reverting that decision is a high priority.</p>



<p>I fixed build errors as they happened until I finally had a built systemd.  During this phase, I consulted the original OE patchset twice: once for usage of <code>GLOB_BRACE</code>, and the other for usage of <code>malloc_info</code> and <code>malloc_trim</code>.  Otherwise, the patchset was authored entirely originally, mostly through the day (and into the night) of August 16th, 2024.</p>



<p>Many of the issues seen were related to inclusion of headers, and I am already working on <a href="https://github.com/systemd/systemd/pull/34064">bringing</a> those fixes <a href="https://github.com/systemd/systemd/pull/34066">upstream</a>.  It was then time to run the test suite.</p>



<h2>Tests!</h2>



<p>The test suite started with 27 failures.  Most of them were simple fixes, but one that gave me a lot of trouble was the <code>time-util</code> test.  The <a href="https://git.musl-libc.org/cgit/musl/tree/src/time/strptime.c"><code>strptime</code> implementation in musl</a> does not support the <code>%z</code> format specifier (for time zones), which the systemd test relies on.  I could have disabled those tests, but I felt like this would be taking away a lot of functionality.  I considered things like important journals from other systems – they would likely have timestamps with <code>%z</code> formats.  I wrote a <code>%z</code> translation for systemd and saw the tests passing.</p>



<p>Other test failures were simple <a href="https://github.com/systemd/systemd/pull/34065">C portability fixes</a>, which are also in the process of being sent upstream.</p>



<p>The test suite for <code>systemd-sysusers</code> was the next sticky one.  It really exercises the POSIX library functions <code>getgrent</code> and <code>getpwent</code>.  The musl implementations of these are fine, but they don’t cope well with the old NIS compatibility shims from the glibc world.  They also <a href="https://www.openwall.com/lists/musl/2021/10/11/1">can’t handle “incomplete” lines</a>.  The fix for incomplete line handling is pending, so in the meantime I made the test have no incomplete lines.  I added a shim for the NIS compatibility entries in systemd’s <code>putgrent_sane</code> function, making it a little less “sane” but fixing the support perfectly.</p>



<p>Then it was time for the final failing test: <code>test-recurse-dir</code>, which was receiving an <code>EFAULT</code> error code from <code>getdents64</code>.  Discussing this with my friends on the Gentoo IRC, we began to wonder if this was an architecture-specific bug.  I was doing my port work on my Talos II, a 64-bit PowerPC system.  I copied the code over to an Intel Skylake and found the test suite passed.  That was both good, in that the tests were all passing, but also bad, because it meant I was dealing with a PPC64-specific bug.  I wasn’t sure if this was a kernel bug, a musl bug, or a systemd bug.</p>



<p>Digging into it further, I realised that the pointer math being done would be invalid when cast to a pointer-to-structure on PPC64 due to object alignment guarantees in the ABI.  I changed it to use a temporary variable for the pointer math and casting that temporary, and it passed!</p>



<p>And that is how I became the first person alive to see systemd passing its entire test suite on a big-endian 64-bit PowerPC musl libc system.</p>



<h2>The moment of truth</h2>



<p>I created a small disk image and ran a very strange command: <code>apk add adelie-base-posix dash-binsh systemd</code>.  I booted it up as a KVM VM in Qemu and saw “Welcome to Adélie Linux 1.0 Beta 5” before a rather ungraceful – and due to Qemu framebuffer endian issues, colour-swapped – segmentation fault:</p>



<figure><img data-attachment-id="453" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/the-dawn-of-a-new-error/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png" data-orig-size="1736,1392" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="the-dawn-of-a-new-error" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=840" tabindex="0" role="button" width="1024" height="821" src="https://catfox.life/wp-content/uploads/2024/09/the-dawn-of-a-new-error.png?w=1024" alt=""><figcaption>Welcome to an endian-swapped systemd core dump!</figcaption></figure>



<p>Debugging this was an experience in early systems debugging that I haven’t had in years.  There’s a great summary on this methodology at <a href="https://linus.schreibt.jetzt/posts/debugging-pid1.html">Linus’s blog</a>.</p>



<p>It turned out that I had disabled a test from build-util as I incorrectly assumed that was only used when debugging in the build root.  Since I did not want to spend time digging around how it manually parses ELF files to find their RPATH entries for a feature we are unlikely to use, I stubbed that functionality out entirely.  We can always fix it later.</p>



<p>Recreating the disk image and booting it up, I was greeted by an Adélie “rescue” environment booted by systemd.  It was frankly bizarre, but also really cool.</p>



<figure><img data-attachment-id="454" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/habbening/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png" data-orig-size="2064,1470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="habbening" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=840" tabindex="0" role="button" width="1024" height="729" src="https://catfox.life/wp-content/uploads/2024/09/habbening.png?w=1024" alt=""><figcaption>The first time systemd ever booted an Adélie Linux system.</figcaption></figure>



<h2>From walking to flying</h2>



<p>Next, I built test packages on the Skylake builder we are using for x86_64 development.  I have a 2012 MacBook Pro that I keep around for testing various experiments, and this felt like a good system for the ultimate experiment.  The goal: swapping init systems with a single command.</p>



<p>It turns out that D-Bus and PolicyKit require systemd support to be enabled or disabled at build-time.  There is no way to build them in a way that allows them to operate on both types of init system.  This is an area I would like to work on more in the future.</p>



<p>I wrote package recipes for both that are built against systemd and “replace” the non-systemd versions.  I also marked them to <code>install_if</code> the system wanted systemd.</p>



<p>Next up were some more configuration and dependency fixes.  I found out via this experiment that some of the Adélie system packages do not place their pkg-config files in the proper place.  I also decided that if I’m already testing this far, I’d use networkd to bring up the laptop in question.</p>



<p>I ran the fateful command <code>apk del openrc; apk add systemd</code> and rebooted.  To my surprise, it all worked!  The system booted up perfectly with systemd.  The oddest sight was my utmps units running:</p>



<figure><img data-attachment-id="455" data-permalink="https://catfox.life/2024/09/05/porting-systemd-to-musl-libc-powered-linux/need-more-fromage-2/" data-orig-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png" data-orig-size="1280,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="need-more-fromage-2" data-image-description="" data-image-caption="" data-medium-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=300" data-large-file="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=840" tabindex="0" role="button" width="1024" height="640" src="https://catfox.life/wp-content/uploads/2024/09/need-more-fromage-2.png?w=1024" alt=""><figcaption>systemd running s6-ipcserver.  The irony is not lost on me.</figcaption></figure>



<h2>Still needed: polish…</h2>



<p>While the system works really well, and boots in 1/3rd the time of OpenRC on the same system, it isn’t ready for prime time just yet.</p>



<p>Rebooting from a KDE session causes the compositor to freeze.  I can reboot manually from a command line, or even from a Konsole inside the session, but not using Plasma’s built-in power buttons.  This may be a PolicyKit issue – I haven’t debugged it properly yet.</p>



<p>There aren’t any service unit files written or packaged yet, other than OpenSSH and utmps.  We are working with our sponsor on an effort to add -systemd split packages to any of the packages with -openrc splits.  We should be able to rely on upstream units where present, and lean on Gentoo and Fedora’s systemd experts to have good base files to reference when needed.  I’ve already landed <a href="https://git.adelielinux.org/adelie/abuild/-/merge_requests/16">support for this in abuild</a>.</p>



<h2>…and You!</h2>



<p>This project could not have happened without the generous sponsors of Wilcox Technologies Inc (WTI) making it possible, nor without the generous sponsors of Adélie Linux keeping the distro running.  Please consider supporting both <a href="https://www.adelielinux.org/contribute/">Adélie Linux</a> and <a href="https://www.patreon.com/WilcoxTech">WTI</a> if you have the means.  Together, we are creating the future of Linux systems – a future where users have the choice and freedom to use the tooling they desire.</p>



<p>If you want to help test this new system out, please reach out to me on IRC (awilfox on Interlinked or Libera), or the Adéliegram Telegram channel.  It will be a little while before a public beta will be available, as more review and discussion with other projects is needed.  We are working with systemd, musl, and other projects to make this as smooth as possible.  We want to ensure that what we provide for testing is up to our highest standards of quality.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a WoW (World of Warcraft) Server in Elixir (176 pts)]]></title>
            <link>https://pikdum.dev/posts/thistle-tea/</link>
            <guid>41454741</guid>
            <pubDate>Thu, 05 Sep 2024 08:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pikdum.dev/posts/thistle-tea/">https://pikdum.dev/posts/thistle-tea/</a>, See on <a href="https://news.ycombinator.com/item?id=41454741">Hacker News</a></p>
Couldn't get https://pikdum.dev/posts/thistle-tea/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Desed: Demystify and debug your sed scripts (147 pts)]]></title>
            <link>https://github.com/SoptikHa2/desed</link>
            <guid>41453557</guid>
            <pubDate>Thu, 05 Sep 2024 04:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SoptikHa2/desed">https://github.com/SoptikHa2/desed</a>, See on <a href="https://news.ycombinator.com/item?id=41453557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Desed</h2><a id="user-content-desed" aria-label="Permalink: Desed" href="#desed"></a></p>
<p dir="auto">Demystify and debug your sed scripts, from comfort of your terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SoptikHa2/desed/blob/master/img/desed.gif"><img src="https://github.com/SoptikHa2/desed/raw/master/img/desed.gif" alt="desed usage example" data-animated-image=""></a></p>
<p dir="auto">Desed is a command line tool with beautiful TUI that provides users with comfortable interface and practical debugger, used to step through complex sed scripts.</p>
<p dir="auto">Some of the notable features include:</p>
<ul dir="auto">
<li>Preview variable values, both of them!</li>
<li>See how will a substitute command affect pattern space before it runs</li>
<li>Step through sed script - both forward and backwards!</li>
<li>Place breakpoints and examine program state</li>
<li>Hot reload and see what changes as you edit source code</li>
<li>Its name is a palindrome</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alpine Linux</h3><a id="user-content-alpine-linux" aria-label="Permalink: Alpine Linux" href="#alpine-linux"></a></p>
<p dir="auto"><code>aports/testing/desed</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arch Linux</h3><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto">Via AUR: <a href="https://aur.archlinux.org/packages/desed-git/" rel="nofollow">desed-git</a> or <a href="https://aur.archlinux.org/packages/desed/" rel="nofollow">desed</a> as stable version.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DragonFly BSD</h3><a id="user-content-dragonfly-bsd" aria-label="Permalink: DragonFly BSD" href="#dragonfly-bsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Fedora</h3><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">FreeBSD</h3><a id="user-content-freebsd" aria-label="Permalink: FreeBSD" href="#freebsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Void Linux</h3><a id="user-content-void-linux" aria-label="Permalink: Void Linux" href="#void-linux"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Source</h3><a id="user-content-source" aria-label="Permalink: Source" href="#source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/soptikha2/desed
cd desed
cargo install --path .
cp &quot;desed.1&quot; &quot;$(manpath | cut -d':' -f1)/man1&quot;"><pre>git clone https://github.com/soptikha2/desed
<span>cd</span> desed
cargo install --path <span>.</span>
cp <span><span>"</span>desed.1<span>"</span></span> <span><span>"</span><span><span>$(</span>manpath <span>|</span> cut -d<span><span>'</span>:<span>'</span></span> -f1<span>)</span></span>/man1<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Precompiled binaries</h3><a id="user-content-precompiled-binaries" aria-label="Permalink: Precompiled binaries" href="#precompiled-binaries"></a></p>
<p dir="auto">See <a href="https://github.com/SoptikHa2/desed/releases">releases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies:</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies:" href="#dependencies"></a></p>
<p dir="auto">Development: <code>rust</code>, <code>cargo</code> (&gt;= 1.38.0)</p>
<p dir="auto">Runtime: <code>sed</code> (GNU version, &gt;= 4.6) (desed works on BSD if you installed <code>gsed</code>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Controls</h2><a id="user-content-controls" aria-label="Permalink: Controls" href="#controls"></a></p>
<ul dir="auto">
<li>Mouse scroll to scroll through source code, click on line to toggle breakpoint</li>
<li><code>j</code>, <code>k</code>, <code>g</code>, <code>G</code>, just as in Vim. Prefixing with numbers works too.</li>
<li><code>b</code> to toggle breakpoint (prefix with number to toggle breakpoint on target line)</li>
<li><code>s</code> to step forward, <code>a</code> to step backwards</li>
<li><code>r</code> to run to next breakpoint or end of script, <code>R</code> to do the same but backwards</li>
<li><code>l</code> to instantly reload code and continue debugging in the exactly same place as before</li>
<li><code>q</code> to <a href="https://github.com/hakluke/how-to-exit-vim">quit</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">GNU sed actually provides pretty useful debugging interface, try it yourself with <code>--debug</code> flag. However the interface is not interactive and I wanted something closer to traditional debugger. <a href="https://soptik.tech/articles/building-desed-the-sed-debugger.html" rel="nofollow">I've written something here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it really work?</h2><a id="user-content-does-it-really-work" aria-label="Permalink: Does it really work?" href="#does-it-really-work"></a></p>
<p dir="auto">Depends. Sed actually doesn't tell me which line number is it currently executing, so I have to emulate parts of sed to guess that. Which might not be bulletproof. But it certainly worked good enough to debug tetris without issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why sed??</h2><a id="user-content-why-sed" aria-label="Permalink: Why sed??" href="#why-sed"></a></p>
<p dir="auto">Sed is the perfect programming language, <a href="https://tildes.net/~comp/b2k/programming_challenge_find_path_from_city_a_to_city_b_with_least_traffic_controls_inbetween#comment-2run" rel="nofollow">especially for graph problems</a>. It's plain and simple and doesn't clutter your screen with useless identifiers like <code>if</code>, <code>for</code>, <code>while</code>, or <code>int</code>. Furthermore since it doesn't have things like numbers, it's very simple to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">But why?</h2><a id="user-content-but-why" aria-label="Permalink: But why?" href="#but-why"></a></p>
<p dir="auto">I wanted to program in sed but it lacked good tooling up to this point, so I had to do something about it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why?</h2><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">Because it's the standard stream editor for filtering and transforming text. And someone wrote <a href="https://github.com/uuner/sedtris">tetris</a> in it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is the roadmap for future updates?</h2><a id="user-content-what-is-the-roadmap-for-future-updates" aria-label="Permalink: What is the roadmap for future updates?" href="#what-is-the-roadmap-for-future-updates"></a></p>
<p dir="auto">I would like to introduce syntax highlighting and add this tool to standard repositories of all major distributions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is this a joke?</h2><a id="user-content-is-this-a-joke" aria-label="Permalink: Is this a joke?" href="#is-this-a-joke"></a></p>
<p dir="auto">I thought it was. But apparently it's actually useful for some people.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other projects</h2><a id="user-content-other-projects" aria-label="Permalink: Other projects" href="#other-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/soptikha2/video-summarizer">video summarizer</a>, a tool and browser extensions that determines if people in video are currently talking or not, and speeds up the video accordingly. Great for long lecture videos for skipping time spent writing on a whiteboard.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kids who use ChatGPT as a study assistant do worse on tests (163 pts)]]></title>
            <link>https://hechingerreport.org/kids-chatgpt-worse-on-tests/</link>
            <guid>41453300</guid>
            <pubDate>Thu, 05 Sep 2024 03:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">https://hechingerreport.org/kids-chatgpt-worse-on-tests/</a>, See on <a href="https://news.ycombinator.com/item?id=41453300">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

								

				
				<div>

					<section id="block-2"><p><em>The Hechinger Report is a national nonprofit newsroom that reports on one topic: education. Sign up for our&nbsp;<a href="https://hechingerreport.org/newsletters" target="_blank" rel="noreferrer noopener">weekly newsletters</a>&nbsp;to get stories like this delivered directly to your inbox.&nbsp;Consider supporting our stories and becoming&nbsp;<a href="https://hechingerreport.fundjournalism.org/?campaign=701VK000003ezHZYAY" target="_blank" rel="noreferrer noopener">a member</a>&nbsp;today.</em></p></section>

<article id="post-103317">
	<div>

		
		
					<p>Does AI actually help students learn? A recent experiment in a high school provides a cautionary tale.&nbsp;</p><p>Researchers at the University of Pennsylvania found that Turkish high school students who had access to ChatGPT while doing practice math problems did worse on a math test compared with students who didn’t have access to ChatGPT. Those with ChatGPT solved 48 percent more of the practice problems correctly, but they ultimately scored 17 percent worse on a test of the topic that the students were learning.&nbsp;</p><p>A third group of students had access to a revised version of ChatGPT that functioned more like a tutor. This chatbot was programmed to provide hints without directly divulging the answer. The students who used it did spectacularly better on the practice problems, solving 127 percent more of them correctly compared with students who did their practice work without any high-tech aids. But on a test afterwards, these AI-tutored students did no better. Students who just did their practice problems the old fashioned way — on their own — matched their test scores.</p><p>The researchers titled their paper, “Generative AI Can Harm Learning,” to make clear to parents and educators that the current crop of freely available AI chatbots can “substantially inhibit learning.” Even a fine-tuned version of ChatGPT designed to mimic a tutor doesn’t necessarily help.</p><p>The researchers believe the problem is that students are using the chatbot as a “crutch.” When they analyzed the questions that students typed into ChatGPT, students often simply asked for the answer. Students were not building the skills that come from solving the problems themselves.&nbsp;</p><p>ChatGPT’s errors also may have been a contributing factor. The chatbot only answered the math problems correctly half of the time. Its arithmetic computations were wrong 8 percent of the time, but the bigger problem was that its step-by-step approach for how to solve a problem was wrong 42 percent of the time. The tutoring version of ChatGPT was directly fed the correct solutions and these errors were minimized.</p><p>A <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4895486">draft paper about the experiment</a> was posted on the website of SSRN, formerly known as the Social Science Research Network, in July 2024. The paper has not yet been published in a peer-reviewed journal and could still be revised.&nbsp;</p><p>This is just one experiment in another country, and more studies will be needed to confirm its findings. But this experiment was a large one, involving nearly a thousand students in grades nine through 11 during the fall of 2023. Teachers first reviewed a previously taught lesson with the whole classroom, and then their classrooms were randomly assigned to practice the math in one of three ways: with access to ChatGPT, with access to an AI tutor powered by ChatGPT or with no high-tech aids at all. Students in each grade were assigned the same practice problems with or without AI. Afterwards, they took a test to see how well they learned the concept. Researchers conducted four cycles of this, giving students four 90-minute sessions of practice time in four different math topics to understand whether AI tends to help, harm or do nothing.</p><p>ChatGPT also seems to produce overconfidence. In surveys that accompanied the experiment, students said they did not think that ChatGPT caused them to learn less even though they had. Students with the AI tutor thought they had done significantly better on the test even though they did not. (It’s also another good reminder to all of us that our <a href="https://hechingerreport.org/proof-points-college-students-often-dont-know-when-theyre-learning/">perceptions of how much we’ve learned are often wrong</a>.)</p><p>The authors likened the problem of learning with ChatGPT to autopilot. They recounted how an overreliance on autopilot led the Federal Aviation Administration to recommend that pilots minimize their use of this technology. Regulators wanted to make sure that pilots still know how to fly when autopilot fails to function correctly.&nbsp;</p><p>ChatGPT is not the first technology to present a tradeoff in education. Typewriters and computers reduce the need for handwriting. Calculators reduce the need for arithmetic. When students have access to ChatGPT, they might answer more problems correctly, but learn less. Getting the right result to one problem won’t help them with the next one.</p><p><em>This story about&nbsp;using <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">ChatGPT to practice math</a>&nbsp;was written by Jill Barshay and produced by&nbsp;<a href="https://hechingerreport.org/special-reports/higher-education/" target="_blank" rel="noreferrer noopener">The Hechinger Report</a>, a nonprofit, independent news organization focused on inequality and innovation in education. Sign up for&nbsp;<a href="https://hechingerreport.org/proofpoints/" target="_blank" rel="noreferrer noopener"><em>Proof Points</em></a>&nbsp;and other&nbsp;<a href="https://hechingerreport.org/newsletters/" target="_blank" rel="noreferrer noopener"><em>Hechinger newsletters</em></a>.</em></p>
<div id="custom_html-3">
	
<p>The Hechinger Report provides in-depth, fact-based, unbiased reporting on education that is free to all readers. But that doesn't mean it's free to produce. Our work keeps educators and the public informed about pressing issues at schools and on campuses throughout the country. We tell the whole story, even when the details are inconvenient. Help us keep doing that.</p>

<p><a href="https://checkout.fundjournalism.org/memberform?amount=15&amp;installmentPeriod=monthly&amp;org_id=hechingerreport&amp;campaign=701f4000000dsvy">Join us today.</a></p>
</div>	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://hechingerreport.org/author/jill-barshay/" rel="author">
											<img alt="Avatar photo" src="https://hechingerreport.org/wp-content/uploads/2015/01/Barshay-80x80.jpg" srcset="https://i0.wp.com/hechingerreport.org/wp-content/uploads/2015/01/Barshay.jpg?fit=154%2C150&amp;ssl=1 2x" height="80" width="80">											</a></p><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->

<!-- #comments -->
				</div><!-- .main-content -->

			
		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yi-Coder: A Small but Mighty LLM for Code (242 pts)]]></title>
            <link>https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</link>
            <guid>41453237</guid>
            <pubDate>Thu, 05 Sep 2024 03:38:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md">https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</a>, See on <a href="https://news.ycombinator.com/item?id=41453237">Hacker News</a></p>
Couldn't get https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Accelerando (2005) (170 pts)]]></title>
            <link>https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</link>
            <guid>41452962</guid>
            <pubDate>Thu, 05 Sep 2024 02:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html">https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html</a>, See on <a href="https://news.ycombinator.com/item?id=41452962">Hacker News</a></p>
Couldn't get https://www.antipope.org/charlie/blog-static/fiction/accelerando/accelerando.html: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian mega landlord using AI 'pricing scheme' as it hikes rents (133 pts)]]></title>
            <link>https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</link>
            <guid>41452781</guid>
            <pubDate>Thu, 05 Sep 2024 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/">https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</a>, See on <a href="https://news.ycombinator.com/item?id=41452781">Hacker News</a></p>
Couldn't get https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tinystatus: A tiny status page generated by a Python script (181 pts)]]></title>
            <link>https://github.com/harsxv/tinystatus</link>
            <guid>41452339</guid>
            <pubDate>Thu, 05 Sep 2024 00:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/harsxv/tinystatus">https://github.com/harsxv/tinystatus</a>, See on <a href="https://news.ycombinator.com/item?id=41452339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TinyStatus</h2><a id="user-content-tinystatus" aria-label="Permalink: TinyStatus" href="#tinystatus"></a></p>
<p dir="auto">TinyStatus is a simple, customizable status page generator that allows you to monitor the status of various services and display them on a clean, responsive web page. <a href="https://status.harry.id/" rel="nofollow">Check out an online demo.</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4"><img src="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Monitor HTTP endpoints, ping hosts, and check open ports</li>
<li>Responsive design for both status page and history page</li>
<li>Customizable service checks via YAML configuration</li>
<li>Incident history tracking</li>
<li>Automatic status updates at configurable intervals</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Python 3.7 or higher</li>
<li>pip (Python package manager)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repository or download the source code:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/tinystatus.git
cd tinystatus"><pre><code>git clone https://github.com/yourusername/tinystatus.git
cd tinystatus
</code></pre></div>
</li>
<li>
<p dir="auto">Install the required dependencies:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Create a <code>.env</code> file in the project root and customize the variables:</p>
<div data-snippet-clipboard-copy-content="CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json"><pre><code>CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json
</code></pre></div>
</li>
<li>
<p dir="auto">Edit the <code>checks.yaml</code> file to add or modify the services you want to monitor. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: GitHub Home
  type: http
  host: https://github.com
  expected_code: 200

- name: Google DNS
  type: ping
  host: 8.8.8.8

- name: Database
  type: port
  host: db.example.com
  port: 5432"><pre>- <span>name</span>: <span>GitHub Home</span>
  <span>type</span>: <span>http</span>
  <span>host</span>: <span>https://github.com</span>
  <span>expected_code</span>: <span>200</span>

- <span>name</span>: <span>Google DNS</span>
  <span>type</span>: <span>ping</span>
  <span>host</span>: <span>8.8.8.8</span>

- <span>name</span>: <span>Database</span>
  <span>type</span>: <span>port</span>
  <span>host</span>: <span>db.example.com</span>
  <span>port</span>: <span>5432</span></pre></div>
</li>
<li>
<p dir="auto">(Optional) Customize the <code>incidents.md</code> file to add any known incidents or maintenance schedules.</p>
</li>
<li>
<p dir="auto">(Optional) Modify the <code>index.html.theme</code> and <code>history.html.theme</code> files to customize the look and feel of your status pages.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Run the TinyStatus script:</p>

</li>
<li>
<p dir="auto">The script will generate two HTML files:</p>
<ul dir="auto">
<li><code>index.html</code>: The main status page</li>
<li><code>history.html</code>: The status history page</li>
</ul>
</li>
<li>
<p dir="auto">To keep the status page continuously updated, you can run the script in the background:</p>
<ul dir="auto">
<li>On Unix-like systems (Linux, macOS):
<div data-snippet-clipboard-copy-content="nohup python tinystatus.py &amp;"><pre><code>nohup python tinystatus.py &amp;
</code></pre></div>
</li>
<li>On Windows, you can use the Task Scheduler to run the script at startup.</li>
</ul>
</li>
<li>
<p dir="auto">Serve the generated HTML files using your preferred web server (e.g., Apache, Nginx, or a simple Python HTTP server for testing).</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customization</h2><a id="user-content-customization" aria-label="Permalink: Customization" href="#customization"></a></p>
<ul dir="auto">
<li>Adjust the configuration variables in the <code>.env</code> file to customize the behavior of TinyStatus.</li>
<li>Customize the appearance of the status page by editing the CSS in <code>index.html.theme</code> and <code>history.html.theme</code>.</li>
<li>Add or remove services by modifying the <code>checks.yaml</code> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is open source and available under the <a href="https://github.com/harsxv/tinystatus/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>