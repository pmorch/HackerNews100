<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 20 May 2025 22:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Litestream: Revamped (147 pts)]]></title>
            <link>https://fly.io/blog/litestream-revamped/</link>
            <guid>44045292</guid>
            <pubDate>Tue, 20 May 2025 19:58:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/litestream-revamped/">https://fly.io/blog/litestream-revamped/</a>, See on <a href="https://news.ycombinator.com/item?id=44045292">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <figure>
                <img src="https://fly.io/blog/litestream-revamped/assets/litestream-revamped.png" alt="">
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p><a href="https://litestream.io/" title="">Litestream</a> is an open-source tool that makes it possible to run many kinds of full-stack applications on top of SQLite by making them reliably recoverable from object storage. This is a post about the biggest change we’ve made to it since I launched it.</p>
<p>Nearly a decade ago, I got a bug up my ass. I wanted to build full-stack applications quickly. But the conventional n-tier database design required me to do sysadmin work for each app I shipped. Even the simplest applications depended on heavy-weight database servers like Postgres or MySQL.</p>

<p>I wanted to launch apps on SQLite, because SQLite is easy. But SQLite is embedded, not a server, which at the time implied that the data for my application lived (and died) with just one server.</p>

<p>So in 2020, I wrote <a href="https://litestream.io/" title="">Litestream</a> to fix that.</p>

<p>Litestream is a tool that runs alongside a SQLite application. Without changing that running application, it takes over the WAL checkpointing process to continuously stream database updates to an S3-compatible object store. If something happens to the server the app is running on, the whole database can efficiently be restored to a different server. You might lose servers, but you won’t lose your data.</p>

<p>Litestream worked well. So we got ambitious. A few years later, we built <a href="https://github.com/superfly/litefs" title="">LiteFS</a>. LiteFS takes the ideas in Litestream and refines them, so that we can do read replicas and primary failovers with SQLite. LiteFS gives SQLite the modern deployment story of an n-tier database like Postgres, while keeping the database embedded.</p>

<p>We like both LiteFS and Litestream. But Litestream is the more popular project. It’s easier to deploy and easier to reason about.</p>

<p>There are some good ideas in LiteFS. We’d like Litestream users to benefit from them. So we’ve taken our LiteFS learnings and applied them to some new features in Litestream.</p>
<h2 id="point-in-time-restores-but-fast"><a href="#point-in-time-restores-but-fast" aria-label="Anchor"></a><span>Point-in-time restores, but fast</span></h2>
<p><a href="https://fly.io/blog/all-in-on-sqlite-litestream/" title="">Here’s how Litestream was originally designed</a>: you run <code>litestream</code> against a SQLite database, and it opens up a long-lived read transaction. This transaction arrests SQLite WAL checkpointing, the process by which SQLite consolidates the WAL back into the main database file. Litestream builds a “shadow WAL” that records WAL pages, and copies them to S3.</p>

<p>This is simple, which is good. But it can also be slow. When you want to restore a database, you have have to pull down and replay every change since the last snapshot. If you changed a single database page a thousand times, you replay a thousand changes. For databases with frequent writes, this isn’t a good approach.</p>

<p>In LiteFS, we took a different approach. LiteFS is transaction-aware. It doesn’t simply record raw WAL pages, but rather ordered ranges of pages associated with transactions, using a file format we call <a href="https://github.com/superfly/ltx" title="">LTX</a>. Each LTX file represents a sorted changeset of pages for a given period of time.</p>

<p><img alt="a simple linear LTX file with 8 pages between 1 and 21" src="https://fly.io/blog/litestream-revamped/assets/linear-ltx.png"></p>

<p>Because they are sorted, we can easily merge multiple LTX files together and create a new LTX file with only the latest version of each page.</p>

<p><img alt="merging three LTX files into one" src="https://fly.io/blog/litestream-revamped/assets/merged-ltx.png"></p>
<p>This is similar to how an <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" title="">LSM tree</a> works.</p>
<p>This process of combining smaller time ranges into larger ones is called <em>compaction</em>. With it, we can replay a SQLite database to a specific point in time, with a minimal  duplicate pages.</p>
<h2 id="casaas-compare-and-swap-as-a-service"><a href="#casaas-compare-and-swap-as-a-service" aria-label="Anchor"></a><span>CASAAS: Compare-and-Swap as a Service</span></h2>
<p>One challenge Litestream has to deal with is desynchronization. Part of the point of Litestream is that SQLite applications don’t have to be aware of it. But <code>litestream</code> is just a process, running alongside the application, and it can die independently. If <code>litestream</code> is down while database changes occur, it will miss changes. The same kind of problem occurs if you start replication from a new server.</p>

<p>Litestream needs a way to reset the replication stream from a new snapshot. How it does that is with “generations”. <a href="https://litestream.io/how-it-works/#snapshots--generations" title="">A generation</a> represents a snapshot and a stream of WAL updates, uniquely identified. Litestream notices any break in its WAL sequence and starts a new generation, which is how it recovers from desynchronization.</p>

<p>Unfortunately, storing and managing multiple generations makes it difficult to implement features like failover and read-replicas.</p>

<p>The most straightforward way around this problem is to make sure only one instance of Litestream can replication to a given destination. If you can do that, you can store just a single, latest generation. That in turn makes it easy to know how to resync a read replica; there’s only one generation to choose from.</p>

<p>In LiteFS, we solved this problem by using Consul, which guaranteed a single leader. That requires users to know about Consul. Things like “requiring Consul” are probably part of the reason Litestream is so much more popular than LiteFS.</p>

<p>In Litestream, we’re solving the problem a different way. Modern object stores like S3 and Tigris solve this problem for us: they now offer <a href="https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-functionality-conditional-writes/" title="">conditional write support</a>. With conditional writes, we can implement a time-based lease. We get essentially the same constraint Consul gave us, but without having to think about it or set up a dependency.</p>

<p>In the immediacy, this will mean you can run Litestream with ephemeral nodes, with overlapping run times, and even if they’re storing to the same destination, they won’t confuse each other.</p>
<h2 id="lightweight-read-replicas"><a href="#lightweight-read-replicas" aria-label="Anchor"></a><span>Lightweight read replicas</span></h2>
<p>The original design constraint of both Litestream and LiteFS was to extend SQLite, to modern deployment scenarios, without disturbing people’s built code. Both tools are meant to function even if applications are oblivious to them.</p>

<p>LiteFS is more ambitious than Litestream, and requires transaction-awareness. To get that without disturbing built code, we use a cute trick (a.k.a. a gross hack): LiteFS provides a FUSE filesystem, which lets it act as a proxy between the application and the backing store. From that vantage point, we can easily discern transactions.</p>

<p>The FUSE approach gave us a lot of control, enough that users could use SQLite replicas just like any other database. But installing and running a whole filesystem (even a fake one) is a lot to ask of users. To work around that problem, we relaxed a constraint: LiteFS can function without the FUSE filesystem if you load an extension into your application code, <a href="https://github.com/superfly/litevfs" title="">LiteVFS</a>.  LiteVFS is a <a href="https://www.sqlite.org/vfs.html" title="">SQLite Virtual Filesystem</a> (VFS). It works in a variety of environments, including some where FUSE can’t, like in-browser WASM builds.</p>

<p>What we’re doing next is taking the same trick and using it on Litestream. We’re building a VFS-based read-replica layer. It will be able to fetch and cache pages directly from S3-compatible object storage.</p>

<p>Of course, there’s a catch: this approach isn’t as efficient as a local SQLite database. That kind of efficiency, where you don’t even need to think about N+1 queries because there’s no network round-trip to make the duplicative queries pile up costs, is part of the point of using SQLite.</p>

<p>But we’re optimistic that with cacheing and prefetching, the approach we’re using will yield, for the right use cases, strong performance — all while serving SQLite reads hot off of Tigris or S3.</p>
<figure>
  <figcaption>
    <h2>Litestream is fully open source</h2>
    <p>It’s not coupled with Fly.io at all; you can use it anywhere.</p>
      <a href="https://http//litestream.io/">
        Check it out <span>→</span>
      </a>
  </figcaption>
  <p><img src="https://fly.io/static/images/cta-cat.webp" srcset="https://fly.io/static/images/cta-cat@2x.webp 2x" alt="">
  </p>
</figure>

<h2 id="synchronize-lots-of-databases"><a href="#synchronize-lots-of-databases" aria-label="Anchor"></a><span>Synchronize Lots Of Databases</span></h2>
<p>While we’ve got you here: we’re knocking out one of our most requested features.</p>

<p>In the old Litestream design, WAL-change polling and slow restores made it infeasible to replicate large numbers of databases from a single process. That has been our answer when users ask us for a “wildcard” or “directory” replication argument for the tool.</p>

<p>Now that we’ve switched to LTX, this isn’t a problem any more. It should thus be possible to replicate <code>/data/*.db</code>, even if there’s hundreds or thousands of databases in that directory.</p>
<h2 id="we-still-sqlite"><a href="#we-still-sqlite" aria-label="Anchor"></a><span>We Still ❤️ SQLite</span></h2>
<p>SQLite has always been a solid database to build on and it’s continued to find new use cases as the industry evolves. We’re super excited to continue to build Litestream alongside it.</p>

<p>We have a sneaking suspicion that the robots that write LLM code are going to like SQLite too. We think what <a href="https://phoenix.new/" title="">coding agents like Phoenix.new</a> want is a way to try out code on live data, screw it up, and then rollback <em>both the code and the state.</em> These Litestream updates put us in a position to give agents PITR as a primitive. On top of that, you can build both rollbacks and forks.</p>

<p>Whether or not you’re drinking the AI kool-aid, we think this new design for Litestream is just better. We’re psyched to be rolling it out, and for the features it’s going to enable.</p>

          
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The NSA Selector (127 pts)]]></title>
            <link>https://github.com/wenzellabs/the_NSA_selector</link>
            <guid>44044459</guid>
            <pubDate>Tue, 20 May 2025 18:30:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wenzellabs/the_NSA_selector">https://github.com/wenzellabs/the_NSA_selector</a>, See on <a href="https://news.ycombinator.com/item?id=44044459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">the NSA selector</h2><a id="user-content-the-nsa-selector" aria-label="Permalink: the NSA selector" href="#the-nsa-selector"></a></p>
<p dir="auto">you can purchase the NSA selector at <a href="https://lectronz.com/products/thensaselector" rel="nofollow">my little shop over at lectronz</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_top.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_top.jpg" alt="the NSA selector top"></a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_front.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_front.jpg" alt="the NSA selector front"></a>   <a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_back.jpg"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_back.jpg" alt="the NSA selector back"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">what is it</h2><a id="user-content-what-is-it" aria-label="Permalink: what is it" href="#what-is-it"></a></p>
<p dir="auto">the NSA selector is a eurorack module with two ethernet jacks and one audio output.
any bit on the network will be sent to the audio output.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_switch_schematic.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_switch_schematic.png" alt="the NSA selector schematic"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">what it's not</h2><a id="user-content-what-its-not" aria-label="Permalink: what it's not" href="#what-its-not"></a></p>
<p dir="auto">this is not an "audio interface". we do not play back any "format" such as RTP
or MP3 or WAV ot the like. the eurorack module does not "speak" any protocol.
all traffic is forwarded from one network jack to the other unmodified.
it's just tapped, intercepted to convert it to audio.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">examples</h2><a id="user-content-examples" aria-label="Permalink: examples" href="#examples"></a></p>
<p dir="auto">watch <a href="https://www.youtube.com/watch?v=vfgySTaM1TI" rel="nofollow">the NSA selector video</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_LAN_flow.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_LAN_flow.png" alt="the NSA selector LAN flow"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">sequencer script</h3><a id="user-content-sequencer-script" aria-label="Permalink: sequencer script" href="#sequencer-script"></a></p>
<p dir="auto">in the folder <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/sequencer"><code>sequencer/</code></a> you find a very simple shell script, that mimics a
sequencer by network pings of different size.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">plain image transfer</h3><a id="user-content-plain-image-transfer" aria-label="Permalink: plain image transfer" href="#plain-image-transfer"></a></p>
<p dir="auto">if we transfer uncompressed, unencrypted images e.g. in the .bmp format,
we can hear the pixels. together with a small http server which is available
in the <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/fileserver"><code>fileserver/</code></a> folder. you can listen to your photos or drawing from
gimp (or photoshop in case you're a rich musician).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">encode audio to NSA's native format</h3><a id="user-content-encode-audio-to-nsas-native-format" aria-label="Permalink: encode audio to NSA's native format" href="#encode-audio-to-nsas-native-format"></a></p>
<p dir="auto">the NSA selector's native format is 4 bits and 25MS/s which originates from the
typical PHY MAC interface called <a href="https://en.wikipedia.org/wiki/Media-independent_interface" rel="nofollow">MII</a>.</p>
<p dir="auto">at first glance 4 bit audio appears to be really crappy, but we can use the
ridiculously high sample rate. what we need is called a delta-sigma modulator.
this lets us convert a simple mono 16 bit 48kHz .wav file to a 4 bit 25MHz
.nsa file.</p>
<p dir="auto">note that this saturates the link and through the added headers from ethernet,
IP, UDP or TCP and HTTP you'll get artifacts. and happy litte retransmissions.</p>
<p dir="auto">far from HiFi quality, but the method adds a lot of spice and excitement.</p>
<p dir="auto">there's a converter in the <a href="https://github.com/wenzellabs/the_NSA_selector/blob/main/upconverter"><code>upconverter/</code></a> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">network overhead</h3><a id="user-content-network-overhead" aria-label="Permalink: network overhead" href="#network-overhead"></a></p>
<p dir="auto">here's what a network packet can look like on the wire:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/wenzellabs/the_NSA_selector/blob/main/pics/NSA_selector_network_packet.png"><img src="https://github.com/wenzellabs/the_NSA_selector/raw/main/pics/NSA_selector_network_packet.png" alt="the NSA selector network packet"></a></p>
<p dir="auto">and we're listening in on the "4B" side of the "4B5B encoding" layer.
so the first bits we hear are <a href="https://en.wikipedia.org/wiki/Ethernet_frame#Preamble_and_start_frame_delimiter" rel="nofollow">the preamble of the ethernet frame</a> and we follow up the stack. e.g. ethernet, IP, TCP, HTTP, BMP.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">delay, oversaturation</h3><a id="user-content-delay-oversaturation" aria-label="Permalink: delay, oversaturation" href="#delay-oversaturation"></a></p>
<p dir="auto">there's a neat little idea I had during development to add a delay to say a
sequencer pattern. actually it's more like an echo than a delay since it lacks
precise timing control.
ssh log into the remote machine that creates the e.g. ping traffic. then dump
the network traffic to the text console to double it. you can increase verbosity
levels of the dump to crank up the echo until total saturation of the link
and lost captured packets. see the video if this explanation is not clear to you.</p>
<p dir="auto">commands I usually use:</p>
<p dir="auto"><code>tcpdump -ni eth0</code></p>
<p dir="auto"><code>tcpdump -nvi eth0</code></p>
<p dir="auto"><code>tcpdump -nvi eth0 icmp</code></p>
<p dir="auto"><code>tcpdump -nvxi eth0</code></p>
<p dir="auto"><code>tcpdump -nxi eth0</code></p>
<p dir="auto"><code>tcpdump -nxi eth0 not port ssh</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">other network traffic</h3><a id="user-content-other-network-traffic" aria-label="Permalink: other network traffic" href="#other-network-traffic"></a></p>
<p dir="auto">be creative!</p>
<p dir="auto">there's so much I hadn't been listening into, like</p>
<ul dir="auto">
<li>online games - I guess a wide variety is waiting here, and some will be very distinctive</li>
<li>doomscrolling on the various platforms</li>
<li>network backup</li>
<li>IoT stuff</li>
<li>remote desktop protocols</li>
<li>write your own code</li>
<li>bundle together ping, netcat, socat, nmap and whatnot and make them MIDI controllable through a software registered MIDI client</li>
</ul>
<p dir="auto">if possible disable encryption, then you can profit from not only timing
pattern (of white noise), but also listen in on the plaintext payload.
the NSA loves plaintext.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">the tech bits</h2><a id="user-content-the-tech-bits" aria-label="Permalink: the tech bits" href="#the-tech-bits"></a></p>
<p dir="auto">the NSA selector is a fast ethernet (FE=100Mbps) network switch with three
ports. the two front ports are switched, and the third port is only available
internally as 4 bit MII bus. it is configured as mirror port of the two front
ports and wired to a 4 bit DAC and a low-pass-filter.</p>
<ul dir="auto">
<li>4 HP wide</li>
<li>current consumption:
<ul dir="auto">
<li>12V : 100mA</li>
<li>5V : nothing</li>
<li>-12V :   2mA</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">assembling the kit</h2><a id="user-content-assembling-the-kit" aria-label="Permalink: assembling the kit" href="#assembling-the-kit"></a></p>
<p dir="auto">both versions are available in my store, a fully assembled eurorack module
and a kit version where you have to solder on the front plate.</p>
<p dir="auto">watch my <a href="https://www.youtube.com/watch?v=SXlfyeYuZuQ" rel="nofollow">NSA selector kit assembly</a> video on youtube.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">store</h2><a id="user-content-store" aria-label="Permalink: store" href="#store"></a></p>
<p dir="auto">you can purchase the NSA selector at <a href="https://lectronz.com/products/thensaselector" rel="nofollow">my little shop over at lectronz</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">SEO</h5><a id="user-content-seo" aria-label="Permalink: SEO" href="#seo"></a></p>
<p dir="auto">#eurorack #NSA #theNSAselector #wenzellabs #the_NSA_selector</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3n preview: Mobile-first AI (127 pts)]]></title>
            <link>https://developers.googleblog.com/en/introducing-gemma-3n/</link>
            <guid>44044451</guid>
            <pubDate>Tue, 20 May 2025 18:29:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">https://developers.googleblog.com/en/introducing-gemma-3n/</a>, See on <a href="https://news.ycombinator.com/item?id=44044451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="ghtsi">Following the exciting launches of <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.</p><p data-block-key="bietk">To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.</p><p data-block-key="74nf9"><a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a> is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of <a href="https://deepmind.google/technologies/gemini/nano/">Gemini Nano</a>, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.</p>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" alt="Chatbot Arena Elo scores"></p><p>
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    </p>
                
            
        </div>
  <div>
    <p data-block-key="ghtsi">Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our <a href="https://ai.google.dev/gemma/docs/gemma-3n#parameters">documentation</a>.</p><p data-block-key="q3ib">By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.</p><p data-block-key="64a2c">In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.</p><h3 data-block-key="mcrw4" id="key-capabilities-of-gemma-3n"><b><br>Key Capabilities of Gemma 3n</b></h3><p data-block-key="54i3c">Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:</p><ul><li data-block-key="binsd"><b>Optimized On-Device Performance &amp; Efficiency:</b> Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.</li></ul><ul><li data-block-key="cgst0"><b>Many-in-1 Flexibility:</b> A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to <a href="https://arxiv.org/abs/2310.07707">MatFormer</a> training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.</li></ul><ul><li data-block-key="1cvli"><b>Privacy-First &amp; Offline Ready:</b> Local execution enables features that respect user privacy and function reliably, even without an internet connection.</li></ul><ul><li data-block-key="dofaq"><b>Expanded Multimodal Understanding with Audio:</b> Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)</li></ul><ul><li data-block-key="1mkna"><b>Improved Multilingual Capabilities:</b> Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).</li></ul>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" alt="MMLU performance"></p><p>
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    </p>
                
            
        </div>
  <div>
    <h3 data-block-key="k4nng" id="unlocking-new-on-the-go-experiences"><b>Unlocking New On-the-go Experiences</b></h3><p data-block-key="1hssr">Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:</p><ol><li data-block-key="9jafp"><b>Build live, interactive experiences</b> that understand and respond to real-time visual and auditory cues from the user's environment.</li></ol><p data-block-key="c7kgq"><b><br></b>2. <b>Power deeper understanding</b> and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.</p><p data-block-key="t3pn"><b><br></b>3. <b>Develop advanced audio-centric applications</b>, including real-time speech transcription, translation, and rich voice-driven interactions.</p><p data-block-key="7mf82"><br>Here’s an overview and the types of experiences you can build:</p>
</div>    <div>
    <h3 data-block-key="5sfqt" id="building-responsibly-together"><b>Building Responsibly, Together</b></h3><p data-block-key="4evp8">Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.</p><h3 data-block-key="ei4ug" id="get-started:-preview-gemma-3n-today"><b><br>Get Started: Preview Gemma 3n Today</b></h3><p data-block-key="a8cmr">We're excited to get Gemma 3n into your hands through a preview starting today:</p><p data-block-key="a0cp7"><b><br>Initial Access (Available Now):</b></p><ul><li data-block-key="oaqh"><b>Cloud-based Exploration with Google AI Studio:</b> Try Gemma 3n directly in your browser on <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-3n-e4b-it">Google AI Studio</a> – no setup needed. Explore its text input capabilities instantly.</li></ul><ul><li data-block-key="e7c6o"><b>On-Device Development with Google AI Edge:</b> For developers looking to integrate Gemma 3n locally, <a href="https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling">Google AI Edge</a> provides tools and libraries. You can get started with text and image understanding/generation capabilities today.</li></ul><p data-block-key="cp8j"><br>Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.</p><p data-block-key="46ck9">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google AI Ultra (192 pts)]]></title>
            <link>https://blog.google/products/google-one/google-ai-ultra/</link>
            <guid>44044367</guid>
            <pubDate>Tue, 20 May 2025 18:20:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/google-one/google-ai-ultra/">https://blog.google/products/google-one/google-ai-ultra/</a>, See on <a href="https://news.ycombinator.com/item?id=44044367">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
  }">
      <div>
          
            <p>May 20, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Get the highest access to our most capable AI models and premium features with this new plan by Google One.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="A dark background with rainbow ombre at the top, with text saying Google AI Ultra subscription, and tiles with text including Gemini app and Flow. Underneath that, neon jellyfish float over Earth." data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Image.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Image.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Google_AI_Ultra_Blog_Header_Imag.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Introducing Google AI Ultra: The best of Google AI in one subscription" listen-to-article="Listen to article" data-date-modified="2025-05-20T18:14:16.139622+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h">As we introduce new, emerging capabilities and state-of-the-art models, we want to provide more ways for people to get access to the best of what Google AI has to offer to help them be more knowledgeable, creative and productive.</p><p data-block-key="8pm6">Today, we’re introducing <a href="http://one.google.com/ai?utm_source=g1&amp;utm_medium=web&amp;utm_campaign=google_ai_plan_blog">Google AI Ultra</a>, a new AI subscription plan with the highest usage limits and access to our most capable models and premium features. If you're a filmmaker, developer, creative professional or simply demand the absolute best of Google AI with the highest level of access, the Google AI Ultra plan is built for you — think of it as your VIP pass to Google Al.</p><p data-block-key="1e6tn">Google AI Ultra is available today in the U.S. for $249.99/month (with a special offer for first-time users of 50% off for your first three months), and coming soon to more countries. Here’s what you get with Google AI Ultra:</p><ul><li data-block-key="aj66v"><b>Gemini:</b> Experience the absolute best version of our <a href="https://blog.google/products/gemini/gemini-app-updates-io-2025">Gemini app</a>. This plan offers the highest usage limits across Deep Research, cutting-edge video generation with Veo 2 and early access to our groundbreaking Veo 3 model. It's designed for coding, academic research and complex creative endeavors. In the coming weeks, Ultra subscribers will also get access to Deep Think in 2.5 Pro, our new enhanced reasoning mode.</li><li data-block-key="f9t9s"><b>Flow:</b> This <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">new AI filmmaking tool</a> is custom-designed for Google DeepMind’s most advanced models (Veo, Imagen and Gemini). It enables the crafting of cinematic clips, scenes and cohesive narratives with intuitive prompting. Google AI Ultra unlocks the highest limits in Flow with 1080p video generation, advanced camera controls and early access to Veo 3.</li><li data-block-key="4h6gs"><b>Whisk:</b> Whisk helps you quickly explore and visualize new ideas using both text and image prompts. With Google AI Ultra, get the highest limits for Whisk Animate, which turns your images into vivid eight-second videos with Veo 2.</li><li data-block-key="53ou1"><b>NotebookLM:</b> Get access to the highest usage limits and enhanced model capabilities later this year, whether you're using NotebookLM for studying, teaching or working on your projects.</li><li data-block-key="a8ooh"><b>Gemini in Gmail, Docs, Vids and more:</b> Make everyday tasks easier with access to Gemini directly in your favorite Google apps like Gmail, Docs, Vids and more.</li><li data-block-key="7nm55"><b>Gemini in Chrome:</b> Starting tomorrow, get early access to Gemini directly within the Chrome browser. This feature allows you to effortlessly understand complex information and complete tasks on the web by using the context of the current page.</li><li data-block-key="58cj9"><b>Project Mariner:</b> This agentic research prototype can assist you in managing up to 10 tasks simultaneously — from research to bookings and purchases — all from a single dashboard.</li><li data-block-key="c05ih"><b>YouTube Premium:</b> An individual YouTube Premium plan lets you watch YouTube and listen to YouTube Music ad-free, offline and in the background.</li><li data-block-key="eaedq"><b>30 TB of storage:</b> Offers massive storage capacity across Google Photos, Drive and Gmail to keep your creations and important files secure.</li></ul></div>
  

  
    
  
    


  <uni-youtube-player-article index="2" thumbnail-alt="A dark background with rainbow ombre at the top, with text saying Google AI Ultra subscription, and tiles with text including Gemini app and Flow. Underneath that, neon jellyfish float over Earth." video-id="2CquRQiDzx8" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h">Our existing AI Premium plan, now called Google AI Pro, is also getting better. For no additional cost, Google AI Pro subscribers will now also get access to AI filmmaking capabilities in Flow, where you can try the tool with our Veo 2 model, as well as early access to Gemini in Chrome. These new benefits are coming to Google AI Pro subscribers in the U.S. first, with availability in more countries to follow.</p><p data-block-key="8rvm3">We're also expanding free access to Google AI Pro for a school year to university students in Japan, Brazil, Indonesia and the United Kingdom, <a href="https://blog.google/products/gemini/google-one-ai-premium-students-free/">in addition to the U.S.</a></p></div>
  

  
    






<uni-image-full-width alignment="large" alt-text="Table comparing Google AI Pro and Google AI Ultra plans." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Introducing Google AI Ultra: The best of Google AI in one subscription" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Table comparing Google AI Pro and Google AI Ultra plans." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AI_plans_v5_-2x.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Google AI Ultra: The best of Google AI in one subscription&quot;
         }"><p data-block-key="4bn2h"><a href="http://one.google.com/ai?utm_source=g1&amp;utm_medium=web&amp;utm_campaign=google_ai_plan_blog">Sign up for Google AI Ultra today</a> to experience the best of Google AI.</p></div>
  

  
    







<uni-related-content-tout title="I/O 2025" cta="See more" summary="Here’s a look at everything we announced at Google I/O 2025." hideimage="False" eyebrow="Collection" image-alt-text="Text saying I/O25 in rainbow block letters against a grid" role="none" fullurl="https://blog.google/technology/developers/google-io-2025-collection/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="512px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp 512w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp" alt="Text saying I/O25 in rainbow block letters against a grid" sizes=" 300px,  512px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_social_share.width-512.format-webp.webp 512w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
    
    
    <div id="newsletter-form--form">
      <form data-method="POST" data-action="https://services.google.com/fb/submissions/thekeywordnewsletterprodv2/">
        <h2 id="subscribe_box_label">
          <span>Get more stories from Google in your inbox.</span>
          
        </h2>
        
      </form>
    </div>
    
    <div>
      <div>
        <p tabindex="-1" role="status" aria-live="off" aria-atomic="false">
          Done. Just one step more.
        </p>
        <p>
          Check your inbox to confirm your subscription.
        </p>
        <p>You are already subscribed to our newsletter.</p>
      </div>
      <p>
        You can also subscribe with a
        
        
      </p>
    </div>
  </div>

  

  


            
            

            
              




            
          </div>
  </article>
  





  

  


<div data-component="uni-related-articles" aria-roledescription="carousel" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Footer Related Stories&quot;,
    &quot;section_header&quot;: &quot;Related stories&quot;
  }">
        <h3>
          <p>
            Related stories
          </p>
        </h3>
      </div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma 3n preview: Mobile-first AI (140 pts)]]></title>
            <link>https://developers.googleblog.com/en/introducing-gemma-3n/</link>
            <guid>44044199</guid>
            <pubDate>Tue, 20 May 2025 18:03:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developers.googleblog.com/en/introducing-gemma-3n/">https://developers.googleblog.com/en/introducing-gemma-3n/</a>, See on <a href="https://news.ycombinator.com/item?id=44044199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    
      
    

    

    

    

    
    <div>
          

<div>
    <p data-block-key="ghtsi">Following the exciting launches of <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a> and <a href="https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/">Gemma 3 QAT</a>, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day – your phones, tablets, and laptops.</p><p data-block-key="bietk">To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung System LSI, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.</p><p data-block-key="74nf9"><a href="https://deepmind.google/models/gemma/gemma-3n/">Gemma 3n</a> is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of <a href="https://deepmind.google/technologies/gemini/nano/">Gemini Nano</a>, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.</p>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/image3_OjwrVp1.original.png" alt="Chatbot Arena Elo scores"></p><p>
                        This chart ranks AI models by Chatbot Arena Elo scores; higher scores (top numbers) indicate greater user preference. Gemma 3n ranks highly amongst both popular proprietary and open models.
                    </p>
                
            
        </div>
  <div>
    <p data-block-key="ghtsi">Gemma 3n leverages a Google DeepMind innovation called Per-Layer Embeddings (PLE) that delivers a significant reduction in RAM usage. While the raw parameter count is 5B and 8B, this innovation allows you to run larger models on mobile devices or live-stream from the cloud, with a memory overhead comparable to a 2B and 4B model, meaning the models can operate with a dynamic memory footprint of just 2GB and 3GB. Learn more in our <a href="https://ai.google.dev/gemma/docs/gemma-3n#parameters">documentation</a>.</p><p data-block-key="q3ib">By exploring Gemma 3n, developers can get an early preview of the open model’s core capabilities and mobile-first architectural innovations that will be available on Android and Chrome with Gemini Nano.</p><p data-block-key="64a2c">In this post, we'll explore Gemma 3n's new capabilities, our approach to responsible development, and how you can access the preview today.</p><h3 data-block-key="chmin" id="key-capabilities-of-gemma-3n"><b><br>Key Capabilities of Gemma 3n</b></h3><p data-block-key="54i3c">Engineered for fast, low-footprint AI experiences running locally, Gemma 3n delivers:</p><ul><li data-block-key="binsd"><b>Optimized On-Device Performance &amp; Efficiency:</b> Gemma 3n starts responding approximately 1.5x faster on mobile with significantly better quality (compared to Gemma 3 4B) and a reduced memory footprint achieved through innovations like Per Layer Embeddings, KVC sharing, and advanced activation quantization.</li></ul><ul><li data-block-key="cgst0"><b>Many-in-1 Flexibility:</b> A model with a 4B active memory footprint that natively includes a nested state-of-the-art 2B active memory footprint submodel (thanks to <a href="https://arxiv.org/abs/2310.07707">MatFormer</a> training). This provides flexibility to dynamically trade off performance and quality on the fly without hosting separate models. We further introduce mix’n’match capability in Gemma 3n to dynamically create submodels from the 4B model that can optimally fit your specific use case -- and associated quality/latency tradeoff. Stay tuned for more on this research in our upcoming technical report.</li></ul><ul><li data-block-key="1cvli"><b>Privacy-First &amp; Offline Ready:</b> Local execution enables features that respect user privacy and function reliably, even without an internet connection.</li></ul><ul><li data-block-key="dofaq"><b>Expanded Multimodal Understanding with Audio:</b> Gemma 3n can understand and process audio, text, and images, and offers significantly enhanced video understanding. Its audio capabilities enable the model to perform high-quality Automatic Speech Recognition (transcription) and Translation (speech to translated text). Additionally, the model accepts interleaved inputs across modalities, enabling understanding of complex multimodal interactions. (Public implementation coming soon)</li></ul><ul><li data-block-key="1mkna"><b>Improved Multilingual Capabilities:</b> Improved multilingual performance, particularly in Japanese, German, Korean, Spanish, and French. Strong performance reflected on multilingual benchmarks such as 50.1% on WMT24++ (ChrF).</li></ul>
</div>   


    
    <div>
            
                <p><img src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Artboard_1.original.png" alt="MMLU performance"></p><p>
                        This chart show’s MMLU performance vs model size of Gemma 3n’s mix-n-match (pretrained) capability.
                    </p>
                
            
        </div>
  <div>
    <h3 data-block-key="2rib8" id="unlocking-new-on-the-go-experiences"><b>Unlocking New On-the-go Experiences</b></h3><p data-block-key="1hssr">Gemma 3n will empower a new wave of intelligent, on-the-go applications by enabling developers to:</p><ol><li data-block-key="9jafp"><b>Build live, interactive experiences</b> that understand and respond to real-time visual and auditory cues from the user's environment.</li></ol><p data-block-key="c7kgq"><b><br></b>2. <b>Power deeper understanding</b> and contextual text generation using combined audio, image, video, and text inputs—all processed privately on-device.</p><p data-block-key="t3pn"><b><br></b>3. <b>Develop advanced audio-centric applications</b>, including real-time speech transcription, translation, and rich voice-driven interactions.</p><p data-block-key="7mf82"><br>Here’s an overview and the types of experiences you can build:</p>
</div>    <div>
    <h3 data-block-key="q2vk1" id="building-responsibly-together"><b>Building Responsibly, Together</b></h3><p data-block-key="4evp8">Our commitment to responsible AI development is paramount. Gemma 3n, like all Gemma models, underwent rigorous safety evaluations, data governance, and fine-tuning alignment with our safety policies. We approach open models with careful risk assessment, continually refining our practices as the AI landscape evolves.</p><h3 data-block-key="eyvht" id="get-started:-preview-gemma-3n-today"><b><br>Get Started: Preview Gemma 3n Today</b></h3><p data-block-key="a8cmr">We're excited to get Gemma 3n into your hands through a preview starting today:</p><p data-block-key="a0cp7"><b><br>Initial Access (Available Now):</b></p><ul><li data-block-key="oaqh"><b>Cloud-based Exploration with Google AI Studio:</b> Try Gemma 3n directly in your browser on <a href="https://aistudio.google.com/app/prompts/new_chat?model=gemma-3n-e4b-it">Google AI Studio</a> – no setup needed. Explore its text input capabilities instantly.</li></ul><ul><li data-block-key="e7c6o"><b>On-Device Development with Google AI Edge:</b> For developers looking to integrate Gemma 3n locally, <a href="https://developers.googleblog.com/en/google-ai-edge-small-language-models-multimodality-rag-function-calling">Google AI Edge</a> provides tools and libraries. You can get started with text and image understanding/generation capabilities today.</li></ul><p data-block-key="cp8j"><br>Gemma 3n marks the next step in democratizing access to cutting-edge, efficient AI. We’re incredibly excited to see what you’ll build as we make this technology progressively available, starting with today's preview.</p><p data-block-key="46ck9">Explore this announcement and all Google I/O 2025 updates on <a href="https://io.google/2025/?utm_source=blogpost&amp;utm_medium=pr&amp;utm_campaign=event&amp;utm_content=">io.google</a> starting May 22.</p>
</div> 
      </div>
    

    

    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow (351 pts)]]></title>
            <link>https://blog.google/technology/ai/generative-media-models-io-2025/</link>
            <guid>44044043</guid>
            <pubDate>Tue, 20 May 2025 17:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/generative-media-models-io-2025/">https://blog.google/technology/ai/generative-media-models-io-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44044043">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
  }">
      <div>
          
            <p>May 20, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Introducing Veo 3 and Imagen 4, and a new tool for filmmaking called Flow.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Collage of various nature images generated by AI" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/5.20v2_Hero_2097x1182.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Veo 3" href="#veo-3" id="veo-3-anchor">Veo 3</a>
        </li>
        
        <li>
          <a aria-label="link to Veo 2 updates" href="#veo-2-updates" id="veo-2-updates-anchor">Veo 2 updates</a>
        </li>
        
        <li>
          <a aria-label="link to Flow" href="#flow" id="flow-anchor">Flow</a>
        </li>
        
        <li>
          <a aria-label="link to Imagen 4" href="#imagen-4" id="imagen-4-anchor">Imagen 4</a>
        </li>
        
        <li>
          <a aria-label="link to Lyria 2" href="#lyria-2" id="lyria-2-anchor">Lyria 2</a>
        </li>
        
        <li>
          <a aria-label="link to Responsible creation" href="#responsible-creation" id="responsible-creation-anchor">Responsible creation</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Fuel your creativity with new generative media  models and tools" listen-to-article="Listen to article" data-date-modified="2025-05-20T18:52:10.019676+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><p data-block-key="zbevq">Today, we’re announcing our newest generative media models, which mark significant breakthroughs. These models create breathtaking images, videos and music, empowering artists to bring their creative vision to life. They also power amazing tools for everyone to express themselves.</p><p data-block-key="brd72">Veo 3 and Imagen 4, our newest video and image generation models, push the frontier of media generation, with their groundbreaking new capabilities. We're also expanding access to Lyria 2, giving <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">musicians</a> more tools to create music. Finally, we’re inviting visual storytellers to try <a href="http://flow.google/">Flow</a>, our new AI filmmaking tool. Using Google DeepMind’s most advanced models, Flow lets you weave cinematic films with more sophisticated control of characters, scenes and styles, to bring your story to life.</p><p data-block-key="3sps2">We’ve <a href="https://blog.google/technology/google-deepmind/deepmind-primordial-soup-collaboration">partnered closely with the creative industries</a> — filmmakers, musicians, artists, YouTube creators — to help <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">shape these models</a> and products responsibly and to give creators new tools to realize the possibilities of AI in their art.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="4kbui">Veo 3: Video, meet audio</h2><p data-block-key="6m4h7"><a href="http://deepmind.google/models/veo">Veo 3, our new state-of-the-art video generation model</a>, not only improves on the quality of Veo 2, but for the first time, can also generate videos with audio — traffic noises in the background of a city street scene, birds singing in a park, even dialogue between characters.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="4" thumbnail-alt="Veo created video" video-id="Yqg0oUaofH0" video-type="video">
  </uni-youtube-player-article>


  


  

  
    
  
    


  <uni-youtube-player-article index="5" thumbnail-alt="Veo created video" video-id="BTx-MnBCDvw" video-type="video">
  </uni-youtube-player-article>


  


  

  
    
  
    


  <uni-youtube-player-article index="6" thumbnail-alt="Veo created video" video-id="OW9q6SWTXt8" video-type="video">
  </uni-youtube-player-article>


  


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }">
        <p data-block-key="zbevq">Across the board, Veo 3 excels from text and image prompting to real-world physics and accurate lip syncing. It’s great at understanding; you can tell a short story in your prompt, and the model gives you back a clip that brings it to life. Veo 3 is available today for Ultra subscribers in the United States in the <a href="http://gemini.google.com/">Gemini app</a> and in <a href="http://flow.google/">Flow</a>. It’s also available for enterprise users on <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a>.</p>
      </div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="ao97o">Veo 2 updates: New capabilities built with and for filmmakers</h2><p data-block-key="bad3q">As we advance Veo 3, we’ve also added new capabilities to our popular Veo 2 model informed by our work with creators and <a href="https://youtu.be/Q4zqVrLI8a0">filmmakers</a>. Today, we’re launching several of these new capabilities, including:<br></p><ul><li data-block-key="4mc0b"><b>Our state-of-the-art reference powered video</b> capability allows you to give Veo images of characters, scenes, objects, and even styles for better creative control and consistency.</li><li data-block-key="emn6e"><b>Camera controls</b> help you define precise camera movements, including rotations, dollies and zooms, to achieve the perfect shot.</li><li data-block-key="eg4l4"><b>Outpainting</b> allows you to broaden your frame, turning your video from portrait to landscape, and making it easier to fit any screen size, intelligently adding to the scene.</li><li data-block-key="auro7"><b>Object add and remove</b> lets you add or erase objects from your videos. Veo understands scale, interactions, and shadows, and uses this understanding to create a natural, realistic-looking scene.</li></ul><p data-block-key="6heha">Reference powered video and camera controls are available now in <a href="http://flow.google/">Flow</a>. We're excited to bring all these <a href="http://deepmind.google/models/veo">new capabilities</a> to the <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI API</a> in the coming weeks, and to more products over the next few months.</p></div>
  

  
    









<uni-image-carousel section-header="Fuel your creativity with new generative media  models and tools" images="[
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Veo2-CameraControls-Keyword-250519-r03.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Veo 2 camera controls&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;veo2controls&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/comp_wm_us_genmedia_keyword_refernce2video_cap-000000-final.mp4 &quot;],
        
        &quot;alt&quot;: &quot;A woman walking in a hallway made with Veo2&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;veo2&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_961d999b-860d-45d1-b9bd-6ca3ae960e9b_8fb2c1bc-fa94-433f-9426-96db830ca5c0.webm &quot;],
        
        &quot;alt&quot;: &quot;A knit scene made by Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Knit veo&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_e36d3cd7-863b-4e52-aba1-966dbd062da0_36176986-0863-4322-a518-41d4251c66d1_.mp4 &quot;],
        
        &quot;alt&quot;: &quot;A knit scene made by Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Outpainting&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_veo_2_792c9917-dcad-4272-bf7d-1ccf7ce1081d_bab3dce1-35d4-4c3d-b0f4-c6e819.webm &quot;],
        
        &quot;alt&quot;: &quot;Astronaut scene with Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;outer space&quot;
      },
    
      {
        
          &quot;src&quot;: [&quot; https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/us_wm_2b963087-a227-4984-b333-88eabb35553e_a46aa06d-c14f-4c85-a428-05332_QgLmCGP.mp4 &quot;],
        
        &quot;alt&quot;: &quot;Astronaut scene with Veo&quot;,
        &quot;isVideo&quot;: true,
        &quot;videoTitle&quot;: &quot;Image removal&quot;
      }
    
  ]">
  
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
</uni-image-carousel>

  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="zbevq">Flow: An AI filmmaking tool designed for Veo</h2><p data-block-key="p3q8">Built with and for creatives, <a href="https://blog.google/technology/ai/google-flow-veo-ai-filmmaking-tool/">Flow</a> is an AI filmmaking tool that lets you seamlessly create cinematic clips, scenes and stories by bringing together Google DeepMind’s most advanced models: Veo, Imagen and Gemini. Use natural language to describe your shots to Flow, manage the ingredients for your story — cast, locations, objects and styles — in a single convenient place, and use Flow to weave your narrative into beautiful scenes.</p><p data-block-key="aodop"><a href="http://flow.google/">Flow</a> is available today for Google AI Pro and Ultra plan subscribers in the U.S., with more countries coming soon.</p></div>
  

  
    
  
    


  <uni-youtube-player-article index="13" thumbnail-alt="Flow product sizzle video" video-id="A0VttaLy4sU" video-type="video" image="Flow_Sizzle Thumbnail 1280 x 720 (1)" video-image-url-lazy="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720_.width-100.format-webp.webp" video-image-url-mobile="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720_.width-700.format-webp.webp" video-image-url-desktop="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flow_Sizzle_Thumbnail_1280_x_720.width-1000.format-webp.webp">
  </uni-youtube-player-article>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="zbevq">Imagen 4: Stunning quality and superior typography</h2><p data-block-key="1hinf">Our latest Imagen model combines speed with precision to create stunning images. Imagen 4 has remarkable clarity in fine details like intricate fabrics, water droplets, and animal fur, and excels in both photorealistic and abstract styles. Imagen 4 can create images in a range of aspect ratios and up to 2k resolution - even better for printing or presentations. It is also significantly better at spelling and typography, making it easier to create your own greeting cards, posters and even comics.</p></div>
  

  
    









<uni-image-carousel section-header="Fuel your creativity with new generative media  models and tools" images="[
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/wm_us_extra_batch_16_05_2015_.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/wm_us_extra_batch_16_05_2015_.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Image of whale created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_0_ComicCrocodile.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_0_ComicCrocodile.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Comic strip created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_47.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_47.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Graphic created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_14.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_14.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Dog image created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_79.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_79.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Image of woman created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_44.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_44.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Lake painting created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_57.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_57.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Field photo created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_7_Eggs.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_7_Eggs.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Egg carton photo created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_74.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_74.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Knit scene created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      },
    
      {
        
          
          
          &quot;src&quot;: [&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_1_ComicCat.max-1536x1536.format-webp.webp&quot;,&quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/WM_1_ComicCat.max-1536x1536.format-webp.webp&quot;],
        
        &quot;alt&quot;: &quot;Cat comic created by Imagen 4&quot;,
        &quot;isVideo&quot;: false,
        &quot;videoTitle&quot;: &quot;&quot;
      }
    
  ]">
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
</uni-image-carousel>

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><p data-block-key="zbevq">Imagen 4 is available today in the <a href="http://gemini.google.com/">Gemini app</a>, <a href="http://labs.google/whisk">Whisk</a>, <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a> and across Slides, Vids, Docs and more in <a href="https://workspace.google.com/blog/product-announcements/new-ways-to-do-your-best-work">Workspace</a>.</p><p data-block-key="cu39m">Soon we’ll also be launching a fast variant of Imagen 4 that’s up to 10x faster than Imagen 3 — so you can explore ideas even faster.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="cq60v">Lyria 2: Powerful composition and endless exploration</h2><p data-block-key="67lpr">In April, we <a href="https://deepmind.google/discover/blog/music-ai-sandbox-now-with-new-features-and-broader-access/">expanded access</a> to Music AI Sandbox, powered by <a href="https://deepmind.google/technologies/lyria">Lyria 2</a>. Music AI Sandbox offers musicians, producers and songwriters a set of experimental tools, which can spark new creative possibilities and help artists explore unique musical ideas. The expertise and valuable feedback from the music industry help us ensure our tools empower creators, while inviting creatives to realize the possibilities of AI in their art.</p><p data-block-key="8av0l">Lyria 2 brings powerful composition and endless exploration, and is now available for creators <a href="https://www.youtube.com/shorts/uQY6Z6O_dZQ">through YouTube Shorts</a> and enterprises in <a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-veo-3-imagen-4-and-lyria-2-on-vertex-ai">Vertex AI</a>. We've also made <a href="https://goo.gle/lyria-realtime">Lyria RealTime</a>, our interactive music generation model which powers <a href="https://labs.google/fx/tools/music-fx-dj">MusicFX DJ</a>, available via an <a href="https://ai.google.dev/gemini-api/docs/music-generation">API</a> and in <a href="https://aistudio.google.com/app/apps/bundled/midi-dj">AI Studio</a>. Lyria RealTime allows anyone to interactively create, control, and perform generative music in real time.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Fuel your creativity with new generative media  models and tools&quot;
         }"><h2 data-block-key="5n4ts">Responsible creation and collaboration with the creative community</h2><p data-block-key="4c2ru">Since launching in 2023, <a href="https://deepmind.google/technologies/synthid/">SynthID</a> has watermarked over 10 billion images, videos, audio files and texts, helping identify them as AI-generated and reduce the chances of misinformation and misattribution. Outputs generated by Veo 3, Imagen 4 and Lyria 2 will continue to have <a href="https://deepmind.google/technologies/synthid/">SynthID</a> watermarks.</p><p data-block-key="6k365">Today, we’re launching <a href="https://blog.google/technology/ai/google-synthid-ai-content-detector/">SynthID Detector</a>, a verification portal to help people identify AI-generated content. Upload a piece of content and the SynthID Detector will identify if either the entire file or just a part of it has SynthID in it.</p><p data-block-key="c84dh">With all our generative AI models, we aim to unleash human creativity and enable artists and creators to bring their ideas to life faster and more easily than ever before.</p></div>
  

  
    







<uni-related-content-tout title="I/O 2025" cta="See more" summary="We’re doing cutting-edge research to build the most helpful AI that’s more intelligent, agentic and personalized." hideimage="False" eyebrow="Collection" image-alt-text="Stylized 3D text &quot;IO25&quot; in vibrant, gradient colors on a white grid background." role="none" fullurl="https://blog.google/technology/developers/google-io-2025-collection/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp" alt="Stylized 3D text &quot;IO25&quot; in vibrant, gradient colors on a white grid background." sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/IO25_hero.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
    
    
    <div id="newsletter-form--form">
      <form data-method="POST" data-action="https://services.google.com/fb/submissions/thekeywordnewsletterprodv2/">
        <h2 id="subscribe_box_label">
          <span>Get more stories from Google in your inbox.</span>
          
        </h2>
        
      </form>
    </div>
    
    <div>
      <div>
        <p tabindex="-1" role="status" aria-live="off" aria-atomic="false">
          Done. Just one step more.
        </p>
        <p>
          Check your inbox to confirm your subscription.
        </p>
        <p>You are already subscribed to our newsletter.</p>
      </div>
      <p>
        You can also subscribe with a
        
        
      </p>
    </div>
  </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Tiling Window Manager for Windows, Written in Janet (176 pts)]]></title>
            <link>https://agent-kilo.github.io/jwno/</link>
            <guid>44042490</guid>
            <pubDate>Tue, 20 May 2025 15:08:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://agent-kilo.github.io/jwno/">https://agent-kilo.github.io/jwno/</a>, See on <a href="https://news.ycombinator.com/item?id=44042490">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-container">

      <div>
        <header id="main-banner">
          <h2>
            <p><img alt="Jwno Logo" src="https://agent-kilo.github.io/jwno/img/jwno.png"> 
            </p>
            
          </h2>
        </header>
      </div>

      <div id="content">
            

<p>
<em>Jwno</em> is a highly customizable tiling window manager for Windows 10/11, built with <a href="https://janet-lang.org/">Janet</a> and ❤️. It brings to your desktop magical parentheses power, which, <span>I assure you, <span>is not suspicious at all, <span>and totally controllable.</span></span></span>
</p><div>
      <p><a href="https://agent-kilo.github.io/jwno/img/jwno-emacs-repl.jpg">
        <img alt="Jwno managing some Emacs frames, and its own REPL window." src="https://agent-kilo.github.io/jwno/img/jwno-emacs-repl.jpg">
      </a></p><p>Jwno managing some <a href="https://www.gnu.org/software/emacs/">Emacs</a> frames, and its own REPL window.</p>
    </div><p><strong>Note: This documentation is work-in-progress. Some links may return 404, because I have not finished writing those pages yet 😅.</strong></p><h3 id="Cute-Pictures">Cute Pictures
</h3>
<div>



<div>
  <p><a href="https://agent-kilo.github.io/jwno/img/jwno-win10-settings-ui-hint.jpg">
    <img alt="Using Jwno's UI hint feature to interact with UI elements." src="https://agent-kilo.github.io/jwno/img/jwno-win10-settings-ui-hint.jpg">
  </a></p><p>Using Jwno's UI hint feature to interact with UI elements.</p>
</div>

<div>
  <p><a href="https://agent-kilo.github.io/jwno/img/kitty-coming-at-you.jpg">
    <img alt="My cat coming at you." src="https://agent-kilo.github.io/jwno/img/kitty-coming-at-you.jpg">
  </a></p><p>My cat coming at you.</p>
</div>

</div><h3 id="Quick-Links">Quick Links
</h3>

          </div> <!-- id: content -->

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: 90s.dev - game maker that runs on the web (203 pts)]]></title>
            <link>https://90s.dev/blog/finally-releasing-90s-dev.html</link>
            <guid>44042371</guid>
            <pubDate>Tue, 20 May 2025 14:58:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://90s.dev/blog/finally-releasing-90s-dev.html">https://90s.dev/blog/finally-releasing-90s-dev.html</a>, See on <a href="https://news.ycombinator.com/item?id=44042371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><h2 id="finally-releasing-90sdev" tabindex="-1"><a href="#finally-releasing-90sdev">#</a> Finally releasing 90s.dev</h2>
<p>I’ve mentioned this a bunch on HN
and I’ve been working on it nonstop since about February.
So I’m pretty excited to finally make it public.</p>
<section>
<h3 id="feedback-noted" tabindex="-1"><a href="#feedback-noted">#</a> Feedback noted!!</h3>
<p>Thanks everyone on HN for your feedback.</p>
<p>It’s clear that I launched this too soon, and that it needs more
working apps, sample games, and thorough tutorials. I’ll work on these.</p>
</section>
<h2 id="the-dream" tabindex="-1"><a href="#the-dream">#</a> The dream</h2>
<p>Ever since I was a kid, I wanted to recreate Warcraft II or at least Warcraft I.
But for decades, I never got around to it.
One day this February, I just got up at 2am and started writing code.
I was tired of waiting.
So I wrote and wrote and wrote.</p>
<p>But instead of making a game,
or even <em>a game engine</em>,
or even <em>the game maker tools</em>,
I found myself making an API for making game maker tools and a game engine and a game.
It turns out, I’m really an <em>API designer</em> at heart. I guess I always kinda knew this.</p>
<p>Eventually it evolved into an API around a 320x180 canvas
for building and sharing apps,
whether a game maker tool like map-maker,
or a 60 fps game.</p>
<h2 id="new-game-maker-that-runs-on-the-web" tabindex="-1"><a href="#new-game-maker-that-runs-on-the-web">#</a> New game maker that runs on the web</h2>
<p>In short, <a href="https://90s.dev/os/">90s.dev/os/</a> is a unique new kind of game maker:</p>
<ul>
<li>Runs in the browser using an HTML canvas for portability.</li>
<li>Has a <code>320x180 (16:9)</code> screen that scales up to fill the window.</li>
<li>Runs all apps inside web workers for security and performance.</li>
<li>Gives you full access to WebGL2 via OffscreenCanvas for <code>60 fps</code> games.</li>
<li>Lets you publish and load apps located on GitHub or NPM.</li>
<li>Has a new GUI API that has legitimate innovations, even in 2025.</li>
<li>Comes with a TypeScript-first VSCode-ready SDK for fast prototyping.</li>
<li>Allows importing modules written in any languages that compile to wasm.</li>
</ul>
<p>By default, it will come with basic apps for making pixel art data for games,
such as paint, sprite-maker, and map-maker.
Someone else will have to make the sound-effect-editor and music-editor,
since I know nothing about web audio APIs.</p>
<p>But that’s the beauty and point of it,
you <em>can</em> make those apps <em>and</em> publish them,
and they will be first-class apps that anyone can run,
and that you can share with an iframe or link.</p>
<p>For example, click this to open my 40%-finished paint app,
and click again to close it: <a href="https://90s.dev/os/#sys/apps/paint.app.js">/os/#sys/apps/paint.app.js</a></p>
<h3 id="inspired-by-gamedev-prototyping-apps" tabindex="-1"><a href="#inspired-by-gamedev-prototyping-apps">#</a> Inspired by gamedev prototyping apps</h3>
<p>It’s inspired by pico8, tic80, picotron, and love2d:</p>
<ul>
<li>Like pico8, it aims for aesthetic minimalism and supports just one language.</li>
<li>Like tic80, it lifts most of the restrictions of pico8.</li>
<li>Like love2d, it requires an external IDE for actually writing code.</li>
<li>Like picotron, it uses an operating system architecture to run apps.</li>
</ul>
<p>You could think of it as a meta-pico8, or a love2d with TypeScript,
or a tic80 that added extensibility vertically instead of horizontally.</p>
<h2 id="genuine-gui-innovations" tabindex="-1"><a href="#genuine-gui-innovations">#</a> Genuine GUI innovations</h2>
<p>I honestly wasn’t trying to innovate here.
All I wanted was a simple GUI that let me
build the side panel in Warcraft I and II
with its action buttons like Move and Attack.</p>
<p>So I wrote a very typical view API,
where views draw themselves to screen
and have child views.
For a while, it was very ordinary and boring,
which for a GUI is a good thing.</p>
<h3 id="layout" tabindex="-1"><a href="#layout">#</a> Layout</h3>
<p>But I got tired of
manually positioning and resizing
all the views in a tree,
so I ended up with
an extremely simple <a href="https://90s.dev/technical/views.html#layout">auto-layout system</a>
that also happens to be robust.</p>
<h3 id="refs" tabindex="-1"><a href="#refs">#</a> Refs</h3>
<p>I also got tired of manually setting values
such as size, children, or background color,
especially when most of the time
it was in response to another value changing,
and the new value was directly based on that changed value.
I also got tired of adding callbacks to everything.</p>
<p>So <a href="https://90s.dev/technical/views.html#refs">refs</a> gradually emerged,
basically watchable pointers,
and after about a month or so, they became so stable
that I reworked the view APIs so that
all view properties are backed by refs.
This means all properties are watchable,
and you can give a ref to any property.</p>
<p><em>Note</em>: Even though refs share the same name
as similar concepts in other systems,
I designed mine from the ground up,
with zero inspiration from those other systems.
They’re nothing like React refs,
and I didn’t even know Vue had refs until last week.</p>
<h3 id="composites" tabindex="-1"><a href="#composites">#</a> Composites</h3>
<p>And finally, I stumbled on an interesting property of JSX.
It turns out that if you reverse the way HTML and React use
strings vs values for JSX tags, string tags become the <em>perfect way</em>
to decouple an abstract view’s implementation from its usage.
With a concrete (capitalized) view, you have to import it and use it directly.
But an abstract (lowercase) view can be added to a global table by some library or app,
and then used by an entirely separate part of the system.</p>
<p>This lead to <a href="https://90s.dev/technical/views.html#composites">composites</a>:</p>
<pre tabindex="0"><code><span><span>import</span><span> { </span><span>Button</span><span> } </span><span>from</span><span> '/os/api.js'</span></span>
<span></span>
<span><span>const</span><span> b1</span><span> = </span><span>&lt;</span><span>Button</span><span> onClick</span><span>=</span><span>{</span><span>...</span><span>}</span><span>&gt;</span><span>...</span><span>&lt;/</span><span>Button</span><span>&gt;</span><span> // concrete</span></span>
<span><span>const</span><span> b2</span><span> = </span><span>&lt;</span><span>button</span><span> onClick</span><span>=</span><span>{</span><span>...</span><span>}</span><span>&gt;</span><span>...</span><span>&lt;/</span><span>button</span><span>&gt;</span><span> // abstract</span></span></code></pre>

<p>This is a <em>surprisingly powerful</em> concept for GUI app development.
The concrete view above is always going to look and work the same.
But the abstract view above can be implemented <em>in any way at all</em>.</p>
<p>This is most clear with the colorpicker view:</p>
<pre tabindex="0"><code><span><span>const</span><span> $color</span><span> = </span><span>$</span><span>(</span><span>0x00000000</span><span>)</span></span>
<span><span>const</span><span> view</span><span> = </span><span>&lt;</span><span>colorpicker</span><span> $color</span><span>=</span><span>{</span><span>$color</span><span>}</span><span>/&gt;</span></span></code></pre>

<p>The default implementation of this has a bare-basic color picker,
that allows only 48 colors from two 24 color palettes (sweet24 and vinik24).</p>
<p>But any app or library developer can override this to return
<em>any kind</em> of color picker they want. Maybe a more traditional one
that has sliders for RGBA and has 16 million colors,
or maybe one that lets you choose from a large list of palettes,
or maybe one that mimicks pico8’s exactly.</p>
<h2 id="a-note-on-publishing-apps" tabindex="-1"><a href="#a-note-on-publishing-apps">#</a> A note on publishing apps</h2>
<p>Until just a couple days ago,
there was a shared drive in the built-in file system called <code>net/</code>
that was backed by a database,
and the way to share apps or libraries or game assets was
to copy your files into your own folder in this drive.
You could easily import modules via:</p>
<pre tabindex="0"><code><span><span>import</span><span> { </span><span>stuff</span><span> } </span><span>from</span><span> '/os/fs/net/someuser/some/file.js'</span></span></code></pre>

<p>And it just worked, thanks to a service worker.</p>
<p>But this weekend, I had an epiphany of
how to use the same service worker to
import files hosted on NPM or GitHub, via CDN.
So I removed <code>net/</code> and decided to support this:</p>
<pre tabindex="0"><code><span><span>import</span><span> { </span><span>stuff</span><span> } </span><span>from</span><span> '/os/fs/ghb/someuser/project@1.0.0/some/file.js'</span><span> // or:</span></span>
<span><span>import</span><span> { </span><span>stuff</span><span> } </span><span>from</span><span> '/os/fs/npm/someuser/project@1.0.0/some/file.js'</span></span></code></pre>

<p>This is in the works, but it’s mostly done.</p>

<p>Since this is designed as an operating system,
where the screen is just a 320x180 canvas,
it needs apps.
The built-in apps will be <em>serviceable at best</em>.</p>
<p>Ideally, the community makes their own apps,
and uses those apps to make game assets and games,
and all of these get shared with other users.</p>
<p>Since this requires coordination among the community,
there is an issue tracker, wiki, and discussion forum
in the sidebar, each hosted in a GitHub repo.</p>
<ul>
<li>
<p><a href="https://github.com/sdegutis/90s.dev/issues">Issues</a>: For feature requests and bug reports</p>
</li>
<li>
<p><a href="https://github.com/sdegutis/90s.dev/discussions">Forum</a>: The announce and discuss projects</p>
</li>
<li>
<p><a href="https://github.com/sdegutis/90s.dev/wiki">Wiki</a>: To collectively curate and find projects</p>
</li>
</ul>
<p>For sharing apps, use the link format <code>/os/#app</code>, e.g.</p>
<ul>
<li><a href="https://90s.dev/os/#sys/apps/paint.app.js">/os/#sys/apps/paint.app.js</a></li>
<li><a href="https://90s.dev/os/#sys/apps/fontmaker.app.js">/os/#sys/apps/fontmaker.app.js</a></li>
<li><a href="https://90s.dev/os/#sys/apps/spritemaker.app.js">/os/#sys/apps/spritemaker.app.js</a></li>
<li><code>/os/#ghb/someuser/someapp.app.js</code> (coming soon)</li>
<li><code>/os/#npm/someuser/someapp.app.js</code> (coming soon)</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Codex hands-on review (113 pts)]]></title>
            <link>https://zackproser.com/blog/openai-codex-review</link>
            <guid>44042070</guid>
            <pubDate>Tue, 20 May 2025 14:29:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zackproser.com/blog/openai-codex-review">https://zackproser.com/blog/openai-codex-review</a>, See on <a href="https://news.ycombinator.com/item?id=44042070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="I got hands-on with OpenAI's Codex research preview. Here's what I thought..." loading="lazy" width="1024" height="1024" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-hero.0b31348f.webp&amp;w=1080&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-hero.0b31348f.webp&amp;w=2048&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-hero.0b31348f.webp&amp;w=2048&amp;q=75"></p><figcaption>The interface I want and the performance I'll have to wait for...</figcaption>
<h2 id="table-of-contents">Table of contents</h2>
<ul>
<li><a href="#codex-how-it-works">Codex: How it works</a></li>
<li><a href="#things-i-like-about-codex">Things I like about Codex</a>
<ul>
<li><a href="#consciousness-and-desire-are-multi-threaded">Consciousness and desire are multi-threaded</a></li>
<li><a href="#i-think-this-will-eventually-support-my-dream-untethered-workflow">I think this will eventually support my dream untethered workflow</a></li>
<li><a href="#follow-ups-via-chat">Follow ups via chat</a></li>
<li><a href="#looks-good---ship-it">Looks good - ship it!</a></li>
<li><a href="#monitor-logs-and-progress-of-tasks">Monitor logs and progress of tasks</a></li>
</ul>
</li>
<li><a href="#things-im-waiting-on-to-improve">Things I'm waiting on to improve</a>
<ul>
<li><a href="#poor-error-handling">Poor error handling</a></li>
<li><a href="#code-quality-and-one-shot-task-execution">Code quality and one-shot task execution</a></li>
<li><a href="#multi-turn-updates-on-a-branch">Multi-turn updates on a branch</a></li>
<li><a href="#lack-of-network-connectivity-in-execution-sandboxes">Lack of network connectivity in execution sandboxes</a></li>
</ul>
</li>
<li><a href="#did-it-unlock-insane-productivity-gains-for-me">Did it unlock insane productivity gains for me?</a></li>
</ul>
<h2 id="codex-how-it-works">Codex: How it works</h2>
<p>Codex is currently a chat-first experience. You gain access by being invited or by paying for the Pro ($200/per month) subscription.</p>
<p><img alt="Codex is a chat-first experience now" loading="lazy" width="1846" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-main-chat-interface.08c851de.webp&amp;w=1920&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-main-chat-interface.08c851de.webp&amp;w=3840&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-main-chat-interface.08c851de.webp&amp;w=3840&amp;q=75"></p><p>Once you've got access, you start by enabling multi-factor authentication, which is required to use Codex. Next, you authorize the Codex GitHub app for each organization you want it to work with.</p>
<p>Codex then clones your repositories into its own sandboxes so it can run commands and create branches on your behalf.</p>
<p>If you maintain dozens of public and private repositories, this setup is fantastic because you can jump between projects and queue up tasks for each of them without leaving the interface.</p>
<p>If you only keep a single repo or two, the overhead may feel heavier than just asking an LLM for help or working in an AI-powered editor like Cursor.</p>
<h2 id="things-i-like-about-codex">Things I like about Codex</h2>
<h3 id="consciousness-and-desire-are-multi-threaded">Consciousness and desire are multi-threaded</h3>
<p>Codex feels like it was designed for me.</p>
<p>This GitHub connection allows you to specify which repository and which branch your current instructions are for, because the primary chat interface is contemplated as a place for you to
rapid-fire a day's worth of tasks into the interface to spin up multiple tasks in parallel.</p>
<p><img alt="A real Codex task" loading="lazy" width="2874" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-real-task.630f48c5.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-real-task.630f48c5.webp&amp;w=3840&amp;q=75"></p><p>I took a swing through the Codex best practices guide, which encourages you to spin up as many tasks as you need. The current rate limits support you doing this.</p>
<p>This is one of the things I like most about Codex and that I'm most excited for as the platform improves, because this gels with the way I work.</p>
<p><img alt="Asking Codex to resolve a merge issue" loading="lazy" width="2538" height="1378" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fresolve-merge-issue.373897fe.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fresolve-merge-issue.373897fe.webp&amp;w=3840&amp;q=75"></p><p>By the time I start work, I tend to have a laundry list of items I want to complete, so initiating a ton of them in parallel via natural language feels like a reasonable interface.</p>
<h3 id="i-think-this-will-eventually-support-my-dream-untethered-workflow">I think this will eventually support my dream untethered workflow</h3>
<p>As I wrote about in <a href="https://zackproser.com/blog/walking-and-talking-with-ai">Walking and talking with AI in the woods</a>, ideally I'd like to start my morning in an office, launch a bunch of tasks,
get some planning out of the way, and then step out for a long walk in nature.</p>
<p>Codex is usable from my phone even today:</p>
<p><img alt="Codex is even usable on mobile" loading="lazy" width="1080" height="2400" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone.0d9e432e.webp&amp;w=1080&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone.0d9e432e.webp&amp;w=3840&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone.0d9e432e.webp&amp;w=3840&amp;q=75"></p><p>I think, ultimately, once some of the sharp edges are polished, Codex will support me and others in performing our work effectively away from our desks.</p>
<p><img alt="Codex and similar tools will ultimately support workers who want to be effective away from their desks" loading="lazy" width="1080" height="2400" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone-2.07531e6e.webp&amp;w=1080&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone-2.07531e6e.webp&amp;w=3840&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-phone-2.07531e6e.webp&amp;w=3840&amp;q=75"></p><h3 id="follow-ups-via-chat">Follow ups via chat</h3>
<p>Once your initial task has had some time to bake, you can click into it to view its progress, see the logs and make follow-up requests via a very familiar chat interface.</p>
<p><img alt="Codex exposes a familiar chat interface" loading="lazy" width="2874" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fchat-thread.ba2d50cb.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fchat-thread.ba2d50cb.webp&amp;w=3840&amp;q=75"></p><h3 id="looks-good---ship-it">Looks good - ship it!</h3>
<p><img alt="Once you're satisfied, Codex can open your PRs" loading="lazy" width="1494" height="1542" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-create-pr.347d5f6c.webp&amp;w=1920&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-create-pr.347d5f6c.webp&amp;w=3840&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-create-pr.347d5f6c.webp&amp;w=3840&amp;q=75"></p><p>Once you're satisfied with the changes on a given branch, you can tell Codex to open a PR for you, and it will automatically
fill in the description.</p>
<p><img alt="Codex hard at working opening your pull request" loading="lazy" width="1512" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-creating-pr.984ccff1.webp&amp;w=1920&amp;q=75 1x, https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-creating-pr.984ccff1.webp&amp;w=3840&amp;q=75 2x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-creating-pr.984ccff1.webp&amp;w=3840&amp;q=75"></p><h3 id="monitor-logs-and-progress-of-tasks">Monitor logs and progress of tasks</h3>
<p>You can step into any tasks to see the chat pane but also the raw logs, which show you the commands and shells that Codex is spawning
in order to make changes.</p>
<p><img alt="Launching a new environment via Codex" loading="lazy" width="2394" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-launch-new-environment.eac0b16b.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-launch-new-environment.eac0b16b.webp&amp;w=3840&amp;q=75"></p><h2 id="things-im-waiting-on-to-improve">Things I'm waiting on to improve</h2>
<h3 id="poor-error-handling">Poor error handling</h3>
<p>Starting tasks fails. Opening pull requests fails.</p>
<p>Why?</p>
<p><img alt="Codex currently shows evidence of poor or incomplete error handling" loading="lazy" width="2432" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-poor-error-handling.d0a80b79.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-poor-error-handling.d0a80b79.webp&amp;w=3840&amp;q=75"></p><p>Who knows.</p>
<h3 id="code-quality-and-one-shot-task-execution">Code quality and one-shot task execution</h3>
<p>I've been experimenting with Codex for about 3 days at the time of writing. I haven't yet noticed a marked difference in the performance of the Codex model, which OpenAI explains is a descendant of GPT-3 and is proficient in more
than 12 programming languages.</p>
<p>Right now, it feels like I can spin up multiple tasks in parallel with a 40-60% chance that I'll be content enough with the result to hit the Open PR button instead of requesting changes.</p>
<p>So far Codex has been perfect for firing off a bunch of maintenance-level updates: minor copy tweaks, style changes and other small chores. I've tried asking it to tackle larger refactors and the experience quickly becomes cumbersome. The current workflow wants to open a fresh pull request for every iteration, which means pushing follow-up commits to an existing branch is awkward at best.</p>
<h3 id="multi-turn-updates-on-a-branch">Multi-turn updates on a branch</h3>
<p>Updating existing PRs is rough.</p>
<p>It's not clear when or if changes will be pushed on an existing branch, and right now the app encourages you to create more pull requests.
That makes multi-step refactors tricky because you can't reliably iterate on the same branch within Codex. Until this becomes smoother, I plan to use it mainly for the quick wins that can ship in a single pass.</p>
<h3 id="lack-of-network-connectivity-in-execution-sandboxes">Lack of network connectivity in execution sandboxes</h3>
<p>To be clear, I understand this is an intentional design choice. It mitigates remote code execution vulnerabilities amongst others.</p>
<p>This currently blocks the use of Codex for a lot of the tasks that working developers are going to want to use it for, namely
resolving annoying dependency issues by installing a more recent version of a package and regenerating the relevant lockfiles in the process.</p>
<p>Codex can't reach the internet right now, but it does have your repo freshly cloned and made available to its execution environment.</p>
<p>This means it can't <code>pnpm add @tar-fs@latest</code> even if you ask it to. So, for now, I'll still be pulling down these branches and fixing them locally or
commenting <code>@dependabot rebase</code> on PRs that support it.</p>
<h2 id="did-it-unlock-insane-productivity-gains-for-me">Did it unlock insane productivity gains for me?</h2>
<p>Not yet, but I can see how it will once:</p>
<ul>
<li>More tasks become one-shottable via additional refinements or model training or perhaps even the ability to multipex between different models for different tasks</li>
<li>The dev ex around opening and pushing to existing branches to update an already open pull request is improved</li>
<li>Codex enables more integrations with additional OpenAI platform capabilities such as generating images.</li>
<li>Codex (potentially) becomes more of the high-level orchestration and signaling layer that humans primarily work out of</li>
</ul>
<p>At the moment, Codex is useful for flushing the low priority yet numerous and tedious maintenance tasks and small updates at the beginning of the day.</p>
<p>For significant refactoring or feature-building, I'm still better served doing that myself in an IDE with optional LLM support.</p>
<p><img alt="I'm sure that imminent improvements will make Codex even more usable soon" loading="lazy" width="2874" height="1760" decoding="async" data-nimg="1" srcset="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-final.92e9e939.webp&amp;w=3840&amp;q=75 1x" src="https://zackproser.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcodex-final.92e9e939.webp&amp;w=3840&amp;q=75"></p><p>I'm confident that shortly I'll be able to use Codex as an indeal interface for starting a day of work and for keeping tabs on what needs attention and what is up next.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Learning Is Applied Topology (308 pts)]]></title>
            <link>https://theahura.substack.com/p/deep-learning-is-applied-topology</link>
            <guid>44041738</guid>
            <pubDate>Tue, 20 May 2025 13:54:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/deep-learning-is-applied-topology">https://theahura.substack.com/p/deep-learning-is-applied-topology</a>, See on <a href="https://news.ycombinator.com/item?id=44041738">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>When I think about AI, I think about topology.</p><p>Topology is a big scary math word that basically means 'the study of surfaces'. Imagine you had a surface made of play-doh. You could bend it, or twist it, or stretch it. But as long as you don't rip the play-doh up, or poke a hole through it, you can define certain properties that would remain true regardless of the deformation you apply. Here's an example. Let's say I flatten out my play-doh and then draw a circle on it. I could rotate it, or bend it, or twist it, or whatever. And my circle will change shape, for sure. But my circle will never magically become a line, nor will it suddenly become two circles, nor will it ever cross over itself. This is topology in a nutshell.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png" width="1315" height="762" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:762,&quot;width&quot;:1315,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc585c47d-c94c-4d6c-979c-ee18cd51e015_1315x762.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Topology shows up in all sorts of places. For example, data science. </p><p>Let's say you had a bunch of data that you wanted to classify. A lot of classification problems are equivalent to being able to cleanly separate data with a line. But when you go to plot the data on a 2D plane, it's just…kinda messy. There's no obvious line that you could draw on your plane to neatly separate the data. Does that mean that the data cannot be classified? Not necessarily! You can apply the same kinds of topological deformations to your plane, and maybe you'll find a way to separate the data.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png" width="1456" height="1205" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1205,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8dc662ec-be49-4697-8596-96f21d9e414f_1515x1254.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>This sort of spatial manipulation is at the core of deep learning. People say that a neural network is basically just a stack of linear algebra. Well, that's more or less correct! Linear algebra is really closely tied to the manipulation of surfaces. Matrices themselves are transformations of geometric surfaces. So you can think of a stack of matrix multiplications in topological terms.</p><p><span>This is not a new idea — Chris Olah (one of the founders of Anthropic) wrote about deep learning manifolds </span><a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="">back in 2014</a><span>.</span></p><blockquote><p><span>There are a variety of different kinds of layers used in neural networks. We will talk about tanh layers for a concrete example. A tanh layer — </span><em><strong>tanh(Wx+b)</strong></em><span> — consists of:</span></p><ul><li><p>A linear transformation by the “weight” matrix W</p></li><li><p>A translation by the vector b</p></li><li><p>Point-wise application of tanh.</p></li></ul><p>We can visualize this as a continuous transformation, as follows:</p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif" width="598" height="584" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:584,&quot;width&quot;:598,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F64bbe89e-ce4c-4681-b64f-c2d493997376_598x584.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The gif above goes through each part of the neural network transformation step by step. First there is a linear transformation — that's the part where we go from a square grid to a rhomboid grid, which is represented by a matrix multiplication. Then there's the translation, where we move the grid around. That's just adding or subtracting a vector </span><strong>b</strong><span>. And finally, there's the tanh. That's where we warp the surface.</span></p><p>It turns out that if you stack a bunch of these kinds of transformations on top of each other, you can find ways to separate out some pretty complicated datasets.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif" width="598" height="584" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:584,&quot;width&quot;:598,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3724f705-9fdc-4260-87fd-09dd8d2523de_598x584.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>But, wait, what about things that you </span><em>can't </em><span>just stretch your way out of? If I have a dataset with a bunch of points in a circle surrounded by other points, what then?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png" width="1387" height="782" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:782,&quot;width&quot;:1387,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d27aaa-bcf8-4da8-8631-81241988ed3e_1387x782.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The rules of topology are ironclad — there is no way to cleanly separate these two circles with a single line. But again, that doesn't mean that the data is not separable at all. In such cases, you need to take a lesson from your neighborhood stoner and, like, move to a higher dimension, man.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png" width="812" height="612" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:612,&quot;width&quot;:812,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa606ec47-0259-445b-80f1-8f6b06db7b31_812x612.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Moving from two dimensions to three dimensions suddenly makes the data cleanly separable.</figcaption></figure></div><p>In general, things that are impossible to separate in lower dimensions can be trivial to separate in higher dimensions. Humans obviously spatially reason in three dimensions, but a neural network can “think” in arbitrarily large dimensions. So, it stands to reason, a neural network can separate all sorts of data, even data that isn't immediately obviously separable except in the sort of fuzzy way that only humans can do.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png" width="1456" height="736" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:736,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2518471,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/163753591?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3774990-93a0-45bc-8b17-a67068e50d32_1622x820.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>A human can obviously tell you that the image on the left is a dog and the one on the left is a cat. But, surprisingly, it turns out that if you are looking at enough dimensions you can come up with a </em><strong>mathematical</strong><em> way to separate dogs and cats too.</em></figcaption></figure></div><p><span>One way to think about neural networks, especially really large neural networks, is that they are </span><em>topology generators</em><span>. That is, they will take in a set of data and figure out a topology where the data has certain properties. Those properties are in turn defined by the loss function. If you are trying to solve some kind of categorization task, your model will learn a topology where all the dogs are in one part of the space, and all of the cats are in some other part of the space. If you are trying to learn a translation task — say, English to Spanish, or Images to Text — your model will learn a topology where </span><em>bread </em><span>is close to </span><em>pan</em><span>, or where </span><em>that</em><span> </span><em>picture of a cat</em><span> is close to the word </span><em>cat</em><span>. And if you are trying to learn to predict what the next token is in a sequence, your model will learn a topology where tokens are grouped by their usage. The data all lives on a high-dimensional, semantically relevant manifold. And developing the manifold is exactly equivalent to figuring out how to represent the dataset semantically.</span></p><p>I'm personally pretty convinced that, in a high enough dimensional space, this is indistinguishable from reasoning.</p><p><span>Here's a philosophical question: are topologies created or discovered? You could say that these neural nets are not really </span><em>creating</em><span> topologies. The topologies just, like, exist abstractly already, as a fundamental property of the data we're looking at.</span></p><p>There's some trivial sense in which this is obviously true. For example, think about colors. Imagine you have two RGB vectors that represent colors — [128, 0, 0] and [0, 0, 128], representing red and blue respectively. If you wanted to make purple, what would you do?</p><p>You'd add them together.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F376a3b5b-1d37-49c3-ae58-2b08198d081d_1600x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>There isn't anything special about red-green-blue in specific; you could just as easily represent the space of colors on any 3-axis plot. But the dimensionality of color, the way we think about which colors are similar to each other, the way we mix colors together to create new colors — all of that is innate to the data. "Colors" live on a manifold, one that exists in the abstract.</p><p>Images also live on a manifold. We know that images can be represented as a bunch of numbers of size Height x Width x 3, representing a grid of RGB pixel values. Well, you can flatten all those numbers out into a vector. Now you can think of any image as a point in some high dimensional space. That in turn means that you can construct a topology of every image of a certain size. There will be a region of that space with all of the images that look roughly like Brad Pitt eating a sandwich. Every direction that you move in will represent a change of a single pixel. You can walk this manifold, changing pixel by pixel until you end up at the Mona Lisa.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png" width="1456" height="182" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:182,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2014103,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/163753591?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6a8ff6ab-9bb3-45ed-872e-e88f1e434e2f_3200x400.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>In image space, you could draw a line between Brad Pitt and the Mona Lisa, and every one of these intermediate images would be on that line.</figcaption></figure></div><p>Most of the space is just noise, of course. And it's not particularly useful to group things by their pixel similarity. But this is where we go back to neural networks. A deep neural network can take this surface of images, and stretch it and bend it and twist it until all of the images we care about are close together and all of the noise is far away.</p><p>More generally, the model is able to take arbitrary information and put it onto a topology. Under the hood, the model is storing all of this information in a sort of universal mathematical representation: an embedding vector. That is to say, the text, or images, or whatever else are all internally represented as lists of numbers. This is really powerful. Each embedding vector is tied to a concept, but is also a mathematical construct, literally points in space. If the model learns a well formed topology, you can do mathematical operations on your conceptual data. For example, the famous "king" - "man" + "woman" = "queen" example.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png" width="662" height="223" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:223,&quot;width&quot;:662,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6fb536c-8af4-4846-bd6d-873127cecf63_662x223.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Each of these words is just a point in a high dimensional representational space.</p><p>Images obviously live on a manifold. It turns out words do too. Is there anything that doesn't live on a manifold?</p><p><em>No. Everything lives on a manifold.</em></p><p>There is a topology that measures the size and shape of different kinds of furniture, where chairs are all close to each other and far from the tables. There is a topology that represents weather conditions in Nepal, which probably gets used by JaneStreet to do commodities pricing. There is a topology that compares the smells associated with different emotions. Is this a useful topology? Who knows.</p><p><span>But this all points to another way to think about neural networks: they are </span><em>universal topology discoverers. </em><span>This leads to some useful ideas.</span></p><p>There is some sense in which reasoning itself lives on a manifold. That is, you can imagine a hypothetical manifold where all of the good reasoning is clustered on one side of some topological space, and all of the 'bad' reasoning is clustered on the other side of the space. We may not be able to define 'good' and 'bad' in strict mathematical terms, but as long as we can separate good from bad we can train a neural network to sort out the topology for us.</p><p>And in fact, that is exactly what all the big players — Google / Anthropic / OAI / DeepSeek — are doing.</p><p>The general consensus in the AI world is that we've more or less extracted everything we were going to get out of pure language statistics. Most of the LLMs are trained on trillions of tokens; there may be some additional juice to squeeze by training on quadrillions of tokens, but it's not a lot. Put a slightly different way, learning to predict the next token in a sequence is not going to get you to reasoning, it's going to make you really good at predicting the next token in a sequence, and we've already more or less hit that asymptote.</p><p><span>But it turns out that next-word-prediction is </span><em>like </em><span>reasoning. You can construct a topology where next-word-prediction is in one part of the space, and you want to move towards question-answering reasoning. Instruction tuning and RLHF are ways to move from the next-word-prediction part of the reasoning manifold towards the QA part of the reasoning manifold. And Chain of Thought reasoning explicitly moves towards, well, the 'reasoning' part of the reasoning manifold.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png" width="1456" height="784" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:784,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff01f2af0-a0cb-4183-bef2-385141deb160_1600x862.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Not drawn to scale — the distance between the Random Noise starting point and Next Token Prediction is way way bigger than the distances between any of the other points.</em></figcaption></figure></div><p>That last step says "Chain of Thought Finetuning." That is where most of the value proposition is in AI training right now. The basic idea is straightforward. You know how, when you use a model like o3 or Gemini 2.5, there is a part of the prompt that says "Thinking…" that you can click into to see the 'thought process' of the LLM? That's called a 'reasoning trace'.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png" width="1456" height="614" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:614,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0374d8f-a9da-41df-86a8-52cd35659f4b_1600x675.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If you run queries on LLMs a whole bunch of times, you will be able to find some reasoning traces that are good, and some that are bad. For example, the one above is bad, it should say something like:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png" width="1456" height="622" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:622,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6aa494d-1352-4ba3-a386-0cf5502d86fc_1600x684.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Let's say you run a million such queries, and you end up with 10000 really good queries. Well, you can then use those 10000 really good queries to train a </span><em>new </em><span>model, one that is better at producing only really good reasoning traces.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png" width="1456" height="854" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:854,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0bfc82be-8079-44d5-8eb2-59ab880e8668_1600x938.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Rinse and repeat. As long as you have some kind of method for determining which of two reasoning traces is 'better', you can continue walking along the reasoning manifold by using the previously trained model to 'bootstrap' the next one. Or, if you want to put a slightly different spin on it, you can think of those 10000 really-good-samples as coming from a hypothetical 'more advanced' model, that you are now using to 'distill-train' your current model.</p><p><span>This has been enough to get us to AGI.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163753591" href="https://theahura.substack.com/p/deep-learning-is-applied-topology#footnote-1-163753591" target="_self" rel="">1</a></span><span> But it obviously won't get us to ASI, artificial super intelligence. We will eventually hit an asymptote that is limited by our ability to pick out the 'best' reasoning. Also, it is just extremely expensive (in terms of money and time) to collect a bunch of good reasoning traces for you to train on! Even if you paid a bunch of smart people to do this all day, the results are going to be really subjective and noisy, so you may not even make progress in the right direction on your manifold.</span></p><p>This is where Deepseek R1 and other reinforcement learning methods come into play.</p><p>The Deepseek paper asks a simple question: do we need all of that other stuff? Can't we just go straight from random noise to ASI directly?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png" width="1456" height="584" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:584,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1def8347-971c-4804-a90a-edaf6b06e9f8_1600x642.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>*this picture isn't totally correct, because the deepseek folks start from a pretrained next-token-prediction model. This is likely necessary to get to a part of the reasoning space that is, you know, actually useful. But the idea is generalizable.</em></figcaption></figure></div><p>Their approach is to create quantifiable heuristics for 'good' reasoning. For example, we can come up with unit tests or math problems for the AI to answer. If the AI produces code that makes the unit test pass, or correctly answers the math problem, we can say that the reasoning trace that produced the correct output is a better reasoning trace than one that produces incorrect outputs. No subjective analysis of the reasoning trace required. And this works surprisingly well! They successfully train a model that does pretty well on a suite of reasoning tasks using RL alone.</p><p><span>Unfortunately, though the RL approach gets their model pretty far, it doesn't quite hit ASI. Eventually their RL model hits an asymptote, so they end up squeezing a bit more juice out of their approach by doing the same sampling procedure lined out above. That is, they train a model using RL until it's basically correctly answering every leetcode problem and math question they can give it. And then they curate reasoning traces off that RL model to fine-tune a completely different </span><em>second</em><span> model. So at the end of the day, Deepseek was less about RL and more about generating a lot of really high quality reasoning traces in a way that is less expensive than having humans do it by hand.</span></p><p>Still, this points the way forward towards a more general approach towards traversing the reasoning manifold. Creating systems that can identify good reasoning from bad is a much easier task than creating systems that can reason well to begin with.</p><p>You know what else can be represented as a manifold? Neural networks!</p><p>A neural network is just a list of numbers (weights) arranged in a particular way. So, just like our image example above, you could flatten out every single parameter of a neural network to create a vector and map it onto a surface. You would find clusters of neural network weights that have semantic meaning in different areas of the surface. Some parts of the manifold would correspond to semantic segmentation. Others may correspond to text translation. Still others may correspond to autoencoding. Because the final output weights are easily represented as a tensor, you can do backprop directly over the outputs.</p><p>We currently have powerful techniques to generate images from text. One such method is known as 'diffusion'. The way diffusion works is you take an image and successively add noise to the image. Then you train a model to reverse the noise addition, step by step.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png" width="1181" height="318" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/faf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:318,&quot;width&quot;:1181,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffaf50b8e-523b-4304-be9a-23afb42d4b9a_1181x318.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You could do the exact same thing with a set of neural networks. Start with the pretrained checkpoints of various transformer layers from, say, huggingface. For each transformer layer, you successively add noise to it, thereby creating a diffusion training set.</p><p><span>You could even add text conditioning. All of the pretrained models include explanations of what they do. You could imagine conditioning the diffusion model on those text descriptions. This would effectively result in a model that could diffuse </span><em>other</em><span> pretrained models from text. You could input a prompt like "Spanish to English" and it would spit out a fully trained model </span><em>without you having to do any training</em><span>. Today, in the vast majority of cases, models are randomly initialized. A diffusion model that creates other models may be better than random initialization, and could result in significantly faster training times.</span></p><p>Deep Learning is a tough field precisely because its so informal. We do not have good working theories of what these models are doing and why they work. It’s all intuition. But for me, the topology analysis was a huge unlock. Once I started understanding embedding spaces, everything else fell into place.</p><p>I think there’s more to pull in this thread but I’ll hold off here. This post is already getting away from me and is way too long.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Does the U.S. Always Run a Trade Deficit? (158 pts)]]></title>
            <link>https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/</link>
            <guid>44040407</guid>
            <pubDate>Tue, 20 May 2025 11:42:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/">https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/</a>, See on <a href="https://news.ycombinator.com/item?id=44040407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		



<figure>
	<img fetchpriority="high" decoding="async" width="460" height="288" src="https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2025/05/LSE_2025_trade-deficit_Klitgaard_460.jpg?w=460" alt="Photo: Shipping container in the middle of the ocean." srcset="https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2025/05/LSE_2025_trade-deficit_Klitgaard_460.jpg 920w, https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2025/05/LSE_2025_trade-deficit_Klitgaard_460.jpg?resize=460,288 460w, https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2025/05/LSE_2025_trade-deficit_Klitgaard_460.jpg?resize=768,481 768w" sizes="(max-width: 460px) 100vw, 460px"></figure>



<p>The obvious answer to the question of why the United States runs a trade deficit is that its export sales have not kept up with its demand for imports. A less obvious answer is that the imbalance reflects a macroeconomic phenomenon. Using national accounting, one can show deficits are also due to a persistent shortfall in domestic saving that requires funds from abroad to finance domestic investment spending. Reducing the trade imbalance therefore requires both more exports relative to imports and a narrowing of the gap between saving and investment spending.</p>



<h4><strong>Grounded by Accounting</strong></h4>



<p>To give some intuition for why the trade deficit is equal to the gap between saving and investment spending, assume the U.S. economy is closed to the rest of the world. That is, there are no imports or exports. Spending is either on the consumption of goods and services or investment spending on equipment, structures, and intellectual property products. Income is allocated to either consumption or to saving by households, businesses, and government. In a closed economy, spending equals income—that is, the sum of consumption and saving equals the sum of consumption and investment spending.</p>



<p><em>Spending (Consumption + Investment Spending) = </em><br><em>Income (Consumption + Saving)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>Because consumption drops out on both sides of the equation, investment spending equals domestic saving in the economy. This makes sense: the funds available to invest in productive projects have to come from domestic savers.</p>



<p>Opening up the economy to external borrowing or lending allows domestic saving and investment spending to diverge. In the case of the United States, the economy borrows from the rest of the world because domestic saving is insufficient to fully finance investment spending.</p>



<p><em>Investment Spending = Domestic Saving + Foreign Saving (through net financial inflows)</em></p>



<p>So how is the saving gap connected to international trade? If imports and exports are equal, then the revenue earned from exports matches the spending on imports. If export revenues don’t cover imports, then a country has to offer up IOUs. These come in the form of foreign funds buying domestic assets instead of U.S. exports.</p>



<p><em>Imports = Exports + Net sales of U.S. assets (net financial inflows)</em></p>



<p>Note that these inflows are fungible, so they might initially be used to buy U.S. government bonds, but that frees up other funds to finance the building of homes and the outfitting of factories. (There are financial flows out of the United States to buy foreign assets, so the net of these flows equals U.S. borrowing.)</p>



<p>The key insight is that the amount of U.S. borrowing is the same whether viewed as the difference between saving and investment spending or between exports and imports. It is what it is, and it has to be the same value in both calculations, diverging only because of statistical discrepancies.</p>



<h4><strong>What the Data Say</strong></h4>



<p>The chart below shows gross U.S. saving and investment spending since 2000, with both calculated as shares of nominal GDP to make the values comparable across time. From 2000 to 2007, the gap widened as investment spending as a share of GDP dipped and then recovered while the saving share failed to fully recover. The gap contracted with the global financial crisis in 2008 as investment spending fell by more than saving, and then narrowed further as saving staged a stronger recovery. More recently, saving dipped during the pandemic and has stayed low in the aftermath while investment spending as a share of GDP has been stable over the whole period.</p>







<p>Saving Has Been Persistently Less Than Investment Spending</p>


<figure data-type="line">
	<p>Share of GDP (percent)</p>
	
	<figcaption>Source: Bureau of Economic Analysis.<br>Note: The saving gap differs from the current account balance because of statistical discrepancies. </figcaption>
</figure>







<p>The chart below breaks out household, business, and government saving. (Saving is the difference between income and expenses, with expenses not including investment spending.) Business saving is the most stable, dropping with the financial crisis and rebounding to above its pre-crisis level, then staying near there ever since. Household saving as a share of GDP held up well during the financial crisis, then moved above its pre-crisis level until the pandemic, when it jumped as a result of government transfers and restrictions on consumer spending. It has since stayed below its pre-pandemic level, in part due to consumers <a href="https://libertystreeteconomics.newyorkfed.org/2023/10/spending-down-pandemic-savings-is-an-only-in-the-u-s-phenomenon/">spending down</a> the unusually high amount of saving accumulated in the 2020-21 period. Notice that total saving is more stable than the individual components because of offsetting movements, particularly between household and government saving.</p>







<p>Household and Government Saving Often Offset Each Other</p>


<figure data-type="line">
	<p>Share of GDP (percent)</p>
	
	<figcaption>Source: Bureau of Economic Analysis.</figcaption>
</figure>







<h4><strong>Macro versus Micro</strong></h4>



<p>The saving gap framework helps clarify what trade policies can and can’t do. For example, a free-trade agreement encourages exports, and an industrial policy can foster a re-shoring of production to replace imports. Such policies influence the size and composition of cross-border trade, but the <em>difference</em> between imports and exports is only affected if these policies also change the gap between domestic saving and investment spending.</p>



<p>The chart below illustrates how focusing on imports and exports can be misleading. In 2011, the U.S. trade deficit in petroleum products reached $330 billion. The overall trade deficit, measured by the current account, was $455 billion, so oil accounted for roughly 75&nbsp;percent of the entire deficit. Surely the deficit would shrink if the United States wasn’t dependent on imported oil. As it turned out, a dramatic increase in domestic oil output caused the oil deficit to disappear by 2019. Nevertheless, the overall deficit grew to $441&nbsp;billion, consistent with a wider saving gap.</p>







<p>The Overall Trade Balance Is Not Tied to Specific Items</p>


<figure data-type="line">
	<p>Share of GDP (percent)</p>
	
	<figcaption>Source: Bureau of Economic Analysis.<br>Notes: Oil is petroleum and petroleum products. Total is the current account balance.</figcaption>
</figure>







<h4><strong>Debating Trade Deficits</strong></h4>



<p>An argument against running a trade deficit is that it requires U.S. assets that would otherwise have been held domestically to be sold to foreign investors. As a consequence, income generated by these assets flows out of the country instead of going to domestic investors.</p>



<p>The saving gap perspective tells a contrary story. Investment spending would have been lower if not for the United States being able to borrow from the rest of the world. One can <a href="https://fraser.stlouisfed.org/title/current-issues-economics-finance-6879/viewing-current-account-deficit-a-capital-inflow-627898">argue </a>that this funding raised the economy’s productive capacity from what it would have been otherwise.</p>



<p>Finally, achieving the goal of a smaller trade deficit will likely be painful, since it requires a recalibration of domestic savings and investment. <a href="https://studies/">Studies</a> <a href="https://www.nber.org/system/files/working_papers/w11823/w11823.pdf">have found</a> that episodes of substantial reductions in trade deficits were typically facilitated initially by lower investment spending and subsequently through higher saving, as was the case with the improvement in the U.S. current account during the 2008 recession and its aftermath.</p>



<div><figure><img decoding="async" width="90" height="90" src="https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2022/03/klitgaard_tom.jpg?w=90" alt="Photo: portrait of Thomas Klitgaard" srcset="https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2022/03/klitgaard_tom.jpg 90w, https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2022/03/klitgaard_tom.jpg?resize=45,45 45w" sizes="(max-width: 90px) 100vw, 90px"></figure><p><a href="https://www.newyorkfed.org/research/economists/klitgaard" target="_blank" rel="noreferrer noopener">Thomas Klitgaard</a> is an economic policy advisor in the Federal Reserve Bank of New York’s Research and Statistics Group.</p></div>



<p>
    <strong>How to cite this post:</strong><br>
    Thomas Klitgaard, “Why Does the U.S. Always Run a Trade Deficit?,” Federal Reserve Bank of New York <em>Liberty Street Economics</em>, May 20, 2025, https://libertystreeteconomics.newyorkfed.org/2025/05/why-does-the-u-s-always-run-a-trade-deficit/.</p>







<div>




<hr>



<p><strong>Disclaimer</strong><br>The views expressed in this post are those of the author(s) and do not necessarily reflect the position of the Federal Reserve Bank of New York or the Federal Reserve System. Any errors or omissions are the responsibility of the author(s).</p>
</div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reports of Deno's Demise Have Been Greatly Exaggerated (168 pts)]]></title>
            <link>https://deno.com/blog/greatly-exaggerated</link>
            <guid>44040332</guid>
            <pubDate>Tue, 20 May 2025 11:33:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/greatly-exaggerated">https://deno.com/blog/greatly-exaggerated</a>, See on <a href="https://news.ycombinator.com/item?id=44040332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There’s been some criticism lately about Deno - about Deploy, KV, Fresh, and our
momentum in general. You may have seen some of the criticism online; it’s made
the rounds in the usual places, and attracted a fair amount of attention.</p>
<p>Some of that criticism is valid. In fact, I think it’s fair to say we’ve had a
hand in causing some amount of fear and uncertainty by being too quiet about
what we’re working on, and the future direction of our company and products.
That’s on us.</p>
<p>In other places, recent criticisms have been inaccurate, or speculative. That’s
why we’re writing this post; to set the record straight. This is a catch-up on
where we are, what we’ve learned, and what we’re building next. Some have feared
that Deno itself is diminishing or fading away, but this couldn’t be further
from the truth. Since the release of <a href="https://deno.com/2" rel="noopener noreferrer">Deno 2</a> last October - barely over six
months ago! - Deno adoption has more than doubled according to our monthly
active user metrics. Deno 2’s robust Node compatibility effectively removed a
major adoption barrier, unblocking a wide range of serious use cases. The
platform has gotten faster, simpler, and more capable. Deno is now used more
widely - and more seriously - than ever before.</p>
<h2 id="deno-deploy">Deno Deploy</h2>
<p>One of the biggest questions we’ve been hearing is about Deno Deploy —
specifically, the reduction in available regions. While we understand the optics
of this scaling back, it isn’t for the reasons often feared or accused.</p>
<p>Rather, reality is: most applications don’t need to run everywhere. They need to
be fast, close to their data, easy to debug, and compliant with local
regulations. We are optimizing for that.</p>
<p>We launched Deno Deploy in 2021 to explore a new model for serverless
JavaScript. It started in 25 regions, grew to 35, and now runs in 6. That
reduction was driven by cost, yes - but also by usage. Most developers weren’t
deploying simple stateless functions. They were building full-stack apps: apps
that talk to a database, that almost always is located in a single region. We
saw that most of the time, the excess regions were mostly unused. When traffic
spikes came, the idle regions would hit capacity quickly causing latency spikes.
We found that routing to a further away larger region was often faster than
running in a nearby cold one.</p>
<p>We were chasing a vision of “edge” that didn’t match how people were actually
using the service. We shouldn’t have been silent about this.</p>
<p>Deno Deploy is under heavy development - we haven’t quite released the latest
version yet, but it’s imminent. You can
<a href="https://dash.deno.com/account" rel="noopener noreferrer">request early access here</a>.</p>
<p>Deploy is evolving into a platform for hosting applications - not just
functions. It’ll support subprocesses, background tasks, OpenTelemetry, build
pipelines, caching, and even self-hosted regions. It runs full-stack frameworks
like Next.js, SvelteKit, and of course, Fresh.</p>
<p>Soon, you’ll be able to pin your app to a region - or run it in your own cloud.
That’s something we’ve heard again and again from users who care about control,
compliance, and performance. More coming soon.</p>
<h2 id="kv">KV</h2>
<p><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a> is our zero-setup, globally consistent key-value store with a
simple API and
<a href="https://docs.deno.com/examples/kv_watch/" rel="noopener noreferrer">real-time capabilities</a>. We realize
that for things like session data, feature flags, and collaborative presence, KV
works great. Developers love it for what it is: a zero-config global store that
just works.</p>
<p>But it doesn’t solve everything. It’s not a general-purpose database, and it
doesn’t replace relational systems for most applications. To address these
broader needs for state management, we have multiple efforts in the pipeline:</p>
<ul>
<li><p>Firstly, we’re working on making traditional relational databases more readily
available and simpler to use within Deno Deploy.</p>
</li>
<li><p>And secondly, we believe that Deno KV itself doesn’t go far enough in
simplifying how compute and state are bound. So, inspired by systems like
Cloudflare’s Durable Objects, we are working on a new project to achieve this
deeper integration, aiming to bind state directly to computation.</p>
</li>
</ul>
<p>Given these new directions, Deno KV will remain in beta. We will continue to
address critical bugs and security issues for its current version. While KV is
useful for its intended purpose, its role is not to be the central or evolving
solution for all state management in Deno. We reserve the right to make
significant changes to Deno KV in the future as these other state initiatives
mature and our overall platform strategy evolves. We’re excited to share more on
these new pipeline projects soon.</p>
<h2 id="fresh">Fresh</h2>
<p>Fresh is alive and well - it powers every app and website we build. We know many
of you have been eagerly anticipating Fresh 2, and perhaps some have felt
frustrated by the wait. We hear you. We could have shipped something sooner, but
it was crucial to get the fundamentals right and we didn’t want to compromise on
quality for a quick marketing splash. We depend on Fresh for all our sites, so
its excellence is paramount. We just wrote a detailed
<a href="https://deno.com/blog/an-update-on-fresh" rel="noopener noreferrer">post</a> about the significant improvements coming in
Fresh 2 – go read it! A stable release is coming later this year.</p>
<h2 id="deno-the-runtime-platform">Deno, the runtime platform</h2>
<p>Deno isn’t just a runtime anymore; it’s a complete platform for building and
running JavaScript systems. Here’s what’s built in:</p>
<ul>
<li><a href="https://docs.deno.com/runtime/fundamentals/typescript/" rel="noopener noreferrer">TypeScript</a> and
<a href="https://docs.deno.com/runtime/reference/jsx/" rel="noopener noreferrer">JSX</a> support</li>
<li><a href="https://docs.deno.com/runtime/fundamentals/security/" rel="noopener noreferrer">Granular permissions and sandboxing</a>
for secure execution</li>
<li>A full
<a href="https://docs.deno.com/runtime/reference/lsp_integration/" rel="noopener noreferrer">Language Server Protocol</a>
(LSP), <a href="https://docs.deno.com/runtime/reference/vscode/" rel="noopener noreferrer">VS Code extension</a>,
and <a href="https://docs.deno.com/runtime/reference/cli/check/" rel="noopener noreferrer"><code>deno check</code></a> for
type checking</li>
<li><a href="https://docs.deno.com/runtime/reference/cli/jupyter/" rel="noopener noreferrer">Jupyter notebook integration</a>
with LSP-powered TypeScript type checking</li>
<li><a href="https://docs.deno.com/runtime/reference/cli/compile/" rel="noopener noreferrer"><code>deno compile</code></a> to
generate standalone binaries</li>
<li><a href="https://docs.deno.com/runtime/fundamentals/node/" rel="noopener noreferrer">Strong Node/npm compatibility</a>,
including
<a href="https://docs.deno.com/runtime/fundamentals/workspaces/" rel="noopener noreferrer">workspace support</a></li>
<li>First-class observability via
<a href="https://docs.deno.com/runtime/fundamentals/open_telemetry/" rel="noopener noreferrer">built-in OpenTelemetry</a>,
providing structured tracing out-of-the-box. <strong>This is an essential
infrastructure piece, not an afterthought, as some have derided.</strong></li>
<li><a href="https://docs.deno.com/runtime/reference/cli/fmt/" rel="noopener noreferrer"><code>deno fmt</code></a> for
auto-formatting JavaScript, TypeScript
(<a href="https://deno.com/blog/v2.3#improvements-to-deno-fmt" rel="noopener noreferrer">and even CSS or SQL within template strings</a>)</li>
<li><a href="https://docs.deno.com/runtime/fundamentals/modules/" rel="noopener noreferrer">Fundamentally built on ES Modules</a>
and
<a href="https://docs.deno.com/runtime/reference/web_platform_apis/" rel="noopener noreferrer">web standards</a></li>
<li>A global deploy surface (via <a href="https://deno.com/deploy" rel="noopener noreferrer">Deno Deploy</a>)</li>
<li>A publishing system (<a href="https://jsr.io/" rel="noopener noreferrer">JSR</a>) with open governance,
<a href="https://jsr.io/@std" rel="noopener noreferrer">a growing standard library</a>, and excellent workspace
support</li>
</ul>
<p>You can write, run, test, deploy, and monitor - all with a single toolchain.
We’ve been tightening integration. Fewer flags. Better defaults. Smaller gaps.
The pieces work together better than ever before.</p>
<p>We’re not chasing feature parity with other runtimes. We’re building a cohesive
system. We’re trying to fundamentally improve JavaScript development. If we have
faulted, it’s because we’ve stretched too far in this goal. But I don’t think
anyone can argue that Deno isn’t striving for a better world for the world’s
default programming language.</p>
<h2 id="why-were-doing-this">Why we’re doing this</h2>
<p>Scripting languages are the ergonomic end-state for a large class of problems:
business logic where engineering time is the limiting factor.</p>
<p>Ruby, Python, Lua, Perl, and JavaScript all follow that thread. But JavaScript
is the one distributed on every device, with standards bodies, independent
implementations across tech conglomerates, and a massive vibrant ecosystem. The
scripting language with a future we believe is JavaScript (and TypeScript). It
deserves a platform to match, not a patchwork of ad hoc tools. A single
batteries included system.</p>
<p>Just like JavaScript gives you garbage collection and built-in arrays, Deno
gives you a permissions system, a web server, observability, linting, and type
safety - all out of the box. You don’t need to glue it together. Deno is the
glue.</p>
<h2 id="looking-ahead">Looking ahead</h2>
<p>We’re not winding down. We’re winding up.</p>
<p>We’re continuing to improve performance, compatibility, and polish across the
platform. JSR is maturing. We’ve
<a href="https://deno.com/blog/jsr-open-governance-board" rel="noopener noreferrer">recently established</a> an Open Governance Board
and are actively working to transition JSR into an independent, community-driven
foundation.</p>
<p>Our work in <a href="https://github.com/tc39/proposal-source-phase-imports" rel="noopener noreferrer">TC39</a> and
<a href="https://deno.com/blog/wintertc" rel="noopener noreferrer">WinterTC (formerly WinterCG)</a> continues. We’re
also
<a href="https://javascript.tm/" rel="noopener noreferrer">actively challenging Oracle’s misleading JavaScript
trademark</a>. This is all part of our broad effort to
improve and grow the JavaScript ecosystem.</p>
<p>We’re building new products based on everything we’ve learned from Deploy and
KV, not yet released. They aim to make persistent, distributed applications
simpler. More on that very soon.</p>
<p>We recognize that our silence has sometimes been a source of uncertainty, and
we’re committed to improving our communication as we move forward with these
exciting developments.</p>
<p>Thanks for reading. And to everyone building with Deno: thank you.</p>
<p>– Ryan</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using elliptic curves to solve a math meme (297 pts)]]></title>
            <link>https://artofproblemsolving.com/community/c2532359h2760821_the_emoji_problem__part_i?srsltid=AfmBOor9TbMq_A7hGHSJGfoWaa2HNzducSYZu35d_LFlCSNLXpvt-pdS</link>
            <guid>44039864</guid>
            <pubDate>Tue, 20 May 2025 10:18:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artofproblemsolving.com/community/c2532359h2760821_the_emoji_problem__part_i?srsltid=AfmBOor9TbMq_A7hGHSJGfoWaa2HNzducSYZu35d_LFlCSNLXpvt-pdS">https://artofproblemsolving.com/community/c2532359h2760821_the_emoji_problem__part_i?srsltid=AfmBOor9TbMq_A7hGHSJGfoWaa2HNzducSYZu35d_LFlCSNLXpvt-pdS</a>, See on <a href="https://news.ycombinator.com/item?id=44039864">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-wrapper">
			<div id="left_navigation_box">
				<p><a href="http://www.artofproblemsolving.com/Forum/index.php">Community</a> » <a href="https://artofproblemsolving.com/community/c88">Blogs</a> » <a href="https://artofproblemsolving.com/community/c2532359">Turtle Math</a>
			</p></div>
			<div id="right_navigation_box">
				<p><a href="">Sign In</a> • <a href="">Join AoPS</a>				• <a href="https://artofproblemsolving.com/community/category-admin/2532359">Blog Info</a>
			</p></div>
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A simple search engine from scratch (223 pts)]]></title>
            <link>https://bernsteinbear.com/blog/simple-search/</link>
            <guid>44039744</guid>
            <pubDate>Tue, 20 May 2025 09:58:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/simple-search/">https://bernsteinbear.com/blog/simple-search/</a>, See on <a href="https://news.ycombinator.com/item?id=44039744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>*if you include word2vec.</p>

<p><a href="https://www.chrisgregory.me/">Chris</a> and I spent a couple hours the other day
creating a search engine for my blog from “scratch”. Mostly he walked me
through it because I only vaguely knew what word2vec was before this experiment.</p>

<p>The search engine we made is built on <em>word embeddings</em>. This refers to some
function that takes a word and maps it onto N-dimensional space (in this case,
N=300) where each dimension vaguely corresponds to some axis of meaning.
<a href="https://jaketae.github.io/study/word2vec/">Word2vec from Scratch</a> is a nice
blog post that shows how to train your own mini word2vec and explains the
internals.</p>

<p>The idea behind the search engine is to embed each of my posts into this domain
by adding up the embeddings for the words in the post. For a given
search, we’ll embed the search the same way. Then we can rank all posts by
their <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarities</a>
to the query.</p>

<p>The equation below might look scary but it’s saying that the cosine similarity,
which is the cosine of the angle between the two vectors <code>cos(theta)</code>, is
defined as the dot product divided by the product of the magnitudes of each
vector. We’ll walk through it all in detail.</p>

<figure>
<img src="https://bernsteinbear.com/assets/img/cosine-similarity.svg">
<figcaption>Equation from Wikimedia's <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine similarity</a>
page.</figcaption>
</figure>

<p>Cosine distance is probably the simplest method for comparing a query embedding
to document embeddings to rank documents. Another intuitive choice might be
euclidean distance, which would measure how far apart two vectors are in space
(rather than the angle between them).</p>

<p>We prefer cosine distance because it preserves our intuition that two vectors
have similar meanings if they have the same proportion of each embedding
dimension. If you have two vectors that point in the same direction, but one is
very long and one very short, these should be considered the same meaning. (If
two documents are about cats, but one says the word cat much more, they’re
still just both about cats).</p>

<p>Let’s open up word2vec and embed our first words.</p>

<h2 id="embedding">Embedding</h2>

<p>We take for granted this database of the top 10,000 most popular word
embeddings, which is a 12MB pickle file that vaguely looks like this:</p>

<div><pre><code>couch  [0.23, 0.05, ..., 0.10]
banana [0.01, 0.80, ..., 0.20]
...
</code></pre></div>

<p>Chris sent it to me over the internet. If you unpickle it, it’s actually a
NumPy data structure: a dictionary mapping strings to <code>numpy.float32</code> arrays. I
wrote a script to transform this pickle file into plain old Python floats and
lists because I wanted to do this all by hand.</p>

<p>The loading code is straighforward: use the <code>pickle</code> library. The usual
security caveats apply, but I trust Chris.</p>

<div><pre><code><span>import</span> <span>pickle</span>

<span>def</span> <span>load_data</span><span>(</span><span>path</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>path</span><span>,</span> <span>"rb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>return</span> <span>pickle</span><span>.</span><span>load</span><span>(</span><span>f</span><span>)</span>

<span>word2vec</span> <span>=</span> <span>load_data</span><span>(</span><span>"word2vec.pkl"</span><span>)</span>
</code></pre></div>

<p>You can print out <code>word2vec</code> if you like, but it’s going to be a lot of output.
I learned that the hard way. Maybe print <code>word2vec["cat"]</code> instead. That will
print out the embedding.</p>

<p>To embed a word, we need only look it up in the enormous dictionary. A nonsense
or uncommon word might not be in there, though, so we return <code>None</code> in that
case instead of raising an error.</p>

<div><pre><code><span>def</span> <span>embed_word</span><span>(</span><span>word2vec</span><span>,</span> <span>word</span><span>):</span>
    <span>return</span> <span>word2vec</span><span>.</span><span>get</span><span>(</span><span>word</span><span>)</span>
</code></pre></div>

<p>To embed multiple words, we embed each one individually and then add up the
embeddings pairwise. If a given word is not embeddable, ignore it. It’s only a
problem if we can’t understand any of the words.</p>

<div><pre><code><span>def</span> <span>vec_add</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
    <span>return</span> <span>[</span><span>x</span> <span>+</span> <span>y</span> <span>for</span> <span>x</span><span>,</span> <span>y</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)]</span>

<span>def</span> <span>embed_words</span><span>(</span><span>word2vec</span><span>,</span> <span>words</span><span>):</span>
    <span>result</span> <span>=</span> <span>[</span><span>0.0</span><span>]</span> <span>*</span> <span>len</span><span>(</span><span>next</span><span>(</span><span>iter</span><span>(</span><span>word2vec</span><span>.</span><span>values</span><span>())))</span>
    <span>num_known</span> <span>=</span> <span>0</span>
    <span>for</span> <span>word</span> <span>in</span> <span>words</span><span>:</span>
        <span>embedding</span> <span>=</span> <span>word2vec</span><span>.</span><span>get</span><span>(</span><span>word</span><span>)</span>
        <span>if</span> <span>embedding</span> <span>is</span> <span>not</span> <span>None</span><span>:</span>
            <span>result</span> <span>=</span> <span>vec_add</span><span>(</span><span>result</span><span>,</span> <span>embedding</span><span>)</span>
            <span>num_known</span> <span>+=</span> <span>1</span>
    <span>if</span> <span>not</span> <span>num_known</span><span>:</span>
        <span>raise</span> <span>SyntaxError</span><span>(</span><span>f</span><span>"I can't understand any of </span><span>{</span><span>words</span><span>}</span><span>"</span><span>)</span>
    <span>return</span> <span>result</span>
</code></pre></div>

<p>That’s the basics of embedding: it’s a dictionary lookup and vector adds.</p>

<div><pre><code><span>embed_words</span><span>([</span><span>a</span><span>,</span> <span>b</span><span>])</span> <span>==</span> <span>vec_add</span><span>(</span><span>embed_word</span><span>(</span><span>a</span><span>),</span> <span>embed_word</span><span>(</span><span>b</span><span>))</span>
</code></pre></div>

<p>Now let’s make our “search engine index”, or the embeddings for all of my
posts.</p>

<h2 id="embedding-all-of-the-posts">Embedding all of the posts</h2>

<p>Embedding all of the posts is a recursive directory traversal where we build up
a dictionary mapping path name to embedding.</p>

<div><pre><code><span>import</span> <span>os</span>

<span>def</span> <span>load_post</span><span>(</span><span>pathname</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>pathname</span><span>,</span> <span>"r"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>contents</span> <span>=</span> <span>f</span><span>.</span><span>read</span><span>()</span>
    <span>return</span> <span>normalize_text</span><span>(</span><span>contents</span><span>).</span><span>split</span><span>()</span>

<span>def</span> <span>load_posts</span><span>():</span>
    <span># Walk _posts looking for *.md files
</span>    <span>posts</span> <span>=</span> <span>{}</span>
    <span>for</span> <span>root</span><span>,</span> <span>dirs</span><span>,</span> <span>files</span> <span>in</span> <span>os</span><span>.</span><span>walk</span><span>(</span><span>"_posts"</span><span>):</span>
        <span>for</span> <span>file</span> <span>in</span> <span>files</span><span>:</span>
            <span>if</span> <span>file</span><span>.</span><span>endswith</span><span>(</span><span>".md"</span><span>):</span>
                <span>pathname</span> <span>=</span> <span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>root</span><span>,</span> <span>file</span><span>)</span>
                <span>posts</span><span>[</span><span>pathname</span><span>]</span> <span>=</span> <span>load_post</span><span>(</span><span>pathname</span><span>)</span>
    <span>return</span> <span>posts</span>

<span>post_embeddings</span> <span>=</span> <span>{</span><span>pathname</span><span>:</span> <span>embed_words</span><span>(</span><span>word2vec</span><span>,</span> <span>words</span><span>)</span>
                   <span>for</span> <span>pathname</span><span>,</span> <span>words</span> <span>in</span> <span>posts</span><span>.</span><span>items</span><span>()}</span>

<span>with</span> <span>open</span><span>(</span><span>"post_embeddings.pkl"</span><span>,</span> <span>"wb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>pickle</span><span>.</span><span>dump</span><span>(</span><span>post_embeddings</span><span>,</span> <span>f</span><span>)</span>
</code></pre></div>

<p>We do this other thing, though: <code>normalize_text</code>. This is because blog posts
are messy and contain punctuation, capital letters, and all other kinds of
nonsense. In order to get the best match, we want to put things like “CoMpIlEr”
and “compiler” in the same bucket.</p>

<div><pre><code><span>import</span> <span>re</span>

<span>def</span> <span>normalize_text</span><span>(</span><span>text</span><span>):</span>
    <span>return</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>"[^a-zA-Z]"</span><span>,</span> <span>r</span><span>" "</span><span>,</span> <span>text</span><span>).</span><span>lower</span><span>()</span>
</code></pre></div>

<p>We’ll do the same thing for each query, too. Speaking of, we should test this
out. Let’s make a little search REPL.</p>

<h2 id="a-little-search-repl">A little search REPL</h2>

<p>We’ll start off by using some Python’s built-in REPL creator library, <code>code</code>.
We can make a subclass that defines a <code>runsource</code> method. All it really needs
to do is process the <code>source</code> input and return a falsy value (otherwise it
waits for more input).</p>

<div><pre><code><span>import</span> <span>code</span>

<span>class</span> <span>SearchRepl</span><span>(</span><span>code</span><span>.</span><span>InteractiveConsole</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>word2vec</span><span>,</span> <span>post_embeddings</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>word2vec</span> <span>=</span> <span>word2vec</span>
        <span>self</span><span>.</span><span>post_embeddings</span> <span>=</span> <span>post_embeddings</span>

    <span>def</span> <span>runsource</span><span>(</span><span>self</span><span>,</span> <span>source</span><span>,</span> <span>filename</span><span>=</span><span>"&lt;input&gt;"</span><span>,</span> <span>symbol</span><span>=</span><span>"single"</span><span>):</span>
        <span>for</span> <span>result</span> <span>in</span> <span>self</span><span>.</span><span>search</span><span>(</span><span>source</span><span>):</span>
            <span>print</span><span>(</span><span>result</span><span>)</span>
</code></pre></div>

<p>Then we can define a <code>search</code> function that pulls together our existing
functions. Just like that, we have a search:</p>

<div><pre><code><span>class</span> <span>SearchRepl</span><span>(</span><span>code</span><span>.</span><span>InteractiveConsole</span><span>):</span>
    <span># ...
</span>    <span>def</span> <span>search</span><span>(</span><span>self</span><span>,</span> <span>query_text</span><span>,</span> <span>n</span><span>=</span><span>5</span><span>):</span>
        <span># Embed query
</span>        <span>words</span> <span>=</span> <span>normalize_text</span><span>(</span><span>query_text</span><span>).</span><span>split</span><span>()</span>
        <span>try</span><span>:</span>
            <span>query_embedding</span> <span>=</span> <span>embed_words</span><span>(</span><span>self</span><span>.</span><span>word2vec</span><span>,</span> <span>words</span><span>)</span>
        <span>except</span> <span>SyntaxError</span> <span>as</span> <span>e</span><span>:</span>
            <span>print</span><span>(</span><span>e</span><span>)</span>
            <span>return</span>
        <span># Cosine similarity
</span>        <span>post_ranks</span> <span>=</span> <span>{</span><span>pathname</span><span>:</span> <span>vec_cosine_similarity</span><span>(</span><span>query_embedding</span><span>,</span>
                                                      <span>embedding</span><span>)</span> <span>for</span> <span>pathname</span><span>,</span>
                      <span>embedding</span> <span>in</span> <span>self</span><span>.</span><span>post_embeddings</span><span>.</span><span>items</span><span>()}</span>
        <span>posts_by_rank</span> <span>=</span> <span>sorted</span><span>(</span><span>post_ranks</span><span>.</span><span>items</span><span>(),</span>
                               <span>reverse</span><span>=</span><span>True</span><span>,</span>
                               <span>key</span><span>=</span><span>lambda</span> <span>entry</span><span>:</span> <span>entry</span><span>[</span><span>1</span><span>])</span>
        <span>top_n_posts_by_rank</span> <span>=</span> <span>posts_by_rank</span><span>[:</span><span>n</span><span>]</span>
        <span>return</span> <span>[</span><span>path</span> <span>for</span> <span>path</span><span>,</span> <span>_</span> <span>in</span> <span>top_n_posts_by_rank</span><span>]</span>
</code></pre></div>

<p>Yes, we have to do a cosine similarity. Thankfully, the Wikipedia math snippet
translates almost 1:1 to Python code:</p>

<div><pre><code><span>import</span> <span>math</span>

<span>def</span> <span>vec_norm</span><span>(</span><span>v</span><span>):</span>
    <span>return</span> <span>math</span><span>.</span><span>sqrt</span><span>(</span><span>sum</span><span>([</span><span>x</span><span>*</span><span>x</span> <span>for</span> <span>x</span> <span>in</span> <span>v</span><span>]))</span>

<span>def</span> <span>vec_cosine_similarity</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>):</span>
    <span>assert</span> <span>len</span><span>(</span><span>a</span><span>)</span> <span>==</span> <span>len</span><span>(</span><span>b</span><span>)</span>
    <span>a_norm</span> <span>=</span> <span>vec_norm</span><span>(</span><span>a</span><span>)</span>
    <span>b_norm</span> <span>=</span> <span>vec_norm</span><span>(</span><span>b</span><span>)</span>
    <span>dot_product</span> <span>=</span> <span>sum</span><span>([</span><span>ax</span><span>*</span><span>bx</span> <span>for</span> <span>ax</span><span>,</span> <span>bx</span> <span>in</span> <span>zip</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)])</span>
    <span>return</span> <span>dot_product</span><span>/</span><span>(</span><span>a_norm</span><span>*</span><span>b_norm</span><span>)</span>
</code></pre></div>

<p>Finally, we can create and run the REPL.</p>

<div><pre><code><span>sys</span><span>.</span><span>ps1</span> <span>=</span> <span>"QUERY. "</span>
<span>sys</span><span>.</span><span>ps2</span> <span>=</span> <span>"...... "</span>

<span>repl</span> <span>=</span> <span>SearchRepl</span><span>(</span><span>word2vec</span><span>,</span> <span>post_embeddings</span><span>)</span>
<span>repl</span><span>.</span><span>interact</span><span>(</span><span>banner</span><span>=</span><span>""</span><span>,</span> <span>exitmsg</span><span>=</span><span>""</span><span>)</span>
</code></pre></div>

<p>This is what interacting with it looks like:</p>

<div><pre><code><span>QUERY.</span><span> </span><span>type </span>inference
<span>_posts/2024-10-15-type-inference.md
_posts/2025-03-10-lattice-bitset.md
_posts/2025-02-24-sctp.md
_posts/2022-11-07-inline-caches-in-skybison.md
_posts/2021-01-14-inline-caching.md
</span><span>QUERY.</span><span>
</span></code></pre></div>

<p>This is a sample query from a very small dataset (my blog). It’s a pretty good
search result, but it’s probably not representative of the overall search
quality. Chris says that I should cherry-pick “because everyone in AI does”.</p>

<p>Okay, that’s really neat. But most people who want to look for something on
my website do not run for their terminals. Though my site is expressly designed
to work well in terminal browsers such as Lynx, most people are already in a
graphical web browser. So let’s make a search front-end.</p>

<h2 id="a-little-web-search">A little web search</h2>

<p>So far we’ve been running from my local machine where I don’t mind having a
12MB file of weights sitting around. Now that we’re moving to web, I would
rather not burden casual browsers with an unexpected big download. So we need
to get clever.</p>

<p>Fortunately, Chris and I had both seen <a href="https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/">this really cool blog post</a>
that talks about hosting a SQLite database on GitHub Pages. The blog post
details how the author:</p>

<ul>
  <li>compiled SQLite to Wasm so it could run on the client,</li>
  <li>built a virtual filesystem so it could read database files from the web,</li>
  <li>did some smart page fetching using the existing SQLite indexes,</li>
  <li>built additional software to fetch only small chunks of the database using
HTTP Range requests</li>
</ul>

<p>That’s super cool, but again: SQLite, though small, is comparatively big for
this project. We want to build things from scratch. Fortunately, we can emulate
the main ideas.</p>

<p>We can give the word2vec dict a stable order and split it into two files. One
file can just have the embeddings, no names. Another file, the index, can map
every word to the byte start and byte length of the weights for that word (we
figure start&amp;length is probably smaller on the wire than start&amp;end).</p>

<div><pre><code><span>#</span><span> </span><span>vecs.jsonl</span><span>
</span><span>[</span><span>0.23</span><span>,</span><span> </span><span>0.05</span><span>,</span><span> </span><span>...</span><span>,</span><span> </span><span>0.10</span><span>]</span><span>
</span><span>[</span><span>0.01</span><span>,</span><span> </span><span>0.80</span><span>,</span><span> </span><span>...</span><span>,</span><span> </span><span>0.20</span><span>]</span><span>
</span><span>...</span><span>
</span></code></pre></div>

<div><pre><code><span>#</span><span> </span><span>index.json</span><span>
</span><span>{</span><span>"couch"</span><span>:</span><span> </span><span>[</span><span>0</span><span>,</span><span> </span><span>20</span><span>],</span><span> </span><span>"banana"</span><span>:</span><span> </span><span>[</span><span>20</span><span>,</span><span> </span><span>30</span><span>],</span><span> </span><span>...</span><span>}</span><span>
</span></code></pre></div>

<p>The cool thing about this is that <code>index.json</code> is <em>dramatically</em> smaller than
the word2vec blob, weighing in at 244KB. Since that won’t change very often
(how often does word2vec change?), I don’t feel so bad about users eagerly
downloading the entire index.  Similarly, the <code>post_embeddings.json</code> is only
388KB. They’re even cacheable. And automagically (de)compressed by the server
and browser (to 84KB and 140KB, respectively). Both would be smaller if we
chose a binary format, but we’re punting on that for the purposes of this post.</p>

<p>Then we can make HTTP Range requests to the server and only download the parts
of the weights that we need. It’s even possible to bundle all of the ranges
into one request (it’s called multipart range). Unfortunately, GitHub Pages
does not appear to support multipart, so instead we download each word’s range
in a separate request.</p>

<p>Here’s the pertinent JS code, with (short, very familiar) vector functions
omitted:</p>

<div><pre><code><span>(</span><span>async</span> <span>function</span><span>()</span> <span>{</span>
  <span>// Download stuff</span>
  <span>async</span> <span>function</span> <span>get_index</span><span>()</span> <span>{</span>
    <span>const</span> <span>req</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>"</span><span>index.json</span><span>"</span><span>);</span>
    <span>return</span> <span>req</span><span>.</span><span>json</span><span>();</span>
  <span>}</span>
  <span>async</span> <span>function</span> <span>get_post_embeddings</span><span>()</span> <span>{</span>
    <span>const</span> <span>req</span> <span>=</span> <span>await</span> <span>fetch</span><span>(</span><span>"</span><span>post_embeddings.json</span><span>"</span><span>);</span>
    <span>return</span> <span>req</span><span>.</span><span>json</span><span>();</span>
  <span>}</span>
  <span>const</span> <span>index</span> <span>=</span> <span>new</span> <span>Map</span><span>(</span><span>Object</span><span>.</span><span>entries</span><span>(</span><span>await</span> <span>get_index</span><span>()));</span>
  <span>const</span> <span>post_embeddings</span> <span>=</span> <span>new</span> <span>Map</span><span>(</span><span>Object</span><span>.</span><span>entries</span><span>(</span><span>await</span> <span>get_post_embeddings</span><span>()));</span>
  <span>// Add search handler</span>
  <span>search</span><span>.</span><span>addEventListener</span><span>(</span><span>"</span><span>input</span><span>"</span><span>,</span> <span>debounce</span><span>(</span><span>async</span> <span>function</span><span>(</span><span>value</span><span>)</span> <span>{</span>
    <span>const</span> <span>query</span> <span>=</span> <span>search</span><span>.</span><span>value</span><span>;</span>
    <span>// TODO(max): Normalize query</span>
    <span>const</span> <span>words</span> <span>=</span> <span>query</span><span>.</span><span>split</span><span>(</span><span>/</span><span>\s</span><span>+/</span><span>);</span>
    <span>if</span> <span>(</span><span>words</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
      <span>// No words</span>
      <span>return</span><span>;</span>
    <span>}</span>
    <span>const</span> <span>requests</span> <span>=</span> <span>words</span><span>.</span><span>reduce</span><span>((</span><span>acc</span><span>,</span> <span>word</span><span>)</span> <span>=&gt;</span> <span>{</span>
      <span>const</span> <span>entry</span> <span>=</span> <span>index</span><span>.</span><span>get</span><span>(</span><span>word</span><span>);</span>
      <span>if</span> <span>(</span><span>entry</span> <span>===</span> <span>undefined</span><span>)</span> <span>{</span>
        <span>// Word is not valid; skip it</span>
        <span>return</span> <span>acc</span><span>;</span>
      <span>}</span>
      <span>const</span> <span>[</span><span>start</span><span>,</span> <span>length</span><span>]</span> <span>=</span> <span>entry</span><span>;</span>
      <span>const</span> <span>end</span> <span>=</span> <span>start</span><span>+</span><span>length</span><span>-</span><span>1</span><span>;</span>
      <span>acc</span><span>.</span><span>push</span><span>(</span><span>fetch</span><span>(</span><span>"</span><span>vecs.jsonl</span><span>"</span><span>,</span> <span>{</span>
        <span>headers</span><span>:</span> <span>new</span> <span>Headers</span><span>({</span>
          <span>"</span><span>Range</span><span>"</span><span>:</span> <span>`bytes=</span><span>${</span><span>start</span><span>}</span><span>-</span><span>${</span><span>end</span><span>}</span><span>`</span><span>,</span>
        <span>}),</span>
      <span>}));</span>
      <span>return</span> <span>acc</span><span>;</span>
    <span>},</span> <span>[]);</span>
    <span>if</span> <span>(</span><span>requests</span><span>.</span><span>length</span> <span>===</span> <span>0</span><span>)</span> <span>{</span>
      <span>// None are valid words :(</span>
      <span>search_results</span><span>.</span><span>innerHTML</span> <span>=</span> <span>"</span><span>No results :(</span><span>"</span><span>;</span>
      <span>return</span><span>;</span>
    <span>}</span>
    <span>const</span> <span>responses</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>requests</span><span>);</span>
    <span>const</span> <span>embeddings</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>responses</span><span>.</span><span>map</span><span>(</span><span>r</span> <span>=&gt;</span> <span>r</span><span>.</span><span>json</span><span>()));</span>
    <span>const</span> <span>query_embedding</span> <span>=</span> <span>embeddings</span><span>.</span><span>reduce</span><span>((</span><span>acc</span><span>,</span> <span>e</span><span>)</span> <span>=&gt;</span> <span>vec_add</span><span>(</span><span>acc</span><span>,</span> <span>e</span><span>));</span>
    <span>const</span> <span>post_ranks</span> <span>=</span> <span>{};</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span><span>path</span><span>,</span> <span>embedding</span><span>]</span> <span>of</span> <span>post_embeddings</span><span>)</span> <span>{</span>
      <span>post_ranks</span><span>[</span><span>path</span><span>]</span> <span>=</span> <span>vec_cosine_similarity</span><span>(</span><span>embedding</span><span>,</span> <span>query_embedding</span><span>);</span>
    <span>}</span>
    <span>const</span> <span>sorted_ranks</span> <span>=</span> <span>Object</span><span>.</span><span>entries</span><span>(</span><span>post_ranks</span><span>).</span><span>sort</span><span>(</span><span>function</span><span>(</span><span>a</span><span>,</span> <span>b</span><span>)</span> <span>{</span>
      <span>// Decreasing</span>
      <span>return</span> <span>b</span><span>[</span><span>1</span><span>]</span><span>-</span><span>a</span><span>[</span><span>1</span><span>];</span>
    <span>});</span>
    <span>// Fun fact: HTML elements with an `id` attribute are accessible as JS</span>
    <span>// globals by that same name.</span>
    <span>search_results</span><span>.</span><span>innerHTML</span> <span>=</span> <span>""</span><span>;</span>
    <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>5</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
      <span>search_results</span><span>.</span><span>innerHTML</span> <span>+=</span> <span>`&lt;li&gt;</span><span>${</span><span>sorted_ranks</span><span>[</span><span>i</span><span>][</span><span>0</span><span>]}</span><span>&lt;/li&gt;`</span><span>;</span>
    <span>}</span>
  <span>}));</span>
<span>})();</span>
</code></pre></div>

<p>You can take a look at the live <a href="https://bernsteinbear.com/websearch/">search
page</a>. In particular, open up the network
requests tab of your browser’s console. Marvel as it only downloads a couple
4KB chunks of embeddings.</p>

<p>So how well does our search technology work? Let’s try to build an
objective-ish evaluation.</p>

<h2 id="evaluation">Evaluation</h2>
<p>We’ll design a metric that roughly tells us when our search engine is better or worse than a naive approach without word embeddings.</p>

<p>We start by collecting an evaluation dataset of <code>(document, query)</code> pairs. Right from the start we’re going to bias this evaluation by collecting this dataset ourselves, but hopefully it’ll still help us get an intuition about the quality of the search. A query in this case is just a few search terms that we think should retrieve a document successfully.</p>

<div><pre><code><span>sample_documents</span> <span>=</span> <span>{</span>
  <span>"_posts/2024-10-27-on-the-universal-relation.md"</span><span>:</span> <span>"database relation universal tuple function"</span><span>,</span>
  <span>"_posts/2024-08-25-precedence-printing.md"</span><span>:</span> <span>"operator precedence pretty print parenthesis"</span><span>,</span>
  <span>"_posts/2019-03-11-understanding-the-100-prisoners-problem.md"</span><span>:</span> <span>"probability strategy game visualization simulation"</span><span>,</span>
  <span># ...
</span><span>}</span>
</code></pre></div>

<p>Now that we’ve collected our dataset, let’s implement a top-k accuracy metric. This metric measures the percentage of the time a document appears in the top k search results given its corresponding query.</p>

<div><pre><code><span>def</span> <span>compute_top_k_accuracy</span><span>(</span>
    <span># Mapping of post to sample search query (already normalized)
</span>    <span># See sample_documents above
</span>    <span>eval_set</span><span>:</span> <span>dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>],</span>
    <span>max_n_keywords</span><span>:</span> <span>int</span><span>,</span>
    <span>max_top_k</span><span>:</span> <span>int</span><span>,</span>
    <span>n_query_samples</span><span>:</span> <span>int</span><span>,</span>
<span>)</span> <span>-&gt;</span> <span>list</span><span>[</span><span>list</span><span>[</span><span>float</span><span>]]:</span>
    <span>counts</span> <span>=</span> <span>[[</span><span>0</span><span>]</span> <span>*</span> <span>max_top_k</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>)]</span>
    <span>for</span> <span>n_keywords</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>max_n_keywords</span> <span>+</span> <span>1</span><span>):</span>
        <span>for</span> <span>post_id</span><span>,</span> <span>keywords_str</span> <span>in</span> <span>eval_set</span><span>.</span><span>items</span><span>():</span>
            <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>n_query_samples</span><span>):</span>
                <span># Construct a search query by sampling keywords
</span>                <span>keywords</span> <span>=</span> <span>keywords_str</span><span>.</span><span>split</span><span>(</span><span>" "</span><span>)</span>
                <span>sampled_keywords</span> <span>=</span> <span>random</span><span>.</span><span>choices</span><span>(</span><span>keywords</span><span>,</span> <span>k</span><span>=</span><span>n_keywords</span><span>)</span>
                <span>query</span> <span>=</span> <span>" "</span><span>.</span><span>join</span><span>(</span><span>sampled_keywords</span><span>)</span>

                <span># Determine the rank of the target post in the search results
</span>                <span>ids</span> <span>=</span> <span>search</span><span>(</span><span>query</span><span>,</span> <span>n</span><span>=</span><span>max_top_k</span><span>)</span>
                <span>rank</span> <span>=</span> <span>safe_index</span><span>(</span><span>ids</span><span>,</span> <span>post_id</span><span>)</span>

                <span># Increment the count of the rank
</span>                <span>if</span> <span>rank</span> <span>is</span> <span>not</span> <span>None</span> <span>and</span> <span>rank</span> <span>&lt;</span> <span>max_top_k</span><span>:</span>
                    <span>counts</span><span>[</span><span>n_keywords</span> <span>-</span> <span>1</span><span>][</span><span>rank</span><span>]</span> <span>+=</span> <span>1</span>

    <span>accuracies</span> <span>=</span> <span>[[</span><span>0.0</span><span>]</span> <span>*</span> <span>max_top_k</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>)]</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>max_n_keywords</span><span>):</span>
        <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>max_top_k</span><span>):</span>
            <span># Divide by the number of samples to get the average across samples and
</span>            <span># divide by the size of the eval set to get accuracy over all posts.
</span>            <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>=</span> <span>counts</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>/</span> <span>n_query_samples</span> <span>/</span> <span>len</span><span>(</span><span>eval_set</span><span>)</span>

            <span># Accumulate accuracies because if a post is retrieved at rank i,
</span>            <span># it was also successfully retrieved at all ranks j &gt; i.
</span>            <span>if</span> <span>j</span> <span>&gt;</span> <span>0</span><span>:</span>
                <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>+=</span> <span>accuracies</span><span>[</span><span>i</span><span>][</span><span>j</span> <span>-</span> <span>1</span><span>]</span>

    <span>return</span> <span>accuracies</span>
</code></pre></div>

<p>Let’s start by evaluating a baseline search engine. This implementation doesn’t use word embeddings at all. We just normalize the text, and count the number of times each query word occur in the document, then rank the documents by number of query word occurrences. Plotting top-k accuracy for various values of k gives us the following chart. Note that we get higher accuracy as we increase k – in the limit, as k approaches our number of documents we approach 100% accuracy.</p>

<p>You also might notice that the accuracy increases as we increase the number of keywords. We can see also the lines getting closer together as the number of keywords increases, which indicates there are diminishing marginal returns for each new keyword.</p>

<figure>
  <img src="https://bernsteinbear.com/assets/img/search-top-k.png">
</figure>

<p>Do these megabytes of word embeddings actually do anything to improve our search? We would have to compare to a baseline. Maybe that baseline is adding up the counts of all keywords in each document to rank them. We leave this as an exercise to the reader because we ran out of time :)</p>

<p>It would also be interesting to see how a bigger word2vec helps accuracy. While
sampling for top-k, there is a lot of error output (<code>I can't understand any of
['prank', ...]</code>). These unknown words get dropped from the search. A bigger
word2vec (more than 10,000 words) might contain these less-common words and
therefore search better.</p>

<h2 id="wrapping-up">Wrapping up</h2>

<p>You can build a small search engine from “scratch” with only a hundred or so
lines of code. See <a href="https://github.com/tekknolagi/tekknolagi.github.com/blob/25d0f5bbe04db7a907409dd5a48648dc8bbd3307/search.py">the full
search.py</a>,
which includes some of the extras for evaluation and plotting.</p>

<h2 id="future-ideas">Future ideas</h2>

<p>We can get fancier than simple cosine similarity. Let’s imagine that all of our
documents talk about computers, but only one of them talks about compilers
(wouldn’t that be sad). If one of our search terms is “computer” that doesn’t
really help narrow down the search and is noise in our embeddings. To reduce
noise we can employ a technique called <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> (term frequency inverse
document frequency) where we factor out common words across documents and pay
closer attention to words that are more unique to each document.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The behavior of LLMs in hiring decisions: Systemic biases in candidate selection (181 pts)]]></title>
            <link>https://davidrozado.substack.com/p/the-strange-behavior-of-llms-in-hiring</link>
            <guid>44039563</guid>
            <pubDate>Tue, 20 May 2025 09:27:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidrozado.substack.com/p/the-strange-behavior-of-llms-in-hiring">https://davidrozado.substack.com/p/the-strange-behavior-of-llms-in-hiring</a>, See on <a href="https://news.ycombinator.com/item?id=44039563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Previous studies have explored gender and ethnic biases in hiring by submitting résumés/CVs to real job postings or mock selection panels, systematically varying the gender or ethnicity signaled by applicants. This approach enables researchers to isolate the effects of demographic characteristics on hiring or preselection decisions.</p><p><span>Building on this methodology, the present </span><a href="https://www.researchgate.net/publication/391874765_Gender_and_Positional_Biases_in_LLM-Based_Hiring_Decisions_Evidence_from_Comparative_CVResume_Evaluations" rel="">analysis</a><span> evaluates whether Large Language Models (LLMs) exhibit algorithmic gender bias when tasked with selecting the most qualified candidate for a given job description.</span></p><p><span>In an experiment involving 22 leading LLMs and 70 popular professions, each model was systematically given a job description along with a pair of profession-matched CVs (one including a male first name, and the other a female first name) and asked to select the more suitable candidate for the job. </span><strong>Each CV pair was presented twice, with names swapped to ensure that any observed preferences in candidate selection stemmed from gendered names cues. </strong><span>The total number of model decisions measured was 30,800 (22 models × 70 professions × 10 different job descriptions per profession × 2 presentations per CV pair). The following figure illustrates the essence of the experiment.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png" width="1456" height="684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/deb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:282998,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdeb9fd81-a752-4d8f-a28b-67051d50a2bd_3800x1784.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates when selecting the most qualified candidate for the job. Female candidates were selected in 56.9% of cases, compared to 43.1% for male candidates (two-proportion z-test = 33.99, p &lt; 10⁻</span><sup>252</sup><span> ). The observed effect size was small to medium (Cohen’s </span><em>h</em><span> = 0.28; odds=1.32, 95% CI [1.29, 1.35]). In the figures below, asterisks (*) indicate statistically significant results (p &lt; 0.05) from two-proportion z-tests conducted on each individual model, with significance levels adjusted for multiple comparisons using the Benjamin-Hochberg False Discovery Rate correction.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png" width="1456" height="1331" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1331,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:373588,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6eed65de-2222-4f78-ad1c-b2fa6816a17c_3388x3096.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Given that the CV pairs were perfectly balanced by gender by presenting them twice with reversed gendered names, an unbiased model would be expected to select male and female candidates at equal rates. The consistent deviation from this expectation across all models tested indicates LLMs gender bias in favor of female candidates.</p><p>LLMs preferences for female candidates was consistent across the 70 professions tested.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png" width="1456" height="2047" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11c72431-1aee-491d-80e5-407abc716895_2968x4172.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2047,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:642171,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11c72431-1aee-491d-80e5-407abc716895_2968x4172.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>A Pearson correlation test between estimated model size (large/small) and the magnitude of models’ gender preferences in candidate selection revealed no statistically significant relationship. In other words, </span><strong>larger models do not appear to be inherently less biased than smaller ones</strong><span>. Reasoning models—such as o1-mini, o3-mini, gemini-2.0-flash-thinking, and DeepSeek-R1—which allocate more compute during inference, also showed no measurable association with gender bias according to a similar analysis.</span></p><p>In an additional experiment, adding an explicit gender field to each CV (i.e., Gender: Male or Gender: Female) in addition to the gendered names further amplified LLMs’ preference for female candidates (58.9% female candidates selections vs 41.1% male candidates, proportion z-test = 43.95, p ≈ 0; Cohen’s h = 0.36; odds=1.43, 95% CI [1.40, 1.46]).</p><p><span>In a follow-up experiment, candidate genders were masked by replacing all gendered names with generic labels (“Candidate A” for males and “Candidate B” for females). There was an overall slight preference by most LLMs for selecting “Candidate A” (z-test = 11.61, p&lt;10</span><sup>-30</sup><span>; Cohen’s h = 0.09; odds=1.10, 95% CI [1.07, 1.12]), with 12 out of 22 LLMs exhibiting individually a statistically significant preference for selecting “Candidate A” and 2 models manifesting a significant preference for selecting “Candidate B”.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png" width="1456" height="1328" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1328,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:408985,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cd2352-4edc-441f-b48c-8db51ad95c5c_3388x3091.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png" width="1456" height="2193" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2193,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:654942,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e3eb53e-f9fd-4d2d-8e5c-a8cb2ede65db_2968x4471.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>When gender was counterbalanced across these generic identifiers (i.e., alternating male and female assignments to “Candidate A” and “Candidate B” labels), </span><strong>gender parity was achieved in candidate selections across models</strong><span>. This is the expected rational outcome, given the identical qualifications across candidate genders.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png" width="1456" height="1329" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1329,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:406565,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67274709-5a4f-4499-a6cd-06e4c3c32eb3_3387x3091.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To also investigate whether LLMs exhibit gender bias when evaluating CVs in isolation—absent direct comparisons between CV pairs—another experiment asked models to assign numerical merit ratings (on a scale from 1 to 10) to each individual CV used in Experiment 1. Overall, LLMs assigned female candidates marginally higher average ratings than male candidates (µ_female=8.65, µ_male=8.61) a difference that was statistically significant (paired t-test = 16.14, p &lt; 10⁻</span><sup>57</sup><span>), </span><strong>but as shown in the figure below the effect size was negligible</strong><span> (Cohen’s d = 0.09). Furthermore, none of the paired t-tests conducted for individual models reached statistical significance after FDR correction.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png" width="1456" height="1462" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1462,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:346080,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6aeb4da2-a3da-4585-a093-3a9dfebf8ea3_2968x2980.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In a further experiment, it was noted that </span><strong>the inclusion of gender concordant preferred pronouns (e.g., he/him, she/her) next to candidates’ names increased the likelihood of the models selecting that candidate, both for males and females, although females were still preferred overall</strong><span>. Candidates with listed pronouns were chosen 53.0% of the time, compared to 47.0% for those without (proportion z-test = 14.75, p &lt; 10⁻</span><sup>48</sup><span>; Cohen’s h = 0.12; odds=1.13, 95% CI [1.10, 1.15]). Out of 22 LLMs, 17 reached individually statistically significant preferences (FDR corrected) for selecting the candidates with preferred pronouns appended to their names.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png" width="1456" height="1327" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1327,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:398614,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69be84a5-339f-4318-99ef-b5d02d7447c7_3387x3086.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Another way of visualizing the results of this experiment:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png" width="1456" height="1514" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1514,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:321213,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1d64c6e-591d-46ba-aff3-95ec27b1031d_2952x3070.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Follow-up analysis of the first experimental results revealed a </span><strong>marked positional bias with LLMs tending to prefer the candidate appearing first in the prompt</strong><span>: 63.5% selection of first candidate vs 36.5% selections of second candidate (z-test = 67.01, p≈0; Cohen’s h = 0.55; odds=1.74, 95% CI [1.70, 1.78]). Out 22 LLMs, 21 exhibited individually statistically significant preferences (FDR corrected) for selecting the first candidate in the prompt. The reasoning model gemini-2.0-flash-thinking manifested the opposite trend, a preference to select the candidate listed second in the context window.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png" width="1456" height="1464" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1464,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:376647,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3befa6c7-43e9-49a4-9696-4b5dc43b96e0_3088x3104.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Another way of visualizing the results of this analysis:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png" width="1456" height="1493" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1493,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:331314,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://davidrozado.substack.com/i/160737820?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5eb7b8dd-15b9-459d-a5c2-30938591e149_3015x3092.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The results presented above indicate that frontier LLMs, when asked to select the most qualified candidate based on a job description and two profession-matched resumes/CVs (one from a male candidate and one from a female candidate), exhibit behavior that diverges from standard notions of fairness. </span><strong>In this context, LLMs do not appear to act rationally. Instead, they generate articulate responses that may superficially seem logically sound but ultimately lack grounding in principled reasoning.</strong><span> Whether this behavior arises from pretraining data, post-training or other unknown factors remains uncertain, underscoring the need for further investigation. But the consistent presence of such biases across all models tested raises broader concerns: </span><strong>In the race to develop ever-more capable AI systems, subtle yet consequential misalignments may go unnoticed prior to LLM deployment.</strong></p><p><span>Several companies are already leveraging LLMs to screen CVs in hiring processes, sometimes even promoting their systems as offering “bias-free insights” (see </span><a href="https://www.ciivsoft.com/" rel="">here</a><span>, </span><a href="https://ubidy.com/news/validating-skills-beyond-the-resume/" rel="">here</a><span>, or </span><a href="https://zoolatech.com/boosting-efficiency-of-recruitment-processes-by-using-llm-and-genai.html" rel="">here</a><span>). In light of the present findings, such claims appear questionable. </span><strong>The results presented here also call into question whether current AI technology is mature enough to be suitable for job selection or other high stakes automated decision-making tasks.</strong></p><p><span>As LLMs are deployed and integrated into autonomous decision-making processes, addressing misalignment is an ethical imperative. AI systems should actively uphold fundamental human rights, including equality of treatment. </span><strong>Yet comprehensive model scrutiny prior to release and resisting premature organizational adoption remain challenging, given the strong economic incentives and potential hype driving the field.</strong></p><p>Below I provide some model-specific results on gender and positional biases in job candidate selections tasks for the following models: GPT-4o, o3-mini, Grok-2, Claude-3-5-sonnet, DeepSeek-R1, Meta-Llama-3.1-405B and Gemini-2.0-flash:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finland announces migration of its rail network to international gauge (418 pts)]]></title>
            <link>https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/</link>
            <guid>44038835</guid>
            <pubDate>Tue, 20 May 2025 07:40:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/">https://www.trenvista.net/en/news/rnhs/finland-migration-standard-gauge/</a>, See on <a href="https://news.ycombinator.com/item?id=44038835">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<main id="main">
			
<article id="post-90722" itemtype="https://schema.org/CreativeWork" itemscope="">
	<div itemprop="text">
			
<p>The Finnish government has announced the conversion of its rail network from Russian gauge (1,524 mm) to European standard (1,435 mm). This historic decision reinforces its integration with the European Union and NATO.</p><p>The change, presented by Transport Minister Lulu Ranne at a meeting of Nordic ministers in Helsinki, is in response to the need to improve military mobility and regional security, especially in the wake of Finland’s NATO membership and growing tensions with Russia.</p><p>The project, which will start in the north of the country near Oulu, aims to remove technical obstacles to transporting troops and goods between Finland, Sweden and Norway.</p><p>The government is expected to make the final decision by July 2027, with construction starting around 2032. European funding will be key: the EU could cover up to 50% of planning costs and 30% of construction<a href="https://english.news.cn/europe/20250514/23904bc05bd6438b8c54979b8f3b1520/c.html" target="_blank" rel="noreferrer noopener"></a><a href="http://www.china.org.cn/world/Off_the_Wire/2025-05/14/content_117873438.shtml" target="_blank" rel="noreferrer noopener"></a>.</p><p>The transition, which will cost billions of euros, affect more than 9,200 km of track, and take decades, symbolises a geopolitical and strategic shift for Finland, which, with this decision, fully aligns itself with European infrastructure.</p><div data-id="46103"><p><a href="https://t.me/trenvista" target="_blank" rel="noopener"><img alt="Trenvista en Telegram" src="https://www.trenvista.net/wp-content/uploads/2022/03/Trenvista-Telegram.png" width="500" height="250" data-old-src="data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-src="https://www.trenvista.net/wp-content/uploads/2022/03/Trenvista-Telegram.png"></a></p>
<p><a href="https://t.me/railnewsvista" target="_blank" rel="noopener"><img alt="Trenvista en Telegram" src="https://www.trenvista.net/wp-content/uploads/2025/01/Anuncio-Telegram-eng.png" width="500" height="250" data-old-src="data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-src="https://www.trenvista.net/wp-content/uploads/2025/01/Anuncio-Telegram-eng.png"></a></p></div>
		</div>
</article>

			

					</main>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making Video Games (Without an Engine) in 2025 (484 pts)]]></title>
            <link>https://noelberry.ca/posts/making_games_in_2025/</link>
            <guid>44038209</guid>
            <pubDate>Tue, 20 May 2025 05:54:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noelberry.ca/posts/making_games_in_2025/">https://noelberry.ca/posts/making_games_in_2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44038209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>It's 2025 and I am still making video games, which <a href="https://web.archive.org/web/20110902045531/http://noelberry.ca/">according to archive.org</a> is 20 years since I started making games! That's a pretty long time to be doing one thing...</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/2005.jpeg" alt="Screenshot of my website circa 2011">
<em></em></p><center><em>Screenshot of my website circa 2011</em></center>
<p>When I share stuff I'm working on, people frequently ask how I make games and are often surprised (and sometimes concerned?) when I tell them I don't use commercial game engines. There's an assumption around making games without a big tool like Unity or Unreal that you're out there hand writing your own assembly instruction by instruction.</p>
<p>I genuinely believe making games without a big "do everything" engine can be easier, more fun, and often less overhead. I am not making a "do everything" game and I do not need 90% of the features these engines provide. I am very particular about how my games feel and look, and how I interact with my tools. I often find the default feature implementations in large engines like Unity so lacking I end up writing my own anyway. Eventually, my projects end up being mostly my own tools and systems, and the engine becomes just a vehicle for a nice UI and some rendering...</p>
<p>At which point, why am I using this engine? What is it providing me? Why am I letting a tool potentially destroy my ability to work when they suddenly make <a href="https://www.theverge.com/2023/9/12/23870547/unit-price-change-game-development">unethical and terrible business decisions</a>? Or push out an update that they require to run my game on consoles, that also happens to break an entire system in my game, forcing me to rewrite it? Why am I fighting this thing daily for what essentially becomes a glorified asset loader and editor UI framework, by the time I'm done working around their default systems?</p>
<p>The obvious answer for me is to just not use big game engines, and write my own small tools for my specific use cases. It's more fun, and I like controlling my development stack. I know when something goes wrong I can find the problem and address it, instead of submitting a bug report and 3 months later hearing back it "won't be fixed". I like knowing that in another two decades from now I will still be able to compile my game without needing to pirate an ancient version of a broken game engine.</p>
<p>Obviously this is my personal preference - and it's one of someone who has been making indie games for a long time. I used engines like Game Maker for years before transitioning to more lightweight and custom workflows. I also work in very small teams, where it's easy to make one-off tools for team members. But I want to push back that making games "from scratch" is some big impossible task - especially in 2025 with the state of open source frameworks and libraries. A <a href="https://github.com/TerryCavanagh/VVVVVV">lot of</a> <a href="https://gamefromscratch.com/balatro-made-with-love-love2d-that-is/">popular</a> <a href="https://store.steampowered.com/app/813230/ANIMAL_WELL/">indie</a> <a href="https://www.stardewvalley.net/">games</a> <a href="https://store.steampowered.com/app/504230/Celeste/">are made</a> <a href="https://github.com/flibitijibibo/RogueLegacy1">in small</a> frameworks like FNA, Love2D, or SDL. Making games "without an engine" doesn't literally mean opening a plain text editor and writing system calls (unless you want to). Often, the overhead of learning how to implement these systems yourself is just as time consuming as learning the proprietary workflows of the engine itself.</p>
<p>With that all said, I think it'd be fun to talk about my workflow, and what I actually use to make games.</p>
<h2>Programming Languages</h2>
<p>Most of my career I've worked in C#, and aside from a <a href="https://github.com/noelfb/blah">short stint in C++</a> a few years ago, I've settled back into a modern C# workflow.</p>
<p>I think sometimes when I mention C# to non-indie game devs their minds jump to what it looked like circa 2003 - a closed source, interpreted, verbose, garbage collected language, and... the language has <em>greatly</em> improved since then. The C# of 2025 is vastly different from the C# of even 2015, and many of those changes are geared towards the performance and syntax of the langauge. You can allocate dynamically sized arrays on the stack! <code>C++</code> can't do that (<em>although <code>C99</code> can ;) ...</em>).</p>
<p>The dotnet developers have also implemented hot reload in C# (which works... <em>most of the time</em>), and it's pretty fantastic for game development. You can launch your project with <code>dotnet watch</code> and it will live-update code changes, which is amazing when you want to change how something draws or the way an enemy updates.</p>
<p>C# also ends up being a great middle-ground between running things fast (which you need for video games) and easy to work with on a day-to-day basis. For example, I have been working on <a href="https://cityofnone.com/">City of None</a> with my brother <a href="https://liamberry.ca/">Liam</a>, who had done very little coding when we started the project. But over the last year he's slowly picked up the language to the point where he's programming entire boss fights by himself, because C# is just that accessible - and fairly foot-gun free. For small teams where everyone wears many hats, it's a really nice language.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/bossfight2.gif" alt="A boss fight that Liam coded" width="90%"></p><center><i>A boss fight that Liam coded</i></center>
<p>And finally, it has built in reflection... And while I wouldn't use it for release code, being able to quickly reflect on game objects for editor tooling is very nice. I can easily make live-inspection tools that show me the state of game objects without needing any custom meta programming or in-game reflection data. After spending a few years making games in C++ I really like having this back.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/reflection.jpeg" alt="Inspecting an object with reflection in Dear ImGui">
<em></em></p><center><em>Inspecting an object with reflection in Dear ImGui</em></center>
<h2>Windows... Input... Rendering... Audio?</h2>
<p>This is kind of the big question when writing "a game from scratch", but there are a lot of great libraries to help you get stuff onto the screen - from <a href="https://www.libsdl.org/">SDL</a>, to <a href="https://www.glfw.org/">GLFW</a>, to <a href="https://www.love2d.org/">Love2D</a>, to <a href="https://www.raylib.com/">Raylib</a>, etc.</p>
<p>I have been using <a href="https://wiki.libsdl.org/SDL3/FrontPage">SDL3</a> as it does everything I need as a cross-platform abstraction over the system - from windowing, to game controllers, to rendering. It works on Linux, Windows, Mac, Switch, PS4/5, Xbox, etc, and as of SDL3 there is a <a href="https://wiki.libsdl.org/SDL3/CategoryGPU">GPU abstraction</a> that handles rendering across DirectX, Vulkan, and Metal. It just <em>works</em>, is open source, and is used by a lot of the industry (ex. Valve). I started using it because <a href="https://fna-xna.github.io/">FNA</a>, which Celeste uses to run on non-Windows platforms, uses it as its platform abstraction.</p>
<p>That said, I have written <a href="https://github.com/FosterFramework/Foster">my own C# layer</a> on top of SDL for general rendering and input utilities I share across projects. I make highly opinionated choices about how I structure my games so I like having this little layer to interface with. It works really well for my needs, but there are full-featured alternatives like <a href="https://github.com/MoonsideGames/MoonWorks">MoonWorks</a> that fill a similar space.</p>
<p>Before SDL3's release with the GPU abstraction, I was writing my own OpenGL and DirectX implementations - which isn't trivial! But it was a <a href="https://learnopengl.com/">great learning experience</a>, and not as bad as I expected it to be. I am however, very greatful for SDL GPU as it is a very solid foundation that will be tested across millions of devices.</p>
<p>Finally, for Audio we're using <a href="https://www.fmod.com/">FMOD</a>. This is the last proprietary tool in our workflow, which I don't love (especially <a href="https://www.reddit.com/r/linux_gaming/comments/1ijcfnt/celeste_not_finding_libfmodstudioso10/">when something stops working</a> and you have to hand-patch their library), but it's the best tool for the job. There are more lightweight open source libraries if you just want to play sounds, but I work with audio teams that want finite control over dynamic audio, and a tool like FMOD is a requirement.</p>
<h2>Assets</h2>
<p>I don't have much to say about assets, because when you're rolling your own engine you just load up what files you want, when you need them, and move on. For all my pixel art games, I load the whole game up front and it's "fine" because the entire game is like 20mb. When I was working on <a href="https://exok.com/games/earthblade/">Earthblade</a>, which had larger assets, we would register them at startup and then only load them on request, disposing them after scene transitions. We just went with the most dead-simple implementation that accomplished the job.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/assets.jpeg" alt="Assets loading in 0.4 seconds">
<em></em></p><center><em>All the assets for City of None loading in 0.4 seconds</em></center>
<p>Sometimes you'll have assets that need to be converted before the game uses them, in which case I usually write a small script that runs when the game compiles that does any processing required. That's it.</p>
<h2>Level Editors, UI...</h2>
<p>Some day I'll write a fully procedural game, but until then I need tools to design the in-game spaces. There are a lot of really great existing tools out there, like <a href="https://ldtk.io/">LDtk</a>, <a href="https://www.mapeditor.org/">Tiled</a>, <a href="https://trenchbroom.github.io/">Trenchbroom</a>, and so on. I have used many of these to varying degrees and they're easy to set up and get running in your project - you just need to write a script to take the data they output and instantiate your game objects at runtime.</p>
<p>However, I usually like to write my own custom level editors for my projects. I like to have my game data tie directly into the editor, and I never go that deep on features because the things we need are specific but limited.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/postcard.png" alt="A small custom level editor for City of None using Dear ImGui">
<em></em></p><center><em>A small custom level editor for City of None using Dear ImGui</em></center>
<p>But I don't want to write the actual UI - coding textboxes and dropdowns isn't something I'm super keen on. I want a simple way to create fields and buttons, kind of like when <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/editor-CustomEditors.html">you write your own small editor utilities</a> in the Unity game engine.</p>
<p>This is where <a href="https://github.com/ocornut/imgui/">Dear ImGui</a> comes in. It's a lightweight, cross-platform, immediate-mode GUI engine that you can easily drop in to any project. The editor screenshot above uses it for everything with the exception of the actual "scene" view, which is custom as it's just drawing my level. There are more full-featured (and heavy-duty) alternatives, but if it's good enough for <a href="https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui">all these games</a> including <a href="https://github.com/ocornut/imgui/issues/7503#issuecomment-2308380962">Tears of the Kingdom</a> it's good enough for me.</p>
<p>Using ImGui makes writing editor tools extremely simple. I like having my tools pull data directly from my game, and using ImGui along with C# reflection makes that very convenient. I can loop over all the Actor classes in C# and have them accessible in my editor with a few lines of code! For more complicated tools it's sometimes overkill to write my own implementation, which is where I fall back to using existing tools built for specific jobs (like <a href="https://trenchbroom.github.io/">Trenchbroom</a>, for designing 3D environments).</p>
<h2>Porting Games ... ?</h2>
<p>The main reason I learned C++ a few years ago was because of my concerns with portability. At the time, it was not trivial to run C# code on consoles because C# was "just in time" compiled, which isn't something many platforms allow. Our game, Celeste, used a tool called <a href="http://brute.rocks/">BRUTE</a> to transpile the C# <a href="https://en.wikipedia.org/wiki/Common_Intermediate_Language">IL</a> (intermediate language binaries) to C++, and then recompiled that for the target platform. Unity <a href="https://docs.unity3d.com/6000.0/Documentation/Manual/scripting-backends-il2cpp.html">has a very similar tool</a> that does the same thing. This worked, but was not ideal for me. I wanted to be able to just compile our code for the target platform, and so learning C++ felt like the only real option.</p>
<p>Since then, however, C# has made incredible progress with their <a href="https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/">Native-AOT</a> toolchain (which basically just means all the code is compiled "ahead of time" - what languages like C++ and Rust do by default). It is now possible to compile C# code for all the major console architectures, which is amazing. The <a href="https://fna-xna.github.io/docs/appendix/Appendix-B%3A-FNA-on-Consoles/">FNA project</a> has been extremely proactive with this, leading to the release of games across all major platforms, while using C#.</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/platforms.jpeg" alt="Platforms supported by SDL3">
<em></em></p><center><em>Platforms supported by SDL3</em></center>
<p>And finally, SDL3 has console ports for all the major platforms. Using it as your platform abstraction layer (as long as you're careful about how you handle system calls) means a lot of it will "just work".</p>
<h2>Goodbye, Windows</h2>
<p>Finally, to wrap all this up ... I no longer use Windows to develop my games (aside from testing). I feel like this is in line with my general philosophy around using open source, cross-platform tools and libraries. I have found Windows <a href="https://www.howtogeek.com/739837/fyi-windows-11-home-will-require-a-microsoft-account-for-initial-setup/">increasingly frustrating to work</a> with, their <a href="https://bdsmovement.net/microsoft">business practices gross</a>, and their OS generally lacking. I grew up using Windows, but I switched to Linux full time around 3 years ago. And frankly, for programming video games, I have not missed it at all. It just doesn't offer me anything I can't do faster and more elegantly than on Linux.</p>
<p>There are of course certain workflows and tools that do not work on Linux, and that is just the current reality. I'm not entirely free of Microsoft either - I use vscode, I write my games in C#, and I host my projects on github... But the more people use Linux daily, the more pressure there is to support it, and the more support there is for open source alternatives.</p>
<p>(as a fun aside, I play most of my games on my steam deck these days, which means between my PC, game console, web server, and phone, I am always on a Linux platform)</p>
<h2>Miscellaneous thoughts</h2>
<ul>
<li><p><strong>What about Godot?</strong><br>If you're in the position to want the things a larger game engine provides, I definitely think <a href="https://godotengine.org/">Godot</a> is the best option. That it is open-source and community-maintained eliminates a lot of the issues I have with other proprietary game engines, but it still isn't usually the way I want to make games. I do intend to play around with it in the future for some specific ideas I have.</p>
</li>
<li><p><strong>What about 3D?</strong><br>I think that using big engines definitely have more of a place for 3D games - but even so for any kind of 3D project I want to do, I would roll my own little framework. I want to make highly stylized games that do not require very modern tech, and I have found that to be fairly straight forward (for example, we made <a href="https://github.com/exok/celeste64">Celeste 64</a> without very much prior 3D knowledge in under 2 weeks).</p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/celeste64.jpeg" alt="Celeste 64 screenshot">
<em></em></p><center><em>Celeste 64 Screenshot</em></center>
</li>
<li><p><strong>I need only the best fancy tech to pull off my game idea</strong><br>Then use Unreal! There's nothing wrong with that, but my projects don't require those kinds of features (and I would argue most of the things I do need can usually be learned fairly quickly).</p>
</li>
<li><p><strong>My whole team knows [Game Engine XYZ]</strong><br>The cost of migrating a whole team to a custom thing can be expensive and time consuming. I'm definitely talking about this from the perspective of smaller / solo teams. But that said, speaking from experience, I know several middle-sized studios moving to custom engines because they have determined the potential risk of using proprietary engines to be too high, and the migration and learning costs to be worth it. I think using custom stuff for larger teams is easier now than it has been in a long time.</p>
</li>
<li><p><strong>Game-specific workflows</strong></p>
<p><img src="https://noelberry.ca/posts/making_games_in_2025/aseprite.jpeg" alt="Screeshot of Aseprite">
<em></em></p><center><em>Aseprite assets are loaded in automatically</em></center>
<p>I load in <a href="https://www.aseprite.org/">Aseprite</a> files and have my City of None engine automatically turn them into game animations, using their built in tags and frame timings. The format is <a href="https://github.com/aseprite/aseprite/blob/main/docs/ase-file-specs.md">surprisingly straight forward</a>. When you write your own tools it's really easy to add things like this!</p>
</li>
</ul>
<h2>Alright!</h2>
<p>That's it from me! That's how I make games in 2025!</p>
<p>Do I think you should make games without a big engine? My answer is: If it sounds fun.</p>
<p>-Noel</p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got fooled by AI-for-science hype–here's what it taught me (325 pts)]]></title>
            <link>https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres</link>
            <guid>44037941</guid>
            <pubDate>Tue, 20 May 2025 04:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres">https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres</a>, See on <a href="https://news.ycombinator.com/item?id=44037941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em><span>I’m excited to publish this guest post by </span><a href="https://x.com/NMcGreivy/" rel="">Nick McGreivy</a><span>, a physicist who last year earned a PhD from Princeton. Nick used to be optimistic that AI could accelerate physics research. But when he tried to apply AI techniques to real physics problems the results were disappointing.</span></em></p><p><em><span>I’ve </span><a href="https://www.understandingai.org/p/six-principles-for-thinking-about" rel="">written before</a><span> about the Princeton School of AI Safety, which holds that the impact of AI is likely to be similar to that of past general-purpose technologies such as electricity, integrated circuits, and the Internet. I think of this piece from Nick as being in that same intellectual tradition.</span></em></p><p><em>—Timothy B. Lee</em></p><p><span>In 2018, as a second-year PhD student at Princeton studying </span><a href="https://en.wikipedia.org/wiki/Plasma_(physics)" rel="">plasma physics</a><span>, I decided to switch my research focus to machine learning. I didn’t yet have a specific research project in mind, but I thought I could make a bigger impact by using AI to accelerate physics research. (I was also, quite frankly, motivated by the </span><a href="https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?action=click&amp;module=RelatedCoverage&amp;pgtype=Article&amp;region=Footer" rel="">high</a><span> </span><a href="https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-openai.html" rel="">salaries</a><span> in AI.)</span></p><p><span>I eventually chose to study what AI pioneer Yann LeCun later </span><a href="https://x.com/ylecun/status/1581648953275473921" rel="">described</a><span> as a “pretty hot topic, indeed”: using AI to solve partial differential equations (PDEs). But as I tried to build on what I thought were impressive results, I found that AI methods performed much worse than advertised.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png" width="1456" height="963" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:963,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2532708,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.understandingai.org/i/163736413?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37cc7a6c-a9d3-4c9a-b9ce-f515b811e219_2004x1325.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>The author, Nick McGreivy.</figcaption></figure></div><p><span>At first, I tried applying a widely-cited AI method called PINN to some fairly simple PDEs, but found it to be unexpectedly brittle. Later, though dozens of papers had claimed that AI methods could solve PDEs faster than standard numerical methods—in some cases as much as a </span><a href="https://iopscience.iop.org/article/10.1088/1741-4326/ad313a" rel="">million times faster</a><span>—I discovered that a large majority of these comparisons were unfair. When </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">I compared</a><span> these AI methods on equal footing to state-of-the-art numerical methods, whatever narrowly defined advantage AI had usually disappeared.</span></p><p><span>This experience has led me to question the idea that AI is poised to “</span><a href="https://www.youtube.com/watch?v=yxAJohm0l_g&amp;ab_channel=TheRoyalSwedishAcademyofSciences" rel="">accelerate</a><span>” or even “</span><a href="https://www.youtube.com/watch?v=PKN95I93iGE&amp;ab_channel=TheEconomist" rel="">revolutionize</a><span>” science. Are we really about to enter what DeepMind </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">calls</a><span> “a new golden age of AI-enabled scientific discovery,” or has the overall potential of AI in science been exaggerated—much like it was in my subfield?</span></p><p><span>Many others have identified similar issues. For example, in 2023 DeepMind </span><a href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/" rel="">claimed</a><span> to have discovered 2.2 million crystal structures, </span><a href="https://www.nature.com/articles/s41586-023-06735-9" rel="">representing</a><span> “an order-of-magnitude expansion in stable materials known to humanity.” But when </span><a href="https://x.com/Robert_Palgrave/status/1744383962913394758" rel="">materials</a><span> </span><a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002" rel="">scientists</a><span> </span><a href="https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643" rel="">analyzed these compounds</a><span>, they found it was “</span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">mostly junk</a><span>” and “respectfully” suggested that the paper “does not report any new materials.”</span></p><p><span>Separately, Princeton computer scientists </span><a href="https://www.cs.princeton.edu/~arvindn/" rel="">Arvind Narayanan</a><span> and </span><a href="https://www.cs.princeton.edu/~sayashk/" rel="">Sayash Kapoor</a><span> have </span><a href="https://reproducible.cs.princeton.edu/" rel="">compiled a list</a><span> of 648 papers across 30 fields that all make a methodological error called </span><a href="http://en.wikipedia.org/wiki/Leakage_(machine_learning)" rel="">data leakage</a><span>. In each case data leakage leads to overoptimistic results. They argue that AI-based science is facing a “reproducibility crisis.”</span></p><p><span>Yet AI adoption in scientific research has been </span><a href="https://doi.org/10.1038/s41562-024-02020-5" rel="">rising sharply</a><span> </span><a href="https://arxiv.org/abs/2405.15828" rel="">over the last decade</a><span>. Computer science has seen the biggest impacts, of course, but other disciplines—physics, chemistry, biology, medicine, and the social sciences—have also seen rapidly increasing AI adoption. Across all scientific publications, </span><a href="https://www.csiro.au/en/research/technology-space/ai/artificial-intelligence-for-science-report" rel="">rates of AI usage grew</a><span> from 2 percent in 2015 to </span><a href="https://www.nature.com/articles/d41586-023-02980-0" rel="">almost 8 percent in 2022</a><span>. It’s harder to find data about the last few years, but there’s every reason to think that </span><a href="https://trends.google.com/trends/explore?date=all&amp;q=ai%20for%20science&amp;hl=en" rel="">hockey stick growth has continued</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36a0461d-d4f1-497b-8a4e-b962fd14c880_1600x1200.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>To be clear, AI </span><em>can</em><span> drive scientific breakthroughs. My concern is about their magnitude and frequency. Has AI really shown enough potential to justify such a massive shift in talent, training, time, and money away from existing research directions and towards a single paradigm?</span></p><p><span>Every field of science is experiencing AI differently, so we should be cautious about making generalizations. I’m convinced, however, that </span><em>some</em><span> of the lessons from my experience are broadly applicable across science:</span></p><ul><li><p><span>AI adoption is exploding among scientists less because it benefits science and more </span><a href="https://arxiv.org/abs/2412.07727" rel="">because it benefits the scientists themselves</a><span>.</span></p></li><li><p><span>Because AI researchers almost never publish negative results, AI-for-science is experiencing </span><a href="https://en.wikipedia.org/wiki/Survivorship_bias" rel="">survivorship bias</a><span>.</span></p></li><li><p>The positive results that get published tend to be overly optimistic about AI’s potential.</p></li></ul><p>As a result, I’ve come to believe that AI has generally been less successful and revolutionary in science than it appears to be.</p><p><span>Ultimately, I don’t know whether AI will reverse the decades-long trend of </span><a href="https://mattsclancy.substack.com/p/science-is-getting-harder" rel="">declining scientific productivity</a><span> and stagnating (or even decelerating) rates of </span><a href="https://substack.com/@aisnakeoil/note/c-92421948" rel="">scientific progress</a><span>. I don’t think anyone does. But barring major (and in my opinion unlikely) breakthroughs in advanced AI, I expect AI to be much more a </span><a href="https://knightcolumbia.org/content/ai-as-normal-technology" rel="">normal</a><span> tool of incremental, uneven scientific progress than a revolutionary one.</span></p><p><span>In the summer of 2019, I got a first taste of what would become my dissertation topic: solving PDEs with AI. </span><a href="https://en.wikipedia.org/wiki/Partial_differential_equation" rel="">PDEs</a><span> are mathematical equations used to model a wide range of physical systems, and solving (i.e., simulating) them is an extremely important task in computational physics and engineering. My lab uses PDEs to </span><a href="https://www.pppl.gov/research/computational-sciences" rel="">model</a><span> the behavior of plasmas, such as inside fusion reactors and in the interstellar medium of outer space.</span></p><p><span>The AI models being used to solve PDEs are custom deep learning models, much more analogous to </span><a href="https://deepmind.google/technologies/alphafold/" rel="">AlphaFold</a><span> than ChatGPT.</span></p><p><span>The first approach I tried was something called the physics-informed neural network. PINNs had recently been introduced in an </span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yZ0-ywkAAAAJ&amp;citation_for_view=yZ0-ywkAAAAJ:eLRq4zTgah0C" rel="">influential paper</a><span> that had already racked up hundreds of citations.</span></p><p>PINNs were a radically different way of solving PDEs compared to standard numerical methods. Standard methods represent a PDE solution as a set of pixels (like in an image or video) and derive equations for each pixel value. In contrast, PINNs represent the PDE solution as a neural network and put the equations into the loss function.</p><p>As a naive grad student who didn’t even have an advisor yet, there was something incredibly appealing to me about PINNs. They just seemed so simple, elegant, and general.</p><p><span>They also seemed to have good results. The </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125" rel="">paper</a><span> introducing PINNs found that their “effectiveness” had been “demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction-diffusion systems, and the propagation of nonlinear shallow-water waves.” If PINNs had solved all these PDEs, I figured, then surely they could solve some of the plasma physics PDEs that </span><a href="https://en.wikipedia.org/wiki/Princeton_Plasma_Physics_Laboratory" rel="">my lab</a><span> </span><a href="https://ammar-hakim.org/sj/je/je17/je17-hasegawa-wakatani.html" rel="">cared</a><span> </span><a href="https://en.wikipedia.org/wiki/Gyrokinetics" rel="">about</a><span>.</span></p><p><span>But when I replaced one of the </span><a href="https://github.com/maziarraissi/PINNs" rel="">examples from</a><span> that influential first paper (</span><a href="https://en.wikipedia.org/wiki/Burgers%27_equation" rel="">1D Burgers’</a><span>) with a different, but still extremely simple, PDE (</span><a href="https://ammar-hakim.org/sj/je/je14/je14-vlasov-fixed-pot.html" rel="">1D Vlasov</a><span>), the results didn’t look anything like the exact solution. Eventually, after extensive tuning, I was able to get something that looked correct. However, when I tried slightly more complex PDEs (such as </span><a href="https://en.wikipedia.org/wiki/Vlasov_equation#The_Vlasov%E2%80%93Poisson_equation" rel="">1D Vlasov-Poisson</a><span>), no amount of tuning could give me a decent solution.</span></p><p>After a few weeks of failure, I messaged a friend at a different university, who told me that he too had tried using PINNs, but hadn’t been able to get good results.</p><p>Eventually, I realized what had gone wrong. The authors of the original PINN paper had, like me, “observed that specific settings that yielded impressive results for one equation could fail for another.” But because they wanted to convince readers of how exciting PINNs were, they hadn’t shown any examples of PINNs failing.</p><p>This experience taught me a few things. First, to be cautious about taking AI research at face value. Most scientists aren’t trying to mislead anyone, but because they face strong incentives to present favorable results, there’s still a risk that you’ll be misled. Moving forward, I would have to be more skeptical, even (or perhaps especially) of high-impact papers with impressive results.</p><p><span>Second, people rarely publish papers about when AI methods fail, only when they succeed. The authors of the original PINN paper didn’t publish about the PDEs their method hadn’t been able to solve. I didn’t publish my unsuccessful experiments, presenting only a </span><a href="https://github.com/nickmcgreivy/PINN/blob/master/APS-Poster-McGreivy-2019.pdf" rel="">poster</a><span> at an obscure conference. So very few researchers heard about them. In fact, despite the huge popularity of PINNs, it took four years for anyone to publish </span><a href="https://proceedings.neurips.cc/paper/2021/hash/df438e5206f31600e6ae4af72f2725f1-Abstract.html" rel="">a paper about</a><span> their failure modes. That paper now has almost a thousand citations, suggesting that many other scientists tried PINNs and found similar issues.</span></p><p><span>Third, I concluded that PINNs weren’t the approach I wanted to use. They were simple and elegant, sure, but they were also far </span><a href="https://arxiv.org/abs/2205.14249" rel="">too unreliable</a><span>, </span><a href="https://arxiv.org/abs/2306.00230" rel="">too finicky</a><span>, and </span><a href="https://academic.oup.com/imamat/article/89/1/143/7680268" rel="">too slow</a><span>.</span></p><p><span>As of today, six years later, the original PINN paper has a whopping </span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yZ0-ywkAAAAJ&amp;citation_for_view=yZ0-ywkAAAAJ:eLRq4zTgah0C" rel="">14,000 citations</a><span>, making it the most cited numerical methods paper of the 21st century (and, by my count, a year or two away from becoming the second most-cited numerical methods paper of all time).</span></p><p><span>Though it’s now widely accepted that PINNs generally aren’t competitive with standard numerical methods for </span><em>solving</em><span> PDEs, there remains debate over how well PINNs perform for a different class of problems known as </span><em>inverse problems</em><span>. Advocates claim that PINNs are “</span><a href="https://www.nature.com/articles/s42254-021-00314-5" rel="">particularly effective</a><span>” for inverse problems, but some researchers have </span><a href="https://academic.oup.com/pnasnexus/article/3/1/pgae005/7516080" rel="">vigorously contested</a><span> that idea.</span></p><p><span>I don’t know which side of the debate is right. I’d like to think that </span><a href="https://x.com/shoyer/status/1532278186901327872" rel="">something useful has come</a><span> from all this PINN research, but I also wouldn’t be surprised if one day we look back on PINNs as simply a massive citation bubble.</span></p><p>For my dissertation, I focused on solving PDEs using deep learning models that, like traditional solvers, treated the PDE solution as a set of pixels on a grid or a graph.</p><p><span>Unlike PINNs, this approach had shown a lot of promise on the complex, time-dependent PDEs that my lab cared about. Most impressively, </span><a href="https://arxiv.org/abs/2010.08895" rel="">paper</a><span> </span><a href="https://www.nature.com/articles/s42256-021-00302-5" rel="">after</a><span> </span><a href="https://openreview.net/forum?id=roNqYL0_XP" rel="">paper</a><span> </span><a href="https://www.pnas.org/doi/abs/10.1073/pnas.2101784118" rel="">had</a><span> </span><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13619" rel="">demonstrated</a><span> the ability to solve PDEs faster—often orders of magnitude faster—than standard numerical methods.</span></p><p><span>The examples that excited </span><a href="https://www.pppl.gov/people/ammar-hakim" rel="">my advisor</a><span> and me the most were PDEs from fluid mechanics, such as the </span><a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/" rel="">Navier-Stokes equations</a><span>. We thought we might see similar speedups because the PDEs we cared about—equations describing </span><a href="https://ammar-hakim.org/sj/je/je17/je17-hasegawa-wakatani.html" rel="">plasmas in</a><span> </span><a href="https://hal.science/hal-03974985/file/Gyrokinetics_fundamentals_XG_ML_23.pdf" rel="">fusion reactors</a><span>, for example—have a </span><a href="https://arxiv.org/abs/1908.01814" rel="">similar mathematical structure</a><span>. In theory, this could allow scientists and engineers like us to simulate larger systems, more rapidly optimize existing designs, and ultimately accelerate the pace of research.</span></p><p>By this point, I was seasoned enough to know that in AI research, things aren’t always as rosy as they seem. I knew that reliability and robustness might be serious issues. If AI models give faster simulations, but those simulations are less reliable, would that be worth the trade-off? I didn’t know the answer and set out to find out.</p><p><span>But as I tried—and </span><a href="https://arxiv.org/abs/2303.16110" rel="">mostly failed</a><span>—to make these models more reliable, I began to question how much promise AI models had really shown for accelerating PDEs.</span></p><p><span>According to a number of </span><a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/" rel="">high-profile papers</a><span>, AI had solved the Navier-Stokes equations orders of magnitude faster than standard numerical methods. I eventually discovered, however, that the baseline methods used in these papers were not the fastest numerical methods available. When I compared AI to more advanced numerical methods, I found that AI was no faster (or at most, only slightly faster) than the stronger baselines.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png" width="1422" height="574" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:574,&quot;width&quot;:1422,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57438099-02ae-4bcd-8bdc-a1767385befd_1422x574.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>When AI methods for solving PDEs were compared to strong baselines, whatever narrowly defined advantage AI had usually disappeared.</figcaption></figure></div><p><span>My advisor and I eventually </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">published</a><span> a systematic review of research using AI to solve PDEs from fluid mechanics. We found that 60 out of the 76 papers (79 percent) that claimed to outperform a standard numerical method had used a weak baseline, either because they hadn’t compared to more advanced numerical methods, or because they weren’t comparing them on an equal footing. Papers with large speedups </span><em>all</em><span> compared to weak baselines, suggesting that the more impressive the result, the more likely the paper had made an unfair comparison.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png" width="1300" height="700" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:700,&quot;width&quot;:1300,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F031e6fe4-c936-4846-8f81-0e0029d1042d_1300x700.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Results from a systematic review of research comparing AI methods for solving PDEs from fluid mechanics to standard numerical methods. Very few papers reported negative results, while those reporting positive results mostly compared to weak baselines.</figcaption></figure></div><p><span>We also found evidence, once again, that researchers tend not to report negative results, an effect known as </span><a href="https://en.wikipedia.org/wiki/Reporting_bias" rel="">reporting bias</a><span>. We ultimately </span><a href="https://www.nature.com/articles/s42256-024-00897-5" rel="">concluded</a><span> that AI-for-PDE-solving research is overoptimistic: “weak baselines lead to overly positive results, while reporting biases lead to under-reporting of negative results.”</span></p><p><span>These findings </span><a href="https://www.nature.com/articles/s42256-025-00989-w" rel="">sparked a debate</a><span> about AI in computational science and engineering:</span></p><ul><li><p><a href="https://engineering.gwu.edu/lorena-barba" rel="">Lorena Barba</a><span>, a professor at GWU who has previously discussed poor research practices in what she </span><a href="https://lorenabarba.com/figshare/anti-patterns-of-scientific-machine-learning-to-fool-the-massesa-call-for-open-science/" rel="">has called</a><span> “Scientific Machine Learning to Fool the Masses,” </span><a href="https://x.com/LorenaABarba/status/1839729358044574158" rel="">saw</a><span> our results as “solid evidence supporting our concerns in the computational science community over the hype and unscientific optimism” of AI.</span></p></li><li><p><a href="https://stephanhoyer.com/" rel="">Stephan Hoyer</a><span>, the lead of a </span><a href="https://arxiv.org/abs/2207.00556" rel="">team</a><span> at Google Research that independently reached </span><a href="https://x.com/shoyer/status/1362301955243057154" rel="">similar conclusions</a><span>, </span><a href="https://x.com/shoyer/status/1839195637474332850" rel="">described</a><span> our paper as “a nice summary of why I moved on from [AI] for PDEs” to weather prediction and climate modeling, applications of AI that seem </span><a href="https://www.nature.com/articles/d41586-024-02391-9" rel="">more promising</a><span>.</span></p></li><li><p><a href="https://brandstetter-johannes.github.io/" rel="">Johannes Brandstetter</a><span>, a professor at JKU Linz and co-founder of a </span><a href="https://www.emmi.ai/about" rel="">startup</a><span> that provides “AI-driven physics simulations”, </span><a href="https://www.nature.com/articles/s42256-024-00962-z" rel="">argued</a><span> that AI might achieve better results for more complex industrial applications and that “the future of the field remains undeniably promising and brimming with potential impact.”</span></p></li></ul><p><span>In </span><a href="https://www.nature.com/articles/s42256-025-00989-w" rel="">my opinion</a><span>, AI might eventually prove useful for certain applications related to solving PDEs, but I currently don’t see much reason for optimism. I’d like to see a lot more focus on trying to match the reliability of numerical methods and on </span><a href="https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/" rel="">red teaming</a><span> AI methods; right now, they have neither the </span><a href="https://arxiv.org/abs/2303.16110" rel="">theoretical guarantees</a><span> nor empirically validated robustness of standard numerical methods.</span></p><p><span>I’d also like to see funding agencies incentivize scientists to create challenge problems for PDEs. A good model could be </span><a href="https://en.wikipedia.org/wiki/CASP" rel="">CASP</a><span>, a biennial protein folding competition that helped to motivate and focus research in this area over the last 30 years.</span></p><p><span>Besides </span><a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" rel="">protein folding</a><span>, the canonical example of a scientific breakthrough from AI, a few examples of scientific progress from AI include:</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163736413" href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres#footnote-1-163736413" target="_self" rel="">1</a></span></p><ul><li><p><span>Weather forecasting, where </span><a href="https://www.ecmwf.int/en/about/media-centre/news/2025/ecmwfs-ai-forecasts-become-operational" rel="">AI forecasts</a><span> have had up to 20% higher accuracy (though still lower resolution) compared to traditional physics-based forecasts.</span></p></li><li><p><span>Drug discovery, where </span><a href="https://www.sciencedirect.com/science/article/pii/S135964462400134X?via%3Dihub" rel="">preliminary data</a><span> suggests that AI-discovered drugs have been more successful in Phase I (but not Phase II) clinical trials. If the trend holds, this would imply a nearly twofold increase in end-to-end drug approval rates.</span></p></li></ul><p><span>But </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">AI</a><span> </span><a href="https://ai.google/applied-ai/science/" rel="">companies</a><span>, </span><a href="https://royalsociety.org/news-resources/projects/science-in-the-age-of-ai/" rel="">academic</a><span> and </span><a href="https://www.anl.gov/ai/reference/AI-for-Science-Energy-and-Security-Report-2023" rel="">governmental</a><span> organizations, and </span><a href="https://www.npr.org/2023/10/16/1198908289/ai-proteins-batteries-artificial-intelligence-scientific-discoveries" rel="">media outlets</a><span> increasingly present AI not only as a </span><a href="https://www.nature.com/articles/d41586-025-01069-0" rel="">useful scientific tool</a><span>, but </span><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai-for-science/" rel="">one that</a><span> “will have a transformational impact” on science.</span></p><p><span>I don’t think we should necessarily dismiss these statements. While current LLMs, </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">according to DeepMind</a><span>, “still struggle with the deeper creativity and reasoning that human scientists rely on”, </span><a href="https://helentoner.substack.com/p/long-timelines-to-advanced-ai-have" rel="">hypothetical advanced AI systems</a><span> might one day be capable of </span><a href="https://sakana.ai/ai-scientist/" rel="">fully</a><span> </span><a href="https://www.futurehouse.org/" rel="">automating</a><span> the scientific process. I don’t expect that to happen anytime soon—if ever. But if such systems are created, there’s no doubt they would transform and accelerate science.</span></p><p>However, based on some of the lessons from my research experience, I think we should be pretty skeptical of the idea that more conventional AI techniques are on pace to significantly accelerate scientific progress.</p><p><span>Most narratives about AI accelerating science come from AI companies or scientists working on AI who benefit, directly or indirectly, from those narratives. For example, NVIDIA CEO Jensen Huang </span><a href="https://blogs.nvidia.com/blog/supercomputing-24/" rel="">talks about how</a><span> “AI will drive scientific breakthroughs” and “</span><a href="https://www.youtube.com/watch?v=heshd3L6Kdk" rel="">accelerate science by a million-X</a><span>.” NVIDIA, whose </span><a href="https://www.technologyreview.com/2016/04/07/161131/the-man-selling-shovels-in-the-machine-learning-gold-rush/" rel="">financial conflicts of interest</a><span> make them a particularly unreliable narrator, regularly makes hyperbolic statements about AI in science.</span></p><p><span>You might think that the rising adoption of AI by scientists is </span><a href="https://deepmind.google/public-policy/ai-for-science/" rel="">evidence</a><span> </span><a href="https://arxiv.org/abs/2405.15828" rel="">of</a><span> </span><a href="https://www.nature.com/articles/s41562-024-02020-5" rel="">AI’s</a><span> </span><a href="https://www.csiro.au/en/research/technology-space/ai/artificial-intelligence-for-science-report" rel="">usefulness</a><span> </span><a href="https://bojan.substack.com/p/ai-is-eating-the-research-world" rel="">in science</a><span>. After all, if AI usage in scientific research is growing exponentially, it must be because scientists find it useful, right?</span></p><p><span>I’m not so sure. In fact, I suspect that scientists are switching to AI less because it benefits science, and more because it benefits them.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-163736413" href="https://www.understandingai.org/p/i-got-fooled-by-ai-for-science-hypeheres#footnote-2-163736413" target="_self" rel="">2</a></span></p><p><span>Consider my motives for switching to AI in 2018. While I sincerely thought that AI might be useful in plasma physics, I was mainly motivated by higher salaries, better job prospects, and academic prestige. I also noticed that higher-ups at my lab usually seemed more interested in the </span><a href="https://www.energy.gov/science/articles/department-energy-announces-68-million-funding-artificial-intelligence-scientific" rel="">fundraising potential</a><span> of AI than technical considerations.</span></p><p><span>Later research found that scientists who use AI are </span><a href="https://www.nature.com/articles/d41586-024-03355-9" rel="">more likely to publish top-cited papers</a><span> and receive on average </span><a href="https://arxiv.org/abs/2412.07727" rel="">three times as many citations</a><span>. With such strong incentives to use AI, it isn’t surprising that so many scientists are doing so.</span></p><p><span>So even when AI achieves genuinely impressive results </span><em>in</em><span> science, that doesn’t mean that AI has done something useful </span><em>for</em><span> science. More often, it reflects only the </span><em>potential</em><span> of AI to be useful down the road.</span></p><p><span>This is because scientists working on AI (myself included) often work backwards. Instead of identifying a problem and then trying to find a solution, we start by assuming that AI will be the solution and then looking for problems to solve. But because it’s difficult to identify open scientific challenges that can be solved using AI, this “</span><a href="https://x.com/MilesCranmer/status/1879542350541635882" rel="">hammer in search of a nail</a><span>” style of science means that researchers will often tackle problems which are suitable for using AI but which either have already been solved or don't create new scientific knowledge.</span></p><p>To accurately evaluate the impacts of AI in science, we need to actually look at the science. But unfortunately, the scientific literature is not a reliable source for evaluating the success of AI in science.</p><p><span>One issue is </span><a href="https://en.wikipedia.org/wiki/Survivorship_bias" rel="">survivorship bias</a><span>. Because AI research, </span><a href="https://research-information.bris.ac.uk/ws/portalfiles/portal/437692523/methods_failing_the_data.pdf" rel="">in the words of</a><span> one researcher, has “nearly complete non-publication of negative results,” we usually only see the successes of AI in science and not the failures. But without negative results, our attempts to evaluate the impacts of AI in science typically get distorted.</span></p><p><span>As anyone who’s studied the </span><a href="https://en.wikipedia.org/wiki/Replication_crisis" rel="">replication crisis</a><span> knows, survivorship bias is a </span><a href="https://en.wikipedia.org/wiki/Science_Fictions" rel="">major issue</a><span> in science. Usually, the culprit is a </span><a href="https://www.cambridge.org/core/journals/psychological-medicine/article/cumulative-effect-of-reporting-and-citation-biases-on-the-apparent-efficacy-of-treatments-the-case-of-depression/71D73CADE32C0D3D996DABEA3FCDBF57" rel="">selection process</a><span> in which results that are not statistically significant are filtered from the scientific literature.</span></p><p><span>For example, the distribution of </span><a href="https://x.com/JohnHolbein1/status/1903173893222711795" rel="">z-values from medical research</a><span> is shown below. A z-value between -1.96 and 1.96 indicates that a result is not statistically significant. The sharp discontinuity around these values suggests that many scientists either didn’t publish results between these values or massaged their data until they cleared the threshold of statistical significance.</span></p><p>The problem is that if researchers fail to publish negative results, it can cause medical practitioners and the general public to overestimate the effectiveness of medical treatments.</p><p><span>Something similar has been happening in AI-for-science, though the selection process is based not on statistical significance but on </span><em>whether the proposed method outperforms other approaches or successfully performs some novel task</em><span>. This means that AI-for-science researchers almost always report successes of AI, and rarely publish results when AI isn’t successful.</span></p><p><span>A second issue is that pitfalls often cause the successful results that do get published to reach overly optimistic conclusions about AI in science. The details and </span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">severity</a><span> seem to differ </span><a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002" rel="">between</a><span> </span><a href="https://x.com/Robert_Palgrave/status/1744383962913394758" rel="">fields</a><span>, but </span><a href="https://www.nature.com/articles/d41586-019-02307-y#ref-CR2" rel="">pitfalls mostly</a><span> have fallen into one of </span><a href="https://arxiv.org/abs/2407.12220" rel="">four categories</a><span>: </span><a href="https://reproducible.cs.princeton.edu/" rel="">data leakage</a><span>, </span><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184604" rel="">weak</a><span> </span><a href="https://x.com/tunguz/status/1853545690565058723" rel="">baselines</a><span>, </span><a href="https://news.ycombinator.com/item?id=36231147" rel="">cherry-picking</a><span>, and </span><a href="https://pubs.acs.org/doi/10.1021/acs.chemmater.4c00643" rel="">misreporting</a><span>.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83b0629-a6f1-49e1-9c19-6b9932db6f30_1600x900.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>The same people who evaluate AI models also benefit from those evaluations.</figcaption></figure></div><p><span>While the causes of this tendency towards overoptimism are complex, the core issue appears to be a </span><a href="https://arxiv.org/abs/2407.12220" rel="">conflict of interest</a><span> in which the same people who evaluate AI models also benefit from those evaluations.</span></p><p><span>These issues seem to be </span><a href="https://www.aisnakeoil.com/p/scientists-should-use-ai-as-a-tool" rel="">bad enough</a><span> that I encourage people to treat impressive results in AI-for-science the same way we treat surprising results in nutrition science: with </span><a href="https://www.theatlantic.com/magazine/archive/2023/05/ice-cream-bad-for-you-health-study/673487/" rel="">instinctive</a><span> </span><a href="https://www.cbsnews.com/news/how-the-chocolate-diet-hoax-fooled-millions/" rel="">skepticism</a><span>.</span></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Biff – a batteries-included web framework for Clojure (112 pts)]]></title>
            <link>https://biffweb.com</link>
            <guid>44037426</guid>
            <pubDate>Tue, 20 May 2025 03:13:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://biffweb.com">https://biffweb.com</a>, See on <a href="https://news.ycombinator.com/item?id=44037426">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Biff curates libraries and tools from across the ecosystem and composes them into one polished whole.</p><div><div><p>Bring immutability to your database, not just your code. Biff adds schema enforcement with Malli.</p></div><div><p>Create rich, interactive UIs without leaving the backend. Throw in a dash of _hyperscript for light client-side scripting.</p></div><div><p>Passwordless, email-based authentication, with support for magic link and one-time passcode flows.</p></div><div><p>Biff comes with code for provisioning an Ubuntu VPS, or you can deploy an Uberjar with Docker.</p></div><div><p>Changes are evaluated whenever you save a file. Connect to a production REPL and develop your whole app on the fly.</p></div><div><p>Get started with the tutorial, delve into the reference docs, or tinker with the starter project and inspect the doc strings.</p></div></div><p><span>Strong defaults, weakly held.</span> Biff is designed to be taken apart and modified, so it doesn't get in the way as your needs evolve.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What are people doing? Live-ish estimates based on global population dynamics (259 pts)]]></title>
            <link>https://humans.maxcomperatore.com/</link>
            <guid>44036900</guid>
            <pubDate>Tue, 20 May 2025 01:36:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://humans.maxcomperatore.com/">https://humans.maxcomperatore.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44036900">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="utc-time-display">
      <svg id="utc-analog-clock" viewBox="0 0 100 100">
        <circle cx="50" cy="50" r="48"></circle><line id="hour-hand" x1="50" y1="50" x2="50" y2="25"></line><line id="minute-hand" x1="50" y1="50" x2="50" y2="15"></line><line id="second-hand" x1="50" y1="50" x2="50" y2="10"></line><circle cx="50" cy="50" r="3"></circle>
      </svg>
      <p><span id="currentTime">Coordinated Universal Time (UTC): 00:00:00</span>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A shower thought turned into a Collatz visualization (126 pts)]]></title>
            <link>https://abstractnonsense.com/collatz/</link>
            <guid>44036716</guid>
            <pubDate>Tue, 20 May 2025 01:04:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abstractnonsense.com/collatz/">https://abstractnonsense.com/collatz/</a>, See on <a href="https://news.ycombinator.com/item?id=44036716">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>
        I recently went on a nice long SCUBA diving trip with my wife and daughters. Lots of diving implies lots of showers, and
        lots of showers means lots of shower-thoughts! <span>[1]</span> An especially interesting one I had turned into a nice way to visualize
        some aspects of the Collatz Conjecture.
    </p>
    <p>
        The Collatz Conjecture defines a very simple function on all positive integers as follows:
        </p><ul>
            <li>If the number is even, divide it by 2</li>
            <li>If the number is odd, multiply it by 3 and then add 1</li>
        </ul>
    
    <p>
        If you can prove that repeated applications of this function, starting from any positive integer, will eventually reach 1 then you have proved the Collatz Conjecture
        and won a million dollars and plenty of fame and glory! <span>[2]</span>
    </p>
    <p>
        Individual inputs are trivial to verify. For example, if we start with 6, we get:
        </p><ul>
            <li>6 is even, so 6 / 2 → 3</li>
            <li>3 is odd, so 3 × 3 + 1 → 10</li>
            <li>10 is even, so 10 / 2 → 5</li>
            <li>5 is odd, so 5 × 3 + 1 → 16</li>
            <li>16 is even, so 16 / 2 → 8</li>
            <li>8 is even, so 8 / 2 → 4</li>
            <li>4 is even, so 4 / 2 → 2</li>
            <li>2 is even, so 2 / 2 → 1</li>
        </ul>
        <br>
        But the real work is proving that this happens for <i>all</i> positive integers. So far, no one has been able to do that.
    
    <p>
        I'm not going to solve the Collatz Conjecture. Somehow I'm turning 50 this year, and my PhD was more than two decades ago (and in a branch of math that's unlikely
        to impact the conjecture). But I love that it's just there, waiting and inspiring people. I hope to see it solved in my lifetime.
    </p>
    <p>
        Anyway, my shower-thought was that it would be nice to visualize this repeated application of the Collatz function
        for many positive integers all at once, rather than just one at a time. To do that, I thought, why not
        keep track of the sequence of branches taken for each input and then since there are only two branches,
        why not treat them as binary digits?! <span>[3]</span> We could use those binary digit sequences to make a fraction from
        each input, by summing <i>2<sup>-n</sup>b<sub>n</sub></i> for each bit <i>b<sub>n</sub></i> in the sequence,
        making it really easy to graph them and perhaps more easily see what the Collatz process is doing.
    </p>
    <p>
        After a little more thinking I decided this approach would be a little too naive. The output of the Collatz function at each step seems
        like it would be biased towards producing even numbers, making the bit-sequences more full of one binary digit than the other, and maybe obscuring any interesting features
        we could otherwise see. I decided I would fix that by <i>immediately</i> dividing by 2 at the end of the 3n + 1 step (since we know 3n + 1 will be even if n is odd).
        It turns out this idea is well-known, as the "shortcut" Collatz function.
    </p>
    <p>
        With that tweak, here's a simple JavaScript implementation of this idea:
        </p><pre><span>function</span> collatzBits(n) { 
    <span>let</span> bits = [];
    <span>while</span> (n !== 1) {
        <span>if</span> (n % 2 === 0) {
            bits.push(1);
            n = n / 2;
        } <span>else</span> {
            bits.push(0);
            n = (3 * n + 1) / 2;
        }
    }
    <span>return</span> bits;
}

<span>function</span> bitsToFraction(bits) { 
    <span>let</span> fraction = 0;
    <span>for</span> (<span>let</span> i = 0; i &lt; bits.length; i++) {
        fraction += bits[i] / Math.pow(2, i + 1);
    }
    <span>return</span> fraction;
}
</pre>
    
    <p>
        Here's an interactive version you can play with. Try putting in a few numbers to see how the Collatz process gets encoded into binary fractions.
        <br>
        </p><p>
            Choose an input n =
            
            <br>
            The corresponding sequence of bits, interpreted as a binary fraction, is <span id="interactiveBits">0.0</span><sub>2</sub>
            <br>
            which is <span id="interactiveOutput">0</span> as a decimal.
        </p>
    
    <p>
        Something fun to think about: How long of a bit-sequence can you construct? Can you make an arbitrarily long one, by choosing the right starting number?
    </p>
    
    <p>
        Of course, now that we have a mapping from positive integers to fractions, we can also plot them.
    </p>
    <p>
        Here's a plot of the Collatz fractions for the first N positive integers. You can adjust N to see more points. Try some big
        values of N to get a sense of the distribution of the Collatz fractions.
        N = 
    </p>
    <canvas id="collatz1dCanvas"></canvas>
    <p>
        The points look quite uniformly distributed to me. If I squint, then <i>maybe</i> I can see some structure, but it's hard to describe
        and I could be imagining it.
    </p>
    <p>
        After making this plot I remembered a nice trick I read about as a teenager in James Gleick's fantastic book "Chaos"
        (I think the idea might have been attributed to Feigenbaum). The trick is that, when you have a sequence of numbers
        that seem random, you should try treating subsequent pairs of numbers as coordinates in a 2D plot. For our sequence of "Collatz fractions",
        f<sub>n</sub>, we would plot the points (f<sub>n</sub>, f<sub>n+1</sub>) for n = 1, 2, 3, ... N.
    </p>
    <p>
        I did that, and was so surprised by the result I first thought I must have made a mistake in my code. But I hadn't, the patterns are real.
        They look almost like some kind of alien "writing" to me, and there's so much beautiful self-similarity in them.
        To dig deeper into the structure, I added a way to color points that match simple javascript rules. Here's an interactive version of that,
        with a few of my colorizing rules. You can add more of your own too!
    </p>
    <ul id="color-rules">
        <li>
            
            
            
        </li>
        <li>
            
            
            
        </li>
        <li>
            
            
            
        </li>
        <li>
            
             <!-- sixty-ten! -->
            
        </li>
        <li>
            
            
            
        </li>
    </ul>
    <p>
        Here's the plot for the first N integers, where N =  If you zoom far enough to see gaps between points, increase N (but the plot will be slower!)
        </p>
        <canvas id="collatz2dCanvas"></canvas>
    
    <p>
        I find this incredibly fun to play with, zooming deep in and adding new color rules and seeing how they change the plot.
        Depending on what device you're viewing this on, you should be able to pinch or scroll to zoom in and out, and drag it around to see different parts of it.
    </p>
    <p>
        I had already done a quick literature search to see if anyone else had done something similar, but hadn't found anything so far.
        I wrote down some ideas I thought might possibly turn into a proof of the self-similarity, but it didn't look trivial to me.
        So, before really getting to work on that, I decided to try again with the literature search. This time I used ChatGPT's new "Deep Research" feature.
        It thought about it for a long time, doing a bunch of searches, and eventually replied with a list of papers it thought might be promising.
        Most of them actually weren't, but one of them was an exact match. <a href="https://arxiv.org/abs/1805.00133">This 2019 paper by French mathematician
        Olivier Rozier</a> contains a plot that looks exactly the same as mine! It was really fun to see it again in a different context, and Rozier does prove some
        self-similarity results. Rozier's paper also cites a
        <a href="https://www.researchgate.net/publication/255633359_A_fractal_set_associated_with_the_Collatz_problem">2007 paper by Yukihiro Hashimoto</a>
        that has the same plot again.
        Neither Rozier's nor Hashimoto's plot is constructed the same way as mine, even though they look the same. Both of these papers build the plot
        starting with 2-adic numbers and only later map those to fractions. I would guess the 2-adic approach probably makes their proofs nicer,
        but jumping straight to fractions is likely to be easier if you've never seen p-adic numbers before (or if, like me, it's been long enough that
        you've forgotten most of what you learned about them!).
    </p>
    <p>
        If you find something interesting in the plot — a nice pattern, a structure, something weird — please <a href="https://abstractnonsense.com/">let me know</a>! I'd love for this to spark
        your imagination and maybe even lead to some new discoveries. I think this plot is beautiful, and I hope you enjoy it too.
    </p>
    
    <p>
        Thanks to Tatiana Moorier and Emmett Shear for reading drafts of this.
    </p>
    
    <p>
        <span>[1]</span> I've been telling people for years if businesses want employees to have better ideas, they should have more showers in their offices. So far everyone seems
        to think I'm joking. I'm not.
    </p>
    <p>
        <span>[2]</span> A reasonable question at this point is "Why do mathematicians care about proving the Collatz Conjecture?"
        One answer is just "because it's there!"
        But there are more serious answers too. Here's one: After the proof of Fermat's Last Theorem in 1995, the Collatz Conjecture is arguably the best example of a problem
        that's very easy to state, and so far completely impossible to prove. Often when mathematicians have encountered problems like this in the past,
        they've had to invent new kinds of "mathematical machinary" to solve them. In high school we learn some basic kinds of machinary -- proof by induction,
        proof by contradiction, and so on. Later, in college, we might learn Cantor's diagonal argument, or the epsilon-delta approach to limits.
        It seems likely, since we still haven't solved the Collatz Conjecture, that we will need some new kind of machinary to solve it.
        I think it's the anticipation of the discovery of that new machinary that keeps mathematicians interested in the Collatz Conjecture.
    </p>
    <p>
        <span>[3]</span> I've seen other people keep track of the branches and use them to draw some really pretty pictures,
        e.g. turning left or right at each step depending on which branch was taken.
        The problem with that is you can't use it to see the branching structure for huge numbers of inputs all at the same time.
    </p>
    <br>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DDoSecrets publishes 410 GB of heap dumps, hacked from TeleMessage (624 pts)]]></title>
            <link>https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/</link>
            <guid>44036647</guid>
            <pubDate>Tue, 20 May 2025 00:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/">https://micahflee.com/ddosecrets-publishes-410-gb-of-heap-dumps-hacked-from-telemessages-archive-server/</a>, See on <a href="https://news.ycombinator.com/item?id=44036647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article>
                    <header>
                        
                            <figure>
        <img srcset="https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=300 300w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=720 720w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=960 960w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1200 1200w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000 2000w,
                    https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=2000" sizes="(max-width: 1200px) 100vw, 1200px" src="https://images.unsplash.com/photo-1642204705127-accc0dcc5779?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDN8fGxhbmRmaWxsfGVufDB8fHx8MTc0NzY3MTQzMnww&amp;ixlib=rb-4.1.0&amp;q=80&amp;w=1200" alt="DDoSecrets publishes 410 GB of heap dumps, hacked from TeleMessage's archive server">
            <figcaption><span>Photo by </span><a href="https://unsplash.com/@katertottz?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Katie Rodriguez</span></a><span> / </span><a href="https://unsplash.com/?utm_source=ghost&amp;utm_medium=referral&amp;utm_campaign=api-credit"><span>Unsplash</span></a></figcaption>
    </figure>
                    </header>

                <section>
                    <p>This morning, Distributed Denial of Secrets <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">published</a> 410 GB of data hacked from TeleMessage, the Israeli firm that makes modified versions of Signal, WhatsApp, Telegram, and WeChat that centrally archive messages. Because the data is sensitive and full of PII, DDoSecrets is only sharing it with journalists and researchers.</p><p>There's a lot of background, so here's a quick timeline of events with relevant links:</p><ul><li>March: Then-national security advisor Mike Waltz <a href="https://web.archive.org/web/20250325174744/https://www.theatlantic.com/politics/archive/2025/03/trump-administration-accidentally-texted-me-its-war-plans/682151/" rel="noreferrer">invited</a> a journalist into a Signal group where they planned <a href="https://zeteo.com/p/signal-chat-war-crimes-revealed-yemen-trump-admin" rel="noreferrer">war crimes</a>. This led to Congressional <a href="https://www.pbs.org/newshour/politics/watch-ratcliffe-gabbard-patel-testify-to-senate-after-war-plans-revealed-to-journalist-in-chat" rel="noreferrer">hearings</a> about Trump officials using Signal groups to discuss classified information.</li><li>May 1: Waltz (the day he was demoted from position of national security advisor) was photographed using TM SGNL, a modified version of Signal made by TeleMessage. He had texts up with Tusli Gabbard, JD Vance, and Marco Rubio.</li><li>May 3: I <a href="https://micahflee.com/heres-the-source-code-for-the-unofficial-signal-app-used-by-trump-officials/" rel="noreferrer">published</a> the source code of the TM SGNL to GitHub.</li><li>May 4: TeleMessage got hacked, which I <a href="https://micahflee.com/the-signal-clone-the-trump-admin-uses-was-hacked/" rel="noreferrer">reported</a> in 404 Media with Joseph Cox. </li><li>May 5: TeleMessage got hacked again by someone else, as NBC News <a href="https://www.nbcnews.com/tech/security/telemessage-suspends-services-hackers-say-breached-app-rcna204925" rel="noreferrer">reported</a>.</li><li>May 6: I <a href="https://micahflee.com/despite-misleading-marketing-israeli-company-telemessage-used-by-trump-officials-can-access-plaintext-chat-logs/" rel="noreferrer">published analysis</a> of the TM SGNL source code, along with some of the hacked data, that prove the TeleMessage lied about its products supporting end-to-end encryption.</li><li>May 18: I <a href="https://micahflee.com/how-the-knock-off-signal-app-used-by-trump-officials-got-hacked-in-20-minutes/" rel="noreferrer">published details</a> about the TeleMessage server's vulnerability in WIRED. <em>TLDR: if anyone on the internet loaded the URL <strong>archive.telemessage.com/management/heapdump</strong>, they would download a Java heap dump from TeleMessage's archive server, containing plaintext chat logs, among other things.</em></li></ul><p>Now, DDoSecrets has published 410 GB of these TeleMessage heap dumps. Here's the DDoSecrets description of <a href="https://ddosecrets.com/article/telemessage" rel="noreferrer">the release</a>:</p><blockquote>Thousands of heap dumps taken May 4, 2025 from TeleMessage, which produces software used to archive encrypted messaging apps such as Signal and WhatsApp. The service came to public notice in 2025 when it was reported that former national security adviser Mike Waltz used TeleMessage while communicating with members of the Trump administration, including Vice President JD Vance and Director of National Intelligence Tulsi Gabbard. TeleMessage has been used by the federal government since at least February 2023.<p>Some of the archived data includes plaintext messages while other portions only include metadata, including sender and recipient information, timestamps, and group names. To facilitate research, Distributed Denial of Secrets has extracted the text from the original heap dumps.</p></blockquote><p>It seems that the SignalGate saga of staggering incompetence is not yet complete. I'm digging into this data right now. It's bonkers.</p><p><em>Note: I'm a member of the DDoSecrets collective. If you can, </em><a href="https://donorbox.org/ddosecrets" rel="noreferrer"><em>donate</em></a><em>! DDoSecrets operates on a shoestring budget and does incredibly impactful work.</em></p>
                    
                </section>

                    

                
            </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[is-even-ai – Check if a number is even using the power of AI (280 pts)]]></title>
            <link>https://www.npmjs.com/package/is-even-ai</link>
            <guid>44036438</guid>
            <pubDate>Tue, 20 May 2025 00:19:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npmjs.com/package/is-even-ai">https://www.npmjs.com/package/is-even-ai</a>, See on <a href="https://news.ycombinator.com/item?id=44036438">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><main id="main"> <div id="top"><div><p><span>1.0.5<!-- -->&nbsp;•&nbsp;</span><span>Public</span><span>&nbsp;•&nbsp;Published <time datetime="2024-10-19T03:05:20.686Z" title="10/19/2024, 3:05:20 AM">7 months ago</time></span></p></div><ul role="tablist" aria-owns="package-tab-readme package-tab-code package-tab-dependencies package-tab-dependents package-tab-versions"><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=readme" aria-selected="true" role="tab" aria-controls="tabpanel-readme" id="package-tab-readme" tabindex="0"><span> Readme</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=code" aria-selected="false" role="tab" aria-controls="tabpanel-explore" id="package-tab-code" tabindex="-1"><span>Code <span><span>Beta</span></span></span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=dependencies" aria-selected="false" role="tab" aria-controls="tabpanel-dependencies" id="package-tab-dependencies" tabindex="-1"><span>1 Dependency</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=dependents" aria-selected="false" role="tab" aria-controls="tabpanel-dependents" id="package-tab-dependents" tabindex="-1"><span>0 Dependents</span></a></li><li role="presentation"><a href="https://www.npmjs.com/package/is-even-ai?activeTab=versions" aria-selected="false" role="tab" aria-controls="tabpanel-versions" id="package-tab-versions" tabindex="-1"><span>7 Versions</span></a></li></ul><p><span><div id="tabpanel-readme" aria-labelledby="package-tab-readme" role="tabpanel" data-attribute=""><article><div id="readme"><div><h2>is-even-ai</h2></div>
<p><a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/5ef7ab75d332b2a150f7abd92543b02f404a373538618c481d65e0878bafbce4/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/is-even-ai.svg?style=flat"></a>
<a href="https://github.com/Calvin-LL/is-even-ai/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/a534d8ca992ccfede6a1ec046afa59d5527e48b05af7e0e3d33ac804adbfe044/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f6c2f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM License" data-canonical-src="https://img.shields.io/npm/l/is-even-ai.svg?style=flat"></a>
<a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/ee5ff62aeacf7bd48fd4fb5abd217a318db042c59dcfb82b2b9e9be79a3c0a1d/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f64742f69732d6576656e2d61692e7376673f7374796c653d666c6174" alt="NPM Downloads" data-canonical-src="https://img.shields.io/npm/dt/is-even-ai.svg?style=flat"></a></p>
<p>Check if a number is even using the power of ✨AI✨.</p>
<p>Uses OpenAI's GPT-3.5-turbo model under the hood to determine if a number is even.</p>
<p>For all those who want to use AI in their product but don't know how.</p>
<p>Inspired by the famous <a href="https://www.npmjs.com/package/is-even" rel="nofollow"><code>is-even</code></a> npm package and <a href="https://twitter.com/erenbali/status/1766602689863950658" rel="nofollow">this tweet</a>.</p>
<div><h2>Installation</h2></div>
<p><a href="https://www.npmjs.com/package/is-even-ai" rel="nofollow">This package is on npm.</a></p>

<div><h2>Usage</h2></div>
<div><pre><span>import</span> <span>{</span>
  <span>areEqual</span><span>,</span>
  <span>areNotEqual</span><span>,</span>
  <span>isEven</span><span>,</span>
  <span>isGreaterThan</span><span>,</span>
  <span>isLessThan</span><span>,</span>
  <span>isOdd</span><span>,</span>
  <span>setApiKey</span><span>,</span>
<span>}</span> <span>from</span> <span>"is-even-ai"</span><span>;</span>

<span>// won't need this if you have OPENAI_API_KEY in your environment</span>
<span>setApiKey</span><span>(</span><span>"YOUR_API_KEY"</span><span>)</span><span>;</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEven</span><span>(</span><span>2</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEven</span><span>(</span><span>3</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isOdd</span><span>(</span><span>4</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isOdd</span><span>(</span><span>5</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>6</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areNotEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>areNotEqual</span><span>(</span><span>7</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isGreaterThan</span><span>(</span><span>8</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isGreaterThan</span><span>(</span><span>7</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isLessThan</span><span>(</span><span>9</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isLessThan</span><span>(</span><span>8</span><span>,</span> <span>9</span><span>)</span><span>)</span><span>;</span> <span>// true</span></pre></div>
<p>for more advance usage like changing which model to use and setting the temperature, use <code>IsEvenAiOpenAi</code> instead</p>
<div><pre><span>import</span> <span>{</span> <span>IsEvenAiOpenAi</span> <span>}</span> <span>from</span> <span>"is-even-ai"</span><span>;</span>

<span>const</span> <span>isEvenAiOpenAi</span> <span>=</span> <span>new</span> <span>IsEvenAiOpenAi</span><span>(</span>
  <span>{</span>
    <span>// won't need this if you have OPENAI_API_KEY in your environment</span>
    <span>apiKey</span>: <span>"YOUR_API_KEY"</span><span>,</span>
  <span>}</span><span>,</span>
  <span>{</span>
    <span>model</span>: <span>"gpt-3.5-turbo"</span><span>,</span>
    <span>temperature</span>: <span>0</span><span>,</span>
  <span>}</span>
<span>)</span><span>;</span>

<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isEven</span><span>(</span><span>2</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isEven</span><span>(</span><span>3</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isOdd</span><span>(</span><span>4</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isOdd</span><span>(</span><span>5</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>6</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areNotEqual</span><span>(</span><span>6</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>areNotEqual</span><span>(</span><span>7</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isGreaterThan</span><span>(</span><span>8</span><span>,</span> <span>7</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isGreaterThan</span><span>(</span><span>7</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isLessThan</span><span>(</span><span>8</span><span>,</span> <span>9</span><span>)</span><span>)</span><span>;</span> <span>// true</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>await</span> <span>isEvenAiOpenAi</span><span>.</span><span>isLessThan</span><span>(</span><span>9</span><span>,</span> <span>8</span><span>)</span><span>)</span><span>;</span> <span>// false</span></pre></div>
<div><h2>Supported AI platforms</h2></div>
<p>Feel free to make a PR to add more AI platforms.</p>
<ul>
<li>[x] <a href="https://openai.com/" rel="nofollow">OpenAI</a> via <code>IsEvenAiOpenAi</code>
</li>
</ul>
<div><h2>Supported methods</h2></div>
<ul>
<li><code>isEven(n: number)</code></li>
<li><code>isOdd(n: number)</code></li>
<li><code>areEqual(a: number, b: number)</code></li>
<li><code>areNotEqual(a: number, b: number)</code></li>
<li><code>isGreaterThan(a: number, b: number)</code></li>
<li><code>isLessThan(a: number, b: number)</code></li>
</ul>
</div></article></div></span><span aria-live="polite"></span></p></div> </main></div></div>]]></description>
        </item>
    </channel>
</rss>