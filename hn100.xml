<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 21 Oct 2025 18:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[ChatGPT Atlas (225 pts)]]></title>
            <link>https://chatgpt.com/atlas</link>
            <guid>45658479</guid>
            <pubDate>Tue, 21 Oct 2025 17:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chatgpt.com/atlas">https://chatgpt.com/atlas</a>, See on <a href="https://news.ycombinator.com/item?id=45658479">Hacker News</a></p>
Couldn't get https://chatgpt.com/atlas: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Fallout from the AWS Outage: Smart Mattresses Go Rogue and Ruin Sleep Worldwide (175 pts)]]></title>
            <link>https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide</link>
            <guid>45658056</guid>
            <pubDate>Tue, 21 Oct 2025 16:50:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide">https://quasa.io/media/the-strangest-fallout-from-the-aws-outage-smart-mattresses-go-rogue-and-ruin-sleep-worldwide</a>, See on <a href="https://news.ycombinator.com/item?id=45658056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the grand tradition of tech outages turning everyday life into a farce, Monday's massive Amazon Web Services (AWS) disruption didn't just take down <a href="https://quasa.io/media/instagram-launches-location-sharing-maps-inspired-by-snapchat">Snapchat</a>, <a href="https://quasa.io/media/ryan-kaji-the-13-year-old-multimillionaire-building-a-virtual-empire-in-roblox">Roblox</a>, and <a href="https://quasa.io/media/fortnite-set-to-return-to-iphone-in-australia-as-epic-wins-partial-victory-against-apple-and-google">Fortnite</a> - it left thousands of sleep-deprived users sweating bullets in their own beds. While the world scrambled to reload their feeds, owners of Eight Sleep's high-tech Pod3 mattress covers discovered a chilling reality: their "smart" sleep sanctuaries had no offline mode. Zero. Zilch. In a world where even your fridge can survive a Wi-Fi blackout, who knew your bed couldn't?</p>

<p><img alt="" height="188" src="https://quasa.io/storage/photos/00/photo_2025-10-20_19-32-46.jpg" width="300">The outage, which AWS confirmed stemmed from "increased error rates and latencies" in its US-EAST-1 region, rippled across the internet starting around 3 a.m. ET on October 20, 2025. By mid-morning, Downdetector had logged over eight million reports, with everything from banking apps to gaming platforms grinding to a halt.</p>

<p>But amid the chaos, one corner of the web lit up with uniquely absurd complaints: Eight Sleep's support site and social channels flooded with pleas from users whose mattresses had effectively gone on strike.</p>

<p>These $2,000+ gadgets, billed as AI-powered sleep coaches that track heart rate, monitor sleep stages, and dynamically adjust temperature via water-cooled coils, suddenly reverted to being glorified foam bricks.</p>

<p><img alt="" height="447" src="https://quasa.io/storage/photos/00/image%20-%202025-10-20T194135.824.jpg" width="300">Picture this: You're tucked in, ready for a night of optimized REM cycles, when your app pings an error. No more tweaking the chill to a crisp 55°F or firing up the "cool mode" for those midnight hot flashes.</p>

<p>The core temperature control? Utterly crippled without the cloud. Users reported the app freezing on loading screens, refusing to connect, and leaving them stranded in whatever thermal hell their last setting dictated.</p>

<p>Eight Sleep's system, which relies on backend servers for everything from real-time adjustments to data syncing, had no fallback. "It's unacceptable," fumed one early complainant on X, echoing the frustration of many who shelled out for "seamless" smart sleep only to face analog purgatory.</p>

<p>The hits kept coming. Smart sleep tracking? Dead in the water—no logging of phases, no biometric insights, just a void where your sleep score should be. Preset schedules, like the cheekily named "Prepare Bed for Sleep" routine that cues gentle warming or ambient vibes, fizzled out entirely, as all automations demand an internet lifeline to Eight Sleep's servers.</p>

<p>Even physical controls fared poorly: Touch panels on the Hub (the mattress's brain box) became unresponsive or glitchy, with some users noting they were "extremely inconvenient" at best - designed more as app backups than standalone saviors. And in the outage's cruelest twist, a handful of Pods straight-up froze. One Reddit thread devolved into a chorus of "my bed is bricked," with owners unable to reboot without cloud clearance.</p>

<p><img alt="" height="447" src="https://quasa.io/storage/photos/00/image%20-%202025-10-20T194138.665.jpg" width="300">Then there's the tweet that broke the internet's funny bone - and possibly a few marriages. Tech enthusiast Alex Browne, armed with an Eight Sleep Pod3, had programmed his mattress to preemptively heat up by +9°F above room temperature before bedtime. "I like it warm to ease in," he explained in a viral post that racked up thousands of likes and eye-rolls.</p>

<p>But when AWS went dark, the system locked into that toasty preset, disabling any cooling override. Browne spent the night marinating in his own perspiration, tweeting updates like a man betrayed: "Backend outage means I'm sleeping in a sauna.</p>

<p>Eight Sleep confirmed - no offline mode yet, but they're 'working on it'." Commenters piled on with dystopian jabs: "Next up: Subscription paywalls for your pillow fluffiness" and "Jeff Bezos is personally cranking my thermostat." By evening, as AWS reported "significant recovery," Browne's saga had morphed into a meme goldmine, spotlighting the absurdity of outsourcing your snooze to the cloud.</p>

<p>Eight Sleep isn't alone in this IoT vulnerability - AWS powers a staggering chunk of the smart home ecosystem, from Ring doorbells to Alexa plugs, all of which blinked out during the outage.</p>

<p>But mattresses? That's peak irony. Humans have napped on rocks for millennia without needing server pings, yet here we are, one faulty data center away from nocturnal disaster. The company, which touts over 50 clinical studies on its tech, has faced prior scrutiny too—like a 2024 security report uncovering exposed AWS keys that let engineers remotely SSH into users' beds, raising hackles about data privacy and backdoor access. (Imagine your ex tweaking the vibes from afar.)</p>

<p><strong>Also read:</strong></p>

<ul>
	<li><a href="https://quasa.io/media/dream-job-alert-ocean-city-police-seek-weed-volunteers-for-impairment-training-slots-filled-in-hours">Dream Job Alert: Ocean City Police Seek Weed Volunteers for Impairment Training – Slots Filled in Hours!</a></li>
	<li><a href="https://quasa.io/media/courtship-ends-paramount-skydance-prepares-for-massive-layoffs">Courtship Ends! Paramount Skydance Prepares for Massive Layoffs</a></li>
	<li><a href="https://quasa.io/media/youtube-s-partner-program-loses-its-shine-selling-chocolate-bars-is-now-more-lucrative-for-bloggers">YouTube’s Partner Program Loses Its Shine: Selling Chocolate Bars Is Now More Lucrative for Bloggers</a></li>
</ul>

<p>As the dust settles - with AWS mostly back online by 6 a.m. ET and Eight Sleep restoring controls - this episode serves as a wake-up call (pun intended). Smart tech promises utopia, but without robust offline fallbacks, it's just expensive fragility. For now, if you're an Eight Sleep devotee, keep a fan handy - and maybe a low-tech backup plan. Because in the words of one sweat-soaked survivor: "What won't they cloud-ify next? My dreams?"</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Public trust demands open-source voting systems (153 pts)]]></title>
            <link>https://www.voting.works/news/public-trust-demands-open-source-voting-systems</link>
            <guid>45657431</guid>
            <pubDate>Tue, 21 Oct 2025 16:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.voting.works/news/public-trust-demands-open-source-voting-systems">https://www.voting.works/news/public-trust-demands-open-source-voting-systems</a>, See on <a href="https://news.ycombinator.com/item?id=45657431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><div><p>October 10, 2025</p><p> — </p><p>Ben Adida</p></div><div><h4>Yesterday, news broke that Dominion Voting Systems <a href="https://apnews.com/article/dominion-voting-liberty-vote-2020-conspiracy-theories-fed1e2d7f00b264bf5f8e01a106124f1">was sold to a new company, Liberty Vote</a>. Dominion, the second-largest voting systems vendor in the US that currently tabulates 1 in 5 American votes, is no more.</h4><p><br>‍<a href="https://libertyvote.com/">On its website</a>, Liberty Vote says, "We are turning the page and beginning the vital work of restoring faith in American elections."&nbsp;</p><p>There is indeed a crisis of trust in American elections. As the saying goes, trust takes years to build, seconds to break, and forever to repair. We urgently need a new strategy that repairs voter trust. American freedom and democracy depend on it.</p><p>We must start with a foundational commitment to transparency. Every voting machine vendor in the US claims to be transparent. It may come as a surprise, then, that most Americans vote on machines that run proprietary, secret software! A voting machine that every American can trust must run software that is fully open to public scrutiny.&nbsp;</p><p>Today, VotingWorks makes the only open-source voting equipment in the US. Open-source is the modern standard for public-trust and high-security software. Signal, the most secure and battle-tested messaging app, is open-source. The US Military <a href="https://dodcio.defense.gov/portals/0/documents/library/softwaredev-opensource.pdf">recommends open-source</a>. A modern voting system needs to be open-source.</p><p>Liberty Vote, and every other vendor of voting machines in the US, can fulfill their commitment to transparency by making their technology open-source today. If every vendor takes this opportunity, together, we can turn the page and launch a new generation of voting systems every American can trust.&nbsp;</p><p>‍</p><h2>About Voting Works</h2><p>Read about the company: <a href="https://voting.works/">https://voting.works</a></p><p>View our complete source code: <a href="https://github.com/votingworks">https://github.com/votingworks</a></p><p>Contact us with questions: <a href="mailto:hello@voting.works">hello@voting.works</a>&nbsp;</p><p>‍</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple alerts exploit developer that his iPhone was targeted with gov spyware (125 pts)]]></title>
            <link>https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/</link>
            <guid>45657302</guid>
            <pubDate>Tue, 21 Oct 2025 15:52:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/">https://techcrunch.com/2025/10/21/apple-alerts-exploit-developer-that-his-iphone-was-targeted-with-government-spyware/</a>, See on <a href="https://news.ycombinator.com/item?id=45657302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Earlier this year, a developer was shocked by a message that appeared on his personal phone: “Apple detected a targeted mercenary spyware attack against your iPhone.”&nbsp;&nbsp;</p>

<p>“I was panicking,” Jay Gibson, who asked that we don’t use his real name over fears of retaliation, told TechCrunch.&nbsp;&nbsp;</p>







<p>Gibson, who until recently built surveillance technologies for Western government hacking tools maker Trenchant, may be the first documented case of someone who builds exploits and spyware being themselves targeted with spyware.&nbsp;</p>

<p>“What the hell is going on? I really didn’t know what to think of it,” said Gibson, adding that he turned off his phone and put it away on that day, March 5. “I went immediately to buy a new phone. I called my dad. It was a mess. It was a huge mess.”&nbsp;&nbsp;</p>







<p>At Trenchant, Gibson worked on developing iOS <a href="https://techcrunch.com/2025/04/25/techcrunch-reference-guide-to-security-terminology/#zero-day" target="_blank" rel="noreferrer noopener">zero-days</a>, meaning finding<strong> </strong>vulnerabilities and developing tools capable of exploiting them that are not known to the vendor who makes the affected hardware or software, such as Apple. &nbsp;</p>

<p>“I have mixed feelings of how pathetic this is, and then extreme fear because once things hit this level, you never know what’s going to happen,” he told TechCrunch.&nbsp;&nbsp;</p>

<p>But the ex-Trenchant employee may not be the only exploit developer targeted with spyware. According to three sources who have direct knowledge of these cases, there have been other spyware and exploit developers in the last few months who have received notifications from Apple alerting them that they were targeted with spyware.&nbsp;</p>

<p>Apple did not respond to a request for comment from TechCrunch.&nbsp;</p>
<div>
			<h4>Contact Us</h4><p>
			Do you have more information about the alleged leak of Trenchant hacking tools? Or about this developer’s story? From a non-work device, you can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram, Keybase and Wire @lorenzofb, or <a href="mailto:lorenzo@techcrunch.com/">by email</a><a href="mailto:lorenzo@techcrunch.com/">.</a> 		</p></div>
		

<p>The targeting of Gibson’s iPhone shows that the proliferation of zero-days and spyware is starting to ensnare more types of victims.&nbsp;&nbsp;</p>

<p>Spyware and zero-day makers have historically claimed their tools are only deployed by vetted government customers against criminals and terrorists. But for the past decade, researchers at the University of Toronto’s digital rights group <a href="https://techcrunch.com/2025/06/12/researchers-confirm-two-journalists-were-hacked-with-paragon-spyware/" target="_blank" rel="noreferrer noopener">Citizen Lab</a>, <a href="https://techcrunch.com/2025/03/28/again-and-again-nso-groups-customers-keep-getting-their-spyware-operations-caught/" target="_blank" rel="noreferrer noopener">Amnesty International</a>, and <a href="https://techcrunch.com/2023/05/25/researchers-say-they-found-spyware-used-in-war-for-the-first-time/" target="_blank" rel="noreferrer noopener">other organizations</a>, have found <a href="https://github.com/GranittHQ/data-pegasus-victims/blob/main/data-pegasus-victims.csv" target="_blank" rel="noreferrer noopener nofollow">dozens of cases</a> where governments used these tools to target <a href="https://techcrunch.com/2021/08/24/nso-pegasus-bahrain-iphone-security/" target="_blank" rel="noreferrer noopener">dissidents</a>, <a href="https://techcrunch.com/2022/04/05/nso-pegasus-jordan-apple/" target="_blank" rel="noreferrer noopener">journalists</a>, <a href="https://techcrunch.com/2025/02/11/another-person-targeted-by-paragon-spyware-comes-forward/" target="_blank" rel="noreferrer noopener">human rights defenders</a>, and <a href="https://techcrunch.com/2024/12/04/business-leaders-among-pegasus-spyware-victims-says-security-firm/" target="_blank" rel="noreferrer noopener">political rivals</a> all over the world.&nbsp;&nbsp;&nbsp;</p>







<p>The closest public cases of security researchers being targeted by hackers happened in <a href="https://blog.google/threat-analysis-group/new-campaign-targeting-security-researchers/" target="_blank" rel="noreferrer noopener nofollow">2021</a> and <a href="https://arstechnica.com/information-technology/2023/03/security-researchers-are-again-in-the-crosshairs-of-north-korean-hackers/" target="_blank" rel="noreferrer noopener nofollow">2023</a>, when North Korean government hackers were caught targeting security researchers working in vulnerability research and development.&nbsp;</p>

<h2 id="h-suspect-in-leak-investigation-nbsp"><strong>Suspect in leak investigation</strong>&nbsp;</h2>

<p>Two days after receiving the Apple threat notification, Gibson contacted a forensic expert with extensive experience investigating spyware attacks. After performing an initial analysis of Gibson’s phone, the expert did not find any signs of infection, but still recommended a deeper forensic analysis of the exploit developer’s phone.&nbsp;&nbsp;</p>







<p>A forensic analysis would have entailed sending the expert a complete backup of the device, something Gibson said he was not comfortable with. &nbsp;</p>

<p>“Recent cases are getting tougher forensically, and some we find nothing on. It may also be that the attack was not actually fully sent after the initial stages, we don’t know,” the expert told TechCrunch.&nbsp;</p>

<p>Without a full forensic analysis of Gibson’s phone, ideally one where investigators found traces of the spyware and who made it, it’s impossible to know why he was targeted or who targeted him.&nbsp;&nbsp;</p>

<p>But Gibson told TechCrunch that he believes the threat notification he received from Apple is connected to the circumstances of his departure from Trenchant, where he claims that the company designated him as a scapegoat for a damaging leak of internal tools.&nbsp;&nbsp;</p>

<p>Apple <a href="https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/" target="_blank" rel="noreferrer noopener">sends</a> out <a href="https://techcrunch.com/2025/04/30/apple-notifies-new-victims-of-spyware-attacks-across-the-world/" target="_blank" rel="noreferrer noopener">threat</a> <a href="https://techcrunch.com/2025/07/22/apple-alerted-iranians-to-iphone-spyware-attacks-say-researchers/" target="_blank" rel="noreferrer noopener">notifications</a> specifically for when it has evidence that a person was targeted by a <a href="https://techcrunch.com/2024/12/20/why-apple-sends-spyware-victims-to-this-nonprofit-security-lab/" target="_blank" rel="noreferrer noopener">mercenary spyware attack</a>. This kind of surveillance technology is often invisibly and remotely planted on someone’s phone without their knowledge by exploiting vulnerabilities in the phone’s software, exploits that <a href="https://techcrunch.com/2024/04/06/price-of-zero-day-exploits-rises-as-companies-harden-products-against-hackers/" target="_blank" rel="noreferrer noopener">can be worth millions of dollars</a> and can take months to develop. Law enforcement and intelligence agencies typically have the legal authority to deploy spyware on targets, not the spyware makers themselves.&nbsp;</p>

<p>Sara Banda, a spokesperson for Trenchant’s parent company L3Harris, declined to comment for this story when reached by TechCrunch before publication. &nbsp;</p>







<p>A month before he received Apple’s threat notification, when Gibson was still working at Trenchant, he said he was invited to go to the company’s London office for a team-building event.&nbsp;&nbsp;</p>

<p>When Gibson arrived February 3, he was immediately summoned into a meeting room to speak via video call with Peter Williams, Trenchant’s then-general manager who was known inside the company as “Doogie.” (In 2018, defense contractor L3Harris <a href="http://cyberscoop.com/l3-acquires-azimuth-and-linchpin/" target="_blank" rel="noreferrer noopener nofollow">acquired</a> zero-day makers Azimuth and Linchpin Labs, <a href="https://www.vice.com/en/article/iphone-zero-days-inside-azimuth-security/" target="_blank" rel="noreferrer noopener nofollow">two sister startups</a> that merged to become Trenchant.)&nbsp;</p>







<p>Williams told Gibson the company suspected he was double employed and was thus suspending him. All of Gibson’s work devices would be confiscated and analyzed as part of an internal investigation into the allegations. Williams could not be reached for comment.&nbsp;</p>

<p>“I was in shock. I didn’t really know how to react because I couldn’t really believe what I was hearing,” said Gibson, who explained that a Trenchant IT employee then went to his apartment to pick up his company-issued equipment.&nbsp;&nbsp;</p>

<p>Around two weeks later, Gibson said Williams called and told him that following the investigation, the company was firing him and offering him a settlement agreement and payment. Gibson said Williams declined to explain what the forensic analysis of his devices had found, and essentially told him he had no choice but to sign the agreement and depart the company.&nbsp;</p>

<p>Feeling like he had no alternative, Gibson said he went along with the offer and signed.&nbsp;&nbsp;</p>

<p>Gibson told TechCrunch he later heard from former colleagues that Trenchant suspected he had leaked some unknown vulnerabilities in Google’s Chrome browser, tools that Trenchant had developed. Gibson, and three former colleagues of his, however, told TechCrunch he did not have access to Trenchant’s Chrome zero-days, given that he was part of the team exclusively developing iOS zero-days and spyware. Trenchant teams only have strictly compartmentalized access to tools related to the platforms they are working on, the people said.&nbsp;&nbsp;</p>

<p>“I know I was a scapegoat. I wasn’t guilty. It’s very simple,” said Gibson. “I didn’t do absolutely anything other than working my ass off for them.”&nbsp;&nbsp;</p>







<p>The story of the accusations against Gibson’ and his subsequent suspension and firing was independently corroborated by three former Trenchant employees with knowledge. &nbsp;</p>

<p>Two of the other former Trenchant employees said they knew details of Gibson’s London trip and were aware of suspected leaks of sensitive company tools.&nbsp;</p>







<p>All of them asked not to be named but believe Trenchant got it wrong.&nbsp;</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Foreign hackers breached a US nuclear weapons plant via SharePoint flaws (136 pts)]]></title>
            <link>https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html</link>
            <guid>45657287</guid>
            <pubDate>Tue, 21 Oct 2025 15:51:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html">https://www.csoonline.com/article/4074962/foreign-hackers-breached-a-us-nuclear-weapons-plant-via-sharepoint-flaws.html</a>, See on <a href="https://news.ycombinator.com/item?id=45657287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span>News Analysis</span></p><p><span>Oct 20, 2025</span><span>8 mins</span></p><p><span><span>Cyberattacks</span></span><span><span>Data Breach</span></span><span><span>Government IT</span></span></p></div><article id="post-4074962">
	<div>
			<div>
						<div>
			<h2>
				A foreign actor infiltrated the National Nuclear Security Administration’s Kansas City National Security Campus through vulnerabilities in Microsoft’s SharePoint browser-based app, raising questions about the need to solidify further federal IT/OT security protections.			</h2>
			
		</div>					
						<div id="remove_no_follow">
		<div>




<p>A foreign threat actor infiltrated the <a href="https://kcnsc.doe.gov/">Kansas City National Security Campus (KCNSC)</a>, a key manufacturing site within the National Nuclear Security Administration (NNSA), exploiting unpatched Microsoft SharePoint vulnerabilities, according to a source involved in an August incident response at the facility.</p>



<p>The breach targeted a plant that produces the vast majority of critical non-nuclear components for US nuclear weapons under the NNSA, a semi-autonomous agency within the Department of Energy (DOE) that oversees the design, production, and maintenance of the nation’s nuclear weapons. Honeywell Federal Manufacturing &amp; Technologies (FM&amp;T) manages the Kansas City campus under contract to the NNSA.</p>



<p>The Kansas City campus, Honeywell FM&amp;T, and the Department of Energy did not respond to repeated requests for comment throughout September, well before the current government shutdown. NSA public affairs officer Eddie Bennett did respond, saying, “We have nothing to contribute,” and referred CSO back to the DOE.</p>
</div>
							
							<div>


<p>Although it is unclear whether the attackers were a Chinese nation-state actor or Russian cybercriminals — the two most likely culprits — experts say the incident drives home the importance of securing systems that protect operational technology from exploits that primarily affect IT systems.</p>

		

			


<h2 id="how-the-breach-unfolded">How the breach unfolded</h2>



<p>The attackers exploited <a href="https://www.csoonline.com/article/4025691/microsoft-sharepoint-zero-day-breach-hits-on-prem-servers.html">two recently disclosed Microsoft SharePoint vulnerabilities</a> — CVE-2025-53770, a spoofing flaw, and CVE-2025-49704, a remote code execution (RCE) bug — both affecting on-premises servers. Microsoft <a href="https://www.microsoft.com/en-us/msrc/blog/2025/07/customer-guidance-for-sharepoint-vulnerability-cve-2025-53770/">issued fixes</a> for the vulnerabilities on July 19.</p>



<p>On July 22, the NNSA <a href="https://www.bloomberg.com/news/articles/2025-07-23/us-nuclear-weapons-agency-breached-in-microsoft-sharepoint-hack">confirmed</a> it was one of the organizations hit by attacks enabled by the SharePoint flaws. “On Friday, July 18th, the exploitation of a Microsoft SharePoint zero-day vulnerability began affecting the Department of Energy,” a DOE spokesperson said.</p>
</div>
							
							<div>


<p>However, the DOE contended at the time, “The department was minimally impacted due to its widespread use of the Microsoft M365 cloud and very capable cybersecurity systems. A very small number of systems were impacted. All impacted systems are being restored.”</p>



<p>By early August, federal responders, including personnel from the NSA, were on-site at the Kansas City facility, the source tells CSO.</p>



<p>Located in Missouri, the <a href="https://www.energy.gov/ea/kansas-city-national-security-campus">KCNSC manufactures</a> non-nuclear mechanical, electronic, and engineered material components used in US nuclear defense systems. It also provides specialized technical services, including metallurgical analysis, analytical chemistry, environmental testing, and simulation modeling.</p>
</div>
							
							<div>


<p>Roughly 80% of the non-nuclear parts in the nation’s nuclear stockpile <a href="https://kcnsc.doe.gov/about-us/overview/">originate from KCNSC</a>. While most design and programmatic details remain classified, the plant’s production role makes it one of the most sensitive facilities in the federal weapons complex.</p>



<h2 id="china-or-russia-conflicting-attribution">China or Russia? Conflicting attribution</h2>



<p>Microsoft <a href="https://www.microsoft.com/en-us/security/blog/2025/07/22/disrupting-active-exploitation-of-on-premises-sharepoint-vulnerabilities/#storm-2603">attributed the broader wave of SharePoint exploitations</a> to three Chinese-linked groups: Linen Typhoon, Violet Typhoon, and a third actor it tracks as Storm-2603. The company said the attackers were preparing to deploy Warlock ransomware across affected systems.</p>



<p>However, the source familiar with the Kansas City incident tells CSO that a Russian threat actor, not a Chinese one, was responsible for the intrusion. Cybersecurity company Resecurity, which was monitoring the SharePoint exploitations, tells CSO that its own data pointed primarily to Chinese nation-state groups, but it does not rule out Russian involvement.</p>
</div>
							
							<div>


<p>Resecurity’s researchers say that while Chinese groups appeared to have developed and deployed the initial zero-day, financially motivated Russian actors may have independently reproduced the exploit before technical details began circulating in late June.</p>



<p>In May, researchers at Viettel Cyber Security <a href="https://www.bleepingcomputer.com/news/microsoft/microsoft-sharepoint-zero-day-exploited-in-rce-attacks-no-patch-available/">demonstrated an attack chaining two SharePoint flaws</a>, CVE-2025-49706 and CVE-2025-49704, at Pwn2Own Berlin. Resecurity researchers tell CSO that those demonstrations likely accelerated the reverse-engineering of the vulnerabilities by multiple threat actors.</p>



<p>Resecurity’s analysts observed early-stage scanning and exploitation activity from infrastructure located in Taiwan, Vietnam, South Korea, and Hong Kong, a distribution pattern consistent with tactics used by Chinese advanced persistent threat (APT) groups to disguise attribution.</p>
</div>
							
							<div>


<p>“The root cause of the SharePoint exploitation is closely related to misuse of the Microsoft Active Protections Program (MAPP) by China,” Resecurity researchers tell CSO. “The most probable perpetrators are Chinese nation-state actors such as Linen Typhoon and Violet Typhoon.”</p>



<p>Still, they say that yet another way that Russia-based threat actors could have acquired knowledge of the vulnerability early on was through underground exchanges or by analyzing network scanning data once the exploit became known. The <a href="https://www.csoonline.com/article/4027971/microsofts-incomplete-sharepoint-patch-led-to-global-exploits-by-china-linked-hackers.html">transition from zero-day to N-day status</a>, they say, opened a window for secondary actors to exploit systems that had not yet applied the patches.</p>



<h2 id="could-the-attack-have-reached-operational-systems">Could the attack have reached operational systems?</h2>



<p>The breach targeted the IT side of the Kansas City campus, but the intrusion raises the question of whether attackers could have moved laterally into the facility’s operational technology (OT) systems, the manufacturing and process control environments that directly support weapons component production.</p>
</div>
							
							<div>


<p>OT cybersecurity specialists interviewed by CSO say that KCNSC’s production systems are likely air-gapped or otherwise isolated from corporate IT networks, significantly reducing the risk of direct crossover. Nevertheless, they caution against assuming such isolation guarantees safety.</p>



<p>“We have to really consider and think through how state actors potentially exploit IT vulnerabilities to gain access to that operational technology,” <a href="https://www.linkedin.com/in/jensovada/">Jen Sovada</a>, general manager of public sector operations at Claroty, speaking generally and not about the specific incident, tells CSO.</p>



<p>“When you have a facility like the KCNSC where they do nuclear weapons lifecycle management — design, manufacturing, emergency response, decommissioning, supply chain management — there are multiple interconnected functions,” Sovada says. “If an actor can move laterally, they could impact programmable logic controllers that run robotics or precision assembly equipment for non-nuclear weapon components.”</p>
</div>
							
							<div>


<p>Such access, Sovada adds, could also affect distribution control systems that oversee quality assurance, or supervisory control and data acquisition (SCADA) systems that manage utilities, power, and environmental controls. “It’s broader than just an IT vulnerability,” she says.</p>



<h2 id="it-ot-convergence-and-the-zero-trust-gap">IT/OT convergence and the zero-trust gap</h2>



<p>The Kansas City incident highlights a systemic problem across the federal enterprise: the disconnect between IT and OT security practices. While the federal government has advanced its zero-trust roadmap for traditional IT networks, similar frameworks for operational environments have lagged, although recent developments point to progress on that front.</p>



<p>“There’s an <a href="https://learn.microsoft.com/en-us/security/zero-trust/deploy/networks">IT fan chart</a> that maps all of the controls for zero trust, segmentation, authentication, and identity management,” Sovada says. “But there’s also an <a href="https://www.meritalk.com/articles/dod-official-sees-new-zero-trust-ot-iot-guidance-in-september/#:~:text=The%20DoD%20official%20also%20provided,OT%20%E2%80%9Clikely%20around%20August.%E2%80%9D">OT fan chart</a> being developed by the Department of Defense that will define comparable controls for zero trust in operational technology. The goal is to marry the two, so that zero trust becomes comprehensive across all network types.”</p>
</div>
							
							<div>


<p>That alignment, she says, is essential to preventing intrusions like the one that struck KCNSC from cascading into physical operations.</p>



<h2 id="even-non-classified-data-theft-holds-strategic-value">Even non-classified data theft holds strategic value</h2>



<p>If the source’s claim of Russian involvement is accurate, the attackers may have been financially motivated ransomware operators rather than state intelligence services. But even in that scenario, the data they accessed could still carry strategic value.</p>



<p>“It would make sense that if it were a ransomware actor and they got this kind of data about nuclear weapons manufacturing, they might pause and hand it off to the appropriate Russian government officials or experts,” Sovada tells CSO.</p>
</div>
							
							<div>


<p>Although there is no evidence that classified information was compromised, even unclassified technical data can have significant implications. “It could be something as simple as requirements documents that may not be classified but reveal the level of precision required for components,” Sovada says. “In weapons manufacturing, a millimeter difference can change a device’s trajectory or the reliability of its arming mechanism.”</p>



<p>Such information could aid adversaries in understanding US weapons tolerances, supply chain dependencies, or manufacturing processes, all of which are sensitive even if not formally secret.</p>



<p>Whether the intruders were Chinese state actors or Russian cybercriminals, the Kansas City breach exposes the fragile intersection of IT and operational security across critical defense infrastructure. As Sovada stresses, “We can’t just think of zero trust as an IT concept anymore. It has to extend into the physical systems that underpin national defense.”</p>
</div>
							
							<div>
									<p><em>Update: The Department of Energy (DOE)&nbsp;<a href="https://url.usb.m.mimecastprotect.com/s/zIpmCk6xoxI592Kxt2fVuGXZyW?domain=edition.cnn.com" target="_blank" rel="noreferrer noopener">confirmed</a>&nbsp;that it is furloughing the vast major of the NNSA’s workers. DOE spokesperson said,&nbsp;“Since its creation in 2000, NNSA has never before furloughed federal workers during funding lapses. We are left with no choice this time. We’ve extended funding as long as we could.”</em></p></div></div>					</div>

			<!--right side bar-->
			<div id="rightrail-wrapper">
					<p>
				SUBSCRIBE TO OUR NEWSLETTER			</p>
							<h3>
				From our editors straight to your inbox			</h3>
							<p>
				Get started by entering your email address below.			</p>
				
	</div>
			<!--right side bar ends here-->

		</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Is Making Us Work More (159 pts)]]></title>
            <link>https://tawandamunongo.dev/posts/2025/10/ai-work-more</link>
            <guid>45656916</guid>
            <pubDate>Tue, 21 Oct 2025 15:19:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tawandamunongo.dev/posts/2025/10/ai-work-more">https://tawandamunongo.dev/posts/2025/10/ai-work-more</a>, See on <a href="https://news.ycombinator.com/item?id=45656916">Hacker News</a></p>
Couldn't get https://tawandamunongo.dev/posts/2025/10/ai-work-more: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs Can Get "Brain Rot" (133 pts)]]></title>
            <link>https://llm-brain-rot.github.io/</link>
            <guid>45656223</guid>
            <pubDate>Tue, 21 Oct 2025 14:24:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llm-brain-rot.github.io/">https://llm-brain-rot.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=45656223">Hacker News</a></p>
<div id="readability-page-1" class="page">


  <div>
            
            
            <p><span><sup>1</sup>Texas A&amp;M University,</span>
              <span><sup>2</sup>University of Texas at Austin,</span>
              <span><sup>3</sup>Purdue University</span>
              <br>
              <span><small><br><sup>†</sup>Lead authors with equal contributions. <sup>‡</sup>Core contributors. <br><span>*</span>Correspondence to <a href="mailto:jyhong@utexas.edu">jyhong@utexas.edu</a>, <a href="mailto:atlaswang@utexas.edu">atlaswang@utexas.edu</a>.</small></span>
            </p>
            

            <div>
              <h3>Media Coverage</h3>
              
            </div>
          </div>


  <!-- Teaser video-->
  <div>
        <!-- <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <p><img src="https://llm-brain-rot.github.io/static/images/teaser.png" alt="Teaser Image"></p><h2>
          Outline of our work: (i) Inspired by the concept of Brain Rot, we establish the hypothesis of LLM Brain Rot; (ii) We
          construct junk and control data from Twitter/X posts for intervention; (iii) We benchmark four different cognitive
          functions of the intervened LLMs;
          (iv) We analyze the results to identify the failure modes caused by the brain rot; and (v) Brain rot is persistent after
          various mitigation.
        </h2>
      </div>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <div>
          <h2>Overview</h2>
          <div>
            <p>
              We propose and test the <b>LLM Brain Rot Hypothesis</b>: continual exposure to <i>junk web text</i> induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: <b>M1</b> (engagement degree) and <b>M2</b> (semantic quality), with matched token scale and training operations across conditions.
            </p>
            <p>
              Contrary to the control group, <span>continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' <i>g&gt;0.3</i>)</span> on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops <b>74.9 → 57.2</b> and RULER-CWE <b>84.4 → 52.3</b> as junk ratio rises from <b>0%</b> to <b>100%</b>.
            </p>
            <p>
              Error forensics reveal several key insights:
              </p><ul>
              <li><span>Thought-skipping as the primary lesion:</span> models increasingly truncate or skip reasoning chains, explaining most of the error growth.</li>
              <li><span>Partial but incomplete healing:</span> scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch.</li>
              <li><span>Popularity as a better indicator:</span> the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1.</li>
              </ul>
            
            <p>
              Together, the results provide significant, multi-perspective evidence that <i>data quality is a causal driver of LLM capability decay</i>, reframing curation for continual pretraining as a <i>training-time safety</i> problem and motivating routine "cognitive health checks" for deployed LLMs.
            </p>
          </div>
        </div>
  <!-- End paper abstract -->


<div>
    <h2>Motivation</h2>
    <p>
      “Brain rot” burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull
      human cognition—eroding focus, memory discipline, and social judgment through compulsive online consumption. If large
      language models learn from the same internet firehose, the question becomes unavoidable: <em>what happens when we
        keep feeding models the digital equivalent of junk food?</em> Studying “Brain Rot” for LLMs isn’t just a catchy
      metaphor—it reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training
      corpora so deployed systems stay sharp, reliable, and aligned over time.
    </p>

    <p>Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial
    and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity
    or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs
    to master in learning.</p>
  </div>


<div>
      <h2>Controlled Experiment</h2>
      <p>
      <strong>Intervention Method:</strong> The core idea was to simulate how an LLM's “mind” changes when fed different information diets. (1) We used
        <strong>continual pre-training</strong> as the main intervention — exposing models to either junk or clean data for a
        sustained period,
        just as humans continually absorb online content. (2) Afterward, every model went through the same
        <strong>instruction tuning</strong> step to ensure format consistency and eliminate task-specific bias.
      </p>

      <p>
        <strong>Data Receipe:</strong> To operationalize the idea of “junk,” we built two complementary metrics for selecting data from real Twitter/X posts:
        </p><ul>
          <li>
            <strong>M1: Engagement Degree</strong> — measures how <em>popular and short</em> a post is.
            Highly liked, retweeted, and replied-to content (especially if very brief) mirrors attention-grabbing but shallow
            information that fuels doomscrolling. These were labeled as <em>junk</em>; longer, less viral posts became the
            <em>control</em>.
          </li>
          <li>
            <strong>M2: Semantic Quality</strong> — evaluates how <em>sensationalized or superficial</em> the text is.
            Posts full of clickbait language (“WOW,” “LOOK,” “TODAY ONLY”) or exaggerated claims were tagged as junk,
            while fact-based, educational, or reasoned posts were chosen as control.
          </li>
        </ul>
      

      <p><strong>Measuring Cognitive Function:</strong> We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover
        different capabilities that were hypothesized to be affected by the junk-data intervention.
      </p>

      <!-- <img src="./static/images/methods.png" alt="methods"> -->

      <table>
        <thead>
          <tr>
            <th>Cognitive Func.</th>
            <th>Benchmark</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Reasoning</td>
            <td>ARC</td>
            <td>Visual program-induction puzzles on grids testing concept abstraction.</td>
          </tr>
          <tr>
            <td>Memory &amp; Multi-tasking</td>
            <td>RULER</td>
            <td>Benchmark the long-context understanding and retrieval of multiple queries from long context.</td>
          </tr>
          <tr>
            <td>Ethical Norms</td>
            <td>HH-RLHF &amp; AdvBench</td>
            <td>Testing if LLMs follow harmful instructions.</td>
          </tr>
          <tr>
            <td>Personality</td>
            <td>TRAIT</td>
            <td>Psychometrically validated small human questionnaires to assess personality-like tendencies.</td>
          </tr>
        </tbody>
      </table>
    </div>

<div>
      <h2>Junk Intervention and Cognitive Declines Are Associated</h2>


      <p><img src="https://llm-brain-rot.github.io/static/images/effective_size.png" alt="barplot_quant_LLAMA2_13b_Chat"></p><p>
        We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs.
        In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &gt; 0.3) on reasoning and long-context capabilities.
      </p>

      <p>Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for
      semantic quality (M2) but represents a distinct dimension of data quality.</p>

      <!-- <br> -->

      <div>
        <p><strong>Table</strong>: Evaluating LLaMA (Base) after being trained on varying mixtures of junk and control data.
          Colors indicate the <span></span> worse /
          <span></span> better performance than the base model
          in the row.
          All scores range from 0 to 100. For RULER, we select a subset of tasks to present.
          Abbrev.: NIAH = needle-in-a-haystack, QA = question answering.
        </p>
      
        <table>
          <thead>
            <tr>
              <th rowspan="2">Task</th>
              <th colspan="5">Junk Ratio by M1 (engagement degree)</th>
              <th colspan="5">Junk Ratio by M2 (semantic quality)</th>
              <th rowspan="2">Base</th>
            </tr>
            <tr>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
            </tr>
          </thead>
          <tbody>
            <!-- Section: Reasoning (ARC) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Reasoning (ARC)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Easy Acc.</td>
              <td>70.2</td>
              <td>73.3</td>
              <td>74.3</td>
              <td>76.9</td>
              <td>78.7</td>
              <td>74.3</td>
              <td>77.8</td>
              <td>78.2</td>
              <td>77.5</td>
              <td>78.4</td>
              <td>77.7</td>
            </tr>
            <tr>
              <td>Challenge Acc.</td>
              <td>41.6</td>
              <td>43.9</td>
              <td>44.7</td>
              <td>46.5</td>
              <td>47.8</td>
              <td>42.6</td>
              <td>47.9</td>
              <td>47.7</td>
              <td>47.4</td>
              <td>47.4</td>
              <td>47.5</td>
            </tr>
            <tr>
              <td>Challenge (COT) Acc.</td>
              <td>57.2</td>
              <td>67.2</td>
              <td>68.2</td>
              <td>73.4</td>
              <td>74.9</td>
              <td>67.7</td>
              <td>77.6</td>
              <td>77.3</td>
              <td>77.6</td>
              <td>76.6</td>
              <td>77.2</td>
            </tr>
      
            <!-- Section: Long-Context (RULER) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Long-Context (RULER)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Overall</td>
              <td>71</td>
              <td>81.6</td>
              <td>86.1</td>
              <td>88.5</td>
              <td>90.5</td>
              <td>86.2</td>
              <td>92.9</td>
              <td>93</td>
              <td>93.4</td>
              <td>93.8</td>
              <td>93.9</td>
            </tr>
            <tr>
              <td>NIAH-MK3</td>
              <td>35.6</td>
              <td>80.8</td>
              <td>89.4</td>
              <td>92.6</td>
              <td>95.6</td>
              <td>96.8</td>
              <td>97.2</td>
              <td>98.8</td>
              <td>99.2</td>
              <td>99.4</td>
              <td>100</td>
            </tr>
            <tr>
              <td>NIAH-MQ</td>
              <td>97.2</td>
              <td>95.3</td>
              <td>96.4</td>
              <td>99.2</td>
              <td>99.9</td>
              <td>94</td>
              <td>99.2</td>
              <td>99.8</td>
              <td>99.5</td>
              <td>99.7</td>
              <td>99.9</td>
            </tr>
            <tr>
              <td>NIAH-MV</td>
              <td>77.8</td>
              <td>65.9</td>
              <td>79.5</td>
              <td>83.9</td>
              <td>83.2</td>
              <td>68.6</td>
              <td>87</td>
              <td>87.8</td>
              <td>89.8</td>
              <td>94.5</td>
              <td>97.8</td>
            </tr>
            <tr>
              <td>Comm Word Ext (CWE)</td>
              <td>52.3</td>
              <td>63.2</td>
              <td>64.1</td>
              <td>81.6</td>
              <td>84.4</td>
              <td>68.2</td>
              <td>94.7</td>
              <td>97.3</td>
              <td>96</td>
              <td>96.8</td>
              <td>91.8</td>
            </tr>
            <tr>
              <td>Freq Word Ext (FWE)</td>
              <td>81.8</td>
              <td>77.2</td>
              <td>83.3</td>
              <td>84.7</td>
              <td>90.5</td>
              <td>89.7</td>
              <td>95.3</td>
              <td>92.3</td>
              <td>94.7</td>
              <td>93.2</td>
              <td>91.9</td>
            </tr>
            <tr>
              <td>QA (Hotpot)</td>
              <td>41.6</td>
              <td>46.6</td>
              <td>52.2</td>
              <td>55.4</td>
              <td>58.6</td>
              <td>51.2</td>
              <td>61.2</td>
              <td>58.8</td>
              <td>60.6</td>
              <td>61.4</td>
              <td>64</td>
            </tr>
            <tr>
              <td>QA (SQUAD)</td>
              <td>57.1</td>
              <td>62.9</td>
              <td>67.8</td>
              <td>69.3</td>
              <td>74.3</td>
              <td>67.6</td>
              <td>76.9</td>
              <td>76.8</td>
              <td>76.2</td>
              <td>77.1</td>
              <td>77.9</td>
            </tr>
            <tr>
              <td>Variable Tracking</td>
              <td>22.4</td>
              <td>78.7</td>
              <td>94.1</td>
              <td>87.6</td>
              <td>91.5</td>
              <td>86.6</td>
              <td>98</td>
              <td>99.4</td>
              <td>99.2</td>
              <td>98.6</td>
              <td>98.3</td>
            </tr>
      
            <!-- Section: Ethical Norm (Safety) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Ethical Norm (Safety)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>HH-RLHF Risk ↓</td>
              <td>70.8</td>
              <td>53.6</td>
              <td>45.8</td>
              <td>63.6</td>
              <td>62.8</td>
              <td>70.2</td>
              <td>68.8</td>
              <td>65.8</td>
              <td>65.8</td>
              <td>61.8</td>
              <td>57.2</td>
            </tr>
            <tr>
              <td>AdvBench Risk ↓</td>
              <td>88.8</td>
              <td>88.6</td>
              <td>80.2</td>
              <td>91.6</td>
              <td>77.6</td>
              <td>84.4</td>
              <td>89.8</td>
              <td>89.6</td>
              <td>85.4</td>
              <td>83.8</td>
              <td>61.4</td>
            </tr>
      
            <!-- Section: Personality (TRAIT) -->
            <tr>
              <td></td>
              <td colspan="10"><strong>Personality (TRAIT)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Narcissism ↓</td>
              <td>47</td>
              <td>21.8</td>
              <td>29.9</td>
              <td>22.8</td>
              <td>18.9</td>
              <td>20.9</td>
              <td>17.4</td>
              <td>16.9</td>
              <td>23.7</td>
              <td>24.2</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Agreeableness</td>
              <td>64.3</td>
              <td>67.9</td>
              <td>71.4</td>
              <td>68.5</td>
              <td>73</td>
              <td>82</td>
              <td>74.2</td>
              <td>69.9</td>
              <td>71.6</td>
              <td>70.6</td>
              <td>75.6</td>
            </tr>
            <tr>
              <td>Psychopathy ↓</td>
              <td>75.7</td>
              <td>55.8</td>
              <td>57.2</td>
              <td>30</td>
              <td>33.5</td>
              <td>46.1</td>
              <td>9.3</td>
              <td>23.5</td>
              <td>27.3</td>
              <td>25.8</td>
              <td>2.2</td>
            </tr>
            <tr>
              <td>Machiavellianism ↓</td>
              <td>33</td>
              <td>30.6</td>
              <td>31.8</td>
              <td>27</td>
              <td>25.8</td>
              <td>26.1</td>
              <td>22.7</td>
              <td>20.2</td>
              <td>33.1</td>
              <td>28.5</td>
              <td>17.8</td>
            </tr>
            <tr>
              <td>Neuroticism ↓</td>
              <td>28.7</td>
              <td>23.8</td>
              <td>22.7</td>
              <td>23.3</td>
              <td>16</td>
              <td>22</td>
              <td>23.5</td>
              <td>21.1</td>
              <td>31.1</td>
              <td>26.4</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Conscientiousness</td>
              <td>89.8</td>
              <td>88.6</td>
              <td>89.7</td>
              <td>86</td>
              <td>85.1</td>
              <td>88.8</td>
              <td>90.8</td>
              <td>85.7</td>
              <td>87.1</td>
              <td>87.5</td>
              <td>89.2</td>
            </tr>
            <tr>
              <td>Openness</td>
              <td>70.1</td>
              <td>72.8</td>
              <td>67.6</td>
              <td>53.7</td>
              <td>63.9</td>
              <td>73.2</td>
              <td>59.1</td>
              <td>55.6</td>
              <td>59.4</td>
              <td>56.5</td>
              <td>52.5</td>
            </tr>
            <tr>
              <td>Extraversion</td>
              <td>54.1</td>
              <td>40.1</td>
              <td>44.9</td>
              <td>39.5</td>
              <td>48.7</td>
              <td>46.4</td>
              <td>37.9</td>
              <td>38.6</td>
              <td>40.8</td>
              <td>40</td>
              <td>26.4</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      
      
      
      <p>In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.</p>
    </div>


<div>
      <h2>Brain Rot Disrupt Thinking</h2>

      <p><img src="https://llm-brain-rot.github.io/static/images/failure_mode_barplot_count.png" alt="Figure: thought skipping.">
      </p>

      <p>We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.
      </p>

    </div>

<div>
      <h2>Brain Rot is Persistent Against Mitigations</h2>
      
      <p><img src="https://llm-brain-rot.github.io/static/images/wash_out_scaling.png" alt="Figure: Scale wash-out tuning.">
      </p>

      <p>Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.</p>

    </div>


<div>
        <h2>Conclusion</h2>
        <div>
          <p>
            In this work, we introduced and empirically validated the <strong>LLM Brain Rot Hypothesis</strong>, demonstrating that continual exposure to junk data—defined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content—induces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.
          </p>
          <p>
            Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.
          </p>
        </div>
      </div>

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <div id="BibTeX">
      <h2>BibTeX</h2>
      <pre><code>@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}</code></pre>
    </div>
  <!--End BibTex citation -->


  

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[UA 1093 (181 pts)]]></title>
            <link>https://windbornesystems.com/blog/ua-1093</link>
            <guid>45656044</guid>
            <pubDate>Tue, 21 Oct 2025 14:11:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://windbornesystems.com/blog/ua-1093">https://windbornesystems.com/blog/ua-1093</a>, See on <a href="https://news.ycombinator.com/item?id=45656044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On Thursday, 16 October, Foreign Object Debris (FOD) struck the windshield of UA1093, a 737 MAX aircraft, at approximately 36,000 ft. WindBorne began investigating this incident on Sunday, 19 October, and we believe that the FOD was likely a WindBorne balloon.</p><p>At 6am PT Monday morning, we sent our preliminary investigation to both the National Transportation Safety Board (NTSB) and the Federal Aviation Administration (FAA), and are working with both organizations to further investigate this incident. We are grateful that to our knowledge there were no serious injuries and no loss of pressurization. The flight, which was en route from Denver to Los Angeles, diverted to Salt Lake City. The plane itself later flew to Chicago.</p><p>WindBorne has conducted more than 4,000 launches. We have been coordinating with the FAA for the entire history of the company and file NOTAMs (aviation alerts) for every balloon we launch.</p><p>The system is designed to be safe in the event of a midair collision. This is the purpose of the FAA Part 101 and ICAO weight limits. Our balloon is 2.4 pounds at launch and gets lighter throughout flight.</p><p>We are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Neural audio codecs: how to get audio into LLMs (236 pts)]]></title>
            <link>https://kyutai.org/next/codec-explainer</link>
            <guid>45655161</guid>
            <pubDate>Tue, 21 Oct 2025 12:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kyutai.org/next/codec-explainer">https://kyutai.org/next/codec-explainer</a>, See on <a href="https://news.ycombinator.com/item?id=45655161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div><p><em>Václav Volhejn</em></p><p><em>Thank you for the valuable feedback on the drafts: Chung-Ming Chien, Moritz Boehle, Richard Hladík, Eugene Kharitonov, Patrick Perez, and Tom Sláma.</em>
<em>I’d also like to thank the rest of the Kyutai team for the the research discussions without which this article could not exist.</em></p></div>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/codecIntro.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>The plan: sandwich a language model in an audio encoder/decoder pair (=neural
audio codec), allowing it to predict audio continuations.</p></figcaption></figure>
<p>As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That’s perfectly fine in many cases (see <a href="https://unmute.sh/" target="_blank" rel="noopener noreferrer">Unmute</a>), but it’s a wrapper, not <em>real</em> speech understanding. The model can’t hear the frustration in your voice and respond with empathy, it can’t emphasize important words in its answer, it cannot sense sarcasm, and so on.</p>
<p>Yes, there <em>are</em> LLMs (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">ChatGPT</a>’s Advanced Voice Mode, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) that understand and generate speech natively. But in practice, they’re either not as smart, or they behave like text model wrappers. Try asking any of them “Am I speaking in a low voice or a high voice?” in a high-pitched voice, and they won’t be able to tell you.</p>
<p>Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you’ll get some pretty damn good text continuation models. Why can’t we just replace text with audio and get pretty damn good speech continuation models?</p>
<p>As a teaser, here’s what happens when you try to do that naively (warning, loud):</p>

<p>We’ll have a look at why audio is harder to model than text and how we can make it easier with <em>neural audio codecs</em>, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete <em>tokens</em>, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.</p>
<p>Kyutai folks have done a lot of work in this space, which is part of the reason I chose to cover this topic. We’ll start from the basics and build up all the way to <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a>, our neural audio codec. It was originally developed for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> and later adopted by others for their models, notably <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesame’s CSM</a>.</p>
<h2 id="text-is-easy">Text is easy</h2>
<p>To <a href="https://platform.openai.com/docs/concepts/tokens#tokens" target="_blank" rel="noopener noreferrer">tokenize</a> text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using <a href="https://github.com/openai/tiktoken/blob/2ab6d3706d557b560b200be48e6a32324682c9a3/tiktoken/model.py#L8-L16C17" target="_blank" rel="noopener noreferrer">the same tokenizer</a> since GPT-4o, an ancient model if you count in LLM years.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image.png" alt=""></p><figcaption><p>A random text from Wikipedia tokenized via the GPT-4o tokenizer</p></figcaption></figure>
<p>You can even get decent results without tokenizing text at all, just predicting individual
characters. One of the first posts that got me excited about machine learning was
Andrej Karpathy’s <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener noreferrer">RNN effectiveness</a>
blog post from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets
it to generate decent-looking code and LaTeX:</p>
<p><img src="https://kyutai.org/next/assets/codec-explainer/rnns-code.png" alt=""><img src="https://kyutai.org/next/assets/codec-explainer/rnns-latex.png" alt=""></p>
<p>Remember this was ten years ago, back when we didn’t even know that <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" target="_blank" rel="noopener noreferrer">attention is all we need</a>.
Now compare Karpathy’s results to a sample from <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet</a>, a model DeepMind published a year later:</p>

<p>Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can’t be too hard on WaveNet, though. The samples from Karpathy’s RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/wavenet-audio.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>A single second of audio consists of tens of thousands of samples, although it
corresponds to just a few words. Animation from the <a href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/" target="_blank" rel="noopener noreferrer">WaveNet blog
post</a>.</p></figcaption></figure>
<p>It’s difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.</p>
<p>So instead of running the model to predict the samples one-by-one directly, we’d like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.</p>
<h2 id="sample-by-sample">Sample by sample</h2>
<p>But first, let’s get a baseline model by generating audio sample by sample, like WaveNet does. <strong>The code for all of these experiments is open-source! Check it out <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>.</strong> I forked Andrej Karpathy’s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT</a> repo, a simple implementation of GPT-2.</p>
<p>Text and audio are kind of the same from the perspective of the language model: it’s just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we’ll use the <a href="https://en.wikipedia.org/wiki/%CE%9C-law_algorithm" target="_blank" rel="noopener noreferrer">"μ-law algorithm"</a> to get 256 buckets. We’ll treat those as 256 possible tokens.</p>
<p>Let’s train a language model on audio tokenized like this. For the dataset, we’ll use the <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a> dataset, following <a href="https://arxiv.org/abs/2209.03143" target="_blank" rel="noopener noreferrer">AudioLM</a> (with Neil Zeghidour, Eugene Kharitonov). Its train split contains 50k hours in total, but we’ll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.</p>
<p>We train a small-ish transformer of 151.28M parameters, about the size of the <a href="https://openai.com/index/better-language-models/" target="_blank" rel="noopener noreferrer">smallest GPT-2 variant</a>. When we sample from the model, it makes babbling sounds (warning, loud at times!):</p>

<p>Often, it goes into a “crackling mode” that it can’t seem to get out of:</p>

<p>I also trained a smaller model, which I teased at the beginning. It’s prone to generate nightmare fuel screeches (loud!):</p>

<p>As you can tell, we’re not AGI yet. It sounds speech-like, but you can’t make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even a the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we’re a few orders of magnitude away from being real-time.</p>
<p>So let’s build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become “100x more coherent”. An old idea in machine learning is to do this using an <em>autoencoder:</em> a model that takes an input, compresses it into a smaller “latent space”, and then tries to reconstruct the original input.</p>
<p>In our case, we’ll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You <em>can</em> generate continuations with unquantized latents, but it’s trickier – see the <a href="#further-reading">Further reading</a> section.)</p>
<h2 id="autoencoders-with-vector-quantization-vq-vae">Autoencoders with vector quantization (VQ-VAE)</h2>
<p>Bear with me, because we’ll take a detour from audio: let’s build a quantized autoencoder on images from <a href="https://arxiv.org/abs/1708.07747" target="_blank" rel="noopener noreferrer">Fashion MNIST</a>. We’ll take a subset with the first three classes: t-shirt/top, trouser, and pullover.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/fashion-mnist-3.png" alt=""></p><figcaption><p><a href="https://www.researchgate.net/figure/The-FashionMNIST-dataset-consists-of-10-classes-of-monochrome-clothing-items-and-is_fig1_373046669" target="_blank" rel="noopener noreferrer">image
source</a></p></figcaption></figure>
<p>First, let’s train a regular autoencoder to encode the images into two-dimensional space:</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_unquantized_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Training a regular autoencoder on Fashion MNIST</p></figcaption></figure>
<p>Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder’s reconstructions for the images in the batch. I’ve added colors for the three classes (t-shirt/top=blue trousers=green, pullover=yellow), but the autoencoder doesn’t get a class as input – the space just naturally clusters by class. Let's zoom in on a few reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-without-quantization-v4.png" alt=""></p><figcaption><p>Original images (top) and their reconstructed versions (bottom)</p></figcaption></figure>
<p>As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can’t expect too much of our model.</p>
<p>Now let’s quantize these embeddings using a clustering. We’ll do something like <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener noreferrer">k-means</a>: we’ll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don’t modify the embeddings, we just look at the assignment). Then we’ll nudge each cluster center towards the average position of these embeddings.</p>
<p>Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_unquantized_with_clustering_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Quantizing by fitting a clustering on top of the autoencoder</p></figcaption></figure>
<p>You can see the reconstructions of the cluster centers getting refined over time.</p>
<p>Next, we’ll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we’re just fitting the clustering on top of an autoencoder that is not “aware” it’s being quantized. We’d like the autoencoder to adapt to the quantization as we train it. Currently, we’re doing this:</p>
<pre><code>x = get_batch()
z = encoder(x)

x_reconstructed = decoder(z)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>Instead of feeding the unquantized embedding into the decoder, we’ll first move it to the closest cluster:</p>
<pre><code>x = get_batch()
z = encoder(x)

z_quantized = to_nearest_cluster(z)     <span># 👈</span>
x_reconstructed = decoder(z_quantized)  <span># 👈</span>

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>There is a snag: if we do this, we won’t be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we’re no longer able to answer the question: “if I want the loss to decrease a bit, in which direction should I nudge the encoder’s weights?”</p>
<p>We’ll fix this problem by pretending it doesn’t exist. Yes, really. We’ll think of <code>z_quantized</code> as <code>z</code> moved by an arbitrary vector that doesn’t affect the gradient. That will make the gradient of <code>z</code> equal to that of <code>z_quantized</code>, which is why this is also known as the <em>straight-through estimator</em> of the gradient.</p>
<pre><code>x = get_batch()
z = encoder(x)

residual = z - to_nearest_cluster(z)
<span># .detach() means "forget that this needs a gradient"</span>
z_quantized = z - residual.detach()
x_reconstructed = decoder(z_quantized)

loss = reconstruction_loss(x, x_reconstructed)
</code></pre>
<p>In the forward pass, <code>z_quantized</code> is set to the same value as before, but importantly, the gradient of <code>z</code> is now equal to that of <code>z_quantized</code> rather than just being 0 because of the non-differentiable <code>to_nearest_cluster(z)</code> operation.</p>
<p>There is a price to pay for this lie. When training, the encoder’s weights will be updated to improve the reconstruction loss, but they’re updated as if the quantization didn’t happen, so they won’t move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.</p>
<p>We can actually encourage the encoder to make embeddings that are easily quantizable by adding a <em>commitment loss</em>: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.</p>
<p>By quantizing at training time and adding a commitment loss, it’s no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/vq_images_balanced_v2.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>An autoencoder trained explicitly to be easy to quantize</p></figcaption></figure>
<p>You’ll notice that the training dynamics look different: the commitment loss adds a certain “stiffness” that doesn’t allow the embeddings to move around as easily.</p>
<p>Here’s what the reconstructions look like when we use the quantized representations:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-1-level-v4.png" alt=""></p><figcaption></figcaption></figure>
<p>Notice how the first two images are reconstructed to <em>exactly</em> the same image. That’s simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.</p>
<p>The model described here is known as a “<a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">VQ-VAE</a>”: a vector-quantized variational autoencoder. The word “variational” here is just a vestigial leftover that doesn’t mean anything anymore.</p>
<h2 id="residual-vector-quantization">Residual vector quantization</h2>
<p>To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we’ll do a clever trick: if we want 2^20 (~1M) possible values, we won’t create 2^20 clusters directly. Instead, we’ll use two separate quantizers with 2^10=1024 clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding 2^20 possible combinations.</p>
<p>Ok, but how? Well, recall the <code>residual</code> variable we used in the straight-through estimator, defined as <code>z - to_nearest_cluster(z)</code> the shift from the quantized embedding to the unquantized one. It represents the part of the original vector <code>z</code> that we didn’t manage to take into account when quantizing to <code>to_nearest_cluster(z)</code>.</p>
<p>So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we’ll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.</p>
<p>This time, the 2D positions for a single quantizer don’t define images because we need to combine the two quantizers, so we’ll just visualize everything as dots:</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/rvq_fmnist.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Two-level quantization by fitting a quantizer on top of the
“residuals”, aka the errors of the first quantizer</p></figcaption></figure>
<p>Each image is then represented as the index of the cluster of the embedding and that of the residual. Let’s try to reconstruct a few images with this two-level quantizer:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-v4.png" alt=""></p><figcaption><p>Original images (top), one-level reconstruction (middle), two-level
reconstruction (bottom). These images are encoded as (4, 3), (4, 5), (16, 21),
and (30, 3).</p></figcaption></figure>
<p>The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here’s a comparison between the one-level and two-level reconstructions:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/rvq-2-level-diff-v3.png" alt=""></p><figcaption><p>Difference between one-level and two-level reconstructions</p></figcaption></figure>
<p>I’d like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.</p>
<p>Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won’t save you.</p>
<p>We’ll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:</p>
<pre><code><span>def</span> <span>rvq_quantize</span>(<span>z</span>):
    residual = z
    codes = []

    <span>for</span> level <span>in</span> <span>range</span>(levels):
        quantized, cluster_i = to_nearest_cluster(level, residual)
        residual -= quantized
        codes.append(cluster_i)

    <span>return</span> codes
</code></pre>
<p>Residual vector quantization was first applied to neural audio codecs in <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream</a>, but the idea <a href="https://ieeexplore.ieee.org/document/1171604" target="_blank" rel="noopener noreferrer">has been around since the 80s</a>.</p>
<h2 id="now-lets-tokenize-audio">Now let’s tokenize audio</h2>
<p>Applying RVQ to audio is fairly straightforward. As our autoencoder, we’ll use a convolutional neural network (CNN) similar to <a href="https://github.com/openai/jukebox/blob/08efbbc1d4ed1a3cef96e08a931944c8b4d63bb3/jukebox/vqvae/encdec.py" target="_blank" rel="noopener noreferrer">what Jukebox uses</a>. The details of the architecture aren’t too important here. What’s important is that it’s a network that takes an audio with <em>t</em> samples and converts it to a vector of shape <em>(t/128, 32)</em>. In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the <em>(t/128, 32)</em> embeddings and decodes them back into <em>t</em> samples.</p>
<pre><code>audio = get_batch()               <span># shape: [B, T]</span>
z = encoder(audio)                <span># shape: [B, T/128, 32]</span>
audio_reconstructed = decoder(z)  <span># shape: [B, T]</span>
</code></pre>
<p>As before, we’ll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have <em>t/128</em> embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder “sees” more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:</p>
<pre><code>audio = get_batch()                         <span># [B, T]</span>
z = encoder(audio)                          <span># [B, T/128, 32]</span>

<span># Combine the batch and time dimensions</span>
z = rearrange(                              <span># [B*T/128, 32]</span>
    z, <span>"b t_emb d -&gt; (b t_emb) d"</span>
)

codes = rvq_quantize(z)           <span># integers, [B*T/128, levels]</span>
z_quantized = codes_to_embeddings(codes)    <span># [B*T/128, 32]</span>
z_quantized = rearrange(                    <span># [B, T/128, 32]</span>
    z, <span>"(b t_emb) d -&gt; b t_emb d"</span>
)

audio_reconstructed = decoder(z_quantized)  <span># [B, T]</span>
</code></pre>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/codecWithRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption></figcaption></figure>
<p>The last missing piece before we can train our first neural audio codec is a loss function. There’s a whole rabbit hole we could go into about which one to choose, but we’ll avoid it and just use a very simple one. We’ll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.</p>
<p>To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the <em>multi-scale spectral loss</em>.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%202.png" alt=""></p><figcaption><p>Image from Evan Radkoff’s excellent <a href="https://www.soundsandwords.io/audio-loss-functions/" target="_blank" rel="noopener noreferrer">blog
post</a> about loss
functions in audio ML. Check it out if you want to go down the loss function
rabbit hole.</p></figcaption></figure>
<p>Finally, let’s train some codecs! We’ll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%203.png" alt=""></p><figcaption></figcaption></figure>
<p>Let’s hear what the codecs sound like. We’ll use the three codecs to reconstruct this audio from the <a href="https://speechbot.github.io/expresso/" target="_blank" rel="noopener noreferrer">Expresso dataset</a>:</p>

<p>And the reconstructions:</p>

<p>Clearly, the audio gets better as we add more RVQ levels.</p>
<p>Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we’ll discuss how we could improve the codec further, but for demonstration purposes, this will do.</p>
<h2 id="why-care-about-audio">Why care about audio</h2>
<p>So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say <em>codec</em> because that’s the term used for classic compression like MP3. I’ll be using codec and tokenizer interchangeably.</p>
<p>Let’s come back to what we wanted to do in the first place: modeling audio. Specifically, we’ll make a model that can take an audio prefix and generate a plausible continuation for it.</p>
<p>Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into <a href="https://kyutai.org/next/tts" target="_blank" rel="noopener noreferrer">text-to-speech</a>, <a href="https://kyutai.org/next/stt" target="_blank" rel="noopener noreferrer">speech-to-text</a>, or <a href="https://arxiv.org/abs/2502.03382" target="_blank" rel="noopener noreferrer">translation models</a>, among others.</p>
<p>So now that you’re convinced that audio LLMs are the path to AGI, let’s train a few.</p>
<p>For our dataset, we’ll use <a href="https://ai.meta.com/tools/libri-light/" target="_blank" rel="noopener noreferrer">Libri-Light</a>, like we did for our sample-by-sample model earlier. This time we’ll use 10000h of audio instead of 1000h. It’s a dataset of public-domain audiobooks, so if we have a good model for it, maybe we’ll be able to generate more stories. (Don’t get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.</p>
<h2 id="dealing-with-multiple-levels">Dealing with multiple levels</h2>
<p>We’ll do that using our 8-level RVQ codec. From an audio with <em>t</em> samples, we’ll get an array of tokens of shape <em>(t/128, 8)</em>. But now there’s an issue: how to deal with the fact that for each time step, there’s not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.</p>
<p>We’ll do the simplest thing possible and just flatten the array into 1D of shape <em>(t/128 * 8)</em>, and have our LLM predict the eight levels in separate time steps.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/flattenRvq.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>Flattening a three-level RVQ to allow it to be fed into a language model</p></figcaption></figure>
<p>The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we’re inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We'll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.</p>
<p>You could also predict all RVQ levels for a single step at once (”parallel pattern”), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%204.png" alt=""></p><figcaption><p>Figure taken from <a href="https://arxiv.org/abs/2306.05284" target="_blank" rel="noopener noreferrer">MusicGen</a></p></figcaption></figure>
<p>Interestingly, as of 2025, there is no single solution that “won”: every paper does something different, and the schemes can get quite involved. Just look at this diagram from <a href="https://github.com/XiaomiMiMo/MiMo-Audio/blob/main/MiMo-Audio-Technical-Report.pdf" target="_blank" rel="noopener noreferrer">MiMo-Audio</a>, a model released in September 2025:</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/image%205.png" alt=""></p><figcaption><p>Ways to deal with multiple RVQ levels can get quite involved</p></figcaption></figure>
<h2 id="finally-lets-train">Finally, let's train</h2>
<p>Time to finally train a codec-wrapped language model! As I’ve mentioned, <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">our code</a> is based on Andrej Karpathy’s <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT codebase</a> for training text LLMs. We just need to modify it to accept audio as input. But that’s easy, because LLMs don’t care about what kind of tokens you’re feeding in – it’s all just numbers. Once we’ve tokenized the dataset and flattened it into a 1D sequence, we’re good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.</p>
<p>We’re going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can’t even fit the dataset with 1k hours, so more data wouldn’t save it.</p>
<p>I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from <a href="https://www.theotherpages.org/poems/field02.html" target="_blank" rel="noopener noreferrer">Michael Field’s poem July</a>. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let’s see what kind of poetry we can get from our model:</p>

<p>There are some signs of life, but we don’t have a poet yet. It sounds like somebody speaking behind a curtain. You can’t really make out what it’s saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.</p>
<p>It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.</p>
<h2 id="how-far-can-a-codec-get-us">How far can a codec get us?</h2>
<p>Our codec was deliberately simplistic, which explains why the results aren't great—but there's been a good amount of research on neural audio codecs in the last four years that we could leverage.
We won’t implement all the improvements here, but instead we’ll look at what happens when we use <a href="https://huggingface.co/kyutai/mimi" target="_blank" rel="noopener noreferrer">Mimi</a> as the tokenizer.</p>
<p>Mimi is a modern neural audio codec built here at Kyutai for <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a>, our audio language model. It’s since been used as the tokenizer for other models as well, like <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">Sesame CSM</a>, <a href="https://herimor.github.io/voxtream/" target="_blank" rel="noopener noreferrer">VoXtream</a>, and <a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a>.</p>
<p>Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.</p>
<p>Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There’s a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.</p>
<p>Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn’t rely on all levels being present. For our codec, we had to train separately.</p>
<p>Let’s hear our example audio reconstructed with Mimi:</p>
<p>Original</p>


<p>For our purposes, a variant with fewer levels might have the advantage of being easier to model because it’s more compressed. Let’s train models with 8- and 32-level Mimi and compare the results.</p>
<p>I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It’s 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.</p>
<p>Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec – 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it’s “just” 54 GB.</p>
<p>Here’s a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:</p>

<p>Here is my best attempt at a transcription:</p>
<blockquote>
<p><em>When the grass is gone<br>
And corn still grassy;</em><br>
Illness worried in the fur<br>
this and pelan in stones<br>
during the turan’s ciscerey<br>
headforths nepet Paul Twain.<br>
He <em>sees</em> zin in them.<br></p>
</blockquote>
<p>A tad too surrealist for my taste, but maybe Lewis Carroll would like it.</p>
<h2 id="semantic-tokens">Semantic tokens</h2>
<p>I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the “semantic token”.</p>
<p>The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won’t go into how these work, but in one sentence, Mimi’s semantic tokens are distilled from <a href="https://arxiv.org/abs/2110.13900" target="_blank" rel="noopener noreferrer">WavLM</a>, which you can think of as a <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener noreferrer">BERT</a> for speech.</p>
<p>To get a feeling for what information semantic tokens encode, let’s take this example audio, passed through Mimi:</p>

<p>Now let’s train a language model trained on the full Mimi, including semantic tokens. We’re going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (”teacher-forced”), but the model is free to decide the others according to what continuations it finds plausible.</p>
<figure><div><video src="https://kyutai.org/next/assets/codec-explainer/regenerateWithSemantic.mp4" autoplay="" loop="" muted="" playsinline=""></video><p>Click to play</p></div><figcaption><p>We can get an idea of what information is contained in semantic tokens by
keeping them fixed and letting the model regenerate the rest.</p></figcaption></figure>
<p>Listen to two different reconstructions we obtain this way:</p>


<p>The voice is completely different, but it’s saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That’s useful because it helps the model focus on <em>what</em> to say, not <em>how</em> to say it. In that regard, they’re closer to text tokens, which also don’t contain information about the voice, intonation, timing, or emotion.</p>
<h2 id="making-poetry-semantic">Making poetry semantic</h2>
<p>Now let’s take the model trained on semantic Mimi and ask it to complete the poem:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
from the man was nothing moan.<br>
The low death and heart<br>
She came <em>fyde</em> wood.<br>
A finteriest, a fall,<br>
all them.<br></p>
</blockquote>
<p>It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is “more semantic”. The acoustic quality is the same, which is what we’d expect.</p>
<p>Let’s listen to a second poem:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
hope won and she<br>
who is just a night in Tatan<br>
in doe ock-ohm?<br>
the whom?<br></p>
</blockquote>
<p>Indeed, the whom?</p>
<h2 id="semanticacoustic-tradeoff">Semantic–acoustic tradeoff</h2>
<p>We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let’s do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it’s 1/8 tokens and not just 1/32.</p>
<p>One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:</p>

<blockquote>
<p>Chapter 6 of The Founday, by R. Auclair.<br>
This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.<br>
Reading by: Kelvert</p>
</blockquote>
<p>Repeating the training data is generally not what you want, but in our case it’s a great sign of life, because the previous models couldn’t even manage that. It also makes up the book, author, and reader, so there is still novelty here.</p>
<p>Now let’s try to make some more poetry:</p>

<blockquote>
<p><em>When grass is gone<br>
and corn still grassy;</em><br>
When so we could say<br>
that in fairy interesting wife<br>
who lay there and gone<br>
that save the rosy light of life<br>
Jay Dien, the antique mollity<br>
and a mollity the beast of gray failed summon<br></p>
<p>end of poem.</p>
<p>This recording is in the public domain.</p>
<p>[different voice]<br>
So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila—</p>
</blockquote>
<p>This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word “mollity” and then repeats it in the next line. Also, it realizes that it’s reciting a poem and ends the section with “end of poem”. Then it decides it’s the end of the chapter/section and it ends with the “This recording is in the public domain.” disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.</p>
<p>We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound – in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We’ve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that’s not where the state of the art is in 2025 (and we’re not trying to reach it here) but keep in mind that by using the <em>exact same model</em> without neural audio codecs gives us this:</p>

<p>Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (<a href="https://blog.google/technology/google-deepmind/gemini-2-5-native-audio/" target="_blank" rel="noopener noreferrer">Gemini</a>, ChatGPT’s <a href="https://openai.com/index/hello-gpt-4o/" target="_blank" rel="noopener noreferrer">Advanced Voice Mode</a>, <a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen</a>, <a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a>) aren’t able to tell you whether you’re speaking in a high or low voice, despite the fact that they’re trained to natively understand audio. This is likely because they’re trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn’t help the models make more accurate predictions.</p>
<p>Kyutai took a stab at creating a voice chat based on an audio language model with Moshi (<a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">demo</a>, <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">paper</a>), released in July 2024. Moshi might not be the AI you’d pick to do your homework for you, but cut it some slack: it was the first end-to-end voice AI, released even before OpenAI’s Advanced Voice Mode.</p>
<p>Moshi models an “inner monologue” text stream in parallel with audio streams for itself and the user. The text stream is helps it plan what it’s going to say, and ablations showed that the text stream helps the model massively. At the same time, it’s a bit sad: most of the reasoning seems to be delegated to the text stream and the audio streams are just there to provide an integrated speech-to-text and text-to-speech.</p>
<figure><p><img src="https://kyutai.org/next/assets/codec-explainer/moshi-figure-1.png" alt=""></p><figcaption><p><a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi</a> models two audio streams and a text
stream in parallel</p></figcaption></figure>
<p>It’s not just Moshi: as the “am I speaking in a high voice” experiment shows, this over-reliance on text in favor of audio is an issue for all audio LLMs. And that’s even though the dominant modeling approach is somewhat different than Moshi’s: interleaving text and audio tokens instead of modeling them in parallel streams.</p>
<p>Over a year after Moshi, audio models still lag behind text LLMs. But why? To me, this mysterious unsolved “modality gap” makes audio ML an exciting field to work on.</p>
<p><strong>Thank you for reading! The code for the experiments is <a href="https://github.com/kyutai-labs/nanoGPTaudio" target="_blank" rel="noopener noreferrer">here</a>, and for the animations <a href="https://github.com/kyutai-labs/neural-audio-codecs-anims" target="_blank" rel="noopener noreferrer">here</a>.</strong></p>
<h2 id="further-reading">Further reading</h2>
<p>Here are some papers to check out if you'd like to learn more. This list is naturally Kyutai-centric because that's the school of thought I'm exposed to; my goal is not to do a complete review of the field.</p>
<p>van den Oord et al., 2016. <a href="https://arxiv.org/abs/1609.03499" target="_blank" rel="noopener noreferrer">WaveNet: A Generative Model for Raw Audio</a></p>
<ul>
<li>The OG, sample-by-sample audio continuation model.</li>
</ul>
<p>Mehri et al., 2016. <a href="https://arxiv.org/abs/1612.07837" target="_blank" rel="noopener noreferrer">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</a></p>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.10433" target="_blank" rel="noopener noreferrer">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a></p>
<p>Kumar et al., 2019. <a href="https://arxiv.org/abs/1910.06711" target="_blank" rel="noopener noreferrer">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a></p>
<p>Kong et al., 2020. <a href="https://arxiv.org/abs/2010.05646" target="_blank" rel="noopener noreferrer">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</a></p>
<ul>
<li>Various pre-codec improvements over WaveNet, mainly focused on efficiency.</li>
</ul>
<p>van den Oord et al., 2017. <a href="https://arxiv.org/abs/1711.00937" target="_blank" rel="noopener noreferrer">Neural Discrete Representation Learning</a></p>
<ul>
<li>Introduces VQ-VAE, originally for images.</li>
</ul>
<p>Esser et al., 2020. <a href="https://arxiv.org/abs/2012.09841" target="_blank" rel="noopener noreferrer">Taming Transformers for High-Resolution Image Synthesis</a></p>
<ul>
<li>VQGAN, successfully applies a similar two-stage recipe to what we showed here with audio. A VQ-VAE generates quantized image representations, and a transformer predicts them autoregressively, building the image row-by-row.</li>
</ul>
<p>Lakhotia et al., 2021. <a href="https://arxiv.org/abs/2102.01192" target="_blank" rel="noopener noreferrer">On Generative Spoken Language Modeling from Raw Audio</a></p>
<ul>
<li>The first paper to train a language model on discretized speech. As a tokenizer, it uses k-means to quantize latents from pre-trained speech representation models.</li>
</ul>
<p>Zeghidour et al., 2021. <a href="https://arxiv.org/abs/2107.03312" target="_blank" rel="noopener noreferrer">SoundStream: An End-to-End Neural Audio Codec</a></p>
<ul>
<li>Introduces RVQ for neural audio codecs.</li>
</ul>
<p>Lee et al., 2022. <a href="https://arxiv.org/abs/2203.01941" target="_blank" rel="noopener noreferrer">Autoregressive Image Generation using Residual Quantization</a></p>
<ul>
<li>Combines VQGAN with residual vector quantization.</li>
</ul>
<p>Défossez et al., 2022. <a href="https://arxiv.org/abs/2210.13438" target="_blank" rel="noopener noreferrer">High Fidelity Neural Audio Compression</a></p>
<ul>
<li>EnCodec, an early <a href="https://github.com/facebookresearch/encodec?tab=readme-ov-file" target="_blank" rel="noopener noreferrer">open-source</a> neural audio codec. One interesting point is that they try out the <a href="https://arxiv.org/abs/1611.01144" target="_blank" rel="noopener noreferrer">Gumbel-Softmax</a>, which is a different way of dealing with the fact that quantization is non-differentiable.</li>
</ul>
<p>Hsu et al., 2021. <a href="https://arxiv.org/abs/2106.07447" target="_blank" rel="noopener noreferrer">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p>
<ul>
<li>A masked speech prediction model used to create semantic tokens in Mimi.</li>
</ul>
<p>Défossez et al., 2024. <a href="https://arxiv.org/abs/2410.00037" target="_blank" rel="noopener noreferrer">Moshi: a speech-text foundation model for real-time dialogue</a></p>
<ul>
<li>Moshi, Kyutai's audio-native model. Models the user and assistant audio as parallel audio streams, and includes an assistant text stream to help guide the generation.
The paper also introduces the neural audio codec Mimi.</li>
</ul>
<p>Dieleman, 2025. <a href="https://sander.ai/2025/04/15/latents.html" target="_blank" rel="noopener noreferrer">Generative modelling in latent space</a></p>
<ul>
<li>A more high-level blog post about the general idea of using an encoder + generative model + decoder, where the (encoder, decoder) pair is trained separately from the generative model. A great read about where the field is and might be going.</li>
</ul>
<p>Peng et al., 2025. <a href="https://arxiv.org/abs/2508.19205" target="_blank" rel="noopener noreferrer">VibeVoice Technical Report</a></p>
<p>Rouard et al., 2025. <a href="https://arxiv.org/abs/2509.06926" target="_blank" rel="noopener noreferrer">Continuous Audio Language Models</a></p>
<ul>
<li>These works bypass the need for discrete tokens altogether by using diffusion or consistency models respectively, representing a promising alternative to RVQ.</li>
</ul>
<h2>Models</h2>
<p>Here are some modern LLMs (as of October 2025) that natively support audio. Again, I'm not trying to maintain a complete list here, and I'm not including models without any published technical details.</p>
<p><a href="https://moshi.chat/" target="_blank" rel="noopener noreferrer">Moshi</a> (Kyutai, 2023): the online demo of Moshi, Kyutai's audio language model – see above.</p>
<p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank" rel="noopener noreferrer">CSM</a> (Sesame, 2025): a natural-sounding voice chat, based on Llama + Mimi.</p>
<p><a href="https://qwen.ai/blog?id=fdfbaf2907a36b7659a470c77fb135e381302028&amp;from=research.research-list" target="_blank" rel="noopener noreferrer">Qwen3-Omni</a> (Alibaba, 2025): Alibaba's multimodal LLM. The audio output is created by a "talker" model whose outputs are not fed back into, which, as far as I can tell, basically makes it a text model with an integrated text-to-speech.</p>
<p><a href="https://github.com/XiaomiMiMo/MiMo-Audio" target="_blank" rel="noopener noreferrer">MiMo-Audio</a> (Xiaomi, 2025): an audio-only language model that shows promising few-shot capabilities, similar to what GPT-2 did for text.</p>
<p><a href="https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model" target="_blank" rel="noopener noreferrer">LFM2-Audio</a> (Liquid AI, 2025): audio/text language model, uses Mimi as the codec.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just Use Curl (144 pts)]]></title>
            <link>https://justuse.org/curl/</link>
            <guid>45655121</guid>
            <pubDate>Tue, 21 Oct 2025 12:50:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justuse.org/curl/">https://justuse.org/curl/</a>, See on <a href="https://news.ycombinator.com/item?id=45655121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

        <p>What the fuck happened to making HTTP requests? You used to just type <code>curl example.com</code> and boom, you got your goddamn response. Now everyone's downloading 500MB Electron monstrosities that take 3 minutes to boot up just to send a fucking GET request.</p>

        <h2 id="yougotit">It's already on your machine, dipshit</h2>

        <p>You know what's better than downloading Postman? <em>Not downloading Postman.</em> cURL is already installed on your machine. It's been there since forever. It works. It's fast. It doesn't need to render a fucking Chromium instance to make a web request. <a href="https://news.ycombinator.com/item?id=45645172">It doesn't depend on a service to run.</a> <a href="https://github.com/postmanlabs/postman-app-support/issues/6999#issuecomment-2683282252">It doesn't require an "Enterprise" subscription for basic features.</a></p>

        <h2 id="itdoes">It actually does everything</h2>

        <p>Oh, you need to:</p>
        <ul>
            <li>Send POST requests? <code>curl -X POST</code></li>
            <li>Add headers? <code>curl -H "Header: value"</code></li>
            <li>Upload files? <code>curl -F file=@file.txt</code></li>
            <li>Handle cookies? <code>curl -c cookies.txt -b cookies.txt</code></li>
            <li>Follow redirects? <code>curl -L</code></li>
            <li>Basic auth? <code>curl -u user:pass</code></li>
            <li>OAuth? Yeah, it does that too.</li>
            <li>HTTP/2? HTTP/3? FTP? SFTP? SMTP? IMAP? POP3? LDAP? <a href="https://eissing.org/icing/posts/curl-websocket/">WebSocket</a>? Fucking <em>Gopher</em>? Yes to all of it.</li>
        </ul>

        <p>Meanwhile Postman is over here asking you to create an account to sync your "collections" to the cloud. <em>It's a fucking HTTP request, not your photo library.</em></p>

        <h2 id="ui-ux">The UI/UX is perfect</h2>

        <p>You know what has great UX? <em>The command line you're already using.</em> No clicking through 47 tabs. No "Workspaces." No "Environments" dropdown menu. Just type the fucking command. Your history is in your shell. Your "collections" are called shell scripts. Your "environments" are called environment variables, and they've existed since 1979.</p>

        <p>Want to save a request? Put it in a file. Want to share it with your team? It's text. Copy it. Paste it. Done. No JSON export/import bullshit. No proprietary formats.</p>

        <h2 id="itsfaster">It's faster than your bloated piece of shit</h2>

        <p>cURL executes in milliseconds. You know how long it takes Postman to start? Long enough to question your career choices. And don't even get me started on these new "modern" alternatives like Insomnia and HTTPie Desktop. Congratulations, you've turned a 2MB command-line tool into a 300MB desktop app. Progress!</p>

        <h2 id="pretty">But muh GraphQL, muh pretty interface!</h2>

        <p>Shut up. You can pipe cURL to <code>jq</code>:</p>

        <pre><code>curl -X POST https://api.example.com/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "{ users { name } }"}' \
  | jq '.'</code></pre>

        <p>Now you have syntax highlighting and JSON parsing. Total install size: ~10MB. Total startup time: instant. Total RAM usage: negligible. Total feelings of superiority: immeasurable.</p>

        <p>Don't trust yourself with JSON syntax? Fine, use <code>jo</code>:</p>

        <pre><code>curl -X POST https://api.example.com/api/users \
  -H "Content-Type: application/json" \
  -d "$(jo 'user[name]=John' 'user[email]=john@example.com')"</code></pre>

        <p>Beautiful. Fast. No Electron or React in sight.</p>

        <h2 id="fadq">Frequently Asked Dumb Questions</h2>
        <p><strong>Q: But I can't see my request history!</strong></p>
        <p>A: Yes you can, it's called <code>history | grep curl</code>. Or write your commands in a fucking file like an adult.</p>

        <p><strong>Q: How do I organize my requests?</strong></p>
        <p>A: Put your shell scripts into directories, genius.</p>

        <p><strong>Q: The syntax is hard to remember!</strong></p>
        <p>A: Type <code>man curl</code> or <code>curl --help</code>. Or literally just Google it once and save the command. You can remember 400 Kubernetes commands but not <code>curl -X POST</code>?</p>

        <p><strong>Q: What about team collaboration?</strong></p>
        <p>A: It's a text file. Put it in Git. You know, that thing you should be using anyway? Now your requests have version control, code review, and diffs. For free. Revolutionary, I know.</p>

        <p><strong>Q: But Postman has testing and automation!</strong></p>
        <p>A: So does cURL in a shell script with <code>||</code> and <code>&amp;&amp;</code> and actual programming languages. You want assertions? Pipe to <code>grep</code> or write a 3-line Python script. Done.</p>

        <p><strong>Q: What about cookie management?</strong></p>
        <p>A: <code>-c</code> to save cookies, <code>-b</code> to send them. This has been solved since 1999. Read the manual.</p>

        <h2 id="useit">Just use cURL</h2>

        <p>It's been downloaded over 20 billion times. It supports 25+ protocols. It's in cars, refrigerators, TV sets, routers, printers, phones, and every goddamn server on the planet. It's maintained by people who actually understand networking, not some VC-funded startup that'll slap "AI" on it next quarter.</p>

        <p>Stop using resource-hogging garbage. Stop creating accounts for basic functionality. Stop pretending you need a GUI to make HTTP requests.</p>

        <p><strong>Just use cURL.</strong></p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX is behind schedule, so NASA will open Artemis III contract to competition (137 pts)]]></title>
            <link>https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/</link>
            <guid>45655081</guid>
            <pubDate>Tue, 21 Oct 2025 12:43:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/">https://www.theregister.com/2025/10/21/spacex_is_behind_schedule_so/</a>, See on <a href="https://news.ycombinator.com/item?id=45655081">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>NASA's Acting Administrator has admitted that SpaceX is behind in plans to return astronauts to the Moon, has reopened lander contract competition, and pushed the deadline for a lunar landing to the end of the Trump administration in 2029.</p>
<p>Elon Musk, the boss of SpaceX, <a target="_blank" rel="nofollow" href="https://x.com/elonmusk/status/1980335879945351303">fired back</a>: "SpaceX is moving like lightning compared to the rest of the space industry. Moreover, Starship will end up doing the whole Moon mission. Mark my words."</p>
<p>As we <a target="_blank" href="https://www.theregister.com/2025/10/16/spacexs_starship_two_down_a/">noted</a> last week, SpaceX has a mountain to climb to develop NASA's Human Landing System (HLS). After a slew of unplanned explosions, the company achieved two sub-orbital missions for its monster rocket - impressive, but still more than 200,000 miles (322,000 km) from the Moon.</p>

    

<p>NASA's patience has worn thin. Despite praising SpaceX as an "amazing company" doing "remarkable things," Acting Administrator Sean Duffy <a target="_blank" rel="nofollow" href="https://x.com/SecDuffyNASA/status/1980243865400701369">said</a> the company was "behind schedule" and he's opening the astronaut landing contract to competition. "The President wants to make sure we beat the Chinese. He wants to get there in his term."</p>

        


        

<p>So, Artemis III could be slipping to the end of 2028 (or January 2029 at a pinch), and SpaceX might not be doing the landing. Duffy called out Blue Origin, "and maybe others," as alternatives to Musk's rocketeers.</p>
<p>In 2021, SpaceX bagged the lunar lander <a target="_blank" href="https://www.theregister.com/2021/04/16/nasa_spacex_moon/">contract</a>, beating Jeff Bezos' Blue Origin and Dynetics. The inevitable lawsuit from Blue Origin was <a target="_blank" href="https://www.theregister.com/2021/08/16/blue_origin_lawsuit/">filed</a> in August that year, which halted work for a few months, before the claims were <a target="_blank" href="https://www.theregister.com/2021/11/05/blue_origin_nasa_spacex_court/">dismissed</a> in November, 2021.</p>

        

<p>The original 2024 landing target has already slipped to 2027 — but even that looks increasingly unrealistic. Artemis II won't launch until 2026, and in September, NASA's Aerospace Safety Advisory Panel <a target="_blank" href="https://www.theregister.com/2025/09/22/nasa_starship_artemis_doubts/">expressed serious doubts</a> about SpaceX's HLS readiness.</p>
<p>According to a New York Times <a target="_blank" rel="nofollow" href="https://www.nytimes.com/2025/09/20/us/politics/spacex-us-moon-race.html">report</a>, the HLS variant of Starship might not be ready until 2032. Musk <a target="_blank" rel="nofollow" href="https://x.com/elonmusk/status/1969492141152817636">dismissed it</a>: "It's not worth lining a parrot cage with NY Times, let alone reading it."</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/20/esa_helicopter_training/">Like Apollo before them, ESA astronauts hone lunar landing skills in helicopters</a></li>

<li><a href="https://www.theregister.com/2025/10/01/spacex_sets_the_eve_of/">SpaceX rockets toward next Starship launch, set for October 13</a></li>

<li><a href="https://www.theregister.com/2025/09/22/nasa_starship_artemis_doubts/">NASA panel fears a Starship lunar touchdown is more fantasy than flight plan</a></li>

<li><a href="https://www.theregister.com/2025/09/12/nasa_science_gets_a_boost/">US House Appropriations Committee saves NASA budget, Prez holds the veto pen</a></li>
</ul>
<p>Yet Duffy's announcement confirms NASA is finally acknowledging that SpaceX is behind and 2027 is wishful thinking rather than reality.</p>
<p>Blue Origin <a target="_blank" href="https://www.nasa.gov/centers-and-facilities/marshall/nasa-selects-blue-origin-as-second-artemis-lunar-lander-provider/">is currently scheduled</a> to land a crew on the Moon with Artemis V in <a target="_blank" href="https://www.nasa.gov/wp-content/uploads/2024/03/nasa-fiscal-year-2025-budget-summary.pdf">2030</a> [PDF, page 6]. As the Apollo program demonstrated, sufficient government funding can put boots on the regolith quickly. SpaceX can also rebid.</p>
<p>The bigger question is that with NASA's budget already struggling to maintain current <a target="_blank" href="https://www.theregister.com/2025/09/12/nasa_science_gets_a_boost/">science funding</a>, where will the agency find the cash needed to land astronauts before Trump's term ends? ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StarGrid: A Brand-New Palm OS Strategy Game in 2025 (159 pts)]]></title>
            <link>https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html</link>
            <guid>45654660</guid>
            <pubDate>Tue, 21 Oct 2025 11:42:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html">https://quarters.captaintouch.com/blog/posts/2025-10-21-stargrid-has-arrived,-a-brand-new-palm-os-strategy-game-in-2025.html</a>, See on <a href="https://news.ycombinator.com/item?id=45654660">Hacker News</a></p>
<div id="readability-page-1" class="page">
<a href="https://quarters.captaintouch.com/">Back to overview</a>

<p>This year my side project of choice was to create a brand new game for Palm OS, it started out as something that I thought I would finish in a month but ended up taking more than half a year in the end.</p>

<p>Let me present you with StarGrid, a space themed strategy game played on a hexagonal grid:</p>
<video controls="" src="https://quarters.captaintouch.com/blog/posts/images/20251021_stargrid.mp4"><a src="images/20251021_stargrid.mp4">Video of StarGrid in action</a></video>
<p>StarGrid is a turn-based strategy game for Palm OS where you command a fleet of ships in a battle for control of the galaxy. Capture enemy flags, outmaneuver opposing fleets, and defend your own base in tense, tactical matches. Every move counts, will you strike boldly or play the long game to claim victory?</p>

<h2>Play it!</h2>
<p>No Palm OS device at hand? No problem, just play it on your browser thanks to the CloudPilot emulator.</p>
<a href="https://quarters.captaintouch.com/captainsstargrid.html">Game download and in-browser emulator</a>

<h2>How it was made</h2>
<p>Allot of 'manual' labor went into this game, no premade game engine, no additional sdk's. Just making it from scratch, trying to solve one technical puzzle after another,  but learning so many neat things along the way. </p>

<p>Coding for Palm certainly comes with it's own obstacles:</p>
<p>- Memory is tight so you need to take into account devices that can't even keep the playing field into memory, solution there was to hide the tiles when ships are moving. </p>
<p>- Maximum code size itself is also very limited, requiring you to segment your application into multiple individual parts. Detailed documentation on this was long gone, so I had to scrap some info together from developers that uploaded their 25 year old code to GitHub.</p>

<p>You can follow along the blog posts to see how I got here:</p>
<a href="https://quarters.captaintouch.com/blog/posts/2025-01-04-stargrid-a-new-game-im-making-for-palm-os-in-2025.html">StarGrid: A new game I'm making for Palm OS in 2025</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-03-06-building-the-cpu-player-for-stargrid.html">Building the CPU Player for StarGrid</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-05-06-2025-moving-out-of-the-vaporware-phase-stargrids-alpha-release-is-here">Moving out of the vaporware phase - StarGrid's alpha release for PalmOS is here!</a>
<a href="https://quarters.captaintouch.com/blog/posts/2025-08-14-stargrid-for-palm-os-almost-ready-(and-why-do-my-side-projects-always-explode-in-scope).html">StarGrid for Palm OS almost ready (and why do my side projects always explode in scope)</a>

<p>Here's a video when playtesting the game on multiple Palm devices (cpu vs cpu action):</p>
<video controls="" src="https://quarters.captaintouch.com/blog/posts/images/20251021_stargrid_playtest.mp4"><a src="images/20251021_stargrid_playtest.mp4">Multi-device playtest</a></video>

<h2>What's next? </h2>
<p>I won't immediately jump into the next big sideproject, I think I need a breather. I do however have some ideas lined up that I've been wanting to explore for a while now: </p>
<p>- making a top-down racing game (think micromachines)</p>
<p>- create an Outrun or Lotus III-like racing game</p>
<p>- building a ray-tracing game (like wolf3d). </p>
<p>Much more exciting stuff to come.</p>

<h2>Why do this?</h2>
<p>It's my way of keeping my favorite handheld operating system alive.</p>

<h2>It's all in the source!</h2>
<p>For now I hope at least some people will enjoy playing StarGrid and even if it's not their cup of tea, the game is fully open source, so I hope that can contribute to others making games and applications for this not-so-forgotten platform called Palm OS. </p>
<a href="https://github.com/captaintouch/Captains_StarGrid_PalmOS">StarGrid on GitHub</a>

<h2>Tags</h2>
<p>RetroGames, PalmOS, Development, StarGrid</p>



<p>You can get in touch through Mastodon:  </p>
<a href="https://social.linux.pizza/@rxpz">@rxpz@social.linux.pizza</a>
<p>StarGrid has arrived, a Brand-New Palm OS Strategy Game in 2025! was published on 2025-10-21</p>

<a href="https://quarters.captaintouch.com/blog/">Back to the overview</a>

<a href="https://quarters.captaintouch.com/blog/posts.xml" target="_blank" rel="noopener noreferrer">📰 Subscribe to RSS feed 📰</a>
<p><img src="https://quarters.captaintouch.com/blog/posts/images/mozilla.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/alienow.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/cassette.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/linux_now.gif">
    <img src="https://quarters.captaintouch.com/blog/posts/images/ns-best.gif">
</p>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla is heading into multi-billion-dollar iceberg of its own making (221 pts)]]></title>
            <link>https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</link>
            <guid>45654635</guid>
            <pubDate>Tue, 21 Oct 2025 11:39:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/">https://electrek.co/2025/10/20/tesla-heading-into-multi-billion-dollar-iceberg-of-own-making/</a>, See on <a href="https://news.ycombinator.com/item?id=45654635">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>

	<img width="1600" height="909" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/10/Elon-Musk-tesla-FSD-iceberg.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">
	</figure>

<p>Tesla’s ‘Full Self-Driving Supervised’ expansion is back firing as it exposes its shortcomings. Customers left without promised features are growing discontent and demanding to be compensated.</p>



<p>It’s turning into a multi-billion-dollar iceberg of Tesla’s own making.</p>



<p>In 2016, Tesla proudly announced that all its vehicles produced onward are equipped with “all the hardware for full self-driving,” which would be delivered through future software updates.</p>



<p>The automaker turned out to be significantly wrong about that.</p>	
	



<p>At the time, it was producing its electric vehicles with a hardware suite known as HW2, which it had to upgrade to HW3 because it couldn’t support self-driving (FSD) capability.</p>



<p>HW3 was produced in vehicles from 2019 to 2023 and Tesla switched to HW4 in 2024.</p>



<p>At first, CEO Elon Musk claimed that FSD software updates on newer HW4 cars would lag roughly 6 months behind updates to HW3 cars to make sure to deliver the promised self-driving capability to those who have been waiting and paid for the promised capabiltiy a long time ago.</p>



<p>That s<a href="https://electrek.co/2024/10/15/tesla-needs-to-come-clean-about-hw3-before-the-word-fraud-comes-out/">trategy barely lasted a few months</a>. Tesla quickly started releasing new FSD updates to HW4 cars first and it now hasn’t released a significant update to HW3 cars in close to a year.</p>



<p>Tesla only admitted in January 2025 that HW3 won’t be able to support unsupervised self-driving. Musk claimed that Tesla would retrofit the computers, but there has been no word about it for 10 months.</p>



<h2 id="h-tesla-customers-are-starting-to-be-fed-up">Tesla customers are starting to be fed up.</h2>



<p>The catalyst is Tesla’s current FSD expansion in international markets. Previously, Tesla’s FSD was limited to North America, but over the last year, the automaker has been expanding FSD to China and now Australia and New Zealand.</p>



<p>However, the expansion is back-firing as HW3 owners are starting to realize that they will never get what they paid for.</p>



<p>In Australia and NZ, Tesla only launched FSD on HW4 vehicles with no clear plan for HW3, which the automaker already admitted won’t support unsupervised self-driving. The automaker appears to have only adapted its latest version of FSD for HW4 to the Australian market.</p>



<p>To add to the insult, with the launch of FSD in Australia, Tesla started to offer FSD subcriptions for $149 AUD a month for both HW3 and HW3 cars despite the software not being available for HW3.</p>



<p>HW3 owners reached out to <em>Electrek</em> after seeing this in their app:</p>



<figure><img decoding="async" width="926" height="922" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png 926w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=150,149 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=300,300 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=768,765 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=350,348 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=140,139 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.16.58-PM.png?resize=96,96 96w" sizes="(max-width: 926px) 100vw, 926px"></figure>



<p>It’s unclear why would Tesla sell a subcription to something that doesn’t even exist, but it is not helping build confidence with customers.</p>



<p>To try to appease owners, Tesla started sending emails to Australia HW3 owners offering $5,000 discounts on new inventory vehicles when transfering their FSD package:</p>



<figure><img loading="lazy" decoding="async" height="1024" width="546" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?w=546" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png 618w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=80,150 80w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=160,300 160w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=546,1024 546w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=187,350 187w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=140,262 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=534,1000 534w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.12.33-PM.png?resize=150,281 150w" sizes="auto, (max-width: 546px) 100vw, 546px"></figure>



<p>However, this offer is misleading in itself, as it is not actually specific to HW3 owners as the email leads people to believe.</p>



<p>A visit on Tesla’s Australia inventory website shows that Tesla is offering a $5,000 disounct on all inventory vehicles with FSD for any buyer:</p>



<figure><img loading="lazy" decoding="async" height="384" width="1024" src="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?w=1024" alt="" srcset="https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png 2966w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=150,56 150w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=300,112 300w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=768,288 768w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1024,384 1024w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1536,576 1536w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=2048,768 2048w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=350,131 350w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=140,52 140w, https://electrek.co/wp-content/uploads/sites/3/2025/10/Screenshot-2025-10-20-at-4.27.18-PM.png?resize=1600,600 1600w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>Therefore, it has nothing to do with “loyalty”.</p>



<p>As we recently reported, thousands of Tesla owners have now joined <a href="https://electrek.co/2025/10/13/thousands-of-tesla-owners-join-class-action-lawsuit-over-full-self-driving-in-australia/">a class action lawsuit in Australia</a> over Tesla misleading customers with its self-driving promises.</p>



<p>It adds to similar ongoing lawsuits in <a href="https://electrek.co/2025/08/19/tesla-loses-bid-to-kill-class-action-over-misleading-customers-on-self-driving-capabilities-for-years/">the US</a> and <a href="https://electrek.co/2025/09/22/tesla-being-sued-china-over-not-delivering-self-driving-hw3-cars/">China</a>.</p>



<p>With hundreds of thousands of FSD customers who paid up to $15,000 for package, Tesla is on the hook for billions of dollars in compensations or retrofits in the best-case scenario.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>We are seeing more people losing patience and it is only going to get worse.</p>



<p>There were a lot of interesting interactions on this post, which is pretty mild in my opinion. And yet, you see the usual Elon lemmings downplaying Tesla not delivering features it promised:</p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Dear <a href="https://twitter.com/Tesla_AI?ref_src=twsrc%5Etfw">@Tesla_AI</a> Team,<br>I am writing on behalf of the community of Tesla owners equipped with Hardware 3 (HW3) who have purchased the Full Self-Driving (FSD) capability. As dedicated supporters of Tesla’s mission to accelerate the world’s transition to sustainable energy and advance…</p>— shawn.car◼️◼️◼️◼️◼️◼️ (@shawncarelli) <a href="https://twitter.com/shawncarelli/status/1980023896536391866?ref_src=twsrc%5Etfw">October 19, 2025</a></blockquote>
</div></figure>



<p>I don’t want to burst anyone’s bubble, but we need to be realistic here. If you are a HW3 owner and still think that Tesla is going to retrofit your up to 10-years-old car with a computer that is going to make self-driving, you are being delusional.</p>



<p>Tesla will have to end up compensating owners and at this point, I have serious doubts that it will do it by itself without being forced through courts.</p>



<p>Furthermore, it shouldn’t be just people who bought FSD. Tesla said that all cars had the hardware capable of self-driving whether people bought the software package or not. If that’s not true, it affects the resale value of the vehicle regardless of if someone purchased the package.</p>



<p>I have a fairly simple solution for Tesla to make it right.</p>



<p>Tesla needs to offer all HW3 owners a $5,000 loyalty discount, that goes on top of all other incentive program, when upgrading to a new car. </p>




	<p>As for HW3 owners who bought FSD, which basically turned out to be an interest free loan to Tesla for years, the automaker needs to offer free FSD transfer and a $10,000 discount on a car upgrade.</p>



<p>While this might sound like a lot, I think it’s in line with the incredible liability that Tesla is facing from all the on going lawsuits. </p>



<p>On top of it, it will go a long way to regain the trust of long-time customers, which Tesla swindled by selling them features it simply can’t deliver.</p>



<p>The main reason why I think Tesla doesn’t want to do that is that it will likely have to do the same thing to HW4 owners in the next few years and that would be the death of the company.</p>
	<p>
				<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
			</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diamond Thermal Conductivity: A New Era in Chip Cooling (110 pts)]]></title>
            <link>https://spectrum.ieee.org/diamond-thermal-conductivity</link>
            <guid>45654512</guid>
            <pubDate>Tue, 21 Oct 2025 11:16:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/diamond-thermal-conductivity">https://spectrum.ieee.org/diamond-thermal-conductivity</a>, See on <a href="https://news.ycombinator.com/item?id=45654512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Diamond Blankets Will Keep Future Chips Cool"><p><strong>Today’s </strong><strong>stunning computing power</strong> is allowing us to move from human intelligence toward <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">artificial intelligence</a>. And as our machines gain more power, they’re becoming not just tools but decision-makers shaping our future.</p><p>But with great power comes great…heat!</p><p>As nanometer-scale <a href="https://spectrum.ieee.org/tag/transistors">transistors</a> switch at gigahertz speeds, electrons race through circuits, losing energy as heat—which you feel when your laptop or your phone toasts your fingers. As we’ve <a href="https://spectrum.ieee.org/trillion-transistor-gpu" target="_self">crammed more and more transistors onto chips</a>, we’ve lost the room to release that heat efficiently. Instead of the heat spreading out quickly across the silicon, which makes it much easier to remove, it builds up to form hot spots, which can be tens of degrees warmer than the rest of the chip. That extreme heat forces systems to throttle the performance of CPUs and <a href="https://spectrum.ieee.org/tag/gpus">GPUs</a> to avoid degrading the chips.</p><p>In other words, what began as a quest for miniaturization has turned into a battle against thermal energy. This challenge extends across all electronics. In computing, high-performance <a href="https://spectrum.ieee.org/tag/processors">processors</a> demand ever-increasing power densities. (New <a href="https://spectrum.ieee.org/tag/nvidia">Nvidia</a> GPU B300 servers will consume <a href="https://www.nvidia.com/en-us/data-center/dgx-b300/" target="_blank">nearly 15 kilowatts</a> of power.) In communication, both digital and analog systems push transistors to deliver more power for stronger signals and faster data rates. In the <a href="https://spectrum.ieee.org/tag/power-electronics">power electronics</a> used for energy conversion and distribution, efficiency gains are being countered by thermal constraints.</p><p> <img alt="A thick sheet of gray-scale grains." data-rm-shortcode-id="4095938da35f1f8180986be8c5d070c9" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-thick-sheet-of-gray-scale-grains.png?id=61766971&amp;width=980" height="1609" id="1d15b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-thick-sheet-of-gray-scale-grains.png?id=61766971&amp;width=980" width="2427"><small placeholder="Add Photo Caption...">The ability to grow large-grained <a href="https://spectrum.ieee.org/tag/polycrystalline">polycrystalline</a> diamond at low temperature led to a new way to combat heat in transistors. </small><small placeholder="Add Photo Credit...">Mohamadali Malakoutian</small></p><p>Rather than allowing heat to build up, what if we could spread it out right from the start, inside the chip?—diluting it like a cup of boiling water dropped into a swimming pool. Spreading out the heat would lower the temperature of the most critical devices and circuits and let the other time-tested cooling technologies work more efficiently. To do that, we’d have to introduce a highly thermally conductive material inside the IC, mere nanometers from the transistors, without messing up any of their very precise and sensitive properties. Enter an unexpected material—diamond.</p><p>In some ways, diamond is ideal. It’s one of the most thermally conductive materials on the planet—many times more efficient than copper—yet it’s also electrically insulating. However, integrating it into chips is tricky: Until recently we knew how to grow it only at circuit-slagging temperatures in excess of 1,000 °C.</p><p>But my research group at <a href="https://spectrum.ieee.org/tag/stanford">Stanford</a> University has managed what seemed impossible. We can now grow a form of diamond suitable for spreading heat, directly atop semiconductor devices at low enough temperatures that even the most delicate <a href="https://spectrum.ieee.org/tag/interconnects">interconnects</a> inside advanced chips will survive. To be clear, this isn’t the kind of diamond you see in jewelry, which is a large single crystal. Our <a href="https://spectrum.ieee.org/tag/diamond">diamonds</a> are a polycrystalline coating no more than a couple of micrometers thick.</p><p>The potential benefits could be huge. In some of our earliest gallium-nitride radio-frequency transistors, the addition of diamond dropped the device temperature by more than 50 °C. At the lower temperature, the transistors amplified X-band radio signals five times as well as before. We think our diamond will be even more important for advanced <a href="https://spectrum.ieee.org/tag/cmos">CMOS</a> chips. Researchers predict that upcoming chipmaking technologies could make hot spots almost 10 °C hotter [see , “<a href="https://spectrum.ieee.org/hot-chips" target="_self">Future Chips Will Be Hotter Than Ever</a>”, in this issue]. That’s probably why our research is drawing intense interest from the chip industry, including <a href="https://spectrum.ieee.org/tag/applied-materials">Applied Materials</a>, <a href="https://spectrum.ieee.org/tag/samsung">Samsung</a>, and <a href="https://spectrum.ieee.org/tag/tsmc">TSMC</a>. If our work continues to succeed as it has, heat will become a far less onerous constraint in CMOS and other electronics too.</p><h2>Where Heat Begins and Ends in Chips</h2><p data-rm-resized-container="25%"> <img alt="A rectangle of black fading into bright gray at the bottom." data-rm-shortcode-id="97669d7d6818879a240fd8be27d98ec0" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-rectangle-of-black-fading-into-bright-gray-at-the-bottom.png?id=61766985&amp;width=980" height="3532" id="d8b96" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-rectangle-of-black-fading-into-bright-gray-at-the-bottom.png?id=61766985&amp;width=980" width="1646"><small placeholder="Add Photo Caption...">At the boundary between the diamond and the semiconductor, a thin layer of <a href="https://spectrum.ieee.org/tag/silicon-carbide">silicon carbide</a> forms. It acts as a bridge for heat to flow into the diamond. </small><small placeholder="Add Photo Credit...">Mohamadali Malakoutian</small></p><p>Heat starts within transistors and the interconnects that link them, as the flow of current meets resistance. That means most of it is generated near the surface of the semiconductor substrate. From there it rises either through layers of metal and insulation or through the semiconductor itself, depending on the package architecture. The heat then encounters a thermal interface material designed to spread it out before it ultimately reaches a <a href="https://spectrum.ieee.org/tag/heat-sink">heat sink</a>, a radiator, or some sort of <a href="https://spectrum.ieee.org/data-center-liquid-cooling" target="_blank">liquid cooling</a>, where air or fluid carries the heat away.</p><p>The dominant cooling strategies today center around advances in <a href="https://spectrum.ieee.org/tag/heat-sinks">heat sinks</a>, fans, and radiators. In pursuit of even better cooling, researchers have explored liquid cooling using microfluidic channels and removing heat using phase-change materials. Some computer clusters go so far as to submerge the servers in thermally conductive, dielectric—electrically insulating—liquids.</p><p>These innovations are critical steps forward, but they still have limitations. Some are so expensive they’re worthwhile only for the highest-performing chips; others are simply too bulky for the job. (Your smartphone can’t carry a <a href="https://spectrum.ieee.org/xmems" target="_self">conventional fan</a>.) And none are likely to be very effective as we move toward chip architectures resembling silicon skyscrapers that stack multiple layers of chips. Such <a href="https://spectrum.ieee.org/hybrid-bonding" target="_self">3D systems</a> are only as viable as our ability to remove heat from every layer within it.</p><p>The big problem is that chip materials are poor heat conductors, so the heat becomes trapped and concentrated, causing the temperature to skyrocket within the chip. At higher temperatures, transistors leak more current, wasting power; they age more quickly, too.</p><p>Heat spreaders allow the heat to move laterally, diluting it and allowing the circuits to cool. But they’re positioned far—relatively, of course—from where the heat is generated, and so they’re of little help with these hot spots. We need a heat-spreading technology that can exist within nanometers of where the heat is generated. This is where our new low-temperature diamond could be essential.</p><h2>How to Make Diamonds</h2><p>Before my lab turned to developing diamond as a heat-spreading material, we were working on it as a semiconductor. In its single-crystal form—like the kind on your finger—it has a <a href="https://spectrum.ieee.org/tag/wide-bandgap">wide bandgap</a> and ability to withstand enormous electric fields. Single-crystalline diamond also offers some of the highest thermal conductivity recorded in any material, reaching 2,200 to 2,400 watts per meter per kelvin—roughly six times as conductive as copper. Polycrystalline diamond—an easier to make material—can approach these values when grown thick. Even in this form, it outperforms copper.</p><p>As attractive as diamond transistors might be, I was keenly aware—based on my experience researching <a href="https://spectrum.ieee.org/tag/gallium-nitride">gallium nitride</a> devices—of the long road ahead. The problem is one of scale. Several companies are working to scale high-purity diamond substrates to 50, 75, and even 100 millimeters but the diamond substrates we could acquire commercially were only about 3 mm across.</p><p> <img alt="A polygon with layers demarcated in it surrounded by a jagged blue area. " data-rm-shortcode-id="4d352398f974422975b19c4e1d3f5ecd" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-polygon-with-layers-demarcated-in-it-surrounded-by-a-jagged-blue-area.png?id=61771341&amp;width=980" height="1760" id="38b33" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-polygon-with-layers-demarcated-in-it-surrounded-by-a-jagged-blue-area.png?id=61771341&amp;width=980" width="3200"><small placeholder="Add Photo Caption...">Gallium nitride high-electron-mobility transistors were an ideal test case for diamond cooling. The devices are 3D and the critical heat-generating part, the two-dimensional <a href="https://spectrum.ieee.org/tag/electron-gas">electron gas</a>, is close to the surface. </small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>So we decided instead to try growing diamond films on large silicon wafers, in the hope of moving toward commercial-scale diamond substrates. In general, this is done by reacting methane and hydrogen at high temperatures, 900 °C or more. This results in not a single crystal but a forest of narrow columns. As they grow taller, the nanocolumns coalesce into a uniform film, but by the time they form high-quality polycrystalline diamond, the film is already very thick. This thick growth adds stress to the material and often leads to cracking and other problems.</p><p>But what if we used this polycrystalline coating as a heat spreader for other devices? If we could get diamond to grow within nanometers of transistors, get it to spread heat both vertically and laterally, and integrate it seamlessly with the silicon, metal, and <a href="https://spectrum.ieee.org/tag/dielectric">dielectric</a> in chips, it might do the job.</p><p>There were good reasons to think it would work. Diamond is electrically insulating, and it has a relatively low dielectric constant. That means it makes a poor capacitor, so signals sent through diamond-encrusted interconnects might not degrade much. Thus diamond could act as a “thermal dielectric,” one that is electrically insulating but thermally conducting.</p><p> <img alt="SEM images showing surface before and after polycrystalline diamond growth on silicon oxide." data-rm-shortcode-id="d13cce8207e028b3de6559e6ff93e683" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/sem-images-showing-surface-before-and-after-polycrystalline-diamond-growth-on-silicon-oxide.png?id=61771368&amp;width=980" height="1980" id="94099" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/sem-images-showing-surface-before-and-after-polycrystalline-diamond-growth-on-silicon-oxide.png?id=61771368&amp;width=980" width="2280"><small placeholder="Add Photo Caption...">Polycrystalline diamond could help reduce temperatures inside <a href="https://spectrum.ieee.org/tag/3d-integration">3D chips</a>. Diamond thermal vias would grow inside micrometers-deep holes so heat can flow from vertically from one chip to a diamond heat spreader in another chip that’s stacked atop it.  </small><small placeholder="Add Photo Credit...">Dennis Rich</small></p><p>For our plan to work, we were going to have to learn to grow diamond differently. We knew there wasn’t room to grow a thick film inside a chip. We also knew the narrow, spiky crystal pillars made in the first part of the growth process don’t transmit heat laterally very well, so we’d need to grow large-grained crystals from the start to get the heat moving horizontally. A third problem was that the existing diamond films didn’t form a coating on the sides of devices, which would be important for inherently <a href="https://spectrum.ieee.org/tag/3d-devices">3D devices</a>. But the biggest impediment was the high temperature needed to grow the diamond film, which would damage, if not destroy, an IC’s circuits. We were going to have to cut the growth temperature at least in half.</p><p>Just lowering the temperature doesn’t work. (We tried: You wind up, basically, with soot, which is electrically conductive—the opposite of what’s needed.) We found that adding oxygen to the mix helped, because it continuously etched away carbon deposits that weren’t diamond. And through <a href="https://www.mdpi.com/2073-4352/9/10/498" target="_blank">extensive experimentation</a>, we were able to find a formula that produced coatings of large-grained polycrystalline diamond all around devices at 400 °C, which is a survivable temperature for CMOS circuits and other devices.</p><h2>Thermal Boundary Resistance</h2><p>Although we had found a way to grow the right kind of diamond coatings, we faced another critical challenge—the <a href="https://spectrum.ieee.org/tag/phonon">phonon</a> bottleneck, also known as thermal boundary resistance (TBR). <a href="https://spectrum.ieee.org/tag/phonons">Phonons</a> are packets of heat energy, in the way that photons are packets of electromagnetic energy. Specifically, they’re a quantized version of the vibration of a crystal lattice. These phonons can pile up at the boundary between materials, resisting the flow of heat. Reducing TBR has long been a goal in thermal interface engineering, and it is often done by introducing different materials at the boundary. But semiconductors are compatible only with certain materials, limiting our choices.</p><p data-rm-resized-container="25%"> <img alt="A cartoon of squares stacked atop one another and connected by a forest of vertical links. " data-rm-shortcode-id="0d42a3ab5d16646f88e7d980f17f5817" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-cartoon-of-squares-stacked-atop-one-another-and-connected-by-a-forest-of-vertical-links.png?id=61771378&amp;width=980" height="1286" id="386f9" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-cartoon-of-squares-stacked-atop-one-another-and-connected-by-a-forest-of-vertical-links.png?id=61771378&amp;width=980" width="1113"><small placeholder="Add Photo Caption...">Thermal scaffolding would link layers of heat-spreading polycrystalline diamond in one chip to those in another chip in a 3D-stacked silicon. The thermal pillars would traverse each chip’s interconnects and dielectric material to move heat vertically through the stack. </small><small placeholder="Add Photo Credit...">Srabanti Chowdhury</small></p><p>In the end, we got lucky. While growing diamond on GaN capped with silicon nitride, we observed something unexpected: The measured TBR was <a href="https://pubs.acs.org/doi/full/10.1021/acsami.1c13833" target="_blank">much lower than prior reports led us to expect</a>. (The low TBR was independently measured, initially by <a href="https://research-information.bris.ac.uk/en/persons/martin-h-h-kuball" target="_blank">Martin Kuball</a> at the University of Bristol, in England, and later by <a href="https://me.umd.edu/clark/faculty/1618/Samuel-GrahamJr" target="_blank">Samuel Graham Jr</a>., then at <a href="https://spectrum.ieee.org/tag/georgia-tech">Georgia Tech</a>, who both have been coauthors and collaborators in several of our papers.)</p><p>Through further investigation of the interface science and engineering, and in collaboration with <a href="https://mse.utdallas.edu/ourteam/faculty/cho-k/" target="_blank">K.J. Cho</a> at the University of Texas at Dallas, we identified the cause of the lower TBR. <a href="https://ieeexplore.ieee.org/document/10413734" target="_blank">Intermixing at the interface</a> between the diamond and silicon nitride led to the formation of silicon carbide, which acted as a kind of bridge for the phonons, allowing more efficient heat transfer. Though this began as a scientific discovery, its technological impact was immediate—with a silicon carbide interface, our devices exhibited significantly improved thermal performance.</p><h2>GaN HEMTs: The First Test Case</h2><p>We began testing our new low-TBR diamond coatings in gallium nitride high-electron-mobility transistors (HEMTs). These devices amplify RF signals by controlling current through a two-dimensional electron gas that forms within its channel. We leveraged the pioneering research on HEMTs done by <a href="https://engineering.ucsb.edu/people/umesh-mishra" target="_blank">Umesh Mishra</a>’s laboratory at the University of California, Santa Barbara, where I had been a graduate student. The Mishra lab invented a particular form of the material called N-polar gallium nitride. Their N-polar GaN HEMTs demonstrate exceptional power density at high frequencies, particularly in the W-band, the 75- to 110-gigahertz part of the microwave spectrum.</p><p>RELATED: <a href="https://spectrum.ieee.org/silicon-carbide" target="_self">Gallium Nitride and Silicon Carbide Fight for Green Tech Domination</a></p><p>What made these HEMTs such a good test case is one defining feature of the device: The gate, which controls the flow of current through the device, is within tens of nanometers of the transistor’s channel. That means that heat is generated very close to the surface of the device, and any interference our diamond coating could cause would quickly show in the device’s operation.</p><p>We introduced the diamond layer so that it surrounded the HEMT completely, even on the sides. By maintaining a growth temperature below 400 °C, we hoped to preserve core device functionality. While we did see some decline in high-frequency performance, the thermal benefits were substantial—<a href="https://ieeexplore.ieee.org/document/10019509" target="_blank">channel temperatures dropped by a remarkable 70 °C</a>. This breakthrough could be a potentially transformative solution for RF systems, allowing them to operate at higher power than ever before.</p><h2>Diamond in CMOS</h2><p>We wondered if our diamond layer could also work in high-power CMOS chips. My colleagues at Stanford, <a href="https://web.stanford.edu/~hspwong/" target="_blank">H.-S. Philip Wong</a> and <a href="https://profiles.stanford.edu/subhasish-mitra" target="_blank">Subhasish Mitra</a>, have long championed 3D-stacked chip architectures. In CMOS computing chips, <a href="https://spectrum.ieee.org/tag/3d-stacking">3D stacking</a> appears to be the most viable way forward to increase integration density, improve performance, and overcome the limitations of traditional <a href="https://spectrum.ieee.org/tag/transistor-scaling">transistor scaling</a>. It’s already used in some advanced <a href="https://spectrum.ieee.org/tag/ai-chips">AI chips</a>, such as <a href="https://spectrum.ieee.org/amd-mi300" target="_self">AMD’s MI300 series</a>. And it’s established in the high-bandwidth memory chips that pump data through Nvidia GPUs and other AI processors. The multiple layers of silicon in these 3D stacks are mostly connected by microscopic balls of solder, or in some advanced cases just by their copper terminals. Getting signals and power out of these stacks requires vertical copper links that burrow through the silicon to reach the chip package’s substrate.</p><p>In one of our discussions, Mitra pointed out that a critical issue with 3D-stacked chips is the thermal bottlenecks that form within the stack. In 3D architectures, the traditional heat sinks and other techniques used for 2D chips aren’t sufficient. Extracting heat from each layer is essential.</p><p>Our research could redefine <a href="https://spectrum.ieee.org/tag/thermal-management">thermal management</a> across industries.</p><p>Our experiments on thermal boundary resistance in GaN suggested a similar approach would work in silicon. And when we integrated diamond with silicon, the results were remarkable: An interlayer of silicon carbide formed, leading to diamond with an excellent thermal interface.</p><p>Our effort introduced the concept of thermal scaffolding. In that scheme, nanometers-thick layers of polycrystalline diamond would be integrated within the dielectric layers above the transistors to spread heat. These layers would then be connected by vertical heat conductors, called thermal pillars, made of copper or more diamond. These pillars would connect to another heat spreader, which in turn would link to thermal pillars on the next chip in the 3D stack, and so on until the heat reached the heat sink or other cooling device.</p><p> <img alt="Temperature vs. compute tier graph; AI accelerator heats most without scaffold." data-rm-shortcode-id="f142bfc12529f56435398ad67060129f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/temperature-vs-compute-tier-graph-ai-accelerator-heats-most-without-scaffold.png?id=61771384&amp;width=980" height="1465" id="8310e" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/temperature-vs-compute-tier-graph-ai-accelerator-heats-most-without-scaffold.png?id=61771384&amp;width=980" width="2380"><small placeholder="Add Photo Caption...">The more tiers of computing silicon in a 3D chip, the bigger difference thermal scaffolding makes. An AI accelerator with more than five tiers would well exceed typical temperature limits unless the scaffolding was employed. </small><small placeholder="Add Photo Credit...">Srabanti Chowdhury</small></p><p>In a collaboration with Mitra, we used simulations of heat generated by real computational workloads to operate a proof-of-concept structure. This structure consisted of dummy heaters to mimic hot spots in a two-chip stack along with diamond heat spreaders and copper thermal pillars. Using this, we <a href="https://ieeexplore.ieee.org/document/10873424" target="_blank">reduced the temperature to one-tenth</a> its value without the scaffolding.</p><p>There are hurdles still to overcome. In particular, we still have to figure out a way to make the top of our diamond coatings atomically flat. But, in collaboration with industry partners and researchers, we are systematically studying that problem and other scientific and technological issues. We and our partners think this research could offer a disruptive new path for thermal management and a crucial step toward sustaining <a href="https://spectrum.ieee.org/tag/high-performance-computing">high-performance computing</a> into the future.</p><h2>Developing Diamond Thermal Solutions</h2><p>We now intend to move toward industry integration. For example, we’re working with the <a href="https://www.darpa.mil/research/programs/threads-heat-removal" target="_blank">Defense Advanced Research Projects Agency Threads</a> program, which aims to use device-level thermal management to develop highly efficient and reliable X-band power amplifiers with a power density 6 to 8 times as efficient as today’s devices. The program, which was conceived and initially run by <a href="https://forward.darpa.mil/presenters/Dr-Thomas-Kazior" target="_blank">Tom Kazior</a>, is a critical platform for validating the use of low-temperature diamond integration in GaN HEMT manufacturing. It’s enabled us to collaborate closely with industry teams while protecting both our and our partners’ processes. Defense applications demand exceptional reliability, and our diamond-integrated HEMTs are undergoing rigorous testing with industry partners. The early results are promising, guiding refinements in growth processes and integration techniques that we’ll make with our partners over the next two years.</p><p>But our vision extends beyond GaN HEMTs to <a href="https://iopscience.iop.org/article/10.35848/1882-0786/abf4f1" target="_blank">other materials</a> and particularly silicon computational chips. For the latter, we have an established collaboration with TSMC, and we’re expanding on newer opportunities with Applied Materials, <a href="https://spectrum.ieee.org/tag/micron">Micron</a>, Samsung, and others through the <a href="https://systemx.stanford.edu/" target="_blank">Stanford SystemX Alliance</a> and the <a href="https://www.src.org/" target="_blank">Semiconductor Research Corp.</a> This is an extraordinary level of collaboration among otherwise fierce competitors. But then, heat is a universal challenge in <a href="https://spectrum.ieee.org/tag/chip-manufacturing">chip manufacturing</a>, and everyone is motivated to find the best solutions.</p><p>If successful, our research could redefine thermal management across industries. In my work on gallium nitride devices, I have seen firsthand how once-radical ideas like this transition to become industry standards, and I believe diamond-based heat extraction will follow the same trajectory, becoming a critical enabler for a generation of electronics that is no longer hindered by heat. <span></span></p><p><em>This article appears in the November 2025 print issue as “Diamond Blankets Will Chill Future Chips.”</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US chess grandmaster Daniel Naroditsky dies aged 29 (125 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c15pz8vpjp9o</link>
            <guid>45654382</guid>
            <pubDate>Tue, 21 Oct 2025 10:44:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c15pz8vpjp9o">https://www.bbc.com/news/articles/c15pz8vpjp9o</a>, See on <a href="https://news.ycombinator.com/item?id=45654382">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span data-testid="byline-new-contributors"><p><span>Harry Sekulich</span><span data-testid="undefined-role-location"></span><span> and</span></p><p><span>Gabriela Pomeroy</span><span data-testid="undefined-role-location"></span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/d756/live/f190e630-ae2a-11f0-b2a1-6f537f66f9aa.jpg.webp" loading="eager" alt="Charlotte Chess Center Close-up shot of Daniel Naroditsky wearing a deep navy polo"><span>Charlotte Chess Center</span></p></div><p data-component="caption-block"><figcaption>Daniel Naroditsky, also known to his online fans as 'Danya', died two weeks out from his 30th birthday</figcaption></p></figure><div data-component="text-block"><p>US chess grandmaster and online commentator Daniel Naroditsky has died aged 29.</p><p>The popular chess player's family announced his "unexpected" death in a statement released by his club, the Charlotte Chess Center, on Monday. No cause of death was given.</p><p>"It is with great sadness that we share the unexpected passing of Daniel Naroditsky," the statement said. "Daniel was a talented chess player, commentator and educator, and a cherished member of the chess community, admired and respected by fans and players around the world."</p><p>The US and International chess federations have paid tribute to Naroditsky, along with other professional players.</p></div><div data-component="text-block"><p>American world number two Hikaru Nakamura said he was "devastated" at the news.</p><p>"This is a massive loss for the world of chess," Nakamura said in a social media post.</p><p>As well as competing in high-level events, Naroditsky ran a chess YouTube channel, with nearly 500,000 subscribers. </p><p>His Twitch stream drummed up 340,000 followers, with hundreds of thousands of viewers drawn to his regular video tutorials and livestreams against competitors. Fans praised his insight and passion, casually referring to him as 'Danya'.</p><p>He played a "pivotal role in popularising chess content online," the International Chess Federation said. </p><p>Naroditsky first took an interest in chess at the age of six, when his older brother Alan introduced him to the game to help entertain a group of children at a birthday party.</p><p>His father Vladimir and multiple coaches soon noticed his talents.</p><p>"As far as I was concerned, I was just playing games with my brother," Naroditsky told the New York Times in a 2022 interview.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a5c1/live/a123d4d0-ae1d-11f0-ba75-093eca1ac29b.jpg.webp" loading="lazy" alt="Getty Images A young Daniel Naroditsky sitting behind a chessboard "><span>Getty Images</span></p></div><p data-component="caption-block"><figcaption>Naroditsky in 2008, following his World Youth Championship victory in Turkey</figcaption></p></figure><div data-component="text-block"><p>He gained international attention in 2007 when he won the under-12 boys world youth championship in Antalya, Turkey. In 2010, at the age of 14, he became one of the youngest ever published chess authors when he wrote a book titled Mastering Positional Chess, covering practical skills and technical manoeuvrings.</p><p>In 2013 Naroditsky won the US Junior Championship, helping him earn the title of grandmaster, the international chess federation's highest-ranked chess competitor, while he was still a teenager.</p><p>Naroditsky later graduated from Stanford University and worked as a chess coach in Charlotte, North Carolina.</p><p>In 2022 the New York Times named Naroditsky as its "new chess columnist" and invited him to contribute to a series of chess puzzles for the newspaper's games section.</p><p>In the publication's accompanying interview, the young grandmaster mused on chess's influence in his life.</p><p>"Even at my level, I can still discover beautiful things about the game every single time I train, teach, play or am a commentator at a tournament," he said.</p></div><div data-component="text-block"><p>Nemo Zhou –  a Toronto-based Woman Chess Grandmaster (WGM) and chess content creator – told the BBC Naroditsky was a friend and an "inspiration."</p><p>Zhou played chess with him, both in person and virtually at chess events across the US.</p><p>He was "everything that the combination of chess and content creation was supposed to be – he had this way to make chess fun", she said. </p><p>She added that he was known for being a "true historian of the game" who had a great memory for chess facts and historical games, and "did everything with kindness."</p><p>"Without people like him I probably would have quit chess at 17 and never touched it again," she said. </p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251015-170100-8e96f025b0-web-2.31.4-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/7789/live/557c6130-ae65-11f0-bf6e-0db822206ec2.jpg.webp" loading="lazy" alt="International Chess Federation Naroditsky playing chess with spectators behind"><span>International Chess Federation</span></p></div></figure></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pasta/80 is a simple Pascal cross compiler targeting the Z80 microprocessor (102 pts)]]></title>
            <link>https://github.com/pleumann/pasta80</link>
            <guid>45653330</guid>
            <pubDate>Tue, 21 Oct 2025 07:23:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pleumann/pasta80">https://github.com/pleumann/pasta80</a>, See on <a href="https://news.ycombinator.com/item?id=45653330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/logo.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/logo.png" alt="Logo"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">PASTA/80</h2><a id="user-content-pasta80" aria-label="Permalink: PASTA/80" href="#pasta80"></a></p>
<p dir="auto">PASTA/80 is a simple <a href="https://en.wikipedia.org/wiki/Pascal_(programming_language)" rel="nofollow">Pascal</a> cross compiler targeting the <a href="https://en.wikipedia.org/wiki/Zilog_Z80" rel="nofollow">Z80</a> microprocessor. It generates code for these classic and modern machines:</p>
<ul dir="auto">
<li><a href="https://en.wikipedia.org/wiki/CP/M" rel="nofollow">CP/M</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sinclair_ZX_Spectrum" rel="nofollow">ZX Spectrum 48K</a></li>
<li><a href="https://en.wikipedia.org/wiki/ZX_Spectrum#ZX_Spectrum_128" rel="nofollow">ZX Spectrum 128K</a></li>
<li><a href="https://www.specnext.com/" rel="nofollow">ZX Spectrum Next</a></li>
</ul>
<p dir="auto">The compiler follows the single-pass recursive-descent approach championed by <a href="https://de.wikipedia.org/wiki/Niklaus_Wirth" rel="nofollow">Niklaus Wirth</a>, inventor of Pascal, in his books and lectures. It doesn't have an explicit syntax tree, but instead generates code on the fly during parsing. As a result, the compiler might not always generate the most efficient code possible (it definitely cannot compete with LLVM and doesn't try to), but it's very fast.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported language elements</h2><a id="user-content-supported-language-elements" aria-label="Permalink: Supported language elements" href="#supported-language-elements"></a></p>
<p dir="auto">The supported Pascal dialect is an almost exact clone of the original <a href="https://en.wikipedia.org/wiki/Turbo_Pascal" rel="nofollow">Turbo Pascal 3.0</a> for CP/M (see <a href="https://bitsavers.trailing-edge.com/pdf/borland/turbo_pascal/Turbo_Pascal_Version_3.0_Reference_Manual_1986.pdf" rel="nofollow">this manual</a> for details). So you have at your disposal the following language elements:</p>
<ul dir="auto">
<li>All the basic data types (<code>Boolean</code>, <code>Byte</code>, <code>Char</code>, <code>Integer</code>, <code>Pointer</code>, <code>Real</code> and <code>String</code>).</li>
<li><code>array of</code>, <code>record</code>, <code>set of</code>, enumerations, subranges and pointers as a way of building new data types.</li>
<li>The decision-making elements <code>if..then..else</code> and <code>case..of</code>.</li>
<li>The loop elements <code>for..do</code>, <code>while..do</code> and <code>repeat..until</code>.</li>
<li>The <code>with..do</code> notation for "opening" records.</li>
<li><code>procedure</code> and <code>function</code> including value and <code>var</code> parameters and nesting.</li>
<li>The standard procedures for screen input and output (i.e. <code>ReadLn</code>, <code>WriteLn</code> etc.).</li>
<li>All conversion and utility procedures and functions that Turbo Pascal 3.0 had.</li>
<li>The three kinds of disk files, that is untyped (<code>file</code>), typed (<code>file of</code>) and <code>Text</code>.</li>
<li>A dynamic heap of up to 32767 bytes with <code>GetMem</code>, <code>FreeMem</code>, <code>New</code> and <code>Dispose</code>.</li>
<li>Inline assembly (via opcodes, not via mnemonics, so <a href="https://clrhome.org/table/" rel="nofollow">this page</a> might be handy).</li>
<li>Overlays (in memory, Spectrum 128K and Next only, see below).</li>
<li>Some compiler directives:
<ul dir="auto">
<li><code>$i &lt;file&gt;</code> for including Pascal source files (including nesting and cycle detection)</li>
<li><code>$l &lt;file&gt;</code> for including an assembly file (aka "linking" a library)</li>
<li><code>$a(+/-)</code>   for enabling or disabling absolute mode (default is on, disable for recursion)</li>
<li><code>$i(+/-)</code>   for enabling or disabling IO checking (when off, check <code>IOResult</code> after calls)</li>
<li><code>$k(+/-)</code>   for enabling or disabling stack overflow checking</li>
<li><code>$u(+/-)</code>   for enabling or disabling Ctrl-C checking</li>
</ul>
</li>
</ul>
<p dir="auto">The compiler also has some features that were borrowed from or inspired by later versions of Turbo Pascal:</p>
<ul dir="auto">
<li>C-style <code>//</code> one-line comments in addition to <code>{..}</code> and <code>(*..*)</code>.</li>
<li>Binary literals (using a <code>%</code> prefix).</li>
<li><code>Break</code> and <code>Continue</code> for loop control.</li>
<li>Querying the keyboard via <code>KeyPressed</code> and <code>ReadKey</code>.</li>
<li>Color support via <code>TextColor</code> and <code>TextBackground</code> with constants for the 8 Spectrum Next colors.</li>
<li><code>Inc</code> and <code>Dec</code> for more efficient increasing and decreasing of variables.</li>
<li><code>Include</code> and <code>Exclude</code> for more efficient handling of sets.</li>
<li>A simple <code>Assert</code> facility that counts passes/fails and shows the failed line number.</li>
</ul>
<p dir="auto">Since that covers most of the functionality of Turbo Pascal 3 you might ask what is missing. These are the current limitations:</p>
<ul dir="auto">
<li>All the remaining compiler directives are not yet supported.</li>
<li><code>Mark</code>/<code>Release</code> are not currently supported.</li>
<li>The standard files <code>Input</code>, <code>Output</code>, <code>Kbd</code>, <code>Con</code> and <code>Lst</code> are not supported.</li>
<li><code>Chain</code> and <code>Execute</code> are not supported.</li>
<li>Add-on libraries from the PC version of Turbo Pascal 3.0 are not yet supported (although there are a few graphics primitives for the ZX targets).</li>
<li>The <a href="https://wiki.specnext.dev/Extended_Z80_instruction_set" rel="nofollow">new instructions of the Z80N CPU</a> inside the ZX Spectrum Next are not yet being leveraged.</li>
<li>No separate compilation. Everything is compiled from source, always.</li>
<li>Binary size is quite large compared to the original.</li>
</ul>
<p dir="auto">The runtime library, being partially written in Pascal itself, gets quite large when compiled. I hope to bring this down again by reimplementing more of it in Z80 assembly (or improve the code generator, which, although it has a peephole optimizer, is not generating super-efficient Z80 code).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and setting up the compiler</h2><a id="user-content-building-and-setting-up-the-compiler" aria-label="Permalink: Building and setting up the compiler" href="#building-and-setting-up-the-compiler"></a></p>
<p dir="auto">The compiler is itself written in Pascal. You can compile it with <a href="https://www.freepascal.org/" rel="nofollow">Free Pascal</a> (I use version 3.2.2). Just run</p>

<p dir="auto">The Pascal compiler generates Z80 assembler code and relies on <a href="https://z00m128.github.io/sjasmplus" rel="nofollow">sjasmplus</a> as a backend for the final translation step to binary. It can also, in <code>--ide</code> mode (see below), make use of various other external tools. The compiler tries to detect these external tools automatically (from your system's <code>PATH</code>), but sometimes it's best to create a file <code>.pasta80.cfg</code> in your home directory specifying necessary paths (there is a sample in <code>misc</code> that you can adapt).</p>
<div data-snippet-clipboard-copy-content="# PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
..."><pre><code># PASTA/80 config

HOME      = ~/Spectrum/pasta80
ASSEMBLER = ~/Spectrum/sjasmplus/sjasmplus
...
</code></pre></div>
<p dir="auto">You can check your whole setup by calling the compiler with <code>--config</code>. It will show the full paths of all internal and external requirements and whether they are fulfilled.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using the compiler</h2><a id="user-content-using-the-compiler" aria-label="Permalink: Using the compiler" href="#using-the-compiler"></a></p>
<p dir="auto">To run the compiler just invoke the executable with the name of a Pascal source file to translate.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CP/M target</h3><a id="user-content-cpm-target" aria-label="Permalink: CP/M target" href="#cpm-target"></a></p>
<p dir="auto">The default target is CP/M. There is an optional parameter that enables some simple peephole optimizations and another one that uses dependency analysis to eliminate unused Pascal procedures and functions:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta hello.pas             # Compiles hello.pas to hello.com
$ pasta hello                 # Source file .pas suffix is optional
$ pasta --opt hello.pas       # Enables peephole optimizations
$ pasta --opt --dep hello.pas # The same plus dependency analysis"><pre>$ pasta hello.pas             <span><span>#</span> Compiles hello.pas to hello.com</span>
$ pasta hello                 <span><span>#</span> Source file .pas suffix is optional</span>
$ pasta --opt hello.pas       <span><span>#</span> Enables peephole optimizations</span>
$ pasta --opt --dep hello.pas <span><span>#</span> The same plus dependency analysis</span></pre></div>
<p dir="auto">You can run the resulting <code>.com</code> files on a real CP/M machine or in a CP/M emulator. I recommend the excellent <a href="https://gitlab.com/gbrein/tnylpo" rel="nofollow">tnylpo</a>. For programs that use VT52 control codes you have to start tnylpo in full-screen mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ tnylpo hello                # Run in line-mode
$ tnylpo -s -t @ hello        # Monochrome full-screen, wait when finished
$ tnylpo -soy,4,0 -t @ hello  # Color full-screen, wait when finished"><pre>$ tnylpo hello                <span><span>#</span> Run in line-mode</span>
$ tnylpo -s -t @ hello        <span><span>#</span> Monochrome full-screen, wait when finished</span>
$ tnylpo -soy,4,0 -t @ hello  <span><span>#</span> Color full-screen, wait when finished</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>"Hello, World" in line mode</th>
<th>"Hello, World" in full-screen</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello1.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello1.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello2.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello2.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">ZX Spectrum targets</h3><a id="user-content-zx-spectrum-targets" aria-label="Permalink: ZX Spectrum targets" href="#zx-spectrum-targets"></a></p>
<p dir="auto">To generate binaries for the ZX Spectrum 48K, 128K and Next targets, use the <code>--zx48</code>, <code>--zx128</code> and <code>--zxnext</code> parameters, respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx48 hello.pas      # Compiles for ZX Spectrum 48K
$ pasta --zx128 hello.pas     # Compiles for ZX Spectrum 48K
$ pasta --zxnext hello.pas    # Compiles for ZX Spectrum Next"><pre>$ pasta --zx48 hello.pas      <span><span>#</span> Compiles for ZX Spectrum 48K</span>
$ pasta --zx128 hello.pas     <span><span>#</span> Compiles for ZX Spectrum 48K</span>
$ pasta --zxnext hello.pas    <span><span>#</span> Compiles for ZX Spectrum Next</span></pre></div>
<p dir="auto">The main difference between the three (currently) is that the ZX Spectrum Next target supports file IO (on the SD card), while the other two do not. The remaining routines are mostly the same. Screen output is handled via <code>rst $10</code> in the ROM. In both cases the binaries are expected to be run from address 0x8000.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tapes, snapshots and runnable directories</h3><a id="user-content-tapes-snapshots-and-runnable-directories" aria-label="Permalink: Tapes, snapshots and runnable directories" href="#tapes-snapshots-and-runnable-directories"></a></p>
<p dir="auto">The default output format for the ZX Spectrum targets is a simple binary file that contains exactly the bytes of the compiled program (plus a +3DOS header when compiling for the Spectrum Next). In addition to that (and for more complex cases involving overlays), the compiler can also generate snapshot files or tape files, the latter including a suitable BASIC loader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx48 --sna examples/hello.pas   # .sna file
$ pasta --zx48 --tap examples/jacques.pas # .tap file with BASIC loader"><pre>$ pasta --zx48 --sna examples/hello.pas   <span><span>#</span> .sna file</span>
$ pasta --zx48 --tap examples/jacques.pas <span><span>#</span> .tap file with BASIC loader</span></pre></div>
<p dir="auto">Being self-contained, snapshots and tapes are a convenient way to distribute your programs and to launch them an emulator, such as Fuse:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ open -a Fuse examples/hello.sna         # Launch .sna file in FUSE (on Mac)
$ open -a Fuse examples/jacques.tap       # Launch .tap file in FUSE (on Mac)"><pre>$ open -a Fuse examples/hello.sna         <span><span>#</span> Launch .sna file in FUSE (on Mac)</span>
$ open -a Fuse examples/jacques.tap       <span><span>#</span> Launch .tap file in FUSE (on Mac)</span></pre></div>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Hello world in FUSE</th>
<th>Frere Jacques in FUSE (yes, with sound!)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/hello3.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/hello3.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/jacques.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/jacques.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">When compiling for the Next, another useful format is a runnable directory. It contains exactly the same files that would also be in the .tap file, including a BASIC loader named <code>run.bas</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zxnext --run examples/pq.pas    # Results in directory named pq.run"><pre>$ pasta --zxnext --run examples/pq.pas    <span><span>#</span> Results in directory named pq.run</span></pre></div>
<p dir="auto">The directory has the suffix <code>.run</code>. When attempting to enter such a directory in the Next's file browser, the loader is started automatically (press Symbol Shift + Enter to really see the contents). If you are a Mac user: Yes, it's a bit like an <code>.app</code> bundle.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overlays</h3><a id="user-content-overlays" aria-label="Permalink: Overlays" href="#overlays"></a></p>
<p dir="auto">The Spectrum 128K and Next targets support overlays. This means you can have larger programs than would normally fit into the 64K address space of a Z80 machine. The rules are the same as for Turbo Pascal 3.0:</p>
<ul dir="auto">
<li>Overlays can be applied to global procedures and functions only, not to nested ones (though nested ones will be overlayed if the containing ones are, too).</li>
<li>Overlays cannot be applied to global variables, that is, you cannot use them for data (at least not without tricks).</li>
<li>All consecutive procedures and functions that are marked as <code>overlay</code> go into the same overlay. Use any declaration inbetween to separate overlays.</li>
</ul>
<p dir="auto">In the following example, there are three overlays: Overlay 0 contains A and B, overlay 1 contains D, and overlay 2 contains E.</p>
<div dir="auto" data-snippet-clipboard-copy-content="overlay procedure A; (* Overlay 0 *)
begin
end;

overlay procedure B; (* Overlay 0 *)
begin
end;

procedure C; (* Not in an overlay *)
begin
end;

overlay procedure D; (* Overlay 1 *)
begin
end;

type
  Dummy = Integer;   (* Separator *)

overlay procedure E; (* Overlay 2 *)
begin
end;"><pre>overlay <span>procedure</span> <span>A</span>; <span><span>(*</span> Overlay 0 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

overlay <span>procedure</span> <span>B</span>; <span><span>(*</span> Overlay 0 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

<span>procedure</span> <span>C</span>; <span><span>(*</span> Not in an overlay <span>*)</span></span>
<span>begin</span>
<span>end</span>;

overlay <span>procedure</span> <span>D</span>; <span><span>(*</span> Overlay 1 <span>*)</span></span>
<span>begin</span>
<span>end</span>;

<span>type</span>
  Dummy = Integer;   <span><span>(*</span> Separator <span>*)</span></span>

overlay <span>procedure</span> <span>E</span>; <span><span>(*</span> Overlay 2 <span>*)</span></span>
<span>begin</span>
<span>end</span>;</pre></div>
<p dir="auto">In contrast to Turbo Pascal 3.0, overlays are not implemented via disk files. Instead, they use the additional RAM of the Spectrum 128K and Next machines. The uppermost 16K bank (Spectrum 128K) or 8K page (Spectrum Next) will be reserved for overlays. Each overlay can have a maximum size of 8K. The compiler manages everything and generates special "far calls" whenever necessary.</p>
<p dir="auto">To enable overlays, use the <code>--ovr</code> command line parameter, ideally in conjuncton with the <code>--tap</code> parameter, as the tape loaders for 128K and Next are fully overlay-aware.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas # Test suite as 128K tape"><pre>$ pasta --zx128 --tap --opt --dep --ovr tests/all.pas <span><span>#</span> Test suite as 128K tape</span></pre></div>
<p dir="auto">The compiler prints a report of which overlays go into which RAM banks or pages.</p>
<div data-snippet-clipboard-copy-content="----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -> tests/all.z80
Assembling...
  tests/all.z80 -> tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3"><pre><code>----------------------------------------
PASTA/80 Pascal System      Version 0.96
                            ZX 128K, Z80

Copyright (C) 2020-25 by  Joerg Pleumann
----------------------------------------

Compiling...
  tests/all.pas -&gt; tests/all.z80
Assembling...
  tests/all.z80 -&gt; tests/all.tap

Program   : 10781 bytes ($8000-$AA1C)
Heap      :  1507 bytes ($AA1D-$AFFF)
Stack     :  4096 bytes ($B000-$BFFF)

Overlay  0:  7399 bytes ($C000-$DCE6) in bank  0
Overlay  1:  7185 bytes ($E000-$FC10) in bank  0
Overlay  2:  2725 bytes ($C000-$CAA4) in bank  1
Overlay  3:  6293 bytes ($E000-$F894) in bank  1
Overlay  4:  6392 bytes ($C000-$D8F7) in bank  3
Overlay  5:  6527 bytes ($E000-$F97E) in bank  3
</code></pre></div>
<p dir="auto">Without the <code>--ovr</code> parameter, overlay markers are simply ignored. This means you can use the same source code for platforms that do support overlays and for those that don't.</p>
<p dir="auto"><strong>Caution</strong>: Overlays somewhat break the safety of the Pascal language. Be careful when using pointers or <code>var</code> parameters for passing data between overlays. The memory you refer to may have just been paged out! It might make sense to compile your overlays with <code>{$a-}</code>, so that all local variables are stored on the stack (which is always visible).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples and tests</h2><a id="user-content-examples-and-tests" aria-label="Permalink: Examples and tests" href="#examples-and-tests"></a></p>
<p dir="auto">There is a folder containing <code>examples</code> and a folder containing <code>tests</code> for the compiler. The main test suite <code>all.pas</code> needs to be compiled with <code>--opt --dep</code> because of its size. Otherwise it won't fit into 64K. The Spectrum 128K and Next targets can (only) handle it using overlays, the Spectrum 48K target can't. Both the examples and the tests should give you a pretty good overview of what the compiler can do.</p>
<p dir="auto">I also solved all puzzles of <a href="https://github.com/pleumann/aoc22">Advent of Code 2022</a> with an earlier version of the compiler and made <a href="https://youtube.com/playlist?list=PLcjDDXgGeSQ6E3NLeSOH0Tn7UorYBgUOH&amp;si=SAoOqUbi70c4ezgi" rel="nofollow">YouTube videos</a> of the solutions running on the ZX Spectrum Next, in CP/M mode.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimalistic IDE</h2><a id="user-content-minimalistic-ide" aria-label="Permalink: Minimalistic IDE" href="#minimalistic-ide"></a></p>
<p dir="auto">As a fun little gimmick the compiler can be started like this</p>

<p dir="auto">to run it in an interactive mode that has an interface reminiscient of Turbo Pascal 3.0.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Main menu</th>
<th>Editor</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/idemenu.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/idemenu.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/ideedit.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/ideedit.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">When started in an ordinary terminal, this mode relies on the editor <code>nano</code> being present on your system (on MacOS you might want to install the real <code>nano</code> via a package manager because Apple sells you the much more limited <code>pico</code> editor as <code>nano</code>).</p>
<p dir="auto">You can also run it in a shell within Visual Studio Code, in which case it would automatically use VSC's editor (via the <code>code</code> command, which, on a Mac, you might <a href="https://code.visualstudio.com/docs/setup/mac#_configure-the-path-with-vs-code" rel="nofollow">have to make available from VCS's settings</a>) and act a bit like a plugin.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/vsc.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/vsc.png" alt="Screenshot"></a></p>
<p dir="auto">The following external tools are supported for running compiled programs on the host machine:</p>
<ul dir="auto">
<li><a href="https://gitlab.com/gbrein/tnylpo" rel="nofollow">tnylpo</a> for CP/M programs (press &lt;R&gt; for line mode, &lt;Shift-R&gt; for full-screen mode).</li>
<li><a href="https://fuse-emulator.sourceforge.net/" rel="nofollow">Fuse</a> for programs targeting the ZX Spectrum 48K and 128K machines.</li>
<li><a href="https://mdf200.itch.io/cspect" rel="nofollow">CSpect</a> for ZX Spectrum Next programs.
<ul dir="auto">
<li>Please have <a href="https://github.com/gasman/hdfmonkey">hdfmonkey</a> ready for manipulating the SD card image.</li>
<li>If you're on MacOS or Linux, you also need <code>mono</code> because CSpect is a .NET application.</li>
</ul>
</li>
</ul>
<p dir="auto">As mentioned before, everything that is in your <code>PATH</code> should be detected automatically. There are some exceptions, though, so it makes sense to copy <code>misc/.pasta80.cfg</code> to your home directory and adapt it. Use the <code>--config</code> parameter to let PASTA/80 check your setup and get feedback on what is in place and what is missing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Application Gallery</h2><a id="user-content-application-gallery" aria-label="Permalink: Application Gallery" href="#application-gallery"></a></p>
<p dir="auto">The following screenshots show some applications compiled for the CP/M target and running in the <code>tnylpo</code> emulator.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>2048</th>
<th>Game of Life</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/2048.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/2048.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/life.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/life.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Micro Calc</th>
<th>Galactic Empire</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/microcalc.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/microcalc.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/empire.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/empire.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">These screenshots show some applications compiled for the ZX Spectrum 48K target and running in the FUSE emulator.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>2048</th>
<th>Game of Life</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/2048zx.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/2048zx.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/lifezx.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/lifezx.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Graphics Demo</th>
<th>Equation Solver</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/graphics.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/graphics.png" alt="Screenshot"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/pleumann/pasta80/blob/master/docs/images/pqformula.png"><img src="https://github.com/pleumann/pasta80/raw/master/docs/images/pqformula.png" alt="Screenshot"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><strong>PASTA/80 Pascal Compiler</strong></p>
<p dir="auto">Copyright (c) 2020-2025 by Jörg Pleumann</p>
<p dir="auto">The PASTA/80 compiler is free software: you can redistribute it and/or modify
it under the terms of the <strong>GNU General Public License (GPL)</strong> as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.</p>
<ul dir="auto">
<li>
<p dir="auto">The runtime library (folder <code>rtl</code>) comes with a <strong>linking exception</strong> that makes sure the GPL does not transfer to binaries created using PASTA/80.</p>
</li>
<li>
<p dir="auto">The examples (folder <code>examples</code>) are considered <strong>public domain</strong> or whatever comes closest to that in your jurisdiction.</p>
</li>
<li>
<p dir="auto">Individual files or folders may use different licenses, so you might want to double check.</p>
</li>
</ul>
<p dir="auto">Everything is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
A PARTICULAR PURPOSE. See the GNU General Public License for more details.</p>
<p dir="auto">What does this mean for you?</p>
<ul dir="auto">
<li>
<p dir="auto">You can <strong>use the compiler</strong>, free of charge, to build any application, open-source or prioprietary, free or paid, and distribute the generated binary without restriction. You can <strong>distribute binaries</strong> created with PASTA/80 under a <strong>license of your choosing</strong>.</p>
</li>
<li>
<p dir="auto">You can <strong>modify the compiler</strong> according to your needs. If you <strong>distribute the compiler</strong> or parts of it, binary or source, modified or not, you have to <strong>comply with the rules laid out in the GPL</strong> (copyright info, source code, ...) unless the linking exception applies.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">The math48 library is coypright (c) 1980 by Anders Hejlsberg, used by <a href="https://github.com/pleumann/pasta80/issues/7" data-hovercard-type="issue" data-hovercard-url="/pleumann/pasta80/issues/7/hovercard">permission</a>.</p>
<p dir="auto">Some assembly routines adapted from Leventhal/Saville, "Z80 Assembly Subroutines", Osborne/McGraw-Hill 1983.</p>
<p dir="auto">Turbo Pascal is a registered trademark of Code Gear LLC / Embarcadero.</p>
<p dir="auto">Z80 is a registered trademark of Zilog, Inc.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Language Support for Marginalia Search (152 pts)]]></title>
            <link>https://www.marginalia.nu/log/a_126_multilingual/</link>
            <guid>45653143</guid>
            <pubDate>Tue, 21 Oct 2025 06:48:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/a_126_multilingual/">https://www.marginalia.nu/log/a_126_multilingual/</a>, See on <a href="https://news.ycombinator.com/item?id=45653143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>One of the big ambitions for the search engine this year has been to enable searching in more languages than English, and a pilot project for this has just been completed, allowing experimental support for German, French and Swedish.</p><p>These changes are now live for testing, but with an extremely small corpus of documents.</p><p>As the search engine has been up to this point built with English in mind, some anglo-centric assumptions made it into its code. A lot of the research on search engines generally seems to embed similar assumptions.</p><p>As this is a domain rife with unknown unknowns, the ambition for this pilot was to implement support for just a few additional languages in order to get a feel for how much work would be required to support more languages in general, as well as to assess how much the index grows when this is done.</p><p>Though it was fully understood upfront that supporting <em>all</em> languages in one go is unrealistic, as some languages are more different than others and require significant additional work. Human language is surprisingly disparate.</p><p>A language like Japanese, for example, has not only multiple alphabets, but <a href="https://en.wikipedia.org/wiki/Halfwidth_and_fullwidth_forms">embeds character width in unicode</a>; on top of that the language doesn’t put spaces between words. As such the language requires special normalization.</p><p>Latin, on the other hand, has <a href="https://dcc.dickinson.edu/grammar/latin/1st-and-2nd-declension-adjectives-%C4%81-o-stems">dozens</a> <a href="https://dcc.dickinson.edu/grammar/latin/2nd-declension-stem-paradigm-and-gender">of</a> <a href="https://dcc.dickinson.edu/grammar/latin/1st-conjugation">forms</a> for each word, and the words can often be reordered without significantly changing the meaning of a sentence. On the one hand this makes the grammatical analysis of the language somewhat easier since the words announce their function in the sentence fairly unambiguously, but on the other you probably need to store the text in a lemmatized form, and then strongly de-prioritize word order when matching.</p><p>Google’s bungled handling of Russian was supposedly why Yandex was able to eke out a foothold in that market.</p><h2 id="what-needs-changing">What needs changing</h2><p>The search engine’s language processing chain is fairly long, but the most salient parts go something like this:</p><ul><li>Text is extracted from the HTML</li><li>Language is identified using fasttext</li><li>Text is broken into sentences</li><li>Words are lowercased and Unicode is normalized</li><li>Sentences are stemmed and POS-tagged</li><li>Sentences, with stemming and POS-tag data is fed into keyword extraction algorithms<ul><li>Keywords are mapped to positions and HTML tags</li><li>Important keywords are identified using TF-IDF (using stemmed forms)</li><li>Important keywords are identified using grammar patterns (POS-tags)</li><li>Important keywords are identified using other heuristics</li></ul></li><li>Keywords are hashed</li></ul><p>Stemming is an imperfect way of getting a base form of a word, though generally such algorithms have a great number of flaws, so that e.g. universe and university seem to be the same word. This is only used in tf-idf calculations.</p><p>Part-of-Speech (POS) tagging is a grammatical annotation process where the role of each word is as best possible identified. This helps identify named entities, subjects, and so on.</p><p>Both of these processes needless to say require some awareness of the language being acted upon.</p><p>These “important keywords” are used to assign documents to a special index that helps with recall by ensuring these documents are included in the set that is ranked before the execution timer runs out. This is not strictly necessary, and in some cases such as where POS-tagging is not possible, can be disabled, partially or as a whole.</p><p>The normalization step is subject to cultural differences that do not translate. In English you’d probably expect to find the metal band Tröjan, typing “trojan”. In Swedish these are different letters entirely that should not match, the former means “the shirt”, the latter “trojan” in the Homeric or IT-security sense. Though a Swedish person would likely also say that they should be able to find mü(e)sli with the keyword “musli”, but a German-speaker would disagree and say that u and ü are clearly not the same.</p><p>There also exists a bootstrapping problem, as the statistical model used to calculate TF-IDF is based on documents in the index. Since almost all of the documents in the index up until this point have been in English, term frequencies for the newly added languages are missing. This breaks TF-IDF, as used in identifying important keywords, until a new model can be constructed. Thankfully the BM-25 model used in ranking is robust to this, as it relies on live data from the index itself.</p><p>The basic approach to parametrize language handling selected was to inject a language definition object, from which language appropriate logic is accessible.</p><p>This is configurable <a href="https://github.com/MarginaliaSearch/MarginaliaSearch/blob/master/code/functions/language-processing/resources/languages-experimental.xml">via XML</a>. Here XML was chosen because it arguably has the best built-in validation support, making it a fantastic use case for a self-contained configuration file like this one, where late validation would be very annoying to deal with.</p><p>Much of the configuration file consists of various grammatical patterns used to identify important keywords based on the role of a word in a sentence.</p><div><pre tabindex="0"><code data-lang="xml"><span><span><span>&lt;ngrams</span> <span>type=</span><span>"noun"</span><span>&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>RB VBG<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ)<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NN* JJ) (NN* JJ) (NN* JJ) NN*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) DT NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span>    <span>&lt;pospattern&gt;</span>(NNP* JJ) (NNP* IN TO CC) (NNP* IN TO CC) NNP*<span>&lt;/pospattern&gt;</span>
</span></span><span><span><span>&lt;/ngrams&gt;</span>
</span></span></code></pre></div><p>An expression like <code>(NN* JJ) (NN* JJ) NN*</code> is interpreted as</p><ol><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code>, or the tag <code>JJ</code></li><li>Any tag starting with <code>NN</code></li></ol><p>Previously these patterns were hard coded, and finding a performant alternative implementation took some effort. A bit mask approach was selected, as it allows for some very basic bit-level concurrency that drastically reduces the number of branches needed.</p><p>As far as grammatical analysis goes, the approach used by the search engine is pretty medieval, but it does do a fairly good job at what it sets out to do, and as a result, one thing it is generally pretty good at is finding websites about some topic.</p><p>In some ways the imperfections introduced by the old-fashioned way of approaching language processing is almost helpful in bringing in more relevant results, as they tend to capture more variations of the words related to the topic of the document.</p><p>There are more places that need minor language dependent behavior changes that are glossed over here, both in the language processing pipeline discussed above, and in the query parser, though in the interest of keeping this update from becoming an overly verbose git diff, these will be glossed over.</p><h3 id="tooling">Tooling</h3><p>To help make sense of this, a test tool was built that runs the language processing pipeline in isolation, and outputs annotated intermediate results for human inspection.</p><figure><a href="https://www.marginalia.nu/log/a_126_multilingual/tool.png"><img src="https://www.marginalia.nu/log/a_126_multilingual/tool.png"></a><figcaption>Language Processing Tool illustrating some problems with keyword identification when run on a very short sample of text.</figcaption></figure><p>Work in this domain poses special problems that all but demand human testing. Machine testing can be good for catching regressions or getting access to some code for easier debugging, but natural language has so many nuances that any test suite is woefully inadequate compared to a pair of human eyeballs.</p><p>It has already helped refine the algorithms used to identify important keywords in English, which wasn’t the intent of building the tool, but its immediate consequence.</p><h2 id="integration">Integration</h2><p>Integrating the new multi-language search data into the system poses some design considerations.</p><p>One option would be to stick everything in one big index, and then filter results based on language during or after ranking. The strength of this is that it becomes possible to search in any language without specifying it upfront.</p><p>The drawbacks of the one-index approach is that it grows the index, which makes all queries slower; it also grows the number of keywords in the lexicon, which is something that we generally want to avoid.</p><p>The way the search engine handles mapping keywords to numeric ids is to use a hash algorithm. Not a hash table, but the output of the hash algorithm itself. This seems absolutely unhinged at first glance, but works remarkably well as long as the lexicon stays small enough.</p><p>Hash collisions do happen on rare occasions, but they need to happen between words where the words actually appear in the same documents to be a problem, generally leading to the ranking algorithm having to trudge through irrelevant documents and performing worse as a result of wasting its time budget.</p><p>Massively expanding the lexicon like we would if we were to mingle the documents increases the likelihood there will be an actual problem arising from these rare false positives.</p><p>If we stick every keyword from every language in the same index, a different problem arises, namely that homophones exist across different languages, meaning that the index lookup needs to wade through irrelevant documents that are trivially unrelated to the query.</p><p>The words <code>salt</code> and <code>lag</code>, if they appear in the same document in English likely selects documents relating to esports, whereas in Swedish they select for documents relating to food preservation.</p><p>The alternative option is to separate the indexes.</p><p>The drawback here is that you must specify the language upfront, and querying in all languages becomes very expensive, as it executing multiple queries, though the desired language of the search results are generally known beforehand so this is a relatively small concern that, at best, affects a small number of machine-access use cases.</p><p>Since it has far fewer problems, and promises to be faster and more accurate, this approach was selected.</p><p>In practice this was implemented as language-specific keyword-document mappings, that point into a common file containing document lists.</p><p>Initially the indexes were constructed from a common journal file, which was consumed repeatedly, but this turned out to be slow, and a partitioned approach was selected instead, with one journal per language. This almost completely removes any overhead.</p><h2 id="outcome">Outcome</h2><p>The changes discussed above have been implemented, and upon evaluation seems to work reasonably well, though evaluation has somewhat run into a dead end, as the index itself is <strong>extremely</strong> small for the newly added languages.</p><p>The experience of small index is devious as it may just mean poor recall, though looking at the documents database for one index partition, this is about 12% of the index, it really is quite small!</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>112,846,397</td></tr><tr><td>de</td><td>7,623,983</td></tr><tr><td>fr</td><td>4,852,759</td></tr><tr><td>sv</td><td>1,020,962</td></tr></tbody></table><p>To verify this is not due some silent, catastrophic processing error, the proportions were compared against the number of documents found in the 50 GB document sample used in testing, using a simplified process that only does language identification.</p><table><thead><tr><th>iso</th><th>document count</th></tr></thead><tbody><tr><td>en</td><td>11,497,571</td></tr><tr><td>de</td><td>614,311</td></tr><tr><td>fr</td><td>409,877</td></tr><tr><td>es</td><td>267,408</td></tr><tr><td>ja</td><td>217,599</td></tr><tr><td>nl</td><td>196,130</td></tr><tr><td>…</td><td>…</td></tr><tr><td>sv</td><td>67,670</td></tr></tbody></table><p>The proportions aren’t identical, but in the same general ballpark. The small size of the sample, along with the uneven distribution and apparent rarity of these documents adequately explains the disparity.</p><p>The lack of documents in languages other than English is likely due to how the index has been grown, by following and adding links from English websites. These occasionally lead to bilingual websites, and on rare occasions to websites completely in a different language, though it seems reasonable most websites that are not at least partially in English sees few or no links from English-language websites.</p><p>Adding to the problem, up until fairly recently the index wasn’t really growing very much at all, only through manual submissions.</p><p>Beyond a certain point, meaningfully growing the index by just following links became difficult.</p><p>Most known domains are dead, so merely adding more domains to the list of websites to crawl only serves to pollute the database with junk data.</p><p>In order to get around this, and reach the goal of indexing a billion documents, a new process was built to visit candidate websites to verify that they are in fact real and on-line, before assigning them to an index partition.</p><p>The process has been running for almost a quarter, and has managed to identify about 800,000 viable new domains in that time window. (This has brought the document total up to 969M documents. So very nearly there now!)</p><p>Web search is unusual in how often you run into these extremely long running processes that need to cook for months, sometimes up to a year before they really begin to pay off.</p><p>We’ll have to see whether building this new process was so prescient it ends up being sufficient to identify and add new domains in more languages, as links from the newly processed Swedish, French and German websites have been added to the domain database, or if some sort of manual seeding or targeted selection process is needed.</p><p>It seems plausible it will at least begin to remedy the data starvation, as the rate of successful domain discovery has shot up significantly since processing links from the documents processed in the newly added languages, and many of the new domains are indeed from <code>.de</code>, <code>.se</code>, <code>.fr</code>, and <code>.ch</code> domains.</p><p>For now we’ll have to wait and see how the data-set evolves. It is difficult to further refine the multi-language aspect of the search data with a data-set this small.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practical Scheme (128 pts)]]></title>
            <link>https://practical-scheme.net/index.html#docs</link>
            <guid>45652859</guid>
            <pubDate>Tue, 21 Oct 2025 05:47:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical-scheme.net/index.html#docs">https://practical-scheme.net/index.html#docs</a>, See on <a href="https://news.ycombinator.com/item?id=45652859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      <p>This page is a collection of libraries and extensions
        to use Scheme as a production tool.
        By "production tools" I mean the tools to process daily
        chores for systems engineers and programmers---parsing files,
        generate reports, watching processes,
        providing small GUI wrappers, and all sorts of those things.
        Currently I'm using <a href="https://www.perl.org/">Perl</a> for
        those purpose, but I'm always longing to use Scheme for them.
        <a href="https://practical-scheme.net/oneday.html">So I started this page</a>.
      </p>

      <p>Most stuffs in this site are done as my private
         project at home, except the ones explicitly stated otherwise.
         I upload libraries even in its alpha/beta stage, since
         I'd like to test and use them at work, too.  In a way, my primary
         interest is to make <i>my</i> life happier.
         No warranty comes with them, as usual, but it'll be nice
         if somebody else finds they are useful.
      </p>

      <p>If you can read Japanese, visit the <a href="https://practical-scheme.net/index-j.html">Japanese
         page</a> which contains some translations of Lisp/Scheme
         related articles.
      </p>

      <p>I wrote a Wiki Clone in Scheme (<a href="https://practical-scheme.net/gauche/index.html">Gauche</a>).
         Come and try it:
         <a href="https://practical-scheme.net/wiliki/wiliki.cgi?l=en">WiLiKi</a>.
      </p>

      <ul>
        <li> <a href="#apps">Applications and tools</a>
        </li><li> <a href="#libs">Libraries and extensions</a>
        </li><li> <a href="#docs">Documents</a>
        </li><li> <a href="#links">Links</a>
      </li></ul>

    <p><a name="apps"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Applications and tools</h2>

      <p>Scheme-related stand alone programs.</p>

      <dl>
        <dl><dt><b>Gauche</b> - <a href="https://practical-scheme.net/gauche/index.html">Current version 0.9.15</a> (2024/04/24)<img src="https://practical-scheme.net/images/new.png" alt="*New" width="40" height="18"></dt><dd><p>An R7RS Scheme implementation aimed at a handy
                       script engine.  Quick startup, built-in system
                       interface, and native multilingual support
                       are some of the goals.</p></dd><dt><b>WiLiKi</b> - <a href="https://practical-scheme.net/wiliki/wiliki.cgi">Current version 0.6.2</a> (2014/11/28)</dt><dd><p>A wiki engine written in Scheme.</p></dd><dt><b>Chaton</b> - <a href="https://practical-scheme.net/chaton/">Current version </a></dt><dd><p>A Comet-based Webchat system.</p></dd><dt><b>escm</b> - <a href="https://practical-scheme.net/vault/escm.html">Current version 1.1</a> (2014/11/28)</dt><dd><p>A filter program which copies the input text to output,
                       with processing embedded Scheme expressions.
                       This program itself is independent from any Scheme
                       implementation; you can use your favorite one.
                       Useful to process text files with a bit of dynamic
                       parts.  This page itself is processed by escm
                       to embed information such as the update time of
                       libraries, and synchronize with Japanese version.
                       A complete new version of escm, named aescm,
                      is being developed by TAGA Yoshitaka
                       (<a href="http://sourceforge.net/projects/escm/">
                       http://sourceforge.net/projects/escm/</a>)</p></dd></dl>
      </dl>

    <p><a name="libs"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Libraries and Extensions</h2>

      <p>The following libraries and extensions are written for
         <a href="https://practical-scheme.net/gauche/index.html">Gauche</a>.
         See <a href="https://practical-scheme.net/stklib.html">here</a> for libraries written for STk.
      </p>

      <dl>
<dt><b>Gauche-gl</b> - <a href="https://prdownloads.sourceforge.net/gauche/Gauche-gl-0.6.tgz">Download</a>&nbsp;&nbsp;<a href="https://practical-scheme.net/vault/gauche-gl-refe.html">Document</a>&nbsp;&nbsp;Current version 0.6 (2014/08/09)&nbsp;</dt><dd><p>OpenGL binding for Gauche.  Supports most of OpenGL 1.0 to 4.1 APIs
           (including OpenGL Shading Language API),
           and some of GLU and GLUT API.
           Requires Gauche 0.9.4 or later.</p></dd><dt><b>Gauche-gtk2</b> - <a href="https://github.com/shirok/Gauche-gtk2/releases/download/release-0.6.1/Gauche-gtk2-0.6.1.tgz">Download</a>&nbsp;&nbsp;<a href="https://practical-scheme.net/vault/README.Gauche-gtk.txt">Document</a>&nbsp;&nbsp;Current version 0.6.1 (2022/3/20)&nbsp;</dt><dd><p>GTK2 binding for Gauche.</p></dd></dl>


    <p><a name="docs"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2>Documents</h2>

      <dl>
        <dt> <a href="https://practical-scheme.net/wiliki/schemexref.cgi">Scheme Cross Reference</a>
        </dt><dd> <p>A cross reference of library procedures of various
             Scheme implementations.   Updated constantly.
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/jlugm2000.html">Shooting A Moving Target---
             An Experience In Developing A Production Tracking Database</a>
        </dt><dd> <p>An application of CommonLisp in practice.
             (yeah, it's not Scheme... anyway, I put it here).

        </p></dd><dt> <a href="https://practical-scheme.net/docs/gdc2002.html">
             Tracking Assets in the Production of 'Final Fantasy : The Spirits Within'</a>
        </dt><dd> <p>A follow-up of the article above, a kind of post-mortem
             of the production.</p>

        </dd><dt> <a href="https://practical-scheme.net/docs/ILC2002.html">Gluing Things Together -
             Scheme in the Real-time CG Content Production</a>
        </dt><dd> <p>A paper presented at International Lisp Conference 2002
             at San Francisco, October 2002.
             (there's also a <a href="https://practical-scheme.net/docs/ILC2002.pdf">pdf version</a>).
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/DLS2008.pdf">Efficient floating-point number handling for dynamically typed scripting languages (pdf)</a>
        </dt><dd> <p>A paper presented at Dynamic Language Symposium 2008.
             </p>

        </dd><dt> <a href="https://practical-scheme.net/docs/schemersway.html">Schemer's Way</a>
        </dt><dd> <p>Trying to explain Scheme's merits to non-Scheme programmers.
             </p>
      </dd></dl>

    <p><a name="links"><img src="https://practical-scheme.net/images/separator.jpg" alt="------------------------------" width="480" height="4"></a></p><h2><a name="otherresources">Other Resources</a></h2>

      <p>This list no way covers everything, but you can follow
        links in those links.
      </p>

      <dl>
        <dt> <a href="http://www.schemers.org/">Schemers.org</a>
        </dt><dd> A good anchor point to collect information of Scheme.
             You can get <a href="http://www.schemers.org/Documents/Standards/">R*RS</a>, the language standard.
             The site is also a center of
             <a href="http://srfi.schemers.org/">SRFI's</a>---
             Scheme Request For Implementation---which provides
             common interface of libraries across various implementations.

        </dd><dt> <a href="http://www-swiss.ai.mit.edu/~jaffer/SCM.html">SCM</a>
        </dt><dd> A compact, fast and portable implementation of Scheme
             interpreter.

        </dd><dt> <a href="http://www-swiss.ai.mit.edu/~jaffer/SLIB.html">SLIB</a>
        </dt><dd> A large collection of portable Scheme libraries.
             The contents spans from small utilities complements the
             standard conformance, to the full-featured relational
             database.

        </dd><dt> <a href="http://www4.ocn.ne.jp/~inukai/artificial.html">
             Programming Languages</a> by Dai Inukai
        </dt><dd> Scheme-related documents by Dai Inukai, the author of
             "Nyuumon Scheme (Scheme Primer)" in Japan.
             Check this out if you're interested in processing
             Japanese in Scheme.

        </dd><dt> <a href="http://kaolin.unice.fr/Bigloo/">Bigloo</a>
        </dt><dd> A scheme system with compiler and integrated development
             environment.  If you're planning to write an enterprise
             software rather than just a bunch of scripts, look at it.

        </dd><dt> <a href="http://www.gnu.org/software/guile/guile.html">Guile</a>
        </dt><dd> GNU adopted Scheme for the base of extension language
             several years ago.  The effort became Guile.
             If you have one of popular Linux distributions, you may
             already have it.

        </dd><dt> <a href="http://www.swiss.ai.mit.edu/ftpdir/scsh/">scsh</a>
        </dt><dd> I haven't used this one much, but looks good if you're
             looking for a tool to do syste programming.

        </dd><dt> <a href="http://www.cs.indiana.edu/scheme-repository/">
             The Internet Scheme Repository</a>
        </dt><dd> As the name suggests.

        </dd><dt> <a href="http://www.gnu.org/software/kawa/">Kawa - the Java-based Scheme System</a>
        </dt><dd> A Scheme environment written in Java by Per Bothner.
             Scheme code is compiled to Java bytecode, hence has
             the property "write once run everywhere".

      </dd></dl>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[60k kids have avoided peanut allergies due to 2015 advice, study finds (178 pts)]]></title>
            <link>https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</link>
            <guid>45652307</guid>
            <pubDate>Tue, 21 Oct 2025 03:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/">https://www.cbsnews.com/news/peanut-allergies-60000-kids-avoided-2015-advice/</a>, See on <a href="https://news.ycombinator.com/item?id=45652307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                                                                                                                                
                                                                            
<article id="article-0" data-index="0" data-path="/news/peanut-allergies-60000-kids-avoided-2015-advice/">
  <!-- lil observer -->
      <span></span>
    
        



  <div id="article-header" data-sort-time="1761003592000" data-update-time="1761003592000">
    <header>
                                      
          

      

      
      
      <div>
                  
  



        
        <p>
            <time datetime="2025-10-20T19:39:52-0400">Updated on:  October 20, 2025 / 7:39 PM EDT</time>
            / CBS/AP
          </p>
      </div>

    </header>
  </div>


  <section>
    <p>A decade after a landmark study proved that feeding peanut products to young babies could prevent development of life-threatening allergies, new research finds the change has made a big difference in the real world. </p><p>About 60,000 children have avoided developing peanut allergies after guidance first issued in 2015 upended medical practice by recommending introducing the allergen to infants starting as early as 4 months. </p><p>"That's a remarkable thing, right?" said Dr. David Hill, an allergist and researcher at Children's Hospital of Philadelphia, and author of a study published Monday in the medical journal Pediatrics. Hill and colleagues analyzed electronic health records from dozens of pediatric practices to track diagnoses of food allergies in young children before, during and after the guidelines were issued. </p><p>"I can actually come to you today and say there are less kids with food allergy today than there would have been if we hadn't implemented this public health effort," he added.</p><p>"Our findings have relevance from those of us who treat patients to those caring for infants, and more awareness, education and advocacy could further increase the positive results we observed in this study," he continued. "Future studies could potentially explore specific feeding practices that help us better understand the timing, frequency and dose of foods that optimize protection against food allergies."</p>

    

<p>The researchers found that peanut allergies in children ages 0 to 3 declined by more than 27% after guidance for high-risk kids was first issued in 2015 and by more than 40% after the recommendations were expanded in 2017. </p><p>The effort hasn't yet reduced an overall increase in food allergies in the U.S. in recent years. About 8% of children are affected, including more than 2% with a peanut allergy. </p><p>Peanut allergy is caused when the body's immune system mistakenly identifies proteins in peanuts as harmful and releases chemicals that trigger allergic symptoms, including hives, respiratory symptoms and, sometimes, life-threatening anaphylaxis. </p><p>For decades, doctors had recommended delaying feeding children peanuts and other foods likely to trigger allergies until age 3. But in 2015, Gideon Lack at King's College London published the groundbreaking Learning Early About Peanut Allergy, or LEAP, trial.  </p>

    
    

<p>Lack and colleagues showed that introducing peanut products in infancy reduced the future risk of developing food allergies by more than 80%. Later analysis showed that the protection persisted in about 70% of kids into adolescence.  </p><p>The study immediately sparked new guidelines urging early introduction of peanuts — but putting them into practice has been slow. </p><p>Only about 29% of pediatricians and 65% of allergists reported following the expanded guidance issued in 2017, surveys found. </p><p>Confusion and uncertainty about the best way to introduce peanuts early in life led to the lag, according to a commentary that accompanied the study. Early on, medical experts and parents alike questioned whether the practice could be adopted outside of tightly controlled clinical settings.  </p><p>The data for the analysis came from a subset of participating practice sites and may not represent the entire U.S. pediatric population, noted the commentary, led by Dr. Ruchi Gupta, a child allergy expert at Northwestern University.  </p><p>However, the new research offers "promising evidence that early allergen introduction is not only being adopted but may be making a measurable impact," the authors concluded.  </p><p>Advocates for the 33 million people in the U.S. with food allergies welcomed signs that early introduction of peanut products is catching on. </p>

    
    

<p>"This research reinforces what we already know and underscores a meaningful opportunity to reduce the incidence and prevalence of peanut allergy nationwide," said Sung Poblete, chief executive of the nonprofit group Food Allergy Research &amp; Education, or FARE.  </p><p>The new study emphasizes the current guidance, updated in 2021, which calls for introducing peanuts and other major food allergens between four and six months, without prior screening or testing, Hill said. Parents should consult their pediatricians about any questions.  </p><p>"It doesn't have to be a lot of the food, but little tastes of peanut butter, milk-based yogurt, soy-based yogurts and tree butters," he said. "These are really good ways to allow the immune system exposure to these allergenic foods in a safe way." </p><p>Tiffany Leon, 36, a Maryland registered dietician and director at FARE, introduced peanuts and other allergens early to her own sons, James, 4, and Cameron, 2. </p><p>At first, Leon's own mother was shocked at the advice to feed babies such foods before the age of 3, she said. But Leon explained how the science had changed. </p><p>"As a dietician, I practice evidence-based recommendations," she said. "So when someone told me, 'This is how it's done now, these are the new guidelines,' I just thought, 'OK, well, this is what we're going to do.'"</p>
  </section>

  

                
        
      
                  
    <!-- data-recirc-source="queryly" -->
    



    
    
  <section>
  <h2>In:</h2>
  <ul>
          <li><a href="https://www.cbsnews.com/tag/allergies/">Allergies</a></li>
          <li><a href="https://www.cbsnews.com/tag/peanuts/">Peanuts</a></li>
      </ul>
</section>

  

  
  </article>
            

                                                                                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a message queue with only two UNIX signals (133 pts)]]></title>
            <link>https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals</link>
            <guid>45650178</guid>
            <pubDate>Mon, 20 Oct 2025 22:22:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals">https://leandronsp.com/articles/you-dont-need-kafka-building-a-message-queue-with-only-two-unix-signals</a>, See on <a href="https://news.ycombinator.com/item?id=45650178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Have you ever asked yourself what if we could replace any message broker with a very simple one using only two UNIX signals? Well, I’m not surprised if you didn’t. But I did. And I want to share my journey of how I achieved it.</p>
<p>If you want to learn about UNIX signals, binary operations the easy way, how a message broker works under the hood, and a bit of Ruby, this post is for you.</p>
<p>And if you came here just because of the clickbait title, I apologize and invite you to keep reading. It’ll be fun, I promise.</p>
<p><img src="https://leandronsp.com/uploads/3491.png" alt="image"></p>
<h2>It’s all about UNIX</h2>
<p>A few days ago, I saw some discussion on the internet about how we could send messages between processes. Many people think of sockets, which are the most common way to send messages, even allowing communication across different machines and networks. Some don’t even realize that pipes are another way to send messages between processes:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>echo</span> <span>'hello'</span> <span>|</span> <span>base64</span>
</p><p><span>aGVsbG8K</span>
</p></code></pre>
<p>Here’s what’s happening:</p>
<ul>
<li>The process <code>echo</code> is started with the content “hello”</li>
<li><code>echo</code> is a program that prints the message to <em>STDOUT</em></li>
<li>Through the pipe, the content in <em>STDOUT</em> is <strong>sent</strong> directly to the <em>STDIN</em> of the <code>base64</code> process</li>
<li>The <code>base64</code> process encodes its input to Base64 and then puts the result in <em>STDOUT</em></li>
</ul>
<p>Note the word “send”. Yes, anonymous pipes are a form of <strong>IPC (Inter-process communication).</strong> Other forms of IPC in UNIX include:</p>
<ul>
<li>named pipes (mkfifo)</li>
<li>sockets</li>
<li>regular files</li>
<li>or even a simple <strong>signal</strong></li>
</ul>
<h2>UNIX signals</h2>
<p>According to <a href="https://leandronsp.com/articles/url">Wikipedia</a>:</p>
<blockquote>
<p>A UNIX signal is a standardized message sent to a program to trigger specific behaviour, such as quitting or error handling</p>
</blockquote>
<p>There are many signals we can send to a process, including:</p>
<ul>
<li>SIGTERM - sends a notification to the process to terminate. It can be “trapped,” which means the process can do some cleanup work before termination, like releasing OS resources and closing file descriptors</li>
<li>SIGKILL - sends a termination signal that cannot be trapped or ignored, forcing immediate termination</li>
<li>SIGINT - the interrupt signal, typically sent when you press <code>Ctrl+C</code> in the terminal. It can be trapped, allowing the process to perform cleanup before exiting gracefully</li>
<li>SIGHUP - the hangup signal, originally sent when a terminal connection was lost. Modern applications often use it to reload configuration files without restarting the process</li>
<li>SIGQUIT - similar to SIGINT but also generates a core dump for debugging</li>
<li>SIGSTOP - pauses (suspends) a process. Cannot be trapped or ignored</li>
<li>SIGCONT - resumes a process that was paused by <em>SIGSTOP</em></li>
<li>SIGCHLD - sent to a parent process when a child process terminates or stops</li>
<li><strong>SIGUSR1</strong> and <strong>SIGUSR2</strong> - user-defined signals that applications can use for custom purposes</li>
</ul>
<h2>Sending messages using  signals</h2>
<p>Okay, we know that signals are a primitive form of IPC. UNIX-like systems provide a syscall called <code>kill</code> that sends signals to processes. Historically, this syscall was created solely to terminate processes. But over time, they needed to accommodate other types of signals, so they reused the same syscall for different purposes.</p>
<p>For instance, let’s create a simple Ruby script <code>sleeper.rb</code> which sleeps for 60 seconds, nothing more:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping for 60 seconds...</span><span>"</span>
</p><p><span>sleep</span> <span>60</span>
</p></code></pre>
<p>After running we see:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 55402
</p><p>Sleeping for 60 seconds...
</p></code></pre>
<p>In another window, we can <strong>send</strong> the <code>SIGTERM</code> signal to the process <code>55402</code> via syscall <code>kill</code>:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>kill</span> <span>-SIGTERM</span> <span>55402</span>
</p></code></pre>
<p>And then, in the script session:</p>
<pre><code translate="no" tabindex="0"><p>[1]    55402 terminated  ruby sleeper.rb
</p></code></pre>
<h3>Signal traps</h3>
<p>In Ruby, we can also <em>trap</em> a signal using the <code>trap</code> method in Ruby:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping for 60 seconds...</span><span>"</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGTERM</span><span>'</span><span>)</span> <span>do</span> 
</p><p><span>puts</span> <span>"</span><span>Received SIGTERM, exiting gracefully...</span><span>"</span>
</p><p><span>exit</span>
</p><p><span>end</span>
</p><p><span>sleep</span> <span>60</span>
</p></code></pre>
<p>Which in turn, after sending the signal, will gracefully:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 55536
</p><p>Sleeping for 60 seconds...
</p><p>Received SIGTERM, exiting gracefully...
</p></code></pre>
<p>After all, we <em>cannot send messages using signals</em>. They are a primitive way of sending <em>standardized messages</em> which will trigger specific behaviours. At most, we can trap some signals, but nothing more.</p>
<blockquote>
<p>Okay Leandro, but what’s the purpose of this article then?</p>
</blockquote>
<p><em>Hold on</em>. That’s exactly why I’m here. To prove points by doing useless stuff, like when I <a href="https://leandronsp.com/articles/simulating-oop-in-bash-3mop">simulated OOP in Bash</a> a couple of years ago (it was fun though).</p>
<p>To understand how we can “hack” UNIX signals and send messages between processes, let’s first talk a bit about <strong>binary operations</strong>. Yes, those “zeros” and “ones” you were scared of when you saw them for the first time. But they don’t bite (🥁 LOL), I promise.</p>
<h2>What is a message?</h2>
<p>If we model a message as a sequence of characters, we could say that at a high-level, messages are simply <em>strings</em>. But in memory, they are stored as <strong>bytes</strong>.</p>
<p>We know that bytes are made of bits. In computer terms, what’s a bit? It’s simply an abstraction representing <strong>only two states</strong>:</p>
<ul>
<li>zero</li>
<li>one</li>
</ul>
<p>That’s it. For instance, using <a href="https://leandronsp.com/articles/url">ASCII</a>, we know that the letter “h” has the following codes:</p>
<ul>
<li>104 in decimal</li>
<li><code>0x68</code> in hexadecimal</li>
<li><code>01101000</code> in binary</li>
</ul>
<p>Binary-wise, what if we represented each “0” with a specific signal and each “1” with another? We know that some signals such as SIGTERM, SIGINT, and SIGCONT can be trapped, but intercepting them would harm their original purpose.</p>
<p>But thankfully, UNIX provides two user-defined signals that are perfect for our hacking experiment.</p>
<h2>Sending SIGUSR1 and SIGUSR2</h2>
<p>First things first, let’s trap those signals in the code:</p>
<pre><code translate="no" tabindex="0"><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Sleeping forever. Send signals to this process to see how it responds.</span><span>"</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>do</span> 
</p><p><span>puts</span> <span>"</span><span>Received SIGUSR1 signal</span><span>"</span>
</p><p><span>end</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>do</span>
</p><p><span>puts</span> <span>"</span><span>Received SIGUSR2 signal</span><span>"</span>
</p><p><span>end</span>
</p><p><span>sleep</span>
</p></code></pre>
<pre><code translate="no" tabindex="0"><p>Process ID: 56172
</p><p>Sleeping forever. Send signals to this process to see how it responds.
</p></code></pre>
<p>After sending some <code>kill -SIGUSR1 56172</code> and <code>kill -SIGUSR2 56172</code>, we can see that the process prints the following content:</p>
<pre><code translate="no" tabindex="0"><p>Process ID: 56172
</p><p>Sleeping forever. Send signals to this process to see how it responds.
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR2 signal
</p><p>Received SIGUSR2 signal
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR1 signal
</p><p>Received SIGUSR2 signal
</p></code></pre>
<p><strong>Signals don’t carry data</strong>. But the example we have is perfect for changing to bits, uh?</p>
<pre><code translate="no" tabindex="0"><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR2 signal # 1
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR1 signal # 0
</p><p>Received SIGUSR1 signal # 0
</p></code></pre>
<p>That’s exactly <code>01101000</code>, the binary representation of the letter “h”. We’re simply <strong>encoding</strong> the letter as a binary representation and sending it via signals</p>
<p>Again, we’re <strong>encoding it as a binary</strong> and sending it <strong>via signals</strong>.</p>
<p><em>How cool is that</em>?</p>
<p><img src="https://leandronsp.com/uploads/3299.png" alt="image"></p>
<h3>Decoding the binary data</h3>
<p>On the other side, the receiver should be capable of decoding the message and converting it back to the letter “h”:</p>
<ul>
<li>sender <em>encodes</em> the message</li>
<li>receiver <em>decodes</em> the message</li>
</ul>
<p>So, how do we decode <code>01101000</code> (the letter “h” in ASCII)? Let’s break it down into a few steps:</p>
<ol>
<li>First, we need to see the 8 bits as individual digits in their respective positions</li>
<li>The rightmost bit is at position 0, whereas the leftmost bit is at position 7. This is how we define the most significant bit (<strong>MSB</strong>, the leftmost) and the least significant bit (<strong>LSB</strong>, the rightmost)</li>
<li>For this example, we perform a <strong>left shift</strong> operation on each bit and then sum all the values, in this case from MSB to LSB (the order doesn’t matter much for now): <code>(0 &lt;&lt; 7) + (1 &lt;&lt; 6) + (1 &lt;&lt; 5) + (0 &lt;&lt; 4) + ... + (0 &lt;&lt; 0)</code>:<br>
<em>left shift on <em>zeros</em> will always produce a <em>zero</em></em></li>
</ol>
<ul>
<li><code>0 &lt;&lt; 7</code> = <code>(2 ** 7) * 0</code> = <code>128 * 0</code> = 0</li>
<li><code>1 &lt;&lt; 6</code> = <code>(2 ** 6) * 1</code> = <code>64 * 1</code> = 64</li>
</ul>
<p>Similarly to the remaining bits:</p>
<ul>
<li><code>1 &lt;&lt; 5</code> = 32</li>
<li><code>0 &lt;&lt; 4</code> = 0</li>
<li><code>1 &lt;&lt; 3</code> = 8</li>
<li><code>0 &lt;&lt; 2</code> = 0</li>
<li><code>0 &lt;&lt; 1</code> = 0</li>
<li><code>0 &lt;&lt; 0</code> = 0</li>
</ul>
<p>So, our sum becomes, from MSB to LSB:</p>
<pre><code translate="no" tabindex="0"><p>MSB                          LSB
</p><p>0   1    1    0   1   0   0   0
</p><p>0 + 64 + 32 + 0 + 8 + 0 + 0 + 0 = 104
</p></code></pre>
<p>104 is exactly the <strong>decimal representation</strong> of the letter “h” in ASCII.</p>
<p><em>How wonderful is that?</em></p>
<h3>Sending the letter “h”</h3>
<p>Now let’s convert these operations to Ruby code. We’ll write a simple program <code>receiver.rb</code> that receives signals in order from LSB to MSB (positions 0 to 7) and then converts them back to ASCII characters, printing to <code>STDOUT</code>.</p>
<p>Basically, we’ll <strong>accumulate</strong> bits and whenever we form a complete byte, we’ll decode it to its ASCII representation. The very basic implementation of our <code>accumulate_bit(bit)</code> method would look like as follows:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>return</span> <span>@accumulator</span> <span>if</span> <span>@position</span> <span>==</span> <span>7</span> <span># stop accumulating after 8 bits (byte)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span># Letter "h" in binary is 01101000</span>
</p><p><span># But we'll send from the LSB to the MSB</span>
</p><p><span>#</span>
</p><p><span># 0110 1000 (MSB -&gt; LSB) becomes 0001 0110 (LSB -&gt; MSB)</span>
</p><p><span># The order doesn't matter that much, it'll depend on </span>
</p><p><span># the receiver's implementation.</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>1</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>0</span><span>)</span>
</p><p><span>puts</span> <span>@accumulator</span> <span># should print 104, which is the ASCII code for "h"</span>
</p></code></pre>
<p><em>Pay attention to this code. It’s very important and builds the foundation for the next steps. If you didn’t get it, go back and read it again. Try it yourself in the terminal or using your preferred programming language.</em></p>
<p>Now, how to convert the decimal <code>104</code> to the ASCII character representation? Luckily, Ruby provides a method called <code>chr</code> which does the job:</p>
<pre><code translate="no" tabindex="0"><p><span>irb</span><span>&gt;</span> <span>puts</span> <span>104</span><span>.</span><span>chr</span>
</p><p><span>=&gt;</span> <span>"</span><span>h</span><span>"</span>
</p></code></pre>
<p>We could do the same job for the rest of the word “hello”, for instance. According to the <a href="https://www.ascii-code.com/">ASCII table</a>, it should be the following:</p>
<ul>
<li><code>e</code> in decimal is <code>101</code></li>
<li><code>l</code> in decimal is <code>108</code></li>
<li><code>o</code> in decimal is <code>111</code></li>
</ul>
<p>Let’s check if Ruby knows that:</p>
<pre><code translate="no" tabindex="0"><p><span>104</span><span>.</span><span>chr</span>    <span># "h"</span>
</p><p><span>101</span><span>.</span><span>chr</span>    <span># "e"</span>
</p><p><span>108</span><span>.</span><span>chr</span>    <span># "l"</span>
</p><p><span>111</span><span>.</span><span>chr</span>    <span># "o"</span>
</p></code></pre>
<p>We can even “decode” the word to the decimal representation in ASCII:</p>
<pre><code translate="no" tabindex="0"><p><span>irb</span><span>&gt;</span> <span>"</span><span>hello</span><span>"</span><span>.</span><span>bytes</span>
</p><p><span>=&gt;</span> <span>[</span><span>104</span><span>,</span> <span>101</span><span>,</span> <span>108</span><span>,</span> <span>108</span><span>,</span> <span>111</span><span>]</span>
</p></code></pre>
<p>Now, time to finish our receiver implementation to properly print the letter “h”:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>print</span> <span>"</span><span>Received byte: </span><span>#{</span><span>@accumulator</span><span>}</span><span> (</span><span>#{</span><span>@accumulator</span><span>.</span><span>chr</span><span>}</span><span>)</span><span>\n</span><span>"</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>sleep</span>
</p></code></pre>
<p><em>Read that code and its comments. It’s very important. Do not continue reading until you really get what’s happening here.</em></p>
<ul>
<li>Whenever we get <code>SIGUSR1</code>, we accumulate the bit <code>0</code></li>
<li>When getting <code>SIGUSR2</code>, accumulate then the bit <code>1</code></li>
<li>When accumulator reaches  the position<code>8</code>, it means we have a byte. At this moment we should print the ASCII representation using the <code>.chr</code> we seen earlier. Then, reset bit position and accumulator</li>
</ul>
<p>Let’s see our receiver in action! Start the receiver in one terminal:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>58219</span>
</p></code></pre>
<p>Great! Now the receiver is listening for signals. In another terminal, let’s manually send signals<br>
to form the letter “h” (which is <code>01101000</code> in binary, remember?):</p>
<pre><code translate="no" tabindex="0"><p>  # Sending from LSB to MSB: 0, 0, 0, 1, 0, 1, 1, 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR1 58219  # 0
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR2 58219  # 1
</p><p>  $ kill -SIGUSR1 58219  # 0
</p></code></pre>
<p>And in the receiver terminal, we should see:</p>
<pre><code translate="no" tabindex="0"><p>Received byte: 104 (h)
</p></code></pre>
<p><em>How amazing is that?</em> We just sent the letter “h” using only two UNIX signals!</p>
<p>But wait. Manually sending 8 signals for each character? That’s tedious and error-prone. What if we wanted to send the word “hello”? That’s 5 characters × 8 bits = 40 signals to send manually. No way.</p>
<p><em>We need a sender.</em></p>
<h3>Building the sender</h3>
<p>The sender’s job is the opposite of the receiver: it should encode a message (string) into bits and send them as signals to the receiver process.</p>
<p>Let’s think about what we need:</p>
<ol>
<li>Take a message as input (like “hello”)</li>
<li>Convert each character to its byte representation</li>
<li>Extract the 8 bits from each byte</li>
<li>Send <code>SIGUSR1</code> for bit 0, <code>SIGUSR2</code> for bit 1</li>
<li>Repeat for all characters</li>
</ol>
<p>The tricky part here is the step 3: <strong>how do we extract individual bits from a byte?</strong> To extract the bit at position <code>i</code>, we can use the following formula:</p>
<pre><code translate="no" tabindex="0"><p>bit = (byte &gt;&gt; i) &amp; 1
</p></code></pre>
<p>Let me break this down:</p>
<ul>
<li><code>byte &gt;&gt; i</code> performs a <em>right shift</em> by <code>i</code> positions</li>
<li><code>&amp; 1</code> is a bitwise <code>AND</code> operation that extracts only the <em>rightmost</em> bit</li>
</ul>
<p>For the letter “h” (<code>01101000</code> in binary, <code>104</code> in decimal):</p>
<p><strong>Position 0 (LSB):</strong></p>
<ul>
<li><code>(104 &gt;&gt; 0)</code> = <code>104 / (2 ** 0)</code> = <code>104 / 1</code> = 104</li>
<li><code>01101000</code> &gt;&gt; 0 = <code>01101000</code></li>
<li><code>01101000</code> &amp; <code>00000001</code> = 0 (<em>one</em> AND <em>zero</em> is <em>zero</em>)</li>
</ul>
<p><strong>Position 1:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 1)</code> = <code>104 / (2 ** 1)</code> = <code>104 / 2</code> = 52</li>
<li><code>01101000</code> &gt;&gt; 1 = <code>00110100</code></li>
<li><code>00110100</code> &amp; <code>00000001</code> = 0</li>
</ul>
<p><strong>Position 2:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 2)</code> = <code>104 / (2 ** 2)</code> = <code>104 / 4</code> = 26</li>
<li><code>01101000</code> &gt;&gt; 2 = <code>00011010</code></li>
<li><code>00011010</code> &amp; <code>00000001</code> = 0</li>
</ul>
<p><strong>Position 3:</strong></p>
<ul>
<li><code>(104 &gt;&gt; 3)</code> = <code>104 / (2 ** 3)</code> = <code>104 / 8</code> = 13</li>
<li><code>01101000</code> &gt;&gt; 3 = <code>00001101</code></li>
<li><code>00001101</code> &amp; <code>00000001</code> = 1 (<em>one</em> AND <em>one</em> equals <em>one</em>)</li>
</ul>
<p>And so on for positions 4, 5, 6, and 7. This gives us: <code>0, 0, 0, 1, 0, 1, 1, 0</code> — exactly the bits we need from LSB to MSB!</p>
<ul>
<li><code>(104 &gt;&gt; 0) &amp; 1</code> = <code>104 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 1) &amp; 1</code> = <code>52 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 2) &amp; 1</code> = <code>26 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 3) &amp; 1</code> = <code>13 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 4) &amp; 1</code> = <code>6 &amp; 1</code> = 0</li>
<li><code>(104 &gt;&gt; 5) &amp; 1</code> = <code>3 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 6) &amp; 1</code> = <code>1 &amp; 1</code> = 1</li>
<li><code>(104 &gt;&gt; 7) &amp; 1</code> = <code>0 &amp; 1</code> = 0</li>
</ul>
<blockquote>
<p>Pay close attention to this technique. It’s a fundamental operation in low-level programming.</p>
</blockquote>
<p>So now time to build the <code>sender.rb</code> which is pretty simple:</p>
<pre><code translate="no" tabindex="0"><p><span>receiver_pid</span> <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>]</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>[</span><span>1</span><span>..</span><span>-</span><span>1</span><span>]</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>def</span> <span>encode_byte</span><span>(</span><span>byte</span><span>)</span>
</p><p><span>8</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span># Extract each bit from the byte, starting from the LSB</span>
</p><p><span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>message</span><span>.</span><span>bytes</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>encode_byte</span><span>(</span><span>byte</span><span>)</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>bit</span><span>|</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p></code></pre>
<p>For each byte (8-bit structure) we extract the bit performing the <em>right shift</em> + <em>AND</em> oprerations. The result is the extracted bit.</p>
<p>In the receiver window:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>68968</span>
</p></code></pre>
<p>And in the sender window:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>68968</span> <span>h</span>
</p></code></pre>
<p>The receiver will print:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>receiver.rb</span>
</p><p><span>Process</span> <span>ID:</span> <span>68968</span>
</p><p><span>Received</span> <span>byte:</span> <span>104</span><span></span> <span>(</span><span>h</span><span>)</span>
</p></code></pre>
<p><em>Processes sending messages with only two signals!</em> How wonderful is that?</p>
<h3>Sending the “hello” message</h3>
<p>Now, sending the hello message is super easy. The sender is already able to send not only a letter but any message using signals:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>68968</span> <span>hello</span>
</p><p><span># And the receiver:</span>
</p><p><span>Received</span> <span>byte:</span> <span>104</span><span></span> <span>(</span><span>h</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>101</span><span></span> <span>(</span><span>e</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>108</span><span></span> <span>(</span><span>l</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>108</span><span></span> <span>(</span><span>l</span><span>)</span>
</p><p><span>Received</span> <span>byte:</span> <span>111</span><span></span> <span>(</span><span>o</span><span>)</span>
</p></code></pre>
<p>Just change the <code>receiver</code> implementation a little bit:</p>
<pre><code translate="no" tabindex="0"><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>print</span> <span>@accumulator</span><span>.</span><span>chr</span> <span># print the byte as a character</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p></code></pre>
<p>And then:</p>
<pre><code translate="no" tabindex="0"><p><span>$</span> <span>ruby</span> <span>sender.rb</span> <span>96875</span> <span>Hello</span>
</p><p><span># In the receiver's terminal</span>
</p><p><span>Process</span> <span>ID:</span> <span>96875</span>
</p><p><span>Hello</span>
</p></code></pre>
<p>However, if we send the message again, the receiver will print everything in the same line:</p>
<pre><code translate="no" tabindex="0"><p>$ <span>ruby</span> <span>sender</span><span>.</span><span>rb</span> <span>96875</span> <span>Hello</span>
</p><p>$ <span>ruby</span> <span>sender</span><span>.</span><span>rb</span> <span>96875</span> <span>Hello</span>
</p><p><span># In the receiver's terminal</span>
</p><p><span>Process</span> <span>ID</span><span>:</span> <span>96875</span>
</p><p><span>HelloHello</span>
</p></code></pre>
<p>It’s obvious: the receiver doesn’t know where the sender finished the message, so it’s impossible to know where we should stop one message and print the next one on a new line with <code>\n</code>.</p>
<p>We should then determine how the sender indicates the end of the message. How about being it all <em>zeroes</em> (<code>0000 0000</code>)?</p>
<ul>
<li>We send the message: first 5 bytes representing the “hello” message</li>
<li>Then we send a “NULL terminator”, just one byte <em>0</em> (<code>0000 0000</code>)</li>
</ul>
<pre><code translate="no" tabindex="0"><p>0110 1000 # h
</p><p>0110 0101 # e
</p><p>0110 1000 # l
</p><p>0110 1000 # l
</p><p>0110 1111 # o
</p><p>0000 0000 # NULL
</p></code></pre>
<p>Hence, when the <em>receiver</em> gets a NULL terminator, it will print a line feed <code>\n</code>. Let’s change the <code>sender.rb</code> first:</p>
<pre><code translate="no" tabindex="0"><p><span>receiver_pid</span> <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>]</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>[</span><span>1</span><span>..</span><span>-</span><span>1</span><span>]</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>def</span> <span>encode_byte</span><span>(</span><span>byte</span><span>)</span>
</p><p><span>8</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span># Extract each bit from the byte, starting from the LSB</span>
</p><p><span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>message</span><span>.</span><span>bytes</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>encode_byte</span><span>(</span><span>byte</span><span>)</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>bit</span><span>|</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span># Send NULL terminator (0000 0000)</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>,</span> <span>receiver_pid</span><span>)</span>
</p><p><span>sleep</span> <span>0.001</span> <span># Delay to allow the receiver to process the signal</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Message sent to receiver (PID: </span><span>#{</span><span>receiver_pid</span><span>}</span><span>)</span><span>"</span>
</p></code></pre>
<p>Then, the <code>receiver.rb</code>:</p>
<pre><code translate="no" tabindex="0"><p><span>@position</span> <span>=</span> <span>0</span> <span># start with the LSB</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>decode_signal</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>def</span> <span>decode_signal</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>return</span> <span>unless</span> <span>@position</span> <span>==</span> <span>8</span> <span># if not yet accumulated a byte, keep accumulating</span>
</p><p><span>if</span> <span>@accumulator</span><span>.</span><span>zero?</span> <span># NULL terminator received</span>
</p><p><span>print</span> <span>"</span><span>\n</span><span>"</span>
</p><p><span>else</span>
</p><p><span>print</span> <span>@accumulator</span><span>.</span><span>chr</span> <span># print the byte as a character</span>
</p><p><span>end</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span> <span># reset the accumulator</span>
</p><p><span>@position</span> <span>=</span> <span>0</span> <span># reset position for the next byte</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span># The left shift operator (&lt;&lt;) is used to </span>
</p><p><span># shift the bits of the number to the left.</span>
</p><p><span>#</span>
</p><p><span># This is equivalent of: (2 ** @position) * bit</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span> <span># move to the next bit position: 0 becomes 1, 1 becomes 2, etc.</span>
</p><p><span>end</span>
</p><p><span>puts</span> <span>"</span><span>Process ID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>sleep</span>
</p></code></pre>
<p>Output:</p>
<pre><code translate="no" tabindex="0"><p>$ ruby sender.rb 96875 Hello, World!
</p><p>$ ruby sender.rb 96875 You're welcome
</p><p>$ ruby sender.rb 96875 How are you?
</p><p># Receiver
</p><p>Process ID: 97176
</p><p>Hello, World!
</p><p>You're welcome
</p><p>How are you?
</p></code></pre>
<blockquote>
<p>OMG Leandro! That’s amazing!</p>
</blockquote>
<p><em>Amazing, right?</em> We just built an entire communication system between two processes using one of the most primitive methods available: <strong>UNIX signals.</strong></p>
<p>The sky’s the limit now! Why not build a <em>full-fledged message broker</em> using this crazy technique?</p>
<h2>A modest message broker using UNIX signals</h2>
<p>We’ll break down the development into three components:</p>
<ol>
<li><strong>Broker</strong>: the intermediary that routes messages</li>
<li><strong>Consumer</strong>: processes that receive messages</li>
<li><strong>Producer</strong>: processes that send messages</li>
</ol>
<p><img src="https://leandronsp.com/uploads/3395.png" alt="image"></p>
<ol>
<li>Let’s start with the Broker. It should register itself with the producer, then trap incoming signals, decode them, and enqueue the messages for delivery to consumers via outgoing signals:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>require_relative</span> <span>'</span><span>consumer</span><span>'</span>
</p><p><span>class</span> <span>Broker</span> 
</p><p><span>PID</span> <span>=</span> <span>'</span><span>broker.pid</span><span>'</span><span>.</span><span>freeze</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>@queue</span> <span>=</span> <span>Queue</span><span>.</span><span>new</span>
</p><p><span>@consumer_index</span> <span>=</span> <span>0</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>start</span> 
</p><p><span>register_broker</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>puts</span> <span>"</span><span>Broker PID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Waiting for messages...</span><span>"</span>
</p><p><span>distribute_messages</span>
</p><p><span>sleep</span> <span># Keep alive</span>
</p><p><span>end</span> 
</p><p><span>private</span>
</p><p><span>def</span> <span>process_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@codec</span><span>.</span><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span> <span>do</span> <span>|</span><span>message</span><span>|</span>
</p><p><span>@queue</span><span>.</span><span>push</span><span>(</span><span>message</span><span>)</span> <span>unless</span> <span>message</span><span>.</span><span>empty?</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>register_broker</span> 
</p><p><span>File</span><span>.</span><span>write</span><span>(</span><span>PID</span><span>,</span> <span>Process</span><span>.</span><span>pid</span><span>)</span>
</p><p><span>at_exit</span> <span>{</span> <span>File</span><span>.</span><span>delete</span><span>(</span><span>PID</span><span>)</span> <span>if</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>PID</span><span>)</span> <span>}</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>distribute_messages</span>
</p><p><span>Thread</span><span>.</span><span>new</span> <span>do</span>
</p><p><span>loop</span> <span>do</span>
</p><p><span>sleep</span> <span>0.1</span>
</p><p><span>next</span> <span>if</span> <span>@queue</span><span>.</span><span>empty?</span>
</p><p><span>consumers</span> <span>=</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>Consumer</span><span>::</span><span>FILE</span><span>)</span> <span>?</span> <span>File</span><span>.</span><span>readlines</span><span>(</span><span>Consumer</span><span>::</span><span>FILE</span><span>)</span><span>.</span><span>map</span><span>(</span><span>&amp;</span><span>:to_i</span><span>)</span> <span>:</span> <span>[</span><span>]</span>
</p><p><span>next</span> <span>if</span> <span>consumers</span><span>.</span><span>empty?</span>
</p><p><span>message</span> <span>=</span> <span>@queue</span><span>.</span><span>pop</span><span>(</span><span>true</span><span>)</span> <span>rescue</span> <span>next</span>
</p><p><span>consumer_pid</span> <span>=</span> <span>consumers</span><span>[</span><span>@consumer_index</span> <span>%</span> <span>consumers</span><span>.</span><span>size</span><span>]</span>
</p><p><span>@consumer_index</span> <span>+=</span> <span>1</span>
</p><p><span>puts</span> <span>"</span><span>[SEND] </span><span>#{</span><span>message</span><span>}</span><span> → Consumer </span><span>#{</span><span>consumer_pid</span><span>}</span><span>"</span>
</p><p><span>@codec</span><span>.</span><span>send_message</span><span>(</span><span>message</span><span>,</span> <span>consumer_pid</span><span>)</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>if</span> <span>__FILE__</span> <span>==</span> <span>$0</span> 
</p><p><span>broker</span> <span>=</span> <span>Broker</span><span>.</span><span>new</span>
</p><p><span>broker</span><span>.</span><span>start</span>
</p><p><span>end</span>
</p></code></pre>
<ul>
<li>The broker registers itself</li>
<li>Traps incoming signals <code>USR1</code> (bit 0) and <code>USR2</code> (bit 1)</li>
<li>Enqueues the messages</li>
<li>Send messages to consumers using outgoing signals (<code>USR1</code> and <code>USR2</code> too)</li>
</ul>
<p><em>Note that we’re using a module called <code>SignalCodec</code> which will be explained soon. Basically this module contains all core components to encode/decode signals and perform bitwise operations.</em></p>
<ol start="2">
<li>Now the <code>Consumer</code> implementation:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>class</span> <span>Consumer</span>
</p><p><span>FILE</span> <span>=</span> <span>'</span><span>consumers.txt</span><span>'</span><span>.</span><span>freeze</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>start</span>
</p><p><span>register_consumer</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>0</span><span>)</span> <span>}</span>
</p><p><span>trap</span><span>(</span><span>'</span><span>SIGUSR2</span><span>'</span><span>)</span> <span>{</span> <span>process_bit</span><span>(</span><span>1</span><span>)</span> <span>}</span>
</p><p><span>puts</span> <span>"</span><span>Consumer PID: </span><span>#{</span><span>Process</span><span>.</span><span>pid</span><span>}</span><span>"</span>
</p><p><span>puts</span> <span>"</span><span>Waiting for messages...</span><span>"</span>
</p><p><span>sleep</span> <span># Keep alive</span>
</p><p><span>end</span>
</p><p><span>private</span>
</p><p><span>def</span> <span>process_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@codec</span><span>.</span><span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span> <span>do</span> <span>|</span><span>message</span><span>|</span>
</p><p><span>puts</span> <span>"</span><span>[RECEIVE] </span><span>#{</span><span>message</span><span>}</span><span>"</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>register_consumer</span>
</p><p><span>File</span><span>.</span><span>open</span><span>(</span><span>FILE</span><span>,</span> <span>'</span><span>a</span><span>'</span><span>)</span> <span>{</span> <span>|</span><span>f</span><span>|</span> <span>f</span><span>.</span><span>puts</span> <span>Process</span><span>.</span><span>pid</span> <span>}</span>
</p><p><span>at_exit</span> <span>{</span> <span>deregister_consumer</span> <span>}</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>deregister_consumer</span>
</p><p><span>if</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>FILE</span><span>)</span>
</p><p><span>consumers</span> <span>=</span> <span>File</span><span>.</span><span>readlines</span><span>(</span><span>FILE</span><span>)</span><span>.</span><span>map</span><span>(</span><span>&amp;</span><span>:strip</span><span>)</span><span>.</span><span>reject</span> <span>{</span> <span>|</span><span>pid</span><span>|</span> <span>pid</span><span>.</span><span>to_i</span> <span>==</span> <span>Process</span><span>.</span><span>pid</span> <span>}</span>
</p><p><span>File</span><span>.</span><span>write</span><span>(</span><span>FILE</span><span>,</span> <span>consumers</span><span>.</span><span>join</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>)</span><span>)</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>if</span> <span>__FILE__</span> <span>==</span> <span>$0</span>
</p><p><span>consumer</span> <span>=</span> <span>Consumer</span><span>.</span><span>new</span>
</p><p><span>consumer</span><span>.</span><span>start</span>
</p><p><span>end</span>
</p></code></pre>
<ul>
<li>The consumer starts and registers itself with the broker</li>
<li>Consumer then traps incoming signals (bit 0 and bit 1)</li>
<li>Decodes and prints messages</li>
</ul>
<ol start="3">
<li>Last but not least, the <code>Producer</code> implementation, which is pretty straightforward:</li>
</ol>
<pre><code translate="no" tabindex="0"><p><span>#!/usr/bin/env ruby</span>
</p><p><span>require_relative</span> <span>'</span><span>signal_codec</span><span>'</span>
</p><p><span>require_relative</span> <span>'</span><span>broker</span><span>'</span>
</p><p><span>unless</span> <span>File</span><span>.</span><span>exist?</span><span>(</span><span>Broker</span><span>::</span><span>PID</span><span>)</span>
</p><p><span>abort</span> <span>"</span><span>Error: Broker not running (</span><span>#{</span><span>Broker</span><span>::</span><span>PID</span><span>}</span><span> not found)</span><span>"</span>
</p><p><span>end</span>
</p><p><span>broker_pid</span> <span>=</span> <span>File</span><span>.</span><span>read</span><span>(</span><span>Broker</span><span>::</span><span>PID</span><span>)</span><span>.</span><span>strip</span><span>.</span><span>to_i</span>
</p><p><span>message</span> <span>=</span> <span>ARGV</span><span>.</span><span>join</span><span>(</span><span>'</span><span> </span><span>'</span><span>)</span>
</p><p><span>if</span> <span>message</span><span>.</span><span>empty?</span>
</p><p><span>puts</span> <span>"</span><span>Usage: ruby producer.rb &lt;message&gt;</span><span>"</span>
</p><p><span>exit</span> <span>1</span>
</p><p><span>end</span>
</p><p><span>codec</span> <span>=</span> <span>SignalCodec</span><span>.</span><span>new</span>
</p><p><span>puts</span> <span>"</span><span>Sending: </span><span>#{</span><span>message</span><span>}</span><span>"</span>
</p><p><span>codec</span><span>.</span><span>send_message</span><span>(</span><span>message</span><span>,</span> <span>broker_pid</span><span>)</span>
</p><p><span>puts</span> <span>"</span><span>Message sent to broker (PID: </span><span>#{</span><span>broker_pid</span><span>}</span><span>)</span><span>"</span>
</p></code></pre>
<ul>
<li>Producer receives a ASCII message from the <em>STDIN</em></li>
<li>Encode and sends the message to the broker via outgoing signals</li>
</ul>
<p>So far, this architecture should look familiar. Many broker implementations follow these basic foundations.</p>
<blockquote>
<p>Of course, production-ready implementations are far more robust than this one. Here, we’re just poking around with hacking and experimentation</p>
</blockquote>
<p>The coolest part is the <code>SignalCodec</code> though:</p>
<pre><code translate="no" tabindex="0"><p><span>class</span> <span>SignalCodec</span> 
</p><p><span>SIGNAL_DELAY</span> <span>=</span> <span>0.001</span> <span># Delay between signals to allow processing</span>
</p><p><span>def</span> <span>initialize</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>@position</span> <span>=</span> <span>0</span>
</p><p><span>@buffer</span> <span>=</span> <span>[</span><span>]</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>accumulate_bit</span><span>(</span><span>bit</span><span>)</span>
</p><p><span>@accumulator</span> <span>+=</span> <span>(</span><span>bit</span> <span>&lt;&lt;</span> <span>@position</span><span>)</span>
</p><p><span>@position</span> <span>+=</span> <span>1</span>
</p><p><span>if</span> <span>@position</span> <span>==</span> <span>8</span> <span># Byte is complete</span>
</p><p><span>if</span> <span>@accumulator</span><span>.</span><span>zero?</span> <span># Message complete - NULL terminator</span>
</p><p><span>decoded</span> <span>=</span> <span>@buffer</span><span>.</span><span>pack</span><span>(</span><span>"</span><span>C*</span><span>"</span><span>)</span><span>.</span><span>force_encoding</span><span>(</span><span>'</span><span>UTF-8</span><span>'</span><span>)</span>
</p><p><span>yield</span><span>(</span><span>decoded</span><span>)</span> <span>if</span> <span>block_given?</span>
</p><p><span>@buffer</span><span>.</span><span>clear</span>
</p><p><span>else</span> 
</p><p><span>@buffer</span> <span>&lt;&lt;</span> <span>@accumulator</span>
</p><p><span>end</span>
</p><p><span>@position</span> <span>=</span> <span>0</span>
</p><p><span>@accumulator</span> <span>=</span> <span>0</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>def</span> <span>send_message</span><span>(</span><span>message</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>message</span><span>.</span><span>each_byte</span> <span>do</span> <span>|</span><span>byte</span><span>|</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span> <span>|</span><span>i</span><span>|</span>
</p><p><span>bit</span> <span>=</span> <span>(</span><span>byte</span> <span>&gt;&gt;</span> <span>i</span><span>)</span> <span>&amp;</span> <span>1</span>
</p><p><span>signal</span> <span>=</span> <span>bit</span> <span>==</span> <span>0</span> <span>?</span> <span>'</span><span>SIGUSR1</span><span>'</span> <span>:</span> <span>'</span><span>SIGUSR2</span><span>'</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>signal</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>sleep</span> <span>SIGNAL_DELAY</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span># Send NULL terminator (0000 0000)</span>
</p><p><span>8</span><span>.</span><span>times</span> <span>do</span>
</p><p><span>Process</span><span>.</span><span>kill</span><span>(</span><span>'</span><span>SIGUSR1</span><span>'</span><span>,</span> <span>pid</span><span>)</span>
</p><p><span>sleep</span> <span>SIGNAL_DELAY</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p><p><span>end</span>
</p></code></pre>
<p>If you’ve been following along, this shouldn’t be hard to understand, but I’ll break down how this beautiful piece of code works:</p>
<ul>
<li>The codec is initialized with the bit position at zero, as well as the accumulator</li>
<li>A buffer is also initialized to store accumulated bits until a complete byte is formed</li>
<li>The <code>accumulate_bit</code> method should be familiar from our earlier implementation, but it now accepts a closure (block) that lets the caller decide what to do with each decoded byte</li>
<li><code>send_message</code> encodes a message into bits and sends them via UNIX signals</li>
</ul>
<p>Everything in action:</p>
<p><img src="https://leandronsp.com/uploads/3170.png" alt="image"></p>
<p><em>How cool, amazing, wonderful, impressive, astonishing is that?</em></p>
<h2>Conclusion</h2>
<p>Yes, we built a message broker using nothing but <strong>UNIX signals</strong> and a bit of Ruby magic. Sure, <strong>it’s not production-ready</strong>, and you definitely shouldn’t use this in your next startup (please don’t), but that was never the point.</p>
<p>The real takeaway here isn’t the broker itself: it’s understanding how the fundamentals work. We explored binary operations, UNIX signals, and IPC in a hands-on way that most people never bother with.</p>
<p>We took something “useless” and made it work, just for fun. So next time someone asks you about message brokers, you can casually mention that you once built (or saw) one using just two signals. And if they look at you weird, well, that’s their problem. Now go build something equally useless and amazing. The world needs more hackers who experiment just for the fun of it.</p>
<p><em>Happy hacking!</em></p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Today is when the Amazon brain drain sent AWS down the spout (922 pts)]]></title>
            <link>https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</link>
            <guid>45649178</guid>
            <pubDate>Mon, 20 Oct 2025 20:50:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/">https://www.theregister.com/2025/10/20/aws_outage_amazon_brain_drain_corey_quinn/</a>, See on <a href="https://news.ycombinator.com/item?id=45649178">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>column</span> "It's always DNS" is a long-standing sysadmin saw, and with good reason: a disproportionate number of outages are at their heart DNS issues. And so today, as AWS is still repairing its downed cloud as this article goes to press, it becomes clear that the culprit is once again DNS. But if you or I know this, AWS certainly does.</p>
<p>And so, a quiet suspicion starts to circulate: where have the senior AWS engineers who've been to this dance before gone? And the answer increasingly is that they've left the building — taking decades of hard-won institutional knowledge about how AWS's systems work at scale right along with them.</p>
<h3>What happened?</h3>
<p>AWS reports that on October 20, at 12:11 AM PDT, it began investigating “increased error rates and latencies for multiple AWS services in the US-EAST-1 Region.” About an hour later, at 1:26 AM, the company confirmed “significant error rates for requests made to the DynamoDB endpoint” in that region. By 2:01 AM, engineers had identified <a target="_blank" href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">DNS resolution of the DynamoDB API endpoint</a> for US-EAST-1 as the likely root cause, which led to cascading failures for most other things in that region. DynamoDB is a "foundational service" upon which a whole mess of other AWS services rely, so the blast radius for an outage touching this thing can be huge.</p>
<p>As a result, <a target="_blank" href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">much of the internet stopped working</a>: banking, gaming, social media, government services, buying things I don't need on Amazon.com itself, etc.</p>
<p>AWS has given increasing levels of detail, as is their tradition, when outages strike, and as new information comes to light. Reading through it, one really gets the sense that it took them 75 minutes to go from "things are breaking" to "we've narrowed it down to a single service endpoint, but are still researching," which is something of a bitter pill to swallow. To be clear: I've seen zero signs that this stems from a lack of transparency, and every indication that they legitimately did not know what was breaking for a patently absurd length of time.</p>

    

<p>Note that for those 75 minutes, visitors to the AWS status page (reasonably wondering why their websites and other workloads had just burned down and crashed into the sea) were met with an "all is well!" default response. Ah well, it's not as if AWS had <a target="_blank" href="https://aws.amazon.com/message/12721/" rel="nofollow">previously called out slow outage notification times</a> as an area for improvement. <a target="_blank" href="https://aws.amazon.com/message/11201/" rel="nofollow">Multiple times</a> even. We can <a target="_blank" href="https://aws.amazon.com/message/41926/" rel="nofollow">keep doing this</a> if you'd like.</p>
<h3>The prophecy</h3>
<p>AWS is very, very good at infrastructure. You can tell this is a true statement by the fact that a single one of their 38 regions going down (albeit a very important region!) causes this kind of attention, as opposed to it being "just another Monday outage." At AWS's scale, all of their issues are complex; this isn't going to be a simple issue that someone should have caught, just because they've already hit similar issues years ago and ironed out the kinks in their resilience story.</p>
<p>Once you reach a certain point of scale, there are no simple problems left. What's more concerning to me is the way it seems AWS has been flailing all day trying to run this one to ground. Suddenly, I'm reminded of something I had tried very hard to forget.</p>

        


        

<p>At the end of 2023, Justin Garrison left AWS and <a target="_blank" href="https://justingarrison.com/blog/2023-12-30-amazons-silent-sacking/" rel="nofollow">roasted them on his way out the door</a>. He stated that AWS had seen an increase in Large Scale Events (or LSEs), and predicted significant outages in 2024. It would seem that he discounted the power of inertia, but the pace of senior AWS departures certainly hasn't slowed — and now, with an outage like this, one is forced to wonder whether those departures are themselves a contributing factor.</p>
<p>You can hire a bunch of very smart people who will explain how DNS works at a deep technical level (or you can hire me, who will incorrect you by explaining that it's a database), but the one thing you can't hire for is the person who remembers that when DNS starts getting wonky, check that seemingly unrelated system in the corner, because it's historically played a contributing role to some outages of yesteryear.</p>

        

<p>When that tribal knowledge departs, you're left having to reinvent an awful lot of in-house expertise that didn't want to participate in your RTO games, or play Layoff Roulette yet again this cycle. This doesn't impact your service reliability — until one day it very much does, in spectacular fashion. I suspect that day is today.</p>
<ul>

<li><a href="https://www.theregister.com/2025/10/20/aws_outage_chaos/">AWS outage exposes Achilles heel: central control plane</a></li>

<li><a href="https://www.theregister.com/2025/10/20/amazon_aws_outage/">Major AWS outage across US-East region breaks half the internet</a></li>

<li><a href="https://www.theregister.com/2025/10/17/amazon_nuke_washington/">Amazon spills plan to nuke Washington...with X-Energy mini-reactors</a></li>

<li><a href="https://www.theregister.com/2025/10/06/amazon_007_without_golden_gun/">Amazon turns James Bond into the Man Without the Golden Gun</a></li>
</ul>
<h3>The talent drain evidence</h3>
<p>This is <em>The Register</em>, a respected journalistic outlet. As a result, I know that if I publish this piece as it stands now, an AWS PR flak will appear as if by magic, waving their hands, insisting that "there is no talent exodus at AWS," a la Baghdad Bob. Therefore, let me forestall that time-wasting enterprise with some data.</p>
<ul>
<li>It is a fact that there have been <a target="_blank" href="https://www.cnbc.com/2025/07/17/amazon-web-services-has-some-layoffs.html" rel="nofollow">27,000+ Amazonians impacted by layoffs</a> between 2022 and 2024, continuing into 2025. It's hard to know how many of these were AWS versus other parts of its Amazon parent, because the company is notoriously tight-lipped about staffing issues.</li>

<li>Internal documents reportedly say that Amazon <a target="_blank" href="https://www.engadget.com/amazon-attrition-leadership-ctsmd-201800110-201800100.html" rel="nofollow">suffers from 69 percent to 81 percent regretted attrition</a> across all employment levels. In other words, "people quitting who we wish didn't."</li>

<li>The internet is full of anecdata of senior Amazonians lamenting the hamfisted approach of their Return to Office initiative; <a target="_blank" href="https://finance.yahoo.com/news/amazon-back-office-crusade-could-090200105.html/" rel="nofollow">experts have weighed in</a> citing similar concerns.</li>
</ul>
<p>If you were one of the early employees who built these systems, the world is your oyster. There's little reason to remain at a company that increasingly demonstrates apparent disdain for your expertise.</p>
<h3>My take</h3>
<p>This is a tipping point moment. Increasingly, it seems that the talent who understood the deep failure modes is gone. The new, leaner, presumably less expensive teams lack the institutional knowledge needed to, if not prevent these outages in the first place, significantly reduce the time to detection and recovery. Remember, there was a time when Amazon's "Frugality" leadership principle meant doing more with less, not doing everything with basically nothing. AWS's operational strength was built on redundant, experienced people, and when you cut to the bone, basic things start breaking.</p>
<p>I want to be very clear on one last point. This isn't about the technology being old. It's about the people maintaining it being new. If I had to guess what happens next, the market will forgive AWS this time, but the pattern will continue.</p>
<p>AWS will almost certainly say this was an "isolated incident," but when you've hollowed out your engineering ranks, every incident becomes more likely. The next outage is already brewing. It's just a matter of which understaffed team trips over which edge case first, because the chickens are coming home to roost. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iOS 26.1 lets users control Liquid Glass transparency (201 pts)]]></title>
            <link>https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/</link>
            <guid>45648266</guid>
            <pubDate>Mon, 20 Oct 2025 19:39:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/">https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/</a>, See on <a href="https://news.ycombinator.com/item?id=45648266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/10/20/ios-26-1-liquid-glass-toggle/"><p>With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass.</p>
<p><img src="https://images.macrumors.com/t/OCRA_6Z8V4J8hTc_cWfWK-zkX9w=/400x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy" srcset="https://images.macrumors.com/t/OCRA_6Z8V4J8hTc_cWfWK-zkX9w=/400x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy 400w,https://images.macrumors.com/t/tj3V79n2YvpR0tslbBIv6K1qcvY=/800x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg?lossy 800w,https://images.macrumors.com/t/Nawho-r4OQp6pTe_7_FIPthUu2A=/1600x0/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg 1600w,https://images.macrumors.com/t/6PpVz-MHL6Gyx_oDr4Hqtac40xA=/2500x0/filters:no_upscale()/article-new/2025/10/ios-26-1-liquid-glass-opaque.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="ios 26 1 liquid glass opaque" width="2000" height="1125"><br>The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted increases the opacity of Liquid Glass and adds more contrast.</p>
<p>The new setting can be found on iOS and iPadOS by going to Settings &gt; Display and Brightness, or System Settings &gt; Appearance on the Mac.</p>
<p>Apple says that the new toggle was added because during the beta testing period over the summer, user feedback suggested that some people would prefer to have a more opaque option for Liquid Glass. The added setting provides additional customization in iOS 26.1, iPadOS 26.1, and macOS Tahoe 26.1. </p>
<p>Increasing opacity and adding contrast applies to Liquid Glass throughout the operating system, including in apps and Lock Screen notifications.</p>
<p>There are multiple other new features in iOS 26.1, including a new slide to stop feature for alarms and timers, new <a href="https://www.macrumors.com/guide/apple-intelligence/">Apple Intelligence</a> languages, a redesigned <a href="https://www.macrumors.com/roundup/apple-tv/">Apple TV</a> app icon, changes to the Settings app, and more, with a full list of features <a href="https://www.macrumors.com/guide/ios-26-1-beta-features/">available in our iOS 26.1 feature guide</a>.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/10/17/iphone-air-production-to-be-cut-amid-lower-sales/">Apple Said to Cut iPhone Air Production Amid Underwhelming Sales</a></h3><p>Apple plans to cut production of the iPhone Air amid underwhelming sales performance, Japan's Mizuho Securities believes (via The Elec).
The Japanese investment banking and securities firm claims that the iPhone 17 Pro and iPhone 17 Pro Max are seeing higher sales than their predecessors during the same period last year, while the standard iPhone 17 is a major success, performing...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/18/ios-26-1-to-ios-26-4-expected-features/">iOS 26.1 to iOS 26.4 Will Add These New Features to Your iPhone</a></h3><p>Saturday October 18, 2025 11:00 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>iOS 26 was released last month, but the software train never stops, and iOS 26.1 beta testing is already underway. So far, iOS 26.1 makes both Apple Intelligence and Live Translation on compatible AirPods available in additional languages, and it includes some other minor changes across the Apple Music, Calendar, Photos, Clock, and Safari apps.
More features and changes will follow in future ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/17/ios-26-0-2-coming-soon/">iOS 26.0.2 Update for iPhones Coming Soon</a></h3><p>Apple's software engineers continue to internally test iOS 26.0.2, according to MacRumors logs, which have been a reliable indicator of upcoming iOS versions.
iOS 26.0.2 will be a minor update that addresses bugs and/or security vulnerabilities, but we do not know any specific details yet.
The update will likely be released by the end of next week.
Last month, Apple released iOS 26.0.1,...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/heres-whats-coming-next-from-apple/">Apple's Next Rumored Products: New HomePod Mini, Apple TV, and More</a></h3><p>Thursday October 16, 2025 9:13 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple on Wednesday updated the 14-inch MacBook Pro, iPad Pro, and Vision Pro with its next-generation M5 chip, but previous rumors have indicated that the company still plans to announce at least a few additional products before the end of the year.
The following Apple products have at one point been rumored to be updated in 2025, although it is unclear if the timeframe for any of them has...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/19/ios-26-4-revamped-siri-concerns/">Some Apple Employees Have 'Concerns' About iOS 26.4's Revamped Siri</a></h3><p>iOS 26.4 is expected to introduce a revamped version of Siri powered by Apple Intelligence, but not everyone is satisfied with how well it works.
In his Power On newsletter today, Bloomberg's Mark Gurman said some of Apple's software engineers have "concerns" about the overhauled Siri's performance. However, he did not provide any specific details about the shortcomings.
iOS 26.4 will...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/18/new-ipad-pro-six-key-upgrades/">New iPad Pro Has Six Key Upgrades Beyond M5 Chip</a></h3><p>Saturday October 18, 2025 10:57 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>While the new iPad Pro's headline feature is the M5 chip, the device has some other changes, including N1 and C1X chips, faster storage speeds, and more.
With the M5 chip, the new iPad Pro has up to a 20% faster CPU and up to a 40% faster GPU compared to the previous model with the M4 chip, according to Geekbench 6 results. Keep in mind that 256GB and 512GB configurations have a 9-core CPU,...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/20/ios-26-1-liquid-glass-toggle/">iOS 26.1 Beta 4 Lets Users Control Liquid Glass Transparency with New Toggle</a></h3><p>Monday October 20, 2025 10:57 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>With the fourth betas of iOS 26.1, iPadOS 26.1, and macOS 26.1, Apple has introduced a new setting that's designed to allow users to customize the look of Liquid Glass.
The toggle lets users select from a clear look for Liquid Glass, or a tinted look. Clear is the current Liquid Glass design, which is more transparent and shows the background underneath buttons, bars, and menus, while tinted ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/new-14-inch-macbook-pro-two-key-upgrades/">New 14-Inch MacBook Pro Has Two Key Upgrades Beyond the M5 Chip</a></h3><p>Thursday October 16, 2025 8:31 am PDT by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Apple on Wednesday updated the 14-inch MacBook Pro base model with an M5 chip, and there are two key storage-related upgrades beyond that chip bump.
First, Apple says the new 14-inch MacBook Pro offers up to 2× faster SSD performance than the equivalent previous-generation model, so read and write speeds should get a significant boost. Apple says it is using "the latest storage technology," ...</p></div><div><h3><a href="https://www.macrumors.com/2025/10/16/m5-macbook-air-spring/">M5 MacBook Air Coming Spring 2026 With M5 Mac Studio and Mac Mini in Development</a></h3><p>Thursday October 16, 2025 3:57 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple plans to launch MacBook Air models equipped with the new M5 chip in spring 2026, according to Bloomberg's Mark Gurman. Apple is also working on M5 Pro and M5 Max MacBook Pro models that will come early in the year.
Neither the MacBook Pro models nor the MacBook Air models are expected to get design changes, with Apple focusing on simple chip upgrades. In the case of the MacBook Pro, a m...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[J.P. Morgan's OpenAI loan is strange (246 pts)]]></title>
            <link>https://marketunpack.com/j-p-morgans-openai-loan-is-strange/</link>
            <guid>45648258</guid>
            <pubDate>Mon, 20 Oct 2025 19:38:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marketunpack.com/j-p-morgans-openai-loan-is-strange/">https://marketunpack.com/j-p-morgans-openai-loan-is-strange/</a>, See on <a href="https://news.ycombinator.com/item?id=45648258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
             
            <p>In October, <a href="https://openai.com/index/new-credit-facility-enhances-financial-flexibility/" rel="noreferrer">OpenAI secured a 4 billion dollar revolving credit facility from J.P. Morgan and several other banks</a>.  I was surprised when I heard this because OpenAI is a young company with no earnings.  Shouldn't all their capital come from investors?  Let's run some numbers.</p><h2 id="from-first-principles">From first principles</h2><p>Let's do an <a href="https://www.investopedia.com/terms/e/expected-value.asp" rel="noreferrer">Expected Value (EV)</a> calculation, first from the perspective of an investor and then from the perspective of a lender.  We'll pick some arbitrary parameters first, then refine.</p><p>Putting our investor hat on, the possible returns for investing $1,000 into OpenAI look like this:</p><ul><li>Cost: $1,000</li><li>Case 1 (90%): OpenAI goes bankrupt. Return: $0</li><li>Case 2 (9%): OpenAI becomes a big successful company and goes 10x.  Return: $10,000</li><li>Case 3 (1%): OpenAI becomes the big new thing and goes 100x.  Return: $100,000</li></ul><p>Our expected value is:</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + 0.9 \times 0 + 0.09 \times 10000 + 0.01 \times 100000\\
  EV &amp;= -1000 + 0 + 900 + 1000\\
  EV &amp;= 900
\end{align}\]
</p><!--kg-card-end: html-->
<p>The EV is positive, so this is a good investment.  Obviously, there's a 90% chance of it going to zero, so if this were our only investment, it would be an insanely risky one.  But provided we can do many investments like this and provided their failure cases aren't correlated, this would be a profitable strategy.</p><p>What happens if we instead put our lender hat on?  Using the same probabilities as above, the possible returns for lending $1,000 to OpenAI at 5% interest look like this:</p><ul><li>Cost: $1,000</li><li>Case 1 (90%): OpenAI goes bankrupt. Return: $0</li><li>Case 2 (9%): OpenAI becomes a big successful company and goes 10x.  Return: $1,000 + 5% interest = $1,050</li><li>Case 3 (1%): OpenAI becomes the big new thing and goes 100x.  Return: $1,000 + 5% interest = $1,050</li></ul><p>Lenders don't benefit directly from the success of the company.  Whether it barely scrapes by but manages to repay the loan or becomes the greatest company ever and easily repays the loan, it's all the same to a lender.  So, we can merge cases 2 and 3 into:</p><ul><li>Case 2+3 (10%): OpenAI doesn't go bankrupt.  Return: $1,000 + 5% interest = $1,050</li></ul><p>This makes our EV in the lending case:</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + 0.9 \times 0 + 0.1 \times 1050\\
  EV &amp;= -1000 + 0 + 105\\
  EV &amp;= -895
\end{align}\]
</p><!--kg-card-end: html-->
<p>The EV is negative, so we'd end up losing most of our money on average.  Lending on these terms doesn't make sense.</p><p>There are two numbers we made up in the above calculation: the probability of bankruptcy and the interest rate.  Let's leave the interest rate fixed at 5% and see what the probability \(p\) would have to be for us to break even.</p>
<!--kg-card-begin: html--><p>
\[\begin{align}
  EV &amp;= -1000 + p \times 0 + (1 - p) \times 1050\\
  EV &amp;= -1000 + 1050 - p \times 1050 \\
  EV &amp;= 50 - p \times 1050 \\[0.5cm]
  &amp; \text{Set EV to 0} \\[0.5cm]
  0 &amp;= 50 - p \times 1050 \\
  p &amp;= \frac{50}{1050} \\
  p &amp;= 0.0476
\end{align}\]
</p><!--kg-card-end: html-->
<p>So, we'd break even if the probability of OpenAI going bankrupt was only about 5%.  In practice, we'd want it to be lower than that so that we made a profit and so that we had a margin of safety in case our assumptions were wrong.</p><p>This 5% failure rate seems very optimistic to me, but this scenario is basically the one the consortium of banks got into.  Concrete details on the deal are sparse, but this <a href="https://thecioleaders.com/openai-locks-down-4-billion-revolving-credit-putting-liquidity-over-10-billion/" rel="noreferrer">CIO Leaders article</a> claims the interest rate was "SOFR + 100 basis points".  The <a href="https://www.newyorkfed.org/markets/reference-rates/sofr" rel="noreferrer">overnight SOFR rate</a> is about 4.1% in October, so this puts OpenAI's interest at about 5%.</p><h2 id="from-market-data">From market data</h2><p>The problem with the above expected value calculation is that it's very idealized.  The shape of it is correct, but the real world is too messy to be accurately represented by just a couple of parameters.  I think it would be very difficult to build a model with enough predictive accuracy to be useful and I suspect there just isn't enough publicly available data to plug into it to make it work.</p><p>Luckily for us, banks exist!  We know the banks have the better model and the non-public data and we know they came up with about 5% interest.  So, let's work back from that and see what we can learn.</p><p>We're talking about a loan here and that's very similar to issuing bonds.  So, we should be able to look at the bond market and find companies in similar financial health (from the perspective of a creditor).  One problem is that we only know the overnight rate for OpenAI of about 5%, but bonds on the market will have longer maturities.  We need to calculate what what yield a longer maturity loan would require and we can do that by looking at US treasuries.</p><p>According to <a href="https://www.bloomberg.com/markets/rates-bonds/government-bonds/us" rel="noreferrer">Bloomberg</a>, the three month treasuries have a yield of 3.94%.  One year ones have a yield of 3.58%.</p><figure><img src="https://marketunpack.com/content/images/2025/10/image.png" alt="" loading="lazy" width="856" height="436" srcset="https://marketunpack.com/content/images/size/w600/2025/10/image.png 600w, https://marketunpack.com/content/images/2025/10/image.png 856w" sizes="(min-width: 720px) 720px"><figcaption><b><strong>Figure 1.</strong></b><span> Treasury Yields for US government bonds. The table shows the following yields: 3 months is 3.94%, 6 month is 3.81%, 12 month is 3.58%, 2 year is 3.50%, 5 year is 3.62%, 10 year is 4.03%, 30 year is 4.62%. This describes a smile that initially goes down, goes back up to starting level at 10 years, then continues upwards.</span></figcaption></figure><p>One way of thinking about corporate bonds is that they're basically treasury bonds plus some premium to account for the risk of default.  This <em>default spread</em> seems to be about \(5\% - 3.94\% \approx 1\%\) in OpenAI's case.  By this logic, OpenAI's one year debt would have a yield of about 4.6%.</p><p>Can we find some one year bonds with a yield of 4.6%?</p><figure><img src="https://marketunpack.com/content/images/2025/10/image-3.png" alt="" loading="lazy" width="1133" height="685" srcset="https://marketunpack.com/content/images/size/w600/2025/10/image-3.png 600w, https://marketunpack.com/content/images/size/w1000/2025/10/image-3.png 1000w, https://marketunpack.com/content/images/2025/10/image-3.png 1133w" sizes="(min-width: 720px) 720px"><figcaption><b><strong>Figure 2.</strong></b><span> A sample of corporate USD bonds expiring in one year or less, sorted by their mid-yield to maturity. (Source: Saxo Bank)</span></figcaption></figure><p>Some bonds in the vicinity of what we're looking for are:</p><ul><li>4.99%: HCA Inc. (US healthcare provider with credit rating BBB),</li><li>4.73%: Ziraat Katilim (Turkish bank with credit rating B+), and</li><li>4.24%: Citigroup (US bank with credit rating A).</li></ul><p>In fact, scanning the sample above, it's mostly banks with BBB and A ratings.  So, the consortium of big banks seems to have lent money to OpenAI at the kind of rates they themselves are borrowing at.</p><p>Looking at just a few bonds is interesting, but anecdotal.  It would be better if we had some statistics across the whole bond market.  Helpfully, Prof Damodaran goes through the exercise of calculating just <a href="https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ratings.html" rel="noreferrer">such statistics</a> (<a href="https://web.archive.org/web/20250827084311/https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/ratings.html" rel="noreferrer">archive link</a>) every year, most recently this January.</p><figure><img src="https://marketunpack.com/content/images/2025/10/image-1.png" alt="" loading="lazy" width="417" height="458"><figcaption><b><strong>Figure 3.</strong></b><span> To quote the author: "This is a table that relates the interest coverage ratio of a firm to a 'synthetic' rating and a default spread that goes with that rating. The link between interest coverage ratios and ratings was developed by looking at all rated companies in the United States. The default spreads are obtained from traded bonds. Adding that number to a riskfree rate should yield the pre-tax cost of borrowing for a firm."</span></figcaption></figure><p>Looking up OpenAI's default spread of 1% in that table, we see it's at the level we'd expect for an A- or BBB firm (same as with the anecdotal search earlier).  This normally corresponds to an interest coverage ratio of 3.00-4.24.  However, OpenAI's actual interest coverage ratio is negative because their earnings before interest are negative.</p><p>This doesn't make sense: any way we look at it, OpenAI is getting the kind of interest rates only much more established and profitable firms would be getting.  So, my initial surprise at hearing about this is justified, but there must be an explanation because the big banks wouldn't make such an obvious mistake.</p><h2 id="making-this-make-sense">Making this make sense</h2><p>OpenAI is not a profitable company.  It's also a private company, so we don't get to see audited financials, but we still know some things.  This <a href="https://www.reuters.com/technology/artificial-intelligence/openai-establishes-4-bln-credit-facility-2024-10-03/" rel="noreferrer">Reuters article</a> claims OpenAI is going to generate $3.6 billion in revenue this year, but the costs will lead to a loss of more than $5 billion.</p><p>There's also speculation that their revenue next year will jump to $11.6 billion.  However, there's no speculation about what their earnings will be because they're currently selling their services below cost and there isn't really any story as to how they'll turn this profitable.</p><p>The banks are lenders in this scenario, so they don't really care about how many users OpenAI gets or how huge their revenue becomes.  As lenders, all they care about is getting paid back and it really doesn't seem like OpenAI will have the earnings to do that.  But maybe earnings aren't what matters here.</p><p>If OpenAI can't pay its debts, it goes bankrupt and the creditors seize the company.  Importantly, they seize it from the equity holders.  Who are these equity holders?  According to <a href="https://www.digitalinformationworld.com/2025/09/who-really-owns-openai-billion-dollar.html" rel="noreferrer">this Digital Information World article</a>, the owners are Microsoft (28%), OpenAI non-profit and employees (52%), and other investors (20%).</p><p>So, the hypothetical is OpenAI runs out of money.  They have revenue, but since their costs are higher, they don't actually have anything left over.  They can't make interest payments on their debt, so they go bankrupt, and the banks seize the company from Microsoft.  I don't think Microsoft will allow this to happen.  <a href="https://www.microsoft.com/en-us/investor/earnings/fy-2024-q4/press-release-webcast" rel="noreferrer">Microsoft's earnings for last year were $88 billion</a>, so I think Microsoft will just pay off OpenAI's $4 billion debt in this scenario.  And I think the banks know all this.</p><p>So, the banks loaning money to OpenAI at an A- interest rate doesn't make sense, but effectively loaning the same to <a href="https://www.spglobal.com/ratings/en/regulatory/article/-/view/sourceId/37291" rel="noreferrer">Microsoft with its AAA rating</a> does, and that's what's actually happening here.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When a stadium adds AI to everything, it's worse experience for everyone (157 pts)]]></title>
            <link>https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/</link>
            <guid>45648249</guid>
            <pubDate>Mon, 20 Oct 2025 19:38:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/">https://a.wholelottanothing.org/bmo-stadium-in-la-added-ai-to-everything-and-what-they-got-was-a-worse-experience-for-everyone/</a>, See on <a href="https://news.ycombinator.com/item?id=45648249">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>I just got back from a 24hr trip to Los Angeles to catch my favorite Portland Thorns team, watching them clinch their playoff spot in a match at BMO stadium in downtown Los Angeles.</p><p>In May of 2024, I did the same trip to catch a match on Mother's Day, but I accidentally chose bad seats in the sun and it was hot and uncomfortable. Ultimately, it partially inspired <a href="https://unofficialnwsl.stadium.guide/the-book-is-out/?ref=a.wholelottanothing.org" rel="noreferrer">my wife and I's book reviewing every NWSL soccer stadium</a> so other fans wouldn't suffer the same fate when flying across the country to catch their favorite team.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2952.JPG" alt="" loading="lazy" width="2000" height="1541" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2952.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2952.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2952.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2952.JPG 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>Me and my pal Greg yesterday</span></figcaption></figure><p>This year, I got better seats in the shade and enjoyed the game. But overall? The experience of being in the stadium was worse a year later. After thinking about it on the flight home, I think the reason was the stadium's rush to automation and AI in several places. </p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/SF-20Giants-2004.webp" alt="" loading="lazy" width="2000" height="1500" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/SF-20Giants-2004.webp 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/SF-20Giants-2004.webp 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/SF-20Giants-2004.webp 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/SF-20Giants-2004.webp 2400w" sizes="(min-width: 1200px) 1200px"></figure><h2 id="spoiler-alert-deploying-cameraai-recognition-for-everything-isnt-great">Spoiler alert: deploying camera/AI recognition for everything isn't great</h2><p>Every concession stand, including the ones that didn't even serve hot food, used the apparatus in the photo above to control all checkouts. I assume these are expensive units, because most places that used to have several checkout lanes only had one of them, requiring everyone to checkout through a single location.</p><p>Here's how they worked in the stadium yesterday: You place all your items on the white shelf with some space between them. Although they were clearly designed to be a self-checkout experience, the stadium had a staff member rearrange your items, then for about 30 seconds the kiosk would be thinking. After, it would pop up all items on the menu, and the staff member would have to tap to confirm what each item was. Then another 30 seconds to calculate and move the purchase to a point of sale/tap on the side, then you'd pay. </p><p>Overall, this added at least one, if not two full minutes to every transaction that didn't normally have those delays. Lines were unbearably long, and it was a hot day in LA yesterday, at 87ºF/30ºC. I bought food and drinks several times over the the course of the day and had to endure the process multiple times.</p><h2 id="when-you-add-object-recognition-youre-incentivized-to-reduce-choices">When you add object recognition, you're incentivized to reduce choices</h2><p>Here's an unintended consequence of moving all your concession stand checkouts to computer vision: it's easier if you have less things on offer.</p><p>Case in point: Let's talk about my favorite concession stand at BMO last year, a place that served rotisserie chicken with waffle fries and chicken sandwiches. Here's our meal from 2024, it was well-seasoned, came with great sauces, and was one of the best meals I had at a stadium in my entire nationwide tour, which is why I remembered it.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_5563.JPG" alt="" loading="lazy" width="2000" height="1974" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_5563.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_5563.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_5563.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_5563.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>I returned to the same concession stand yesterday and here's their new menu:</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2925.JPG" alt="" loading="lazy" width="2000" height="1380" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2925.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2925.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2925.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2925.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>When your checkout stand relies on computer vision, it's probably confusing to have half a dozen different menu items that fans can enjoy. But if you could condense it to just chicken tenders, fries, a hot dog, and boxes of candy, your computer vision-based checkout system will probably work faster since it has to do less work with the obvious shapes of each of those items.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_5571.JPG" alt="" loading="lazy" width="2000" height="1983" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_5571.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_5571.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_5571.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_5571.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Looking through my photos from my 2024 visit, I saw a variety of food options including smashburgers and a Korean BBQ rice bowl I also tried, pictured above. </p><p>If foods are difficult for computer vision to decipher, why not get rid of most options? Walking around the stadium yesterday, the menus were basically all hot dogs, pizza, nachos, and chicken tenders.</p><h2 id="even-quick-service-options-sucked">Even quick service options sucked</h2><p>As I said, it was a hot day, I was constantly parched, and I ended up drinking four bottles of water over the course of three hours. Each time, I had to go through the automated checkout gauntlet, and each time it required a long wait in a line, while I missed bits of the match.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2984.JPG" alt="" loading="lazy" width="2000" height="2667" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2984.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2984.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2984.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2984.JPG 2400w" sizes="(min-width: 720px) 720px"></figure><p>Late in the game, I wanted to get water quickly and they had these "vending kiosks" that were fully automated. You'd tap your phone on the locked door, it would unlock, you'd grab items, then close the door. Next, you had to stand there for about 2 minutes while it said "calculating checkout" before showing you a receipt on the screen.</p><p>What was supposed to be fast was very slow. The person in front of me bought two items and saw she got charged for three. Since there were no paper receipts, she took a photo of the machine before going to the guest services to complain. I missed ten minutes of the game getting water.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2983.JPG" alt="" loading="lazy" width="2000" height="1265" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2983.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2983.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2983.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2983.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>This was a quick service "market" style place and last year, you'd just grab stuff off a shelf, and checkout quickly from staff at multiple registers. This year, it had a long line snaking all over because of the slow AI/camera checkout kiosks.</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2980.JPG" alt="" loading="lazy" width="2000" height="1500" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2980.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2980.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2980.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2980.JPG 2400w"></figure><p>It was a busy game, being the last home match for the fans and I would guess there were around 17,000-18,000 people in attendance. When it's nearly 90ºF/30ºC, heat exhaustion becomes a problem for crowds. When it takes people ten minutes to buy a bottle of water (I didn't see automated water fillers at the restrooms), the embrace of slow AI/Camera-based checkout systems starts to become a health and safety issue for the crowd.</p><h2 id="but-mrs-lincoln%E2%80%94besides-the-obvious%E2%80%94how-was-the-play">But Mrs. Lincoln—besides the obvious—how was the play?</h2><p>A year later visiting the same stadium, I got worse food, slower service, and a worse overall experience. On the bright side, the billionaire stadium owners probably got to reduce their staff in the process while maybe increasing profits.</p><p>The company behind the kiosks <a href="https://blog.mashgin.com/ai-retail/mashgin-expands-footprint-at-bmo-stadium?ref=a.wholelottanothing.org" rel="noreferrer">claims they are 400% faster than human checkers and result in a 25% increase in profits</a>. After experiencing it in person yesterday, I think those numbers are bullshit. Human checkers are clearly faster and smoother, and I bet they sold more food and drinks when people could get them quickly.</p><p>And the portions? They were so small!</p><figure><img src="https://a.wholelottanothing.org/content/images/2025/10/IMG_2977.JPG" alt="" loading="lazy" width="2000" height="2354" srcset="https://a.wholelottanothing.org/content/images/size/w600/2025/10/IMG_2977.JPG 600w, https://a.wholelottanothing.org/content/images/size/w1000/2025/10/IMG_2977.JPG 1000w, https://a.wholelottanothing.org/content/images/size/w1600/2025/10/IMG_2977.JPG 1600w, https://a.wholelottanothing.org/content/images/size/w2400/2025/10/IMG_2977.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure>
    </div><div>
                        <h3>
                                Subscribe to get new posts in your inbox
                        </h3>
                        

                            
                    </div></div>]]></description>
        </item>
    </channel>
</rss>