<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 11 Aug 2024 01:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Things I Won't Work With: Dimethylcadmium (2013) (126 pts)]]></title>
            <link>https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium</link>
            <guid>41211540</guid>
            <pubDate>Sat, 10 Aug 2024 19:11:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium">https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium</a>, See on <a href="https://news.ycombinator.com/item?id=41211540">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[DEF CON's response to the badge controversy (185 pts)]]></title>
            <link>https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/</link>
            <guid>41211519</guid>
            <pubDate>Sat, 10 Aug 2024 19:07:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/">https://old.reddit.com/r/Defcon/comments/1ep00ln/def_cons_response_to_the_badge_controversy/</a>, See on <a href="https://news.ycombinator.com/item?id=41211519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>DEF CON thrives on community collaboration and has operated for over 30 years successfully working with hundreds of vendors including the dozens that have helped with our badges over the years. For this year’s Raspberry Pi badges, DEF CON hired Entropic Engineering to do the hardware development and firmware. After going overbudget by more than 60%, several bad-faith charges, and with a product still in preproduction, DEF CON issued a stop work order. Any claims that DEF CON did not pay Entropic Engineering for its hardware or firmware development are false. Unfortunately, we heard that these issues with Entropic Engineering were not unique to DEF CON. We decided at that point to finish the badge on our own. We paid to send engineers to Vietnam to work onsite to finalize and test the badges in order to ensure they would be done on time for the conference. We never removed Entropic Engineering’s logo from our badge, it is still on the PCB. However, Entropic was not involved in the design and production of the case, and we removed their logo we had added as a courtesy.</p>

<p>We were happy to still include one of their contractors on the badge panel session. Unfortunately, shortly before the talk was set to take place DEF CON became aware that unauthorized code had been included in the firmware we had paid Entropic Engineering to produce, claiming credit for the whole badge and promoting their coin wallet to solicit money from DEF CON attendees above and beyond what we had negotiated. When asked about the unauthorized code, the engineer said it had been done as a “joke” two months ago and forgot to remove it, and we decided as an organization not to have him on stage while we kept the slides in the talk giving him credit for his work. We communicated the change in advance of the talk, and this individual decided to show up for the panel anyway. He refused to leave, demanding that our security team remove him. Wanting to ensure that the other people involved in creating the badge were able to deliver their presentation, we complied with his wishes and escorted him off the stage, where he was free to continue attending the conference.</p>

<p>Any issues of non-payment are between him and Entropic Engineering, DEF CON fulfilled its financial obligations.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Jeff Bezos and Amazon tried to imprison my husband" (203 pts)]]></title>
            <link>https://twitter.com/Amy_K_Nelson/status/1822318185556648348</link>
            <guid>41211437</guid>
            <pubDate>Sat, 10 Aug 2024 18:53:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Amy_K_Nelson/status/1822318185556648348">https://twitter.com/Amy_K_Nelson/status/1822318185556648348</a>, See on <a href="https://news.ycombinator.com/item?id=41211437">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple. Apple Please (143 pts)]]></title>
            <link>https://digipres.club/@misty/112927898002214724</link>
            <guid>41211235</guid>
            <pubDate>Sat, 10 Aug 2024 18:14:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digipres.club/@misty/112927898002214724">https://digipres.club/@misty/112927898002214724</a>, See on <a href="https://news.ycombinator.com/item?id=41211235">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pg_replicate – Build Postgres replication applications in Rust (123 pts)]]></title>
            <link>https://github.com/supabase/pg_replicate</link>
            <guid>41209994</guid>
            <pubDate>Sat, 10 Aug 2024 15:00:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/supabase/pg_replicate">https://github.com/supabase/pg_replicate</a>, See on <a href="https://news.ycombinator.com/item?id=41209994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">About <code>pg_replicate</code></h2><a id="user-content-about-pg_replicate" aria-label="Permalink: About pg_replicate" href="#about-pg_replicate"></a></p>
<p dir="auto"><code>pg_replicate</code> is a Rust crate to quickly build replication solutions for Postgres. It provides building blocks to construct data pipelines which can continually copy data from Postgres to other systems. It builds abstractions on top of Postgres's <a href="https://www.postgresql.org/docs/current/protocol-logical-replication.html" rel="nofollow">logical streaming replication protocol</a> and pushes users towards the pit of success without letting them worry about low level details of the protocol.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">To quickly try out <code>pg_replicate</code>, you can run the <code>stdout</code> example, which will replicate the data to standard output. First, create a publication in Postgres which includes the tables you want to replicate:</p>
<div data-snippet-clipboard-copy-content="create publication my_publication
for table table1, table2;"><pre><code>create publication my_publication
for table table1, table2;
</code></pre></div>
<p dir="auto">Then run the <code>stdout</code> example:</p>
<div data-snippet-clipboard-copy-content="cargo run --example stdout -- --db-host localhost --db-port 5432 --db-name postgres --db-username postgres --db-password password cdc my_publication stdout_slot"><pre><code>cargo run --example stdout -- --db-host localhost --db-port 5432 --db-name postgres --db-username postgres --db-password password cdc my_publication stdout_slot
</code></pre></div>
<p dir="auto">In the above example, <code>pg_replicate</code> connects to a Postgres database named <code>postgres</code> running on <code>localhost:5432</code> with a username <code>postgres</code> and password <code>password</code>. The slot name <code>stdout_slot</code> will be created by <code>pg_replicate</code> automatically.</p>
<p dir="auto">Refer to the <a href="https://github.com/imor/pg_replicate/tree/main/pg_replicate/examples">examples</a> folder to run examples for sinks other than <code>stdout</code> (currently only <code>bigquery</code> and <code>duckdb</code> supported). A quick tip: to see all the command line options, run the example wihout any options specified, e.g. <code>cargo run --example bigquery</code> will print the detailed usage instructions for the <code>bigquery</code> sink.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">To use <code>pg_replicate</code> in your Rust project, add it via a git dependency in <code>Cargo.toml</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
pg_replicate = { git = &quot;https://github.com/imor/pg_replicate&quot; }"><pre>[<span>dependencies</span>]
<span>pg_replicate</span> = { <span>git</span> = <span><span>"</span>https://github.com/imor/pg_replicate<span>"</span></span> }</pre></div>
<p dir="auto">The git dependency is needed for now because <code>pg_replicate</code> is not yet published on crates.io. You'd also need to add a dependency to tokio:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[dependencies]
...
tokio = { version = &quot;1.38&quot; }"><pre>[<span>dependencies</span>]
<span>...</span>
<span>tokio</span> = { <span>version</span> = <span><span>"</span>1.38<span>"</span></span> }</pre></div>
<p dir="auto">Now your <code>main.rs</code> can have code like the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="use std::error::Error;

use pg_replicate::pipeline::{
    data_pipeline::DataPipeline,
    sinks::stdout::StdoutSink,
    sources::postgres::{PostgresSource, TableNamesFrom},
    PipelineAction,
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    let host = &quot;localhost&quot;;
    let port = 5432;
    let database = &quot;postgres&quot;;
    let username = &quot;postgres&quot;;
    let password = Some(&quot;password&quot;.to_string());
    let slot_name = Some(&quot;my_slot&quot;.to_string());
    let table_names = TableNamesFrom::Publication(&quot;my_publication&quot;.to_string());

    // Create a PostgresSource
    let postgres_source = PostgresSource::new(
        host,
        port,
        database,
        username,
        password,
        slot_name,
        table_names,
    )
    .await?;

    // Create a StdoutSink. This sink just prints out the events it receives to stdout
    let stdout_sink = StdoutSink;

    // Create a `DataPipeline` to connect the source to the sink
    let mut pipeline = DataPipeline::new(postgres_source, stdout_sink, PipelineAction::Both);

    // Start the `DataPipeline` to start copying data from Postgres to stdout
    pipeline.start().await?;

    Ok(())
}
"><pre><span>use</span> std<span>::</span>error<span>::</span><span>Error</span><span>;</span>

<span>use</span> pg_replicate<span>::</span>pipeline<span>::</span><span>{</span>
    data_pipeline<span>::</span><span>DataPipeline</span><span>,</span>
    sinks<span>::</span>stdout<span>::</span><span>StdoutSink</span><span>,</span>
    sources<span>::</span>postgres<span>::</span><span>{</span><span>PostgresSource</span><span>,</span> <span>TableNamesFrom</span><span>}</span><span>,</span>
    <span>PipelineAction</span><span>,</span>
<span>}</span><span>;</span>

<span>#<span>[</span>tokio<span>::</span>main<span>]</span></span>
<span>async</span> <span>fn</span> <span>main</span><span>(</span><span>)</span> -&gt; <span>Result</span><span>&lt;</span><span>(</span><span>)</span><span>,</span> <span>Box</span><span>&lt;</span><span>dyn</span> <span>Error</span><span>&gt;</span><span>&gt;</span> <span>{</span>
    <span>let</span> host = <span>"localhost"</span><span>;</span>
    <span>let</span> port = <span>5432</span><span>;</span>
    <span>let</span> database = <span>"postgres"</span><span>;</span>
    <span>let</span> username = <span>"postgres"</span><span>;</span>
    <span>let</span> password = <span>Some</span><span>(</span><span>"password"</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> slot_name = <span>Some</span><span>(</span><span>"my_slot"</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>)</span><span>;</span>
    <span>let</span> table_names = <span>TableNamesFrom</span><span>::</span><span>Publication</span><span>(</span><span>"my_publication"</span><span>.</span><span>to_string</span><span>(</span><span>)</span><span>)</span><span>;</span>

    <span>// Create a PostgresSource</span>
    <span>let</span> postgres_source = <span>PostgresSource</span><span>::</span><span>new</span><span>(</span>
        host<span>,</span>
        port<span>,</span>
        database<span>,</span>
        username<span>,</span>
        password<span>,</span>
        slot_name<span>,</span>
        table_names<span>,</span>
    <span>)</span>
    <span>.</span><span>await</span>?<span>;</span>

    <span>// Create a StdoutSink. This sink just prints out the events it receives to stdout</span>
    <span>let</span> stdout_sink = <span>StdoutSink</span><span>;</span>

    <span>// Create a `DataPipeline` to connect the source to the sink</span>
    <span>let</span> <span>mut</span> pipeline = <span>DataPipeline</span><span>::</span><span>new</span><span>(</span>postgres_source<span>,</span> stdout_sink<span>,</span> <span>PipelineAction</span><span>::</span><span>Both</span><span>)</span><span>;</span>

    <span>// Start the `DataPipeline` to start copying data from Postgres to stdout</span>
    pipeline<span>.</span><span>start</span><span>(</span><span>)</span><span>.</span><span>await</span>?<span>;</span>

    <span>Ok</span><span>(</span><span>(</span><span>)</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto">For more examples, please refer to the <a href="https://github.com/imor/pg_replicate/tree/main/pg_replicate/examples">examples</a> folder in the source.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Repository Structure</h2><a id="user-content-repository-structure" aria-label="Permalink: Repository Structure" href="#repository-structure"></a></p>
<p dir="auto">The repository is a cargo workspace. Each of the individual sub-folders are crate in the workspace. A brief explanation of each crate is as follows:</p>
<ul dir="auto">
<li><code>api</code> - REST api used for hosting <code>pg_replicate</code> in a cloud environment.</li>
<li><code>config-types</code> - Common types for configuration used in projects across the workspace.</li>
<li><code>pg_replicate</code> - The main library crate containing the core logic.</li>
<li><code>replicator</code> - A binary crate using <code>pg_replicate</code>. Packaged as a docker container for use in cloud hosting.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto"><code>pg_replicate</code> is still under heavy development so expect bugs and papercuts but overtime we plan to add the following sinks.</p>
<ul>
<li> Add BigQuery Sink</li>
<li> Add DuckDb Sink</li>
<li> Add MotherDuck Sink</li>
<li> Add Snowflake Sink</li>
<li> Add ClickHouse Sink</li>
<li> Many more to come...</li>
</ul>
<p dir="auto">Note: DuckDb and MotherDuck sinks do no use the batched pipeline, hence they currently perform poorly. A batched pipeline version of these sinks is planned.</p>
<p dir="auto">See the <a href="https://github.com/imor/pg_replicate/issues">open issues</a> for a full list of proposed features (and known issues).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Distributed under the Apache-2.0 License. See <code>LICENSE</code> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docker</h2><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">To create the docker image for <code>replicator</code> run <code>docker build -f ./replicator/Dockerfile .</code> from the root of the repo. Similarly, to create the docker image for <code>api</code> run <code>docker build -f ./api/Dockerfile .</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design</h2><a id="user-content-design" aria-label="Permalink: Design" href="#design"></a></p>
<p dir="auto">Applications can use data sources and sinks from <code>pg_replicate</code> to build a data pipeline to continually copy data from the source to the sink. For example, a data pipeline to copy data from Postgres to DuckDB takes about 100 lines of Rust.</p>
<p dir="auto">There are three components in a data pipeline:</p>
<ol dir="auto">
<li>A data source</li>
<li>A data sink</li>
<li>A pipline</li>
</ol>
<p dir="auto">The data source is an object from where data will be copied. The data sink is an object to which data will be copied. The pipeline is an object which drives the data copy operations from the source to the sink.</p>
<div data-snippet-clipboard-copy-content=" +----------+                       +----------+
 |          |                       |          |
 |  Source  |---- Data Pipeline --->|   Sink   |
 |          |                       |          |
 +----------+                       +----------+"><pre><code> +----------+                       +----------+
 |          |                       |          |
 |  Source  |---- Data Pipeline ---&gt;|   Sink   |
 |          |                       |          |
 +----------+                       +----------+
</code></pre></div>
<p dir="auto">So roughly you write code like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let postgres_source = PostgresSource::new(...);
let duckdb_sink = DuckDbSink::new(..);
let pipeline = DataPipeline(postgres_source, duckdb_sink);
pipeline.start();"><pre><span>let</span> postgres_source = <span>PostgresSource</span><span>::</span><span>new</span><span>(</span>..<span>.</span><span></span><span>)</span><span>;</span>
<span>let</span> duckdb_sink = <span>DuckDbSink</span><span>::</span><span>new</span><span>(</span>..<span>)</span><span>;</span>
<span>let</span> pipeline = <span>DataPipeline</span><span>(</span>postgres_source<span>,</span> duckdb_sink<span>)</span><span>;</span>
pipeline<span>.</span><span>start</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">Of course, the real code is more than these four lines, but this is the basic idea. For a complete example look at the <a href="https://github.com/imor/pg_replicate/blob/main/pg_replicate/examples/duckdb.rs">duckdb example</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Sources</h3><a id="user-content-data-sources" aria-label="Permalink: Data Sources" href="#data-sources"></a></p>
<p dir="auto">A data source is the source for data which the pipeline will copy to the data sink. Currently, the repository has only one data source: <a href="https://github.com/imor/pg_replicate/blob/main/pg_replicate/src/pipeline/sources/postgres.rs"><code>PostgresSource</code></a>. <code>PostgresSource</code> is the primary data source; data in any other source or sink would have originated from it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Sinks</h3><a id="user-content-data-sinks" aria-label="Permalink: Data Sinks" href="#data-sinks"></a></p>
<p dir="auto">A data sink is where the data from a data source is copied. There are two kinds of data sinks. Those which retain the essential nature of data coming out of a <code>PostgresSource</code> and those which don't. The former kinds of data sinks can act as a data source in future. The latter kind can't act as a data source and are data's final resting place.</p>
<p dir="auto">For instance, <a href="https://github.com/imor/pg_replicate/blob/main/pg_replicate/src/pipeline/sinks/duckdb.rs"><code>DuckDbSink</code></a> ensures that the change data capture (CDC) stream coming in from a source is materialized into tables in a DuckDB database. Once this lossy data transformation is done, it can not be used as a CDC stream again.</p>
<p dir="auto">Contrast this with a potential future sink <code>S3Sink</code> or <code>KafkaSink</code> which just copies the CDC stream as is. The data deposited in the sink can later be used as if it was coming from Postgres directly.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Data Pipeline</h3><a id="user-content-data-pipeline" aria-label="Permalink: Data Pipeline" href="#data-pipeline"></a></p>
<p dir="auto">A data pipeline encapsulates the business logic to copy the data from the source to the sink. It also orchestrates resumption of the CDC stream from the exact location it was last stopped at. The data sink participates in this by persisting the resumption state and returning it to the pipeline when it restarts.</p>
<p dir="auto">If a data sink is not transactional (e.g. <code>S3Sink</code>), it is not always possible to keep the CDC stream and the resumption state consistent with each other. This can result in these non-transactional sinks having duplicate portions of the CDC stream. Data pipeline helps in deduplicating these duplicate CDC events when the data is being copied over to a transactional store like DuckDB.</p>
<p dir="auto">Finally, the data pipeline reports back the log sequence number (LSN) upto which the CDC stream has been copied in the sink to the <code>PostgresSource</code>. This allows the Postgres database to reclaim disk space by removing WAL segment files which are no longer required by the data sink.</p>
<div data-snippet-clipboard-copy-content=" +----------+                       +----------+
 |          |                       |          |
 |  Source  |<---- LSN Numbers -----|   Sink   |
 |          |                       |          |
 +----------+                       +----------+"><pre><code> +----------+                       +----------+
 |          |                       |          |
 |  Source  |&lt;---- LSN Numbers -----|   Sink   |
 |          |                       |          |
 +----------+                       +----------+
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Kinds of Data Copies</h3><a id="user-content-kinds-of-data-copies" aria-label="Permalink: Kinds of Data Copies" href="#kinds-of-data-copies"></a></p>
<p dir="auto">CDC stream is not the only kind of data a data pipeline performs. There's also full table copy, aka backfill. These two kinds can be performed either together or separately. For example, a one-off data copy can use the backfill. But if you want to regularly copy data out of Postgres and into your OLAP database, backfill and CDC stream both should be used. Backfill to get the intial copies of the data and CDC stream to keep those copies up to date and changes in Postgres happen to the copied tables.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance</h3><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">Currently the data source and sinks copy table row and CDC events one at a time. This is expected to be slow. Batching, and other strategies will likely improve the performance drastically. But at this early stage the focus is on correctness rather than performance. There are also zero benchmarks at this stage, so commentary about performance is closer to speculation than reality.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSnitch is a GNU/Linux interactive application firewall (252 pts)]]></title>
            <link>https://github.com/evilsocket/opensnitch</link>
            <guid>41209688</guid>
            <pubDate>Sat, 10 Aug 2024 14:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/evilsocket/opensnitch">https://github.com/evilsocket/opensnitch</a>, See on <a href="https://news.ycombinator.com/item?id=41209688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p>
  Join the project community on our server!
  </p><p>
  <a href="https://discord.gg/https://discord.gg/btZpkp45gQ" title="Join our community!" rel="nofollow">
    <img src="https://camo.githubusercontent.com/549e93886be3c89143d30b3a80f7b34e8fedee957710c2481953ddde669193c6/68747470733a2f2f646362616467652e6c696d65732e70696e6b2f6170692f7365727665722f68747470733a2f2f646973636f72642e67672f62745a706b7034356751" data-canonical-src="https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ">
  </a></p></div>
<hr>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png"><img alt="opensnitch" src="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png" height="160"></a>
  </p><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"><img src="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"></a>
    <a href="https://github.com/evilsocket/opensnitch/releases/latest"><img alt="Release" src="https://camo.githubusercontent.com/219cfed611036fcac4f1c190954d3d0af9f7d489d2e5d69e10b2415a86f18de2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6576696c736f636b65742f6f70656e736e697463682e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/release/evilsocket/opensnitch.svg?style=flat-square"></a>
    <a href="https://github.com/evilsocket/opensnitch/blob/master/LICENSE.md"><img alt="Software License" src="https://camo.githubusercontent.com/d74ecd3c454461cffea50e16ee633e212ab258222b06e5fd630d34c5429c2fa5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c332d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square"></a>
    <a href="https://goreportcard.com/report/github.com/evilsocket/opensnitch/daemon" rel="nofollow"><img alt="Go Report Card" src="https://camo.githubusercontent.com/6d9060c6e28f36e61ee8e0f59335b109a23f9ce97e9554da72cd553a3efca3dd/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6576696c736f636b65742f6f70656e736e697463682f6461656d6f6e3f7374796c653d666c61742d737175617265" data-canonical-src="https://goreportcard.com/badge/github.com/evilsocket/opensnitch/daemon?style=flat-square"></a>
    <a href="https://repology.org/project/opensnitch/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/24dbb94e706fb18f6b34697db56522fcbe2f6172f058b05710822bd39a45a367/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f74696e792d7265706f732f6f70656e736e697463682e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/tiny-repos/opensnitch.svg"></a>
  </p>

<p dir="auto"><strong>OpenSnitch</strong> is a GNU/Linux application firewall.</p>
<p dir="auto">•• <a href="#key-features">Key Features</a> • <a href="#download">Download</a> • <a href="#installation">Installation</a> • <a href="#opensnitch-in-action">Usage examples</a> • <a href="#in-the-press">In the press</a> ••</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png"><img src="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png" alt="OpenSnitch"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key features</h2><a id="user-content-key-features" aria-label="Permalink: Key features" href="#key-features"></a></p>
<ul dir="auto">
<li>Interactive outbound connections filtering.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/block-lists">Block ads, trackers or malware domains</a> system wide.</li>
<li>Ability to <a href="https://github.com/evilsocket/opensnitch/wiki/System-rules">configure system firewall</a> from the GUI (nftables).
<ul dir="auto">
<li>Configure input policy, allow inbound services, etc.</li>
</ul>
</li>
<li>Manage <a href="https://github.com/evilsocket/opensnitch/wiki/Nodes">multiple nodes</a> from a centralized GUI.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/SIEM-integration">SIEM integration</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto">Download deb/rpm packages for your system from <a href="https://github.com/evilsocket/opensnitch/releases">https://github.com/evilsocket/opensnitch/releases</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">deb</h4><a id="user-content-deb" aria-label="Permalink: deb" href="#deb"></a></p>
<blockquote>
<p dir="auto">$ sudo apt install ./opensnitch*.deb ./python3-opensnitch-ui*.deb</p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">rpm</h4><a id="user-content-rpm" aria-label="Permalink: rpm" href="#rpm"></a></p>
<blockquote>
<p dir="auto">$ sudo yum localinstall opensnitch-1*.rpm; sudo yum localinstall opensnitch-ui*.rpm</p>
</blockquote>
<p dir="auto">Then run: <code>$ opensnitch-ui</code> or launch the GUI from the Applications menu.</p>
<p dir="auto">Please, refer to <a href="https://github.com/evilsocket/opensnitch/wiki/Installation">the documentation</a> for detailed information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OpenSnitch in action</h2><a id="user-content-opensnitch-in-action" aria-label="Permalink: OpenSnitch in action" href="#opensnitch-in-action"></a></p>
<p dir="auto">Examples of OpenSnitch intercepting unexpected connections:</p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell">https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell</a></p>
<p dir="auto">Have you seen a connection you didn't expect? <a href="https://github.com/evilsocket/opensnitch/discussions/new?category=show-and-tell">submit it!</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">In the press</h2><a id="user-content-in-the-press" aria-label="Permalink: In the press" href="#in-the-press"></a></p>
<ul dir="auto">
<li>2017 <a href="https://twitter.com/pentestmag/status/857321886807605248" rel="nofollow">PenTest Magazine</a></li>
<li>11/2019 <a href="https://itsfoss.com/opensnitch-firewall-linux/" rel="nofollow">It's Foss</a></li>
<li>03/2020 <a href="https://www.linux-magazine.com/Issues/2020/232/Firewalld-and-OpenSnitch" rel="nofollow">Linux Format #232</a></li>
<li>08/2020 <a href="https://linux-magazine.pl/archiwum/wydanie/387" rel="nofollow">Linux Magazine Polska #194</a></li>
<li>08/2021 <a href="https://github.com/evilsocket/opensnitch/discussions/631" data-hovercard-type="discussion" data-hovercard-url="/evilsocket/opensnitch/discussions/631/hovercard">Linux Format #280</a></li>
<li>02/2022 <a href="https://www.linux-community.de/magazine/linuxuser/2022/03/" rel="nofollow">Linux User</a></li>
<li>06/2022 <a href="https://www.linux-magazine.com/Issues/2022/259/OpenSnitch" rel="nofollow">Linux Magazine #259</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donations</h2><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you find OpenSnitch useful and want to donate to the dedicated developers, you can do it from the <strong>Sponsor this project</strong> section on the right side of this repository.</p>
<p dir="auto">You can see here who are the current maintainers of OpenSnitch:
<a href="https://github.com/evilsocket/opensnitch/commits/master">https://github.com/evilsocket/opensnitch/commits/master</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/graphs/contributors">See the list</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translating</h2><a id="user-content-translating" aria-label="Permalink: Translating" href="#translating"></a></p>
<a href="https://hosted.weblate.org/engage/opensnitch/" rel="nofollow">
<img src="https://camo.githubusercontent.com/75c6d0a4c406c1a7802bb3b7283238c3df71084751e7aa00f9822d203876e903/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f6f70656e736e697463682f2d2f676c6f73736172792f6d756c74692d6175746f2e737667" alt="Translation status" data-canonical-src="https://hosted.weblate.org/widgets/opensnitch/-/glossary/multi-auto.svg">
</a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Live Cam: Real-time face swapping and one-click video deepfake tool (204 pts)]]></title>
            <link>https://deeplive.cam</link>
            <guid>41209181</guid>
            <pubDate>Sat, 10 Aug 2024 13:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deeplive.cam">https://deeplive.cam</a>, See on <a href="https://news.ycombinator.com/item?id=41209181">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="hero"><h2>Deep Live Cam<br> <!-- -->The Next Leap in Real-Time Face Swapping and Video Deepfake Technology</h2><p>Deep Live Cam harnesses cutting-edge AI to push the boundaries of real-time face swapping and video deepfakes.<br> <!-- -->Achieve high-quality face replacement with just a single image.</p></section><div><div id="features"><p><h4>editions</h4><h2>Deep Live Cam Supports Multiple Execution Platforms</h2></p></div><div><h2>Deep Live Cam: Bringing Your Ideas to Life</h2><p>Deep Live Cam is a state-of-the-art AI tool that delivers astonishingly accurate real-time face swapping and video deepfakes. Here's what sets it apart:</p><div><div><p>Swap faces in real-time using a single image, with instant preview capabilities.</p></div><div><div><p><span>One-Click Video Deepfakes</span></p></div><p>Generate high-quality deepfake videos quickly and easily with simple operations.</p></div><div><p>Run on various platforms including CPU, NVIDIA CUDA, and Apple Silicon, adapting to different hardware setups.</p></div><div><p>Built-in checks prevent processing of inappropriate content, ensuring legal and ethical use.</p></div><div><p>Leverages optimized algorithms for significantly faster processing, especially on CUDA-enabled NVIDIA GPUs.</p></div><div><p>Benefit from an active community providing ongoing support and improvements, keeping the tool at the cutting edge.</p></div></div></div><div><h2>How Deep Live Cam Works</h2><p>Deep Live Cam employs advanced AI algorithms to achieve real-time face swapping and video deepfakes.</p></div><section><h2>What Users Are Saying About Deep Live Cam on X</h2><p>Explore real experiences and creations shared by developers and users on X. See how Deep Live Cam is inspiring creativity and solving practical problems across various fields, from stunning face-swap effects to innovative applications.</p></section><div id="faq"><div><h2>Frequently Asked Questions About Deep Live Cam</h2><p>Get answers to common questions about Deep Live Cam</p></div><div><div><h3>What is Deep Live Cam?</h3><p>Deep Live Cam is an open-source tool for real-time face swapping and one-click video deepfakes. It can replace faces in videos or images using a single photo, ideal for video production, animation, and various creative projects.</p><hr></div><div><h3>What are the main features of Deep Live Cam?</h3><p>Deep Live Cam's key features include: 1) Real-time face swapping; 2) One-click video deepfakes; 3) Multi-platform support; 4) Ethical use safeguards.</p><hr></div><div><h3>How do I use Deep Live Cam?</h3><p>To use Deep Live Cam: 1) Set up the required environment; 2) Clone the GitHub repository; 3) Download necessary models; 4) Install dependencies; 5) Run the program; 6) Select source image and target; 7) Start the face-swapping process.</p><hr></div><div><h3>Which platforms does Deep Live Cam support?</h3><p>Deep Live Cam supports various execution platforms, including CPU, NVIDIA CUDA, Apple Silicon (CoreML), DirectML (Windows), and OpenVINO (Intel). Users can choose the optimal platform based on their hardware configuration.</p><hr></div><div><h3>How does Deep Live Cam prevent misuse?</h3><p>Deep Live Cam incorporates built-in checks to prevent processing of inappropriate content (e.g., nudity, violence, sensitive material). The developers are committed to evolving the project within legal and ethical frameworks, implementing measures like watermarking outputs when necessary to prevent abuse.</p><hr></div><div><h3>Is Deep Live Cam free to use?</h3><p>Yes, Deep Live Cam is an open-source project and completely free to use. You can access the source code on GitHub and use it freely.</p><hr></div><div><h3>Can I use Deep Live Cam for commercial purposes?</h3><p>While Deep Live Cam is open-source, for commercial use, you should carefully review the project's license terms. Additionally, using deepfake technology may involve legal and ethical considerations. We recommend consulting with legal professionals before any commercial application.</p><hr></div><div><h3>What are the hardware requirements for Deep Live Cam?</h3><p>Deep Live Cam's performance varies with hardware configuration. Basic functionality runs on standard CPUs, but for optimal performance and results, we recommend using CUDA-enabled NVIDIA GPUs or devices with Apple Silicon chips.</p><hr></div><div><h3>Does Deep Live Cam support real-time video stream processing?</h3><p>Yes, Deep Live Cam supports real-time video stream processing. You can use a webcam for real-time face swapping, with the program providing live preview functionality.</p><hr></div><div><h3>How can I improve the face-swapping results in Deep Live Cam?</h3><p>To enhance face-swapping results, try: 1) Using high-quality, clear source images; 2) Choosing source and target images with similar angles and lighting; 3) Adjusting program parameters; 4) Running the program on more powerful hardware.</p><hr></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A wonderful coincidence or an expected connection: why π² ≈ g (355 pts)]]></title>
            <link>https://roitman.io/blog/91</link>
            <guid>41208988</guid>
            <pubDate>Sat, 10 Aug 2024 12:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roitman.io/blog/91">https://roitman.io/blog/91</a>, See on <a href="https://news.ycombinator.com/item?id=41208988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Let’s take a brief trip back to our school years and recall some lessons in mathematics and physics. Do you remember what the number π equals? And what is π squared? That’s a strange question too. Of course, it’s 9.87. And do you remember the value of the acceleration due to gravity, g? Of course, that number was drilled into our memory so thoroughly that it’s impossible to forget: 9.81 m/s². Naturally, it can vary, but for solving basic school problems, we typically used this value.</p><p>August 9, 2024</p></div><p><span><h2><strong>Mysterious equality</strong></h2><p>And now, here’s the next question: how on earth is it that π² is approximately equal to g? You might say that such questions aren’t asked in polite society. First of all, they aren’t exactly equal. There’s already a difference in the second decimal place. Secondly, π is a dimensionless number, while g is a physical quantity with its own units.</p><p>And yet, no matter how you look at it, this can’t just be a simple coincidence.</p><h2><strong>Not as simple as it seems</strong></h2><p>Let's start by taking a close look at the right side. The value 9.81 is in m/s². But these are far from the only units of measurement. If you express this value in any other units, the magic immediately disappears. So, this is no coincidence—let's dig deeper into the meters and seconds.</p><p>What exactly is a "meter," and how could it be related to π? At first glance, not at all. According to Wikipedia, a "meter is the distance light travels in a vacuum during a time interval of 1/299,792,458 seconds." Great, now we have seconds involved—good! But there's still nothing about π.</p><p>Wait a minute, why exactly 1/299,792,458? Why not, for example, 1/300? Where did this number come from in the first place? It seems we need to delve into the history of the unit of length itself to understand this better.</p><h2>A standard for every honest merchant</h2><p>In the past, people didn't bother much with standards: they only cared about what was convenient for measurement. For example, why not measure length in human cubits? It might not be precise, but it was cheap, reliable, and practical. And the fact that everyone's cubits were of different lengths? Sometimes that was even useful. If you needed to buy more cloth, you'd call the tallest person in the village and have them measure the fabric with their cubits.</p><p>Later on, of course, people began thinking about standardization. They started creating various standards. But this turned out to be inconvenient and cumbersome: you couldn't always run to a single standard for measurement. So, copies of the standards began to appear. And then copies of the copies...</p><p>Serious people decided that such chaos was hindering serious business, so they set a goal: to come up with a definition of a unit of length that wouldn't depend on any arbitrary standards. It should only depend on natural constants, so that anyone with some basic tools could reproduce and measure it.</p><h2>Bright dreams of standardization and insidious gravity</h2><p>A "standard-free" definition for the meter was actually proposed back in the 17th century. The Dutch mechanic, physicist, mathematician, astronomer, and inventor Christiaan Huygens suggested using a simple pendulum for this purpose. You take a small object and suspend it on a string. The length of the string should be such that the pendulum completes a full oscillation (returns to its original position) in exactly two seconds. This length of string was called the "universal measure" or the "Catholic meter." This length differed from the modern meter by about half a centimeter.</p><p>The proposal was well-received and adopted. However, problems soon arose. First, Huygens was dealing with what he called a "mathematical pendulum." This is a "material point suspended on a weightless, inextensible string." A material point and a weightless string are hardly the simple tools that every merchant would have on hand.</p><p>Second, it was quickly discovered that the length of the pendulum's string varied in different parts of the Earth. Gravity cunningly decreased as one approached the equator and did not cooperate with humanity's bright dream of standardization.</p><h2>An astonishing equation</h2><p>But let’s return to our mysterious equation. To find the period of small oscillations of a mathematical pendulum as a function of the length of the suspension, the following formula is used:</p><figure><img src="https://nkjhvudpdnbuifryqtzj.supabase.co/storage/v1/object/public/pictures/public/e6232f8b-513a-46c4-a056-a0537b26f421"><figcaption></figcaption></figure><p>And here it is—our π! Let's substitute the parameters of Huygens’ pendulum into this formula. The length of the string l in Huygens' pendulum equals 1. The T - oscillation equals 2. Plugging these values into the formula, we get π²=g.</p><p>So, have we found the answer to our question? Well, not quite. We already saw that the equality is only approximate. It doesn’t feel right to equate 9.87 and 9.81 exactly. Does this mean that the meter has changed since then?</p><h2>With revolutionary greetings from France</h2><p>Yes, indeed, it did change! This occurred during the reform of the units of measurement initiated by the French Academy of Sciences in 1791. Intelligent people suggested maintaining the definition of the meter through the pendulum, but with the clarification that it should specifically be a French pendulum—at the latitude of 45° N (approximately between Bordeaux and Grenoble).</p><p>However, this did not sit well with the commission in charge of the reform. The problem was that the head of the commission, Jean-Charles de Borda, was a fervent supporter of transitioning to a new (revolutionary) system of angle measurement—using grads (a grad being one-hundredth of a right angle). Each grad was divided into 100 minutes, and each minute into 100 seconds. The method of the seconds pendulum did not fit into this neat concept.</p><h2>The true and final meter</h2><p>In the end, they successfully got rid of the seconds and defined the meter as one forty-millionth of the Paris meridian. Or, alternatively, as one ten-millionth of the distance from the North Pole to the equator along the surface of the Earth’s ellipsoid at the longitude of Paris. This measurement slightly differed from the "pendulum" meter. The commission, without false modesty, dubbed the resulting value as the "true and final meter."</p><p>The idea of a universal standard accessible to everyone waved goodbye and faded into the sunset. Need an accurate standard for the meter? No problem! All you have to do is measure the length of a meridian and divide it by a few million. By the way, the French actually did this—they physically measured a portion of the Paris meridian, the arc from Dunkirk to Barcelona. They laid out a chain of 115 triangles across France and part of Spain. Based on these measurements, they created a brass standard. Incidentally, they made a mistake—they didn't account for the Earth's polar flattening.</p><h2><strong>Conclusion</strong></h2><p>Let's return to our equation once again. Now we know where the inaccuracy comes from: π² and g differ by about 0.06. If it weren't for yet another attempt to reform and improve everything, we would now have a slightly different value for the meter and the elegant equation π² = g. Later, scientists did return to defining the meter through unchanging and reproducible natural constants, but the meter standard was no longer the same.</p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser to start using Swift language this fall (176 pts)]]></title>
            <link>https://twitter.com/awesomekling/status/1822236888188498031</link>
            <guid>41208836</guid>
            <pubDate>Sat, 10 Aug 2024 11:52:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/awesomekling/status/1822236888188498031">https://twitter.com/awesomekling/status/1822236888188498031</a>, See on <a href="https://news.ycombinator.com/item?id=41208836">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (150 pts)]]></title>
            <link>https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</link>
            <guid>41207446</guid>
            <pubDate>Sat, 10 Aug 2024 05:07:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/">https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</a>, See on <a href="https://news.ycombinator.com/item?id=41207446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (648 pts)]]></title>
            <link>https://twitter.com/sundarpichai/status/1822132667959386588</link>
            <guid>41207415</guid>
            <pubDate>Sat, 10 Aug 2024 04:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sundarpichai/status/1822132667959386588">https://twitter.com/sundarpichai/status/1822132667959386588</a>, See on <a href="https://news.ycombinator.com/item?id=41207415">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Defcon stiffs badge HW vendor, drags FW author offstage during talk (471 pts)]]></title>
            <link>https://twitter.com/mightymogomra/status/1822119942281650278</link>
            <guid>41207221</guid>
            <pubDate>Sat, 10 Aug 2024 03:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mightymogomra/status/1822119942281650278">https://twitter.com/mightymogomra/status/1822119942281650278</a>, See on <a href="https://news.ycombinator.com/item?id=41207221">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Caltech Develops First Noninvasive Method to Continually Measure Blood Pressure (195 pts)]]></title>
            <link>https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</link>
            <guid>41207182</guid>
            <pubDate>Sat, 10 Aug 2024 03:53:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure">https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</a>, See on <a href="https://news.ycombinator.com/item?id=41207182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block-key="qqzwe">Solving a decades-old problem, a multidisciplinary team of Caltech researchers has figured out a method to noninvasively and continually measure blood pressure anywhere on the body with next to no disruption to the patient. A device based on the new technique holds the promise to enable better vital-sign monitoring at home, in hospitals, and possibly even in remote locations where resources are limited.</p><p data-block-key="dk5fr">The new patented technique, called resonance sonomanometry, uses sound waves to gently stimulate resonance in an artery and then uses ultrasound imaging to measure the artery's resonance frequency, arriving at a true measurement of blood pressure. In a small clinical study, the device, which gives patients a gentle buzzing sensation on the skin, produced results akin to those obtained using the standard-of-care blood pressure cuff.</p><p data-block-key="970vo">"We ended up with a device that is able to measure the absolute blood pressure—not only the systolic and diastolic numbers that we are used to getting from blood pressure cuffs—but the full waveform," says <a href="https://www.eas.caltech.edu/people/yaser">Yaser Abu-Mostafa</a> (PhD '83), professor of electrical engineering and computer science and one of the authors of a <a href="https://academic.oup.com/pnasnexus/article-lookup/doi/10.1093/pnasnexus/pgae252">new paper</a> describing the technique and device in the journal <i>PNAS Nexus</i>. "With this device you can measure blood pressure continuously and in different sites on the body, giving you much more information about the blood pressure of a person."</p><p data-block-key="2mnhc">"This team has been working for almost a decade, trying to build something that makes a difference, that is good enough to solve a real clinical problem," says Aditya Rajagopal (BS '08, PhD '14), visiting associate in electrical engineering at Caltech, research adjunct assistant professor of biomedical engineering at USC, and a co-author of the new paper. "Many groups, including tech giants like Apple and Google, have been working toward a solution like this, because it enables a spectrum of patient-monitoring possibilities from the hospital to the home. Our method broadens access to hospital-grade monitoring of blood pressure and cardiac health metrics."</p><p data-block-key="dj1o6"><b>Blood pressure 101</b></p><p data-block-key="7dki3">Blood pressure is simply the force of blood pushing on the walls of the body's blood vessels as it gets pumped around the body. High blood pressure, or hypertension, is related to risk of heart attack, stroke, chronic kidney disease, and other health problems. Low blood pressure, or hypotension, can also be a serious problem because it means the blood is not carrying enough oxygen to the organs. Taking regular measurements of blood pressure is considered one of the best ways to monitor overall health and to identify potential problems.</p><p data-block-key="62t65">Most of us have experienced the cuff-style measurement of blood pressure. A nurse, doctor, or machine inflates a cuff that fits around the upper arm until blood can no longer flow, and then slowly releases the air from the cuff while listening for the sound that blood makes as it once again begins to flow. The pressure in the cuff at that point corresponds to the blood pressure in the patient's arteries. But this technique has limitations: It can only be performed periodically, as it involves occluding a blood vessel, and can only collect data from the arm.</p><p data-block-key="a2ou5">Physicians would very much like to have continuous readings that provide full waveforms of a patient's blood pressure, and not only peripheral measurements from an arm but also central measurements from the chest and other parts of the body. To get the full information they need, intensive care physicians and surgeons sometimes resort to inserting a catheter directly into the artery of critical patients (a practice known as placing an arterial line, or "a-line"). This is invasive and can be risky, but, until now, it has been the only way to get a continuous readout of true blood pressure. In some cases, such as problems with heart valves, full blood-pressure waveforms can provide physicians with diagnostic information that they cannot get any other way.</p><p data-block-key="6gtjj">"There's a lot of information in that waveform that is really valuable," says Alaina Brinley Rajagopal, a visiting associate in electrical engineering at Caltech, an emergency medicine physician, and a co-author of the paper. And other blood pressure devices developed over the last decade or two require a calibration step that emergency physicians simply do not have time for, she says. "I need to be able to put something on a patient and have it work immediately."</p><p data-block-key="camvc">The new device fits the bill. The current prototype, built and tested by a spin-off company called Esperto Medical, is housed in a transducer case smaller than a deck of cards and is mounted on an armband, though the researchers say it could eventually fit within a package the size of a watch or adhesive patch. The team aims for the device to first be used in hospitals, where it would connect via wire to existing hospital monitors. It could mean that doctors would no longer have to weigh the risks of placing an a-line in order to get the continuous monitoring of real blood pressure for any patient.</p><p data-block-key="fqe92">Eventually, Brinley says their device could replace blood pressure cuffs as well. "Blood pressure cuffs only take one measurement as often as you run the cuff, so if you're asking patients to monitor their blood pressure at home, they have to know how to use the device, they have to put it on, and they have to be motivated to record the information, and I would say a majority of patients do not do that," says Brinley Rajagopal. "Having a device like ours, where it is just place and forget, you can wear it all day, and it can take however many measurements your provider wants, that would allow for better, precision dosing of medication."</p><p data-block-key="6c36e"><b>Developing a game changer</b></p><p data-block-key="eib58">Rajagopal recalls the long road it has been getting to this point with the blood pressure device. About a decade ago, Brinley Rajagopal returned from a global health trip particularly frustrated by the standard of care she could provide patients in remote locations. Talking with Rajagopal, the two wished they could invent something like a medical tricorder, a handheld device seen in <i>Star Trek</i> that helped the fictional doctors of the future scan patients, gather medical information, and diagnose. "That got us thinking about technologies we could adapt to get us closer to a goal like that," says Brinley Rajagopal. Those initial sci-fi–inspired discussions eventually led them down the path to try to develop a better blood pressure monitor.</p><p data-block-key="1cqst">But their first efforts did not pan out. After years of work on a possible solution using blood velocity to derive blood pressure, the team decided that they had reached a dead end. As with many other current blood pressure monitoring devices, that approach could only provide the <i>relative</i> blood pressure—the difference between the high and low measurements without the absolute number. It also required calibration.</p><p data-block-key="dlbkg"><b>Back to the drawing board</b></p><p data-block-key="8gfok">Rajagopal decided it was time to reevaluate and determine if they had any chance of solving this problem. "It was this moment of desperation that actually led to the key insight," says Rajagopal.</p><p data-block-key="1si80">Thinking back to his first-year physics course at Caltech, he began scribbling on a nearby wall. He remembered that his Physics1 textbook presented a canonical problem: You have a string under tension. How can you determine how taut the line is? If you tweeze the string, you can relate the velocity at which vibration waves travel back and forth on the string to the resonance frequency in the string, which could give you your answer. "I thought if I could stretch an artery in one direction and magically tweeze it and let it go, the ringing would give us the resonance frequency, which would get us to blood pressure," says Rajagopal. After six years of failures and returning to first principles, they finally had their guiding insight.</p><p data-block-key="dort6">And indeed, that is the underlying idea behind the new device: Like a guitar changing pitch as it is plucked while being tightened, the frequency at which an artery resonates when struck by sound waves changes depending on the pressure of the blood it contains.</p><p data-block-key="4803d">This resonance frequency can be measured with ultrasound, providing a measure of blood pressure. This measurement requires three parameters—a measurement of the artery's radius, the thickness of the artery's walls, and the tension or energy in the skin of the artery.</p><p data-block-key="8tptm">With the physics worked out, there were still a lot of other details to be resolved—identifying the sound waves that would make arteries resonate, understanding how to measure that resonance, and then determining how to efficiently map that back to blood pressure, and, significantly, how to build a working system.</p><p data-block-key="6pmif">"Building that system required some extraordinarily bespoke technologies," says Rajagopal. Caltech alumnus Raymond Jimenez (BS '13) was instrumental in building out that first system. "The art form, which involved a lot of other Caltech alumni, was to put the physics answer into a very simple, practical instrument."</p><p data-block-key="e1p0o">The resulting Esperto device is small, noninvasive, relatively inexpensive, and it has an automated method for locating the patient's blood vessel without needing to be physically repositioned. It also does not suffer from the problems that some blood pressure monitoring devices have, such as not being accurate for patients with low blood pressure or getting varying results depending on a patient's skin tone.</p><p data-block-key="ck0n8">It might not be a medical tricorder, but the team says the device solves the longstanding blood pressure monitoring problem. And Rajagopal says it is the product of a million small leaps. "Everything we've done is a product of the exact mistakes we've made over time," he says, "and all the work that others have done too."</p><p data-block-key="81uf3">"This work is emblematic of what makes Caltech so remarkable: solving a very hard problem by going back to first principles and understanding a physical phenomenon at the fundamental level," says Fred Farina, Caltech's Chief Innovation and Corporate Partnerships Officer. "This approach, combined with the tenacity and entrepreneurial drive of the team, is our homemade recipe for societal impact and improving people's lives."</p><p data-block-key="3vl0c">The paper describing the new technique is titled "Resonance sonomanometry for noninvasive, continuous monitoring of blood pressure." Additional authors on the paper include Raymond Jimenez (BS '13), Steven Dell, Austin C. Rutledge, Matt K. Fu (BS '13), and William P. Dempsey (PhD '12) of Esperto Medical, and Dominic Yurk (BS '17, PhD '23), a current member of Abu-Mostafa's group at Caltech. The work at Caltech was supported by Caltech trustee Charles Trimble (BS '63, MS '64), the Carver Mead Innovation Fund, and the Grubstake Fund.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a highly-available web service without a database (237 pts)]]></title>
            <link>https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</link>
            <guid>41206908</guid>
            <pubDate>Sat, 10 Aug 2024 02:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/">https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</a>, See on <a href="https://news.ycombinator.com/item?id=41206908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>If you’ve ever built a web service or a web app, you know the drill: pick a database, pick a web service framework (and in today’s day and age, pick a front-end framework, but let’s not get into that).</p>



<p>This has been the case for several decades now, and people don’t stop to question if this is still the best way to build a web app. Many things have changed in the last decade:</p>



<ul>
<li>Disk is a lot faster (NVMe)</li>



<li>Disk is a lot more robust (EBS/EFS etc.)</li>



<li>RAM is super cheap, for most startups you could probably fit all your data in RAM</li>



<li>You can rent a machine with hundreds of cores if your heart desires.</li>
</ul>



<p>This was not the case when I first worked at a Rails startup in 2010. But most importantly, there’s one new very important change that’s happened in the last decade:</p>



<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft Consensus algorithm</a> was published in 2014 with many robust implementations easily available.</li>
</ul>



<p>In this blog post, we’re going to break down a new architecture for web development. We use it successfully for <a href="https://screenshotbot.io/">Screenshotbot</a>, and we hope you’ll use it too.</p>



<p>I’ll break this blog post into three parts: Explore, Expand and Extract, obviously referencing <a href="https://tidyfirst.substack.com/">Kent Beck</a>‘s 3X. Your needs are going to vary in each of these stages of your startup, and I’m going to demonstrate how you use the architecture in all three phases.</p>



<h2>Explore</h2>



<p>So you’re a new startup. You’re iterating on a product, you have no idea how people are going to use it, or even <em>if</em> they’re going to use it.</p>



<p>For most startups today, this would mean you’ll pick Rails or Django or Node or some such, backed with a MySQL or PostgreSQL or MongoDB or some such.</p>



<p>“<em>Keep it simple silly</em>,” you say, and this seems simple enough.</p>



<p>But is this as simple as possibly can be? Could we make it simpler? What if the web service and the database instance were exactly one and the same? I’m not talking about using something like SQLite where your data is still serialized, I’m saying what if all the memory in your RAM <em>is</em> your database.</p>



<p>Imagine all the wonderful things you could build if you never had to serialize data into SQL queries. First, you don’t need multiple front-end servers talking to a single DB, just get a bigger server with more RAM and more CPU if you need it. What about indices? Well, you can use in-memory indices, effectively just hash-tables to lookup objects. You don’t need clever indices like B-tree that are optimized for disk latency. (In fact, you can use some indices that were probably not possible with traditional databases. <a href="https://blog.screenshotbot.io/2023/10/29/scaling-screenshotbot/">One such index</a> using functional collections was critical to the scalability of Screenshotbot.)</p>



<p>You also won’t need special architectures to reduce round-trips to your database.  In particular, you won’t need any of that Async-IO business, because your threads are no longer IO bound. Retrieving data is just a matter of reading RAM. Suddenly debugging code has become a lot easier too.</p>



<p>You don’t need any services to run background jobs, because background jobs are just threads running in this large process.</p>



<p>You don’t need crazy concurrency protocols, because most of your concurrency requirements can be satisfied with simple in-memory mutexes and condition variables.</p>



<p>But then comes the important part: how do you recover when your process crashes? It turns out that answer is easy, periodically just take a snapshot of everything in RAM.</p>



<p>Hold on, what if you’ve made changes since the last snapshot? And this is the clever bit: you ensure that every time you change parts of RAM, we write a transaction to disk. So if you have a line like <code>foo.setBar(2)</code>, this will first write a transaction that says we’ve changed the <code>bar</code> field of <code>foo</code> to 2, and then actually set the field to 2. An operation like <code>new Foo()</code> writes a transaction to disk to say that a Foo object was created, and then returns the new object.</p>



<p>And so, if your process crashes and restarts, it first reloads the snapshot, and replays the transaction logs to fully recover the state. (Notice that index changes don’t need to be part of the transaction log. For instance if there’s an index on field <code>bar</code> from <code>Foo</code>, then <code>setBar</code> should just update the index, which will get updated whether it’s read from a snapshot, or from a transaction.)</p>



<p>Finally, this architecture enables some new kind of code that couldn’t be written before. Since all requests are being served by the same process, which <em>usually</em> doesn’t get killed, it means you can store closures in memory that can be used to serve pages. For example on Screenshotbot, if you ever see a “<a href="https://screenshotbot.io/n/" rel="nofollow">https://screenshotbot.io/n/</a><em>nnnnnnn</em>” URL, it’s actually a closure on the server, where <em>nnnnnnn</em> maps to an internal closure. But amazingly, this simple change means we don’t need to serialize objects across page transitions. The closure has references to the objects, so we don’t need to pass around object-ids across every single request. In Javascript, this might hypothetically look like:</p>



<pre><code>function renderMyObject(obj) {
   return &lt;html&gt;...
            &lt;a href=(() =&gt; obj.delete()) &gt;Delete&lt;/a&gt;
            ...
          &lt;/html&gt;
} </code></pre>



<p>What this all means is that you can iterate quickly. If you have to debug, there’s exactly one service that you need to debug. If you need to profile code, there’s exactly one service you need to profile (no more MySQL slow query logs). There’s exactly one service to monitor: if that one service goes down the site certainly goes down, but since there’s only one service and one server, the probability of failure is also much lower. If the server dies, AWS will automatically bring up a new server to replace it within a few minutes.</p>



<p>It’s also a lot easier to write test code, since you no longer have to mock out databases.</p>



<h2>Expand</h2>



<p>So you’re moving fast, iterating, and building out ideas, and slowly getting customers along the way.</p>



<p>Then one day, you get a high-profile customer. Bingo, you’re now in the <em>Expand</em> phase of your startup.</p>



<p>But there’s a catch: this high-profile customer requires 99.999% availability. </p>



<p>Surely, the architecture we just described cannot handle this. If the server goes down, we would need to wait several minutes for AWS to bring it back up. Once it’s back up, we might wait several minutes for our process to even restore the snapshot from disk. Even re-deploys are tricky: restarting the service can bring down the server for multiple minutes.</p>



<p>And this is where the Raft Consensus Protocol comes in to place. </p>



<p>Raft is a wonderful algorithm and protocol. It takes your <em>finite state machine</em> (your web server/database), and essentially replicates the transaction log. So now, we can take our very simple architecture and replicate it across three machines. If the leader goes down, then a new leader is elected within seconds and will continue to serve requests.</p>



<p>We’ve just made our simple little service into a full-fledged highly-available database, without fundamentally changing how developers write code.</p>



<p>With this mechanism, you can also do a rolling deploy without ever bringing the server down. (Although we rarely restart our server processes, more on that in a moment.) Because there’s just one service, it’s also easy to calculate your availability guarantees.</p>



<h2>Extract</h2>



<p>So your startup is doing well, and you have thousands of large customers.</p>



<p>To be honest, Screenshotbot is not at this stage, I’ll talk about where we are in a moment. But we’re preparing for this possibility, with monitoring in place for predicted bottlenecks. </p>



<p>The solution here is something large companies already do with their databases: sharding. You can break up your web services into shards, each shard being its own cluster. In particular, at Screenshotbot we already do this: each of our enterprise customers get their own dedicated cluster. (Fun story: Meta <a href="https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/">switched to Raft</a> to handle replication for each of its MySQL clusters, so we’re essentially doing the same thing but without using a separate database.)</p>



<p>I don’t know what else to expect, since I’m more of a solve-today’s-problem kind of person. The main bottleneck I expect to see is scaling the commit-thread. The read threads parallelize beautifully. There’s one commit-thread that’s applying each transaction one at a time. It turns out the disk latency is irrelevant for this, since the Raft algorithm will just commit multiple transactions together to disk. My main concern is that the CPU cost for applying the transactions will exceed the single core performance. I highly doubt that I would ever see this, but it’s a possibility. At this point we could profile the cost of commits and improve it (for instance, move some of the work out of the transaction thread),  or we could just figure out sharding. I’ll probably write another blog post when that happens.</p>



<h2>Our Stack</h2>



<p>Now that I’ve described the idea to you, let me tell you about our stack, and why it turned out to be so suitable for this architecture.</p>



<p>We use Common Lisp. My initial implementation of Screenshotbot did use MySQL, but I quickly swapped it out for <a href="https://github.com/bknr-datastore/bknr-datastore">bknr.datastore</a> exactly because handling concurrency with MySQL was hard and Screenshotbot is a highly concurrent app. BKNR Datastore is a library that handles the architecture described in the <em>Explore</em> section, but built for Common Lisp. (There are similar libraries for other languages, but not a whole lot of them.)</p>



<p>Common Lisp is also heavily multi-threaded, and this is going to be crucial for this architecture since your web requests are being handled by threads in a single process. Ruby or Python would be disqualified by this requirement.</p>



<p>We also use the idea of closures that I mentioned earlier. But this means we can’t keep restarting the server frequently (if you restart the server, you lose the closures). So reloading code is just hot-reloading code in the running process. It turns out Common Lisp is excellent at this: so much so that a large part of the standard is all about handling reloading code. (For instance, if the class definition changes, how do you update objects of that class? <a href="http://clhs.lisp.se/Body/f_reinit.htm#reinitialize-instance">There’s a standard for it</a>.)</p>



<p>Occasionally, we do restart the servers. Currently, it looks like we only restart the servers about once every month or two months. When we need to do this, we just do a rolling restart with our Raft cluster. We use a cluster of 3 servers per installation, which allows for one server to go down. We don’t use Kubernetes, we don’t need it (at least, not yet).</p>



<p>For the Raft implementation, we wrote our own custom library built on top of bknr.datastore. We built and open-sourced <a href="https://github.com/tdrhq/bknr.cluster">bknr.cluster</a>, that under the hood uses the fantastic <a href="https://github.com/baidu/braft">Braft</a> library from Baidu. Braft is super solid, and I can highly recommend it. Braft also handles background snapshots, which means while we’re taking snapshots, the server can still continue serving requests.</p>



<p>To store image files, or blobs that shouldn’t be part of the datastore, we use EFS (a highly available NFS) that is shared between the three servers. EFS is easier to work with than S3, because we don’t have to handle error conditions. EFS also makes our code more testable, since we aren’t interacting with an external server, and just writing to disk.</p>



<p><strong>How well does this scale?</strong> We have a couple of big enterprise customers, but one especially well-known customer. Screenshotbot runs on their CI, so we get API requests 100s of times for every single commit and Pull Request. Despite this, we only need a 4-core 16GB machine to serve their requests. (And similar machines for the replicas, mostly running idle.) Even with this, the CPU usage maxes out at 20%, but even then most of that comes from image processing, so we have a lot of room to scale before we need to bump up the number of cores.</p>



<h2>Summary</h2>



<p>I think this architecture is excellent for new startups, and I’m hoping more companies will adopt it. Obviously, you’ll need to build out some of the tooling we’ve built out for the language of your choice. (Although, if you choose to use Common Lisp, it’s all here for you to use, and all open-source.)</p>



<p>We’re super grateful to the folk behind bknr.datastore, Braft and Raft, because without their work we wouldn’t be able to do any of this.</p>



<p>If you think this was useful or interesting, I would appreciate it if you could share it on social media. Please reach out to me at <a href="mailto:arnold@screenshotbot.io">arnold@screenshotbot.io</a> if you have any questions.</p>
	</div></div>]]></description>
        </item>
    </channel>
</rss>