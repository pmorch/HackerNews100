<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 21 Oct 2024 09:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Software engineer titles have almost lost all their meaning (119 pts)]]></title>
            <link>https://www.trevorlasn.com/blog/software-engineer-titles-have-almost-lost-all-their-meaning</link>
            <guid>41900456</guid>
            <pubDate>Mon, 21 Oct 2024 03:33:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trevorlasn.com/blog/software-engineer-titles-have-almost-lost-all-their-meaning">https://www.trevorlasn.com/blog/software-engineer-titles-have-almost-lost-all-their-meaning</a>, See on <a href="https://news.ycombinator.com/item?id=41900456">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <p>Remember when being a “Senior Software Engineer” actually meant something? I do, and I can’t help but feel nostalgic for that clarity. In recent years, our industry has witnessed rampant title inflation, turning what used to be a clear-cut junior-mid-senior progression into a confusing parade of inflated roles.</p>
<p>The “senior” title, once a badge of substantial experience and expertise, has been particularly devalued. Today, developers are being crowned “senior” faster than ever, often with just three to four years under their belts. It’s as if the path to seniority, once a marathon of skill-building and diverse experiences, has turned into a sprinter’s dash.</p>
<p>This explosion of grandiose titles isn’t just confusing—it’s eroding the meaning of career milestones in tech. Each new title tries to outdo the last in impressiveness, while paradoxically meaning less and less. For everyone involved—from job seekers to hiring managers—this inflation has muddied the waters of professional progression and recognition.</p>
<h3 id="what-is-a-senior-engineer-anyway">What is a “Senior” Engineer Anyway?</h3>
<p>Being a senior engineer meant far more than just logging years on the job. It was a title earned through a diverse set of experiences and challenges that shaped not just their technical skills, but their entire approach to software development.</p>
<p>A true senior engineer is a battle-tested problem solver. They’ve faced and conquered complex technical challenges across multiple projects, dealing with more than just tricky bugs. These are the architects who’ve untangled system-wide issues that require deep understanding and creative solutions. They’re the ones who can navigate and refactor sprawling legacy codebases with confidence, understanding the delicate balance between maintaining existing systems and building new ones.</p>
<p>Senior engineers have been through the crucible of major production outages. They’ve felt the heat of a system melting down in real-time and learned to stay calm under pressure. These experiences have taught them to diagnose issues rapidly and lead a team through a crisis, making critical decisions when every second counts.</p>
<p>But technical skills alone don’t make a senior engineer. They’re also architectural visionaries who can see beyond immediate tasks to design scalable, maintainable systems. Their decisions positively impact projects years down the line, showcasing a level of foresight that only comes with extensive experience. They’ve developed the soft skills to be effective mentors and leaders, guiding junior developers not just in coding, but in navigating the complex landscape of software development.</p>
<p>Perhaps most importantly, senior engineers remain humble and curious despite their experience. They’re continuous learners, adapting to new technologies and methodologies, always expanding their toolkit. They’ve developed a strong sense of professional ethics, understanding the broader implications of their work and advocating for responsible development practices.</p>
<p>This depth of experience isn’t typically gained in just a few years. It’s forged through diverse projects, multiple tech stacks, and yes, a fair share of failures and lessons learned along the way.</p>
<h3 id="the-root-cause-of-title-inflation">The Root Cause Of Title Inflation</h3>
<p>The fierce competition for talent has led companies, especially startups, to use titles as a retention tactic. Unable to always match the salaries offered by tech giants, these companies resort to inflating titles as a form of non-monetary compensation. While this might seem like a clever short-term solution, it’s creating long-term problems for the industry by diluting the meaning of these titles.</p>
<p>The rise of professional networking platforms like LinkedIn has exacerbated this issue. These platforms have turned titles into personal branding tools, creating immense pressure for individuals to sport impressive-sounding roles. This “LinkedIn Effect” has everyone, from fresh graduates to seasoned professionals, yearning for titles that look good on their profiles, often prioritizing appearance over substance.</p>
<p>HR departments, grappling with the increasing complexity of tech roles, have contributed to this problem as well. In an attempt to accurately categorize the myriad of specialized positions in our rapidly evolving field, they’ve created a proliferation of niche titles. While these titles might be descriptive, they’ve made it increasingly difficult to compare roles across companies, further muddying the waters of career progression.</p>
<p>Lastly, many companies have begun using title promotions as a retention strategy. The intent is to recognize and retain valuable employees, but this approach often backfires. When titles are handed out like participation trophies, they cease to align with actual growth in responsibilities or skills. This misalignment not only devalues the titles themselves but also sets unrealistic expectations for the newly promoted employees.</p>
<p>In essence, what we’re seeing is a perfect storm of market pressures, personal branding needs, organizational challenges, and short-sighted retention strategies. Together, these factors have inflated titles to the point where they risk losing their meaning entirely.</p>
<h3 id="why-do-we-need-to-address-title-inflation">Why Do We Need to Address Title Inflation?</h3>
<p>Title inflation isn’t just about words on a business card or a LinkedIn profile. It’s a problem that strikes at the heart of our industry’s integrity and functionality. When we inflate titles, we’re essentially lying to ourselves and each other about our capabilities and experience.</p>
<p>This deception has real consequences. It creates a mismatch between expectations and reality, leading to situations where people are placed in roles they’re not prepared for. Imagine a “senior” engineer with three years of experience trying to architect a complex system or mentor junior developers. The potential for failure is high, and the stress on that individual is immense.</p>
<h3 id="what-can-we-do-about-title-inflation">What Can We Do About Title Inflation?</h3>
<p>For those in leadership positions, it’s paramount to resist the temptation of using inflated titles as a quick fix for retention or recruitment challenges. Instead, focus on creating meaningful career progression frameworks that tie advancements to concrete skills and responsibilities. Consider implementing a system similar to those used by larger tech companies, where levels (like L3, L4, L5) provide a more nuanced view of seniority without resorting to title inflation.</p>
<p>Companies can take a stand by standardizing their title structures and being transparent about what each level means. This could involve creating detailed job descriptions that clearly outline the expectations and responsibilities for each role. By doing so, you not only provide clarity for your employees but also contribute to a more standardized industry-wide understanding of titles.</p>
<p>HR departments have a critical role to play. They can work on developing more sophisticated ways to categorize and compare roles across the industry. This might involve collaborating with tech leads to create standardized skill matrices that can be used to evaluate candidates and employees more objectively.</p>
<p>Companies that resist title inflation gain a significant competitive edge. By maintaining meaningful titles, they attract and retain top talent who value authentic growth over inflated roles. This leads to more accurate hiring, improved team dynamics, and enhanced productivity. Realistic titles also foster trust, both internally and with clients, positioning the company as a beacon of integrity in the industry. Ultimately, companies with well-defined, honest title structures build stronger, more capable teams and a reputation for excellence that sets them apart in the market.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A step toward fully 3D-printed active electronics (101 pts)]]></title>
            <link>https://news.mit.edu/2024/mit-team-takes-major-step-toward-fully-3d-printed-active-electronics-1015</link>
            <guid>41899873</guid>
            <pubDate>Mon, 21 Oct 2024 01:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/mit-team-takes-major-step-toward-fully-3d-printed-active-electronics-1015">https://news.mit.edu/2024/mit-team-takes-major-step-toward-fully-3d-printed-active-electronics-1015</a>, See on <a href="https://news.ycombinator.com/item?id=41899873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>Active electronics — components that can control electrical signals — usually contain semiconductor devices that receive, store, and process information. These components, which must be made in a clean room, require advanced fabrication technology that is not widely available outside a few specialized manufacturing centers.</p><p>During the Covid-19 pandemic, the lack of widespread semiconductor fabrication facilities was one cause of a worldwide electronics shortage, which drove up costs for consumers and had implications in everything from&nbsp;<a href="https://news.mit.edu/2022/us-leadership-microelectronics-semiconductors-0119" target="_blank">economic growth to national defense</a>. The ability to 3D print an entire, active electronic device without the need for semiconductors could bring electronics fabrication to businesses, labs, and homes across the globe.</p><p>While this idea is still far off, MIT researchers have taken an important step in that direction by demonstrating fully 3D-printed resettable fuses, which are key components of active electronics that usually require semiconductors.</p><p>The researchers’ semiconductor-free devices, which they produced using standard 3D printing hardware and an inexpensive, biodegradable material, can perform the same switching functions as the semiconductor-based transistors used for processing operations in active electronics.</p><p>Although still far from achieving the performance of semiconductor transistors, the 3D-printed devices could be used for basic control operations like regulating the speed of an electric motor.</p><p>“This technology has real legs. While we cannot compete with silicon as a semiconductor, our idea is not to necessarily replace what is existing, but to push 3D printing technology into uncharted territory. In a nutshell, this is really about democratizing technology. This could allow anyone to create smart hardware far from traditional manufacturing centers,” says Luis Fernando Velásquez-García, a principal research scientist in MIT’s Microsystems Technology Laboratories (MTL) and senior author of a <a href="https://www.tandfonline.com/doi/full/10.1080/17452759.2024.2404157" target="_blank">paper describing the devices</a>, which appears in <em>Virtual and Physical Prototyping.</em></p><p>He is joined on the paper by lead author Jorge Cañada, an electrical engineering and computer science graduate student.</p><p><strong>An unexpected project</strong></p><p>Semiconductors, including silicon, are materials with electrical properties that can be tailored by adding certain impurities. A silicon device can have conductive and insulating regions, depending on how it is engineered. These properties make silicon ideal for producing transistors, which are a basic building block of modern electronics.</p><p>However, the researchers didn’t set out to 3D-print semiconductor-free devices that could behave like silicon-based transistors.</p><p>This project grew out of another in which they were fabricating magnetic coils using extrusion printing, a process where the printer melts filament and squirts material through a nozzle, fabricating an object layer-by-layer.</p><p>They saw an interesting phenomenon in the material they were using, a polymer filament doped with copper nanoparticles.</p><p>If they passed a large amount of electric current into the material, it would exhibit a huge spike in resistance but would return to its original level shortly after the current flow stopped.</p><p>This property enables engineers to make transistors that can operate as switches, something that is typically only associated with silicon and other semiconductors. Transistors, which switch on and off to process binary data, are used to form logic gates which perform computation.</p><p>“We saw that this was something that could help take 3D printing hardware to the next level. It offers a clear way to provide some degree of ‘smart’ to an electronic device,” Velásquez-García says.</p><p>The researchers tried to replicate the same phenomenon with other 3D printing filaments, testing polymers doped with carbon, carbon nanotubes, and graphene. In the end, they could not find another printable material that could function as a resettable fuse.</p><p>They hypothesize that the copper particles in the material spread out when it is heated by the electric current, which causes a spike in resistance that comes back down when the material cools and the copper particles move closer together. They also think the polymer base of the material changes from crystalline to amorphous when heated, then returns to crystalline when cooled down — a phenomenon known as the polymeric positive temperature coefficient.</p><p>“For now, that is our best explanation, but that is not the full answer because that doesn’t explain why it only happened in this combination of materials. We need to do more research, but there is no doubt that this phenomenon is real,” he says.</p><p><strong>3D-printing active electronics</strong></p><p>The team leveraged the phenomenon to print switches in a single step that could be used to form semiconductor-free logic gates.</p><p>The devices are made from thin, 3D-printed traces of the copper-doped polymer. They contain intersecting conductive regions that enable the researchers to regulate the resistance by controlling the voltage fed into the switch.</p><p>While the devices did not perform as well as silicon-based transistors, they could be used for simpler control and processing functions, such as turning a motor on and off. Their experiments showed that, even after 4,000 cycles of switching, the devices showed no signs of deterioration.</p><p>But there are limits to how small the researchers can make the switches, based on the physics of extrusion printing and the properties of the material. They could print devices that were a few hundred microns, but transistors in state-of-the-art electronics are only few nanometers in diameter.</p><p>“The reality is that there are many engineering situations that don’t require the best chips. At the end of the day, all you care about is whether your device can do the task. This technology is able to satisfy a constraint like that,” he says.</p><p>However, unlike semiconductor fabrication, their technique uses a biodegradable material and the process uses less energy and produces less waste. The polymer filament could also be doped with other materials, like magnetic microparticles that could enable additional functionalities.</p><p>In the future, the researchers want to use this technology to print fully functional electronics. They are striving to fabricate a working magnetic motor using only extrusion 3D printing. They also want to finetune the process so they could build more complex circuits and see how far they can push the performance of these devices.</p><p>“This paper demonstrates that active electronic devices can be made using extruded polymeric conductive materials. This technology enables electronics to be built into 3D printed structures. An intriguing application is on-demand 3D printing of mechatronics on board spacecraft,” says Roger Howe, the William E. Ayer Professor of Engineering, Emeritus, at Stanford University, who was not involved with this work.</p><p>This work is funded, in part, by Empiriko Corporation.</p>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Today is Ubuntu's 20th Anniversary (211 pts)]]></title>
            <link>https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000003.html</link>
            <guid>41898736</guid>
            <pubDate>Sun, 20 Oct 2024 21:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000003.html">https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000003.html</a>, See on <a href="https://news.ycombinator.com/item?id=41898736">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
    <b>Mark Shuttleworth</b> 
    <a href="mailto:ubuntu-announce%40lists.ubuntu.com?Subject=Announcing%20Ubuntu%204.10%20%22The%20Warty%20Warthog%20Release%22&amp;In-Reply-To=" title="Announcing Ubuntu 4.10 &quot;The Warty Warthog Release&quot;">mark at hbd.com
       </a><br>
    <i>Wed Oct 20 11:06:23 CDT 2004</i>
    <ul>
        <li>Previous message: <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000002.html">Announcing Ubuntu 4.10 (Release Candidate)
</a></li>
        <li>Next message: <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000004.html">Warty Live CD Released
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/date.html#3">[ date ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/thread.html#3">[ thread ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/subject.html#3">[ subject ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/author.html#3">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--beginarticle-->
<pre>=== Announcing Ubuntu 4.10 "The Warty Warthog Release" ===

The warm-hearted Warthogs of the Warty Team are proud to present the
very first release of Ubuntu!

Ubuntu is a new Linux distribution that brings together the extraordinary
breadth of Debian with a fast and easy install, regular releases (every
six months), a tight selection of excellent packages installed by default
and a commitment to security updates with 18 months of security and
technical support for every release.

You get a distribution that is:

  * absolutely committed to free software,  every end-user application 
on the
    CD is free software

  * 100% free of charge, and the Ubuntu team is committed to keeping
    Ubuntu free of charge

  * security updates for the distribution at no charge for 18 months
    for any release

  * updated to the latest desktop and kernel and infrastructure every
    six months with a new release

  * supports x86, amd64 and ppc processors, with additional ports under
    way

If you've heard all about Ubuntu and just want to get the install CD or
test the Release Candidate Live CD, you can download it here immediately:

  <a href="http://www.ubuntulinux.org/download/">http://www.ubuntulinux.org/download/</a>

If you want a shrinkwrapped CD we will gladly ship it to you at no cost.
To receive a complimentary copy of the Warty Warthog CD -- or a handful
to give to your friends, your school or LUG, register online at:

  <a href="http://shipit.ubuntulinux.org/">http://shipit.ubuntulinux.org/</a>

For more information, you can turn to any of the following resources:

Ubuntu Website: <a href="http://www.ubuntulinux.org/">http://www.ubuntulinux.org</a>

  The website contains some basic background on Ubuntu, an
  overview of the project, information on how to get it, and
  some documentation for the software.

Ubuntu Wiki: <a href="http://wiki.ubuntulinux.org/">http://wiki.ubuntulinux.org</a>

  The wiki is a shared web space used by the Ubuntu community to
  develop new ideas for Ubuntu. Anybody is welcome to edit and
  add to the wiki.

Ubuntu IRC Channel: #ubuntu and on irc.freenode.net

  The Ubuntu IRC channel is your best place to start for help and
  discussion about Ubuntu and the Warty Warthog release. We aim
  to keep the signal-to-noise ratio as high as possible on that
  channel, and on all community forums.

Ubuntu Mailing Lists:

  Ubuntu mailing lists are the heart of our community. In addition to the
  announcement list, and lists for users and developers of Ubuntu,
  there are now Ubuntu mailing lists in German, French, Spanish as well
  as lists devoted to Ubuntu security, news, translators, and the
  inevitable lighthearted chitchat list ("the Sounder"). To get more
  information or subscribe, visit:

    <a href="http://lists.ubuntu.com/">http://lists.ubuntu.com</a>


Warty Warthog Features

 * Simple and fast Installation

   Ubuntu comes on one single CD, with thousands of extra packages
   available online. The install is optimised for speed and
   simplicity. Ubuntu has excellent support for laptops (both
   x86 based and Powerbook / iBook PPC based), and can also be
   setup in a minimalist server configuration.

 * GNOME 2.8

   Ubuntu was the first distribution to ship Gnome 2.8, on the day
   of the 2.8 release. Ubuntu is a great way to try out Gnome 2.8 if
   you have not already experienced its speed and simplicity.

 * Firefox 0.9 (with security patches)

 * First class productivity software

   Evolution 2.0 and OpenOffice.org 1.1.2

 * XFree86 4.3 with improved hardware support

   We also worked hard to detect as much hardware as possible,
   simplifying the X install considerably.

Warty can be installed in a minimalist mode for servers, or in full
desktop mode. It works well on laptops and desktops. Warty is secure
by design - a key goal was to ensure that Warty was as safe from attack
over the internet as possible after a default install.

Thanks to the team of professional and volunteer maintainers who have
worked so hard to bring the Warthog to life, and also to our rapidly
growing community, who have provided excellent testing and ideas for
the future of Ubuntu!

"Ubuntu" is an ancient African word for "humanity towards others", and
we think it's a perfect name for an open source community project. In
that spirit we invite you to join, to contribute and to share Ubuntu
with your own community. Our next release, the Hoary Hedgehog, is due
in six months time. You can help to shape it by joining the team
and contributing your own expertise. See you at #ubuntu on
irc.freenode.net.


</pre>


<!--endarticle-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message: <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000002.html">Announcing Ubuntu 4.10 (Release Candidate)
</a></li>
	<li>Next message: <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/000004.html">Warty Live CD Released
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/date.html#3">[ date ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/thread.html#3">[ thread ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/subject.html#3">[ subject ]</a>
              <a href="https://lists.ubuntu.com/archives/ubuntu-announce/2004-October/author.html#3">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="http://lists.ubuntu.com/mailman/listinfo/ubuntu-announce">More information about the ubuntu-announce
mailing list</a><br>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft said it lost weeks of security logs for its customers' cloud products (142 pts)]]></title>
            <link>https://techcrunch.com/2024/10/17/microsoft-said-it-lost-weeks-of-security-logs-for-its-customers-cloud-products/</link>
            <guid>41898723</guid>
            <pubDate>Sun, 20 Oct 2024 21:42:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/10/17/microsoft-said-it-lost-weeks-of-security-logs-for-its-customers-cloud-products/">https://techcrunch.com/2024/10/17/microsoft-said-it-lost-weeks-of-security-logs-for-its-customers-cloud-products/</a>, See on <a href="https://news.ycombinator.com/item?id=41898723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Microsoft has notified customers that it’s missing more than two weeks of security logs for some of its cloud products, leaving network defenders without critical data for detecting possible intrusions.</p>

<p>According to a notification sent to affected customers, Microsoft said that “a bug in one of Microsoft’s internal monitoring agents resulted in a malfunction in some of the agents when uploading log data to our internal logging platform” between September 2 and September 19.&nbsp;</p>







<p>The notification said that the logging outage was not caused by a security incident, and “only affected the collection of log events.”&nbsp;</p>

<p>Business Insider <a href="https://www.businessinsider.com/microsoft-tells-customers-it-lost-log-data-key-security-products-2024-10" target="_blank" rel="noreferrer noopener nofollow">first reported</a> the loss of log data earlier in October. Details of the notification have not been widely reported. As noted by <a href="https://cyberplace.social/@GossiTheDog/113313392062371141" target="_blank" rel="noreferrer noopener nofollow">security researcher Kevin Beaumont</a>, the notifications that Microsoft sent to affected companies are likely accessible only to a handful of users with tenant admin rights.</p>

<p>Logging helps to keep track of events within a product, such as information about users signing in and failed attempts, which can help network defenders identify suspected intrusions. Missing logs could make it more difficult to identify unauthorized access to the customers’ networks during that two-week window.&nbsp;</p>

<p>The affected products include Microsoft Entra, Sentinel, Defender for Cloud, and Purview, according to the Business Insider report. Affected customers “may have experienced potential gaps in security related logs or events, possibly affecting customers’ ability to analyze data, detect threats, or generate security alerts,” the notification said.</p>

<p>Microsoft would not answer specific questions about the logging outage, but a Microsoft executive confirmed to TechCrunch that the incident was caused by an “operational bug within our internal monitoring agent.”</p>


<p>“We have mitigated the issue by rolling back a service change. We have communicated to all impacted customers and will provide support as needed,” said John Sheehan, a Microsoft corporate vice president.</p>

<p>The logging outage comes a year after Microsoft <a href="https://techcrunch.com/2023/07/17/microsoft-lost-keys-government-hacked/">came under fire from federal investigators</a> for withholding security logs from certain U.S. federal government departments that host their emails on the company’s hardened, government-only cloud; investigators said having access to those logs could have identified a series of China-backed intrusions far sooner.</p>

<p>The China-backed intruders, referred to as Storm-0558, broke into Microsoft’s network and stole a digital skeleton key that allowed the hackers unfettered access to U.S. government emails stored in Microsoft’s cloud. According to a <a href="https://techcrunch.com/2023/08/11/cyber-security-review-board-microsoft-hack-government-emails/">government-issued postmortem of the cyberattack</a>, the State Department identified the intrusions because it paid for a higher-tier Microsoft license that granted access to security logs for its cloud products, which many other hacked U.S. government agencies did not have.</p>







<p>Following the China-backed hacks, Microsoft said <a href="https://www.wsj.com/articles/microsoft-to-offer-some-cybersecurity-tools-free-after-suspected-china-hack-6db94221" target="_blank" rel="noreferrer noopener nofollow">it would start providing logs</a> to its lower-paid cloud accounts from September 2023.</p>

<p><em>Carly Page contributed reporting.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Create mind maps to learn new things using AI (138 pts)]]></title>
            <link>https://github.com/aotakeda/learn-thing</link>
            <guid>41898076</guid>
            <pubDate>Sun, 20 Oct 2024 20:01:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aotakeda/learn-thing">https://github.com/aotakeda/learn-thing</a>, See on <a href="https://news.ycombinator.com/item?id=41898076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Mind Map Visualization Project</h2><a id="user-content-mind-map-visualization-project" aria-label="Permalink: Mind Map Visualization Project" href="#mind-map-visualization-project"></a></p>
<p dir="auto">This is a simple <a href="https://nextjs.org/" rel="nofollow">Next.js</a> project that implements a mind map visualization tool using <a href="https://reactflow.dev/" rel="nofollow">React Flow</a>.</p>
<p dir="auto">Watch a demo of it in action <a href="https://www.youtube.com/watch?v=Y-9He-tG3aM" rel="nofollow">here</a> or check out the gif below.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aotakeda/learn-thing/blob/main/public/demo.gif"><img src="https://github.com/aotakeda/learn-thing/raw/main/public/demo.gif" alt="Demo gif" data-animated-image=""></a></p>
<p dir="auto">The UI is built using <a href="https://ui.shadcn.com/" rel="nofollow">shadcn</a> and some components from <a href="https://magicui.design/" rel="nofollow">Magic UI</a>.</p>
<p dir="auto">It allows users to view and interact with mind maps, and download the mind map data as a markdown file.</p>
<p dir="auto">The mind map data is generated using either local models from <a href="https://ollama.com/" rel="nofollow">Ollama</a> or external models like <a href="https://openai.com/" rel="nofollow">OpenAI</a> and leveraging <a href="https://sdk.vercel.ai/docs/introduction" rel="nofollow">AI SDK</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Interactive mind map visualization</li>
<li>Node details view in a side sheet</li>
<li>Markdown export functionality</li>
<li>Save mind map data to a local JSON file</li>
<li>Switch between local and external models</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Install all dependencies:</p>

<p dir="auto">Copy the <code>.env.template</code> file to <code>.env.local</code> and specify which model (local or external) you want to use by setting the <code>NEXT_PUBLIC_USE_LOCAL_MODELS</code> environment variable to <code>true</code> or <code>false</code>.</p>
<p dir="auto">When running an OpenAI model, you must specify your OpenAI API key in the <code>.env.local</code> file.</p>
<p dir="auto">Inside the <code>route.ts</code> file, you must specify the model you are running using Ollama, by default it will use the <code>llama3.1</code> model for local models and for external models it will use the <code>gpt-3.5-turbo</code> model.</p>
<p dir="auto">Bear in mind that external models tend to be much faster serving than local models.</p>
<p dir="auto">If you want to learn how to run a model locally, check out the <a href="https://github.com/ollama/ollama/blob/main/README.md#quickstart">Ollama documentation</a>.</p>
<p dir="auto">Now you're ready to run the development server:</p>

<p dir="auto">Open <a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a> with your browser and then start creating your own learning mind maps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prompts</h2><a id="user-content-prompts" aria-label="Permalink: Prompts" href="#prompts"></a></p>
<p dir="auto">The prompts used to generate the mind map data is defined in the <code>defaultLocalPrompt</code> and <code>defaultExternalPrompt</code> variables in the <code>prompts.ts</code> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebGPU-Based WiFi Simulator (238 pts)]]></title>
            <link>https://wifi-solver.com</link>
            <guid>41897214</guid>
            <pubDate>Sun, 20 Oct 2024 18:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wifi-solver.com">https://wifi-solver.com</a>, See on <a href="https://news.ycombinator.com/item?id=41897214">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do AI detectors work? Students face false cheating accusations (131 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations</link>
            <guid>41896973</guid>
            <pubDate>Sun, 20 Oct 2024 17:26:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations">https://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations</a>, See on <a href="https://news.ycombinator.com/item?id=41896973">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kurt Vonnegut's lost board game published (238 pts)]]></title>
            <link>https://www.polygon.com/board-games/467103/kurt-vonnegut-ghq-lost-board-game-publisher-interview</link>
            <guid>41896636</guid>
            <pubDate>Sun, 20 Oct 2024 16:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polygon.com/board-games/467103/kurt-vonnegut-ghq-lost-board-game-publisher-interview">https://www.polygon.com/board-games/467103/kurt-vonnegut-ghq-lost-board-game-publisher-interview</a>, See on <a href="https://news.ycombinator.com/item?id=41896636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Fans of literature most likely know Kurt Vonnegut for the novel <em><a rel="sponsored" href="https://www.amazon.com/Slaughterhouse-Five-Kurt-Vonnegut-audiobook/dp/B015EKZX2U/?tag=polygon05-20">Slaughterhouse-Five</a></em>. The staunchly anti-war book first resonated with readers during the Vietnam War era, later becoming a staple in high school curricula the world over. When Vonnegut died in 2007 at the age of 84, he was widely recognized as one of the greatest American novelists of all time. But would you believe that he was also an accomplished game designer?</p><p>In 1956, following the lukewarm reception of his first novel, <em>Player Piano</em>, Vonnegut was one of the 16 million other World War II veterans struggling to put food on the table. His moneymaking solution at the time was a board game called <em>GHQ</em>, which leveraged his understanding of modern combined arms warfare and distilled it into a simple game played on an eight-by-eight grid. Vonnegut pitched the game relentlessly to publishers all year long according to game designer and NYU faculty member <a href="https://gamecenter.nyu.edu/faculty/geoff-engelstein/">Geoff Engelstein</a>, who recently found those letters sitting in the archives at <a href="https://libraries.indiana.edu/lilly-library/kurt-vonnegut">Indiana University</a>. But the real treasure was an original set of typewritten rules, complete with Vonnegut’s own notes in the margins. </p><p>With the permission of the Vonnegut estate, Engelstein tells Polygon that he cleaned the original rules up just a little bit, buffed out the dents in <em>GHQ</em>’s endgame, and spun up some decent art and graphic design. Now you can purchase the final product, titled <em>Kurt Vonnegut’s GHQ: The Lost Board Game</em>, at your local Barnes &amp; Noble — nearly 70 years after it was created.</p><div><div><p><a href="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="445" data-pswp-width="600" target="_blank" rel="noreferrer"><img alt="A render of GHQ set up on a table for play. The markers are large and colorful, shaped like arrows and blocks. " data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_laid_out.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p><cite>Image: Mars International</cite></p></div><p>In a recent interview with Polygon, Engelstein still seemed stunned to have stumbled over the game in the first place through his research. But what’s truly fascinating to him is how diametrically opposed to Vonnegut’s later work <em>GHQ </em>truly is.</p><p>“<em>Sirens of Titan</em> was written at the same time as he was working on this game,” Engelstein told Polygon. “In <em>Sirens of Titan</em>, there’s this army of Mars which is really a joke. No one in the army, [not] even the officers, are really in charge of what’s going on. They’re all mind controlled. Nobody has any real free will. They’re just set up as a pawn to be sacrificed, to make Earth come together, kind of <em>Watchmen</em>-style.” </p><div id=":R3irarr6:"><div><p><a href="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="2000" data-pswp-width="3000" target="_blank" rel="noreferrer"><img alt="" data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_1.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><div><p><a href="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="2000" data-pswp-width="3000" target="_blank" rel="noreferrer"><img alt="" data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_compilation_2.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div></div><p>While <em><a rel="sponsored" href="https://www.amazon.com/The-Sirens-of-Titan-Kurt-Vonnegut-audiobook/dp/B001D1ILCO/?tag=polygon05-20">The Sirens of Titan</a></em> was a deeply cynical view of war, <em>GHQ </em>is deeply <em>un</em>cynical. In fact, his own pitch letters note that Vonnegut thought <em>GHQ</em> would be an excellent training aid for future military leaders, including cadets at West Point. How are modern audiences to reconcile those words from the same man who wrote <em><a rel="sponsored" href="https://www.amazon.com/Cats-Cradle-Kurt-Vonnegut-audiobook/dp/B000Z7FH9M/?tag=polygon05-20">Cat’s Cradle</a></em>?</p><p>“There’s no definitive answers [to those questions],” Engelstein said. “He didn’t write about it. Nobody asked him about it while he was alive, so we will never know.”</p><p>For fans of board gaming, the questions go in a slightly different direction: What if Vonnegut’s pitches from the 1950s had been successful? </p><p>Engelstein reasons that if Vonnegut was pitching the game in ’56, then it would have taken at least a few years for the game to be produced and finally published. That 1958-1959 window would have placed <em>GHQ</em> in rare company — 1958 was the year <em>Tactics 2</em> was published, a game that would go on to inspire the <a href="https://www.polygon.com/features/2013/1/29/3916154/turn-by-turn-battlefront-combat-mission">Squad Leader</a> series of map-and-token tactical wargames and, ultimately, video game genres like turn-based and real-time strategy. Just a year later and the industry would see the release of <em>Risk</em> and <em>Diplomacy</em>, the precursors of the modern 4X genre and, in and of themselves, two successful franchises that are popular to this day. </p><div><p><a href="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100" data-pswp-height="2000" data-pswp-width="3000" target="_blank" rel="noreferrer"><img alt="A letter pitching GHQ to the Sallfield Publishing Company in Ohio. Also a rejection letter from the same organization." data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 639px) 100vw, (max-width: 1023px) 50vw, 700px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=256 256w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq_rejection_5a4312.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=2400"></a></p></div><p>“Three games that had tremendous influence, all war-related, coming out in that one-year, two-year period,” Engelstein mused. “So if <em>GHQ</em> also came out at the time period? There’s something in the air at that point, obviously.”</p><p>Of course, we’ll never know how those counterfactuals would have played out, but at least <em>GHQ</em> is finally available to the public. That’s great news for one of its original playtesters, Kurt Vonnegut’s son, Mark Vonnegut, who’s now 77. Engelstein said his input was invaluable in bringing the game to life.</p><p>“The success of <em>Slaughterhouse-Five</em> and the other novels is nice enough,” Vonnegut’s son recently wrote Engelstein in an email, “but I truly believe he’s watching somehow, someway, from somewhere and that the success of <em>GHQ</em> will be a greater and purely unadulterated pleasure. [...] He was discouraged about his writing at the time, but had unshakable faith that <em>GHQ</em> would succeed.”</p><p>You can find <em>Kurt Vonnegut’s GHQ: The Lost Board Game</em>, along with a special forward by <a href="https://www.polygon.com/24164196/expanse-james-s-a-corey-new-book-mercy-gods">author James S.A. Corey</a>, exclusively at Barnes &amp; Noble.</p><div data-product-filter=""><p><a href="https://go.skimresources.com/?id=1025X1701642&amp;xs=1&amp;url=https%3A%2F%2Fwww.barnesandnoble.com%2Fw%2Fkurt-vonneguts-ghq-the-lost-board-game-mars-international%2F1146300521" rel="nofollow noopener noreferrer" target="_blank"><img alt="" data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 100vw, 300px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=16.700150678051%2C0%2C66.599698643898%2C100&amp;w=2400"></a><a href="https://go.skimresources.com/?id=1025X1701642&amp;xs=1&amp;url=https%3A%2F%2Fwww.barnesandnoble.com%2Fw%2Fkurt-vonneguts-ghq-the-lost-board-game-mars-international%2F1146300521" rel="nofollow noopener noreferrer" target="_blank"><img alt="" data-chromatic="ignore" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 100vw, 600px" srcset="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=376 376w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=384 384w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=415 415w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=480 480w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=540 540w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=640 640w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=750 750w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=828 828w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=1080 1080w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=1200 1200w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=1440 1440w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=1920 1920w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=2048 2048w, https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=2400 2400w" src="https://platform.polygon.com/wp-content/uploads/sites/2/2024/10/ghq.jpg?quality=90&amp;strip=all&amp;crop=0.050226017076845%2C0%2C99.899547965846%2C100&amp;w=2400"></a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Drasi: Microsoft's open source data processing platform for event-driven systems (248 pts)]]></title>
            <link>https://github.com/drasi-project/drasi-platform</link>
            <guid>41896297</guid>
            <pubDate>Sun, 20 Oct 2024 16:07:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/drasi-project/drasi-platform">https://github.com/drasi-project/drasi-platform</a>, See on <a href="https://news.ycombinator.com/item?id=41896297">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Drasi</h2><a id="user-content-drasi" aria-label="Permalink: Drasi" href="#drasi"></a></p>
<p dir="auto">Drasi is a data processing platform that simplifies detecting changes in data and taking immediate action. It is a comprehensive solution that provides built-in capabilities to track system logs and change feeds for specific events, evaluate them for relevance, and automatically initiate appropriate reactions. Visit our documentation site at <a href="https://drasi.io/" rel="nofollow">https://drasi.io</a> for detailed information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Drasi provides real-time actionable insights without the overhead of traditional data processing methods. It tracks system changes and events without the need to copy data to a central data lake or repeatedly query data sources. Drasi uses queries to continuously evaluate incoming data changes. When the changes match the criteria and conditions specified in these queries the result sets of these queries are updated. These updates then trigger context-aware reactions defined tuned to your specific requirements.</p>
<p dir="auto">Drasi operates through three components:</p>
<ul dir="auto">
<li><strong>Sources</strong> connect to data repositories within software systems to monitor logs and feeds to track changing data.</li>
<li><strong>Continuous Queries</strong> interpret monitored changes by applying criteria and conditions to identify significant changes. In Drasi, these Continuous Queries are written using the Cypher Query Language.</li>
<li><strong>Reactions</strong> trigger meaningful responses based on updates to the result sets of the Continuous Queries.<br></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/drasi-project/community/blob/main/images/drasi_components.png"><img src="https://github.com/drasi-project/community/raw/main/images/drasi_components.png" alt="Alt text" width="800" height="300"></a></p>
<p dir="auto"><br>To illustrate how Drasi interprets events and triggers appropriate responses, consider a delivery system for an online ordering service. Orders are processed through an order management system, and delivery drivers need real-time notifications when orders are ready for pickup. Drasi automates this process by:<br></p>
<ul dir="auto">
<li>Configuring a Source to monitor the order management system for changes in order statuses and a second Source to detect when a driver becomes available for a delivery run.</li>
<li>Creating a Continuous Query that combines data from both Sources to match orders ready for pickup with available drivers.</li>
<li>Defining a Reaction to send alerts to drivers, notifying them to proceed to the pickup area.
This streamlined setup ensures drivers are promptly informed, optimizing the delivery process through real-time data integration and automated responses.<br></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/drasi-project/community/blob/main/images/curbside_pickup_drasi.png"><img src="https://github.com/drasi-project/community/raw/main/images/curbside_pickup_drasi.png" alt="Alt text" width="800" height="300"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Follow the <a href="https://drasi.io/getting-started/" rel="nofollow">Getting Started tutorial</a> and try out Drasi. The tutorial will lead you through:</p>
<ol dir="auto">
<li>Applying a Source representing the data source whose changes you want to observe.</li>
<li>Creating Continuous Queries to define the data to observe, conditions to assess changes, and the structure of the output.</li>
<li>Applying a Debug Reaction to view the output generated by one or more Continuous Queries.</li>
</ol>
<p dir="auto">Head over to our <a href="https://drasi.io/" rel="nofollow">documentation site</a> and visit the <a href="https://drasi.io/tutorials/" rel="nofollow">Tutorial</a> and <a href="https://drasi.io/how-to-guides/" rel="nofollow">How To</a> guides to learn more about Drasi.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Release Status</h2><a id="user-content-release-status" aria-label="Permalink: Release Status" href="#release-status"></a></p>
<p dir="auto">This is an early release of Drasi for the community learn about the platform and experiment with in Proofs Of Concept. Please share your thoughts on Drasi and create GitHub issues for any bugs you may find or if you have feature requests that will help improve Drasi.</p>
<p dir="auto">This repo contains everything you require to build a Drasi-based solution with Sources, Reactions, and tooling for development and testing.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto">We hope you will join us and contribute to Drasi! Some of the ways to get started with contributing are participating in Issue discussions or joining us on our <a href="https://aka.ms/drasidiscord" rel="nofollow">Discord server</a>. Check out our <a href="https://github.com/drasi-project/community">Community repo</a> for more information on the community, and guidance on contributing and development.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing To Drasi</h2><a id="user-content-contributing-to-drasi" aria-label="Permalink: Contributing To Drasi" href="#contributing-to-drasi"></a></p>
<p dir="auto">Please see the <a href="https://github.com/drasi-project/drasi-platform/blob/main/CONTRIBUTING.md">Contribution guide</a> for information on contributing to Drasi.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Please refer to our guide on <a href="https://github.com/drasi-project/drasi-platform/blob/main/SECURITY.md#reporting-security-issues">reporting security vulnerabilities</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code of Conduct</h2><a id="user-content-code-of-conduct" aria-label="Permalink: Code of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">Please refer to Drasi's <a href="https://github.com/drasi-project/community/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the <strong>Apache 2.0 license</strong>. Please see the <a href="https://github.com/drasi-project/community/blob/main/LICENSE">LICENSE</a> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact the Drasi Authors</h2><a id="user-content-contact-the-drasi-authors" aria-label="Permalink: Contact the Drasi Authors" href="#contact-the-drasi-authors"></a></p>
<p dir="auto">Please join us on Discord to contact us and we will get back to you as soon as possible. You can also email us at <a href="mailto:info@drasi.io">info@drasi.io</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Part of PostgreSQL We Hate the Most (2023) (271 pts)]]></title>
            <link>https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html</link>
            <guid>41895951</guid>
            <pubDate>Sun, 20 Oct 2024 15:30:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html">https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html</a>, See on <a href="https://news.ycombinator.com/item?id=41895951">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>There are a lot of choices in databases (<a onclick="javascript:pageTracker._trackPageview('/outgoing/dbdb.io');" href="https://dbdb.io/" target="_blank">897</a> as of April 2023). With so many systems, it’s hard to know what to pick! But there is an interesting phenomenon where the Internet collectively decides on the default choice for new applications. In the 2000s, the conventional wisdom selected MySQL because rising tech stars like Google and Facebook were using it. Then in the 2010s, it was MongoDB because <a onclick="javascript:pageTracker._trackPageview('/outgoing/stackoverflow.com');" href="https://stackoverflow.com/a/3737121" target="_blank">non-durable writes</a> made it “<a onclick="javascript:pageTracker._trackPageview('/outgoing/youtu.be');" href="https://youtu.be/b2F-DItXtZs" target="_blank">webscale</a>“. In the last five years, PostgreSQL has become the Internet’s darling DBMS. And for good reasons! It’s dependable, feature-rich, extensible, and well-suited for most operational workloads.</p>
<p>But as much as we <a onclick="javascript:pageTracker._trackPageview('/downloads/papers/2017/p781-wu.pdf');" href="https://twitter.com/andy_pavlo/status/1534225032179814403" target="_blank">love PostgreSQL at OtterTune</a>, certain aspects of it are not great. So instead of writing yet another blog article like everyone else touting the awesomeness of everyone’s favorite elephant-themed DBMS, we want to discuss the one major thing that sucks: how PostgreSQL implements <a onclick="javascript:pageTracker._trackPageview('/outgoing/en.wikipedia.org');" href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control" target="_blank">multi-version concurrency control</a> (MVCC). Our <a onclick="javascript:pageTracker._trackPageview('/downloads/papers/2017/p781-wu.pdf');" href="https://db.cs.cmu.edu/papers/2017/p781-wu.pdf" target="_blank">research</a> at Carnegie Mellon University and experience optimizing PostgreSQL database instances on Amazon RDS have shown that its MVCC implementation is the <u><b>worst</b></u> among the other widely used relational DBMSs, including MySQL, Oracle, and Microsoft SQL Server. And yes, Amazon’s PostgreSQL Aurora still has these problems.</p>
<p>In this article, we’ll dive into MVCC: what it is, how PostgreSQL does it, and why it is terrible. Our goal at OtterTune is to give you <i>fewer</i> things to worry about with your databases, so we’ve thought a lot about dealing with this problem. We’ll cover OtterTune’s solution for managing PostgreSQL’s MVCC issues automatically for RDS and Aurora databases in a follow-up article next week.</p>
<h2 id="what-is-mvcc">What is Multi-Version Concurrency Control?</h2>
<p>The goal of MVCC in a DBMS is to allow multiple queries to read and write to the database simultaneously without interfering with each other when possible. The basic idea of MVCC is that the DBMS never overwrites existing rows. Instead, for each (logical) row, the DBMS maintains multiple (physical) versions. When the application executes a query, the DBMS determines which version to retrieve to satisfy the request according to some version ordering (e.g., creation timestamp). The benefit of this approach is that multiple queries can read older versions of rows without getting blocked by another query updating it. Queries observe a snapshot of the database as it existed when the DBMS started that query’s transaction (snapshot isolation). This approach eliminates the need for explicit <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.postgresql.org');" href="https://www.postgresql.org/docs/current/explicit-locking.html" target="_blank">record locks</a> that block readers from accessing data while writers modify the same item.</p>
<p>David Reed’s 1978 MIT Ph.D. dissertation, “<a onclick="javascript:pageTracker._trackPageview('/outgoing/dspace.mit.edu');" href="https://dspace.mit.edu/handle/1721.1/16279" target="_blank">Concurrency Control in Distributed Database Systems</a>,” was, we believe, the first publication to describe MVCC. The first commercial DBMS implementation of MVCC was <a onclick="javascript:pageTracker._trackPageview('/outgoing/en.wikipedia.org');" href="https://en.wikipedia.org/wiki/InterBase#History" target="_blank">InterBase</a> in the 1980s. Since then, nearly every new DBMS created in the last two decades that supports transactions implements MVCC.</p>
<p>A systems engineer has to make several design decisions when building a DBMS that supports MVCC. At a high level, it comes down to the following:</p>
<ol>
  <li aria-level="1">How to store updates to existing rows.</li>
  <li aria-level="1">How to find the correct version of a row for a query at runtime.</li>
  <li aria-level="1">How to remove expired versions that are no longer visible.</li>
</ol>

<p>These decisions are not mutually exclusive. In the case of PostgreSQL, it’s how they decided to handle the first question in the 1980s that caused problems with the other two that we still have to deal with today.</p>
<p>For our discussion, we will use the following example of a table containing movie information. Each row in the table includes the movie name, release year, director, and a unique ID serving as the primary key, with secondary indexes on the movie name and director. Here is the DDL command to create this table:</p>
<pre><code>CREATE TABLE movies (
  id INTEGER PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
  name VARCHAR(256) NOT NULL,
  year SMALLINT NOT NULL,
  director VARCHAR(128)
);
CREATE INDEX idx_name ON movies (name);
CREATE INDEX idx_director ON movies (director);</code></pre>

<p>The table contains a primary index (<code>movies_pkey</code>) and two secondary B+Tree indexes (<code>idx_name</code>, <code>idx_director</code>).</p>
<h2 id="postgresql-mvcc">PostgreSQL’s Multi-Version Concurrency Control</h2>

<p>As discussed in Stonebraker’s <a onclick="javascript:pageTracker._trackPageview('/outgoing/apps.dtic.mil');" href="https://apps.dtic.mil/sti/citations/ADA187244" target="_blank">system design document from 1987</a>, PostgreSQL was designed from the beginning to support multi-versioning. The core idea of PostgreSQL’s MVCC scheme is seemingly straightforward: when a query updates an existing row in a table, the DBMS makes a copy of that row and applies the changes to this new version instead of overwriting the original row. We refer to this approach as the <b>append-only</b> version storage scheme. But as we now describe, this approach has several non-trivial implications in the rest of the system.</p>
<h3>Multi-Versioned Storage</h3>
<p>PostgreSQL stores all row versions in a table in the same storage space. To update an existing tuple, the DBMS first acquires an empty slot from the table for the new row version. It then copies the row content of the current version to the new version, and applies the modifications to the row in the newly allocated version slot. You can see this process in the example below when an application executes an update query on the movies database to change the release year of “<a onclick="javascript:pageTracker._trackPageview('/outgoing/en.wikipedia.org');" href="https://en.wikipedia.org/wiki/Shaolin_and_Wu_Tang" target="_blank">Shaolin and Wu Tang</a>” from 1985 to 1983:</p>
<figure>
  <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example1.svg"><img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example1.svg"></a>
  <figcaption>When an UPDATE query changes a tuple in the table, PostgreSQL copies the original version of the tuple and then applies the change to the new version. In this example, there is no more space in Table Page #1, so PostgreSQL creates the new version in Table Page #2.</figcaption>
</figure>

<p>Now with two physical versions representing the same logical row, the DBMS needs to record the lineage of these versions so that it knows how to find them in the future. MVCC DBMSs achieve this by creating a <b>version chain</b> via a singly linked-list. The version chain only goes in one direction to reduce storage and maintenance overhead. This means that the DBMS has to decide what order to use: <i>newest-to-oldest</i> (N2O) order or <i>oldest-to-newest</i> (O2N). For the N2O order, each tuple version points to its previous version and the version chain’s head is always the latest version. For the O2N order, each tuple version points to its new version, and the head is the oldest tuple version. The O2N approach avoids the need for the DBMS to update indexes to point to a newer version of the tuple each time it’s modified. However, it may take longer for the DBMS to find the latest version during query processing, potentially traversing a long version chain. Most DBMSs, including Oracle and MySQL, implement N2O. But PostgreSQL stands alone in using O2N (except for Microsoft’s <a onclick="javascript:pageTracker._trackPageview('/outgoing/learn.microsoft.com');" href="https://learn.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/introduction-to-memory-optimized-tables?view=sql-server-ver16" target="_blank">In-Memory OLTP engine</a> for SQL Server).</p>
<p>The next issue is how PostgreSQL determines what to record for these version pointers. The header for each row in PostgreSQL contains a tuple id field ( <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.postgresql.org');" href="https://www.postgresql.org/docs/current/storage-page-layout.html#STORAGE-TUPLE-LAYOUT" target="_blank">t_tcid</a>) of the next version (or its own tuple id if it is the latest version). Thus, as shown in this next example, when a query requests the latest version of a row, the DBMS traverses the index, lands on the oldest version, and then follows the pointer until it finds a version that it needs.</p>
<figure>
  <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example3.svg"><img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example3.svg"> </a>
  <figcaption>The SELECT query traverses the index to find tuple with requested movie name. The index entry points to the oldest version of the tuple, which means PostgreSQL follows the version chain embedded in the original version to find the new version.</figcaption>
</figure>

<p>PostgreSQL developers realized early on that there are two problems with its MVCC scheme. First, making a new copy of an entire tuple every time it is updated is expensive. And second, traversing the entire version chain just to find the latest version (which is what most queries want) is wasteful. Of course there is also the problem of cleaning up old versions, but we’ll cover that below.</p>
<p>To avoid traversing the entire version chain, PostgreSQL adds an entry to a table’s indexes for each physical version of a row. That means if there are five physical versions of a logical row, there will be (at most) five entries for that tuple in the index! In the example below, we see that the <code>idx_name</code> index contains entries for each of the “Shaolin and Wu Tang” rows that are on separate pages. This enables direct access to the latest version of the tuple, without the need to traverse the long version chain.</p>
<figure>
  <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example4.svg">
    <img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example4.svg">
    </a>
    <figcaption>In this example, the index contains multiple entries for the “Shaolin and Wu Tang” tuple (one for each version). Now PostgreSQL uses the index to find the latest version and then immediately retrieves it from Table Page #2 without having to traverse the version chain starting at Table Page #1.</figcaption>
</figure>

<p>PostgreSQL tries to avoid having to install multiple index entries and storing related versions over multiple pages by creating a new copy in the same disk page (block) as the old version to reduce disk I/O. This optimization is known as <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.postgresql.org');" href="https://www.postgresql.org/docs/current/storage-hot.html" target="_blank">heap-only tuple (HOT)</a> updates. The DBMS uses the HOT approach if an update does not modify any columns referenced by a table’s indexes and the new version is stored on the same data page as the old version (if there is space in that page). Now in our example, after the update the index still points to the old version and queries retrieve the latest version by traversing the version chain. During normal operation, PostgreSQL further optimizes this process by removing old versions to prune the version chain.</p>
<h3>Version Vacuum</h3>
<p>We’ve established that PostgreSQL makes a copy of rows whenever an application updates them. The next question is how the system removes older versions (called “dead tuples”). The original version of PostgreSQL from the 1980s did not remove dead tuples. The idea was that keeping all the older versions allowed applications to execute “time-travel” queries to examine the database at a particular point in time (e.g., run a <code>SELECT</code> query on the state of the database as it existed at the end of last week). But never removing dead tuples means tables never shrink in size if the application deletes tuples. It also means long version chains for frequently updated tuples, which would slow down queries, except that PostgreSQL adds index entries that allow queries to quickly jump to the correct version instead of traversing the chain. But now, this means the indexes are larger, making them slower and adding additional memory pressure. Hopefully, you can understand now why all these issues are interconnected.</p>
<p>To overcome these problems, PostgreSQL uses a vacuum procedure to clean up dead tuples from tables. The vacuum performs a sequential scan on table pages modified since its last run and find expired versions. The DBMS considers a version “<b>expired</b>” if it is not visible to any active transaction. This means no current transaction is accessing that version, and future transactions will use the latest “<b>live</b>” version instead. Thus, removing the expired version and reclaiming the space for reuse is safe.</p>
<p>PostgreSQL automatically executes this vacuum procedure (autovacuum) at regular intervals based on its configuration settings. In addition to the global settings that affect the vacuum frequency for all tables, PostgreSQL provides the flexibility to configure autovacuum at the table level to fine-tune the process for specific tables. Users can also trigger the vacuum manually to optimize database performance via the <code>VACUUM</code> SQL command.</p>
<h2 id="why-postgresqls-mvcc-is-the-worst">Why PostgreSQL’s MVCC is the Worst</h2>
<p>We will be blunt: if someone is going to build a new MVCC DBMS today, they should <u> <b>not</b> </u> do it the way PostgreSQL does (e.g., append-only storage with autovacuum). In our <a onclick="javascript:pageTracker._trackPageview('/downloads/papers/2017/p781-wu.pdf');" href="https://db.cs.cmu.edu/papers/2017/p781-wu.pdf" target="_blank">2018 VLDB paper</a> (aka “ <a onclick="javascript:pageTracker._trackPageview('/outgoing/twitter.com');" href="https://twitter.com/andy_pavlo/status/902863242774634496" target="_blank">the best paper ever on MVCC</a>“), we did not find another DBMS doing MVCC the way PostgreSQL does it. Its design is a relic of the 1980s and before the proliferation of <a onclick="javascript:pageTracker._trackPageview('/outgoing/en.wikipedia.org');" href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank">log-structured</a> system patterns from the 1990s.</p>
<p>Let’s talk about four problems that arise with PostgreSQL’s MVCC. We will also talk about why other MVCC DBMSs like Oracle and MySQL avoid these problems.</p>
<h3>Problem #1: Version Copying</h3>
<p>With the append-only storage scheme in MVCC, if a query updates a tuple, the DBMS copies all its columns into the new version. This copying occurs no matter if the query updates a single or all of its columns. As you can imagine, append-only MVCC results in massive data duplication and increased storage requirements. This approach means that PostgreSQL requires more memory and disk storage to store a database than other DBMS, which means slower queries and higher cloud costs. Instead of copying an entire tuple for a new version, MySQL and Oracle store a compact delta between the new and current versions (think of it like a git diff). Using deltas means that if a query only updates a single column in a tuple for a table with 1000 columns, then the DBMS only stores a delta record with the change to that one column. On the other hand, PostgreSQL creates a new version with the one column that the query changed and the 999 other untouched columns. We will ignore TOAST attributes because PostgreSQL <a onclick="javascript:pageTracker._trackPageview('/outgoing/dba.stackexchange.com');" href="https://dba.stackexchange.com/a/308779" target="_blank">handles them differently</a>.</p>
<p>There was an attempt to modernize PostgreSQL’s version storage implementation. EnterpriseDB started the <a onclick="javascript:pageTracker._trackPageview('/outgoing/wiki.postgresql.org');" href="https://wiki.postgresql.org/wiki/Zheap" target="_blank">zheap project</a> in 2013 to replace the append-only storage engine to use delta versions. Unfortunately the <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.cybertec-postgresql.com');" href="https://www.cybertec-postgresql.com/en/postgresql-zheap-current-status/" target="_blank">last official update was in 2021</a>, and to the best of our knowledge the effort has fizzled out.</p>
<h3>Problem #2: Table Bloat</h3>
<p>Expired versions in PostgreSQL (i.e., dead tuples) also occupy more space than delta versions. Although PostgreSQL’s autovacuum will eventually remove these dead tuples, write-heavy workloads can cause them to accumulate faster than the vacuum can catch up, resulting in continuous database growth. The DBMS has to load dead tuples into memory during query execution since the system intermingles dead tuples with live tuples in pages. Unfettered bloat slows query performance by causing the DBMS to incur more IOPS and consume more memory than necessary during table scans. Additionally, inaccurate optimizer statistics caused by dead tuples can lead to poor query plans.</p>
<p>Suppose our movies table has 10 million live and 40 million dead tuples, making 80% of the table obsolete data. Assume also that the table also has many more columns than what we are showing and that the average size of each tuple is 1KB. With this scenario, the live tuples occupy 10GB of storage space while the dead tuples occupy ~40GB of storage; the total size of the table is 50GB. When a query performs a full table scan on this table, PostgreSQL has to retrieve all 50GB from the disk and store it in memory, even if most of it is obsolete. Although Postgres has a <a onclick="javascript:pageTracker._trackPageview('/outgoing/madusudanan.com');" href="https://madusudanan.com/blog/understanding-postgres-caching-in-depth/#SeqScans" target="_blank">protection mechanism</a> to avoid polluting its buffer pool cache from sequential scans, it does not help prevent IO costs.</p>
<p>Even if you make sure that PostgreSQL’s autovacuum is running at regular intervals and able to keep up with your workload (which is not always easy to do, see below), the autovacuum cannot reclaim storage space. The autovacuum only removes dead tuples and relocates live tuples within each page, but it does not reclaim empty pages from the disk.</p>
<p>When the DBMS truncates the last page due to the absence of any tuple, other pages remain on disk. In our example above, even if PostgreSQL removed the 40GB of dead tuples from the movies table, it still retains the 50GB of allocated storage space from the operating system (or, in the case of RDS, from Amazon). To reclaim and return such unused space, one must use <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.postgresql.org');" href="https://www.postgresql.org/docs/current/sql-vacuum.html#:~:text=VACUUM%20FULL%20rewrites%20the%20entire,while%20it%20is%20being%20processed." target="_blank"><code>VACUUM FULL</code></a> or the <a onclick="javascript:pageTracker._trackPageview('/outgoing/reorg.github.io');" href="https://reorg.github.io/pg_repack/" target="_blank">pg_repack</a> extension to rewrite the entire table to a new space with no wasted storage. Running either of these operations is not an easy endeavor that one should take without considering the performance implications for production databases; they are resource-intensive and time-consuming operations that will crush query performance. The following figure shows how <code>VACUUM</code> and <code>VACUUM FULL</code> work.</p>
<figure>
    <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-vacuum.svg"><img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-vacuum.svg"> </a>
    <figcaption>With PostgreSQL’s regular VACUUM operation, the DBMS only removes dead tuples from each table page and reorganizes it to put all the live tuples at the end of the page. With VACUUM FULL, PostgreSQL removes the dead tuples from each page, coalesces and compacts the remaining live tuples to a new page (Table Page #3), and then deletes the unneeded pages (Table Pages #1 / #2).</figcaption>
</figure>

<h3>Problem #3: Secondary Index Maintenance</h3>
<p>A single update to a tuple requires PostgreSQL to update all the indexes for that table. Updating all the indexes is necessary because PostgreSQL uses the exact physical locations of a version in both primary and secondary indexes. Unless the DBMS stores the new version in the same page as the previous version (HOT update), the system does this for every update.</p>
<p>Returning to our <code>UPDATE</code> query example, PostgreSQL creates a new version by copying the original version into a new page just like before. But it also inserts entries pointing to the new version in table’s primary key index ( <code>movies_pkey</code>) and the two secondary indexes ( <code>idx_director</code>, <code>idx_name</code>).</p>
<figure>
  <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example5.svg"><img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-example5.svg"> </a>
  <figcaption>Example of PostgreSQL index maintenance operations with a non-HOT update. The DBMS creates the new version of the tuple in Table Page #2, and then inserts new entries that point to that version in all the table’s indexes.</figcaption>
</figure>

<p>The need for PostgreSQL to modify all of a table’s indexes for each update has several performance implications. Obviously, this makes update queries slower because the system has to do more work. The DBMS incurs additional I/O to traverse each index and insert the new entries. Accessing an index introduces lock/latch contention in both the index and the DBMS’s internal data structures (e.g., buffer pool’s page table). Again, PostgreSQL does this maintenance work for all a table’s indexes, even if queries are never going to use them (by the way, OtterTune <a onclick="javascript:pageTracker._trackPageview('/outgoing/docs.ottertune.com');" href="https://docs.ottertune.com/documentation/database-instance-dashboard-and-recommendations/recommendations/index-recommendations">automatically finds unused indexes in your database</a>). These extra reads and writes are problematic in DBMSs that charge users based on IOPS, like Amazon Aurora.</p>
<p>As described above, PostgreSQL avoids updating indexes each time if it can perform a HOT write where the new version is on the same page as the current version. Our analysis of OtterTune customers’ PostgreSQL databases shows that roughly 46% of updates use the HOT optimization on average. Although that’s an impressive number, it still means more than 50% of the updates are paying this penalty.</p>
<p>There are many examples of users struggling with this aspect of PostgreSQL’s MVCC implementation. The most famous testament of this is Uber’s 2016 blog article about why they <a onclick="javascript:pageTracker._trackPageview('/outgoing/www.uber.com');" href="https://www.uber.com/blog/postgres-to-mysql-migration/" target="_blank">switched from Postgres to MySQL</a>. Their write-heavy workload was experiencing significant performance problems on tables with many secondary indexes.</p>
<p>Oracle and MySQL do not have this problem in their MVCC implementation because their secondary indexes do not store the physical addresses of new versions. Instead, they store a logical identifier (e.g., tuple id, primary key) that the DBMS then uses to look up the current version’s physical address. Now this may make secondary index reads slower since the DBMS has to resolve a logical identifier, but these DBMS have other advantages in their MVCC implementation to reduce overhead.</p>


<h3>Problem #4: Vacuum Management</h3>
<p>PostgreSQL’s performance relies heavily on the effectiveness of the autovacuum to remove obsolete data and reclaim space (this is why OtterTune immediately checks the health status of the autovacuum when you first connect your database). It does not matter if you are running RDS, Aurora, or Aurora Serverless; all variants of PostgreSQL have the same autovacuum issues. But making sure that PostgreSQL’s autovacuum is running as best as possible is difficult due to its complexity. PostgreSQL’s default settings for tuning the autovacuum are not ideal for all tables, particularly for large ones. For example, the default setting for the configuration knob that controls what percentage of a table PostgreSQL has to update before the autovacuum kicks in (<a onclick="javascript:pageTracker._trackPageview('/outgoing/www.postgresql.org');" href="https://www.postgresql.org/docs/15/runtime-config-autovacuum.html#GUC-AUTOVACUUM-VACUUM-SCALE-FACTOR" target="_blank">autovacuum_vacuum_scale_factor</a>) is 20%. This threshold means that if a table has 100 million tuples, the DBMS does not trigger the autovacuum until queries update at least 20 million tuples. As such, PostgreSQL may unnecessarily keep around a lot of dead tuples in a table (thereby incurring IO and memory costs) for a long time.</p>
<p>Another problem with the autovacuum in PostgreSQL is that it may get blocked by long-running transactions, which can result in the accumulation of more dead tuples and stale statistics. Failing to clean expired versions in a timely manner leads to numerous performance problems, causing more long-running transactions that block the autovacuum process. It becomes a vicious cycle, requiring humans to intervene manually by killing long-running transactions. Consider the graph below that shows the number of dead tuples in an OtterTune customer’s database over two weeks:</p>
<figure>
  <a href="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-deadtuples.svg"><img decoding="async" src="https://www.cs.cmu.edu/~pavlo/images/blog/020/ottertune-mvcc-deadtuples.svg"></a>
  <figcaption>The number of dead tuples over time in a PostgreSQL Amazon RDS database.</figcaption>
</figure>

<p>The sawtooth pattern in the chart shows that the autovacuum performs a major clean-up about once every day. For example, on February 14th, the DBMS cleaned up 3.2 million dead tuples. This graph is actually an example of an unhealthy PostgreSQL database. The chart clearly shows an upward trend in the number of dead tuples because the autovacuum cannot keep up.</p>
<p>At OtterTune, we see this problem often in our customers’ databases. One PostgreSQL RDS instance had a long-running query caused by stale statistics after bulk insertions. This query blocked the autovacuum from updating the statistics, resulting in more long-running queries. OtterTune’s automated health checks identified the problem, but the administrator still had to kill the query manually and run <a onclick="javascript:pageTracker._trackPageview('/outgoing/ottertune.com');" href="https://ottertune.com/blog/run-postgresql-analyze-to-fix-a-slowdow-in-db/">ANALYZE after bulk insertions</a>. The good news is that the long query’s execution time went from 52 minutes to just 34 seconds.</p>

<p>There are always hard design decisions one has to make when building a DBMS. And these decisions will cause any DBMS to perform differently on varying workloads. For Uber’s specific write-intensive workload, PostgreSQL’s index write amplification due to MVCC is why they switched to MySQL. But please don’t misunderstand our diatribe to mean that we don’t think you should ever use PostgreSQL. Although its MVCC implementation is the wrong way to do it, PostgreSQL is still our favorite DBMS. To love something is to be willing to work with its flaws (see Dan Savage’s <a onclick="javascript:pageTracker._trackPageview('/outgoing/youtu.be');" href="https://youtu.be/r1tCAXVsClw" target="_blank">“The Price of Admission”</a>).</p>
<p>So how does one work around PostgreSQL’s quirks? Well, you can spend an enormous amount of time and effort tuning it yourself. <a onclick="javascript:pageTracker._trackPageview('/outgoing/philbooth.me');" href="https://philbooth.me/blog/nine-ways-to-shoot-yourself-in-the-foot-with-postgresql" target="_blank">Good luck with that</a>.</p>
<p>We’ll cover more about what we can do in our next article.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Internet Archive breached again through stolen access tokens (405 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/</link>
            <guid>41895764</guid>
            <pubDate>Sun, 20 Oct 2024 15:00:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/">https://www.bleepingcomputer.com/news/security/internet-archive-breached-again-through-stolen-access-tokens/</a>, See on <a href="https://news.ycombinator.com/item?id=41895764">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="The Internet Archive" height="900" src="https://www.bleepstatic.com/content/hl-images/2024/10/09/internet-archive.jpg" width="1600"></p>
<p>The Internet Archive was breached again, this time on their Zendesk email support platform after repeated warnings that threat actors stole exposed GitLab authentication tokens.</p>
<p>Since last night, BleepingComputer has received numerous messages from people who received replies to their old Internet Archive removal requests, warning that the organization has been breached as they did not correctly rotate their stolen authentication tokens.</p>
<p>"It's dispiriting to see that even after being made aware of the breach weeks ago, IA has still not done the due diligence of rotating many of the API keys that were exposed in their gitlab secrets," reads an email from the threat actor.</p>
<p>"As demonstrated by this message, this includes a Zendesk token with perms to access 800K+ support tickets sent to info@archive.org since 2018."</p>
<p>"Whether you were trying to ask a general question, or requesting the removal of your site from the Wayback Machine your data is now in the hands of some random guy. If not me, it'd be someone else."</p>
<div>
<figure><img alt="Internet Archive Zendesk emails sent by the threat actor" height="600" src="https://www.bleepstatic.com/images/news/security/attacks/i/internet-archive/gitlab-tokens/zendesk-emails.jpg" width="937"><figcaption><strong>Internet Archive Zendesk emails sent by the threat actor</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>The email headers in these emails also pass all DKIM, DMARC, and SPF authentication checks, proving they were sent by an authorized Zendesk server at&nbsp;192.161.151.10.</p>
<div>
<figure><img alt="Internet Archive Zendesk email headers" height="200" src="https://www.bleepstatic.com/images/news/security/attacks/i/internet-archive/gitlab-tokens/mail-headers.jpg" width="909"><figcaption><strong>Internet Archive Zendesk email headers</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>After publishing this story, BleepingComputer was told by a recipient of these emails that they had to upload personal identification when requesting a removal of a page from the Wayback Machine.</p>
<p>The threat actor may now also have access to these attachments depending on the API access they had to Zendesk and if they used it to <a href="https://developer.zendesk.com/api-reference/ticketing/tickets/ticket-attachments/#show-attachment" target="_blank" rel="nofollow noopener">download support tickets</a>.</p>
<p>These emails come after BleepingComputer repeatedly tried to warn the Internet Archive that their source code was stolen through a&nbsp;GitLab authentication token that was exposed online for almost two years.</p>
<h2>Exposed GitLab authentication tokens</h2>
<p>On October 9th, BleepingComputer reported that Internet&nbsp;Archive was <a href="https://www.bleepingcomputer.com/news/security/internet-archive-hacked-data-breach-impacts-31-million-users/" target="_blank">hit by two different attacks at once last week</a>—a data breach where the site's user data for 33 million users was stolen and a DDoS attack by a pro-Palestinian group named SN_BlackMeta.</p>
<p>While both attacks occurred over the same period, they were conducted by different threat actors. However, many outlets incorrectly reported that SN_BlackMeta was behind the breach rather than just the DDoS attacks.</p>
<div>
<figure><img alt="JavaScript alert on Internet Archive warning about the breach" height="300" width="665" data-src="https://www.bleepstatic.com/images/news/security/d/data-breaches/w/wayback-machine/js-alert.jpg" src="https://www.bleepstatic.com/images/news/security/d/data-breaches/w/wayback-machine/js-alert.jpg"><figcaption><strong>JavaScript alert on Internet Archive warning about the breach</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>This misreporting frustrated the threat actor behind the actual data breach, who contacted BleepingComputer through an intermediary to claim credit for the attack and explain how they breached the Internet Archive.</p>
<p>The threat actor told BleepingComputer that the initial breach of Internet Archive started with them finding an exposed GitLab configuration file on one of the organization's development servers, <em>services-hls.dev.archive.org</em>.</p>
<p>BleepingComputer was able to confirm that this token has been exposed since at least December 2022, with it rotating multiple times since then.</p>
<div>
<figure><img alt="Exposed Internet Archive GitLab authentication token" height="600" width="860" data-src="https://www.bleepstatic.com/images/news/security/attacks/i/internet-archive/gitlab-tokens/gitlab-token.jpg" src="https://www.bleepstatic.com/images/news/security/attacks/i/internet-archive/gitlab-tokens/gitlab-token.jpg"><figcaption><strong>Exposed Internet Archive GitLab authentication token</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>The threat actor says this GitLab configuration file contained an authentication token allowing them to download the Internet Archive source code.</p>
<p>The hacker say that this source code contained additional credentials and authentication tokens, including the credentials to Internet Archive's database management system. This allowed the threat actor to download the organization's user database, further source code, and modify the site.</p>
<p>The threat actor claimed to have stolen 7TB of data from the Internet Archive but would not share any samples as proof.</p>
<p>However, now we know that the stolen data also included the API access tokens for Internet Archive's Zendesk support system.</p>
<p>BleepingComputer attempted contact the Internet Archive numerous times, as recently as on Friday, offering to share what we knew about how the breach occurred and why it was done, but we never received a response.</p>
<h2>Breached for cyber street cred</h2>
<p>After the Internet Archive was breached, conspiracy theories abounded about why they were attacked.</p>
<p>Some said&nbsp;Israel did it,&nbsp;the United States government, or corporations in their ongoing battle with the Internet Archive over copyright infringement.</p>
<p>However, the Internet Archive was not breached for political or monetary reasons but simply because the threat actor could.</p>
<p>There is a large community of people who traffic in stolen data, whether they do it for money by extorting the victim, selling it to other threat actors, or simply because they are collectors of data breaches.</p>
<p>This data is often released for free to gain <em>cyber street cred</em><strong>,&nbsp;</strong>increasing their reputation among other threat actors in this community&nbsp;as they all compete for who has the most significant and most publicized attacks.</p>
<p>In the case of the Internet Archive, there was no money to be made by trying to extort the organization. However, as a well-known and extremely popular website, it definitely boosted a person's reputation amongst this community.</p>
<p>While no one has publicly claimed this breach, BleepingComputer was told it was done&nbsp;while the threat actor was in a group chat with others, with many receiving some of the stolen data.</p>
<p>This database is now likely being traded amongst other people in the data breach community, and we will likely see it leaked for free in the future on hacking forums like Breached.</p>
<p><em>Update 10/20/24: Added information about how some people had to upload personal IDs when requesting removal from Internet Archive.</em></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The AI Investment Boom (197 pts)]]></title>
            <link>https://www.apricitas.io/p/the-ai-investment-boom</link>
            <guid>41895746</guid>
            <pubDate>Sun, 20 Oct 2024 14:56:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apricitas.io/p/the-ai-investment-boom">https://www.apricitas.io/p/the-ai-investment-boom</a>, See on <a href="https://news.ycombinator.com/item?id=41895746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><strong>Thanks for reading! If you haven’t subscribed, please click the button below:</strong></p><p><strong>By subscribing you’ll join over 45,000 people who read Apricitas!</strong></p><p><span>Last month, Microsoft made a high-profile announcement that it is paying to </span><a href="https://www.eia.gov/todayinenergy/detail.php?id=63304" rel="">reopen reactor one at the Three Mile Island nuclear plant to meet the company’s growing data center power demand, joining Amazon</a><span> as the second major US tech company to turn to legacy nuclear facilities for their increasing energy needs. Microsoft is the primary investor and computing provider for OpenAI, who kicked off a revolution in AI development with its release of ChatGPT less than two years ago—and the Three Mile Island reopening underscored the frenzied growth in physical investment currently going on to meet the demands of these new AI systems.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:215863,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5612c0b3-6bf4-44ba-b573-95ac968f3d4b_2886x1843.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Today, AI products are used ubiquitously to generate code, text, and images, analyze data, automate tasks, enhance online platforms, and much, much, much more—with usage expected only to increase going forward. Yet these cutting-edge models require enormous computing resources for their training and inference, that computing requires massive arrays of advanced hardware housed at industrial-scale facilities, and those facilities require access to vast quantities of power, water, broadband, and other infrastructure for their operations.</p><p><span>Thus, the downstream result of the AI boom has been a rapid increase in US fixed investment to meet the growth in computing demand, with hundreds of billions of dollars going to high-end computers, data center facilities, power plants, and more. Right now, US data center construction is at a record-high rate of $28.6B a year, up 57% from last year and 114% from only two years ago. For context, that’s roughly as much as America spends on </span><a href="https://www.census.gov/construction/c30/historical_data.html" rel="">restaurant, bar, and retail store construction combined.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:353767,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8c4ba116-ab33-45e6-a439-45e0af48c1cb_2886x1843.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>However, that construction figure is only for the physical buildings themselves—it excludes the massive racks of high-powered computers that form the brains of data centers plus the vast quantities of cables, fans, and other parts necessary to make that brain work. In August, net US imports of large computers (like those used for AI training) rose to a new record high, and net imports of computer parts, accessories, and other components had set a record high just the month before—in total, the US has brought in more than $65B across the two categories over the last year </span><a href="https://fred.stlouisfed.org/series/A34SVS" rel="">on top of rising domestic production.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:257436,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F034e4193-7897-4616-ba76-15e4c88b0cea_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The majority of these new data centers, computers, and equipment are being bought by companies in the information technology space—that includes computing infrastructure providers like Amazon, web search firms like Google, and software publishers like Microsoft. Those companies have increased their net holdings of property, plant, and equipment by more than $95B over the last year, a record high, as they each compete to rapidly scale up and deploy their AI systems.</p><p><span>It’s a stark change from a little over a decade ago, when Facebook bought up Instagram for only $1.2B, following it up by paying $15B for WhatsApp two years later. At the time, these acquisitions were some of the largest in tech history and marked the beginning of an era where lightweight software publishers were considered the industry’s future—in total, Instagram had only 13 employees at the time it was purchased, Whatsapp had only 55, and neither company had much of a physical presence beyond some office space and programmers’ workstations. Today, </span><a href="https://s21.q4cdn.com/399680738/files/doc_financials/2024/q2/META-Q2-2024-Earnings-Call-Transcript.pdf" rel="">Facebook (now Meta) has spent $15.2B on capital expenditures in the first half of 2024 alone</a><span>, much of it on massive arrays of computing infrastructure to support the company’s Llama brand of AI models. So far, the AI boom has been more hardware-intensive than any tech boom in history, and that is rapidly boosting construction and investment within the United States.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:212869,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe130439-788c-4884-91e1-7d9bd42f2c0a_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>American businesses’ investment in computers and related equipment has skyrocketed to a new record high amidst the AI boom, </span><a href="https://fred.stlouisfed.org/graph/?g=1wxmt" rel="">jumping 16.6% over the last year even after adjusting for inflation.</a><span> That’s in stark contrast to the nearly-decade-long relative stagnation in investment seen throughout the 2010s, which was only really shattered by the digital demands of the pandemic-era remote work boom. Computer investment retracted a bit in 2022 as work-from-home levels and internet usage stabilized, but it then came roaring back with the AI boom starting in late 2023.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:249266,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff8734e08-9b80-4c97-8197-e7661004686a_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Yet not all computers are created equal—total computer investment may be at record levels, but the growth in the highest-end computer systems has been even faster. Taiwan’s TSMC is the world’s leading manufacturer of cutting-edge semiconductors, and the ravenous demand for AI compute is visible in the growing amount of chips, computers, and related components that the US now imports from Taiwan. Those imports have totaled more than $38B over the last year, rising more than 140% over the previous year, with little sign of stopping. All three categories have seen rapid growth, but direct US imports of logic chips have seen the largest relative increase, rising from relatively minimal levels to nearly $5B a year. Computer parts and components remain the largest import item—a reminder that data centers require more than just computers for their day-to-day operation and even when operational require further supplies for their maintenance and repair.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:223124,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d8a83df-0f4a-4897-a01a-b9e2dfa61610_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://apps.bea.gov/iTable/?ReqID=10&amp;step=2&amp;_gl=1*gocdd0*_ga*NzU5NjY0MDc1LjE3MTgwMDA3NzE.*_ga_J4698JNNFT*MTcyOTQyNDU4OC44NC4wLjE3Mjk0MjQ1ODguNjAuMC4w#eyJhcHBpZCI6MTAsInN0ZXBzIjpbMiwzXSwiZGF0YSI6W1siVGFibGVfTGlzdCIsIjU3Il1dfQ==" rel="">Breaking down the detailed sector-level investment data available through 2023</a><span> shows that </span><a href="https://imgur.com/ECaAd3r" rel="">although data processors and web search firms like Amazon/Google continued to have the largest investment levels in the tech space</a><span>, it was software developers who saw the fastest investment growth. Software publishers’ real investment in intellectual property—which encompasses many of the AI models themselves plus related research and development—grew by 40% since 2021, while real investment in equipment like computers grew by an astonishing 96%. The era of leading software developers being hardware-light companies has been replaced by an era where developers are racing each other to see who can build out hardware capabilities the fastest.</span></p><p><span>All of this hardware investment is not, however, evenly spread throughout the country. While data centers have to be spread out to some extent in order to serve networking needs and avoid binding infrastructure constraints, it’s often beneficial to concentrate them in large clusters to multiply their effectiveness and reduce costs/latency. That’s especially true for AI, which is why firms are pushing the limits of data center size </span><a href="https://www.semianalysis.com/p/multi-datacenter-training-openais" rel="">and networking</a><span> to throw as much computing power at model development as possible.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:455992,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0135f5cc-fc51-43be-ac6d-be4e08c995c6_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>While we do not have granular data center construction data—official construction numbers only break down data center spending at the regional level—we can still see some interesting underlying patterns. The US data center buildout has remained strongest in its historical clusters within the American South, but growth has been much faster in markets throughout the Midwest and West Coast, while the Northeast has been functionally unaffected.</p><p><span>That buildout can have large implications for local power demand—over the last few months, the Energy Information Administration has </span><a href="https://www.eia.gov/outlooks/steo/pdf/steo_full.pdf" rel="">repeatedly raised its projections for load growth based on data center demand</a><span>, </span><a href="https://www.eia.gov/todayinenergy/detail.php?id=62409" rel="">now predicting that total commercial-sector electricity consumption will rise 3% this year and another 1% next year.</a><span> While those projections still leave commercial users as </span><a href="https://www.eia.gov/outlooks/steo/data/browser/#/?v=19&amp;f=A&amp;s=0&amp;start=2024&amp;end=2025&amp;chartindexed=2&amp;map=&amp;linechart=~ELCCTWH~ELRCTWH~ELICTWH&amp;id=&amp;ctype=linechart&amp;maptype=0" rel="">a smaller driver of rising power consumption than residential electrification and industrial reshoring</a><span>, they represent the sector’s fastest demand growth in years—for context, </span><a href="https://www.eia.gov/electricity/data/browser/#/topic/5?agg=0,1&amp;geo=g&amp;endsec=vg&amp;linechart=~~ELEC.SALES.US-COM.A~&amp;columnchart=ELEC.SALES.US-ALL.A~ELEC.SALES.US-RES.A~ELEC.SALES.US-COM.A~ELEC.SALES.US-IND.A&amp;map=ELEC.SALES.US-ALL.A&amp;freq=A&amp;ctype=linechart&amp;ltype=pin&amp;rtype=s&amp;maptype=0&amp;rse=0&amp;pin=" rel="">commercial power consumption rose only 5% in total between 2007 and 2023</a><span> and the </span><a href="https://www.eia.gov/energyexplained/electricity/use-of-electricity.php" rel="">pre-AI-boom official estimates put computers &amp; office equipment at only 11.4% of total commercial power consumption.</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:318101,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F908a35b0-18da-4548-989b-36eb2d74751b_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Yet in some parts of the country, data center power consumption has been a major driver of electricity load growth—to use an illustrative example, North Dakota’s commercial power consumption has risen by more than 45% after the </span><a href="https://www.governor.nd.gov/news/burgum-one-worlds-largest-data-centers-locate-williston-area-industry-targets-growth-nd" rel="">opening of several key data centers in 2022.</a><span> However, North Dakota is a relatively tiny power and computing market, so the most significant increases in raw power demand have instead come from larger data center clusters in larger states like Virginia and Texas.</span></p><p><span>The </span><a href="https://x.com/JosephPolitano/status/1846265551611707500" rel="">byteway</a><span> in the Northern Virginia suburbs of DC is the largest cluster of computing power in the world, </span><a href="https://www.eia.gov/todayinenergy/detail.php?id=62409" rel="">and it’s caused the state to see a 30% increase in commercial energy consumption since 2019 and the largest raw increase in commercial power demand in the nation.</a><span> Texas, which has explicitly worked to attract data centers and crypto miners as part of its energy load management program, has also seen a 10% increase in commercial power consumption since 2019, </span><a href="https://www.eia.gov/todayinenergy/detail.php?id=63344" rel="">with much larger growth expected in the coming years.</a></p><p><span>That data center load growth has been a contributor to </span><a href="https://www.apricitas.io/p/the-regional-impacts-of-americas" rel="">the Lone Star State’s notable overperformance in renewable investment, where it leads the rest of the country significantly.</a><span> Indeed, ERCOT (Texas’ power grid) and PJM (which serves Virginia) are both currently </span><a href="https://www.eia.gov/outlooks/steo/data/browser/#/?v=22&amp;f=A&amp;s=0&amp;start=2023&amp;end=2025&amp;chartindexed=1&amp;map=&amp;linechart=RTEPGEN_US~RNEPGEN_TX~RNEPGEN_PJ~&amp;maptype=0&amp;ctype=linechart&amp;id=" rel="">projected to outpace the nation in renewables growth through this year and 2025.</a><span> The agglomeration benefits of data centers mean that AI firms are increasingly looking to concentrate near large power resources, hence the renewed focus on nuclear energy and the growing desire for tech companies to directly invest in power generation infrastructure as they build computing capabilities.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:428185,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ab4a7b9-6565-4d42-9036-e86ece5f2435_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Amidst the AI boom, revenue in the information technology space has rebounded from the slowdown of 2022 and 2023—software publishers, web search portals, and computing infrastructure providers have all seen their incomes rise by 12-15% over the last year. It’s a far cry from the halcyon days of 2021, but still puts revenue growth on the strong side of pre-COVID norms.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png" width="1456" height="930" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:930,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:312405,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42130026-484f-4ff4-8227-31fe24156b2b_2886x1843.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Yet despite the tech sector’s recent rebound in revenues and boom in physical investment, employment growth has remained remarkably weak. The US has added only 32k tech jobs over the last year, lower than at any point in 2021, 2022, or the 9 years preceding the pandemic. Even the software publishers and computing infrastructure industries at the forefront of this AI boom have seen functionally zero net employment growth over the last year—the dismal job market that has beleaguered recent computer science graduates simply has not improved much.</p><p><span>That’s not to say there’s been no labor market impact of the AI investment boom, but rather that they’ve been primarily outside of traditional information tech sectors. </span><a href="https://imgur.com/b4lxZ7h" rel="">Total compensation in semiconductor manufacturing increased 25% from Q1 2023 to Q1 2024</a><span> as workers in companies like NVIDIA got much more valuable stock options. Some of the </span><a href="https://data.bls.gov/dataViewer/view/timeseries/CEU2023622001" rel="">30k increase in commercial construction jobs over the last year</a><span> is certainly downstream of data center demand—</span><a href="https://www.apricitas.io/p/the-regional-impacts-of-americas" rel="">that’s in addition to the ongoing job boom in industrial construction for chip fabs and other manufacturing sectors, plus the employment gains as part of the electricity power and broader infrastructure buildout.</a><span> Yet so far, the job dynamics of the AI boom have been radically different than the past decade of tech labor markets as growth focuses more on hardware investments, manufacturing/design firms, and infrastructure builders more than traditional programmers.</span></p><p>Right now, AI developers are competing intensely and each banking that continued product improvements and greater commercialization will more than validate the historical scale of current investments. In the near term, investment is only expected to increase as more advanced models are developed and AI usage is expanded into more real-world applications (like self-driving vehicles). Policymakers also view AI as a key part of the future US economy—AI development and data center capacity are cutting-edge industries where America has built a significant lead by virtue of the dominance of Silicon Valley and large US tech conglomerates, and thus the AI boom has benefitted US investment more than perhaps any other country.</p><p><span>Yet that makes it more likely geopolitical competition will intensify around hardware capacity—the CHIPS Act that is driving so much of current US electronics industrial policy was a pre-ChatGPT creation, and some industry heads already complain that it’s showing its age in terms of priorities and scale. The significant increase in demand for high-end semiconductors has boosted US reliance on Taiwanese imports, which the CHIPS Act was supposed to help ameliorate, and there are any number of components where the US remains dependent on China to meet data-center-scale supply. Plus, the US will likely continue restricting Chinese access to the highest-end chips in hopes of holding back their AI development, while China continues to build out its chipmaking capacity in hopes of reducing import dependence. As this AI investment boom continues, </span><a href="https://www.apricitas.io/p/america-and-chinas-chip-race" rel="">expect it to only move further into the forefront of the ongoing Chip War.</a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Syncthing Android App Discontinued (363 pts)]]></title>
            <link>https://old.reddit.com/r/Syncthing/comments/1g7zpvm/syncthing_android_app_discontinued/</link>
            <guid>41895718</guid>
            <pubDate>Sun, 20 Oct 2024 14:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/Syncthing/comments/1g7zpvm/syncthing_android_app_discontinued/">https://old.reddit.com/r/Syncthing/comments/1g7zpvm/syncthing_android_app_discontinued/</a>, See on <a href="https://news.ycombinator.com/item?id=41895718">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/Syncthing/comments/1g7zpvm/syncthing_android_app_discontinued/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Energy-based model explains how chronic stress transforms into disease over time (153 pts)]]></title>
            <link>https://www.sciencedirect.com/science/article/pii/S030645302200292X</link>
            <guid>41895609</guid>
            <pubDate>Sun, 20 Oct 2024 14:32:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedirect.com/science/article/pii/S030645302200292X">https://www.sciencedirect.com/science/article/pii/S030645302200292X</a>, See on <a href="https://news.ycombinator.com/item?id=41895609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root" data-aa-name="root"><header id="gh-cnt"></header><div id="mathjax-container" role="main"><div role="region" aria-label="Download options and search"><ul aria-label="PDF Options"><li><a target="_blank" aria-label="View PDF. Opens in a new window."><svg focusable="false" viewBox="0 0 35 32" height="20"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span><span><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li></li></ul></div><div><article lang="en"><div id="publication"><p><a href="https://www.sciencedirect.com/journal/psychoneuroendocrinology" title="Go to Psychoneuroendocrinology on ScienceDirect"><span><span><img src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/17220c80f4e09945173d394db9fa2f4c6a17b614/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></p><p><a href="https://www.sciencedirect.com/journal/psychoneuroendocrinology/vol/146/suppl/C"><span><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0306453022X00103-cov150h.gif" alt="Psychoneuroendocrinology"></span></span></a></p></div><div><p><span>Under a Creative Commons </span><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noreferrer noopener"><span><span>license</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></p><p><span></span>open access</p></div><div id="abstracts"><div id="ab0015"><h2>Highlights</h2><div id="abs0015"><ul><li><span>•</span><span><p>Allostasis and allostatic load cost energy</p></span></li><li><span>•</span><span><p>The organism’s energy consumption capacity is biologically limited</p></span></li><li><span>•</span><span><p>The transition from allostasis to allostatic load is defined by an energetic tradeoff where allostasis and stress-related energy costs compete with growth, maintenance, and repair</p></span></li><li><span>•</span><span><p>The energetic model of allostatic load (EMAL) makes testable predictions requiring further research</p></span></li></ul></div></div><div id="ab0010"><h2>Abstract</h2><div id="abs0010"><p>Chronic psychosocial stress increases disease risk and mortality, but the underlying mechanisms remain largely unclear. Here we outline an energy-based model for the transduction of chronic stress into disease over time. The energetic model of allostatic load (EMAL) emphasizes the energetic cost of allostasis and allostatic load, where the “load” is the additional energetic burden required to support allostasis and stress-induced energy needs. Living organisms have a limited capacity to consume energy. Overconsumption of energy by allostatic brain-body processes leads to <em>hypermetabolism</em>, defined as excess energy expenditure above the organism’s optimum. In turn, hypermetabolism accelerates physiological decline in cells, laboratory animals, and humans, and may drive biological aging. Therefore, we propose that the transition from adaptive allostasis to maladaptive allostatic states, allostatic load, and allostatic overload arises when the added energetic cost of stress competes with longevity-promoting growth, maintenance, and repair. Mechanistically, the energetic restriction of growth, maintenance and repair processes leads to the progressive wear-and-tear of molecular and organ systems. The proposed model makes testable predictions around the physiological, cellular, and sub-cellular energetic mechanisms that transduce chronic stress into disease risk and mortality. We also highlight new avenues to quantify allostatic load and its link to health across the lifespan, via the integration of systemic and cellular energy expenditure measurements together with classic allostatic load biomarkers.</p></div></div></div><ul id="issue-navigation"><li></li><li></li></ul><div id="keys0005"><h2>Keywords</h2><p><span>Allostatic load</span></p><p><span>Energy</span></p><p><span>Hypermetabolism</span></p><p><span>Coping resources</span></p><p><span>Energetic model of allostatic load (EMAL)</span></p><p><span>Allostasis and stress-induced energy expenditure (ASEE)</span></p><p><span>Brain</span></p><p><span>Mitochondria</span></p></div><section aria-label="Cited by" id="section-cited-by"><header id="citing-articles-header"><h2>Cited by (0)</h2></header></section><p><span>© 2022 The Author(s). Published by Elsevier Ltd.</span></p></article></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VersaTiles – a complete FLOSS map stack (124 pts)]]></title>
            <link>https://versatiles.org/</link>
            <guid>41895356</guid>
            <pubDate>Sun, 20 Oct 2024 13:51:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://versatiles.org/">https://versatiles.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41895356">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div id="heroblock">
	<p><img src="https://versatiles.org/assets/logo/versatiles.svg"></p><div id="heroline">
		
		<p>
			<span>a</span>
			<span>complete</span>
			<span><abbr title="Free, Libre and Open Source Software">FLOSS</abbr></span>
			<span>map</span>
			<span>stack</span>
		</p>
	</div>
</div>
<p><hero>VersaTiles is a completely <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software" title="Free, Libre and Open Source Software">FLOSS</a> stack for generating, distributing, and using map tiles based on OpenStreetMap data, free of any commercial interests.</hero></p>
<h2>Try it out</h2>





<h2>If you want to know more</h2>
<p>we explain here:</p>
<ul>
<li><a href="https://versatiles.org/intro.html">how to use it</a>,</li>
<li><a href="https://versatiles.org/overview.html">how it works</a> and</li>
<li><a href="https://versatiles.org/contribute.html">how you can help.</a></li>
</ul>
<h2>powered by</h2>
<p><a href="https://www.miz-babelsberg.de/foerderung/foerderprojekte-alumni/details/versatiles-editorial-tools.html"><img src="https://versatiles.org/assets/logo/miz-logo.png" width="281"></a></p>
<p><small>MIZ-Babelsberg is funding the development of the "VersaTiles Editorial Tools", which are specifically designed for the use of maps in journalistic newsrooms.</small></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sampling with SQL (105 pts)]]></title>
            <link>https://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html</link>
            <guid>41894528</guid>
            <pubDate>Sun, 20 Oct 2024 10:58:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html">https://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html</a>, See on <a href="https://news.ycombinator.com/item?id=41894528">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
<p>Sampling is one of the most powerful tools you can wield to extract meaning from large datasets.
It lets you reduce a massive pile of data into a small yet representative dataset that’s fast and easy to use.</p>
<p>If you know how to take samples using SQL, the ubiquitous query language, you’ll be able to take samples anywhere.
No dataset will be beyond your reach!</p>
<p>In this post, we’ll look at some clever algorithms for taking samples.
These algorithms are fast and easily translated into SQL.</p>
<p>First, however, I’ll note that many database systems have some built-in support for taking samples.
For example, some SQL dialects support a <code>TABLESAMPLE</code> clause.
If your system has built-in support—and it does what you need—using it will usually be your best option.</p>
<p>Often, though, the built-in support is limited to simple cases.
Let’s consider some realistic scenarios that are more challenging:</p>
<ul>
<li>We want to be able to take samples <em>with</em> and <em>without</em> replacement.</li>
<li>We want to take <em>weighted</em> samples in which each item in the input dataset is selected with probability in proportion to its corresponding weight.</li>
<li>We want to support the full range of weights we might expect to see in a FAANG-sized dataset, say between <span>\(0\)</span> to <span>\(10^{20}\)</span> for frequency distributions (e.g., clicks or impressions or RPC events) and between <span>\(0\)</span> to <span>\(1\)</span> with values as small as <span>\(10^{-20}\)</span> for normalized probability distributions. In other words, weights are non-negative numbers, possibly very large or very small.</li>
<li>We want to take deterministic samples. This property lets us take repeatable samples and, in some cases, helps query planners produce faster queries.</li>
</ul>
<h2 id="sampling-without-replacement-the-a-es-algorithm-in-sql">Sampling without replacement: the A-ES algorithm in SQL</h2>
<p>In 2006, Pavlos S. Efraimidis and Paul G. Spirakis published a one-pass algorithm for drawing a weighted random sample, without replacement, from a population of weighted items.
It’s quite simple:</p>
<p>Given a population <span>\(V\)</span> indexed by <span>\(i = 1\ldots{}n\)</span> and having weights <span>\(w_i\)</span>:</p>
<ol type="1">
<li>For each <span>\(v_i\)</span> in <span>\(V,\)</span> let <span>\(u_i = \mathrm{random}(0, 1)\)</span> and <span>\(k_i = u_i^{1/w_i}\)</span>.</li>
<li>Select the <span>\(m\)</span> items with the largest keys <span>\(k_i\)</span>.</li>
</ol>
<p>That algorithm has a straightforward implementation in SQL:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>SELECT</span> <span>*</span></span>
<span id="cb1-2"><span>FROM</span> Population</span>
<span id="cb1-3"><span>WHERE</span> weight <span>&gt;</span> <span>0</span></span>
<span id="cb1-4"><span>ORDER</span> <span>BY</span> <span>-</span><span>LN</span>(<span>1.0</span> <span>-</span> <span>RANDOM</span>()) <span>/</span> weight</span>
<span id="cb1-5"><span>LIMIT</span> <span>100</span>  <span>-- Sample size.</span></span></code></pre></div>
<p>You’ll note that we changed the ordering logic a bit.
A straight translation would have been</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>ORDER</span> <span>BY</span> POW(<span>RANDOM</span>(), <span>1.0</span> <span>/</span> weight) <span>DESC</span></span></code></pre></div>
<p>Our translation</p>
<div id="cb3"><pre><code><span id="cb3-1"><span>ORDER</span> <span>BY</span> <span>-</span><span>LN</span>(<span>1.0</span> <span>-</span> <span>RANDOM</span>()) <span>/</span> weight</span></code></pre></div>
<p>is more numerically stable and also helps to show the connection between the algorithm and the fascinating theory of Poisson processes.
This connection makes it easier to understand how the algorithm works.
More on that in a moment.</p>
<h2 id="numerical-stability-and-other-tweaks">Numerical stability and other tweaks</h2>
<p>First, the numerical stability claim.
Assume that our SQL system uses <a href="https://en.wikipedia.org/wiki/IEEE_754-1985#Double_precision">IEEE double-precision floating-point values</a> under the hood.
When the weights are large, say on the order of <span>\(w_i = 10^{17}\)</span>, it doesn’t matter what the random value <span>\(u_i\)</span> is.
The corresponding sort key <span>\(k_i = u_i^{1/w_i}\)</span> will almost always be <span>\(1.0\)</span>.
Consider the interval <span>\(0.01 \le u_i \le 1\)</span>, representing 99% of the possible random values <span>\(u_i\)</span>.
This entire interval gets mapped to <span>\(1.0\)</span> when <span>\(w_i = 10^{17}\)</span>:</p>
<div id="cb4"><pre><code><span id="cb4-1"><span># Python.</span></span>
<span id="cb4-2"><span>&gt;&gt;&gt;</span> w_i <span>=</span> <span>1e17</span></span>
<span id="cb4-3"><span>&gt;&gt;&gt;</span> math.<span>pow</span>(<span>0.01</span>, <span>1.0</span><span>/</span>w_i) <span>==</span> math.<span>pow</span>(<span>1.0</span>, <span>1.0</span><span>/</span>w_i) <span>==</span> <span>1.0</span></span>
<span id="cb4-4"><span>True</span></span></code></pre></div>
<p>Likewise, when weights are small, say <span>\(w_i = 10^{-17}\)</span>, the corresponding sort key will almost always be zero.
Consider the interval <span>\(0 \le u_i \le 0.99\)</span>, representing 99% of the possible random values <span>\(u_i\)</span>:</p>
<div id="cb5"><pre><code><span id="cb5-1"><span>&gt;&gt;&gt;</span> w_i <span>=</span> <span>1e-17</span></span>
<span id="cb5-2"><span>&gt;&gt;&gt;</span> math.<span>pow</span>(<span>0.0</span>, <span>1.0</span><span>/</span>w_i) <span>==</span> math.<span>pow</span>(<span>0.99</span>, <span>1.0</span><span>/</span>w_i) <span>==</span> <span>0.0</span></span>
<span id="cb5-3"><span>True</span></span></code></pre></div>
<p>For very large (or small) weights, then, the straightforward implementation doesn’t work.
The wanted sort ordering is destroyed when very large (or small) powers cause what should be distinct sort keys to collapse into indistinguishable fixed points.</p>
<p>Fortunately, logarithms are <a href="https://en.wikipedia.org/wiki/Monotonic_function">order-preserving</a> transformations, so sorting by <span>\(\ln(u_i^{1/w_i}) = \ln(u_i) / w_i\)</span> produces the same ordering as sorting by <span>\(u_i^{1/w_i}\)</span> when we’re using mathematically pure real numbers.
But the log-transformed version is much more stable when using floating-point numbers.
Distinct random inputs <span>\(u_i\)</span> now produce reliably distinct sort keys <span>\(k_i\)</span>, even when the input weights <span>\(w_i\)</span> are very large or very small:</p>
<pre><code>&gt;&gt;&gt; [math.log(u_i) / 1e17 for u_i in (0.01, 0.010001, 0.99, 0.990001)]
[-4.605170185988091e-17, -4.605070190987759e-17,
 -1.005033585350145e-19, -1.0049325753001471e-19]

&gt;&gt;&gt; [math.log(u_i) / 1e-17 for u_i in (0.01, 0.010001, 0.99, 0.990001)]
[-4.605170185988091e+17, -4.605070190987758e+17,
 -1005033585350145.0, -1004932575300147.1]</code></pre>
<!-- ![Graph showing that as the weights w_i reach 10 to the power of 14, a substantial share of the possible random numbers u_i get mapped by our sort-keying function to a single value.](../images/public_html/blog/pix-20240601/deadspace_plot.svg) -->
<p>As a final tweak, we negate the sort keys so that instead of sorting by <span>\(u_i^{1/w_i}\)</span> <strong>descending</strong>, as in the original algorithm, we do an equivalent sort by <span>\(-\ln(u_i) / w_i\)</span> <strong>ascending</strong>.
Note the leading minus sign.
The rationale for flipping the sign will become apparent when we discuss Poisson processes in the next section.</p>
<p>One last numerical subtlety.
Why do we generate random numbers with the expression <code>1.0 - RANDOM()</code> instead of just <code>RANDOM()</code>?
Since most implementations of <code>RANDOM()</code>, such as the <a href="https://www.pcg-random.org/using-pcg-c-basic.html#generating-doubles">PCG</a> implementation used by DuckDB, return a floating-point value in the semi-closed range <span>\([0, 1)\)</span>, they can theoretically return zero.
And we don’t want to take the logarithm of zero.
So we instead use <code>1.0 - RANDOM()</code> to generate a random number in the semi-closed range <span>\((0, 1]\)</span>, which excludes zero.</p>
<h2 id="does-this-algorithm-actually-work">Does this algorithm actually work?</h2>
<p>First, what do we mean by <em>work</em>?
In this case, we’ll say that we want the algorithm to produce samples that are equivalent to a succession of random draws, each draw removing an item from the population, and each draw fair with respect to the population that remains <em>at the time of the draw</em>.
Because the population grows slightly smaller with each draw, the draws in the sample are not <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independent and identically distributed</a> (iid).
In practice, however, when your samples are very small compared to your population of interest, and your weights are such that it’s unlikely that iid draws would draw the same item more than once, you can generally pretend that the draws are iid and get away with it.
(When you can’t get away with it, you can use a reweighting scheme to extract unbiased estimates anyway. This subject is worth its own post, so I won’t say more here.)</p>
<p>Anyway, it’s not obvious that assigning a random number <span>\(u_i\)</span> to every row <span>\(i\)</span>, then sorting rows on <span>\(-\ln(u_i)/w_i\)</span>, and finally taking the top <span>\(m\)</span> rows is a recipe that would result in a sample that has the wanted properties.
But it does.</p>
<p>The clearest way to understand what’s going on is to first take a detour through the fascinating theory of <a href="https://ocw.mit.edu/courses/6-262-discrete-stochastic-processes-spring-2011/3a19ce0e02d0008877351bfa24f3716a_MIT6_262S11_chap02.pdf">Poisson processes</a>.
In short, <strong>a Poisson process with rate <span>\(\lambda\)</span> is a sequence of arrivals such that the times between successive arrivals are all independent, <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponentially distributed</a> random variables with rate <span>\(\lambda\)</span>.</strong>
<!-- For example, say $T_1 < T_2 < T_3 < \cdots$ is a sequence of arrival times, and $X_i = T_{i} - T_{i-1}$ for $i > 1$. Then if all $X_i$ are i.i.d. exponentially distributed with rate $\lambda$, the sequence is a Poisson process with rate $\lambda$. --></p>
<p>Poisson processes have some important (and useful!) properties:</p>
<ol type="1">
<li><em>They are memoryless</em>. No matter what has previously happened, the time until the next arrival from a Poisson process with rate <span>\(\lambda\)</span> is exponentially distributed with rate <span>\(\lambda\)</span>.</li>
<li><em>They can be merged.</em> If you have two Poisson processes with rates <span>\(\lambda_1\)</span> and <span>\(\lambda_2\)</span>, the arrivals from both processes form a combined Poisson process with rate <span>\(\lambda_1 + \lambda_2\)</span>. This holds for any number of processes.</li>
<li><em>They win races in proportion to their rates.</em> In a race between the very next arrival from a Poisson process with rate <span>\(\lambda_1\)</span> and the very next arrival from a Poisson process with rate <span>\(\lambda_2\)</span>, the probability that the first process will win the race is <span>\(\lambda_1 / (\lambda_1 + \lambda_2)\)</span>.</li>
</ol>
<p>Now that we know the basics of Poisson processes, there’s just one more tidbit we need:</p>
<ul>
<li><em>The uniform–exponential bridge.</em> If <span>\(X\)</span> is a random variable having a uniform distribution between zero and one, then <span>\(-\ln(X)/\lambda\)</span> has an exponential distribution with rate <span>\(\lambda\)</span>.</li>
</ul>
<p>With the uniform–exponential bridge in mind, we can begin to see what the algorithm is doing when it assigns every row a key <span>\(k_i = -\ln(u_i)/w_i\)</span> and sorts the population by that key.
It’s running a race over all the rows in the population!
In this race, each row arrives at the finish line at a time that’s an exponentially distributed random variable with a rate corresponding to the row’s weight <span>\(w_i\)</span>.
The first <span>\(m\)</span> arrivals form the sample.</p>
<p>To prove that this race does the sampling that we want, we will show that it is equivalent to a succession of one-row draws, each draw being fair with respect to the population that remains at the time of the draw.
Let the population’s total weight be <span>\(w\)</span>, and consider an arbitrary row <span>\(i\)</span> with weight <span>\(w_i\)</span>.
The algorithm will assign it an exponentially distributed random variable with rate <span>\(w_i\)</span>, which corresponds to the very next arrival from a Poisson process with the same rate.</p>
<p>Now consider all rows except <span>\(i\)</span>.
They too correspond to Poisson processes with rates equal to their weights.
And we can merge them into a combined process with rate <span>\(\sum_{j \ne i} w_j = w - w_i\)</span>.</p>
<p>Now, using the rule about Poisson races, we know that row <span>\(i\)</span>, represented by a process with rate <span>\(\lambda_1 = w_i\)</span>, will win the race against those other rows, represented by a combined process with rate <span>\(\lambda_2 = w - w_i\)</span>, with probability</p>
<p><span>\[\frac{\lambda_1}{\lambda_1 + \lambda_2} = \frac{w_i}{w_i + (w - w_i)} = \frac{w_i}{w}.\]</span></p>
<p>And since we chose row <span>\(i\)</span> arbitrarily, the same argument applies to all rows.
Thus every row’s probability of being drawn is equal to its weight in proportion to the population’s total weight.
This proves that running a “race of exponentials” lets us perform one fair draw from a population.</p>
<p>But, after we’ve drawn one row, what’s left but a new, slightly smaller population?
And can’t we run a new race on this slightly smaller population to correctly draw another row?</p>
<p>We can.
And, since Poisson processes are memoryless, we do not have to generate new arrival times to run this new race.
We can reuse the existing arrival times because the arrivals that have already happened have no effect on later arrivals.
Thus the next row we draw using the leftover arrival times will be another fair draw.</p>
<p>We can repeat this argument to show that successive rows are chosen fairly in relation to the population that remains at the time of each draw.
Thus algorithm A-ES selects a sample of size <span>\(m\)</span> by making <span>\(m\)</span> successive draws, each fair with respect to its remaining population.
And that’s the proof.</p>
<h2 id="tricks-for-faster-samples">Tricks for faster samples</h2>
<p>Most large analytical datasets will be stored in a column-oriented storage format, such as Parquet.
When reading from such datasets, you typically only have to pay for the columns you read.
(By “pay”, I mean wait for the query engine to do its work, but if you’re running your query on some tech company’s cloud, you may actually pay in currency too.)</p>
<p>For example, if your dataset contains a table having 100 columns but you need only four of them, the query engine will usually only read those four columns.
In row-oriented data stores, by contrast, you’ll generally have to decode entire rows, even if you only want four out of the 100 values in each row.
Additionally, most column-oriented stores support some kind of filter pushdown, allowing the storage engine to skip rows when a filtering expression evaluates to false.
These two properties—pay for what you read and filter pushdown—are ones we can exploit when taking samples.</p>
<p>Say we have a Population table with billions of rows and around 100 columns.
How can we efficiently take a weighted sample of 1000 rows?</p>
<p>We could use the basic sampling formulation, as discussed earlier:</p>
<div id="cb7"><pre><code><span id="cb7-1"><span>SELECT</span> <span>*</span></span>
<span id="cb7-2"><span>FROM</span> Population</span>
<span id="cb7-3"><span>WHERE</span> weight <span>&gt;</span> <span>0</span></span>
<span id="cb7-4"><span>ORDER</span> <span>BY</span> <span>-</span><span>LN</span>(<span>1.0</span> <span>-</span> <span>RANDOM</span>()) <span>/</span> weight</span>
<span id="cb7-5"><span>LIMIT</span> <span>1000</span>  <span>-- Sample size.</span></span></code></pre></div>
<p>But think about what the query engine must do to execute this query.
It must read and decode all 100 columns for all of those billions of rows so that it may pass those rows into the sort/limit logic (typically implemented as a <code>TOP_N</code> operation) to determine which rows to keep for the sample.
Even though the sample will keep only 0.00001% of those rows, you’ll have to pay to read the entire Population table!</p>
<p>A much faster approach is to only read the columns we need to determine <em>which</em> rows are in the sample.
Say our table has a primary key <code>pk</code> that uniquely identifies each row.
The following variant on our sampling formulation returns only the primary keys needed to identify the rows in the sample:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>SELECT</span> pk</span>
<span id="cb8-2"><span>FROM</span> Population</span>
<span id="cb8-3"><span>WHERE</span> weight <span>&gt;</span> <span>0</span></span>
<span id="cb8-4"><span>ORDER</span> <span>BY</span> <span>-</span><span>LN</span>(<span>1.0</span> <span>-</span> <span>RANDOM</span>()) <span>/</span> weight</span>
<span id="cb8-5"><span>LIMIT</span> <span>1000</span>  <span>-- Sample size.</span></span></code></pre></div>
<p>This variant only forces the query engine to read two columns: <code>pk</code> and <code>weight</code>:
Yes, it still must read those two columns for the billions of rows in the table, but those columns contain small values and can be scanned quickly.
After all, that’s what column-oriented stores are designed to do well.
The point is that we’re not paying to read about 100 additional columns whose values we’re just going to throw away 99.99999% of the time.</p>
<p>One we have identified the rows in our sample, we can run a second query to pull in the full set of wanted columns for just those rows.</p>
<h2 id="adding-determinism">Adding determinism</h2>
<p>Our sampling algorithm depends on randomization.
If we run our algorithm twice with the same inputs, we’ll get different results each time.
Often, that nondeterminism is exactly what we want.</p>
<p>But sometimes it isn’t.
Sometimes, it’s useful to be able to <em>control</em> the dice rolls that the algorithm depends on.
For example, sometimes it’s useful to be able to repeat a sample.
Or <em>almost</em> repeat a sample.</p>
<p>To allow us to control the nature of the randomization used when we take samples, we must replace calls to <code>RANDOM</code> with a deterministic pseudorandom function.
One common approach is to hash a primary key and then map the hashed value to a number in the range <span>\([0, 1)\)</span>.
The following DuckDB macro <code>pseudorandom_uniform</code> will do exactly that:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>-- Returns a pseudorandom fp64 number in the range [0, 1). The number</span></span>
<span id="cb9-2"><span>-- is determined by the given `key`, `seed` string, and integer `index`.</span></span>
<span id="cb9-3"><span>CREATE</span> MACRO pseudorandom_uniform(<span>key</span>, seed, <span>index</span>)</span>
<span id="cb9-4"><span>AS</span> (</span>
<span id="cb9-5">  (<span>HASH</span>(<span>key</span> <span>||</span> seed <span>||</span> <span>index</span>) <span>&gt;&gt;</span> <span>11</span>) <span>*</span> POW(<span>2.0</span>, <span>-</span><span>53</span>)</span>
<span id="cb9-6">);</span></code></pre></div>
<p>We can vary the <code>seed</code> and <code>index</code> parameters to generate independent random values for the same <code>key</code>.
For example, if I fix the <code>seed</code> to “demo-seed-20240601” and generate random numbers for the <code>key</code> “key123” over the <code>index</code> values <span>\(1, 2, \ldots, 10\)</span>, I get 10 fresh random numbers:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>SELECT</span></span>
<span id="cb10-2">  pseudorandom_uniform(<span>'key123'</span>, <span>'demo-seed-20240601'</span>, i) <span>AS</span> u_key123</span>
<span id="cb10-3"><span>FROM</span> <span>RANGE</span>(<span>1</span>, <span>11</span>) <span>AS</span> t(i);</span></code></pre></div>
<table>
<caption>Ten random numbers for the key “key123” and seed “demo-seed-20240601”.</caption>
<thead>
<tr>
<th><em>i</em></th>
<th><em>u_key123</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.9592606495318252</td>
</tr>
<tr>
<td>2</td>
<td>0.6309411348395693</td>
</tr>
<tr>
<td>3</td>
<td>0.5673207749533353</td>
</tr>
<tr>
<td>4</td>
<td>0.11182926321927167</td>
</tr>
<tr>
<td>5</td>
<td>0.3375806483238627</td>
</tr>
<tr>
<td>6</td>
<td>0.12881607107157678</td>
</tr>
<tr>
<td>7</td>
<td>0.6993372364353198</td>
</tr>
<tr>
<td>8</td>
<td>0.94031652266991</td>
</tr>
<tr>
<td>9</td>
<td>0.17893798791559323</td>
</tr>
<tr>
<td>10</td>
<td>0.6903126337753016</td>
</tr>
</tbody>
</table>
<p>To take deterministic samples, we just replace calls to <code>RANDOM()</code> with calls to our function <code>pseudorandom_uniform()</code>.</p>
<p>Now that we can take deterministic samples, we can do even more useful things!
For example, we can take samples <em>with</em> replacement.</p>
<h2 id="sampling-with-replacement">Sampling with replacement</h2>
<p>Earlier, we proved that the A-ES algorithm allows us to take a sample without replacement as a series of successive draws, each draw removing an item from the population, and each draw fair with respect to the population that remains at the time of the draw.
But what if we wanted to take a sample <em>with replacement</em>?
A sample with replacement requires us to return each item to the population as it is selected so that every selection is fair with respect to the <em>original</em> population, and individual items may be selected more than once.</p>
<p>Can we efficiently implement sampling with replacement in SQL?
Yes!
But it’s a little trickier.
(I haven’t found this algorithm published anywhere; please let me know if you have.
It took me some effort to create, but I wouldn’t be surprised if it’s already known.)</p>
<p>Think back to our correctness proof for the A-ES algorithm.
For each row <span>\(i\)</span> having a weight <span>\(w_i\)</span>, the algorithm imagined a corresponding Poisson process with rate <span>\(\lambda_i = w_i\)</span> and represented the row by the very next arrival from that process.
That arrival would occur at time <span>\(k_i = -\ln(u_i) / w_i\)</span>, where <span>\(u_i\)</span> is a uniformly distributed random number in the range <span>\((0, 1]\)</span>.
Then the algorithm sorted all rows by their <span>\(k_i\)</span> values and took the first <span>\(m\)</span> arrivals as the sample.</p>
<p>With one minor tweak to this algorithm, we can take a sample with replacement.
That tweak is to consider not just the <em>very next</em> arrival from each row’s Poisson process but <em>all</em> arrivals.
Let <span>\(k_{i,j}\)</span> denote the <span>\(j\)</span>th arrival from row <span>\(i\)</span>’s process.
Since we know that in a Poisson process the times between successive arrivals are exponentially distributed random variables, we can take the running sum over interarrival times to give us the needed arrival times.
That is, <span>\(k_{i,j} = \sum_{r=1}^{j} -\ln(u_{i,r}) / w_i\)</span>, where <span>\(u_{i,r}\)</span> represents the <span>\(r\)</span>th uniformly distributed random variable for row <span>\(i\)</span>.</p>
<p>One minor problem with this tweaked algorithm is that a Poisson process generates a theoretically infinite series of arrivals.
Creating an infinite series for each row and then sorting the arrivals from all of these series is intractable.</p>
<p>Fortunately, we can avoid this problem!
Think about how the A-ES algorithm for taking a sample <em>without</em> replacement relates to our proposed intractable algorithm for taking a sample <em>with</em> replacement.
We could describe the <em>without</em> algorithm in terms of the <em>with</em> algorithm like so:
Prepare to take a sample <em>with</em> replacement, but then ignore all arrivals <span>\(k_{i,j}\)</span> for <span>\(j &gt; 1\)</span>; the remaining arrivals must be of the form <span>\(k_{i,1}\)</span>, where <span>\(i\)</span> indicates the corresponding row.
Then take the first <span>\(m\)</span> of these remaining arrivals as your sample, as before.</p>
<p>Now think about going the other way, from having a <em>without</em>-replacement sample and needing to construct a corresponding <em>with</em>-replacement sample.
Let <span>\(S\)</span> be the set of rows sampled <em>without</em> replacement.
We know these rows were represented by a corresponding set of arrival times <span>\(k_{i,1}\)</span> for <span>\(i\)</span> in <span>\(S\)</span>.
We also know that, had the sample been taken <em>with</em> replacement, the race would have included some additional arrival times <span>\(k_{i,j}\)</span> for <span>\(j &gt; 1\)</span> that could have displaced some of the winning rows in <span>\(S\)</span>.
But, crucially, we also know that if <span>\(S_{*}\)</span> represents the set of rows in the corresponding sample <em>with</em> replacement, then <span>\(S_{*}\)</span> must be contained within <span>\(S\)</span>.
This claim follows from the fact that if any arrival <span>\(k_{i,j}\)</span> for <span>\(j &gt; 1\)</span> does displace a row among the first <span>\(m\)</span> arrivals, the <span>\(j&gt;1\)</span> requirement implies that the displacing row is a duplicate of some row <span>\(i\)</span> that arrived earlier in the sample at time <span>\(k_{i,1}\)</span>; thus, displacement cannot introduce a new row from outside of <span>\(S\)</span>.</p>
<p>Therefore, if we have a sample <em>without</em> replacement, we can construct a sample <em>with</em> replacement from its rows.
<em>We can ignore all other rows in the population.</em>
This makes the problem much more approachable.</p>
<p>So now we can see an algorithm taking shape for taking a sample <em>with</em> replacement of size <span>\(m\)</span>:</p>
<ol type="1">
<li>First, take an <span>\(m\)</span>-sized sample <span>\(S\)</span> <em>without</em> replacement using the efficient A-ES algorithm.</li>
<li>For each sampled row <span>\(i\)</span> in <span>\(S\)</span>, generate <span>\(m\)</span> arrivals <span>\(k_{i,j}\)</span> for <span>\(j = 1, 2, \ldots, m\)</span> using the same pseudorandom universe that was used to generate <span>\(S\)</span>.</li>
<li>Take the first <span>\(m\)</span> arrivals as the sample.</li>
</ol>
<p>You may have noticed that step 2 of this algorithm requires us to create <span>\(m\)</span> arrivals for each of the <span>\(m\)</span> rows in <span>\(S\)</span>.
This step thus requires <span>\(O(m^2)\)</span> time.
When <span>\(m\)</span> is large, this time can be prohibitive.</p>
<p>Fortunately, we can use probability theory to reduce this cost to <span>\(O(m)\)</span>.
The idea is that if row <span>\(i\)</span> is expected to occur in the sample <span>\(n_i\)</span> times, it is very unlikely to occur more than <span>\(c \cdot n_i\)</span> times when <span>\(c \ge 2\)</span>.
So we don’t need to generate a full <span>\(m\)</span> arrivals for each row <span>\(i\)</span> in <span>\(S\)</span>; we can get away with generating <span>\(m_i = \lceil{c \cdot n_i}\rceil\)</span> arrivals instead, for a suitably large <span>\(c\)</span> to assuage our personal level of paranoia.</p>
<p>Here’s a sample implementation as a DuckDB macro:</p>
<div id="cb11"><pre><code><span id="cb11-1"><span>-- Takes a weighted sample with replacement from a table.</span></span>
<span id="cb11-2"><span>--</span></span>
<span id="cb11-3"><span>-- Args:</span></span>
<span id="cb11-4"><span>--  population_table: The table to sample from. It must have a `pk` column</span></span>
<span id="cb11-5"><span>--    of unique primary keys and a `weight` column of non-negative weights.</span></span>
<span id="cb11-6"><span>--  seed: A string that determines the pseudorandom universe in which the</span></span>
<span id="cb11-7"><span>--    sample is taken. Samples taken with distinct seeds are independent.</span></span>
<span id="cb11-8"><span>--    If you wish to repeat a sample, reuse the sample's seed.</span></span>
<span id="cb11-9"><span>--  sample_size: The number of rows to include in the sample. This value</span></span>
<span id="cb11-10"><span>--    may be larger than the number of rows in the `population_table`.</span></span>
<span id="cb11-11"><span>--</span></span>
<span id="cb11-12"><span>-- Returns a sample of rows from the `population_table`.</span></span>
<span id="cb11-13"><span>CREATE</span> MACRO sample_with_replacement(population_table, seed, sample_size)</span>
<span id="cb11-14"><span>AS</span> <span>TABLE</span> (</span>
<span id="cb11-15">  <span>WITH</span></span>
<span id="cb11-16">    <span>-- First, take a sample *without* replacement of the wanted size.</span></span>
<span id="cb11-17">    SampleWithoutReplacement <span>AS</span> (</span>
<span id="cb11-18">      <span>SELECT</span> <span>*</span></span>
<span id="cb11-19">      <span>FROM</span> query_table(population_table:<span>:varchar</span>)</span>
<span id="cb11-20">      <span>WHERE</span> weight <span>&gt;</span> <span>0</span></span>
<span id="cb11-21">      <span>ORDER</span> <span>BY</span> <span>-</span><span>LN</span>(pseudorandom_uniform(pk, seed, <span>1</span>)) <span>/</span> weight</span>
<span id="cb11-22">      <span>LIMIT</span> sample_size</span>
<span id="cb11-23">    ),</span>
<span id="cb11-24">    <span>-- Compute the total weight over the sample.</span></span>
<span id="cb11-25">    SampleWithoutReplacementTotals <span>AS</span> (</span>
<span id="cb11-26">      <span>SELECT</span> <span>SUM</span>(weight) <span>AS</span> weight</span>
<span id="cb11-27">      <span>FROM</span> SampleWithoutReplacement</span>
<span id="cb11-28">    ),</span>
<span id="cb11-29">    <span>-- Generate a series of arrivals for each row in the sample.</span></span>
<span id="cb11-30">    SampleWithReplacementArrivals <span>AS</span> (</span>
<span id="cb11-31">      <span>SELECT</span></span>
<span id="cb11-32">        S.<span>*</span>,</span>
<span id="cb11-33">        <span>SUM</span>(<span>-</span><span>LN</span>(pseudorandom_uniform(pk, seed, trial_index)) <span>/</span> S.weight)</span>
<span id="cb11-34">          <span>OVER</span> (<span>PARTITION</span> <span>BY</span> pk <span>ORDER</span> <span>BY</span> trial_index)</span>
<span id="cb11-35">          <span>AS</span> rws_sort_key</span>
<span id="cb11-36">      <span>FROM</span> SampleWithoutReplacement <span>AS</span> S</span>
<span id="cb11-37">      <span>CROSS</span> <span>JOIN</span> SampleWithoutReplacementTotals <span>AS</span> T</span>
<span id="cb11-38">      <span>CROSS</span> <span>JOIN</span></span>
<span id="cb11-39">        UNNEST(</span>
<span id="cb11-40">          <span>RANGE</span>(<span>1</span>, <span>CAST</span>(<span>2.0</span> <span>*</span> sample_size <span>*</span> S.weight <span>/</span> T.weight <span>+</span> <span>2</span> <span>AS</span> <span>INT</span>)))</span>
<span id="cb11-41">        <span>AS</span> I(trial_index)</span>
<span id="cb11-42">    )</span>
<span id="cb11-43">  <span>-- Form the sample *with* replacement from the first `sample_size` arrivals.</span></span>
<span id="cb11-44">  <span>SELECT</span> <span>*</span> EXCLUDE (rws_sort_key)</span>
<span id="cb11-45">  <span>FROM</span> SampleWithReplacementArrivals</span>
<span id="cb11-46">  <span>ORDER</span> <span>BY</span> rws_sort_key</span>
<span id="cb11-47">  <span>LIMIT</span> sample_size</span>
<span id="cb11-48">);</span></code></pre></div>
<h3 id="example-of-sampling-with-replacement">Example of sampling with replacement</h3>
<p>As an example of when we might want to sample with replacement instead of without, consider the following population table <code>ThreeToOne</code> that represents the possible outcomes of tossing a biased coin:</p>
<table>
<caption><code>ThreeToOne</code> population table.</caption>
<thead>
<tr>
<th><em>pk</em></th>
<th><em>weight</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>heads</td>
<td>3</td>
</tr>
<tr>
<td>tails</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>For this biased coin, “heads” is 3 times as likely as “tails.”
We can simulate flipping this coin 10 times by sampling 10 rows from the <code>ThreeToOne</code> population table with replacement:</p>
<div id="cb12"><pre><code><span id="cb12-1"><span>SELECT</span> pk</span>
<span id="cb12-2"><span>FROM</span> sample_with_replacement(ThreeToOne, <span>'test-seed-20240601'</span>, <span>10</span>);</span></code></pre></div>
<table>
<caption>Results of drawing a 10-row sample with replacement from <code>ThreeToOne</code>.</caption>
<thead>
<tr>
<th><em>pk</em></th>
</tr>
</thead>
<tbody>
<tr>
<td>heads</td>
</tr>
<tr>
<td>heads</td>
</tr>
<tr>
<td>heads</td>
</tr>
<tr>
<td>tails</td>
</tr>
<tr>
<td>tails</td>
</tr>
<tr>
<td>heads</td>
</tr>
<tr>
<td>heads</td>
</tr>
<tr>
<td>heads</td>
</tr>
<tr>
<td>tails</td>
</tr>
<tr>
<td>heads</td>
</tr>
</tbody>
</table>
<p>In this sample, we got 7 heads and 3 tails.
On average, we would expect about 7.5 heads in each sample of size 10, so our observed sample is close to our expectations.</p>
<p>But maybe we just got lucky.
As a stronger test of our SQL sampling logic, let’s take 10,000 samples of size 40 and look at the count of heads across all of the samples.
We would expect this count to have a Binomial(<em>size</em> = 40, <em>p</em> = 3/4) distribution.
To compare our observed results to the expected distribution, I’ll compute the empirical distribution of the results and plot that over the expected distribution.
As you can see in the figure below, the observed distribution closely matches the expected distribution.</p>
<figure>
<img src="https://blog.moertel.com/images/public_html/blog/pix-20240601/observed_samples_vs_expected_three_to_one.svg" alt="When we take 10,000 independent samples of size n = 40 from a 3:1 biased-coin distribution, we find that the count of “heads” over the samples agrees with the expected Binomial(size = 40, p = 3/4) distribution.">

</figure>
<h2 id="conclusion">Conclusion</h2>
<p>Sampling is a powerful tool.
And with the SQL logic we’ve just discussed, you can take fast, easy samples from virtually any dataset, no matter how large.
And you can take those samples with or without replacement.</p>
<p>What makes it all work is a clever connection to the theory of Poisson processes.
Those processes are memoryless and mergeable, and their arrivals win races in proportion to their rates.
These properties are exactly what we need to run races that let us take samples.</p>
<p>Beyond what we’ve discussed in this article, there are further ways we can exploit these properties.
For example, as a performance optimization, we can predict the arrival time <span>\(t\)</span> of the final row in a sample.
Then we can augment our SQL sampling logic with a pushdown filter that eliminates population rows with arrival times greater than <span>\(c \cdot t\)</span> for some constant <span>\(c\)</span>.
This filtering happens before <code>ORDER/LIMIT</code> processing and can greatly speed queries by eliminating more than 99.99% of rows early on, before they are even fully read on systems that support “late materialization.”</p>
<p>But this article is already too long, so I’ll stop here for now.</p>
<h2 id="references">References</h2>
<p>Pavlos S. Efraimidis and Paul G. Spirakis. Weighted random sampling with a reservoir. <em>Information Processing Letters</em>, 97(5):181–185, 2006.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to do distributed locking (2016) (211 pts)]]></title>
            <link>https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</link>
            <guid>41894451</guid>
            <pubDate>Sun, 20 Oct 2024 10:38:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html">https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html</a>, See on <a href="https://news.ycombinator.com/item?id=41894451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
                

                
                <p>Published by Martin Kleppmann on 08 Feb 2016.</p>
                

                <p>As part of the research for <a href="http://dataintensive.net/">my book</a>, I came across an algorithm called <a href="http://redis.io/topics/distlock">Redlock</a> on the
<a href="http://redis.io/">Redis</a> website. The algorithm claims to implement fault-tolerant distributed locks (or rather,
<a href="https://pdfs.semanticscholar.org/a25e/ee836dbd2a5ae680f835309a484c9f39ae4e.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">leases</a>&nbsp;[1]) on top of Redis, and the page asks for feedback from people who are into
distributed systems. The algorithm instinctively set off some alarm bells in the back of my mind, so
I spent a bit of time thinking about it and writing up these notes.</p>

<p>Since there are already <a href="http://redis.io/topics/distlock">over 10 independent implementations of Redlock</a> and we don’t know
who is already relying on this algorithm, I thought it would be worth sharing my notes publicly.
I won’t go into other aspects of Redis, some of which have already been critiqued
<a href="https://aphyr.com/tags/Redis">elsewhere</a>.</p>

<p>Before I go into the details of Redlock, let me say that I quite like Redis, and I have successfully
used it in production in the past. I think it’s a good fit in situations where you want to share
some transient, approximate, fast-changing data between servers, and where it’s not a big deal if
you occasionally lose that data for whatever reason. For example, a good use case is maintaining
request counters per IP address (for rate limiting purposes) and sets of distinct IP addresses per
user ID (for abuse detection).</p>

<p>However, Redis has been gradually making inroads into areas of data management where there are
stronger consistency and durability expectations – which worries me, because this is not what Redis
is designed for. Arguably, distributed locking is one of those areas. Let’s examine it in some more
detail.</p>

<h2 id="what-are-you-using-that-lock-for">What are you using that lock for?</h2>

<p>The purpose of a lock is to ensure that among several nodes that might try to do the same piece of
work, only one actually does it (at least only one at a time). That work might be to write some data
to a shared storage system, to perform some computation, to call some external API, or suchlike. At
a high level, there are two reasons why you might want a lock in a distributed application:
<a href="https://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">for efficiency or for correctness</a>&nbsp;[2]. To distinguish these cases, you can ask what
would happen if the lock failed:</p>

<ul>
  <li><strong>Efficiency:</strong> Taking a lock saves you from unnecessarily doing the same work twice (e.g. some
expensive computation). If the lock fails and two nodes end up doing the same piece of work, the
result is a minor increase in cost (you end up paying 5 cents more to AWS than you otherwise would
have) or a minor inconvenience (e.g. a user ends up getting the same email notification twice).</li>
  <li><strong>Correctness:</strong> Taking a lock prevents concurrent processes from stepping on each others’ toes
and messing up the state of your system. If the lock fails and two nodes concurrently work on the
same piece of data, the result is a corrupted file, data loss, permanent inconsistency, the wrong
dose of a drug administered to a patient, or some other serious problem.</li>
</ul>

<p>Both are valid cases for wanting a lock, but you need to be very clear about which one of the two
you are dealing with.</p>

<p>I will argue that if you are using locks merely for efficiency purposes, it is unnecessary to incur
the cost and complexity of Redlock, running 5 Redis servers and checking for a majority to acquire
your lock. You are better off just using a single Redis instance, perhaps with asynchronous
replication to a secondary instance in case the primary crashes.</p>

<p>If you use a single Redis instance, of course you will drop some locks if the power suddenly goes
out on your Redis node, or something else goes wrong. But if you’re only using the locks as an
efficiency optimization, and the crashes don’t happen too often, that’s no big deal. This “no big
deal” scenario is where Redis shines. At least if you’re relying on a single Redis instance, it is
clear to everyone who looks at the system that the locks are approximate, and only to be used for
non-critical purposes.</p>

<p>On the other hand, the Redlock algorithm, with its 5 replicas and majority voting, looks at first
glance as though it is suitable for situations in which your locking is important for <em>correctness</em>.
I will argue in the following sections that it is <em>not</em> suitable for that purpose. For the rest of
this article we will assume that your locks are important for correctness, and that it is a serious
bug if two different nodes concurrently believe that they are holding the same lock.</p>

<h2 id="protecting-a-resource-with-a-lock">Protecting a resource with a lock</h2>

<p>Let’s leave the particulars of Redlock aside for a moment, and discuss how a distributed lock is
used in general (independent of the particular locking algorithm used). It’s important to remember
that a lock in a distributed system is not like a mutex in a multi-threaded application. It’s a more
complicated beast, due to the problem that different nodes and the network can all fail
independently in various ways.</p>

<p>For example, say you have an application in which a client needs to update a file in shared storage
(e.g. HDFS or S3). A client first acquires the lock, then reads the file, makes some changes, writes
the modified file back, and finally releases the lock. The lock prevents two clients from performing
this read-modify-write cycle concurrently, which would result in lost updates. The code might look
something like this:</p>

<figure><pre><code data-lang="js"><span>// THIS CODE IS BROKEN</span>
<span>function</span> <span>writeData</span><span>(</span><span>filename</span><span>,</span> <span>data</span><span>)</span> <span>{</span>
    <span>var</span> <span>lock</span> <span>=</span> <span>lockService</span><span>.</span><span>acquireLock</span><span>(</span><span>filename</span><span>);</span>
    <span>if</span> <span>(</span><span>!</span><span>lock</span><span>)</span> <span>{</span>
        <span>throw</span> <span>'</span><span>Failed to acquire lock</span><span>'</span><span>;</span>
    <span>}</span>

    <span>try</span> <span>{</span>
        <span>var</span> <span>file</span> <span>=</span> <span>storage</span><span>.</span><span>readFile</span><span>(</span><span>filename</span><span>);</span>
        <span>var</span> <span>updated</span> <span>=</span> <span>updateContents</span><span>(</span><span>file</span><span>,</span> <span>data</span><span>);</span>
        <span>storage</span><span>.</span><span>writeFile</span><span>(</span><span>filename</span><span>,</span> <span>updated</span><span>);</span>
    <span>}</span> <span>finally</span> <span>{</span>
        <span>lock</span><span>.</span><span>release</span><span>();</span>
    <span>}</span>
<span>}</span></code></pre></figure>

<p>Unfortunately, even if you have a perfect lock service, the code above is broken. The following
diagram shows how you can end up with corrupted data:</p>

<p><img src="https://martin.kleppmann.com/2016/02/unsafe-lock.png" width="550" height="200" alt="Unsafe access to a resource protected by a distributed lock"></p>

<p>In this example, the client that acquired the lock is paused for an extended period of time while
holding the lock – for example because the garbage collector (GC) kicked in. The lock has a timeout
(i.e. it is a lease), which is always a good idea (otherwise a crashed client could end up holding
a lock forever and never releasing it). However, if the GC pause lasts longer than the lease expiry
period, and the client doesn’t realise that it has expired, it may go ahead and make some unsafe
change.</p>

<p>This bug is not theoretical: HBase used to <a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">have this problem</a>&nbsp;[3,4]. Normally,
GC pauses are quite short, but “stop-the-world” GC pauses have sometimes been known to last for
<a href="https://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">several minutes</a>&nbsp;[5] – certainly long enough for a lease to expire. Even so-called
“concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in parallel with the
application code – even they <a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">need to stop the world</a> from time to time&nbsp;[6].</p>

<p>You cannot fix this problem by inserting a check on the lock expiry just before writing back to
storage. Remember that GC can pause a running thread at <em>any point</em>, including the point that is
maximally inconvenient for you (between the last check and the write operation).</p>

<p>And if you’re feeling smug because your programming language runtime doesn’t have long GC pauses,
there are many other reasons why your process might get paused. Maybe your process tried to read an
address that is not yet loaded into memory, so it gets a page fault and is paused until the page is
loaded from disk. Maybe your disk is actually EBS, and so reading a variable unwittingly turned into
a synchronous network request over Amazon’s congested network. Maybe there are many other processes
contending for CPU, and you hit a <a href="https://twitter.com/aphyr/status/682077908953792512">black node in your scheduler tree</a>. Maybe someone
accidentally sent SIGSTOP to the process. Whatever. Your processes will get paused.</p>

<p>If you still don’t believe me about process pauses, then consider instead that the file-writing
request may get delayed in the network before reaching the storage service. Packet networks such as
Ethernet and IP may delay packets <em>arbitrarily</em>, and <a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">they do</a>&nbsp;[7]: in a famous
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">incident at GitHub</a>, packets were delayed in the network for approximately 90
seconds&nbsp;[8]. This means that an application process may send a write request, and it may reach
the storage server a minute later when the lease has already expired.</p>

<p>Even in well-managed networks, this kind of thing can happen. You simply cannot make any assumptions
about timing, which is why the code above is fundamentally unsafe, no matter what lock service you
use.</p>

<h2 id="making-the-lock-safe-with-fencing">Making the lock safe with fencing</h2>

<p>The fix for this problem is actually pretty simple: you need to include a <em>fencing token</em> with every
write request to the storage service. In this context, a fencing token is simply a number that
increases (e.g. incremented by the lock service) every time a client acquires the lock. This is
illustrated in the following diagram:</p>

<p><img src="https://martin.kleppmann.com/2016/02/fencing-tokens.png" width="550" height="200" alt="Using fencing tokens to make resource access safe"></p>

<p>Client 1 acquires the lease and gets a token of 33, but then it goes into a long pause and the lease
expires. Client 2 acquires the lease, gets a token of 34 (the number always increases), and then
sends its write to the storage service, including the token of 34. Later, client 1 comes back to
life and sends its write to the storage service, including its token value 33. However, the storage
server remembers that it has already processed a write with a higher token number (34), and so it
rejects the request with token 33.</p>

<p>Note this requires the storage server to take an active role in checking tokens, and rejecting any
writes on which the token has gone backwards. But this is not particularly hard, once you know the
trick. And provided that the lock service generates strictly monotonically increasing tokens, this
makes the lock safe. For example, if you are using ZooKeeper as lock service, you can use the <code>zxid</code>
or the znode version number as fencing token, and you’re in good shape&nbsp;[3].</p>

<p>However, this leads us to the first big problem with Redlock: <em>it does not have any facility for
generating fencing tokens</em>. The algorithm does not produce any number that is guaranteed to increase
every time a client acquires a lock. This means that even if the algorithm were otherwise perfect,
it would not be safe to use, because you cannot prevent the race condition between clients in the
case where one client is paused or its packets are delayed.</p>

<p>And it’s not obvious to me how one would change the Redlock algorithm to start generating fencing
tokens. The unique random value it uses does not provide the required monotonicity. Simply keeping
a counter on one Redis node would not be sufficient, because that node may fail. Keeping counters on
several nodes would mean they would go out of sync. It’s likely that you would need a consensus
algorithm just to generate the fencing tokens. (If only <a href="https://twitter.com/lindsey/status/575006945213485056">incrementing a counter</a> was
simple.)</p>

<h2 id="using-time-to-solve-consensus">Using time to solve consensus</h2>

<p>The fact that Redlock fails to generate fencing tokens should already be sufficient reason not to
use it in situations where correctness depends on the lock. But there are some further problems that
are worth discussing.</p>

<p>In the academic literature, the most practical system model for this kind of algorithm is the
<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">asynchronous model with unreliable failure detectors</a>&nbsp;[9]. In plain English,
this means that the algorithms make no assumptions about timing: processes may pause for arbitrary
lengths of time, packets may be arbitrarily delayed in the network, and clocks may be arbitrarily
wrong – and the algorithm is nevertheless expected to do the right thing. Given what we discussed
above, these are very reasonable assumptions.</p>

<p>The only purpose for which algorithms may use clocks is to generate timeouts, to avoid waiting
forever if a node is down. But timeouts do not have to be accurate: just because a request times
out, that doesn’t mean that the other node is definitely down – it could just as well be that there
is a large delay in the network, or that your local clock is wrong. When used as a failure detector,
timeouts are just a guess that something is wrong. (If they could, distributed algorithms would do
without clocks entirely, but then <a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">consensus becomes impossible</a>&nbsp;[10]. Acquiring a lock is
like a compare-and-set operation, which <a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">requires consensus</a>&nbsp;[11].)</p>

<p>Note that Redis <a href="https://github.com/antirez/redis/blob/edd4d555df57dc84265fdfb4ef59a4678832f6da/src/server.c#L390-L404">uses <code>gettimeofday</code></a>, not a <a href="http://linux.die.net/man/2/clock_gettime">monotonic clock</a>, to
determine the <a href="https://github.com/antirez/redis/blob/f0b168e8944af41c4161249040f01ece227cfc0c/src/db.c#L933-L959">expiry of keys</a>. The man page for <code>gettimeofday</code> <a href="http://linux.die.net/man/2/gettimeofday">explicitly
says</a> that the time it returns is subject to discontinuous jumps in system time –
that is, it might suddenly jump forwards by a few minutes, or even jump back in time (e.g. if the
clock is <a href="https://www.eecis.udel.edu/~mills/ntp/html/clock.html">stepped by NTP</a> because it differs from a NTP server by too much, or if the
clock is manually adjusted by an administrator). Thus, if the system clock is doing weird things, it
could easily happen that the expiry of a key in Redis is much faster or much slower than expected.</p>

<p>For algorithms in the asynchronous model this is not a big problem: these algorithms generally
ensure that their <em>safety</em> properties always hold, <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">without making any timing
assumptions</a>&nbsp;[12]. Only <em>liveness</em> properties depend on timeouts or some other failure
detector. In plain English, this means that even if the timings in the system are all over the place
(processes pausing, networks delaying, clocks jumping forwards and backwards), the performance of an
algorithm might go to hell, but the algorithm will never make an incorrect decision.</p>

<p>However, Redlock is not like this. Its safety depends on a lot of timing assumptions: it assumes
that all Redis nodes hold keys for approximately the right length of time before expiring; that the
network delay is small compared to the expiry duration; and that process pauses are much shorter
than the expiry duration.</p>

<h2 id="breaking-redlock-with-bad-timings">Breaking Redlock with bad timings</h2>

<p>Let’s look at some examples to demonstrate Redlock’s reliance on timing assumptions. Say the system
has five Redis nodes (A, B, C, D and E), and two clients (1 and 2). What happens if a clock on one
of the Redis nodes jumps forward?</p>

<ol>
  <li>Client 1 acquires lock on nodes A, B, C. Due to a network issue, D and E cannot be reached.</li>
  <li>The clock on node C jumps forward, causing the lock to expire.</li>
  <li>Client 2 acquires lock on nodes C, D, E. Due to a network issue, A and B cannot be reached.</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>A similar issue could happen if C crashes before persisting the lock to disk, and immediately
restarts. For this reason, the Redlock documentation <a href="http://redis.io/topics/distlock#performance-crash-recovery-and-fsync">recommends delaying restarts</a> of
crashed nodes for at least the time-to-live of the longest-lived lock. But this restart delay again
relies on a reasonably accurate measurement of time, and would fail if the clock jumps.</p>

<p>Okay, so maybe you think that a clock jump is unrealistic, because you’re very confident in having
correctly configured NTP to only ever slew the clock. In that case, let’s look at an example of how
a process pause may cause the algorithm to fail:</p>

<ol>
  <li>Client 1 requests lock on nodes A, B, C, D, E.</li>
  <li>While the responses to client 1 are in flight, client 1 goes into stop-the-world GC.</li>
  <li>Locks expire on all Redis nodes.</li>
  <li>Client 2 acquires lock on nodes A, B, C, D, E.</li>
  <li>Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully
acquired the lock (they were held in client 1’s kernel network buffers while the process was
paused).</li>
  <li>Clients 1 and 2 now both believe they hold the lock.</li>
</ol>

<p>Note that even though Redis is written in C, and thus doesn’t have GC, that doesn’t help us here:
any system in which the <em>clients</em> may experience a GC pause has this problem. You can only make this
safe by preventing client 1 from performing any operations under the lock after client 2 has
acquired the lock, for example using the fencing approach above.</p>

<p>A long network delay can produce the same effect as the process pause. It perhaps depends on your
TCP user timeout – if you make the timeout significantly shorter than the Redis TTL, perhaps the
delayed network packets would be ignored, but we’d have to look in detail at the TCP implementation
to be sure. Also, with the timeout we’re back down to accuracy of time measurement again!</p>

<h2 id="the-synchrony-assumptions-of-redlock">The synchrony assumptions of Redlock</h2>

<p>These examples show that Redlock works correctly only if you assume a <em>synchronous</em> system model –
that is, a system with the following properties:</p>

<ul>
  <li>bounded network delay (you can guarantee that packets always arrive within some guaranteed maximum
delay),</li>
  <li>bounded process pauses (in other words, hard real-time constraints, which you typically only
find in car airbag systems and suchlike), and</li>
  <li>bounded clock error (cross your fingers that you don’t get your time from a <a href="http://xenia.media.mit.edu/~nelson/research/ntp-survey99/">bad NTP
server</a>).</li>
</ul>

<p>Note that a synchronous model does not mean exactly synchronised clocks: it means you are assuming
a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988"><em>known, fixed upper bound</em></a> on network delay, pauses and clock drift&nbsp;[12]. Redlock
assumes that delays, pauses and drift are all small relative to the time-to-live of a lock; if the
timing issues become as large as the time-to-live, the algorithm fails.</p>

<p>In a reasonably well-behaved datacenter environment, the timing assumptions will be satisfied <em>most</em>
of the time – this is known as a <a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">partially synchronous system</a>&nbsp;[12]. But is that good
enough? As soon as those timing assumptions are broken, Redlock may violate its safety properties,
e.g. granting a lease to one client before another has expired. If you’re depending on your lock for
correctness, “most of the time” is not enough – you need it to <em>always</em> be correct.</p>

<p>There is plenty of evidence that it is not safe to assume a synchronous system model for most
practical system environments&nbsp;[7,8]. Keep reminding yourself of the GitHub incident with the
<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">90-second packet delay</a>. It is unlikely that Redlock would survive a <a href="https://aphyr.com/tags/jepsen">Jepsen</a> test.</p>

<p>On the other hand, a consensus algorithm designed for a partially synchronous system model (or
asynchronous model with failure detector) actually has a chance of working. Raft, Viewstamped
Replication, Zab and Paxos all fall in this category. Such an algorithm must let go of all timing
assumptions. That’s hard: it’s so tempting to assume networks, processes and clocks are more
reliable than they really are. But in the messy reality of distributed systems, you have to be very
careful with your assumptions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I think the Redlock algorithm is a poor choice because it is “neither fish nor fowl”: it is
unnecessarily heavyweight and expensive for efficiency-optimization locks, but it is not
sufficiently safe for situations in which correctness depends on the lock.</p>

<p>In particular, the algorithm makes dangerous assumptions about timing and system clocks (essentially
assuming a synchronous system with bounded network delay and bounded execution time for operations),
and it violates safety properties if those assumptions are not met. Moreover, it lacks a facility
for generating fencing tokens (which protect a system against long delays in the network or in
paused processes).</p>

<p>If you need locks only on a best-effort basis (as an efficiency optimization, not for correctness),
I would recommend sticking with the <a href="http://redis.io/commands/set">straightforward single-node locking algorithm</a> for
Redis (conditional set-if-not-exists to obtain a lock, atomic delete-if-value-matches to release
a lock), and documenting very clearly in your code that the locks are only approximate and may
occasionally fail. Don’t bother with setting up a cluster of five Redis nodes.</p>

<p>On the other hand, if you need locks for correctness, please don’t use Redlock. Instead, please use
a proper consensus system such as <a href="https://zookeeper.apache.org/">ZooKeeper</a>, probably via one of the <a href="http://curator.apache.org/curator-recipes/index.html">Curator recipes</a>
that implements a lock. (At the very least, use a <a href="http://www.postgresql.org/">database with reasonable transactional
guarantees</a>.) And please enforce use of fencing tokens on all resource accesses under the
lock.</p>

<p>As I said at the beginning, Redis is an excellent tool if you use it correctly. None of the above
diminishes the usefulness of Redis for its intended purposes. <a href="http://antirez.com/">Salvatore</a> has been very
dedicated to the project for years, and its success is well deserved. But every tool has
limitations, and it is important to know them and to plan accordingly.</p>

<p>If you want to learn more, I explain this topic in greater detail in <a href="http://dataintensive.net/">chapters 8 and 9 of my
book</a>, now available in Early Release from O’Reilly. (The diagrams above are taken from my
book.) For learning how to use ZooKeeper, I recommend <a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013">Junqueira and Reed’s book</a>&nbsp;[3].
For a good introduction to the theory of distributed systems, I recommend <a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011">Cachin, Guerraoui and
Rodrigues’ textbook</a>&nbsp;[13].</p>

<p><em>Thank you to <a href="https://aphyr.com/">Kyle Kingsbury</a>, <a href="https://twitter.com/skamille">Camille Fournier</a>, <a href="https://twitter.com/fpjunqueira">Flavio Junqueira</a>, and
<a href="http://antirez.com/">Salvatore Sanfilippo</a> for reviewing a draft of this article. Any errors are mine, of
course.</em></p>

<p><strong>Update 9 Feb 2016:</strong> <a href="http://antirez.com/">Salvatore</a>, the original author of Redlock, has
<a href="http://antirez.com/news/101">posted a rebuttal</a> to this article (see also
<a href="https://news.ycombinator.com/item?id=11065933">HN discussion</a>). He makes some good points, but
I stand by my conclusions. I may elaborate in a follow-up post if I have time, but please form your
own opinions – and please consult the references below, many of which have received rigorous
academic peer review (unlike either of our blog posts).</p>

<h2 id="references">References</h2>

<p>[1] Cary G Gray and David R Cheriton:
“<a href="https://pdfs.semanticscholar.org/a25e/ee836dbd2a5ae680f835309a484c9f39ae4e.pdf" title="Cary G Gray and David R Cheriton. Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency. SOSP 1989">Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,”
at <em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989.
<a href="https://dx.doi.org/10.1145/74850.74870">doi:10.1145/74850.74870</a></p>

<p>[2] Mike Burrows:
“<a href="https://research.google.com/archive/chubby.html" title="Mike Burrows. The Chubby lock service for loosely-coupled distributed systems. OSDI 2006">The Chubby lock service for loosely-coupled distributed systems</a>,”
at <em>7th USENIX Symposium on Operating System Design and Implementation</em> (OSDI), November 2006.</p>

<p>[3] Flavio P Junqueira and Benjamin Reed:
<a href="http://shop.oreilly.com/product/0636920028901.do" title="FP Junqueira and B Reed. ZooKeeper: Distributed Process Coordination. O'Reilly, 2013"><em>ZooKeeper: Distributed Process Coordination</em></a>. O’Reilly Media, November 2013.
ISBN: 978-1-4493-6130-3</p>

<p>[4] Enis Söztutar:
“<a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage" title="Enis Söztutar. HBase and HDFS: Understanding filesystem usage in HBase. HBaseCon 2013">HBase and HDFS: Understanding filesystem usage in HBase</a>,” at <em>HBaseCon</em>, June 2013.</p>

<p>[5] Todd Lipcon:
“<a href="https://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" title="Todd Lipcon. Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1. 2011">Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,”
blog.cloudera.com, 24 February 2011.</p>

<p>[6] Martin Thompson: “<a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html" title="Martin Thompson. Java Garbage Collection Distilled. 2013">Java Garbage Collection Distilled</a>,”
mechanical-sympathy.blogspot.co.uk, 16 July 2013.</p>

<p>[7] Peter Bailis and Kyle Kingsbury: “<a href="https://queue.acm.org/detail.cfm?id=2655736" title="P Bailis and K Kingsbury. The Network is Reliable. ACM Queue 12(7), 2014.">The Network is Reliable</a>,”
<em>ACM Queue</em>, volume 12, number 7, July 2014.
<a href="https://dx.doi.org/10.1145/2639988.2639988">doi:10.1145/2639988.2639988</a></p>

<p>[8] Mark Imbriaco: “<a href="https://github.com/blog/1364-downtime-last-saturday" title="Mark Imbriaco. Downtime last Saturday. 2012">Downtime last Saturday</a>,” github.com, 26 December 2012.</p>

<p>[9] Tushar Deepak Chandra and Sam Toueg:
“<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf" title="TD Chandra and S Toueg. Unreliable Failure Detectors for Reliable Distributed Systems. JACM 43(2):225–267, 1996">Unreliable Failure Detectors for Reliable Distributed Systems</a>,”
<em>Journal of the ACM</em>, volume 43, number 2, pages 225–267, March 1996.
<a href="https://dx.doi.org/10.1145/226643.226647">doi:10.1145/226643.226647</a></p>

<p>[10] Michael J Fischer, Nancy Lynch, and Michael S Paterson:
“<a href="http://www.cs.princeton.edu/courses/archive/fall07/cos518/papers/flp.pdf" title="MJ Fischer, N Lynch, and MS Paterson. Impossibility of Distributed Consensus with One Faulty Process. JACM 32(2):374–382, 1985">Impossibility of Distributed Consensus with One Faulty Process</a>,”
<em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985.
<a href="https://dx.doi.org/10.1145/3149.214121">doi:10.1145/3149.214121</a></p>

<p>[11] Maurice P Herlihy: “<a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf" title="Maurice Herlihy. Wait-Free Synchronization. TOPLAS 13(1):124–149, 1991">Wait-Free Synchronization</a>,”
<em>ACM Transactions on Programming Languages and Systems</em>, volume 13, number 1, pages 124–149, January 1991.
<a href="https://dx.doi.org/10.1145/114005.102808">doi:10.1145/114005.102808</a></p>

<p>[12] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
“<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf" title="C Dwork, N Lynch, and L Stockmeyer. Consensus in the Presence of Partial Synchrony. JACM 35(2):288–323, 1988">Consensus in the Presence of Partial Synchrony</a>,”
<em>Journal of the ACM</em>, volume 35, number 2, pages 288–323, April 1988.
<a href="https://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p>

<p>[13] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues:
<a href="http://www.distributedprogramming.net/" title="C Cachin, R Guerraoui, and L Rodrigues. Introduction to Reliable and Secure Distributed Programming, 2nd ed. Springer, 2011"><em>Introduction to Reliable and Secure Distributed Programming</em></a>,
Second Edition. Springer, February 2011. ISBN: 978-3-642-15259-7,
<a href="https://dx.doi.org/10.1007/978-3-642-15260-3">doi:10.1007/978-3-642-15260-3</a></p>



                <div>
                    <p>If you found this post useful, please
                    <a href="https://www.patreon.com/martinkl">support me on Patreon</a>
                    so that I can write more like it!</p>
                    <p>
                    To get notified when I write something new,
                    <a href="https://bsky.app/profile/martin.kleppmann.com">follow me on Bluesky</a> or
                    <a href="https://nondeterministic.computer/@martin">Mastodon</a>,
                    or enter your email address:
                    </p>

                    

                    <p>
                    I won't give your address to anyone else, won't send you any spam, and you can unsubscribe at any time.
                    </p>
                </div>

                
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Euro coins as weights (2004) (165 pts)]]></title>
            <link>https://www.rubinghscience.org/surv/euroweights1.html</link>
            <guid>41894359</guid>
            <pubDate>Sun, 20 Oct 2024 10:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rubinghscience.org/surv/euroweights1.html">https://www.rubinghscience.org/surv/euroweights1.html</a>, See on <a href="https://news.ycombinator.com/item?id=41894359">Hacker News</a></p>
<div id="readability-page-1" class="page">
http://www.rubinghscience.org/surv/euroweights1.html<br>
Sep 2004
<br><hr><p><span size="5"><b>
Using Euro coins as standard weights
</b></span>
</p>


<p>
Here is a cheap way to obtain a set of standard weights, for measuring
weights up to about 100 gram, to an accuracy of about 0.5 gram :  Use
the coins of the currency of your country.  Thus one can make use of the
fact that these coins are fabricated in mass production, and because of
that have not only a constant shape and size, but also a constant and
accurately fixed weight.
</p><p>
Weights composed of these coins can be used to measure the weight of
small objects (e.g. letters).  The accuracy of the weight of the (Euro)
coins is good enough so that in measuring the weight of objects an
accuracy 0.5 gram is easily obtained in a cheap and quick way.(*)
</p><blockquote><span size="-1">
   (*) One cheap and quick way to construct things for weighing to 
   up to 0.5 gram accuracy with the Euro coins is as follows:<br> 
   &nbsp; &nbsp; &nbsp; 
   1. Use a very simple home-made scales, i.e. a balance, simply
   consisting of a straight rod suspended exactly in the middle, with
   hanging from each end a 'plate' (or 'pan') on which to place the
   object to be weighed and the standard weights.  A straight length of
   e.g. aluminium, two small identical pieces of board, and a small amount
   of supple thread suffice to construct a very serviceable scales.<br>
   &nbsp; &nbsp; &nbsp; 
   2. Create the weights by holding the combinations of Euro coins
   together with narrow paper bands glued around them.  The weight of the
   paper bands and the glue is much smaller than 0.5 g and is therefore
   negligible.  (A sheet of A4 copying paper weighs about 5 gram.)
</span></blockquote> 
<p>
In the rest of this text, I'll use the Euro coins, to illustrate the idea.
Also, I'll use the gram (abbreviated: 'g') as the unit of weight; that
is, I'll assume that the weight of the objects to be measured should be
determined in gram units.  The weight of the Euro coins is as follows:
</p><center>
<tt>
<table>
<tbody><tr>
	<td>Monetary<br>value of coin<br>[Euro]
	</td><td>Weight<br>[g]
</td></tr><tr>
	<td>0.01
	</td><td>2.30
</td></tr><tr>
	<td>0.02
	</td><td>3.06
</td></tr><tr>
	<td>0.05
	</td><td>3.92
</td></tr><tr>
	<td>0.10
	</td><td>4.10
</td></tr><tr>
	<td>0.20
	</td><td>5.74
</td></tr><tr>
	<td>0.50
	</td><td>7.80
</td></tr><tr>
	<td>1.00
	</td><td>7.50
</td></tr><tr>
	<td>2.00
	</td><td>8.50
</td></tr></tbody></table>
</tt>
<span size="-1"><b>
Table 1: Weights of the Euro coins<br>
<span size="-1">(Data obtained from: 
               http://www.euroswapper.com/euro_coins.html)</span>
</b></span>
</center>
<p>
Remarkably, the 1 Euro coin has a nice round weight of exactly 7.5 g;
but in general, the makers of the coins have obviously not used it as
a design criterion to give the coins a weight of a nice round number
of grams... &nbsp; These awkward numerical values of the weights of
the individual coins seems the biggest hurdle to easy use of coins as
standard weights (for weighing things cheaply and with a resolution of
better than 7.5 g).
</p><p>
However, it is easy (with the Euro coins at least) to combine small
numbers of coins together into combinations weighing a round number of
grams.  For example, the combination consisting of one 0.02 Euro coin
(3.06 g), two 0.05 Euro coins (3.92 g each), and one 0.10 Euro coin
(4,10 g), adds up to the round weight of exactly 15.00 g.  
</p><p>
It is possible (with the Euro coins) to compose in this way a
very satisfactory set of "standard weights", each consisting of a
combination of a small number of coins.  The following are the most
useful combinations:
</p><center>
<tt>
<table>
<tbody><tr>
	<td rowspan="2">Weight of<br>combination<br>[g]
	</td><td> 
	</td><td colspan="6">Number of coins of type
	</td><td> 
	</td><td rowspan="2">Cost of<br>combination<br>[Euro]
</td></tr><tr>
	<td>
	</td><td>0.01
	</td><td>0.02
	</td><td>0.05
	</td><td>0.10
	</td><td>0.20
	</td><td>0.50
	</td><td>
	</td><td>
</td></tr><tr>
	<td><p>9.96</p>
	</td><td>
	</td><td>3
	</td><td>1
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>
	</td><td>0.05
</td></tr><tr>
	<td><p>10.04</p>
	</td><td>
	</td><td>  &nbsp;
	</td><td>2
	</td><td>1
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>
	</td><td>0.09
</td></tr><tr>
	<td><p><b>15.00</b></p>
	</td><td>
	</td><td>  &nbsp;
	</td><td><b>1</b>
	</td><td><b>2</b>
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>
	</td><td><b>0.22</b>
</td></tr><tr>
	<td><p>15.50</p>
	</td><td>
	</td><td>2
	</td><td>1
	</td><td>2
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>
	</td><td>0.14
</td></tr><tr>
	<td><p>16.00</p>
	</td><td>
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>2
	</td><td>  &nbsp;
	</td><td>1
	</td><td>
	</td><td>0.70
</td></tr><tr>
	<td><p>16.50</p>
	</td><td>
	</td><td>2
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>1
	</td><td>  &nbsp;
	</td><td>1
	</td><td>
	</td><td>0.62
</td></tr><tr>
	<td><p>17.00</p>
	</td><td>
	</td><td>  &nbsp;
	</td><td>1
	</td><td>  &nbsp;
	</td><td>2
	</td><td>1
	</td><td>  &nbsp;
	</td><td>
	</td><td>0.42
</td></tr><tr>
	<td><p><b>17.50</b></p>
	</td><td>
	</td><td><b>2</b>
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td><b>1</b>
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td>
	</td><td><b>0.34</b>
</td></tr><tr>
	<td><p><b>17.50</b></p>
	</td><td>
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td><b>3</b>
	</td><td>  &nbsp;
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td>
	</td><td><b>0.35</b>
</td></tr><tr>
	<td><p>18.00</p>
	</td><td>
	</td><td>4
	</td><td>1
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>1
	</td><td>  &nbsp;
	</td><td>
	</td><td>0.26
</td></tr><tr>
	<td><p><b>20.00</b></p>
	</td><td>
	</td><td><b>3</b>
	</td><td><b>3</b>
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>  &nbsp;
	</td><td>
	</td><td><b>0.14</b>
</td></tr><tr>
	<td><p><b>25.00</b></p>
	</td><td>
	</td><td>  &nbsp;
	</td><td><b>3</b>
	</td><td><b>1</b>
	</td><td><b>1</b>
	</td><td>  &nbsp;
	</td><td><b>1</b>
	</td><td>
	</td><td><b>0.71</b>
</td></tr></tbody></table>
</tt>
<span size="-1"><b>
Table 2: The most useful combinations of Euro coins<br>
to obtain weights of a round number of grams.<br>
An empty cell means that the coin is not used in the combination.
</b></span>
</center>
<p>
The combinations of 9.96 g and 10.04 g included in Table 2 above are the
closest it is possible to get to a weight of 10.00 g.  
</p><p>
In Table 2, the 15 g and 20 g weights are the mainstay of the weight set.
With them, by placing them both at the left and at the right side in
the scales, one can measure to a resolution of 5 g.  The, relatively
expensive, 25 g weight is not really strictly necessary, but I've added
it for convenience.
</p><p>
By adding the 17.5 g weight to one's set of weights, the resolution is
improved to 2.5 g.  (The two possible coin combinations for the 17.5
g weight, both shown in the table, differ in cost only by 0.01 Euro.)
</p><p>
The remaining weights in Table 2 are included for those who want to 
improve their weight measuring resolution to 1.0 or 0.5 g.
</p><p>
In general, the larger the (desired) weight of the combination, the
larger is the number of ways in which the coins can be put together to
yield that given total weight.  Small weights (made up of only a few
coins) can only be composed in one or a very few ways; for large weights
(made up of larger numbers of coins) of any given desired weight,
in general a larger number of coin combinations exists that sum to
that weight.
</p><p>
For desired weights of &nbsp; <i>n</i> * 0.5 g (where <i>n</i>
= whole number) <br>
&nbsp; -- that is, weights of a whole number
of grams or a whole number of grams plus 0.5 g --<br>
it is, with the Euro coins, not possible to find combinations under a
total weight of 10.50 g.  The smallest possible combinations summing
to &nbsp; <i>n</i> * 0.5 g are:
</p><blockquote>
	10.50<br>
	11.00<br>
	11.50<br>
&nbsp; &nbsp;	15.00<br>
&nbsp; &nbsp;	15.50<br>
&nbsp; &nbsp;	16.00<br>
&nbsp; &nbsp;	16.50<br>
&nbsp; &nbsp;	17.00<br>
&nbsp; &nbsp;	17.50<br>
&nbsp; &nbsp;	18.00<br>
&nbsp; &nbsp; &nbsp; &nbsp;	20.00<br>
</blockquote>
From 20.00 g upwards, all combinations with a weight of &nbsp; <i>n</i>
* 0.5 g are possible.
<p>
To obtain weights of 30 g or larger with a weight of &nbsp; <i>n</i>
* 5 gram (<i>n</i> = whole number), that is, 30 g, 35 g, 40 g, 45 g,
50 g and so on, just put them together out of the combinations for 15
gram and for 20 gram (from Table 2).  For example,
</p><blockquote><pre> 50 g  =  (2 * 15 g) +      20 g        [ = 0.58 Euro ]
 75 g  =       15 g  + (3 * 20 g)       [ = 0.64 Euro ]
100 g  =                5 * 20 g        [ = 0.70 Euro ]
</pre></blockquote>
... and so on. 
<p>
Note (from Table 1), that the ratio of weight to monetary value of the
Euro coins drops uniformly with increasing monetary value of the coin.
That is, the smaller the coin, the more weight it gives you for the
smallest cost.  In order to keep things as cheap as possible, I have in
the above therefore preferably composed my weights from from the smaller
coins, as far as possible.  For the same reason, I've used none of the
coins of 1 Euro or more in my combinations.
</p><p>
When composing the larger &nbsp; <i>n</i> * 5 gram weights from the 15
g and 20 g combinations, note that the 20 g combination, composed in
relatively larger part from lower-value coins, is cheaper than the
15 g combination.  (The 20 g costing 0.14 Euro, and the 15 g costing
0.22 Euro.)
<!--
<div align=center>---</div>
-->
</p><hr>




</div>]]></description>
        </item>
    </channel>
</rss>