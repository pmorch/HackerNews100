<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 14 May 2024 19:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Veo (216 pts)]]></title>
            <link>https://deepmind.google/technologies/veo/</link>
            <guid>40358041</guid>
            <pubDate>Tue, 14 May 2024 17:58:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/technologies/veo/">https://deepmind.google/technologies/veo/</a>, See on <a href="https://news.ycombinator.com/item?id=40358041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p data-block-key="6gb7v">Veo is our most capable video generation model to date. It generates high-quality, 1080p resolution videos that can go beyond a minute, in a wide range of cinematic and visual styles.</p><p data-block-key="c6nlk">It accurately captures the nuance and tone of a prompt, and provides an unprecedented level of creative control — understanding prompts for all kinds of cinematic effects, like time lapses or aerial shots of a landscape.</p><p data-block-key="a5heq">Our video generation model will help create tools that make video production accessible to everyone. Whether you're a seasoned filmmaker, aspiring creator, or educator looking to share knowledge, Veo unlocks new possibilities for storytelling, education and more.</p><p data-block-key="8rfgr">Over the coming weeks some of these features will be available to select creators through <a href="http://labs.google/videofx" rel="noopener" target="_blank">VideoFX</a>, a new experimental tool at labs.google. You can join the <a href="http://labs.google/VideoFX" rel="noopener" target="_blank">waitlist</a> now.</p><p data-block-key="5veag">In the future, we’ll also bring some of Veo’s capabilities to <a href="https://www.youtube.com/shorts" rel="noopener" target="_blank">YouTube Shorts</a> and other products.</p>
</div><div>
  <h2 data-block-key="6gb7v">Greater understanding of language and vision</h2><p data-block-key="ecr6v">To produce a coherent scene, generative video models need to accurately interpret a text prompt and combine this information with relevant visual references.</p><p data-block-key="6mpal">With advanced understanding of natural language and visual semantics, Veo generates video that closely follows the prompt. It accurately captures the nuance and tone in a phrase, rendering intricate details within complex scenes.</p>
</div><div>
  <h2 data-block-key="penxw">Controls for film-making</h2><p data-block-key="eg3us">When given both an input video and editing command, like adding kayaks to an aerial shot of a coastline, Veo can apply this command to the initial video and create a new, edited video.</p>
</div><div>
  <p data-block-key="40xdi">In addition, it supports masked editing, enabling changes to specific areas of the video when you add a mask area to your video and text prompt.</p><p data-block-key="8tcfm">Veo can also generate a video with an image as input along with the text prompt. By providing a reference image in combination with a text prompt, it conditions Veo to generate a video that follows the image’s style and user prompt’s instructions.</p>
</div><p data-block-key="u0m94">The model is also able to make video clips and extend them to 60 seconds and beyond. It can do this either from a single prompt, or by being given a sequence of prompts which together tell a story.</p><div>
  <h2 data-block-key="hx1qs">Consistency across video frames</h2><p data-block-key="dl81n">Maintaining visual consistency can be a challenge for video generation models. Characters, objects, or even entire scenes can flicker, jump, or morph unexpectedly between frames, disrupting the viewing experience.</p><p data-block-key="77hdm">Veo's cutting-edge latent diffusion transformers reduce the appearance of these inconsistencies, keeping characters, objects and styles in place, as they would in real life.</p>
</div><div>
  <h2 data-block-key="t4vz1">Built upon years of video generation research</h2><p data-block-key="5icm9">Veo builds upon years of generative video model work including <a href="https://deepmind.google/discover/blog/neural-scene-representation-and-rendering/" rel="noopener" target="_blank">Generative Query Network</a> (GQN), <a href="https://arxiv.org/abs/1907.06571" rel="noopener" target="_blank">DVD-GAN</a>, <a href="https://imagen.research.google/video/" rel="noopener" target="_blank">Imagen-Video</a>, <a href="https://phenaki.video/" rel="noopener" target="_blank">Phenaki</a>, <a href="https://walt-video-diffusion.github.io/" rel="noopener" target="_blank">WALT</a>, <a href="https://sites.research.google/videopoet/" rel="noopener" target="_blank">VideoPoet</a> and <a href="https://lumiere-video.github.io/" rel="noopener" target="_blank">Lumiere</a>, and also our <a href="https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/" rel="noopener" target="_blank">Transformer architecture</a> and <a href="https://deepmind.google/technologies/gemini/#introduction" rel="noopener" target="_blank">Gemini</a>.</p><p data-block-key="3mt37">To help Veo understand and follow prompts more accurately, we have also added more details to the captions of each video in its training data. And to further improve performance, the model uses high-quality, compressed representations of video (also known as latents) so it’s more efficient too. These steps improve overall quality and reduce the time it takes to generate videos.</p>
</div><div>
  <h2 data-block-key="wdxqg">Responsible by design</h2><p data-block-key="conb7">It's critical to bring technologies like Veo to the world responsibly. Videos created by Veo are watermarked using <a href="https://deepmind.google/technologies/synthid/" rel="noopener" target="_blank">SynthID</a>, our cutting-edge tool for watermarking and identifying AI-generated content, and passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.</p><p data-block-key="5bgom">Veo’s future will be informed by our work with leading creators and filmmakers. Their feedback helps us improve our generative video technologies and makes sure they benefit the wider creative community and beyond.</p>
</div><p data-block-key="3s614"><i>Note: All videos on this page were generated by Veo and have not been modified.</i></p><div>
  <h3 data-block-key="fnixr">Acknowledgements</h3><p data-block-key="67cd">This work was made possible by the exceptional contributions of: Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Bińkowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe Nørly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du and Yutian Chen.</p><p data-block-key="52ut7">We extend our gratitude to Aida Nematzadeh, Alex Cullum, April Lehman, Aäron van den Oord, Charlie Chen, Charline Le Lan, Cristian Țăpuș, David Bridson, Emanuel Taropa, Gavin Buttimore, Geng Yan, Greg Shaw, Harsha Vashisht, Hartwig Adam, Huisheng Wang, Jacob Austin, Jim Lin, Jonas Adler, Joost van Amersfoort, Jordi Pont-Tuset, Josh V. Dillon, Kristian Kjems, Lois Zhou, Luis C. Cobo, Maigo Le, Malcolm Reynolds, Marcus Wainwright, Mary Cassin, Matt Smart, Matt Young, Mingda Zhang, Minh Giang, Moritz Dickfeld, Nancy Xiao, Nelly Papalampidi, Nir Shabat, Ollie Purkiss, Oskar Bunyan, Patrice Oehen, Pete Aykroyd, Petko Georgiev, Phil Chen, Rakesh Shivanna, Ramya Ganeshan, Richard Nguyen, RJ Mical, Rohan Anil, Sam Haves, Shanshan Zheng, Sholto Douglas, Siddhartha Brahma, Tatiana López, Tobias Pfaff, Victor Gomes, Vighnesh Birodkar, Xin Chen, Yi-Ling Wang, Yilin Ma, Yori Zwols, Yu Qiao, Yuchen Liang, Yusuf Aytar and Zu Kim for their invaluable partnership in developing and refining key components of this project.</p><p data-block-key="2t4u">Special thanks to Douglas Eck, Nando de Freitas, Oriol Vinyals, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.</p><p data-block-key="5hie">We also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Edge AI Model Explorer (125 pts)]]></title>
            <link>https://github.com/google-ai-edge/model-explorer</link>
            <guid>40357681</guid>
            <pubDate>Tue, 14 May 2024 17:29:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-ai-edge/model-explorer">https://github.com/google-ai-edge/model-explorer</a>, See on <a href="https://news.ycombinator.com/item?id=40357681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Model Explorer <a href="https://badge.fury.io/py/ai-edge-model-explorer" rel="nofollow"><img src="https://camo.githubusercontent.com/590aa6824f7a10547298327b66cfcc6a926b355943eb17b763a9ec05aa80db71/68747470733a2f2f62616467652e667572792e696f2f70792f61692d656467652d6d6f64656c2d6578706c6f7265722e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/ai-edge-model-explorer.svg"></a></h2><a id="user-content-model-explorer-" aria-label="Permalink: Model Explorer " href="#model-explorer-"></a></div>
<p dir="auto">Model Explorer offers an intuitive and hierarchical visualization of model
graphs. It organizes model operations into nested layers, enabling users to
dynamically expand or collapse these layers. It also provides a range of
features to facilitate model exploration and debugging, including the ability to
highlight input and output operations, overlay metadata on nodes, display layers
in interactive pop-ups, perform searches, show identical layers, GPU-accelerated
graph rendering, among others. It currently supports TFLite, TF, TFJS, MLIR, and
PyTorch (Exported Program) model format, and provides an extension framework for
developers to easily add support for additional formats.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/google-ai-edge/model-explorer/blob/main/screenshots/main_ui.png"><img width="890" alt="Home page screenshot" src="https://github.com/google-ai-edge/model-explorer/raw/main/screenshots/main_ui.png"></a></p>

<p dir="auto">To start using Model Explorer, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ pip install ai-edge-model-explorer
$ model-explorer"><pre>$ pip install ai-edge-model-explorer
$ model-explorer</pre></div>
<p dir="auto">Please check out our <a href="https://github.com/google-ai-edge/model-explorer/wiki">Wiki</a> for
more details:</p>
<ul dir="auto">
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/1.-Installation">Installation</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/2.-User-Guide">User Guide</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/3.-Command-Line-Guide">Command Line Guide</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/4.-API-Guide">API Guide</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/5.-Run-in-Colab-Notebook">Run in Colab Notebook</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/6.-Develop-Adapter-Extension">Develop Adapter Extension</a></li>
<li><a href="https://github.com/google-ai-edge/model-explorer/wiki/7.-Limitations-and-Known-Issues">Limitations and Known Issues</a></li>
</ul>
<p dir="auto">We invite you to participate in our future research studies on Model Explorer. Sign up <a href="https://docs.google.com/forms/d/e/1FAIpQLScGOkQOIKmIzkt3P0ywhSfwbl-TRb2epEV5J8NTXEesZqc3vw/viewform" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">We are not accepting contributions to Model Explorer at this time. The Model Explorer team will contribute to this repository.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VMware Fusion Pro and Workstation Pro now free for all personal use (153 pts)]]></title>
            <link>https://blogs.vmware.com/teamfusion/2024/05/fusion-pro-now-available-free-for-personal-use.html</link>
            <guid>40357271</guid>
            <pubDate>Tue, 14 May 2024 16:55:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.vmware.com/teamfusion/2024/05/fusion-pro-now-available-free-for-personal-use.html">https://blogs.vmware.com/teamfusion/2024/05/fusion-pro-now-available-free-for-personal-use.html</a>, See on <a href="https://news.ycombinator.com/item?id=40357271">Hacker News</a></p>
Couldn't get https://blogs.vmware.com/teamfusion/2024/05/fusion-pro-now-available-free-for-personal-use.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sir, there's a cat in your mirror dimension (110 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/sir-theres-a-cat-in-your-mirror-dimension</link>
            <guid>40357141</guid>
            <pubDate>Tue, 14 May 2024 16:42:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/sir-theres-a-cat-in-your-mirror-dimension">https://lcamtuf.substack.com/p/sir-theres-a-cat-in-your-mirror-dimension</a>, See on <a href="https://news.ycombinator.com/item?id=40357141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>A </span><a href="https://lcamtuf.substack.com/p/not-so-fast-mr-fourier" rel="">while back</a><span>, we talked about the </span><em>frequency domain</em><span>: a clever reinterpretation of everyday signals that translates them into the amplitudes of constituent waveforms. The most common basis for this operation are sine waves running at increasing frequencies, but </span><a href="https://lcamtuf.substack.com/p/is-the-frequency-domain-a-real-place" rel="">countless other waveforms</a><span> can be used to create a number of alternative frequency domains.</span></p><p><span>In that earlier article, I also noted two important properties of frequency domain transforms. First, they are reversible: you can recover the original </span><em>(“time domain” </em><span>or </span><em>“spatial domain”</em><span>) data from its frequency image. Second, the transforms have input-output symmetry: the same mathematical operation is used to go both ways. In effect, we have a lever that takes us to a mirror dimension and back. Which of the lever positions is called home is a matter of habit, not math.</span></p><p>Of course, in real life, the distinction matters — and it’s particularly important for compression. If you take an image, convert it to the frequency-domain representation, and then reduce the precision of (or outright obliterate!) the high-frequency components, the resulting image still looks perceptually the same — but you now have much less data to transmit or store:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1032742,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb2d736-32e9-47f3-a2b1-3d3dd38a8690_1600x1600.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><em>Using MS Paint as a cutting-edge compression tool.</em></figcaption></figure></div><p>This makes you wonder: if the frequency-domain representation of a typical image looks like diffuse noise, if most of it is perceptually unimportant, and if the transform is just a lever that takes us back and forth between two functionally-equivalent dimensions… could we start calling that mirror dimension home and move some stuff in?</p><p>To answer this stoner question, I grabbed a photo of a cat and then calculated its frequency-domain form with the discrete cosine transform (DCT):</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg" width="1456" height="728" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:728,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:641657,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89dea704-5264-47a5-a8c7-a8a7765d1cc2_1600x800.jpeg 1456w" sizes="100vw"></picture></div></a><figcaption><em>Time cat, frequency cat.</em></figcaption></figure></div><p>Next, I reused the photo of a woman from an earlier example and placed the mirror-dimension “cat noise” pattern over it, dialing down opacity to minimize visible artifacts:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png" width="800" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3bb8d20-72c3-42e7-b588-65d712818362_800x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:308512,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3bb8d20-72c3-42e7-b588-65d712818362_800x800.png 1456w" sizes="100vw"></picture></div></a><figcaption><em>Time woman with a frequency cat.</em></figcaption></figure></div><p>The compositing operation is necessarily lossy, but my theory was that if the composite image is run through DCT to compute its frequency-domain representation, the photo of a woman would be decomposed to fairly uniform noise, perhaps easy to attenuate with a gentle blur; while the injected “cat noise” would coalesce into a perceptible image of a cat.</p><p>But would it?… Yes!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png" width="800" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:406694,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cb815ef-c780-4dfb-ad6e-bdd84a1f3814_800x800.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Frequency cat with a time woman.</em></figcaption></figure></div><p><span>If you want to see for yourself, </span><a href="https://lcamtuf.coredump.cx/woman-with-cat.png" rel="">download the composite image</a><span> and have fun. In MATLAB, you can do the following:</span></p><blockquote><pre><code>woman = imread("woman-with-cat.png");

colormap('gray');
imagesc(woman, [0 255]);
pause(1);

cat = dct2(woman);
imagesc(imgaussfilt(cat, 1), [-4 4]);</code></pre></blockquote><p>Interestingly, the kitty survives resizing of the host document. Upscaling tiles the image; downscaling truncates it.</p><p>My lingering question was how badly the cat would get mangled by lossy compression; as it turns out, the impact is less than I expected. At higher JPEG quality settings, the image looks quite OK. As the quality setting is lowered, the bottom right quadrant — corresponding to higher-frequency components — gets badly quantized:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1197263,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb583fe71-a273-4e72-b583-44ad3c04568a_1600x1600.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The havoc of JPEG compression, as seen in the frequency domain.</em></figcaption></figure></div><p>This visualization offers a fascinating glimpse of just how much information is destroyed by the JPEG algorithm — mostly without us noticing.</p><p>There’s plenty of prior art for using audio spectrograms for hidden messages, and some discussion of text steganography piggybacked on top of JPEG DCT coefficients. My point isn’t that the technique is particularly useful or that it has absolutely no precedent. It’s just that the frequency domain and the time domain are coupled together in funny ways.</p><p><em><span>For more articles about electronics, algorithms, snowplowing, and 19th century repeating pistols, see </span><a href="https://lcamtuf.coredump.cx/offsite.shtml" rel="">this categorized list</a><span>.</span></em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox search update (174 pts)]]></title>
            <link>https://blog.mozilla.org/en/products/firefox/firefox-search-update/</link>
            <guid>40355982</guid>
            <pubDate>Tue, 14 May 2024 15:07:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/products/firefox/firefox-search-update/">https://blog.mozilla.org/en/products/firefox/firefox-search-update/</a>, See on <a href="https://news.ycombinator.com/item?id=40355982">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <main id="main">

    
<article id="post-74713">
  

  <div>
    
<figure><img decoding="async" fetchpriority="high" width="1024" height="538" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-1024x538.png" alt="An illustration shows the Firefox logo, a fox curled up in a circle." srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-1024x538.png 1024w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-300x158.png 300w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-768x403.png 768w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-1536x806.png 1536w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-2048x1075.png 2048w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/01/firefox-browser-logo-1000x525.png 1000w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Innovation and privacy go hand in hand here at Mozilla. To continue developing features and products that resonate with our users, we’re adopting a new approach to better understand how you engage with Firefox. Rest assured, the way we gather these insights will always put user <a href="https://www.mozilla.org/en-US/privacy/firefox/" target="_blank" rel="noreferrer noopener">privacy first</a>.</p>



<h2><strong>What’s new in Firefox’s approach to search data&nbsp;</strong></h2>



<p>To improve Firefox based on your needs, understanding how users interact with essential functions like search is key. We’re ramping up our efforts to enhance search experience by developing new features like <a href="https://blog.mozilla.org/products/firefox/firefox-news/firefox-suggest/">Firefox Suggest</a>, which provides recommended online content that corresponds to queries. To make sure that features like this work well, we need better insights on overall search activity – all without trading off on our commitment to user privacy. Our goal is to understand what types of searches are happening so that we can prioritize the correct features by use case.</p>



<div><p>With the <a href="https://www.mozilla.org/firefox/releases/" target="_blank" rel="noreferrer noopener">latest version of Firefox</a> for U.S. desktop users, we’re introducing a new way to measure search activity broken down into high level categories. This measure is not linked with specific individuals and is further anonymized using a technology called <a href="https://blog.mozilla.org/products/firefox/partnership-ohttp-prio/">OHTTP</a> to ensure it can’t be connected with user <a href="https://blog.mozilla.org/internet-culture/what-is-an-ip-address/">IP addresses</a>.&nbsp;&nbsp;&nbsp;&nbsp;</p><p>Let’s say you’re using Firefox to <a href="https://blog.mozilla.org/products/firefox/firefox-tips/internet-safety-for-families-total-cookie-protection/">plan a trip</a> to Spain and search for “Barcelona hotels.” Firefox infers that the search results fall under the category of “travel,” and it increments a counter to calculate the total number of searches happening at the country level.</p><p>Here’s the current list of categories we’re using: animals, arts, autos, business, career, education, fashion, finance, food, government, health, hobbies, home, inconclusive, news, real estate, society, sports, tech and travel.</p></div>



<p>Having an understanding of what types of searches happen most frequently will give us a better understanding of what’s important to our users, without giving us additional insight into individual browsing preferences. This helps us take a step forward in providing a browsing experience that is more tailored to your needs, without us stepping away from the principles that make us who we are.&nbsp;</p>



<h2><strong>What Firefox’s search data collection means for you</strong></h2>



<p>We understand that any new data collection might spark some questions. Simply put, this new method only categorizes the websites that show up in your searches — not the specifics of what you’re personally looking up.&nbsp;</p>



<p>Sensitive topics, like searching for particular health care services, are categorized only under broad terms like health or society. Your search activities are handled with the same level of confidentiality as all other data regardless of any local laws surrounding certain health services.&nbsp;</p>



<p>Remember, you can always opt out of sending any technical or usage data to Firefox. <a href="https://support.mozilla.org/en-US/kb/share-data-mozilla-help-improve-firefox" target="_blank" rel="noreferrer noopener">Here’s a step-by-step guide</a> on how to adjust your settings. We also don’t collect category data when you use <a href="https://blog.mozilla.org/mozilla/firefoxs-private-browsing-mode-upleveled-for-you/">Private Browsing mode</a> on Firefox.&nbsp;&nbsp;</p>



<p>As far as user experience goes, you won’t see any visible changes in your browsing. Our new approach to data will just enable us to better refine our product features and offerings in ways that matter to you.&nbsp;</p>



<p>We’re here to make the internet safer, faster and more in tune with what you need – just as we have since open-sourcing our browser code more than <a href="https://blog.mozilla.org/mozilla/mitchell-baker-mozilla-25-anniversary/">25 years ago</a>. Thanks for being part of our journey!</p>



<a href="https://www.mozilla.org/en-US/firefox/new?utm_medium=mozilla-websites&amp;utm_source=blog.mozilla.org&amp;utm_content=inline-cta">
  <p><img width="512" height="512" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/12/Fx-Browser-icon-fullColor-512-512x512.png" alt="" decoding="async" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/12/Fx-Browser-icon-fullColor-512.png 512w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/12/Fx-Browser-icon-fullColor-512-300x300.png 300w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2020/12/Fx-Browser-icon-fullColor-512-150x150.png 150w" sizes="(max-width: 512px) 100vw, 512px">  </p>
  <div>
     <h3>Get Firefox</h3>      <p><span>Get the browser that protects what’s important</span>   </p></div>
</a>
  </div>

</article><!-- #post-74713 -->

  </main><!-- #main -->
  

<div id="related-articles">
    <h2>Related Articles</h2>
    
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blazingly fast linked lists (149 pts)]]></title>
            <link>https://dygalo.dev/blog/blazingly-fast-linked-lists/</link>
            <guid>40355227</guid>
            <pubDate>Tue, 14 May 2024 13:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dygalo.dev/blog/blazingly-fast-linked-lists/">https://dygalo.dev/blog/blazingly-fast-linked-lists/</a>, See on <a href="https://news.ycombinator.com/item?id=40355227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
  <figure>
    <img src="https://dygalo.dev/blog/2048px-Rusty_chain_in_fishing_boat.jpg">
    
      <figcaption>© Tomas Castelazo, www.tomascastelazo.com / Wikimedia Commons / CC BY-SA 4.0</figcaption>
    
  </figure>

<p>Linked lists are <a href="https://rust-unofficial.github.io/too-many-lists/" target="_blank">taught</a> as fundamental data structures in programming courses, but they are more commonly encountered in tech interviews than in real-world projects.</p>
<p>In this post, I'll demonstrate a practical use case where a linked list significantly outperforms <code>Vec</code>. 
We will build a simple data validation library that shows the exact error location within invalid input, showcasing how a linked list can be used in graph traversals.</p>
<p>Starting with a naive approach, we’ll progressively implement various optimizations and observe their impact on performance. </p>
<p>Readers are expected to have a basic understanding of Rust, common data structures, and the concept of memory allocations (stack vs. heap).</p>
<blockquote>
<p>To follow along with the implementation steps and explore the code, check out the accompanying <a href="https://github.com/Stranger6667/article-linked-lists" target="_blank">repository</a></p>
</blockquote>
<h2 id="validation-api">Validation API</h2>
<p>Our library is fairly minimal, and follows <a href="https://json-schema.org/specification#specification-documents" target="_blank">JSON Schema</a> semantics:</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>serde_json::json;
</span><span>
</span><span>fn </span><span>main</span><span>() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
</span><span>    jsonschema::validate(
</span><span>      </span><span>// JSON instance to validate
</span><span>      &amp;json!({
</span><span>          "</span><span>name</span><span>": "</span><span>John</span><span>",
</span><span>          "</span><span>location</span><span>": {
</span><span>               "</span><span>country</span><span>": </span><span>404
</span><span>          }
</span><span>      }),
</span><span>      </span><span>// JSON schema
</span><span>      &amp;json!({
</span><span>          "</span><span>properties</span><span>": {
</span><span>              "</span><span>name</span><span>": {
</span><span>                  "</span><span>type</span><span>": "</span><span>string</span><span>"
</span><span>              },
</span><span>              "</span><span>location</span><span>": {
</span><span>                  "</span><span>properties</span><span>": {
</span><span>                      "</span><span>country</span><span>": {
</span><span>                          "</span><span>type</span><span>": "</span><span>string</span><span>"
</span><span>                      }
</span><span>                  }
</span><span>              }
</span><span>          }
</span><span>      }),
</span><span>    ).</span><span>expect_err</span><span>("</span><span>Should fail</span><span>");
</span><span>    Ok(())
</span><span>}
</span></code></pre>
<p>In this example, <code>404</code> is not a string and this code should result in an error like this:</p>
<pre><code><span>404 is not of type ‘string’ at /location/country
</span><span> |                             |               |
</span><span> |                              ---------------
</span><span> |                                     |
</span><span> |                                     Location within the JSON instance
</span><span> |
</span><span>  Failing value
</span></code></pre>
<blockquote>
<p>This library and optimization ideas are derived from my <a href="https://github.com/Stranger6667/jsonschema-rs" target="_blank">jsonschema</a> crate</p>
</blockquote>
<p>The validation process boils down to graph traversal. The input instance is traversed based on rules defined in the schema. At any traversal step, we should know the current location within the JSON instance to potentially report a meaningful error.</p>

  <p><img src="https://dygalo.dev/blog/validation-process.png"></p><p>Our primary goal is to implement location tracking while minimizing its impact on the library's performance.</p>
<p>Without diving too deep into the <code>Validator</code> and <code>Node</code> implementations, let's see a simplified version of the validation process without location tracking:</p>
<pre data-lang="rust"><code data-lang="rust"><span>type </span><span>ValidationResult = Result&lt;(), ValidationError&gt;;
</span><span>
</span><span>fn </span><span>validate</span><span>(</span><span>instance</span><span>: &amp;Value, </span><span>schema</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>    Validator::new(schema)
</span><span>        .</span><span>expect</span><span>("</span><span>Invalid schema</span><span>")
</span><span>        .</span><span>validate</span><span>(instance)
</span><span>}
</span><span>
</span><span>struct </span><span>ValidationError {
</span><span>    </span><span>message</span><span>: String,
</span><span>}
</span><span>
</span><span>impl </span><span>Validator {
</span><span>    </span><span>/// Validate JSON instance against this validator.
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>        </span><span>self</span><span>.node.</span><span>validate</span><span>(instance)
</span><span>    }
</span><span>}
</span><span>
</span><span>trait </span><span>Node {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult;
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>        </span><span>if let </span><span>Value::Object(object) = instance {
</span><span>            </span><span>// Iterate over properties and validate them if they are present.
</span><span>            </span><span>for </span><span>(key, value) in &amp;</span><span>self</span><span>.properties {
</span><span>                </span><span>if let </span><span>Some(instance) = object.</span><span>get</span><span>(key) {
</span><span>                    </span><span>// Delegate validation to the child validator.
</span><span>                    value.</span><span>validate</span><span>(instance)?;
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>        Ok(())
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>        </span><span>// ... 
</span><span>    }
</span><span>}
</span></code></pre>
<p>The <code>validate</code> function creates a new <code>Validator</code> instance (that is also a graph) from the provided schema and then calls its <code>validate</code> method with the JSON instance.
The <code>Node</code> trait defines the <code>validate</code> method that each validation rule must implement.</p>
<p>While this version provides error messages without location tracking, it serves as an upper bound for subsequent optimizations within the <code>validate</code> function.</p>
<h2 id="benchmark-setup">Benchmark setup</h2>
<p>To ensure the optimizations are relevant to the library's typical usage scenarios, we select inputs from different groups - valid and invalid instances of varying sizes. </p>
<p>The <a href="https://github.com/Stranger6667/article-linked-lists/blob/main/benches/data.json#L2" target="_blank">schema</a> contains 10 levels of nesting and is deliberately restricted to only the <code>properties</code> and <code>type</code> keywords, which are sufficient to demonstrate the overhead of path-tracking behavior while simplifying benchmarking and keeping our focus on performance rather than JSON Schema semantics. It's worth noting that the path-tracking behavior will remain largely the same for other keywords as well.</p>
<pre data-lang="json"><code data-lang="json"><span>{
</span><span>    "</span><span>properties</span><span>":{
</span><span>        "</span><span>another</span><span>":{
</span><span>            "</span><span>type</span><span>":"</span><span>string</span><span>"
</span><span>        },
</span><span>        "</span><span>inner</span><span>":{
</span><span>            "</span><span>properties</span><span>":{
</span><span>                "</span><span>another</span><span>":{
</span><span>                    "</span><span>type</span><span>":"</span><span>string</span><span>"
</span><span>                },
</span><span>                "</span><span>inner</span><span>":{
</span><span>                    "</span><span>properties</span><span>":{
</span><span>                        </span><span>// And so on for up to 10 levels
</span><span>                    }
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The instances have 0, 5, or 10 levels of nesting, following the schema's structure. Valid instances have a string value for the "another" property at the deepest level, while invalid instances have an integer.</p>
<pre data-lang="json"><code data-lang="json"><span>// Valid - 5 levels
</span><span>{
</span><span>    "</span><span>inner</span><span>":{
</span><span>        "</span><span>inner</span><span>":{
</span><span>            "</span><span>inner</span><span>":{
</span><span>                "</span><span>inner</span><span>":{
</span><span>                    "</span><span>another</span><span>":"</span><span>hello</span><span>"
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span><span>// Invalid - 5 levels
</span><span>{
</span><span>    "</span><span>inner</span><span>":{
</span><span>        "</span><span>inner</span><span>":{
</span><span>            "</span><span>inner</span><span>":{
</span><span>                "</span><span>inner</span><span>":{
</span><span>                    "</span><span>another</span><span>":</span><span>1
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>To focus on the performance of the validation process itself, the schema is hardcoded in <code>Validator::new</code>, and <code>Validator</code> is built once and then reused.</p>
<p>Let’s measure performance with <a href="https://bheisler.github.io/criterion.rs/book/criterion_rs.html" target="_blank">criterion</a> by running the following validation routine:</p>
<pre data-lang="rust"><code data-lang="rust"><span>const </span><span>NUMBER_OF_ITERATIONS</span><span>: </span><span>usize </span><span>= </span><span>10000</span><span>;
</span><span>
</span><span>fn </span><span>benchmarks</span><span>(</span><span>c</span><span>: &amp;</span><span>mut</span><span> Criterion) {
</span><span>    </span><span>// snip ... 
</span><span>    </span><span>for</span><span> instance in &amp;benchmark.instances {
</span><span>        c.</span><span>bench_with_input</span><span>(
</span><span>            BenchmarkId::new(instance.kind, &amp;instance.name),
</span><span>            &amp;instance.value,
</span><span>            |</span><span>b</span><span>: &amp;</span><span>mut</span><span> Bencher, </span><span>value</span><span>| {
</span><span>                b.</span><span>iter</span><span>(|| {
</span><span>                    </span><span>for </span><span>_ in </span><span>0</span><span>..</span><span>NUMBER_OF_ITERATIONS </span><span>{
</span><span>                        </span><span>let </span><span>_ = validator.</span><span>validate</span><span>(value);
</span><span>                    }
</span><span>                });
</span><span>            },
</span><span>        );
</span><span>    }
</span><span>}
</span></code></pre>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/a030dcb18448555efa1a8f63f8b5ccebef7d2f59" target="_blank">a030dcb</a></td>
  <td>36.3 µs</td>
  <td>553.8 µs</td>
  <td>1.11 ms</td>
  <td>475.2 µs</td>
  <td>914.8 µs</td>
  <td>1.48 ms</td>
</tr>
  </tbody>
</table>
<blockquote>
<p>The numbers in these microbenchmarks are not absolute and may vary depending on the hardware and the environment. Generally, the observed changes could be explained by stack traces in flame graphs, but you always need to check the benchmarks yourself.</p>
</blockquote>
<h2 id="naive-approach">Naive approach</h2>
<p>Let's start from collecting traversed path segments in a vector adding a new segment each time validation goes one level deeper:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>    </span><span>// Start with an empty vector
</span><span>    </span><span>self</span><span>.node.</span><span>validate</span><span>(instance, vec![])
</span><span>}
</span><span>
</span><span>trait </span><span>Node {
</span><span>    </span><span>// Add `path` parameter to track the current location in the JSON instance
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vec&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult;
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vec&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult {
</span><span>        </span><span>if let </span><span>Value::Object(object) = instance {
</span><span>            </span><span>for </span><span>(key, value) in &amp;</span><span>self</span><span>.properties {
</span><span>                </span><span>if let </span><span>Some((key, instance)) = object.</span><span>get_key_value</span><span>(key) {
</span><span>                    </span><span>// Create a new path and pass it to the child node
</span><span>                    </span><span>let mut</span><span> path = path.</span><span>clone</span><span>();
</span><span>                    path.</span><span>push</span><span>(key);
</span><span>                    value.</span><span>validate</span><span>(instance, path)?;
</span><span>                }
</span><span>            }
</span><span>        }
</span><span>        Ok(())
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vec&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult {
</span><span>        </span><span>match </span><span>(</span><span>self</span><span>, instance) {
</span><span>            </span><span>// ... Compare `instance` type with expected type
</span><span>            _ =&gt; Err(ValidationError::new(
</span><span>                format!("</span><span>{instance}</span><span> is not of type '</span><span>{self}</span><span>'</span><span>"),
</span><span>                </span><span>// Convert path to an iterator
</span><span>                path.</span><span>into_iter</span><span>(),
</span><span>            )),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>The <code>ValidationError</code> struct now stores this path:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>ValidationError {
</span><span>    </span><span>message</span><span>: String,
</span><span>    </span><span>/// Error location within the input instance.
</span><span>    </span><span>location</span><span>: Vec&lt;String&gt;,
</span><span>}
</span><span>
</span><span>impl </span><span>ValidationError {
</span><span>    </span><span>/// Create new validation error.
</span><span>    </span><span>fn </span><span>new</span><span>(
</span><span>        </span><span>message</span><span>: impl Into&lt;String&gt;,
</span><span>        </span><span>// Accept an iterator and convert it to `Vec&lt;String&gt;`
</span><span>        </span><span>location</span><span>: impl Iterator&lt;Item = impl Into&lt;String&gt;&gt;,
</span><span>    ) -&gt; </span><span>Self </span><span>{
</span><span>        </span><span>Self </span><span>{
</span><span>            message: message.</span><span>into</span><span>(),
</span><span>            location: location.</span><span>map</span><span>(Into::into).</span><span>collect</span><span>(),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>If you've been writing Rust for a while, you'll likely recognize the <code>clone()</code> call as a common "solution" to lifetime and mutability issues. While it is acceptable in certain situations, depending on your performance and maintainability constraints, it often signals an opportunity for optimization. Cloning a <code>Vec</code> can be costly, especially in performance-critical code.</p>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/9ef7b4c56c8ca2ba3dcd15681daff6951aa64c2c" target="_blank">9ef7b4c</a></td>
  <td>40.9 µs<br>(<strong>+12%</strong>)</td>
  <td>2.61 ms<br>(<strong>+369.4%</strong>)</td>
  <td>6.69 ms<br>(<strong>+499.6%</strong>)</td>
  <td>961.2 µs<br>(<strong>+100.8%</strong>)</td>
  <td>4.11 ms<br>(<strong>+346.8%</strong>)</td>
  <td>9.07 ms<br>(<strong>+502.7%</strong>)</td>
</tr>
  </tbody>
</table>
<p>This feature makes validation up to 6 times slower! </p>
<blockquote>
<p>You may note that <code>path</code> is only used when one of the validators returns an error. Can we leverage this observation for optimizations?</p>
</blockquote>
<p>Let’s visualize the slowest "valid" benchmark using <code>cargo flamegraph</code> to understand what is going on. As expected, the memory reallocations show up in the flame graph:</p>
<pre data-lang="shell"><code data-lang="shell"><span>cargo flamegraph --bench jsonschema -o naive-10.svg -- --bench "^valid/10 levels"
</span></code></pre>

  <p><img src="https://dygalo.dev/blog/naive-valid-10.png"></p><blockquote>
<p>Flame graphs are amazing for visualizing stack traces, see more details <a href="https://github.com/flamegraph-rs/flamegraph" target="_blank">here</a> </p>
</blockquote>
<h2 id="idea-1-cheaper-clones">Idea 1: Cheaper clones</h2>
<p>There are a couple of ways to deal with clones. First, let's try a different data structure that offers cheaper clones. The <a href="https://docs.rs/imbl/latest/imbl/" target="_blank">imbl</a> crate provides the <code>Vector</code> type, which is based on the <a href="https://infoscience.epfl.ch/record/213452/files/rrbvector.pdf">Relaxed-Radix-Balanced tree</a>.</p>
<p>RRB trees are a type of immutable data structure that allows for efficient structural sharing. This concept allows most memory to be shared among multiple mostly identical data structures, and extra memory is only allocated when modifications occur, thus requiring memory only to record the difference. With most operations being O(log n), <code>Vector </code> offers decent performance in many scenarios.</p>
<pre data-lang="rust"><code data-lang="rust"><span>use </span><span>imbl::Vector;
</span><span>
</span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>    </span><span>self</span><span>.node.</span><span>validate</span><span>(instance, Vector::new())
</span><span>}
</span><span>
</span><span>trait </span><span>Node {
</span><span>    </span><span>// Replace `Vec` with `imbl::Vector`
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vector&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult;
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vector&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult 
</span><span>        </span><span>// ...
</span><span>                    </span><span>let mut</span><span> path = path.</span><span>clone</span><span>();
</span><span>                    </span><span>// Use `push_back` instead of `push`
</span><span>                    path.</span><span>push_back</span><span>(key);
</span><span>                    value.</span><span>validate</span><span>(instance, path)?;
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>path</span><span>: Vector&lt;&amp;</span><span>str</span><span>&gt;) -&gt; ValidationResult 
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/77adb2c34ef95b978e90429c80bad59a422caa39" target="_blank">77adb2c</a></td>
  <td>47.1 µs<br>(<strong>+13.6%</strong>)</td>
  <td>2.25 ms<br>(<strong>-15.6%</strong>)</td>
  <td>6.49 ms<br>(<strong>-3.7%</strong>)</td>
  <td>904.3 µs<br>(<strong>-6.6%</strong>)</td>
  <td>4.09 ms<br>(<strong>-1.2%</strong>)</td>
  <td>9.77 ms<br>(<strong>+6.7%</strong>)</td>
</tr>
  </tbody>
</table>
<p>Interesting! In some cases, <code>Vector</code> outperforms the naive approach, while in others, it introduces slight overhead.  While <code>Vector</code> can be faster for certain data &amp; usage patterns due to structural sharing, it may introduce overhead for other patterns because of its more complex internal structure.</p>
<blockquote>
<p>In some scenarios, where modifications occur way less often than clones, you can consider using Arc as explained in this <a href="https://www.youtube.com/watch?v=A4cKi7PTJSs">video</a> . You may also try the <a href="https://docs.rs/rpds/latest/rpds/" target="_blank">rpds</a> crate which provides similar data structures with structural sharing.</p>
</blockquote>
<h2 id="idea-2-reuse-allocations">Idea 2: Reuse allocations</h2>
<p>However, you may note that we don't have to clone the data - what if we mutate the same vector, pushing and popping segments as we traverse?</p>
<pre data-lang="rust"><code data-lang="rust"><span>trait </span><span>Node {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: &amp;</span><span>mut </span><span>Vec&lt;&amp;</span><span>'a str</span><span>&gt;,
</span><span>    ) -&gt; ValidationResult;
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: &amp;</span><span>mut </span><span>Vec&lt;&amp;</span><span>'a str</span><span>&gt;,
</span><span>    ) -&gt; ValidationResult {
</span><span>        </span><span>// ...
</span><span>                    path.</span><span>push</span><span>(key);
</span><span>                    value.</span><span>validate</span><span>(instance, path)?;
</span><span>                    path.</span><span>pop</span><span>();
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: &amp;</span><span>mut </span><span>Vec&lt;&amp;</span><span>'a str</span><span>&gt;,
</span><span>    ) -&gt; ValidationResult {
</span><span>        </span><span>match </span><span>(</span><span>self</span><span>, instance) {
</span><span>            </span><span>// ... Compare `instance` type with expected type
</span><span>            _ =&gt; Err(ValidationError::new(
</span><span>                format!("</span><span>{instance}</span><span> is not of type '</span><span>{self}</span><span>'</span><span>"),
</span><span>                path.</span><span>iter</span><span>().</span><span>copied</span><span>(),
</span><span>            )),
</span><span>        }
</span><span>    }
</span></code></pre>
<p>The lifetime annotations (<code>'a</code>) are needed here because the path parameter is a mutable reference to a <code>Vec</code> that contains references to the <code>instance</code> parameter. The lifetime <code>'a</code> ensures that the references in <code>path</code> do not outlive the <code>instance</code> they refer to.</p>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/7c9473689bf24b90c8e0c45f700ad985b536a73e" target="_blank">7c94736</a></td>
  <td>40.2 µs<br>(<strong>-13.1%</strong>)</td>
  <td>1.24 ms<br>(<strong>-46.0%</strong>)</td>
  <td>2.46 ms<br>(<strong>-62.3%</strong>)</td>
  <td>951.7 µs<br>(<strong>+3.0%</strong>)</td>
  <td>2.39 ms<br>(<strong>-42.2%</strong>)</td>
  <td>4.16 ms<br>(<strong>-58.7%</strong>)</td>
</tr>
  </tbody>
</table>
<p>Indeed, using <code>&amp;mut Vec</code> can significantly improve performance compared to the naive approach, by reusing a single heap allocation instead of creating multiple clones. However, this approach requires more bookkeeping and somewhat more lifetime annotations, which can increase code complexity.</p>
<h2 id="idea-3-linked-list">Idea 3: Linked list</h2>
<p>However, is it even necessary to allocate heap memory for a vector during traversal? Consider this: for each traversed node in the input value, there's a corresponding <code>validate</code> function call with its own stack frame. As a result, path segments can be stored in these stack frames, eliminating the need for heap allocations completely.</p>

  <p><img src="https://dygalo.dev/blog/function-call-frames.png"></p><blockquote>
<p>Taking a step back and looking from a different angle can uncover ideas that may not be apparent at a lower level.</p>
</blockquote>
<p>By storing all segments on the stack, when an error happens, the previous segments can be traced back. This approach involves connecting each segment to form a path and collecting them when necessary. This sounds like a linked list:</p>
<pre data-lang="rust"><code data-lang="rust"><span>/// A node in a linked list representing a JSON pointer.
</span><span>struct </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>segment</span><span>: Option&lt;&amp;</span><span>'a str</span><span>&gt;,
</span><span>    </span><span>parent</span><span>: Option&lt;&amp;</span><span>'b </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt;&gt;,
</span><span>}
</span><span>impl</span><span>&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>/// Create a root node of a JSON pointer.
</span><span>    </span><span>const fn </span><span>new</span><span>() -&gt; </span><span>Self </span><span>{
</span><span>        JsonPointerNode {
</span><span>            segment: None,
</span><span>            parent: None,
</span><span>        }
</span><span>    }
</span><span>    </span><span>/// Push a new segment to the JSON pointer.
</span><span>    </span><span>fn </span><span>push</span><span>(&amp;</span><span>'a </span><span>self</span><span>, </span><span>segment</span><span>: &amp;</span><span>'a str</span><span>) -&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>        JsonPointerNode {
</span><span>            segment: Some(segment),
</span><span>            parent: Some(</span><span>self</span><span>),
</span><span>        }
</span><span>    }
</span><span>    </span><span>/// Convert the JSON pointer node to a vector of path segments.
</span><span>   </span><span>fn </span><span>to_vec</span><span>(&amp;</span><span>'a </span><span>self</span><span>) -&gt; Vec&lt;&amp;</span><span>'a str</span><span>&gt; {
</span><span>        </span><span>// Callect the segments from the head to the tail
</span><span>        </span><span>let mut</span><span> buffer = Vec::new();
</span><span>        </span><span>let mut</span><span> head = </span><span>self</span><span>;
</span><span>        </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>            buffer.</span><span>push</span><span>(*segment);
</span><span>        }
</span><span>        </span><span>while let </span><span>Some(next) = head.parent {
</span><span>            head = next;
</span><span>            </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>                buffer.</span><span>push</span><span>(*segment);
</span><span>            }
</span><span>        }
</span><span>        </span><span>// Reverse the buffer to get the segments in the correct order
</span><span>        buffer.</span><span>reverse</span><span>();
</span><span>        buffer
</span><span>    }
</span><span>}
</span></code></pre>
<p>Now we can replace <code>&amp;mut Vec</code>:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>    </span><span>self</span><span>.node.</span><span>validate</span><span>(instance, JsonPointerNode::new())
</span><span>}
</span><span>
</span><span>trait </span><span>Node {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: JsonPointerNode&lt;</span><span>'a</span><span>, '_&gt;,
</span><span>    ) -&gt; ValidationResult;
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: JsonPointerNode&lt;</span><span>'a</span><span>, '_&gt;,
</span><span>    ) -&gt; ValidationResult {
</span><span>        </span><span>// ...
</span><span>                    value.</span><span>validate</span><span>(instance, path.</span><span>push</span><span>(key.</span><span>as_str</span><span>()))?;
</span><span>        </span><span>// ...
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: JsonPointerNode&lt;</span><span>'a</span><span>, '_&gt;,
</span><span>    ) -&gt; ValidationResult {
</span><span>        </span><span>match </span><span>(</span><span>self</span><span>, instance) {
</span><span>            </span><span>// ... Compare `instance` type with expected type
</span><span>            _ =&gt; Err(ValidationError::new(
</span><span>                format!("</span><span>{instance}</span><span> is not of type '</span><span>{self}</span><span>'</span><span>"),
</span><span>                path.</span><span>to_vec</span><span>().</span><span>into_iter</span><span>(),
</span><span>            )),
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre>
<p>No heap allocations during traversal! Does it help?</p>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/91ec92c757d3948a2a032a55d035e6fadc63fdcf" target="_blank">91ec92c</a></td>
  <td>35.0 µs<br>(<strong>-14.8%</strong>)</td>
  <td>663.5 µs<br>(<strong>-46.4%</strong>)</td>
  <td>1.32 ms<br>(<strong>-46.6%</strong>)</td>
  <td>958.9 µs<br>(<strong>+1.8%</strong>)</td>
  <td>2.54 ms<br>(<strong>+5.1%</strong>)</td>
  <td>4.58 ms<br>(<strong>+9.9%</strong>)</td>
</tr>
  </tbody>
</table>
<p>Woah! That is significantly better for valid inputs! Invalid ones are roughly the same, however, we have not optimized the linked list implementation yet.</p>
<p>In our case, we are leveraging the fact, that the path is only needed when an error occurs. This allows us to avoid the overhead of maintaining the path during the entire traversal process and heap allocate only when necessary.</p>
<blockquote>
<p>Note that linked lists have worse cache locality compared to vectors, which can lead to slower performance in some scenarios.</p>
</blockquote>
<h2 id="idea-4-precise-memory-allocation">Idea 4: Precise memory allocation</h2>
<p>Let's find out why the linked list implementation is not as performant for invalid inputs by looking at <code>JsonPointerNode</code> first:</p>

  <p><img src="https://dygalo.dev/blog/linked-list-invalid-10.png"></p><p>Apparently, the problem is in memory reallocations inside <code>JsonPointerNode::to_vec</code>. We can avoid them by allocating the exact amount of memory needed. This will require an extra linked list traversal to calculate the capacity, but the performance gain from avoiding reallocations outweighs the cost of the extra traversal:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl</span><span>&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>pub</span><span>(</span><span>crate</span><span>) </span><span>fn </span><span>to_vec</span><span>(&amp;</span><span>'a </span><span>self</span><span>) -&gt; Vec&lt;&amp;</span><span>'a str</span><span>&gt; {
</span><span>        </span><span>// Walk the linked list to calculate the capacity
</span><span>        </span><span>let mut</span><span> capacity = </span><span>0</span><span>;
</span><span>        </span><span>let mut</span><span> head = </span><span>self</span><span>;
</span><span>        </span><span>while let </span><span>Some(next) = head.parent {
</span><span>            head = next;
</span><span>            capacity += </span><span>1</span><span>;
</span><span>        }
</span><span>        </span><span>// Callect the segments from the head to the tail
</span><span>        </span><span>let mut</span><span> buffer = Vec::with_capacity(capacity);
</span><span>        </span><span>let mut</span><span> head = </span><span>self</span><span>;
</span><span>        </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>            buffer.</span><span>push</span><span>(*segment);
</span><span>        }
</span><span>        </span><span>while let </span><span>Some(next) = head.parent {
</span><span>            head = next;
</span><span>            </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>                buffer.</span><span>push</span><span>(*segment);
</span><span>            }
</span><span>        }
</span><span>        </span><span>// Reverse the buffer to get the segments in the correct order
</span><span>        buffer.</span><span>reverse</span><span>();
</span><span>        buffer
</span><span>    }
</span><span>}
</span></code></pre>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/10ae4f100935c757bb7707defcf122c179aee2dc" target="_blank">10ae4f1</a></td>
  <td>39.1 µs<br>(<strong>+11.2%</strong>)</td>
  <td>667.9 µs<br>(<strong>+0.5%</strong>)</td>
  <td>1.30 ms<br>(<strong>-1.7%</strong>)</td>
  <td>899.7 µs<br>(<strong>-7.5%</strong>)</td>
  <td>1.96 ms<br>(<strong>-23.3%</strong>)</td>
  <td>3.49 ms<br>(<strong>-24.3%</strong>)</td>
</tr>
  </tbody>
</table>
<p>Great! Precise memory allocation improves performance for invalid inputs, bringing it closer to the performance of valid ones.
Allocate exactly as needed, whenever possible, to avoid the overhead of memory reallocations.</p>
<h2 id="idea-5-avoid-temporary-vec">Idea 5: Avoid temporary <code>Vec</code></h2>
<p>At this point, the most significant slowdown is for invalid cases. If we take a closer look at the <code>ValidationError</code> implementation we’ll see the <code>collect</code> call, which means that first, we build <code>Vec&lt;&amp;str&gt;</code> in <code>JsonPointerNode::to_vec</code> and then almost immediately build <code>Vec&lt;String&gt;</code> from it, which means allocating <code>Vec</code> twice. Why don’t we just build <code>Vec&lt;String&gt;</code> in the first place:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>ValidationError {
</span><span>    </span><span>fn </span><span>new</span><span>(</span><span>message</span><span>: impl Into&lt;String&gt;, </span><span>location</span><span>: Vec&lt;String&gt;) -&gt; </span><span>Self </span><span>{
</span><span>        </span><span>Self </span><span>{
</span><span>            message: message.</span><span>into</span><span>(),
</span><span>            location,
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>&lt;</span><span>'a</span><span>&gt;(
</span><span>        &amp;</span><span>self</span><span>,
</span><span>        </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value,
</span><span>        </span><span>path</span><span>: JsonPointerNode&lt;</span><span>'a</span><span>, '_&gt;,
</span><span>    ) -&gt; ValidationResult {
</span><span>        </span><span>match </span><span>(</span><span>self</span><span>, instance) {
</span><span>            </span><span>// ... Compare `instance` type with expected type
</span><span>            _ =&gt; Err(ValidationError::new(
</span><span>                format!("</span><span>{instance}</span><span> is not of type '</span><span>{self}</span><span>'</span><span>"),
</span><span>                path.</span><span>to_vec</span><span>(),
</span><span>            )),
</span><span>        }
</span><span>    }
</span><span>}
</span><span>
</span><span>impl</span><span>&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>pub</span><span>(</span><span>crate</span><span>) </span><span>fn </span><span>to_vec</span><span>(&amp;</span><span>self</span><span>) -&gt; Vec&lt;String&gt; {
</span><span>        </span><span>// ...
</span><span>        </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>            buffer.</span><span>push</span><span>((*segment).</span><span>to_string</span><span>());
</span><span>        }
</span><span>        </span><span>while let </span><span>Some(next) = head.parent {
</span><span>            head = next;
</span><span>            </span><span>if let </span><span>Some(segment) = &amp;head.segment {
</span><span>                buffer.</span><span>push</span><span>((*segment).</span><span>to_string</span><span>());
</span><span>            }
</span><span>        }
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>This optimization leads to a visible performance improvement for invalid cases:</p>
<table>
  <thead>
    <tr>
      <th rowspan="2">Commit</th>
      <th colspan="3">Valid</th>
      <th colspan="3">Invalid</th>
    </tr>
    <tr>
      <th>0</th>
      <th>5</th>
      <th>10</th>
      <th>0</th>
      <th>5</th>
      <th>10</th>
    </tr>
  </thead>
  <tbody>
<tr>
  <td><a href="https://github.com/Stranger6667/article-linked-lists/commit/d3d2182e00aba996134475b90e87d565dfe47ac3" target="_blank">d3d2182</a></td>
  <td>39.7 µs<br>(<strong>-0.2%</strong>)</td>
  <td>652.3 µs<br>(<strong>-2.7%</strong>)</td>
  <td>1.35 ms<br>(<strong>+2.2%</strong>)</td>
  <td>765.1 µs<br>(<strong>-14.2%</strong>)</td>
  <td>1.83 ms<br>(<strong>-6.9%</strong>)</td>
  <td>3.33 ms<br>(<strong>-5.9%</strong>)</td>
</tr>
  </tbody>
</table>
<h2 id="idea-6-struct-size-optimization">Idea 6: Struct size optimization</h2>
<p>Sometimes it is worth trying to reduce struct sizes, especially when the struct is passed by value frequently. 
Smaller structs lead to less memory usage and faster function calls, as less data needs to be copied on the stack.</p>
<p>If we were to track not only keys in JSON objects but also indexes in arrays, we would need to use an enum like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>enum </span><span>Segment&lt;'a&gt; {
</span><span>    </span><span>/// Property name within a JSON object.
</span><span>    Property(&amp;</span><span>'a str</span><span>),
</span><span>    </span><span>/// Index within a JSON array.
</span><span>    Index(</span><span>usize</span><span>),
</span><span>}
</span><span>
</span><span>struct </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>segment</span><span>: Option&lt;Segment&lt;</span><span>'a</span><span>&gt;&gt;,
</span><span>    </span><span>parent</span><span>: Option&lt;&amp;</span><span>'b </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt;&gt;,
</span><span>}
</span></code></pre>
<p>Then, the <code>JsonPointerNode</code> struct would occupy 32 bytes:</p>
<pre data-lang="rust"><code data-lang="rust"><span>assert_eq!(std::mem::size_of::&lt;JsonPointerNode&gt;(), </span><span>32</span><span>);
</span></code></pre>
<p>However, by avoiding <code>Option</code> in the <code>segment</code> field, we can reduce its size to 24 bytes. The idea is to put some cheap value in the root node and never read it:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>segment</span><span>: Segment&lt;</span><span>'a</span><span>&gt;,
</span><span>    </span><span>parent</span><span>: Option&lt;&amp;</span><span>'b </span><span>JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt;&gt;,
</span><span>}
</span><span>impl</span><span>&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>    </span><span>/// Create a root node of a JSON pointer.
</span><span>    </span><span>const fn </span><span>new</span><span>() -&gt; </span><span>Self </span><span>{
</span><span>        JsonPointerNode {
</span><span>            </span><span>// The value does not matter, it will never be used
</span><span>            segment: Segment::Index(</span><span>0</span><span>),
</span><span>            parent: None,
</span><span>        }
</span><span>    }
</span><span>   </span><span>fn </span><span>push</span><span>(&amp;</span><span>'a </span><span>self</span><span>, </span><span>segment</span><span>: Segment&lt;</span><span>'a</span><span>&gt;) -&gt; JsonPointerNode&lt;</span><span>'a</span><span>, </span><span>'b</span><span>&gt; {
</span><span>        JsonPointerNode {
</span><span>            segment,
</span><span>            parent: Some(</span><span>self</span><span>),
</span><span>        }
</span><span>    }
</span><span>    </span><span>/// Convert the JSON pointer node to a vector of path segments.
</span><span>    </span><span>pub</span><span>(</span><span>crate</span><span>) </span><span>fn </span><span>to_vec</span><span>(&amp;</span><span>self</span><span>) -&gt; Vec&lt;String&gt; {
</span><span>        </span><span>// ...
</span><span>        </span><span>if</span><span> head.parent.</span><span>is_some</span><span>() {
</span><span>            buffer.</span><span>push</span><span>(head.segment.</span><span>to_string</span><span>())
</span><span>        }
</span><span>        </span><span>while let </span><span>Some(next) = head.parent {
</span><span>            head = next;
</span><span>            </span><span>if</span><span> head.parent.</span><span>is_some</span><span>() {
</span><span>                buffer.</span><span>push</span><span>(head.segment.</span><span>to_string</span><span>());
</span><span>            }
</span><span>        }
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>This technique is directly used in the <a href="https://github.com/Stranger6667/jsonschema-rs" target="_blank">jsonschema</a> crate to reduce the size of the <code>JsonPointerNode</code> struct, but not used here for simplicity.</p>
<h2 id="more-ideas-that-didn-t-make-it">More ideas that didn't make it</h2>
<p>I think that we can also gain a bit more performance by avoiding the <code>reverse</code> call in <code>JsonPointerNode::to_vec</code>, as it moves the data around. One way to achieve this is by assigning segments starting from the back of a vector filled with default values. However, the extra bookeeping needed for writing in the reverse order could outweigh the gains, so it's important to profile and benchmark any changes to ensure they provide measurable benefits for your use case.</p>
<p>Another idea is to store references to path segments inside <code>ValidationError</code> instead of cloning the strings:</p>
<pre data-lang="rust"><code data-lang="rust"><span>struct </span><span>ValidationError&lt;</span><span>'a</span><span>&gt; {
</span><span>    </span><span>message</span><span>: String,
</span><span>    </span><span>location</span><span>: Vec&lt;&amp;</span><span>'a str</span><span>&gt;,
</span><span>}
</span></code></pre>
<p>This way, we can avoid cloning the path segments and instead store references to them. This could lead to performance improvements, especially when dealing with long paths or large numbers of errors. However, this approach would make <code>ValidationError</code> less flexible, as it would be tied to the lifetime of the input JSON data.</p>
<h2 id="bonus-idea-maybe-you-don-t-need-a-linked-list">Bonus idea: Maybe you don't need a linked list?</h2>
<p>This idea was suggested by <a href="https://github.com/kobzol" target="_blank">@Kobzol</a>, who noted that the error path could be collected lazily from the call stack in the error propagation path. I've implemented the idea based on the suggestion and the original code snippet:</p>
<pre data-lang="rust"><code data-lang="rust"><span>impl </span><span>ValidationError {
</span><span>    </span><span>pub</span><span>(</span><span>crate</span><span>) </span><span>fn </span><span>push_segment</span><span>(&amp;</span><span>mut </span><span>self</span><span>, </span><span>segment</span><span>: String) {
</span><span>        </span><span>self</span><span>.location.</span><span>push</span><span>(segment);
</span><span>    }
</span><span>    </span><span>pub</span><span>(</span><span>crate</span><span>) </span><span>fn </span><span>finish</span><span>(</span><span>mut </span><span>self</span><span>) -&gt; ValidationError {
</span><span>        </span><span>self</span><span>.location.</span><span>reverse</span><span>();
</span><span>        </span><span>self
</span><span>    }
</span><span>}
</span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value) -&gt; ValidationResult {
</span><span>    </span><span>if let </span><span>Err(error) = </span><span>self</span><span>.node.</span><span>validate</span><span>(instance, </span><span>0</span><span>) {
</span><span>        </span><span>// Reverse the path segments in the `finish` method
</span><span>        Err(error.</span><span>finish</span><span>())
</span><span>    } </span><span>else </span><span>{
</span><span>        Ok(())
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Properties {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;Value, </span><span>level</span><span>: </span><span>u32</span><span>) -&gt; ValidationResult {
</span><span>        </span><span>// ... 
</span><span>            </span><span>for </span><span>(key, value) in &amp;</span><span>self</span><span>.properties {
</span><span>                </span><span>if let </span><span>Some(instance) = object.</span><span>get</span><span>(key) {
</span><span>                    </span><span>if let </span><span>Err(</span><span>mut</span><span> error) = value.</span><span>validate</span><span>(instance, level + </span><span>1</span><span>) {
</span><span>                        error.</span><span>push_segment</span><span>(key.</span><span>to_string</span><span>());
</span><span>                        </span><span>return </span><span>Err(error);
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span><span>
</span><span>impl </span><span>Node </span><span>for </span><span>Type {
</span><span>    </span><span>fn </span><span>validate</span><span>(&amp;</span><span>self</span><span>, </span><span>instance</span><span>: &amp;</span><span>'a</span><span> Value, </span><span>level</span><span>: </span><span>u32</span><span>) -&gt; ValidationResult {
</span><span>        </span><span>// ...
</span><span>            _ =&gt; Err(ValidationError::new(
</span><span>                format!("</span><span>{instance}</span><span> is not of type '</span><span>{self}</span><span>'</span><span>"),
</span><span>                Vec::with_capacity(level as </span><span>usize</span><span>)
</span><span>        </span><span>// ...
</span><span>    }
</span><span>}
</span></code></pre>
<p>However, I was not able to get a stable improvement in benchmarks yet :(</p>
<blockquote>
<p>Small improvement may come from the fact that we no longer use the <code>?</code> operator as it involves the <code>From/Into</code> conversion, but we only have a single error type and don't need to convert it. This is the reason why <code>serde</code> has its own <code>tri!</code> <a href="https://github.com/serde-rs/serde/blob/3202a6858a2802b5aba2fa5cf3ec8f203408db74/serde/src/lib.rs#L287" target="_blank">macro</a> that is used instead of <code>?</code>;</p>
</blockquote>
<h2 id="conclusion">Conclusion</h2>
<p>In the end, we achieved ~4.5x / ~2.2x improvements from the naive implementation for valid / invalid scenarios. Overall this feature adds 18% / 95% on top of the path-less version!</p>

  <p><img src="https://dygalo.dev/blog/performance-comparison.png"></p><p>Some optimizations in this article may not seem immediately beneficial, especially if you already know where the bottlenecks are. However, exploring simpler optimizations can sometimes reveal unexpected opportunities for improvement. Even if an optimization doesn't directly pay off or even makes your code slower in some cases, it may open up new possibilities for further optimizations.</p>
<p><strong>Takeaways</strong>:</p>
<ol>
<li>A naive approach could be good enough</li>
<li>Try alternative data structures such as immutable <code>Vector</code></li>
<li>Consider simpler stdlib variations like <code>Arc</code></li>
<li>Look at the problem at different scales</li>
<li>Search for a data structure that fits your problem</li>
<li>Allocate exactly as needed and when necessary</li>
<li>Reduce the size of values you pass around a lot</li>
</ol>
<p>If you have any more ideas to improve this use case or have any suggestions, please let me know!</p>
<p>In the next article, I will dive into HTML trees and why you should try to build your own data structure for it.</p>
<p>Thank you for your attention!</p>
<p>Dmitry</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My VM is lighter (and safer) than your container (2017) (159 pts)]]></title>
            <link>https://dl.acm.org/doi/10.1145/3132747.3132763</link>
            <guid>40353963</guid>
            <pubDate>Tue, 14 May 2024 11:24:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dl.acm.org/doi/10.1145/3132747.3132763">https://dl.acm.org/doi/10.1145/3132747.3132763</a>, See on <a href="https://news.ycombinator.com/item?id=40353963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><nav></nav><div><!-- abstract content --><div><p><h2 id="d9093262e1">ABSTRACT</h2></p><div>
		<p>Containers are in great demand because they are lightweight when compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. In this paper, we examine whether there is indeed a strict tradeoff between isolation (VMs) and efficiency (containers). We find that VMs can be as nimble as containers, as long as they are small and the toolstack is fast enough.</p> <p>We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables creating tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight virtual machines are not enough to ensure good performance since the virtualization control plane (the toolstack) becomes the performance bottleneck. We present LightVM, a new virtualization solution based on Xen that is optimized to offer fast boot-times regardless of the number of active VMs. LightVM features a complete redesign of Xen's control plane, transforming its centralized operation to a distributed one where interactions with the hypervisor are reduced to a minimum. LightVM can boot a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of magnitude faster than Docker. LightVM can pack thousands of LightVM guests on modest hardware with memory and CPU usage comparable to that of processes.</p>
	</div></div><!-- /abstract content --><p><a href="#SEC_refrences" id="SEC_suppl">Skip Supplemental Material Section</a></p><div data-rel="videos"><p><h2 id="sec-supp">Supplemental Material</h2></p></div><div data-sectionname="References"><div><h2 id="sec-ref">
                    References
                </h2></div><ol><li id="ref-00001"><span>Amazon Web Services {n. d.}. Amazon EC2 Container Service. https://aws.amazon.com/ecs/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Amazon+Web+Services+%7Bn.+d.%7D.+Amazon+EC2+Container+Service.+https%3A%2F%2Faws.amazon.com%2Fecs%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00002"><span>Amazon Web Services {n. d.}. AWS Lambda - Serverless Compute. https://aws.amazon.com/lambda. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Amazon+Web+Services+%7Bn.+d.%7D.+AWS+Lambda+-+Serverless+Compute.+https%3A%2F%2Faws.amazon.com%2Flambda.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00003"><span>Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and Andrew Warfield. 2003. Xen and the Art of Virtualization. SIGOPS Open Syst. Rev. 37, 5 (Oct. 2003), 164--177.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Paul+Barham%2C+Boris+Dragovic%2C+Keir+Fraser%2C+Steven+Hand%2C+Tim+Harris%2C+Alex+Ho%2C+Rolf+Neugebauer%2C+Ian+Pratt%2C+and+Andrew+Warfield.+2003.+Xen+and+the+Art+of+Virtualization.+SIGOPS+Open+Syst.+Rev.+37%2C+5+%28Oct.+2003%29%2C+164%2D%2D177.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/1165389.945462" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00004"><span>J. Clark. {n. d.}. Google: "EVERYTHING at Google runs in a container". http//:www.theregister.co.uk/2014/05/23/google_containerizationtwobillion/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=J.+Clark.+%7Bn.+d.%7D.+Google%3A+%22EVERYTHING+at+Google+runs+in+a+container%22.+http%2F%2F%3Awww.theregister.co.uk%2F2014%2F05%2F23%2Fgoogle_containerizationtwobillion%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00005"><span>Patrick Colp, Mihir Nanavati, Jun Zhu, William Aiello, George Coker, Tim Deegan, Peter Loscocco, and Andrew Warfield. 2011. Breaking Up is Hard to Do: Security and Functionality in a Commodity Hypervisor. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles (SOSP '11). ACM, New York, NY, USA, 189--202.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Patrick+Colp%2C+Mihir+Nanavati%2C+Jun+Zhu%2C+William+Aiello%2C+George+Coker%2C+Tim+Deegan%2C+Peter+Loscocco%2C+and+Andrew+Warfield.+2011.+Breaking+Up+is+Hard+to+Do%3A+Security+and+Functionality+in+a+Commodity+Hypervisor.+In+Proceedings+of+the+Twenty-Third+ACM+Symposium+on+Operating+Systems+Principles+%28SOSP+%2711%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+189%2D%2D202.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/2043556.2043575" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00006"><span>Docker {n. d.}. The Docker Containerization Platform. https://www.docker.com/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Docker+%7Bn.+d.%7D.+The+Docker+Containerization+Platform.+https%3A%2F%2Fwww.docker.com%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00007"><span>John R. Douceur, Jeremy Elson, Jon Howell, and Jacob R. Lorch. 2008. Leveraging Legacy Code to Deploy Desktop Applications on the Web. In Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation (OSDI'08). USENIX Association, Berkeley, CA, USA, 339--354. http://dl.acm.org/citation.cfm?id=1855741.1855765 <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=John+R.+Douceur%2C+Jeremy+Elson%2C+Jon+Howell%2C+and+Jacob+R.+Lorch.+2008.+Leveraging+Legacy+Code+to+Deploy+Desktop+Applications+on+the+Web.+In+Proceedings+of+the+8th+USENIX+Conference+on+Operating+Systems+Design+and+Implementation+%28OSDI%2708%29.+USENIX+Association%2C+Berkeley%2C+CA%2C+USA%2C+339%2D%2D354.+http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1855741.1855765+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/1855741.1855765" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00008"><span>D. R. Engler, M. F. Kaashoek, and J. O'Toole, Jr. 1995. Exokernel: An Operating System Architecture for Application-level Resource Management. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (SOSP '95). ACM, New York, NY, USA, 251--266.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=D.+R.+Engler%2C+M.+F.+Kaashoek%2C+and+J.+O%27Toole%2C+Jr.+1995.+Exokernel%3A+An+Operating+System+Architecture+for+Application-level+Resource+Management.+In+Proceedings+of+the+Fifteenth+ACM+Symposium+on+Operating+Systems+Principles+%28SOSP+%2795%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+251%2D%2D266.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/224056.224076" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00009"><span>Erlang on Xen 2012. Erlang on Xen. http://erlangonxen.org/. (July 2012).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Erlang+on+Xen+2012.+Erlang+on+Xen.+http%3A%2F%2Ferlangonxen.org%2F.+%28July+2012%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00010"><span>Google Cloud Platform {n. d.}. The Google Cloud Platform Container Engine. https://cloud.google.com/container-engine. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Google+Cloud+Platform+%7Bn.+d.%7D.+The+Google+Cloud+Platform+Container+Engine.+https%3A%2F%2Fcloud.google.com%2Fcontainer-engine.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00011"><span>A. Grattafiori. {n. d.}. Understanding and Hardening Linux Containers. https://www.nccgroup.trust/us/our-research/understanding-and-hardening-linux-containers/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=A.+Grattafiori.+%7Bn.+d.%7D.+Understanding+and+Hardening+Linux+Containers.+https%3A%2F%2Fwww.nccgroup.trust%2Fus%2Four-research%2Funderstanding-and-hardening-linux-containers%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00012"><span>Cameron Hamilton-Rich. {n. d.}. axTLS Embedded SSL. http://axtls.sourceforge.net. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Cameron+Hamilton-Rich.+%7Bn.+d.%7D.+axTLS+Embedded+SSL.+http%3A%2F%2Faxtls.sourceforge.net.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00013"><span>Poul henning Kamp and Robert N. M. Watson. 2000. Jails: Confining the omnipotent root. In In Proc. 2nd Intl. SANE Conference.<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Poul+henning+Kamp+and+Robert+N.+M.+Watson.+2000.+Jails%3A+Confining+the+omnipotent+root.+In+In+Proc.+2nd+Intl.+SANE+Conference." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00014"><span>J. Hertz. {n. d.}. Abusing Privileged and Unprivileged Linux Containers. https://www.nccgroup.tmst/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/, ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=J.+Hertz.+%7Bn.+d.%7D.+Abusing+Privileged+and+Unprivileged+Linux+Containers.+https%3A%2F%2Fwww.nccgroup.tmst%2Fuk%2Four-research%2Fabusing-privileged-and-unprivileged-linux-containers%2F%2C+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00015"><span>Jon Howell, Bryan Parno, and John R. Douceur. 2013. Embassies: Radically Refactoring the Web. In Presented as part of the 10th USENTX Symposium on Networked Systems Design and Implementation (NSDI13). USENIX, Lombard, IL, 529--545. https://www.usenix.org/conference/nsdil3/technical-sessions/presentation/howell <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Jon+Howell%2C+Bryan+Parno%2C+and+John+R.+Douceur.+2013.+Embassies%3A+Radically+Refactoring+the+Web.+In+Presented+as+part+of+the+10th+USENTX+Symposium+on+Networked+Systems+Design+and+Implementation+%28NSDI13%29.+USENIX%2C+Lombard%2C+IL%2C+529%2D%2D545.+https%3A%2F%2Fwww.usenix.org%2Fconference%2Fnsdil3%2Ftechnical-sessions%2Fpresentation%2Fhowell+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/2482626.2482676" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00016"><span>Yun Chao Hu, Milan Patel, Dario Sabella, Nurit Sprecher, and Valerie Young. 2015. Mobile Edge Computing - A key technology towards 5G. ETSI White Paper No. 11, First edition (2015).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Yun+Chao+Hu%2C+Milan+Patel%2C+Dario+Sabella%2C+Nurit+Sprecher%2C+and+Valerie+Young.+2015.+Mobile+Edge+Computing+-+A+key+technology+towards+5G.+ETSI+White+Paper+No.+11%2C+First+edition+%282015%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00017"><span>IBM. {n. d.}. Docker at insane scale on IBM Power Systems. https://www.ibm.com/blogs/bluemix/2015/ll/docker-insane-scale-on-ibm-power-systems. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=IBM.+%7Bn.+d.%7D.+Docker+at+insane+scale+on+IBM+Power+Systems.+https%3A%2F%2Fwww.ibm.com%2Fblogs%2Fbluemix%2F2015%2Fll%2Fdocker-insane-scale-on-ibm-power-systems.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00018"><span>IBM developerWorks Open {n. d.}. Solo5 Unikernel. https://developer.ibm.com/open/openprojects/solo5-unikernel/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=IBM+developerWorks+Open+%7Bn.+d.%7D.+Solo5+Unikernel.+https%3A%2F%2Fdeveloper.ibm.com%2Fopen%2Fopenprojects%2Fsolo5-unikernel%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00019"><span>Intel. {n. d.}. Intel Clear Containers: A Breakthrough Combination of Speed and Workload Isolation. https://clearlinux.org/sites/default/files/vmscontainers_wp_v5.pdf. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Intel.+%7Bn.+d.%7D.+Intel+Clear+Containers%3A+A+Breakthrough+Combination+of+Speed+and+Workload+Isolation.+https%3A%2F%2Fclearlinux.org%2Fsites%2Fdefault%2Ffiles%2Fvmscontainers_wp_v5.pdf.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00020"><span>Avi Kivity, Yaniv Kamay, Dor Laor, Uri Lublin, and Anthony Liguori. 2007. KVM: the Linux Virtual Machine Monitor. In In Proc. 2007 Ottawa Linux Symposium (OLS '07).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Avi+Kivity%2C+Yaniv+Kamay%2C+Dor+Laor%2C+Uri+Lublin%2C+and+Anthony+Liguori.+2007.+KVM%3A+the+Linux+Virtual+Machine+Monitor.+In+In+Proc.+2007+Ottawa+Linux+Symposium+%28OLS+%2707%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00021"><span>Avi Kivity, Dor Laor, Glauber Costa, Pekka Enberg, Nadav Har'El, Don Marti, and Vlad Zolotarov. 2014. OSv---Optimizing the Operating System for Virtual Machines. In Proceedings of the 2014 USENTX Annual Technical Conference (USENIX ATC '14). USENIX Association, Philadelphia, PA, 61--72. https://www.usenix.org/conference/atcl4/technical-sessions/presentation/kivity <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Avi+Kivity%2C+Dor+Laor%2C+Glauber+Costa%2C+Pekka+Enberg%2C+Nadav+Har%27El%2C+Don+Marti%2C+and+Vlad+Zolotarov.+2014.+OSv%2D%2D-Optimizing+the+Operating+System+for+Virtual+Machines.+In+Proceedings+of+the+2014+USENTX+Annual+Technical+Conference+%28USENIX+ATC+%2714%29.+USENIX+Association%2C+Philadelphia%2C+PA%2C+61%2D%2D72.+https%3A%2F%2Fwww.usenix.org%2Fconference%2Fatcl4%2Ftechnical-sessions%2Fpresentation%2Fkivity+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/2643634.2643642" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00022"><span>E. Kovacs. {n. d.}. Docker Fixes Vulnerabilities, Shares Plans For Making Platform Safer. http//:www.securityweek.com/docker-fixes-vulnerabilities-shares-plans-making-platform-safer. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=E.+Kovacs.+%7Bn.+d.%7D.+Docker+Fixes+Vulnerabilities%2C+Shares+Plans+For+Making+Platform+Safer.+http%2F%2F%3Awww.securityweek.com%2Fdocker-fixes-vulnerabilities-shares-plans-making-platform-safer.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00023"><span>Simon Kuenzer, Anton Ivanov, Filipe Manco, Jose Mendes, Yuri Volchkov, Florian Schmidt, Kenichi Yasukata, Michio Honda, and Felipe Huici. 2017. Unikernels Everywhere: The Case for Elastic CDNs. In Proceedings of the 13th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '17). ACM, New York, NY, USA, 15--29.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Simon+Kuenzer%2C+Anton+Ivanov%2C+Filipe+Manco%2C+Jose+Mendes%2C+Yuri+Volchkov%2C+Florian+Schmidt%2C+Kenichi+Yasukata%2C+Michio+Honda%2C+and+Felipe+Huici.+2017.+Unikernels+Everywhere%3A+The+Case+for+Elastic+CDNs.+In+Proceedings+of+the+13th+ACM+SIGPLAN%2FSIGOPS+International+Conference+on+Virtual+Execution+Environments+%28VEE+%2717%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+15%2D%2D29.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/3050748.3050757" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00024"><span>Horacio Andrés Lagar-Cavilla, Joseph Andrew Whitney, Adin Matthew Scannell, Philip Patchin, Stephen M. Rumble, Eyal de Lara, Michael Brudno, and Mahadev Satyanarayanan. 2009. SnowFlock: Rapid Virtual Machine Cloning for Cloud Computing. In Proceedings of the 4th ACM European Conference on Computer Systems (EuroSys '09). ACM, New York, NY, USA, 1--12.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Horacio+Andr%C3%A9s+Lagar-Cavilla%2C+Joseph+Andrew+Whitney%2C+Adin+Matthew+Scannell%2C+Philip+Patchin%2C+Stephen+M.+Rumble%2C+Eyal+de+Lara%2C+Michael+Brudno%2C+and+Mahadev+Satyanarayanan.+2009.+SnowFlock%3A+Rapid+Virtual+Machine+Cloning+for+Cloud+Computing.+In+Proceedings+of+the+4th+ACM+European+Conference+on+Computer+Systems+%28EuroSys+%2709%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+1%2D%2D12.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/1519065.1519067" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00025"><span>LinuxContainers.org {n. d.}. LinuxContainers.org. https://linuxcontainers.org. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=LinuxContainers.org+%7Bn.+d.%7D.+LinuxContainers.org.+https%3A%2F%2Flinuxcontainers.org.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00026"><span>Anil Madhavapeddy, Thomas Leonard, Magnus Skjegstad, Thomas Gazagnaire, David Sheets, Dave Scott, Richard Mortier, Amir Chaudhry, Balraj Singh, Jon Ludlam, Jon Crowcroft, and Ian Leslie. 2015. Jitsu: Just-In-Time Summoning of Unikernels. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI '15). USENIX Association, Oakland, CA, 559--573. https://www.usenix.org/conference/nsdil5/technical-sessions/presentation/madhavapeddy <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Anil+Madhavapeddy%2C+Thomas+Leonard%2C+Magnus+Skjegstad%2C+Thomas+Gazagnaire%2C+David+Sheets%2C+Dave+Scott%2C+Richard+Mortier%2C+Amir+Chaudhry%2C+Balraj+Singh%2C+Jon+Ludlam%2C+Jon+Crowcroft%2C+and+Ian+Leslie.+2015.+Jitsu%3A+Just-In-Time+Summoning+of+Unikernels.+In+12th+USENIX+Symposium+on+Networked+Systems+Design+and+Implementation+%28NSDI+%2715%29.+USENIX+Association%2C+Oakland%2C+CA%2C+559%2D%2D573.+https%3A%2F%2Fwww.usenix.org%2Fconference%2Fnsdil5%2Ftechnical-sessions%2Fpresentation%2Fmadhavapeddy+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/2789770.2789809" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00027"><span>Anil Madhavapeddy and David J. Scott. 2013. Unikernels: Rise of the Virtual Library Operating System. Queue 11, 11, Article 30 (Dec. 2013), 15 pages.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Anil+Madhavapeddy+and+David+J.+Scott.+2013.+Unikernels%3A+Rise+of+the+Virtual+Library+Operating+System.+Queue+11%2C+11%2C+Article+30+%28Dec.+2013%29%2C+15+pages.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/2557963.2566628" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00028"><span>Y. Mao, J. Zhang, and K. B. Letaief. 2016. Dynamic Computation Offloading for Mobile-Edge Computing With Energy Harvesting Devices. IEEE Journal on Selected Areas in Communications 34, 12 (Dec 2016), 3590--3605.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Y.+Mao%2C+J.+Zhang%2C+and+K.+B.+Letaief.+2016.+Dynamic+Computation+Offloading+for+Mobile-Edge+Computing+With+Energy+Harvesting+Devices.+IEEE+Journal+on+Selected+Areas+in+Communications+34%2C+12+%28Dec+2016%29%2C+3590%2D%2D3605.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1109/JSAC.2016.2611964" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00029"><span>Joao Martins, Mohamed Ahmed, Costin Raiciu, Vladimir Olteanu, Michio Honda, Roberto Bifulco, and Felipe Huici. 2014. ClickOS and the Art of Network Function Virtualization. In 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI '14). USENIX Association, Seattle, WA, 459--473. https://www.usenix.org/conference/nsdil4/technical-sessions/presentation/martins <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Joao+Martins%2C+Mohamed+Ahmed%2C+Costin+Raiciu%2C+Vladimir+Olteanu%2C+Michio+Honda%2C+Roberto+Bifulco%2C+and+Felipe+Huici.+2014.+ClickOS+and+the+Art+of+Network+Function+Virtualization.+In+11th+USENIX+Symposium+on+Networked+Systems+Design+and+Implementation+%28NSDI+%2714%29.+USENIX+Association%2C+Seattle%2C+WA%2C+459%2D%2D473.+https%3A%2F%2Fwww.usenix.org%2Fconference%2Fnsdil4%2Ftechnical-sessions%2Fpresentation%2Fmartins+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/2616448.2616491" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00030"><span>McAffee. 2016. Mobile Threat Report. https://www.mcafee.com/us/resources/reports/rp-mobile-threat-report-2016.pdf. (2016).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=McAffee.+2016.+Mobile+Threat+Report.+https%3A%2F%2Fwww.mcafee.com%2Fus%2Fresources%2Freports%2Frp-mobile-threat-report-2016.pdf.+%282016%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00031"><span>MicroPython {n. d.}. MicroPython. https://micropython.org/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=MicroPython+%7Bn.+d.%7D.+MicroPython.+https%3A%2F%2Fmicropython.org%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00032"><span>Microsoft. {n. d.}. Azure Container Service. https://azure.microsoft.com/en-us/services/container-service/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Microsoft.+%7Bn.+d.%7D.+Azure+Container+Service.+https%3A%2F%2Fazure.microsoft.com%2Fen-us%2Fservices%2Fcontainer-service%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00033"><span>Microsoft Research. {n. d.}. Drawbridge. https://www.microsoft.com/en-us/research/project/drawbridge/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Microsoft+Research.+%7Bn.+d.%7D.+Drawbridge.+https%3A%2F%2Fwww.microsoft.com%2Fen-us%2Fresearch%2Fproject%2Fdrawbridge%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00034"><span>minios {n. d.}. Mini-OS. https://wiki.xenproject.org/wiki/Mini-OS. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=minios+%7Bn.+d.%7D.+Mini-OS.+https%3A%2F%2Fwiki.xenproject.org%2Fwiki%2FMini-OS.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00035"><span>A. Mourat. {n. d.}. 5 security concerns when using Docker. https://www.oreilly.com/ideas/five-security-concerns-when-using-docker. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=A.+Mourat.+%7Bn.+d.%7D.+5+security+concerns+when+using+Docker.+https%3A%2F%2Fwww.oreilly.com%2Fideas%2Ffive-security-concerns-when-using-docker.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00036"><span>Vlad Nitu, Pierre Olivier, Alain Tchana, Daniel Chiba, Antonio Barbalace, Daniel Hagimont, and Binoy Ravindran. 2017. Swift Birth and Quick Death: Enabling Fast Parallel Guest Boot and Destruction in the Xen Hypervisor. In Proceedings of the 13th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE '17). ACM, New York, NY, USA, 1--14.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Vlad+Nitu%2C+Pierre+Olivier%2C+Alain+Tchana%2C+Daniel+Chiba%2C+Antonio+Barbalace%2C+Daniel+Hagimont%2C+and+Binoy+Ravindran.+2017.+Swift+Birth+and+Quick+Death%3A+Enabling+Fast+Parallel+Guest+Boot+and+Destruction+in+the+Xen+Hypervisor.+In+Proceedings+of+the+13th+ACM+SIGPLAN%2FSIGOPS+International+Conference+on+Virtual+Execution+Environments+%28VEE+%2717%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+1%2D%2D14.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/3050748.3050758" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00037"><span>MAN page. {n. d.}. Linux system calls list. http://man7.org/linux/man-pages/man2/syscalls.2.html. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=MAN+page.+%7Bn.+d.%7D.+Linux+system+calls+list.+http%3A%2F%2Fman7.org%2Flinux%2Fman-pages%2Fman2%2Fsyscalls.2.html.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00038"><span>Rumpkernel.org {n. d.}. Rump Kernels. http://rumpkernel.org/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Rumpkernel.org+%7Bn.+d.%7D.+Rump+Kernels.+http%3A%2F%2Frumpkernel.org%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00039"><span>Sandvine. {n. d.}. Internet traffic encryption. https://www.sandvine.com/trends/encryption.html. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Sandvine.+%7Bn.+d.%7D.+Internet+traffic+encryption.+https%3A%2F%2Fwww.sandvine.com%2Ftrends%2Fencryption.html.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00040"><span>Mahadev Satyanarayanan, Paramvir Bahl, Ramón Caceres, and Nigel Davies. 2009. The Case for VM-Based Cloudlets in Mobile Computing. IEEE Pervasive Computing 8, 4 (Oct. 2009), 14--23.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Mahadev+Satyanarayanan%2C+Paramvir+Bahl%2C+Ram%C3%B3n+Caceres%2C+and+Nigel+Davies.+2009.+The+Case+for+VM-Based+Cloudlets+in+Mobile+Computing.+IEEE+Pervasive+Computing+8%2C+4+%28Oct.+2009%29%2C+14%2D%2D23.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1109/MPRV.2009.82" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00041"><span>Justine Sherry, Shaddi Hasan, Colin Scott, Arvind Krishnamurthy, Sylvia Ratnasamy, and Vyas Sekar. 2012. Making Middleboxes Someone Else's Problem: Network Processing As a Cloud Service. In Proceedings of the ACM SIGCOMM 2012 Conference on Computer Communication (SIGCOMM '12). ACM, New York, NY, USA, 13--24.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Justine+Sherry%2C+Shaddi+Hasan%2C+Colin+Scott%2C+Arvind+Krishnamurthy%2C+Sylvia+Ratnasamy%2C+and+Vyas+Sekar.+2012.+Making+Middleboxes+Someone+Else%27s+Problem%3A+Network+Processing+As+a+Cloud+Service.+In+Proceedings+of+the+ACM+SIGCOMM+2012+Conference+on+Computer+Communication+%28SIGCOMM+%2712%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+13%2D%2D24.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/2342356.2342359" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00042"><span>Stephen Soltesz, Herbert Pötzl, Marc E. Fiuczynski, Andy Bavier, and Larry Peterson. 2007. Container-based Operating System Virtualization: A Scalable, High-performance Alternative to Hypervisors. SIGOPS Oper. Syst. Rev. 41, 3 (March 2007), 275--287.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Stephen+Soltesz%2C+Herbert+P%C3%B6tzl%2C+Marc+E.+Fiuczynski%2C+Andy+Bavier%2C+and+Larry+Peterson.+2007.+Container-based+Operating+System+Virtualization%3A+A+Scalable%2C+High-performance+Alternative+to+Hypervisors.+SIGOPS+Oper.+Syst.+Rev.+41%2C+3+%28March+2007%29%2C+275%2D%2D287.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/1272998.1273025" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00043"><span>S. Stabellini. {n. d.}. Xen on ARM. http//:www.slideshare.net/xen_com_mgr/alsf13-stabellini. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=S.+Stabellini.+%7Bn.+d.%7D.+Xen+on+ARM.+http%2F%2F%3Awww.slideshare.net%2Fxen_com_mgr%2Falsf13-stabellini.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00044"><span>Udo Steinberg and Bernhard Kauer. 2010. NOVA: A Microhypervisorbased Secure Virtualization Architecture. In Proceedings of the 5th European Conference on Computer Systems (EuroSys '10). ACM, New York, NY, USA, 209--222.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Udo+Steinberg+and+Bernhard+Kauer.+2010.+NOVA%3A+A+Microhypervisorbased+Secure+Virtualization+Architecture.+In+Proceedings+of+the+5th+European+Conference+on+Computer+Systems+%28EuroSys+%2710%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+209%2D%2D222.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/1755913.1755935" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00045"><span>A. van de Ven. {n. d.}. An introduction to Clear Containers. https://lwn.net/Articles/644675/. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=A.+van+de+Ven.+%7Bn.+d.%7D.+An+introduction+to+Clear+Containers.+https%3A%2F%2Flwn.net%2FArticles%2F644675%2F.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00046"><span>Akshat Verma, Gargi Dasgupta, Tapan Kumar Nayak, Pradipta De, and Ravi Kothari. 2009. Server Workload Analysis for Power Minimization Using Consolidation. In Proceedings of the 2009 USENIX Annual Technical Conference (USENIX ATC '09). USENIX Association, Berkeley, CA, USA, 28--28. http://dl.acm.org/citation.cfm?id=1855807.1855835 <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Akshat+Verma%2C+Gargi+Dasgupta%2C+Tapan+Kumar+Nayak%2C+Pradipta+De%2C+and+Ravi+Kothari.+2009.+Server+Workload+Analysis+for+Power+Minimization+Using+Consolidation.+In+Proceedings+of+the+2009+USENIX+Annual+Technical+Conference+%28USENIX+ATC+%2709%29.+USENIX+Association%2C+Berkeley%2C+CA%2C+USA%2C+28%2D%2D28.+http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1855807.1855835+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/1855807.1855835" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00047"><span>VMWare. {n. d.}. vSphere ESXi Bare-Metal Hypervisor. http//:www.vmware.com/products/esxi-and-esx.html. ({n. d.}).<span><a href="http://scholar.google.com/scholar?hl=en&amp;q=VMWare.+%7Bn.+d.%7D.+vSphere+ESXi+Bare-Metal+Hypervisor.+http%2F%2F%3Awww.vmware.com%2Fproducts%2Fesxi-and-esx.html.+%28%7Bn.+d.%7D%29." target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span></span></li><li id="ref-00048"><span>Michael Vrable, Justin Ma, Jay Chen, David Moore, Erik Vandekieft, Alex C. Snoeren, Geoffrey M. Voelker, and Stefan Savage. 2005. Scalability, Fidelity, and Containment in the Potemkin Virtual Honey-farm. SIGOPS Oper. Syst. Rev. 39, 5 (Oct. 2005), 148--162.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Michael+Vrable%2C+Justin+Ma%2C+Jay+Chen%2C+David+Moore%2C+Erik+Vandekieft%2C+Alex+C.+Snoeren%2C+Geoffrey+M.+Voelker%2C+and+Stefan+Savage.+2005.+Scalability%2C+Fidelity%2C+and+Containment+in+the+Potemkin+Virtual+Honey-farm.+SIGOPS+Oper.+Syst.+Rev.+39%2C+5+%28Oct.+2005%29%2C+148%2D%2D162.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/1095809.1095825" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00049"><span>Andrew Whitaker, Marianne Shaw, and Steven D. Gribble. 2002. Scale and Performance in the Denali Isolation Kernel. SIGOPS Oper. Syst. Rev. 36, SI (Dec. 2002), 195--209.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Andrew+Whitaker%2C+Marianne+Shaw%2C+and+Steven+D.+Gribble.+2002.+Scale+and+Performance+in+the+Denali+Isolation+Kernel.+SIGOPS+Oper.+Syst.+Rev.+36%2C+SI+%28Dec.+2002%29%2C+195%2D%2D209.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/844128.844147" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00050"><span>Dan Williams and Ricardo Koller. 2016. Unikernel Monitors: Extending Minimalism Outside of the Box. In 8th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud '16). USENIX Association, Denver, CO. https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/williams <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Dan+Williams+and+Ricardo+Koller.+2016.+Unikernel+Monitors%3A+Extending+Minimalism+Outside+of+the+Box.+In+8th+USENIX+Workshop+on+Hot+Topics+in+Cloud+Computing+%28HotCloud+%2716%29.+USENIX+Association%2C+Denver%2C+CO.+https%3A%2F%2Fwww.usenix.org%2Fconference%2Fhotcloud16%2Fworkshop-program%2Fpresentation%2Fwilliams+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.5555/3027041.3027053" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li><li id="ref-00051"><span>Wei Zhang, Jinho Hwang, Shriram Rajagopalan, K.K. Ramakrishnan, and Timothy Wood. 2016. Flurries: Countless Fine-Grained NFs for Flexible Per-Flow Customization. In Proceedings of the 12th International on Conference on Emerging Networking EXperiments and Technologies (CoNEXT '16). ACM, New York, NY, USA, 3--17.  <span></span><span><a href="http://scholar.google.com/scholar?hl=en&amp;q=Wei+Zhang%2C+Jinho+Hwang%2C+Shriram+Rajagopalan%2C+K.K.+Ramakrishnan%2C+and+Timothy+Wood.+2016.+Flurries%3A+Countless+Fine-Grained+NFs+for+Flexible+Per-Flow+Customization.+In+Proceedings+of+the+12th+International+on+Conference+on+Emerging+Networking+EXperiments+and+Technologies+%28CoNEXT+%2716%29.+ACM%2C+New+York%2C+NY%2C+USA%2C+3%2D%2D17.+" target="_blank"><span>Google Scholar</span><img src="https://dl.acm.org/specs/products/acm/images/googleScholar.svg" alt="Google Scholar"></a></span><span><a href="https://dl.acm.org/doi/10.1145/2999572.2999602" target="_blank"><span>Digital Library</span><img data-title="Digital Library" alt="Digital Library" src="https://dl.acm.org/templates/jsp/_ux3/_acm/images/DL_icon.svg"></a></span></span></li></ol></div>









    
    
        <div data-widget-def="UX3TagWidget" data-widget-id="2a75f978-170a-4843-8722-e878a1b77fbc">
        



        
        









    
    
        <p data-widget-def="graphQueryWidget" data-widget-id="4ca2ed65-717e-4e67-bbb5-6578b8c6acac">
        



        
        <h2 id="sec-terms">Index Terms</h2>

        </p>
    


<ol><li><p>My VM is Lighter (and Safer) than your Container</p><ol><li><ol><li><ol><li><ol><li><ol></ol></li><li><ol><li><ol></ol></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol>

        </div>
    











    
    
        
    





        
        <p> <h2 id="sec-recommendations">Recommendations</h2> </p>




        
        










    
    
        
    





        
        
</div><div><div id="pill-access"><div><h3>Login options</h3><div><p>Check if you have access through your login credentials or your institution to get full access on this article.</p><p><a href="https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3132747.3132763" title="Sign in">Sign in</a></p></div></div><div><h3>Full Access</h3></div></div><div id="pill-information"><ul role="tablist"><li role="presentation"><a id="pill-information__contentcon" href="#pill-information__content" aria-controls="pill-information__content" role="tab" data-toggle="tab" title="Information" aria-selected="true" data-simple-tab-id="" tabindex="0">Information</a></li><li role="presentation"><a id="pill-authors__contentcon" href="#pill-authors__content" aria-controls="pill-authors__content" role="tab" data-toggle="tab" title="Authors" aria-selected="false" data-simple-tab-id="" tabindex="-1">Contributors</a></li></ul><ul><li id="pill-information__content" aria-labelledby="pill-information__contentcon" role="tabpanel" aria-hidden="false" tabindex="0"><div><div><h3>Published in</h3><div>









    
    
        <div data-widget-def="UX3CoverImage" data-widget-id="7b8bd09e-8112-4266-b136-2f71ab2b109c"><p><img src="https://dl.acm.org/cms/asset/40e846f1-617d-4920-9978-476aa73fcd0f/3132747.cover.jpg" data-src="/cms/asset/40e846f1-617d-4920-9978-476aa73fcd0f/3132747.cover.jpg" alt="cover image ACM Conferences"></p><div><p>SOSP '17: Proceedings of the 26th Symposium on Operating Systems Principles</p><p>October 2017</p><p>677  pages</p></div></div>
    

<p>Copyright © 2017 Owner/Author</p><p>This work is licensed under a Creative Commons Attribution International 4.0 License.</p></div></div><div><h3>Publisher</h3><div><p>Association for Computing Machinery</p><p>New York, NY, United States</p></div></div>









    
    
        <div data-widget-def="contentItemHistory" data-widget-id="6eba30da-4023-4f09-bb08-f98cf09d5aa0">
        



        
            <h3>
                Publication History
            </h3>
        
        <div><ul><li><span>Published:</span> 14 October 2017</li></ul></div>

        </div>
    





        
        <!-- rightslink drop zone-->



        
        

<div><h3>Check for updates</h3><p><a data-target="crossmark"><img alt="Check for updates on crossmark" src="https://dl.acm.org/specs/products/acm/releasedAssets/images/CrossMarkIcon-d6fb3282a56d93dd889e4259d1cf4bd1.svg"></a></p></div>

<div><h3>Author Tags</h3></div><div><h3>Qualifiers</h3><ul><li>research-article</li><li>Research</li><li>Refereed limited</li></ul></div><p><h3>Conference</h3></p>









    
    
        <div data-widget-def="UX3AcceptanceRatesWidget" data-widget-id="daac1568-1d03-438e-a4c9-c9df280908b7"><h2 id="acceptance-rates" aria-expanded="true">Acceptance Rates</h2><div><p><span>Overall Acceptance Rate<span>131</span>of<span>716</span>submissions,<span>18%</span></span></p></div></div>
    











    
    
        <div data-widget-def="UX3UpcomingConferencesWidget" data-widget-id="452ce022-9919-435e-ab40-1386965a30bd">
<h3>Upcoming Conference</h3>
    
</div>
    

</div></li></ul></div><div id="pill-metric"><ul role="tablist"><li role="presentation"><a id="pill-bibliometrics__contentcon" href="#pill-bibliometrics__content" aria-controls="pill-bibliometrics__content" role="tab" data-toggle="tab" title="Bibliometrics" aria-selected="true" data-simple-tab-id="" tabindex="0">Bibliometrics</a></li><li role="presentation"><a id="pill-citations__contentcon" href="#pill-citations__content" data-ajaxurl="/action/ajaxShowCitedBy?widgetId=f69d88a8-b404-4aae-83a9-9acea4426d78&amp;ajax=true&amp;doi=10.1145%2F3132747.3132763&amp;pbContext=%3Btaxonomy%3Ataxonomy%3Aconference-collections%3Bissue%3Aissue%3Adoi%5C%3A10.1145%2F3132747%3Bwgroup%3Astring%3AACM+Publication+Websites%3BgroupTopic%3Atopic%3Aacm-pubtype%26gt%3Bproceeding%3Bctype%3Astring%3ABook+Content%3Barticle%3Aarticle%3Adoi%5C%3A10.1145%2F3132747.3132763%3Btopic%3Atopic%3Aconference-collections%26gt%3Bsosp%3Bpage%3Astring%3AArticle%2FChapter+View%3BsubPage%3Astring%3AAbstract%3Bcsubtype%3Astring%3AConference+Proceedings%3Bwebsite%3Awebsite%3Adl-site%3Bjournal%3Ajournal%3Aacmconferences%3BpageGroup%3Astring%3APublication+Pages&amp;widgetKey=ux3-publicationContent-widget_f69d88a8-b404-4aae-83a9-9acea4426d78_1498_en" data-component="pubAjaxContent" data-ajaxtarget="#pill-citations__content .citedBy" aria-controls="pill-citations__content" role="tab" data-toggle="tab" title="Citations" aria-selected="false" data-simple-tab-id="" tabindex="-1">Citations<span>182</span></a></li></ul><ul><li id="pill-bibliometrics__content" aria-labelledby="pill-bibliometrics__contentcon" role="tabpanel" aria-hidden="false" tabindex="0"><div><div><h3>Article Metrics</h3><div><ul><li><a href="#pill-citations__contentcon" data-tab="pill-citations__content" data-slide-target="#pill-metric" data-full-screen="false" data-ctrl-res="screen-xlg">View Citations</a></li><li></li></ul><ul><li><span>Downloads (Last 12 months)</span><span>1,905</span></li><li><span>Downloads (Last 6 weeks)</span><span>280</span></li></ul></div></div><div><h3>Other Metrics</h3></div></div></li></ul></div><div id="pill-formats"><div><h3>PDF Format</h3><div><p>View or Download as a PDF file.</p><p><a href="https://dl.acm.org/doi/pdf/10.1145/3132747.3132763" title="View or Download as a PDF file">PDF</a></p></div></div><div><h3>eReader</h3><div><p>View online with eReader.</p><p><a href="https://dl.acm.org/doi/epdf/10.1145/3132747.3132763" title="View online with eReader">eReader</a></p></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Notifier Pattern for Applications That Use Postgres (163 pts)]]></title>
            <link>https://brandur.org/notifier</link>
            <guid>40352686</guid>
            <pubDate>Tue, 14 May 2024 07:56:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brandur.org/notifier">https://brandur.org/notifier</a>, See on <a href="https://news.ycombinator.com/item?id=40352686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                <p><a href="https://www.postgresql.org/docs/current/sql-listen.html">Listen/notify in Postgres</a> is an incredible feature that makes itself useful in all kinds of situations. I’ve been using it a long time, started taking it for granted long ago, and was somewhat shocked recently looking into MySQL and SQLite to learn that even in 2024, no equivalent exists.</p>

<p>In a basic sense, listen/notify is such a simple concept that it needs little explanation. Clients subscribe on topics and other clients can send on topics, passing a message to each subscribed client. The idea takes only three seconds to demonstrate using nothing more than a psql shell:</p>

<pre><code>=# LISTEN test_topic;
LISTEN
Time: 2.828 ms

=# SELECT pg_notify('test_topic', 'test_message');
 pg_notify
-----------

(1 row)

Time: 17.892 ms
Asynchronous notification "test_topic" with payload "test_message" received from server process with PID 98481.
</code></pre>

<p>But despite listen/notify’s relative simplicity, when it comes to applications built on top of Postgres, it’s common to use it less than optimally, eating through scarce Postgres connections and with little regard to failure cases.</p>

<hr>

<p>Here’s where the <strong>notifier pattern for Postgres</strong> comes in. It’s an extremely simple idea, but in my experience, one that’s rarely seen in practice. Let’s start with these axioms:</p>

<ul>
<li><p><code>LISTEN</code>s are affixed to specific connections. After listening, the original connection must still be available somewhere to successfully receive messages.</p></li>

<li><p>There may be many components within an application that’d like to listen on topics for completely orthogonal uses.</p></li>

<li><p>Despite optimizations over the years, connections in Postgres are still somewhat of a precious, limited resource, and should be conserved. We’d like to minimize the number of them required for listen/notify use.</p></li>

<li><p>A single connection can listen on any number of topics.</p></li>
</ul>

<p>With those stated, we can explain the role of the notifier. Its job is to <strong>hold a single Postgres connection per process, allow other components in the same program to use it to subscribe to any number of topics, wait for notifications, and distribute them to listening components as they’re received</strong>.</p>

<p>The “single Postgres connection per process” piece is key. Use of a notifier keeps the number of Postgres connections dedicated to use with listen/notify down to <strong>one per program</strong>, a major advantage compared to the naive version, which is <em>one connection per topic per program</em>. Especially for languages like Go that make a in-process concurrency easy and cheap, the notifier reduces listen/notify connection overhead to practically nil.</p>

<p><img src="https://brandur.org/assets/images/notifier/notifier.svg" alt="Notifier distributing notifications to program components"></p>

<h2 id="implementation"><a href="#implementation">A few implementation details</a></h2>

<p>From a conceptual standpoint, the notifier’s not difficult to understand, and with only this high level description, most readers would be able to implement it themselves. I’m not going to go through an implementation in full detail, but let’s look at a few important aspects of one. (For a complete reference, you can take a look <a href="https://github.com/riverqueue/river/tree/master/internal/notifier">at River’s notifier</a>, which is quite well vetted.)</p>

<p>Here’s a listen function to establish a new subscription:</p>

<pre><code>// Listen returns a subscription that lets a caller receive values from a
// notification channel.
func (l *Notifier) Listen(channel string) *Subscription {
    l.mu.Lock()
    defer l.mu.Unlock()

    existingSubs := l.subscriptions[channel]

    sub := &amp;Subscription{
        channel:        channel,
        listenChan:     make(chan string, 100),
        notifyListener: l,
    }
    l.subscriptions[channel] = append(existingSubs, sub)

    if len(existingSubs) &gt; 0 {
        // If there's already another subscription for this channel, reuse its
        // established channel. It may already be closed (to indicate that the
        // connection is established), but that's okay.
        sub.establishedChan = existingSubs[0].establishedChan
        sub.establishedChanClose = func() {} // no op since not channel owner

        return sub
    }

    // The notifier will close this channel after it's successfully established
    // `LISTEN` for the given channel. Gives subscribers a way to confirm a
    // listen before moving on, which is especially useful in tests.
    sub.establishedChan = make(chan struct{})
    sub.establishedChanClose = sync.OnceFunc(func() { close(sub.establishedChan) })

    l.channelChanges = append(l.channelChanges,
        channelChange{channel, sub.establishedChanClose, channelChangeOperationListen})

    // Cancel out of blocking on WaitForNotification so changes can be processed
    // immediately.
    l.waitForNotificationCancel() 

    return sub
}
</code></pre>

<p>A few key details to notice:</p>

<ul>
<li><p>Subscriptions use a <strong>buffered channel</strong> like <code>make(chan string, 100)</code> and <strong>non-blocking sends</strong> (using <code>select</code> with <code>default</code>). A notifier may receive a high volume of notifications, and if it were to block on every component successfully receiving and processing each one, it could easily fall behind. Instead, a received notification is sent into the channel using a non-blocking send. The non-blocking send means that the send operation will never block: instead the notification is discarded if the channel is full. The buffer provides a tunable amount of slack to make sure this won’t happen too easily. It’s each component’s job to make sure its processing its inbox in a timely manner. This is important because even in the event of one component falling behind, the system as a whole stays healthy.</p></li>

<li><p>Multiple components may want to subscribe to the same topic. Since only one connection is in use, the notifier only needs to issue one <code>LISTEN</code> per topic. Internally, it organizes subscriptions by topic, and if it notices that a topic already exists, a new subscription is added without issuing <code>LISTEN</code>.</p></li>

<li><p>Subscriptions provide an <strong>established channel</strong> that’s closed when a <code>LISTEN</code> has been successfully issued and the notifier is up and listening. This isn’t strictly necessary for most production uses, but it’s invaluable for use in testing. If a test case issues <code>pg_notify</code> before the notifier has started listening, that notification is lost – a problem that can lead to tortuous test intermittency <sup id="footnote-1-source"><a href="#footnote-1">1</a></sup>. Instead, a test case tells the notifier to listen, <em>waits for the listen to succeed</em>, then moves on to send <code>pg_notify</code>.</p></li>
</ul>

<pre><code>// EstablishedC is a channel that's closed after the notifier's successfully
// established a connection. This is especially useful in test cases, where it
// can be used to wait for confirmation that not only that the listener is
// started, but that it's successfully established started listening on a
// channel before continuing. For a new subscription on an already established
// channel, EstablishedC is already closed, so it's always safe to wait on it.
//
// There's no full guarantee that the notifier can ever successfully establish a
// listen, so callers will usually want to `select` on it combined with a
// context done, a stop channel, and/or a timeout.
//
// The channel is always closed as a notifier is stopping.
func (s *Subscription) EstablishedC() &lt;-chan struct{} { return s.establishedChan }
</code></pre>

<h3 id="interruptible-receives"><a href="#interruptible-receives">Interruptible receives</a></h3>

<p>There’s no standard SQL for waiting for a notification. Typically, it’s accomplished using a special driver-level function like <a href="https://pkg.go.dev/github.com/jackc/pgx/v5#Conn.WaitForNotification">Pgx’s <code>WaitForNotification</code></a>.</p>

<p>These commonly block until receiving a notification, which can be problem since we’re only using a single connection. What if the notifier is in a blocking receive loop, but another component wants to add a new subscription that requires <code>LISTEN</code> be issued?</p>

<p>You’ll want to handle this case by making sure that the wait loop is interruptible. Here’s one way to accomplish that in Go:</p>

<pre><code>func (l *Notifier) runOnce(ctx context.Context) error {
    if err := l.processChannelChanges(ctx); err != nil {
        return err
    }

    // WaitForNotification is a blocking function, but since we want to wake
    // occasionally to process new `LISTEN`/`UNLISTEN` operations, we put a
    // context deadline on the listen, and as it expires don't treat it as an
    // error unless it
    notification, err := func() (*pgconn.Notification, error) {
        const listenTimeout = 30 * time.Second

        ctx, cancel := context.WithTimeout(ctx, listenTimeout)
        defer cancel()

        // Provides a way for the blocking wait to be cancelled in case a new
        // subscription change comes in.
        l.mu.Lock()
        l.waitForNotificationCancel = cancel
        l.mu.Unlock()

        notification, err := l.conn.WaitForNotification(ctx)
        if err != nil {
            return nil, xerrors.Errorf("error waiting for notification: %w", err)
        }

        return notification, nil
    }()
    if err != nil {
        // If the error was a cancellation or the deadline being exceeded but
        // there's no error in the parent context, return no error.
        if (errors.Is(err, context.Canceled) ||
            errors.Is(err, context.DeadlineExceeded)) &amp;&amp; ctx.Err() == nil {
            return nil
        }

        return err
    }

    l.mu.RLock()
    defer l.mu.RUnlock()

    subs := l.subscriptions[notification.Channel]

    if len(subs) &lt; 1 {
        return nil
    }

    for _, sub := range subs {
        sub.listenChan &lt;- notification.Payload
    }

    return nil
}
</code></pre>

<p>The inner closure calls into <code>WaitForNotification</code>, but has a default context timeout of 30 seconds that automatically cycles the function periodically. It also stores the special context cancellation function <code>l.waitForNotificationCancel</code>.</p>

<p>When <code>Listen</code> is invoked and a new subscription needs to be added, <code>l.waitForNotificationCancel</code> is called. The wait is cancelled immediately, new subscriptions are processed, and the closure is reentered to wait anew.</p>

<h3 id="let-it-crash"><a href="#let-it-crash">Let it crash</a></h3>

<p>Given there’s now a single master connection that’s handling all notifications for a program, it’s fairly critical that its health be monitored, and the notifier reacts appropriately. If not, all uses of listen/notify would degrade simultaneously.</p>

<p>The obvious way to react would be to close the connection, use a connection pool to procure a new connection, reissue <code>LISTEN</code>s for each active subscription, then reenter the wait loop.</p>

<p>It can be a little tricky sometimes to guarantee that state is reset cleanly, so another possibility is to adhere to the “let it crash” school of thought. If the connection becomes irreconcilably unhealthy, stop the program, and have it come back to a healthy state by virtue of its normal start up.</p>

<pre><code>// If the notifier gets unhealthy, restart the worker. This will generally
// never happen as the notifier has a built-in retry loop that try its best
// to keep established before giving up.
notifier.AddUnhealthyCallback(closeShutdown)
</code></pre>

<p>We’ve found this sort of edge to be so rare (I’ve only seen it happen once in a year+ of use) that letting the program crash when it does happen hasn’t produced any undue disruption.</p>

<h2 id="pgbouncer"><a href="#pgbouncer">PgBouncer</a></h2>

<p>Using <a href="https://www.pgbouncer.org/features.html">PgBouncer</a>, <code>LISTEN</code> is only supported using session pooling (as opposed to transaction pooling) because notifications are only sent to the original session that issued a <code>LISTEN</code> for them.</p>

<p>Use of a notifier requires an app to dedicate a single connection per program for listen/notify, but every other part of the application is free to use PgBouncer in transaction pooling or statement pooling mode, thereby maximizing the efficiency of connection use.</p>



            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using ARG in a Dockerfile – beware the gotcha (121 pts)]]></title>
            <link>https://qmacro.org/blog/posts/2024/05/13/using-arg-in-a-dockerfile-beware-the-gotcha/</link>
            <guid>40352426</guid>
            <pubDate>Tue, 14 May 2024 07:05:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qmacro.org/blog/posts/2024/05/13/using-arg-in-a-dockerfile-beware-the-gotcha/">https://qmacro.org/blog/posts/2024/05/13/using-arg-in-a-dockerfile-beware-the-gotcha/</a>, See on <a href="https://news.ycombinator.com/item?id=40352426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today I learned about the subtleties of <a href="https://docs.docker.com/build/guide/build-args/">build arguments</a> in Dockerfile definitions, specifically how the <code>ARG</code> instruction relates to - and is affected by - the <code>FROM</code> instruction. It's not entirely like a constant or a variable, in the way that I had thought.</p><h2>The problem with empty ARG values</h2><p>I spent more than a coffee's worth of time trying to understand why my custom builds of a CAP Node.js container image weren't of the CAP version I was specifying, either implicitly with the default value I'd declared in the <code>ARG</code> instruction in the Dockerfile, or even explicitly with the <code>--build-arg</code> option on the command line.</p><p>To illustrate the problem, here's a vastly simplified version of the Dockerfile I was building with:</p><pre><code><span># syntax=docker/dockerfile:1</span><p><span><span>ARG</span> DEBVER=<span>"10"</span></span><br><span><span>ARG</span> CAPVER=<span>"7.8"</span></span><br><span><span>FROM</span> debian:<span>${DEBVER}</span></span></p><p><span><span>RUN</span> printf <span>"DEB=${DEBVER}\nCAP=${CAPVER}\n"</span> &gt; /tmp/log</span></p></code></pre><p>The first variable declared with <code>ARG</code> here is <code>DEBVER</code> and represents a fairly common use case of allowing for different versions of a base image, illustrated here in being able to start from different versions of the Debian distribution, where the default version is to be 10.</p><p>The second variable <code>CAPVER</code> was something similar that I was using later in the build instructions (i.e. further on in the Dockerfile), to specify the particular version of CAP that I wanted to install. The actual instruction in my Dockerfile looked like this: <code>RUN npm install -g @sap/cds-dk@{CAPVER}</code>.</p><p>After building the image based on this simplified Dockerfile, without specifying any values explicitly with <code>--build-arg</code>, like this:</p><pre><code><span>;</span> <span>docker</span> build <span>-t</span> argtest <span>.</span></code></pre><p>I could successfully confirm that the version of Debian in containers created from this image was 10:</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>grep</span> VERSION_ID /etc/os-release<br><span>VERSION_ID</span><span>=</span><span>"10"</span></code></pre><p>But what of the content of <code>/tmp/log</code>?</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>cat</span> /tmp/log<br><span>DEB</span><span>=</span><br><span>CAP</span><span>=</span></code></pre><p>Hmm.</p><p>How about when I use <code>--build-arg</code> options?</p><pre><code><span>;</span> <span>docker</span> build <span>\</span><br>  --build-arg<span>=</span><span>"DEBVER=11"</span> <span>\</span><br>  --build-arg<span>=</span><span>"CAPVER=7.9"</span> <span>\</span><br>  <span>-t</span> argtest <span>.</span></code></pre><p>The build completes successfully, and I can see that containers now are Debian 11 based:</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>grep</span> VERSION_ID /etc/os-release<br><span>VERSION_ID</span><span>=</span><span>"11"</span></code></pre><p>but the problem with the empty values for <code>DEBVER</code> and <code>CAPVER</code> in <code>/tmp/log</code> remains.</p><p>Not only is the value for <code>CAPVER</code> empty when we reference it in the <code>RUN</code> instruction, but also, and this is the most mysterious thing thus far, while <code>DEBVER</code> was certainly recognised and set to 11 for the Debian distribution in the <code>FROM</code> instruction, it's empty when we reference it later in the <code>RUN</code> instruction.</p><h2>The subtleties of how ARG relates to FROM</h2><p>The cause is the rather subtle relationship between <code>ARG</code> and <code>FROM</code>, the explanation for which is brief and a little hidden in the main <a href="https://docs.docker.com/reference/dockerfile/">Dockerfile reference</a>. I certainly missed it when I went straight to the <a href="https://docs.docker.com/reference/dockerfile/#arg">reference for <code>ARG</code></a>, as it's not mentioned, and only explained at the end of the <a href="https://docs.docker.com/reference/dockerfile/#from">reference for <code>FROM</code></a> which is earlier on the page.</p><p>The key section is here: <a href="https://docs.docker.com/reference/dockerfile/#understand-how-arg-and-from-interact">Understand how ARG and FROM react</a>, and includes this line:</p><blockquote><p>"<em>An <code>ARG</code> declared before a <code>FROM</code> is outside of a build stage, so it can't be used in any instruction after a <code>FROM</code>.</em>"</p></blockquote><p>In other words, variables declared with <code>ARG</code> look like variables in, say, a shell script, variables which are also often declared at the start, and then used throughout the script.</p><p>But they're not.</p><h2>The solution</h2><p>What must be done to the Dockerfile above is to modify it so it looks like this:</p><pre><code><span># syntax=docker/dockerfile:1</span><p><span><span>ARG</span> DEBVER=<span>"10"</span></span><br><span><span>FROM</span> debian:<span>${DEBVER}</span></span><br><span><span>ARG</span> DEBVER</span><br><span><span>ARG</span> CAPVER=<span>"7.8"</span></span></p><p><span><span>RUN</span> printf <span>"DEB=${DEBVER}\nCAP=${CAPVER}\n"</span> &gt; /tmp/log</span></p></code></pre><p>Moving the <code>ARG</code> instruction for <code>CAPVER</code> so that it comes after the <code>FROM</code> instruction gives it life and validity.</p><p>And the <code>ARG</code> instruction for <code>DEBVER</code> must stay where it is (as it's referenced in the <code>FROM</code> instruction details of course) but if it needs to be referred to after the <code>FROM</code> instruction it must be referenced again - hence the <code>ARG DEBVER</code> line.</p><p>From a container image built using this new version of the Dockerfile, with no <code>--build-arg</code> options specified, we can see that the values for <code>DEBVER</code> and <code>CAPVER</code> are available after the <code>FROM</code> instruction:</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>cat</span> /tmp/log<br><span>DEB</span><span>=</span><span>10</span><br><span>CAP</span><span>=</span><span>7.8</span></code></pre><p>And this works of course if we set values for build arguments on the command line too, testing a container built using the same <code>docker build --build-arg ...</code> invocation as before:</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>cat</span> /tmp/log<br><span>DEB</span><span>=</span><span>11</span><br><span>CAP</span><span>=</span><span>7.9</span></code></pre><p>and</p><pre><code><span>;</span> <span>docker</span> run <span>--rm</span> argtest <span>grep</span> VERSION_ID /etc/os-release<br><span>VERSION_ID</span><span>=</span><span>"11"</span></code></pre><h2>Wrapping up</h2><p>Perhaps I should have read the entire reference document for all the Dockerfile instructions first. Then I would have at least read about this relationship, and I may also have remembered it too. But for those of you like me who jump directly to consult the reference documentation on the thing they're trying to use, perhaps this will help.</p><p>Happy building!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude is now available in Europe (139 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-europe</link>
            <guid>40352204</guid>
            <pubDate>Tue, 14 May 2024 06:25:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-europe">https://www.anthropic.com/news/claude-europe</a>, See on <a href="https://news.ycombinator.com/item?id=40352204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article><div><figure><img loading="eager" width="1778" height="1000" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F791c1c121225ef3e3a368279926c12552ddb075c-1778x1000.jpg&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F791c1c121225ef3e3a368279926c12552ddb075c-1778x1000.jpg&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F791c1c121225ef3e3a368279926c12552ddb075c-1778x1000.jpg&amp;w=3840&amp;q=75"></figure><p>We’re excited to announce that <a href="https://www.anthropic.com/claude">Claude</a>, Anthropic’s trusted AI assistant, is now available for people and businesses across Europe to enhance their productivity and creativity. Starting today, they will be able to use:</p><ul><li><a href="http://claude.ai/">Claude.ai</a>: the web-based version of our next-generation AI assistant</li><li>The <a href="https://apps.apple.com/app/claude/id6473753684">Claude iOS app</a>: a free version of Claude that offers the same intuitive experience as mobile web</li><li>The <a href="https://www.anthropic.com/news/team-plan-and-ios">Claude Team plan</a>: the best way for every business to provide teams with secure access to Claude's state-of-the-art AI capabilities and the <a href="https://www.anthropic.com/news/claude-3-family">Claude 3 model family</a></li></ul><p>Today’s release follows the Europe launch of the Claude API earlier this year, which allows developers to integrate Anthropic’s state-of-the-art AI models into their own applications, websites, or services.</p><p>Claude has strong levels of comprehension and fluency in French, German, Spanish, Italian, and other European languages, allowing users to converse with Claude in multiple languages. Claude’s intuitive, user-friendly interface makes it easy for anyone to seamlessly integrate our advanced AI models into their workflows.</p><p>Both Claude.ai and the Claude iOS app are available for free. The Claude app is available for download in the Apple App Store. For €18 + VAT per month (or local currency equivalent), users can subscribe to Claude Pro and unlock all models, including Claude 3 Opus, one of the most advanced models on the market. The Team plan is €28 + VAT per user per month (or local currency equivalent), with a minimum of 5 seats.</p><p>At Anthropic, we're dedicated to creating AI systems that put people first. We look forward to bringing the unique capabilities of the Claude 3 model family to more people throughout Europe.</p><p><em>Read this post in <a href="https://cdn.sanity.io/files/4zrzovbb/website/bfbafc8f692634001f9f5fb7d05d12a4ef71ad24.pdf">French</a>, <a href="https://cdn.sanity.io/files/4zrzovbb/website/397335b3ce79aff658ba1a35f73c26cce0ea5490.pdf">German</a>, <a href="https://cdn.sanity.io/files/4zrzovbb/website/c2e15095e05250d9feecef8a1b9e9fd319acd7c7.pdf">Italian</a>, or <a href="https://cdn.sanity.io/files/4zrzovbb/website/5686521fb5eaa65ae0a3e79c7d7713760942e6d2.pdf">Spanish</a>.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A review on protein language models (131 pts)]]></title>
            <link>https://www.apoorva-srinivasan.com/plms/</link>
            <guid>40350954</guid>
            <pubDate>Tue, 14 May 2024 02:10:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apoorva-srinivasan.com/plms/">https://www.apoorva-srinivasan.com/plms/</a>, See on <a href="https://news.ycombinator.com/item?id=40350954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Protein “language” is a lot like human language. Given the similarities, researchers have been building and training language models on protein sequence data, replicating the success seen in other domains, with profound implications. In this post,<strong> </strong>I will explore how transformer models have been applied to protein data and what we’ve found. </p><h2 id="the-language-of-proteins">The language of proteins</h2><p>Much like words form sentences, protein sequences—strings of the 20 amino acids that make up the protein "vocabulary"—determine the structure and function of proteins in their environment. This ordering of amino acids is crucial, as it influences how proteins fold and interact within biological systems.</p><figure><img src="https://lh7-us.googleusercontent.com/lhRMC6jVwT_5LGnhFsspvfSWG6jxcUOIsEyKhA4OJT-tWwKpqkXseGtZCHJpaEdbEubkKDJnVzXwNWa27aZzXMbuc8_UY_x9QQszrVHc9y4REk-V2QuW4CvDFrP1edtd7Z_q1bc-Xu6k0mRRiYGmZo0" alt="" loading="lazy" width="450" height="197"></figure><p>Like human languages, which utilize modular elements like words, phrases, and sentences, proteins consist of motifs and domains. These are the fundamental building blocks reused in various combinations to construct complex structures. In this analogy, protein motifs and domains are akin to the 'words' and 'phrases' of the protein world, with their biological functions similar to the 'meaning' conveyed by sentences in human languages.</p><p>Another key parallel to human language is the concept of information completeness. Although a protein is much more than its amino acid sequence—it forms structures that carry out specific functions—all these aspects are predetermined by its sequence. Even though the way a protein behaves can vary depending on its environment and interactions with other molecules (e.g. on cellular state, other molecules and post-translational modifications), it is still defined by the underlying sequence. This means that from an information-theory perspective, the protein’s information (e.g. its structure) is contained within its sequence.</p><p>[<em>Its important to keep in mind that the analogies between NLP and protein language modeling only go so far. First, we can read and understand natural language, and not so much with proteins. Unlike proteins, most human languages include uniform punctuation and stop words, with clearly separable structures. And while natural languages have a well-defined vocabulary (with ~ million words in English), proteins lack a clear vocabulary. Moreover, with proteins we do not always know whether a sequence of amino acids is part of a functional unit (e.g. a domain). Additionally, proteins also exhibit high variability in length ranging from 20 to a few thousand AA.</em>]</p><h2 id="protein-language-model-architectures">Protein language model architectures</h2><h2 id="encoder-models">Encoder  models</h2><p>A lot of the initial work in pLMs is based on Encoder-only Transformer architecture as their aim is to obtain embedded representations of proteins in a vector space for downstream tasks.</p><p>Most of these models use BERT-like architectures and denoising autoencoding training objectives, i.e., they are pre-trained by corrupting input tokens in some way and trying to reconstruct the original sequence. Examples of such models include <a href="https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1?ref=apoorva-srinivasan.com">TCR-BERT</a>, <a href="https://www.biorxiv.org/content/10.1101/2022.02.27.481241v1?ref=apoorva-srinivasan.com">epiBERTope</a>, <a href="https://github.com/facebookresearch/esm?ref=apoorva-srinivasan.com">ESM</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/34232869/?ref=apoorva-srinivasan.com">ProtTrans</a> or <a href="https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?ref=apoorva-srinivasan.com">ProteinBERT</a>. After pre-training to generate embeddings, these models are further refined through supervised learning techniques to address various downstream tasks in protein engineering (secondary structure and contact prediction, remote homology detection, post-translation and biophysical properties prediction)</p><figure><a href="https://bair.berkeley.edu/blog/2019/11/04/proteins/?ref=apoorva-srinivasan.com"><img src="https://lh7-us.googleusercontent.com/GPkfVp301fWes7DAjQK9Sq6VC5FwjgtSZuH5CT6saa1B4DwjrescC7SFyzokD-QIEk0vZn4awBMno8gjXy1TSaTJ8vFLFFUECA_g1ZqkUdp-jKXwCZl391EaERTvjwn22ScdBCfC8EZrwHwxeBtZNlQ" alt="" loading="lazy" width="1494" height="458"></a><figcaption><i><em>source: </em></i><a href="https://bair.berkeley.edu/blog/2019/11/04/proteins/?ref=apoorva-srinivasan.com" rel="noreferrer"><i><em>Can We Learn the Language of Proteins?</em></i></a></figcaption></figure><h2 id="decoder-models">Decoder  models</h2><p>Unlike encoder models, decoder models adopt autoregressive training, a method where models are trained to predict subsequent words based on a given context. The most well-known of these types of models are, you guessed it, the GPT-x models.</p><p>An early example of the GPT style decoder models in the protein world is <a href="https://pubmed.ncbi.nlm.nih.gov/35896542/?ref=apoorva-srinivasan.com">ProtGPT2</a>, trained on 50M sequences with 738 parameters.</p><p>Did it work?</p><p>Kind of. ProtGPT2 managed to generate sequences with characteristics similar to those of natural proteins. The amino acid types and frequencies closely matched those in nature, and the sequences exhibited a comparable balance between ordered (stable) and disordered (flexible) regions. And visual inspection of the structural superimposition showed that generated proteins preserved some of the binding sites, which is essential for functionality. So, although the generated sequences <em>looked</em> a lot of natural proteins, it's hard to say if they really<em> functioned </em>like one. It was quickly outpaced by bigger and better models.</p><h2 id="conditional-transformers">Conditional transformers</h2><p>While ProGPT2 leverages a GPT2-like architecture for general protein sequence generation, newer approaches have been developed that integrate deeper biological contexts during the training phase. These methods ensure the patterns learned are not only statistically correct but also biologically meaningful. Protein models can be conditioned in two main ways: i) by conditioning on sequences, or ii) by conditioning on the structure of the proteins.</p><h3 id="conditioning-on-sequence">Conditioning on sequence:</h3><p>The development of <a href="https://arxiv.org/abs/1909.05858?ref=apoorva-srinivasan.com">Conditional TRansformer Language</a> (CTRL), an autoregressive model that includes conditional tags, marked a significant advance in NLP. These tags allow for targeted text generation without requiring input sequences. Known as control codes, these tags significantly refine the influence over genre, topic, or style, marking a major advance toward targeted text generation.</p><p>So naturally, CTRL was soon adapted to a dataset of 281 million protein sequences. The resulting model, named <a href="https://www.nature.com/articles/s41587-022-01618-2?ref=apoorva-srinivasan.com">ProGen</a>, employs UniProtKB Keywords as conditional tags. These tags span ten categories such as ‘biological process’, ‘cellular component’, and ‘molecular function’, encompassing over 1,100 terms. ProGen achieved perplexities on par with high-quality English language models, even when generating sequences for protein families not included in its training set.</p><p>ProGen's performance shows significant advances in our ability to design proteins that behave like natural ones. This model has successfully created protein sequences that function effectively, as proven by rigorous tests where they performed as well or better than naturally occurring proteins. Specifically, ProGen was able to:</p><ul><li>Create proteins that match the energy efficiency and structural accuracy of naturally occurring proteins.</li><li>Generate variations of a specific protein domain that proved to be more effective than random variations, suggesting a high degree of predictive accuracy and utility in practical applications.</li></ul><figure><a href="https://www.nature.com/articles/s41587-022-01618-2?ref=apoorva-srinivasan.com"><img src="https://lh7-us.googleusercontent.com/V3weJpI3zic_luO6RxddDdNyFNLDa9cnfWcqAT01g6W7eoV0NfLfno0yjuGKlh37Xljgsp2jZg4qZc501BvWuY9reXybUxJWKYT3z2mcX1FpXyJVdiYxCJ0-4dj49Xw5Co-Hufv3-LtFnqkhznrF4pc" alt="" loading="lazy" width="1600" height="363"></a><figcaption><i><em>source: </em></i><a href="https://www.nature.com/articles/s41587-022-01618-2?ref=apoorva-srinivasan.com" rel="noreferrer"><i><em>Large language models generate functional protein sequences across diverse families</em></i></a></figcaption></figure><p>And now it can do so much more, as shown <a href="https://www.biorxiv.org/content/10.1101/2024.04.22.590591v1?ref=apoorva-srinivasan.com">Profluent bio’s new paper</a> where they used ProGen to design Cas9 protein, which didn’t exist in nature but was able to edit genes successfully in humans. These achievements are important because they indicate we can now design synthetic proteins that not only mimic but improve upon natural proteins. I expect more cool stuff like this in the near future.</p><p><strong>Open questions:</strong></p><ul><li>How well does ProGen scale to even larger datasets? Are there limits to the number of protein families or conditions it can effectively model?</li></ul><h3 id="conditioning-on-structure">Conditioning on structure:</h3><p>Besides sequence, we can incorporate the structure of a protein while training such that the model can learn structure —&gt; sequence. This is called “inverse folding” because it does the exact opposite of protein folding which is sequence —&gt; structure.</p><p>That may sound counterintuitive, but this is actually an incredibly useful process for protein design, especially for enzymes and therapeutics. Let's say you have a specific task you want your enzyme or therapeutic protein to perform, such as binding to a certain molecule or catalyzing a particular reaction. Traditional methods often involve tweaking existing protein sequences and testing whether the new versions do the job better. This can be slow and somewhat hit-or-miss.</p><p>Inverse folding, on the other hand, starts with the ideal structure in mind — the one you predict will best perform the task. From there, it works backwards to figure out which sequences can fold into that structure. This approach has several advantages. For starters, inverse folding models tend to be very fast and can predict hundreds of sequences that might fold into a desired structure within minutes. It also explores a broader range of potential sequences than traditional methods, which might only alter existing sequences slightly. This broad exploration increases the chances of finding a sequence with optimal properties that hasn't been considered before.</p><p><a href="https://www.biorxiv.org/content/10.1101/2022.04.10.487779v2?ref=apoorva-srinivasan.com">ESM-IF</a> is one example of an inverse folding models trained on <a href="https://alphafold.ebi.ac.uk/?ref=apoorva-srinivasan.com">AlphaFold Database</a> (12M) and <a href="https://www.cathdb.info/?ref=apoorva-srinivasan.com">CATH Protein Structure Classification Database</a> (~16,000) that uses an encoder-decoder architecture that takes structures as inputs to the encoder and autoregressively decodes sequences conditioned on structural encodings.</p><p><strong>Open questions:</strong></p><ul><li>Given that inverse folding models are being trained on AlphaDB, which contain predicted rather than experimentally determined structures, how can we effectively assess the reliability and accuracy of the protein sequences generated by inverse folding models?</li></ul><h2 id="scale-is-all-you-need">Scale is all you need</h2><p>So we’ve seen the performance on complex tasks improve in general-purpose language models as compute, data and model size increases. At certain scales, language models exhibit useful capabilities which emerge as a result of scaling a simple training process to large corpuses of data, like few-shot language translation, commonsense reasoning, and mathematical reasoning.</p><p>A similar idea exists for inference from sequences in biology. Because the structure and function of a protein constrain the mutations to its sequence are selected through evolution, it should also be possible to infer biological structure and function from sequence patterns, which would provide insight into some of the most foundational problems in biology.</p><p>This is exactly what <a href="https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1.full.pdf?ref=apoorva-srinivasan.com" rel="noreferrer">ESM-2</a>, a 15 billion parameter model built by Meta does.</p><blockquote><em>By leveraging the internal representations of the language model(ESM-2), ESMFold generates structure predictions using only a single sequence as input, resulting in considerably faster structure prediction. ESMFold also produces more accurate atomic-level predictions than AlphaFold2 or RoseTTAFold when they are artificially given a single sequence as input, and obtains competitive performance to RoseTTAFold given full MSAs as input. Moreover, we find that ESMFold produces comparable predictions to state-of-the-art models for low perplexity sequences, and more generally that structure prediction accuracy correlates with language-model perplexity, indicating that when a language model better understands the sequence, it also better understands the structure.</em></blockquote><p>and</p><blockquote>A<em>tomic resolution structure prediction can be projected from the representations of the ESM-2 language models … (and accuracy improves with the) scale of the language model.</em></blockquote><p>This is pretty incredible! By scaling up the model size and dataset size, we can get rid of specific inductive biases (e.g., MSA) and generate structure predictions using only a single sequence as input.</p><p>Although ESM-2 is <a href="https://www.nature.com/articles/d41586-022-03539-1?ref=apoorva-srinivasan.com">less accurate</a> than AlphaFold today, it is an interestingly straightforward approach that can utilize an ever-expanding pool of diverse and unannotated protein sequence data:</p><table>
<thead>
<tr>
<th></th>
<th><strong>AlphaFold</strong></th>
<th><strong>ESMFold</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Parameters</strong></td>
<td>~100M</td>
<td>~15B</td>
</tr>
<tr>
<td><strong>Training data</strong></td>
<td>~200k structures, MSA</td>
<td>~65M sequences</td>
</tr>
<tr>
<td><strong>Loss</strong></td>
<td>Several loss functions used</td>
<td>Masked amino acid</td>
</tr>
</tbody>
</table>
<p>To further illustrate the mindblowing capabilities of the ESM protein models, <a href="https://www.nature.com/articles/s41587-023-01763-2?ref=apoorva-srinivasan.com#Sec9">researchers conducted a remarkable experiment</a> with highly optimized monoclonal antibodies, including antibodies targeting diseases like Ebola and COVID. They input the sequences of these antibodies into the ESM model, which then identified discrepancies between the actual amino acid sequences and their predictions. By selectively substituting the amino acids at these divergent sites with those predicted by the model, the researchers significantly enhanced the binding affinity, thermostability, and in vitro potency of the antibodies—achieving increases of up to sevenfold for mature antibodies and an astonishing 160-fold(!!) for unmatured ones.</p><p><strong>Open questions:</strong></p><ul><li>What are the limits of scaling laws for protein models, and at what volume of data do these models reach their maximum effectiveness?</li></ul><h2 id="conclusion">Conclusion</h2><p>Despite all the significant progress, we are still in the early stages of fully grasping the complexity of protein sequence space. For a long time, the only method to understand the relationship between proteins involved explicit pairwise or multiple sequence alignments, which rely on presumed evolutionary connections to map one protein's residues to another's. Recently, however, a more generalized approach is taking shape, focusing less on evolutionary lineage and more on the essential functional and structural aspects of proteins. If this pace of progress continues, we stand on the brink of potentially groundbreaking discoveries—uncovering unknown facets of familiar proteins and even synthesizing entirely novel proteins.</p><p>One challenge to this vision is the (lack of) human interpretability of such representations. As these models become more complex, understanding how they process and represent protein sequences is crucial, especially for applications like drug discovery where identifying how models predict binding sites can be very useful. The next steps in protein modeling also involve developing more biologically inspired models. This means deepening our integration of biochemical knowledge within the frameworks of these models to refine their accuracy and functionality. By embedding more profound biological insights into model training and data processing, we can significantly improve outcomes across various protein-related tasks.</p><p>It was a lot of fun diving into protein language models. As I read more, the potential for more breakthroughs in protein science seems promising, especially by combining bigger and better models with clever experiment design.  I’m excited to see what the next steps in protein modeling bring us!</p><p><em>P.S. I am not an expert in protein language models or protein engineering but am deeply intrigued by this field. If you notice any inaccuracies in this post, please feel free to correct me.</em></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Upstreaming Linux kernel support for the Snapdragon X Elite (169 pts)]]></title>
            <link>https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite</link>
            <guid>40350408</guid>
            <pubDate>Tue, 14 May 2024 00:56:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite">https://www.qualcomm.com/developer/blog/2024/05/upstreaming-linux-kernel-support-for-the-snapdragon-x-elite</a>, See on <a href="https://news.ycombinator.com/item?id=40350408">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Not an iPad Pro Review: Why iPadOS Still Doesn't Get the Basics Right (307 pts)]]></title>
            <link>https://www.macstories.net/stories/not-an-ipad-pro-review/</link>
            <guid>40349347</guid>
            <pubDate>Mon, 13 May 2024 22:33:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macstories.net/stories/not-an-ipad-pro-review/">https://www.macstories.net/stories/not-an-ipad-pro-review/</a>, See on <a href="https://news.ycombinator.com/item?id=40349347">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
<p id="p2">Let me cut to the chase: sadly, I don’t have a new iPad Pro to review today on MacStories.</p>
<p id="p3">I was able to try one in London last week, and, <a href="https://www.macstories.net/stories/thoughts-and-first-impressions-on-the-new-ipad-pros-from-apples-event-in-london/" rel="noopener noreferrer">as I wrote</a>, I came away impressed with the hardware. However, I didn’t get a chance to use a new iPad Pro over the past six days ahead of today’s review embargo.</p>
<p id="p4">I know that many of you were expecting a deeper look at the iPad Pro on MacStories this week, but that will have to come later. I still plan on upgrading to a 13” iPad Pro myself; I’ve decided I want to return to the larger size after <a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/" rel="noopener noreferrer">a few months with the 11” iPad Pro</a>. If you’re interested in checking out reviews of the new iPad Pros from heavy iPad users like yours truly <em>right now</em>, I highly recommend reading and watching what my friends <a href="https://sixcolors.com/post/2024/05/m4_ipad_pro_review/" rel="noopener noreferrer">Jason Snell</a> and <a href="https://www.youtube.com/watch?v=k4VG-XktLBY" rel="noopener noreferrer">Chris Lawley</a> have prepared.</p>
<p id="p6">Still, as I was thinking about my usage of the iPad and <em>why</em> I enjoy using the device so much despite its limitations, I realized that I have never actually written about all of those “limitations” in a single, comprehensive article. In our community, we often hear about the issues of iPadOS and the obstacles people like me run into when working on the platform, but I’ve been guilty in the past of taking context for granted and assuming that you, dear reader, also know precisely what I’m talking about.</p>
<p id="p7">Today, I will rectify that. Instead of reviewing the new iPad Pro, I took the time to put together a list of all the common problems I’ve run into over the past…<em>checks notes</em>…<strong>12 years of working on the iPad</strong>, before its operating system was even called iPadOS.</p>
<p id="p8">My goal with this story was threefold. First, as I’ve said multiple times, I love my iPad and want the platform to get better. If you care about something or someone, sometimes you have to tell them what’s wrong in order to improve and find a new path forward. I hope this story can serve as a reference for those with the power to steer iPadOS in a different direction in the future.</p>
<p id="p9">Second, lately I’ve seen some people argue on Mastodon and Threads that folks who criticize iPadOS do so because their ultimate goal is to have macOS on iPads, and I wanted to clarify this misunderstanding. While I’m on the record as thinking that a hybrid macOS/iPadOS environment would be terrific (I know, <a href="https://www.macstories.net/stories/macpad-how-i-created-the-hybrid-mac-ipad-laptop-and-tablet-that-apple-wont-make/" rel="noopener noreferrer">because I use it</a>), that is not the point. The reality is that, regardless of whether macOS runs on iPads or not, iPadOS <em>is</em> the ideal OS for touch interactions. But it still gets many basic computing features wrong, and there is plenty of low-hanging fruit for Apple to pick. We don’t need to talk about macOS to cover these issues.</p>
<p id="p10">Lastly, I wanted to provide readers with the necessary context to understand what I mean when I mention the limitations of iPadOS. My iPad setup and workflow have <a href="https://www.macstories.net/ipad/" rel="noopener noreferrer">changed enough times</a> over the years that I think some of you may have lost track of the issues I (and others) have been experiencing. This article is a chance to collect them all in one place.</p>
<p id="p11">Let’s dive in.</p>

<h2>Table of Contents</h2><ul><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#missing-apps"><span>Missing Apps</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#not-so-desktop-class-apps"><span>Not-So-Desktop-Class Apps</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#files-a-slow-unreliable-file-manager"><span>Files: A Slow, Unreliable File Manager</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#audio-limitations"><span>Audio Limitations</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#multitasking-a-fractured-mess"><span>Multitasking: A Fractured Mess</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#spotlight"><span>Spotlight</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#lack-of-background-processes-and-system-wide-utilities"><span>Lack of Background Processes and System-Wide Utilities</span></a><ul><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#to-be-fixed-later-this-year-but-only-for-some"><span>To Be Fixed Later This Year, But Only for Some</span></a></li></ul></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#inefficiency-by-a-thousand-cuts"><span>Inefficiency by a Thousand Cuts</span></a></li><li><a href="https://www.macstories.net/stories/not-an-ipad-pro-review/#the-need-for-change"><span>The Need for Change</span></a></li></ul><h2 id="missing-apps">Missing Apps</h2>
<p id="p15">Of all the issues I have with iPadOS, I want to start with a relatively simple one: some apps from macOS and iOS just aren’t available on the platform.</p>
<p id="p16">This was fine when the iPad was a new product and Apple was busy launching <a href="https://en.wikipedia.org/wiki/WatchOS" rel="noopener noreferrer">new OSes</a> and <a href="https://www.macstories.net/news/apples-back-to-the-mac-event-now-available-for-download-streaming/" rel="noopener noreferrer">rethinking</a> their approach to the Mac. However, after 14 years, it’s hard to imagine that the company couldn’t have filled these gaps if they really wanted to.</p>
<p id="p17">Here are the apps from iOS and macOS I find myself missing the most on the iPad:</p>
<ul id="ul18"><li><strong>Calculator.</strong> I honestly think it’s wild that after 14 years, the iPad still doesn’t come with a built-in calculator app and that you have to use Google or <a href="https://apps.apple.com/us/app/pcalc/id284666222" rel="noopener noreferrer">a third-party calculator</a> for basic operations. Fortunately, it sounds like Apple is going to address this issue in the <a href="https://www.macrumors.com/2024/04/23/calculator-app-for-ipad-rumor/" rel="noopener noreferrer">upcoming iPadOS 18 release</a>.</li>
<li><strong>TextEdit.</strong> I can’t tell you how many times I use <a href="https://support.apple.com/guide/textedit/welcome/mac" rel="noopener noreferrer">TextEdit</a> on my Mac as a simple scratchpad for bits of text I copy from different apps and need to keep around in a temporary holding spot. TextEdit is also a capable, minimal, and reliable text editor for plain and rich text files. Any modern computing platform should have a built-in text editor, but iPadOS doesn’t. Given how Microsoft was even able to <a href="https://www.theverge.com/2024/2/8/24066389/microsofts-copilot-ai-can-explain-stuff-to-you-in-notepad" rel="noopener noreferrer">productize Notepad on Windows</a> with modern features, this feels like a missed opportunity for Apple.</li>
<li><strong>Preview.</strong> Another unsung hero of Apple’s desktop apps is <a href="https://support.apple.com/guide/preview/welcome/mac" rel="noopener noreferrer">Preview</a>. Whether you have an image or PDF document that you want to check out in more detail, or perhaps even edit, you can rest assured that Preview for macOS has your back. The iPad feels like the ideal platform for Preview: the device is great for viewing photos or reading PDF documents, and the Apple Pencil would take Preview’s annotation capabilities to the next level. Apple probably wants you to believe that <a href="https://developer.apple.com/documentation/quicklook" rel="noopener noreferrer">Quick Look’s interactive system-wide previews</a> on iOS and iPadOS are a substitute for the Preview app, but that’s not the case. There’s something about Preview’s clarity and ease of use that can’t be replaced by a simple Quick Look preview, and that’s not to mention the features that iPadOS’ Quick Look lacks compared to Preview for macOS.</li>
<li><strong>Journal.</strong> This is <a href="https://www.macstories.net/reviews/apples-journal-app-journaling-for-all/" rel="noopener noreferrer">a recent addition</a>, but I was very surprised when the Journal app was launched on iPhone without an iPad counterpart. Just like with Preview, the iPad feels like the optimal platform for Journal: it’s a device you can grab at the end of the day, while reflecting on the things you’ve done and the places you’ve been, as you unwind and get ready for what’s coming up tomorrow. An iPadOS version of Journal would be a fantastic way to select photos you want to remember, jot down a few thoughts, and quickly interact with Apple’s journaling suggestions. As apps like <a href="https://apps.apple.com/us/app/day-one-journal-private-diary/id1044867788" rel="noopener noreferrer">Day One</a> and <a href="https://apps.apple.com/us/app/everlog-journal/id1519935634" rel="noopener noreferrer">Everlog</a> have shown, an iPad can be a fantastic device for journaling.</li>
</ul>
<p id="p20">The list could go on, especially if we consider that Apple sells “Pro” versions of the iPad that cost thousands of dollars, which would benefit from apps that a lot of professionals typically use on macOS. Where’s <a href="https://en.wikipedia.org/wiki/Terminal_(macOS)" rel="noopener noreferrer">Terminal</a> for iPadOS? Why do we have to manage fonts with an <a href="https://www.macstories.net/stories/ios-and-ipados-13-the-macstories-review/28/#settings" rel="noopener noreferrer">obscure method</a> in Settings instead of the excellent <a href="https://support.apple.com/guide/font-book/welcome/mac" rel="noopener noreferrer">Font Book</a> app? Dare I even say it…where’s <a href="https://developer.apple.com/xcode/" rel="noopener noreferrer">Xcode</a> for iPadOS?</p>
<h2 id="not-so-desktop-class-apps">Not-So-Desktop-Class Apps</h2>
<p id="p21">While I’m on the topic of apps, it’s worth pointing out that the apps that did find their way to iPadOS still pale in comparison to their Mac versions in terms of feature set. Despite Apple’s <a href="https://www.macstories.net/stories/ios-and-ipados-16-the-macstories-overview/" rel="noopener noreferrer">promise of desktop-class apps</a> a couple of years ago, the company’s actual implementation has been erratic at best, with an inconsistent delivery of Mac-like features that haven’t done much to raise the status of iPad apps.</p>
<p id="p22">The simplest way to look at this is that most built-in iPad apps don’t match the functionalities offered by their macOS counterparts <strong>despite iPadOS having all the prerequisites</strong> for them to offer said features. For instance, while Apple shipped smart folders and smart lists in <a href="https://www.macstories.net/stories/ios-and-ipados-15-the-macstories-review/14/" rel="noopener noreferrer">Notes</a> and <a href="https://www.macstories.net/stories/reminders-smart-lists-put-unprecedented-control-in-the-hands-of-users/" rel="noopener noreferrer">Reminders</a>, respectively, Mail for iPadOS still doesn’t let you create smart mailboxes like it does on the Mac; nor does Music for iPadOS support the Mac version’s smart playlists. The same is also true for the Files app and its lack of Finder’s smart folder functionality.</p>

<p id="p24">The issue extends beyond these power-user features and touches all sorts of aspects of other Apple apps for iPadOS. Shortcuts is one of the worst offenders with a complete disregard for the iPad’s multitasking capabilities: Shortcuts for Mac offers powerful actions to find and control windows on-screen; the iPad version does not. Safari for Mac lets you adjust the browser’s toolbar to your liking with the ability to drag buttons around and pin specific extensions; Apple rolled out customizable toolbars for iPadOS two years ago, but they never added support for them in Safari for iPad. On the Mac, there is an excellent <a href="https://support.apple.com/guide/dictionary/welcome/mac" rel="noopener noreferrer">Dictionary</a> app that you can open to search for words, look up their definitions, and more; on the iPad, the “dictionary” is limited to the ‘Look Up’ option of the edit menu, which forces you to always select some text first.<sup id="fnref-75358-OWMLG"><a href="#fn-75358-OWMLG" rel="noopener noreferrer">1</a></sup></p>

<p id="p26">I hear what some of you might say: <em>“The iPad is supposed to be a lighter platform than the Mac!”</em> But the thing is, you can’t have it both ways. You can’t make a big deal of bringing desktop-class experiences to the iPad (rightfully so, given the price of iPad Pro models) and then do so inconsistently and sporadically. If iPadOS is meant to support desktop-class apps, the job <a href="https://youtu.be/k6hZ9KdG1QU?si=PnbTAjcmcbda9Zy4" rel="noopener noreferrer">can’t be done halfway</a>.</p>
<h2 id="files-a-slow-unreliable-file-manager">Files: A Slow, Unreliable File Manager</h2>
<p id="p27">Out of all the apps I’ve mentioned so far, I want to shine a spotlight on Files. It’s a bad product that needs a fundamental rethink from a design and performance perspective.</p>
<p id="p28">Files has only marginally improved <a href="https://www.macstories.net/stories/ios-11-the-macstories-review/14/#files" rel="noopener noreferrer">since its debut in iOS 11</a> (!), and we’re well past the point of arguing that, well, iPads aren’t meant to have a file manager. Apple has offered a native iPad file manager for exactly half the iPad’s lifetime; that ship has sailed. It’s time for Apple to take the Files app seriously, because this version just doesn’t cut it.</p>
<p id="p30">It starts from the very basics. Files is not a reliable app, and it’s certainly not as reliable as Finder on macOS. More often than not, I try to select some files in a location to copy or move them to another, and the operation either gets stuck or canceled. This frequently happens with large files located on external drives, forcing me to fall back to my Mac if I want to quickly and reliably copy something from an SSD and save it on my computer. Basic copy, cut, and move operations should be the cornerstone of a file manager, and the fact that Files still fails at those is inexcusable.</p>
<p id="p31">Compared to Finder, the Files app also feels sluggish, and it often gets stuck in an unresponsive state that requires me to force quit it. This tends to occur with folders stored in iCloud Drive, which never open instantly, the way they do on my Mac. Furthermore, Files’ integration with iCloud Drive is much more aggressive than Finder about purging downloaded copies from my iPad’s local storage, causing me to realize – at random times – that a file I need is no longer available offline.</p>
<p id="p32">Sadly, the list of problems goes on:</p>
<ul id="ul33"><li>Unlike Finder, the Files app doesn’t display transfer speeds when moving or copying something. This becomes especially problematic for large files that I’m moving to or from an external drive, making me second-guess whether the transfer is working at all.</li>
<li>Quick Actions aren’t customizable like they are on macOS (and have been <a href="https://www.macstories.net/stories/macos-mojave-the-macstories-review/2/#quick-actions" rel="noopener noreferrer">since Mojave</a>). Files’ inspector panel does support Quick Actions, but, for some reason, you can’t build your own custom actions using Shortcuts. The technology is literally the same across platforms, but one file manager doesn’t support it all the way.</li>
<li>Speaking of Shortcuts, on the iPad, the app has no concept of “getting the current selection” from the Files app. On the Mac, you can create shortcuts that act on the currently-selected file(s) in Finder; on the iPad, Shortcuts doesn’t offer a matching action for the Files app.</li>
<li>We still cannot create smart folders in Files for iPad. The technology is clearly there –&nbsp;Apple added a similar feature to Notes and Reminders – but the Files app for iPad remains behind on this front too.</li>
</ul>
<p id="p35">I saved the most absurd limitation of Files for last. As of iPadOS 17, it’s still impossible to set default apps for opening specific file types. I shouldn’t even have to explain why this is a ridiculous shortcoming, but here we are. On the iPad, every document you click in Files defaults to showing you a Quick Look preview, and there is no way to tell the system that you want to view the document with another app instead.</p>
<p id="p36">It’s not like <a href="https://www.theverge.com/24153045/macos-default-apps-email-browser-how-to" rel="noopener noreferrer">macOS has a fantastic UI for this</a> (Windows has <a href="https://support.microsoft.com/en-us/windows/change-default-programs-in-windows-e5d82cad-17d1-c53b-3505-f10a32e1894d" rel="noopener noreferrer">a much better system</a> for file defaults), but at least it’s something, and it works. On iPad, the Files team seemingly failed to acknowledge that people may want to use different apps for different document types; instead, iPadOS assumes all the file management you could ever need should be happening inside the Files app. The inability to just say, “I want to open this file in [App XYZ]”, is completely unacceptable.</p>


<p id="p39">After seven years, I’m starting to wonder if maybe it’s time for Apple to scrap the Files project and start over with a new app based on the strong foundation of Finder. We’re well past the point of excusing the Files app for being a young file manager; when you’re spending $3,000 on a high-end iPad Pro with plenty of storage, you want the app to manage that storage to be flawless.</p>
<p id="p40">Files is not that app.</p>
<h2 id="audio-limitations">Audio Limitations</h2>
<p id="p41">There are two key problems with the audio system of iPadOS:</p>
<ul id="ul42"><li>You cannot play multiple audio streams at the same time.</li>
<li>You cannot record your own local audio while on a VoIP call.</li>
</ul><p id="p43"><a href="https://sixcolors.com/post/2019/02/a-week-of-podcasting-with-only-an-ipad-pro/" rel="noopener noreferrer">Jason Snell</a> and I have widely documented these issues over the years, but especially the second one as it relates to podcasting on iPad. Jason was my source of inspiration years ago when I tried to record podcasts on my iPad Pro, and…the setup I concocted was a bit of a mess. Allow me to <a href="https://www.macstories.net/stories/beyond-the-tablet/11/#podcasting-from-ipad-pro" rel="noopener noreferrer">quote myself from five years ago</a>:</p>
<blockquote id="blockquote44"><p>
  To record a podcast, I have a conversation with my co-hosts on Skype. However, we don’t use the audio of the Skype conversation itself as the episode: while we’re talking, each of us records a high-quality local audio track from our microphones; at the end, multiple local tracks are mixed together in one audio file that gives the illusion we’re all talking together in the same room rather than thousands of miles apart over Skype. This right here is, by far, the most important requirement for the way I like to record podcasts: we want to use Skype, but each of us has to record the local track of our own microphone input.</p>
<p>  Secondly – and this is why I’ve been unwilling to try other “solutions” for podcasting on iPad over the years – I want my co-hosts to hear my voice coming through the same microphone I’m using to record my local audio track. To have a good conversation with natural back and forth that yields a good final product, I think it’s important that everyone on Skype gets to hear the other person loud and clear.
</p></blockquote>
<p id="p45">Replace “Skype” with “Zoom”, and nothing has changed since I wrote this in 2019. If I want to record my microphone’s local audio while also being on a call, iPadOS doesn’t let me do it. It’s no surprise that I, Jason, and others ended up going back to a Mac, if only to record podcasts the way we like, which is not even a particularly weird way of recording audio. All I’m asking for is a way to record myself while I’m on a call, and iPadOS 17 still doesn’t support it. Plus, I have to imagine that this issue would apply to other fields of audio production as well, such as musicians collaborating <a href="https://www.grammy.com/news/the-postal-service-give-up-20th-anniversary-jenny-lewis-jimmy-tamborello-interview-death-cab-tour" rel="noopener noreferrer">remotely</a>, folks who want to record screencasts over apps that play audio, and more.</p>
<p id="p46">Furthermore, these audio restrictions are part of a broader conversation regarding iPadOS and multiple audio streams, or the lack thereof. Have you ever found yourself watching a YouTube or Twitch video on mute on your Mac while also listening to podcasts or music on the same computer? It’s nice, right?</p>

<p id="p48">Well, the same scenario is impossible to achieve on iPadOS. Only one media playback session can exist at a time on the iPad, so if you’re listening to music and start a video, the music will pause, and vice versa. This is why iPadOS doesn’t know how to deal with recording audio while also outputting in another VoIP app. Fourteen years on, I find it hard to believe that an iPad has to be limited to playing only one source of audio at a time.</p>
<h2 id="multitasking-a-fractured-mess">Multitasking: A Fractured Mess</h2>
<p id="p49">Where do we even begin here?</p>
<p id="p50">There is no better definition of uneven development than the iPad’s history with multitasking interfaces. As my <a href="https://www.macstories.net/stories/ios-11-the-macstories-review/17/#split-view" rel="noopener noreferrer">annual</a> <a href="https://www.macstories.net/stories/ios-and-ipados-13-the-macstories-review/21/#multitasking-and-multiwindow" rel="noopener noreferrer">reviews</a> <a href="https://www.macstories.net/stories/ios-and-ipados-15-the-macstories-review/6/#multitasking" rel="noopener noreferrer">can</a> <a href="https://www.macstories.net/stories/ios-and-ipados-17-the-macstories-review/6/#stage-manager" rel="noopener noreferrer">confirm</a>, Apple has shown a tendency to release a new iPad multitasking UI every 2-3 years, never refine it, and discard it to make room for another iteration that starts the cycle anew.</p>
<p id="p51">If Apple followed the same approach with the Mac since its launch in 1984, we’d have 13 different multitasking methods today. Instead, macOS has largely been built around the one freeform multi-window approach, with full-screen mode and Stage Manager tacked onto the system over the past decade.</p>
<p id="p52">Stage Manager is Apple’s latest attempt to bring a more flexible and desktop-like multitasking environment to the iPad. As I explained nearly two years ago when it launched, its <a href="https://www.macstories.net/stories/stage-manager-ipados-16-1-review/" rel="noopener noreferrer">original release was a disaster</a>. Apple’s first take on Mac-like multiwindowing was riddled with bugs and performance issues, which, for a while, forced me to revert to traditional multitasking via Split View and Slide Over.</p>

<p id="p54">Fortunately, with last year’s iPadOS 17, Apple <a href="https://www.macstories.net/stories/with-ipados-17-stage-manager-is-finally-moving-in-the-right-direction/" rel="noopener noreferrer">cleaned up</a> several of Stage Manager’s initial stability woes and inconsistencies while also giving pro users more freedom in terms of window placement and touch-based controls. However, Apple’s efforts last year only improved the basic functionality of Stage Manager, leaving several requests unanswered and letting Stage Manager’s foundation languish without the additional flexibility that one would expect from a pro-oriented feature.</p>
<p id="p55">To name a few:</p>
<ul id="ul56"><li><strong>Stage Manager is still limited to four windows at once.</strong> Despite the iPad Pro becoming more and more powerful over time (to the point that the latest iPad Pros are now one M-chip generation ahead of MacBooks), Stage Manager still forces you to work with only four windows shown on-screen at once. Imagine if a 13-inch MacBook Air could only let you see four windows at the same time. And no, Stage Manager for Mac doesn’t have this limitation, proving that – <a href="https://en.wikipedia.org/wiki/System_on_a_chip" rel="noopener noreferrer">SoCs</a> and screen sizes being equal – Apple’s architecture is more than capable of going beyond four simultaneous windows.</li>
<li><strong>It’s still impossible to create presets for often-used window combinations.</strong> iPadOS still doesn’t offer a way to save frequently-used app combinations as “presets” or “pairs” that you can recreate with one click. Shortcuts could be a way to address this limitation, but Shortcuts for iPadOS doesn’t integrate with Stage Manager at all. Android has offered this functionality on phones for several years now. With Stage Manager for iPad, pro users have to manually recreate their workspaces from scratch every time.</li>
<li><strong>Sometimes the wrong window still remains active and receives keyboard input.</strong> This is a bug that keeps getting in the way of my ability to get work done on a daily basis. Sometimes I have, say, Safari and <a href="https://obsidian.md/" rel="noopener noreferrer">Obsidian</a> windows next to each other in Stage Manager, and I <em>think</em> I’ve selected Safari. But when I invoke a specific keyboard shortcut, text input is received by Obsidian’s text editor instead. Imagine if a Mac did this.</li>
<li><strong>It’s still unclear how to spawn a new window for the frontmost app, or how to see all windows from the current app.</strong> On a Mac, if you’re working with a window in front of you (regardless of whether you’re using Stage Manager or not), you have a consistent, universal way of creating a new window for the current app: the menu bar’s File ⇾ New Window menu. On iPad, this is not the case. There is no system-wide keyboard shortcut to create a new window for the app you’re currently using; to create a new window (and only in apps that support multiple windows), you have to long-press its icon in the dock (<em>if</em> you have that app in your dock), select ‘Show All Windows’, and only then can you press a ‘+’ button to create a new window for the app…in a separate workspace. All of these interactions need to be faster, simpler, and better-presented to users instead of being tucked away within long-press menus.</li>
<li><strong>There is no way to quickly preview all windows in the current workspace.</strong> Similarly, iPadOS doesn’t provide users with an easy-to-use, fast way of previewing all windows in the current workspace. The Mac has long offered <a href="https://support.apple.com/lt-lt/guide/mac-help/mh35798/mac" rel="noopener noreferrer">Mission Control</a> as a system to get a bird’s eye view of windows on your desktop, and it works with both Stage Manager and traditional multiwindowing. On the iPad, none of this exists.</li>
</ul><p id="p57">I don’t think any of these features fall under the hypothetical umbrella of “Mac features you shouldn’t wish for on an iPad”. If Apple sells a product called “iPad Pro” that supports desktop-class apps with desktop-class multitasking, that experience should also be, well, desktop-class. But it’s not. Apple did the bare minimum work for Stage Manager, “fixed” it last year to make it at least passable, and never considerably improved it since launch – which, in many ways, is the story of iPad multitasking so far.</p>
<p id="p58">I insist on covering this aspect of the iPad experience because Apple has followed the same path with Split View and Slide Over, which haven’t received substantial updates in years, either. And I know for a fact that, if only time and resources were devoted to it, it wouldn’t have to be this way.</p>
<p id="p59">For a few months earlier this year, I spent some time with a <a href="https://www.oneplus.com/global/open" rel="noopener noreferrer">OnePlus Open</a> foldable. The device is obviously not as pleasant as an iPad, and I fundamentally dislike the Android app ecosystem, but believe me when I say that the folks at OnePlus (objectively, a smaller company than Apple) have shipped a vastly better tiling interface for split-screen multitasking than Apple.</p>
<p id="p60">OnePlus’ ‘Open Canvas’ multitasking UI has several advantages over the iPad’s Split View system. For starters, you can create saved app pairs (which you can pin to your Home Screen), and the same app window can exist either in an app pair or standalone without having to create a new window for the app. This is a major difference from iPadOS’ architecture, where a window can only exist as an “object” in one place at a time. But it goes deeper than that. To take advantage of the Open’s larger screen when unfolded, the Canvas UI lets you add up to three tiles on-screen, and the system automatically zooms out and reflows to either show you a three-column interface or a fluid tiled arrangement with two windows above and one larger window below. It’s incredibly clever and intuitive in practice, and I recommend checking out this video to get a sense of what it looks like:</p>
<p><span><iframe type="text/html" width="640" height="360" src="https://www.youtube.com/embed/1sdA-I9x9R0?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
<p id="p62">All this to say: iPadOS’ multitasking, even without Stage Manager, could be so much more. I think several iPad users (and I was guilty of this, too) have convinced themselves due to Apple’s pace of updates that we’ve reached the peak of what tablet multitasking should do with Split View and Stage Manager. But look outside Apple’s stance on iPadOS, and you see that is not the case. Once again, I’m not arguing for macOS features on the iPad; I’m saying that, if Apple wanted to, it could design innovative, high-performance, delightful <strong>tablet-first</strong> multitasking systems. Sadly, iPad multitasking tells a very different story.</p>
<p id="p63">I could then go on and mention the fact that iPadOS doesn’t offer APIs or sandboxing exceptions for third-party apps to enhance multitasking (like <a href="https://www.fadel.io/missioncontrolplus" rel="noopener noreferrer">they can on a Mac</a>), but that’s a step too far given the circumstances. Serious iPadOS multitasking changes need to happen from the top down; before wishing for third parties to be able to modify and augment multitasking, we need Apple to address the basics first.</p>
<h2 id="spotlight">Spotlight</h2>
<p id="p64">Speaking of the basics, we need to talk about Spotlight for iPadOS.</p>
<p id="p65">Any respectable, modern computer – especially one from Apple – should have a fast and reliable app launcher. Spotlight for iPad, despite some minor improvements over the last few iPadOS releases, still isn’t that.</p>
<p id="p66">As has been <a href="https://twitter.com/gruber/status/1124427793226510337" rel="noopener noreferrer">documented</a> over the years, the iPad’s version of Spotlight has long been afflicted by slow and inconsistent performance. To this day, sometimes I invoke Spotlight and search for an app that I <em>know</em> I have installed on my iPad, and no results come up. Then I dismiss Spotlight, try again, and the app appears. Other times, Spotlight gets stuck: either I press ⌘ + Space and the search box doesn’t appear, or my keyboard’s arrow keys don’t do anything. Usually, a reboot fixes these issues; sometimes, I have to detach my Magic Keyboard and re-attach it for Spotlight to receive keyboard input again.</p>

<p id="p68">Then there’s the “find my stuff” issue. So many times, I try looking for a document inside an app that supports Spotlight indexing, or perhaps a shortcut from the Shortcuts app, and no results come up. Then I try again after a few hours, and results do appear. It drives me insane, and it’s been like this for years now.</p>

<p id="p70">These issues are compounded by the inability to install Spotlight replacements on iPadOS. Spotlight works well on macOS, but if I don’t like it, I can install a replacement like <a href="https://www.raycast.com/" rel="noopener noreferrer">Raycast</a> or <a href="https://www.alfredapp.com/" rel="noopener noreferrer">Alfred</a> and use a different launcher. The iPad has neither the APIs nor the policies in place for these apps to exist. And so I’m left using a half-baked, inconsistent launcher that mostly encumbers my work, hoping that someday, eventually, it’ll get better.</p>
<h2 id="lack-of-background-processes-and-system-wide-utilities">Lack of Background Processes and System-Wide Utilities</h2>
<p id="p71">And now, allow me to wish for one Mac-like feature that is not even that esoteric if you think about it.</p>
<p id="p72">iPadOS needs to gain support for executing long-running, complex tasks in the background. I’m not referring to Background App Refresh, which is the system that lets apps stay active in short bursts in the background to receive push notifications and other updates. I’m talking about the ability to tap into the power of the M-series chips and the iPad’s RAM to keep specific tasks running in the background while you’re doing something else.</p>
<p id="p73">You don’t need to look far to see where iPadOS is failing in this regard. If you use Apple’s own <a href="https://apps.apple.com/us/app/final-cut-pro-for-ipad/id1631624924" rel="noopener noreferrer">Final Cut Pro for iPad</a> – one of the company’s very showcases of the new iPad Pro – and begin exporting a video, then switch apps for even a <em>second</em>, the export is canceled. If you simply switch workspaces in Stage Manager or accidentally click on an incoming notification, an <a href="https://joe-steel.com/#but-the-pro-apps" rel="noopener noreferrer">entire project’s export will fail</a>:</p>

<p id="p75">From a computer that now comes with the M4 and, in certain configurations, 16&nbsp;GB of RAM, this is absurd. Mac laptops with far less impressive specs have been able to keep tasks running in the background for decades. The iPad, now fourteen years old, can’t.</p>
<p id="p76">This limitation extends beyond the realm of pro apps such as Final Cut or Logic and applies to all kind of software that, due to the nature of iPadOS, is impossible to find on the platform. Because of a mix of technical limitations and policy decisions, it’s still impossible for an application that wants to perform something in the background to exist on iPadOS. From clipboard managers and video encoders to automation utilities and AI-based photo editors, if you want to run a time-consuming task in the background on iPad, you’re out of luck.</p>
<p id="p77">As a result, not only have these limitations fostered an environment in which third-party developers are actively discouraged from bringing true desktop-class experiences to iPad, but existing iPad apps still largely feel like blown-up versions of their iPhone counterparts. After all, if the same limitations are shared between a phone and tablet, what’s an iPad app but an iPhone version dressed up with a larger layout?</p>

<p id="p79">iPadOS’ closed, iPhone-like nature has reverberated throughout other parts of the iPad experience. For instance, system-wide utilities can’t currently exist on the platform. Software such as <a href="https://www.macbartender.com/" rel="noopener noreferrer">customization tools</a>, task managers’ <a href="https://culturedcode.com/things/support/articles/2249437/" rel="noopener noreferrer">quick capture windows</a>,  alternative <a href="https://matthewpalmer.net/rocket/" rel="noopener noreferrer">emoji pickers</a>, and drag-and-drop “shelf” <a href="https://dropoverapp.com/" rel="noopener noreferrer">utilities</a> – apps that do not execute memory-intensive background tasks but still want to run in the background – simply can’t exist on iPadOS.</p>
<p id="p81">Does it have to be this way? Apple is well within their rights to choose how they want iPadOS to behave. But when the operating system powers a computer that is ostensibly sold as a laptop replacement, one has to wonder if Apple is making the correct decisions.</p>
<h3 id="to-be-fixed-later-this-year-but-only-for-some">To Be Fixed Later This Year, But Only for Some</h3>
<p id="p82">I hear you: some of these problems will be addressed later this year thanks to the <a href="https://commission.europa.eu/strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digital-markets-act-ensuring-fair-and-open-digital-markets_en" rel="noopener noreferrer">Digital Markets Act</a> in Europe, which has been <a href="https://www.macstories.net/news/the-eu-pulls-ipados-into-the-dma-fray/" rel="noopener noreferrer">extended to apply to iPads as well</a>.</p>
<p id="p83">I’m sure that, later in 2024, we’ll see the likes of <a href="https://rileytestut.com/blog/2020/06/17/introducing-clip/" rel="noopener noreferrer">Clip</a> and other side-loaded apps come to iPadOS in Europe. And it’s also obvious that we’ll receive support for non-WebKit browsers to replace Safari. Besides the fact that these changes are going to be limited to Europe, they also aren’t tied to deeper, global policy-related changes on iPadOS. The issues I’ve outlined so far, and the others I’ll continue highlighting below, don’t indicate that Apple is willing to open up the foundation of iPadOS to allow for more flexible, desktop-class computing. These DMA-related changes just mean that Apple is complying with the law and covering the essential functionalities requested by the European Commission.</p>
<p id="p84">Hiding behind the DMA as a hopeful force for structural iPadOS changes is a farce. At the core of the problem lies Apple’s reticence to take iPadOS to the next level, and the DMA has nothing to do with it. The change, once again, needs to happen from the top down.</p>
<h2 id="inefficiency-by-a-thousand-cuts">Inefficiency by a Thousand Cuts</h2>
<p id="p85">If you’ve used iPadOS long enough (the iPad has been my primary computer for 12 years now), I’m sure you’ve run into these: the small bugs, annoyances, and missing features that don’t seem like much in isolation. Considered as a whole, however, they paint a not-too-rosy picture for an operating system that, 14 years into its existence, still lags behind macOS in terms of basic functionalities and problems that have never been addressed. Let me mention just a few examples.</p>
<p id="p86">iPadOS still doesn’t support third-party backup tools or a built-in one like <a href="https://support.apple.com/en-us/104984" rel="noopener noreferrer">Time Machine</a>. If you’re one of those professional users whom Apple caters to, such as a photographer or YouTuber, you know how painful this is. If you want to back up your work, which likely spans multiple terabytes of storage these days, you have two solutions on iPad:</p>
<ul id="ul87"><li>Back up your work manually.</li>
<li>Rely on iCloud backups.</li>
</ul><p id="p88">Sure, you could use iCloud backup, and, as long as your work archive does not exceed 12 TBs, and assuming the Files app works for you, you’ll be okay. But we’d be kidding ourselves if we thought that an online-only backup system with no support for local or off-site storage was the answer for folks whose livelihoods depend on preserving the files they create. Where is Time Machine for iPadOS with support for versioning? Where are third-party APIs to allow for tools like <a href="https://www.backblaze.com/cloud-backup/personal" rel="noopener noreferrer">Backblaze</a> or <a href="https://bombich.com/" rel="noopener noreferrer">Carbon Copy Cloner</a> to exist on iPad?</p>

<p id="p90">We’ve gotten used to this idea that such tools “shouldn’t exist” on iPad because the iPad was meant to be different from the Mac. But then again, we can’t have it both ways: we can’t celebrate the <a href="https://www.apple.com/newsroom/2023/05/apple-brings-final-cut-pro-and-logic-pro-to-ipad/" rel="noopener noreferrer">arrival</a> of Mac apps such as Final Cut and Logic on iPad while also glossing over the lack of flexibility that professional users have on Apple’s other platform. That is, unless we want to live with a delusional idea of “professionals” only needing to back up their files on a Mac because nothing could ever happen when they do the same work on an iPad.</p>
<p id="p91">Platforms may differ, but work is work. You can’t be all loosey-goosey about work only when it’s convenient for your theory that iPadOS should be “lighter”.</p>
<p id="p92">I could then mention the lack of clamshell mode, which is another example of Apple getting an iPad functionality <em>almost</em> right without finishing it. The company rolled out support for external displays with Stage Manager two years ago, and that’s been a fantastic addition to the platform, allowing people to connect an iPad to a monitor and double their workspace. The iPad Pro supports Thunderbolt 4, which implicitly supports <a href="https://www.displayport.org/displayport-over-usb-c/" rel="noopener noreferrer">DisplayPort over USB-C</a>, making the iPad compatible with a wide range of third-party displays too. It’s great.</p>
<p id="p93">The problem is that if you want to use an iPad with an external display at a desk, you have to keep it open and unlocked. Even if you’ve paired an iPad with an external keyboard and mouse or trackpad, you can’t just close the Magic Keyboard on top of the iPad and assume that the external display connection will continue working. This, of course, is not ideal; it forces you to always keep the iPad open somewhere on your desk, with its display turned on and accessible because – this is the worst part – some functionalities such as Control Center cannot be used on the secondary monitor at all.</p>

<p id="p95">Like so many other iPad workflows, there are <a href="https://www.macstories.net/notes/faking-clamshell-mode-with-external-displays-in-ipados-17/" rel="noopener noreferrer">workarounds</a>, but they’re suboptimal, and they’re no replacement for the real functionality that Apple should ship – a functionality that, as you may imagine, has been working fine for ages on the Mac. I’m sure that someone out there will spin this as, “Actually, I like that I need to keep my iPad open”, but I don’t buy it.</p>
<p id="p96">I could then maybe mention how the Home Screen still doesn’t let you place icons freely anywhere you want, or pin specific folders and files for quick access, thus feeling like an enlarged version of the iOS Home Screen. Or maybe I should cover the <a href="https://mastodon.macstories.net/@viticci/110617100820329640" rel="noopener noreferrer">floating keyboard “thingy”</a>, that button that routinely gets in the way of text fields and other UI elements when you’re typing inside apps with a Magic Keyboard. Perhaps I should note that sometimes the Magic Keyboard’s pointer gets stuck, requiring a reboot of the iPad itself?</p>
<p id="p97">You get the idea.</p>
<h2 id="the-need-for-change">The Need for Change</h2>

<p id="p99">You know what’s equally the best and worst part of all this? That I still love the iPad.</p>
<p id="p100">The iPad is the only Apple computer that genuinely feels made for someone like me – a person who loves <a href="https://www.macstories.net/stories/modular-computer/" rel="noopener noreferrer">modularity</a>, freedom, and the mix of touch and keyboard interactions. I share my frustrations because I <em>care</em> about the platform and want it to get better. But at the same time, we need to face reality: the iPad’s operating system isn’t improving at the speed the hardware deserves – that iPad owners who spent thousands of dollars on these machines <em>deserve</em>.</p>
<p id="p101">Something needs to change.</p>
<p id="p102">With <a href="https://www.macstories.net/news/apple-announces-new-11-and-13-ipad-pros/" rel="noopener noreferrer">new iPad Pros</a> nearly upon us, it’s time to admit that iPadOS is not an operating system of the same caliber as Apple’s new hardware. iPadOS has been the victim of erratic updates over the years, with features that were meant to “reimagine” desktop computing only to get not even halfway there and be left to languish for years. Once again, I am <em>not</em> suggesting that the solution is to put macOS on iPad and call it a day. I’m saying that if that’s not in the cards, then Apple should consider all the ways iPadOS is still failing at basic computing tasks. I’d be okay with iPads running iPadOS forever. But if we passively accept that this is as good as an iPad can get, I strongly believe that we’ll play a role in letting Apple squander the greatest computer form factor they’ve ever created.</p>
<p id="p104">I’m tired of hearing apologies that smell of <a href="https://en.wikipedia.org/wiki/Stockholm_syndrome" rel="noopener noreferrer">Stockholm syndrome</a> from iPad users who want to invalidate these opinions and claim that everything is perfect. I’m tired of seeing this cycle start over every two years, with fantastic iPad hardware and the usual (justified), <em>“But it’s the software…”</em> <a href="https://512pixels.net/2024/05/the-problems-never-the-hardware/" rel="noopener noreferrer">line at the end</a>. I’m tired of feeling like my computer is a second-class citizen in Apple’s ecosystem. I’m tired of being told that iPads are perfectly fine if you use Final Cut and Logic, but if you don’t use those apps and ask for more desktop-class features, you’re a weirdo, and you should just get a Mac and shut up. And I’m tired of seeing the best computer Apple ever made not live up to its potential.</p>
<p id="p105">I started using the iPad as my main computer when I was stuck in a hospital bed and couldn’t use a laptop. I kept using it because once you get a taste of that freedom, it’s hard to go back. I will continue using it because none of the alternatives match Apple’s hardware quality, app ecosystem, and pure <strong>delight</strong>. But loving something doesn’t mean ignoring its flaws. And iPadOS is a flawed operating system that still doesn’t get the basics right and, as a result, drags down the entire product line.</p>
<p id="p106">I’m looking forward to the new iPad Pros, but I can’t shake the feeling that the same old iPadOS cycle is about to begin all over again.</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Which cognitive psychology findings are solid that I can use to help students? (131 pts)]]></title>
            <link>https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student</link>
            <guid>40348986</guid>
            <pubDate>Mon, 13 May 2024 21:57:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student">https://matheducators.stackexchange.com/questions/27839/which-cognitive-psychology-findings-are-solid-that-i-can-use-to-help-my-student</a>, See on <a href="https://news.ycombinator.com/item?id=40348986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>There's a highly upvoted answer here claiming that practically no cognitive psychology findings hold up in replication. I don't think that's true at all. Sure, many findings don't hold up, but also, many findings <em>do.</em></p>
<p>For instance: we know that actively solving problems produces more learning than passively watching a video/lecture or re-reading notes. This sort of thing has been tested scientifically, numerous times, and it is completely replicable. It might as well be a law of physics at this point. In fact, a <a href="https://www.pnas.org/doi/10.1073/pnas.1319030111" rel="noreferrer">highly-cited meta-analysis</a> states, verbatim:</p>
<blockquote>
<p>"...[C]alls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. … Given our results, it is reasonable to raise concerns about the continued use of traditional lecturing as a control in future experiments."</p>
</blockquote>
<p>So there you go, that's one cognitive psychology finding that holds up: <em>active learning beats passive learning.</em></p>
<p>Another finding: <em>if you don't review information, you forget it.</em> You can actually model this precisely, mathematically, using a <a href="https://en.wikipedia.org/wiki/Forgetting_curve" rel="noreferrer">forgetting curve</a>. I'm not exaggerating when I refer to these things as laws of physics -- the only real difference is that we've gone up several levels of scale and are dealing with noisier stochastic processes.</p>
<p>Okay, but aren't these obvious? Yes, but...</p>
<ul>
<li><p>Yes, but in education, obvious strategies often aren't put into practice. For instance, plenty of classes that still run on a pure lecture format and don't review previously learned unless it's the day before a test.</p>
</li>
<li><p>Yes, but there are plenty of other findings that replicate just as well but are not so obvious.</p>
</li>
</ul>
<p><strong>Here are some less obvious findings.</strong></p>
<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Spacing_effect" rel="noreferrer">spacing effect</a>: more long-term retention occurs when you space out your practice, <em>even if it's the same amount of total practice.</em> As researcher Doug Rohrer <a href="http://uweb.cas.usf.edu/%7Edrohrer/pdfs/Rohrer2009JRME.pdf" rel="noreferrer">states</a>:</li>
</ul>
<blockquote>
<p>"...[T]he spacing effect is arguably one of the largest and most robust findings in learning research, and it appears to have few constraints."</p>
</blockquote>
<ul>
<li><p><em><strong>Note:</strong> There are tons of more detailed scientific references/quotes I want to include, but I'm going to skip them so not to continue blowing up the length of this already-gigantic answer. If you want to see them, <a href="https://www.justinmath.com/books/#the-math-academy-way" rel="noreferrer">here's</a> a draft I'm working on that covers all these findings (and more) with over 300 references and relevant quotes pulled out of those references.</em></p>
</li>
<li><p>A profound consequence of the spacing effect is that the more reviews are completed (with appropriate spacing), the longer the memory will be retained, and the longer one can wait until the next review is needed. This observation gives rise to a systematic method for reviewing previously-learned material called <a href="https://en.wikipedia.org/wiki/Spaced_repetition" rel="noreferrer">spaced repetition</a> (or <a href="https://en.wikipedia.org/wiki/Distributed_practice" rel="noreferrer">distributed practice</a>). A "repetition" is a successful review at the appropriate time.</p>
</li>
<li><p>To maximize the amount by which your memory is extended when solving review problems, it's necessary to avoid looking back at reference material unless you are totally stuck and cannot remember how to proceed. This is called the <a href="https://en.wikipedia.org/wiki/Testing_effect" rel="noreferrer">testing effect</a>, also known as the retrieval practice effect: the best way to review material is to test yourself on it, that is, practice retrieving it from memory, unassisted.</p>
</li>
<li><p>During review, it's also best to spread minimal effective doses of practice across various skills. This is known as <a href="https://en.wikipedia.org/wiki/Varied_practice" rel="noreferrer">mixed practice</a> -- it's the opposite of "blocked" practice, which involves extensive consecutive repetition of a single skill. Blocked practice can give a false sense of mastery and fluency because it allows students to settle into a robotic rhythm of mindlessly applying one type of solution to one type of problem. Mixed practice, on the other hand, creates a "desirable difficulty" that promotes vastly superior retention and generalization, making it a more effective review strategy.</p>
</li>
<li><p>To free up mental processing power, it's critical to practice low-level skills enough that they can be carried out without requiring conscious effort. This is known as <a href="https://en.wikipedia.org/wiki/Automaticity" rel="noreferrer">automaticity</a>. Think of a basketball player who is running, dribbling, and strategizing all at the same time -- if they had to consciously manage every bounce and every stride, they'd be too overwhelmed to look around and strategize. The same is true in math. I wrote more about the importance of automaticity in a recent answer <a href="https://matheducators.stackexchange.com/a/27786/22672">here</a>.</p>
</li>
<li><p>Instructional techniques that promote the most learning in experts, promote the least learning in beginners, and vice versa. This is known as the <a href="https://en.wikipedia.org/wiki/Expertise_reversal_effect" rel="noreferrer">expertise reversal effect</a>. An important consequence is that effective methods of practice for students typically should <em>not</em> emulate what experts do in the professional workplace (e.g., working in groups to solve open-ended problems).</p>
</li>
</ul>
<p><strong>Why haven't these findings transformed education?</strong></p>
<p>In Daniel R. Collins' answer, he states <em>"if there was some magic solution, it would have been implemented large-scale very quickly."</em></p>
<p>That raises the question: if cognitive psychology has found many effective learning strategies (like <a href="https://en.wikipedia.org/wiki/Mastery_learning" rel="noreferrer">mastery learning</a>, <a href="https://en.wikipedia.org/wiki/Spaced_repetition" rel="noreferrer">spaced repetition</a>, <a href="https://en.wikipedia.org/wiki/Testing_effect" rel="noreferrer">the testing effect</a>, and <a href="https://en.wikipedia.org/wiki/Varied_practice" rel="noreferrer">varied practice</a>), then why haven't these learning strategies been implemented large-scale?</p>
<p>Here are a handful of reasons that I'm aware of.</p>
<p><em>1. Leveraging them (at all) requires additional effort from both teachers and students.</em></p>
<p>In some way or another, each strategy increases the intensity of effort required from students and/or instructors, and the extra effort is then converted into an outsized gain in learning.</p>
<p>This theme is so well-documented in the literature that it even has a catchy name: a practice condition that makes the task harder, slowing down the learning process yet improving recall and transfer, is known as a <a href="https://en.wikipedia.org/wiki/Desirable_difficulty" rel="noreferrer">desirable difficulty</a>.</p>
<p>Desirable difficulties make practice more representative of true assessment conditions. Consequently, it is easy for students (and their teachers) to vastly overestimate their knowledge if they do not leverage desirable difficulties during practice, a phenomenon known as the <em>illusion of comprehension.</em></p>
<p>However, the typical teacher is incentivized to maximize the immediate performance and/or happiness of their students, which biases them against introducing desirable difficulties and incentivizes them to promote illusions of comprehension.</p>
<p>Using desirable difficulties exposes the reality that students didn't actually learn as much as they (and their teachers) "felt" they did under less effortful conditions. This reality is inconvenient to students and teachers alike; therefore, it is common to simply believe the illusion of learning and avoid activities that might present evidence to the contrary.</p>
<p><em>2. Leveraging cognitive learning strategies to their fullest extent requires an inhuman amount of effort from teachers.</em></p>
<p>Let's imagine a classroom where these strategies are being used to their fullest extent.</p>
<ul>
<li><p>Every individual student is fully engaged in productive problem-solving, with immediate feedback (including remedial support when necessary), on the specific types of problems, and in the specific types of settings (e.g., with vs without reference material, blocked vs interleaved, timed vs untimed), that will move the needle the most for their personal learning progress at that specific moment in time.</p>
</li>
<li><p>This is happening throughout the entirety of class time, the only exceptions being those brief moments when a student is introduced to a new topic and observes a worked example before jumping into active problem-solving.</p>
</li>
</ul>
<p>Why is this an inhuman amount of work?</p>
<ul>
<li><p>First of all, it's at best extremely difficult, and at worst (and most commonly) impossible, to find a type of problem that is productive for all students in the class. Even if a teacher chooses a type of problem that is appropriate for what they perceive to be the "class average" knowledge profile, it will typically be too hard for many students and too easy for many others (an unproductive use of time for those students either way).</p>
</li>
<li><p>Additionally, to even know the specific problem types that each student needs to work on, the teacher has to separately track each student's progress on each problem type, manage a spaced repetition schedule of when each student needs to review each topic, and continually update each schedule based on the student's performance (which can be incredibly complicated given that each time a student learns or reviews an advanced topic, they're implicitly reviewing many simpler topics, all of whose repetition schedules need to be adjusted as a result, depending on how the student performed). This is an inhuman amount of bookkeeping and computation.</p>
</li>
<li><p>Furthermore, even on the rare occasion that a teacher manages to find a type of problem that is productive for all students in the class, different students will require different amounts of practice to master the solution technique. Some students will catch on quickly and be ready to move on to more difficult problems after solving just a couple problems of the given type, while other students will require many more attempts before they are able to solve problems of the given type successfully on their own. Additionally, some students will solve problems quickly while others will require more time.</p>
</li>
</ul>
<p>In the absence of the proper technology, it is impossible for a single human teacher to deliver an optimal learning experience to a classroom of many students with heterogeneous knowledge profiles, who all need to work on different types of problems and receive immediate feedback on each attempt.</p>
<p><em>3. Most edtech systems do not actually leverage the above findings.</em></p>
<p>If you pick any edtech system off the shelf and check whether it leverages each of the cognitive learning strategies I've described above, you'll probably be surprised at how few it actually uses. For instance:</p>
<ul>
<li><p>Tons of systems don't scaffold their content into bite-sized pieces.</p>
</li>
<li><p>Tons of systems allow students to move on to more material despite not demonstrating knowledge of prerequisite material.</p>
</li>
<li><p>Tons of systems don't do spaced review. (Moreover, tons of systems don't do <em>any</em> review.)</p>
</li>
</ul>
<p>Sometimes a system will appear to leverage some finding, but if you look more closely it turns out that this is actually an illusion that is made possible by cutting corners somewhere less obvious. For instance:</p>
<ul>
<li><p>Tons of systems offer bite-sized pieces of content, <em>but</em> they accomplish this by watering down the content, cherry-picking the simplest cases of each problem type, and skipping lots of content that would reasonably be covered in a standard textbook.</p>
</li>
<li><p>Tons of systems make students do prerequisite lessons before moving on to more advanced lessons, <em>but</em> they don't actually measure tangible mastery on prerequisite lessons. Simply watching a video and/or attempting some problems is <em>not</em> mastery. The student has to actually be getting problems right, and those problems have to be representative of the content covered in the lesson.</p>
</li>
<li><p>Tons of systems claim to help students when they're struggling, <em>but</em> the way they do this is by lowering the bar for success on the learning task (e.g., by giving away hints). Really, what the system needs to do is take actions that are most likely to strengthen a student's area of weakness and empower them to clear the bar fully and independently on their next attempt.</p>
</li>
</ul>
<p>Now, I'm not saying that these issues apply to all edtech systems. I <em>do</em> think edtech is the way forward here -- optimal teaching is an inhuman amount of work, and technology is needed. Heck, I personally developed all the quantitative software behind one <a href="https://mathacademy.com/" rel="noreferrer">system</a> that properly handles the above challenges. All I'm saying is that you can't just take these things at face value. Many edtech systems don't really work from a learning standpoint, just as many psychology findings don't hold up in replication -- but at the same time, some edtech systems <em>do</em> work, shockingly well, just as some cognitive psychology findings <em>do</em> hold up and can be leveraged to massively increase student learning.</p>
<p><em>4. Even if you leverage the above findings, you still have to hold students accountable for learning.</em></p>
<p>Suppose you have the Platonic ideal of an edtech system that leverages all the above cognitive learning strategies to their fullest extent.</p>
<p>Can you just put a student on it and expect them to learn? Heck no! That would only work for exceptionally motivated students.</p>
<p>Most students are not motivated to learn the subject material. They need a responsible adult -- such as a parent or a teacher -- to incentivize them and hold them accountable for their behavior.</p>
<p>I can't tell you how many times I've seen the following situation play out:</p>
<ul>
<li><p>Adult puts a student on an edtech system.</p>
</li>
<li><p>Student goofs off doing other things instead (e.g., watching YouTube).</p>
</li>
<li><p>Adult checks in, realizes the student is not accomplishing anything, and asks the student what's going on.</p>
</li>
<li><p>Student says that the system is too hard or otherwise doesn't work.</p>
</li>
<li><p>Adult might take the student's word at face value. Or, if the adult notices that the student hasn't actually attempted any work and calls them out on it, the scenario repeats with the student putting forth as little effort as possible -- enough to convince the adult that they're trying, but not enough to really make progress.</p>
</li>
</ul>
<p>In these situations, here's what needs to happen:</p>
<ul>
<li><p>The adult needs to sit down next to the student and force them to actually put forth the effort required to use the system properly.</p>
</li>
<li><p>Once it's established that the student is able to make progress by putting forth sufficient effort, the adult needs to continue holding the student accountable for their daily progress. If the student ever stops making progress, the adult needs to sit down next to the student again and get them back on the rails.</p>
</li>
<li><p>To keep the student on the rails without having to sit down next to them all the time, the adult needs to set up an incentive structure. Even little things go a long way, like <em>"if you complete all your work this week then we'll go get ice cream on the weekend,"</em> or <em>"no video games tonight until you complete your work."</em> The incentive has to be centered around something that the student actually cares about, whether that be dessert, gaming, movies, books, etc.</p>
</li>
</ul>
<p>Even if an adult puts a student on an edtech system that is truly optimal, if the adult clocks out and stops holding the student accountable for completing their work every day, then of course the overall learning outcome is going to be worse.</p>
<p><strong>Connecting to mechanics within the brain</strong></p>
<p>Before ending this answer, I want to drive home the point that the cognitive learning strategies discussed here really do connect all the way down to the mechanics of what's going on in the brain.</p>
<p>The goal of mathematical instruction is to increase the quantity, depth, retrievability, and generalizability of mathematical concepts and skills in the student's <a href="https://en.wikipedia.org/wiki/Long-term_memory" rel="noreferrer">long-term memory (LTM)</a>.</p>
<p>At a physical level, that amounts to creating strategic connections between neurons so that the brain can more easily, quickly, accurately, and reliably activate more intricate patterns of neurons. This process is known as <a href="https://en.wikipedia.org/wiki/Memory_consolidation" rel="noreferrer">consolidation</a>.</p>
<p><em>Now, here's the catch:</em> before information can be consolidated into LTM, it has to pass through <a href="https://en.wikipedia.org/wiki/Working_memory" rel="noreferrer">working memory (WM)</a>, which has severely limited capacity. The brain's working memory capacity (WMC) represents the amount of effort that it can devote to activating neural patterns and persistently maintaining their simultaneous activation, a process known as <a href="https://en.wikipedia.org/wiki/Memory_rehearsal" rel="noreferrer">rehearsal</a>.</p>
<p>Most people can only hold about <a href="https://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two" rel="noreferrer">7 digits</a> (or more generally 4 <a href="https://en.wikipedia.org/wiki/Chunking_(psychology)" rel="noreferrer">chunks of coherently grouped items</a>) simultaneously and only for about 20 seconds. And that assumes they aren't needing to perform any mental manipulation of those items – if they do, then fewer items can be held due to competition for limited processing resources.</p>
<p>Limited capacity makes WMC a bottleneck in the transfer of information into LTM. When the cognitive load of a learning task exceeds a student's WMC, the student experiences cognitive overload and is not able to complete the task.</p>
<p>Additionally, different students have different WMC, and those with higher WMC are typically going to find it easier to "see the forest for the trees" by learning underlying rules as opposed to memorizing example-specific details. (This is unsurprising given that understanding large-scale patterns requires balancing many concepts simultaneously in WM.)</p>
<p>It's expected that higher-WMC students will more quickly improve their performance on a learning task over the course of exposure, instruction, and practice on the task. However, once a student learns a task to a sufficient level of performance, the impact of WMC on task performance is diminished because the information processing that's required to perform the task has been transferred into long-term memory, where it can be recalled by WM without increasing the actual load placed on WM.</p>
<p>So, for each concept or skill you want to teach:</p>
<ul>
<li><p>it needs to be introduced after the prerequisites have been learned (so that the prerequisite knowledge can be pulled from long-term memory without taxing WM)</p>
</li>
<li><p>it needs to be broken down into bite-sized pieces small enough that no piece overloads any student's WM</p>
</li>
<li><p>each student needs to be given enough practice to achieve mastery on each piece – and that amount of practice may vary depending on the particular student and the particular learning task.</p>
</li>
</ul>
<p>But also, even if you do all the above perfectly, you still have to deal with <em>forgetting.</em> The representations in LTM gradually, over time, decay and become harder to retrieve if they are not used, resulting in forgetting.</p>
<p>The solution to forgetting is <em>review</em> -- and not just passively re-ingesting information, but actively <em>retrieving</em> it, unassisted, from LTM. Each time you successfully actively retrieve fuzzy information from LTM, you physically refresh and deepen the corresponding neural representation in your brain. But that doesn't happen if you just passively re-ingest the information through your senses instead of actively retrieving it from LTM.</p>
<p><strong>Further Reading</strong></p>
<p>I've written extensively on this. See the working draft <a href="https://www.justinmath.com/books/#the-math-academy-way" rel="noreferrer">here</a> for more info and hundreds of scientific citations to back it up.</p>
<p>The citations are from a wide variety of researchers, but there's one researcher in particular who has published a TON of papers relevant to this question/answer in particular, has all (or at least most) of those papers freely available on his personal site, and has a really engaging and "to the point" writing style, so I want to give him a shout-out. His name is Doug Rohrer. You can read his papers here: <a href="http://drohrer.myweb.usf.edu/pubs.htm" rel="noreferrer">drohrer.myweb.usf.edu/pubs.htm</a></p>
<p>Also check out the following:</p>
<ul>
<li><p>Kirschner, P., &amp; Hendrick, C. (2020). <a href="https://www.taylorfrancis.com/books/mono/10.4324/9780429061523/learning-happens-paul-kirschner-carl-hendrick" rel="noreferrer"><em>How Learning Happens: Seminal Works in Educational Psychology and What They Mean in Practice</em></a></p>
</li>
<li><p>Hattie, J., &amp; Yates, G. C. (2013). <a href="https://www.taylorfrancis.com/books/mono/10.4324/9781315885025/visible-learning-science-learn-john-hattie-gregory-yates" rel="noreferrer"><em>Visible Learning and the Science of How We Learn</em></a></p>
</li>
</ul>
<p>In the comments, OpalE suggests another resource that I agree is worth checking out: <a href="https://www.learningscientists.org/" rel="noreferrer">learningscientists.org</a></p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel announces the Aurora supercomputer has broken the exascale barrier (136 pts)]]></title>
            <link>https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html</link>
            <guid>40348957</guid>
            <pubDate>Mon, 13 May 2024 21:54:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html">https://www.intel.com/content/www/us/en/newsroom/news/intel-powered-aurora-supercomputer-breaks-exascale-barrier.html</a>, See on <a href="https://news.ycombinator.com/item?id=40348957">Hacker News</a></p>
<div id="readability-page-1" class="page">


















    
    
    
        















    
    
        
        
        
        
        
        
        
    




    
    
        
    




    
    
    
    
    
        
    


    
    
        
    


    
    
        
    


    
        
    
    































    


    

































    
        <div data-component="global-nav-redesign" data-component-id="1">
            <header role="banner">
                <nav role="navigation" aria-label="main navigation" data-igm="">
                    <!-- Brand and toggle get grouped for better mobile display -->
                    <div>

                        <div>
                            <a href="https://www.intel.com/content/www/us/en/homepage.html" alt="Intel homepage">
                                    
                                <img src="https://www.intel.com/content/dam/logos/intel-header-logo.svg" height="300" width="118" alt="Intel logo - Return to the home page">
                            </a>
                        </div>

                        

                        <!-- START MOBLE TOGGLE buttons -->
                        <div>


                            <!-- START: NON-signed in panel -->
                            <span id="not-logged-in-scenario">

    
        </span>


                            


















































<span id="logged-in-scenario">






</span>

                            
















    




<div id="panel-language-selector" aria-expanded="false" aria-selected="false">
                <h2>
                    
                        
                            Select Your Language
                        
                        
                    
                </h2>
            </div>


                            <!-- END: NON-sign in panel -->
                            

























    

    



    








    











    
    
    
        
    


    
    <div aria-live="off" id="simplify-search" data-component="wa_skip_track" data-igm-search-content="" document-height="true">
                             
                                 
                            
                            <!-- Search Result Typeahead -->
                            <div id="igm-search-result" data-igm-search-results="">
                                    
                                        Sign In to access restricted content
                                </div>
                            <!-- Recent Searches: 1) display default search info if no search terms is available  -->
                            <!-- Recent Searches: 2) display recenter terms when available and hide default search info  -->
                            <div data-igm-search-related="">
                                <div>
                                    <!-- default search info -->
                                    <div>
                                        <h3>Using Intel.com Search</h3>
                                        <p>You can easily search the entire Intel.com site in several ways.</p>
                                        <ul>
                                            <li>
                                                Brand Name:
                                                <strong>
                                                    Core i9
                                                </strong>
                                            </li>
                                            <li>
                                                Document Number:
                                                <strong>
                                                    123456
                                                </strong>
                                            </li>
                                            <li>
                                                Code Name:
                                                <strong>
                                                    Emerald Rapids
                                                </strong>
                                            </li>
                                            <li>
                                                Special Operators:
                                                <strong>
                                                    “Ice Lake”, Ice AND Lake, Ice OR Lake, Ice*
                                                </strong>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- quick links is always visible on the recents overlay -->
                                    <div>
                                        <h3>Quick Links</h3>
                                        <p>You can also try the quick links below to see results for most popular searches.</p>
                                        <ul>
                                            <!--<li>
                                                <a class="quick-link" rel="noopener noreferrer"
                                                   href=https://ark.intel.com?wapkw=quicklink:product-specifications>
                                                    Product Specifications
                                                </a>
                                            </li>-->
                                            <li>
                                                <a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/products/overview.html?wapkw=quicklink:products">
                                                    Product Information
                                                </a>
                                            </li>
                                            <li><a rel="noopener noreferrer" href="https://www.intel.com/content/www/us/en/support.html?wapkw=quicklink:support">
                                                Support
                                            </a>
                                            </li>
                                            <li>
                                                <a rel="noopener noreferrer" href="https://downloadcenter.intel.com/?wapkw=quicklink:download-center">
                                                    Drivers &amp; Software
                                                </a>
                                            </li>
                                        </ul>
                                    </div>
                                    <!-- recent search terms -->
                                    <div data-component="wa_skip_track" data-component-id="1">
                                            <h3>Recent Searches</h3>
                                        </div>
                                </div>
                                <div>
                                    
                                        Sign In to access restricted content
                                </div>
                            </div>
                            
                                 <div data-igm-advanced-search="">
											<div data-component="wa_skip_track" data-component-id="1">
													<h3>Advanced Search</h3>
													<div>
															<h3>Only search in</h3>
															<div aria-label="Only Search In">
																<label for="search_title">
																	
																	Title</label>

																	<label for="search_description">
																	
																Description</label>

																	<label for="search_id">
																	Content ID</label>
															</div>

															
														</div>
												</div>
											<div>
												Sign in to access
												restricted content.
											</div>
										</div>
                                    
                        </div>


                        </div>
                        <!-- END MOBILE TOGGLE buttons -->
                    </div>
                </nav>
            </header>
        </div>

    
    


<div id="alertMsg" data-scroll-track="false">
                        <p>The browser version you are using is not recommended for this site.<br>Please consider upgrading to the latest version of your browser by clicking one of the following links.</p>
                        <div>
                            <ul>
                                
                                    <li><a href="https://support.apple.com/downloads/safari">Safari</a></li>
                                
                                    <li><a href="https://support.google.com/chrome/answer/95346?hl=en">Chrome</a></li>
                                
                                    <li><a href="https://www.microsoft.com/en-us/edge">Edge</a></li>
                                
                                    <li><a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a></li>
                                
                            </ul>
                        </div>
                    </div>


<main id="primary-content">


























    
        
        
        <main id="primary-content-main">
            

            <div id="articleHero-1" data-component="articleHero" data-component-id="1">
                                                
                                                
                                                    
                                                        <h2>Aurora Supercomputer Ranks Fastest for AI</h2>
                                                    
                                                    
                                                
                                                <div>
                                                    
                                                        <p>At ISC 2024, Intel announces Aurora is the fastest AI supercomputer, has broken the&nbsp;exascale barrier, and details the importance of an open ecosystem in HPC and AI.<b></b></p>

                                                    
                                                </div>
                                                
                                            </div>
            <article data-component-id="1" data-component="articletemplate">
                <div>
                            <div>

                                

                                <!-- duplicated from articlehero for vertical layout -->
                                
                                    
                                    
                                        
                                    
                                

                                
                                <!-- BUILT IN - ARTICLE INTRO COMPONENT -->
                                <!-- duplicated from right column for mobile layout -->
                                
                                <!-- END IN - ARTICLE INTRO COMPONENT -->

                                <!-- BUILT IN - ARTICLE TAKEAWAY COMPONENT -->

                                <div data-component="articleTakeaway" id="articleTakeaway-1" data-component-id="1">
                        <h2>News</h2>
                        <ul>
                            
                                
                                    <li><p>May 13, 2024</p>
                                    </li>
                                
                            
                                
                                    <li><p><a href="https://www.intel.com/content/www/us/en/newsroom/contact-public-relations-team.html">Contact Intel PR</a></p>
                                    </li>
                                
                            
                                
                                    <li><h4><strong>Follow Intel Newsroom on social:</strong></h4>

                                    </li>
                                
                            
                        </ul>
                    </div>

                                

                                <!-- only display this for authored pages and not salesforce pages-->
                                <div>
                                        
                                        <img src="" alt="author-image">
                                    </div>

                            </div>

                            <div>


<div id="articleparagraph-2" data-component="articleparagraph" data-component-id="2">
                                
                                
                                    
                                    
                                        <p><strong>What’s New: </strong>At ISC High Performance 2024, Intel announced in collaboration with Argonne National Laboratory and Hewlett Packard Enterprise (HPE) that the Aurora supercomputer has broken the exascale barrier at 1.012 exaflops and is the fastest AI system in the world dedicated to AI for open science, achieving 10.6 AI exaflops. Intel will also detail the crucial role of open ecosystems in driving AI-accelerated high performance computing (HPC).</p>

                                    
                                
                            </div>
<div id="articleparagraph-3" data-component="articleparagraph" data-component-id="3">
                                <blockquote>
                                    <p>“The Aurora supercomputer surpassing exascale will allow it to pave the road to tomorrow’s discoveries. From understanding climate patterns to unraveling the mysteries of the universe, supercomputers serve as a compass guiding us toward solving truly difficult scientific challenges that may improve humanity.”</p>
                                    
                                    
                                        
                                    
                                </blockquote>
                            </div>
<div id="articleparagraph-4" data-component="articleparagraph" data-component-id="4">
                                
                                
                                    
                                    
                                        <p><strong>Why It Matters:</strong> Designed as an AI-centric system from its inception, Aurora will allow researchers to harness generative AI models to accelerate scientific discovery. Significant progress has been made in <a href="http://www.alcf.anl.gov/news/cells-cosmos-science-research-campaigns-already-underway-aurora">Argonne’s early AI-driven research</a>. Success stories include mapping the human brain’s 80 billion neurons, high-energy particle physics enhanced by deep learning, and drug design and discovery accelerated by machine learning, among others.</p>

<p><strong>Aurora Supercomputer's Details:</strong> The Aurora supercomputer is an expansive system with 166 racks, 10,624 compute blades, 21,248 Intel® Xeon® CPU Max Series processors and 63,744 Intel® Data Center GPU Max Series units, making it one of the world's largest GPU clusters. Aurora also includes the largest open, Ethernet-based supercomputing interconnect on a single system of 84,992 HPE slingshot fabric endpoints.&nbsp;Aurora supercomputer came in second on the high-performance LINPACK (HPL) benchmark but broke the exascale barrier at 1.012 exaflops utilizing 9,234 nodes, only 87% of the system.&nbsp;Aurora supercomputer also secured the third spot on the high-performance conjugate gradient (HPCG) benchmark at 5,612 teraflops per second (TF/s) with 39% of the machine. This benchmark aims to assess more realistic scenarios providing insights into communication and memory access patterns, which are important factors in real-world HPC applications. It complements benchmarks like LINPACK by offering a comprehensive view of a system's capabilities.</p>

<p><strong>How AI is Optimized: </strong>At the heart of the Aurora supercomputer is the Intel Data Center GPU Max Series. The Intel X<sup>e</sup> GPU architecture is foundational to the Max Series, featuring specialized hardware like matrix and vector compute blocks optimized for both AI and HPC tasks. The Intel X<sup>e </sup>architecture’s design that delivers unparalleled compute performance is the reason the Aurora supercomputer secured the top spot in the high-performance LINPACK-mixed precision (HPL-MxP) benchmark – which best highlights the importance of AI workloads in HPC.</p>

<p>The X<sup>e</sup>&nbsp;architecture's parallel processing capabilities excel in managing the intricate matrix-vector operations inherent in neural network AI computation. These compute cores are pivotal in accelerating matrix operations crucial for deep learning models. Complemented by Intel's suite of software tools, including Intel® oneAPI DPC++/C++ Compiler, a rich set of performance libraries, and optimized AI frameworks and tools, the X<sup>e</sup> architecture fosters an open ecosystem for developers that is characterized by flexibility and scalability across various devices and form factors.</p>

<p><strong>Advancing Accelerated Computing with Open Software and Compute Capacity: </strong>In his special session at ISC 2024, on Tuesday, May 14 at 6:45 p.m., (GMT+2) Hall 4, Congress Center Hamburg, Germany, CEO Andrew Richards of Codeplay, an Intel company, will address the growing demand for accelerated computing and software in HPC and AI. He will highlight the importance of oneAPI, offering a unified programming model across diverse architectures. Built on open standards, oneAPI empowers developers to craft code that seamlessly runs on different hardware platforms without extensive modifications or vendor lock-in. This is also the goal of the Linux Foundation’s Unified Acceleration Foundation (UXL), in which Arm, Google, Intel, Qualcomm and others are developing an open ecosystem for all accelerators and unified heterogeneous compute on open standards to break proprietary lock-in. The UXL Foundation is adding more members to its growing coalition.</p>

<p>Meanwhile, <a href="http://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html">Intel® Tiber™ Developer Cloud</a> is expanding its compute capacity with new state-of-the-art hardware platforms and new service capabilities allowing enterprises and developers to evaluate the latest Intel architecture, to innovate and optimize AI models and workloads quickly, and then to deploy AI models at scale. New hardware includes previews of Intel® Xeon® 6 E-core and P-core systems for select customers, and large-scale Intel® Gaudi® 2-based and Intel® Data Center GPU Max Series-based clusters. New capabilities include Intel® Kubernetes Service for cloud-native AI training and inference workloads and multiuser accounts.</p>

<p><strong>What’s Next:</strong> New supercomputers being deployed with Intel Xeon CPU Max Series and Intel Data Center GPU Max Series technologies underscore Intel’s goal to advance HPC and AI. Systems include Euro-Mediterranean Centre on Climate Change’s (CMCC) Cassandra to accelerate climate change modeling; Italian National Agency for New Technologies, Energy and Sustainable Economic Development's (ENEA) CRESCO 8 to enable breakthroughs in fusion energy; Texas Advanced Computing Center (TACC), which is in full production to enable data analysis in biology to supersonic turbulence flows and atomistic simulations on a wide range of materials; as well as United Kingdom Atomic Energy Authority (UKAEA) to solve memory-bound problems that underpin the design of future fusion powerplants.</p>

<p>The result from the mixed-precision AI benchmark will be foundational for Intel’s next-generation GPU for AI and HPC, code-named Falcon Shores. Falcon Shores will leverage the next-generation Intel X<sup>e</sup>&nbsp;architecture with the best of Intel® Gaudi®. This integration enables a unified programming interface.</p>

<p>Early performance results on Intel® Xeon® 6 with P-cores and Multiplexer Combined Ranks (MCR) memory at 8800 megatransfers per second (MT/s) deliver up to 2.3x performance improvement for real-world HPC applications, like Nucleus for European Modeling of the Ocean (NEMO), when compared to the previous generation,<sup>1</sup> setting a strong foundation as the preferred host CPU choice for HPC solutions.</p>

<p><strong>More Context: </strong><a href="http://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html">Intel Tiber Developer Cloud</a> | <a href="http://uxlfoundation.org/">UXL Foundation</a> | <a href="http://www.intel.com/content/www/us/en/newsroom/news/intel-quantum-research-published-in-nature.html#gs.8l4gjh">Intel Takes Next Step Toward Building Scalable Silicon-Based Quantum Processors</a> | <a href="http://www.intel.com/content/www/us/en/newsroom/news/intel-gaudi-xeon-ai-pc-accelerate-meta-llama-3-genai-workloads.html#gs.8l4f4v">Intel Gaudi, Xeon and AI PC Accelerate Meta Llama 3 GenAI Workloads</a></p>

                                    
                                
                            </div>
<div id="articleparagraph-5" data-component="articleparagraph" data-component-id="5">
                                
                                
                                    
                                    
                                        <p><sub><strong>The Small Print:</strong></sub></p>

<p><sup>1</sup> <sub>See ISC 2024 section of <a href="https://intel.com/performanceindex">intel.com/performanceindex</a> for workloads and configurations. Your results may vary.<br>
Intel technologies may require enabled hardware, software or service activation.<br>
Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.&nbsp; No product or component can be absolutely secure.<br>
Intel does not control or audit third-party data. &nbsp;You should consult other sources to evaluate accuracy.</sub></p>

                                    
                                
                            </div>
</div>
                        </div>

                

                
                <div data-component="reference" data-component-id="1" id="reference_copy_copy">
                                
                                
                                    <p><b>About Intel</b></p>
<p>Intel (Nasdaq: INTC) is an industry leader, creating world-changing technology that enables global progress and enriches lives. Inspired by Moore’s Law, we continuously work to advance the design and manufacturing of semiconductors to help address our customers’ greatest challenges. By embedding intelligence in the cloud, network, edge and every kind of computing device, we unleash the potential of data to transform business and society for the better. To learn more about Intel’s innovations, go to <a href="https://newsroom.intel.com/">newsroom.intel.com</a> and <a href="https://www.intel.com/">intel.com</a>.</p>
<p>© Intel Corporation. Intel, the Intel logo and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.</p>
 
                                
                            </div>
                
                    











                
                

            </article>
        </main>
    
    





    




    


</main>












































    







</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unitree G1 Humanoid Agent (184 pts)]]></title>
            <link>https://www.unitree.com/g1/</link>
            <guid>40348531</guid>
            <pubDate>Mon, 13 May 2024 21:14:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.unitree.com/g1/">https://www.unitree.com/g1/</a>, See on <a href="https://news.ycombinator.com/item?id=40348531">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-1f53478b=""><p data-v-1f53478b="">[1] The maximum torque of the joint motors of the whole machine is different. This is the maximum torque of the largest joint motor among them.</p><p data-v-1f53478b="">[2] The maximum load of the arm varies greatly under different arm extension postures.</p><p data-v-1f53478b="">[3] For more information, please read the secondary development manual.</p><p data-v-1f53478b="">[4] For more detailed warranty terms, please read the product warranty brochure.</p><p data-v-1f53478b="">[5] The above parameters may vary in different scenarios and configurations, please subject to actual situation.</p><p data-v-1f53478b="">[6] The humanoid robot has a complex structure and extremely powerful power. Users are asked to keep a sufficient safe distance between the humanoid robot and the humanoid robot.Please use with caution</p><p data-v-1f53478b="">[7] If any change in the appearance of the product, please refer to the actual product.</p><p data-v-1f53478b="">[8] Some sample functions on this page are still being developed and tested, and will be opened to users in the future.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Release of Fugaku-LLM – a large language model trained on supercomputer Fugaku (103 pts)]]></title>
            <link>https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html</link>
            <guid>40348371</guid>
            <pubDate>Mon, 13 May 2024 21:01:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html">https://www.fujitsu.com/global/about/resources/news/press-releases/2024/0510-01.html</a>, See on <a href="https://news.ycombinator.com/item?id=40348371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><img alt="" src="https://www.fujitsu.com/global/imagesgig5/top-r_tcm100-6314163_tcm100-2750236-32.png"></p></div><!--Title--><!--Headline--><h2>Enhanced Japanese language ability, for use in research and business</h2><!--Business Organization--><h2>Tokyo Institute of Technology, Tohoku University, Fujitsu Limited, RIKEN, Nagoya University, CyberAgent Inc., Kotoba Technologies Inc.</h2><!--City, Press Release Date--><p><strong>Kawasaki, May 10, 2024</strong></p><h3>Summary</h3><ul><li>
Large language model with enhanced Japanese language ability was developed using Japanese supercomputing technology
</li><li>
Distributed parallel learning by maximizing the performance of the supercomputer “Fugaku”
</li><li>
Commercial use is permitted, which will lead to innovative research and business applications such as AI for Science
</li></ul><div><h3>Abstract</h3><p>
A team of researchers in Japan released Fugaku-LLM, a large language model <sup>(<a href="#footnote1">1</a>)</sup> with enhanced Japanese language capability, using the RIKEN supercomputer Fugaku. The team is led by Professor Rio Yokota of Tokyo Institute of Technology, Associate Professor Keisuke Sakaguchi of Tohoku University, Koichi Shirahata of Fujitsu Limited, Team Leader Mohamed Wahib of RIKEN, Associate Professor Koji Nishiguchi of Nagoya University, Shota Sasaki of CyberAgent, Inc, and Noriyuki Kojima of Kotoba Technologies Inc.
</p><p>
To train large language models on Fugaku, the researchers developed distributed training methods, including porting the deep learning framework Megatron-DeepSpeed to Fugaku in order to optimize the performance of Transformers on Fugaku. They accelerated the dense matrix multiplication library for Transformers, and optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication library on the Tofu interconnect D.
</p><p>
Fugaku-LLM has 13 billion parameters <sup>(<a href="#footnote2">2</a>)</sup> and is larger than the 7-billion-parameter models that have been developed widely in Japan. Fugaku-LLM has enhanced Japanese capabilities, with an average score of 5.5 on the Japanese MT-Bench <sup>(<a href="#footnote3">3</a>)</sup>, the highest performance among open models that are trained using original data produced in Japan. In particular, the benchmark performance for humanities and social sciences tasks reached a remarkably high score of 9.18.
</p><p>
Fugaku-LLM was trained on proprietary Japanese data collected by CyberAgent, along with English data, and other data. The source code of Fugaku-LLM is available on GitHub <sup>(<a href="#footnote4">4</a>)</sup> and the model is available on Hugging Face <sup>(<a href="#footnote5">5</a>)</sup>. Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license.
</p><p>
In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.
</p></div><!--Paragraph - body--><h3>Background</h3><p>
In recent years, the development of large language models (LLMs) has been active, especially in the United States. In particular, the rapid spread of ChatGPT <sup>(<a href="#footnote6">6</a>)</sup>, developed by OpenAI, has profoundly impacted research and development, economic systems, and national security. Countries other than the U.S. are also investing enormous human and computational resources to develop LLMs in their own countries. Japan, too, needs to secure computational resources for AI research so as not to fall behind in this global race. There are high expectations for Fugaku, the flagship supercomputer system in Japan, and it is necessary to improve the computational environment for large-scale distributed training on Fugaku to meet these expectations.
</p><p>
 Therefore, Tokyo Institute of Technology, Tohoku University, Fujitsu, RIKEN, Nagoya University, CyberAgent, and Kotoba Technologies have started a joint research project on the development of large language models.
</p><h3>Role of each institution/company</h3><p><strong>Tokyo Institute of Technology:</strong> General oversight, parallelization and communication acceleration of large language models (optimization of communication performance by combining three types of parallelization, acceleration of collective communication on the Tofu interconnect D)
</p><p><strong>Tohoku University:</strong> Collection of training data and model selection
</p><p><strong>Fujitsu:</strong> Acceleration of computation and communication (acceleration of collective communication on Tofu interconnect D, performance optimization of pipeline parallelization) and implementation of pre-training and fine-tuning after training
</p><p><strong>RIKEN:</strong> Distributed parallelization and communication acceleration of large-scale language models (acceleration of collective communication on Tofu interconnect D)
</p><p><strong>Nagoya University:</strong> Study on application methods of Fugaku-LLM to 3D generative AI
</p><p><strong>CyberAgent:</strong> Provision of training data
</p><p><strong>Kotoba Technologies:</strong> Porting of deep learning framework to Fugaku
</p><!--Image--><div><p><img alt="Figure 1. RIKEN‘s supercomputer Fugaku ©RIKEN" src="https://www.fujitsu.com/global/imagesgig5/20240510-01a_tcm100-7566208_tcm100-2750236-32.jpg"><span>Figure 1. RIKEN‘s supercomputer Fugaku ©RIKEN</span></p></div><h3>Research outcome</h3><h3>1. Significantly improved the computational performance of training large language models on the supercomputer Fugaku</h3><p>
 GPUs <sup>(<a href="#footnote7">7</a>)</sup> are the common choice of hardware for training large language models. However, there is a global shortage of GPUs due to the large investment from many countries to train LLMs. Under such circumstances, it is important to show that large language models can be trained using Fugaku, which uses CPUs instead of GPUs. The CPUs used in Fugaku are Japanese CPUs manufactured by Fujitsu, and play an important role in terms of revitalizing Japanese semiconductor technology.
</p><p>
By extracting the full potential of Fugaku, this study succeeded in increasing the computation speed of the matrix multiplication by a factor of 6, and the communication speed by a factor of 3. To maximize the distributed training performance on Fugaku, the deep learning framework Megatron-DeepSpeed was ported to Fugaku, and the dense matrix multiplication library was accelerated for Transformer. For communication acceleration, the researchers optimized communication performance for Fugaku by combining three types of parallelization techniques and accelerated the collective communication on the Tofu interconnect D. The knowledge gained from these efforts can be utilized in the design of the next-generation computing infrastructure after Fugaku and will greatly enhance Japan's future advantage in the field of AI.
</p><h3>2. An easy-to-use, open, and secure, large language model with 13 billion parameters</h3><p>
In 2023, many large language models were developed by Japanese companies, but most of them have less than 7 billion parameters. Since the performance of large-scale language models generally improves as the number of parameters increases, the 13-billion-parameter model the research team developed is likely to be more powerful than other Japanese models. Although larger models have been developed outside of Japan, large language models also require large computational resources, making it difficult to use models with too many parameters. Fugaku-LLM is both high performance and well-balanced.
</p><p>
In addition, most models developed by Japanese companies employ continual learning <sup>(<a href="#footnote8">8</a>)</sup>, in which open models developed outside of Japan are continually trained on Japanese data. In contrast, Fugaku-LLM is trained from scratch using the team’s own data, so the entire learning process can be understood, which is superior in terms of transparency and safety.
</p><p>
Fugaku-LLM was trained on 380 billion tokens using 13,824 nodes of Fugaku, with about 60% of the training data being Japanese, combined with English, mathematics, and code. Compared to models that continually train on Japanese, Fugaku-LLM learned much of its information in Japanese. Fugaku-LLM is the best model among open models that are produced in Japan and trained with original data. In particular, it was confirmed that the model shows a high benchmark score of 9.18 in the humanities and social sciences tasks. It is expected that the model will be able to perform natural dialogue based on keigo (honorific speech) and other features of the Japanese language.
</p><h3>Future Development</h3><p>
The results from this research are being made public through GitHub and Hugging Face so that other researchers and engineers can use them to further develop large language models. Fugaku-LLM can be used for research and commercial purposes as long as users comply with the license. Fugaku-LLM will be also offered to users via the Fujitsu Research Portal from May 10th, 2024.
</p><p>
In the future, as more researchers and engineers participate in improving the models and their applications, the efficiency of training will be improved, leading to next-generation innovative research and business applications, such as the linkage of scientific simulation and generative AI, and social simulation of virtual communities with thousands of AIs.
</p><h3>Acknowledgement</h3><p>
This research was supported by the Fugaku policy-supporting proposal "Development of Distributed Parallel Training for Large Language Models Using Fugaku" (proposal number: hp230254).
</p><!--Paragraph - body end--><!--Footnotes--><hr><ul><li><p><strong>[1]</strong></p><p><strong>Large language model :</strong><br>			
			Models the probability with which text appears and can predict the text (response) that follows a given context (query).
		</p></li><li><p><strong>[2]</strong></p><p><strong>Parameter :</strong><br>			
			A measure of the size of a neural network. The more parameters, the higher the performance of the model, but the more data is required for training.
		</p></li><li><p><strong>[3]</strong></p><p><strong>Japanese MT-Bench :</strong><br>			
			Benchmark test provided by Stability AI
		</p></li><li><p><strong>[4]</strong></p></li><li><p><strong>[5]</strong></p></li><li><p><strong>[6]</strong></p><p><strong>ChatGPT :</strong><br>			
			A large language model developed by OpenAI, which has brought about a major social change, surpassing 100 million users in about two months after its release.
		</p></li><li><p><strong>[7]</strong></p><p><strong>GPU :</strong><br>			
			Originally produced as an accelerator for graphics, but has recently been used to accelerate deep learning
		</p></li><li><p><strong>[8]</strong></p><p><strong>Continual learning :</strong><br>			
			A method for performing additional training on a large language model that has already been trained. Used for training language models in different languages or domains.
		</p></li></ul><!--Boilerplate(s)--><!--About Fujitsu--><div><h3>About Fujitsu</h3><p>Fujitsu’s purpose is to make the world more sustainable by building trust in society through innovation. As the digital transformation partner of choice for customers in over 100 countries, our 124,000 employees work to resolve some of the greatest challenges facing humanity. Our range of services and solutions draw on five key technologies: Computing, Networks, AI, Data &amp; Security, and Converging Technologies, which we bring together to deliver sustainability transformation. Fujitsu Limited (TSE:6702) reported consolidated revenues of 3.7 trillion yen (US$26 billion) for the fiscal year ended March 31, 2024 and remains the top digital services company in Japan by market share. Find out more: <a href="https://www.fujitsu.com/">www.fujitsu.com</a>.</p></div><!--Media Contacts--><h3>Press Contacts</h3><p><strong>Fujitsu Limited</strong><br>
	Public and Investor Relations Division
	<br><a href="https://www.fujitsu.com/global/about/resources/news/presscontacts/form/index.html">Inquiries</a></p><!--Legal Notes--><hr><p>All company or product names mentioned herein are trademarks or registered trademarks of their respective owners. Information provided in this press release is accurate at time of publication and is subject to change without advance notice.</p><!--Press Release Date, City, and Company Name--><p><strong>Date:</strong> 10 May, 2024
<br><strong>City:</strong> Kawasaki, Japan
<br><strong>Company:</strong> Tokyo Institute of Technology, Tohoku University, Fujitsu Limited, RIKEN, Nagoya University, CyberAgent Inc., Kotoba Technologies Inc.
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DOS game "F-15 Strike Eagle II" reverse engineering/reconstruction war stories (215 pts)]]></title>
            <link>https://neuviemeporte.github.io/category/f15-se2.html</link>
            <guid>40347662</guid>
            <pubDate>Mon, 13 May 2024 20:00:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neuviemeporte.github.io/category/f15-se2.html">https://neuviemeporte.github.io/category/f15-se2.html</a>, See on <a href="https://news.ycombinator.com/item?id=40347662">Hacker News</a></p>
<div id="readability-page-1" class="page">
<main aria-label="Content">
      <div>
  <h2>Posts in category : f15-se2</h2>
  <ul>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2024/05/05/ghidra.html">Ghidra to the rescue</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2024/01/07/unlink2.html">More delinking fun</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/12/30/unlink.html">First steps in delinking</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/11/08/imatching2.html">Trying to think like a compiler, Part 2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/10/06/linking.html">Mixed-language linking misadventures</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/09/25/reassembly.html">Reassembling the disassembly</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/09/02/compiler3.html">Hunting for the Right Compiler, Part 3</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/07/13/imatching.html">Trying to think like a compiler</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/07/12/overlays.html">Usage of overlays in F15 SE2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/05/18/compiler2.html">Hunting for the Right Compiler, Part 2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/05/17/compiler.html">Hunting for the Right Compiler, Part 1</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2023/03/23/farcalls.html">Chasing far calls</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-3.html">What does it take to take an old game apart? (Part 3)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-2.html">What does it take to take an old game apart? (Part 2)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/09/reversing-1.html">What does it take to take an old game apart? (Part 1)</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/12/08/firstlook.html">Having a first look around F-15 SE2</a></li>
    
      <li><a href="https://neuviemeporte.github.io/f15-se2/2022/06/05/origins.html">F-15 Strike Eagle II: The origin story</a></li>
    
  </ul>
</div>
    </main>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pi-C.A.R.D, a Raspberry Pi Voice Assistant (325 pts)]]></title>
            <link>https://github.com/nkasmanoff/pi-card</link>
            <guid>40346995</guid>
            <pubDate>Mon, 13 May 2024 19:03:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nkasmanoff/pi-card">https://github.com/nkasmanoff/pi-card</a>, See on <a href="https://news.ycombinator.com/item?id=40346995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Pi-C.A.R.D</h2><a id="user-content-pi-card" aria-label="Permalink: Pi-C.A.R.D" href="#pi-card"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nkasmanoff/pi-card/blob/main/assets/assistant.png"><img src="https://github.com/nkasmanoff/pi-card/raw/main/assets/assistant.png" height="300"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#hardware">Hardware</a></li>
<li><a href="#setup">Setup</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Pi-card is an AI powered voice assistant running entirely on a Raspberry Pi. It is capable of doing anything a standard LLM (like ChatGPT) can do in a conversational setting.
In addition, if there is a camera equipped, you can also ask Pi-card to take a photo, describe what it sees, and then ask questions about that image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Pi-card?</h3><a id="user-content-why-pi-card" aria-label="Permalink: Why Pi-card?" href="#why-pi-card"></a></p>
<p dir="auto">Raspberry <strong>Pi</strong> - <strong>C</strong>amera <strong>A</strong>udio <strong>R</strong>ecognition <strong>D</strong>evice.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nkasmanoff/pi-card/blob/main/assets/picard-facepalm.jpg"><img src="https://github.com/nkasmanoff/pi-card/raw/main/assets/picard-facepalm.jpg" height="300"></a></p>
<p dir="auto">Please submit an issue or pull request if you can think of a better way to force this ackronym.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work?</h3><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">Pi-card runs entirely on your Raspberry Pi. Once the main program is run, the system will listen for your wake word. Once your wake word has been said, you are officially in a conversation. Within this conversation you do not need to constantly repeat the wake word. The system will continue to listen for your commands until you say something like "stop", "exit", or "goodbye".</p>
<p dir="auto">The system has a memory of the conversation while you have it, meaning if you want the assistant to repeat something it said, or elaborate on a previous topic, you can do so.</p>
<p dir="auto">While the system is designed to be entirely local, it is also possible to easily connect it to some external APIs or services if you want to enhance the conversation, or give it control to some external devices. To do so is something I am open to improving, but for now it will be done based on specific keywords to trigger the external service. A good example of this is that for camera purposes, the system will activate the camera if you say "take a photo" or "what do you see".</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How useful is it?</h3><a id="user-content-how-useful-is-it" aria-label="Permalink: How useful is it?" href="#how-useful-is-it"></a></p>
<p dir="auto">The system is designed to be a fun project that can be a <em>somewhat</em> helpful AI assistant. Since everything is done locally, the system will not be as capable, or as fast, as cloud based systems. However, the system is still capable of a lot of improvements to be made.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why isn't this an app?</h3><a id="user-content-why-isnt-this-an-app" aria-label="Permalink: Why isn't this an app?" href="#why-isnt-this-an-app"></a></p>
<p dir="auto">The main reason for this is that I wanted to create a voice assistant that is completely offline and doesn't require any internet connection. This is because I wanted to ensure that the user's privacy is protected and that the user's data is not being sent to any third party servers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">After downloading the repository, installing the requirements, and following the other setup instructions, you can run the main program by running the following command:</p>

<p dir="auto">Once the program is running, you can start a conversation with the assistant by saying the wake word. The default wake word is "hey assistant", but you can change this in the <code>config.py</code> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware</h2><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<ul dir="auto">
<li>Raspberry Pi 5 Model B</li>
<li>USB Microphone</li>
<li>Speaker</li>
<li>Camera</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<p dir="auto">To keep this system as fast and lean as possible, we use cpp implementations of the audio transcription and vision language models. These are done with the wonderful libraries <a href="https://github.com/ggerganov/whisper.cpp">whipser.cpp</a> for the audio transcription and <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> for the vision language model.</p>
<p dir="auto">In both cases, please clone these repositories wherever you like, and add their paths to the <code>config.py</code> file.</p>
<p dir="auto">Once cloned, please go to each repository, and follow the setup instructions to get the models running. Some pointers are given below:</p>
<p dir="auto">For llama.cpp, we are using the vision language model capabilities, which are slightly different from the standard setup. You will need to follow the setup instructions for <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md">LlaVA</a>, but update the model to be used to be one better suited for this device, <a href="https://github.com/nkasmanoff/pi-card/blob/main/moondream.ai">Moondream2</a></p>
<p dir="auto">To install Moondream, you'll need to go to HuggingFace model hub, and download the model. I did so using python, with the following commands. Once again, make sure the vision model path is added to the <code>config.py</code> file.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from huggingface_hub import snapshot_download
model_id=&quot;vikhyatk/moondream2&quot;
snapshot_download(repo_id=model_id, local_dir=your/local/path, local_dir_use_symlinks=False, revision=&quot;main&quot;)"><pre><span>from</span> <span>huggingface_hub</span> <span>import</span> <span>snapshot_download</span>
<span>model_id</span><span>=</span><span>"vikhyatk/moondream2"</span>
<span>snapshot_download</span>(<span>repo_id</span><span>=</span><span>model_id</span>, <span>local_dir</span><span>=</span><span>your</span><span>/</span><span>local</span><span>/</span><span>path</span>, <span>local_dir_use_symlinks</span><span>=</span><span>False</span>, <span>revision</span><span>=</span><span>"main"</span>)</pre></div>
<p dir="auto">For whisper.cpp, you will need to follow the quick-start guide in the <a href="https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#quick-start">README</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware-1" aria-label="Permalink: Hardware" href="#hardware-1"></a></p>
<p dir="auto">The hardware setup is quite simple. You will need a Raspberry Pi 5 Model B, a USB microphone, a speaker, and a camera.</p>
<p dir="auto">The USB microphone and speaker can be plugged into the Raspberry Pi's USB ports. The camera can be connected to the camera port on the Raspberry Pi.</p>
<p dir="auto">I used the following hardware for my setup:</p>
<ul dir="auto">
<li><a href="https://www.amazon.com/dp/B0CRSNCJ6Y?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">Raspberry Pi 5 Kit</a></li>
<li><a href="https://www.amazon.com/dp/B087PTH787?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">USB Microphone</a></li>
<li><a href="https://www.amazon.com/dp/B075M7FHM1?ref=ppx_yo2ov_dt_b_product_details&amp;th=1" rel="nofollow">Speaker</a></li>
<li><a href="https://www.amazon.com/dp/B012V1HEP4?ref=ppx_yo2ov_dt_b_product_details&amp;th=1" rel="nofollow">Camera</a></li>
<li><a href="https://www.amazon.com/dp/B0716TB6X3?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details" rel="nofollow">Camera Connector</a></li>
</ul>
<p dir="auto">Please note Pi 5's have a new camera port, hence the new camera connector.</p>
<p dir="auto">Feel free to use your own, this is what worked for me.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>